label,summarydescription
1,"IndexWriter can flush too early when flushing by RAM usage. There is a silly bug in how DocumentsWriter tracks its RAM usage:
whenever term vectors are enabled, it incorrectly counts the space
used by term vectors towards flushing, when in fact this space is
recycled per document.

This is not a functionality bug.  All it causes is flushes to happen
too frequently, and, IndexWriter will use less RAM than you asked it
to.  To work around it you can simply give it a bigger RAM buffer.

I will commit a fix shortly."
1,"Indexing configuration ignored when indexing length. The NodeIndexer does not respect the indexing configuration when it adds a field for the property length. NodeIndexer.addValue(Document doc, InternalValue value, Name name) should check it the property actually needs to be indexed."
1,"Mixin removal exception. When trying to remove a mixin from a non nt:unstructured node (in my case nt:resource), you get the following exception:

Unable to alter mixin types: javax.jcr.nodetype.ConstraintViolationException: no matching property definition found for {http://www.day.com/jcr/cq/1.0}lastRolledout

lastRolledout property is defined by the mixin cq:LiveRelationship that I am trying to remove."
1,"problem in jcr-server/project.properties dependencies?. There seems to be a mismatch in the version numbers in the dependencies defined in jcr-server/project.properties. It refers to version 1.0, yet the other components are already at 1.1.

The attached patch solves the problem for me, but I don't claim to fully understand how this works...
"
1,FastVectorHighlighter: highlighted term is out of alignment in multi-valued NOT_ANALYZED field. 
1,"literal plus (+) character in path components of HttpURL is not preserved.. When a literal plus character is included in the path component of an URL, it is
not encoded, but get decoded during getPath() to a space.

Reproducible with the following:

HttpURL httpURL = new HttpURL(""http://localhost/test+test"");
System.out.println(httpURL.getPath());

Output:
""test test""

The following path fixes the issue (This patch does not appear to break anything
 else):

Patch against SVN Repo:
URL: http://svn.apache.org/repos/asf/jakarta/commons/proper/httpclient/trunk
Repository UUID: 13f79535-47bb-0310-9956-ffa450edef68
Revision: 405803

Index: src/java/org/apache/commons/httpclient/URI.java
===================================================================
--- src/java/org/apache/commons/httpclient/URI.java (revision 405803)
+++ src/java/org/apache/commons/httpclient/URI.java (working copy)
@@ -1552,6 +1552,7 @@
         allowed_abs_path.or(abs_path);
         // allowed_abs_path.set('/');  // aleady included
         allowed_abs_path.andNot(percent);
+        allowed_abs_path.clear('+');
     }


@@ -1563,6 +1564,7 @@
     static {
         allowed_rel_path.or(rel_path);
         allowed_rel_path.clear('%');
+        allowed_rel_path.clear('+');
     }"
1,"Nullpointer when creating URI from scheme specific part with null fragment. in org.apache.commons.httpclient.URI class constructor:

public URI(String scheme, String schemeSpecificPart, String fragment)
        throws URIException {
....
_fragment = fragment.toCharArray(); 

should be 

_fragment = fragment==null ? null : fragment.toCharArray();"
1,"LockTest.testLogout fails to refresh session before checking lock from other session. LockTest.testLogout() fails to refresh the session before checking the lock state of a node that was locked by another session.

Proposal:

Insert 

  n1.refresh(false);

before 

  assertTrue(""node must be locked"", n1.isLocked());

"
1,"unsynchronized access on 'itemCache' map in ItemManager . the access 'itemCache' map in ItemManager is mostly synchronized by not via the ItemStateListener methods:
[...]
    public void stateCreated(ItemState created) {
        ItemImpl item = retrieveItem(created.getId());
        if (item != null) {
            item.stateCreated(created);
        }
    }
[...]
    private ItemImpl retrieveItem(ItemId id) {
        return (ItemImpl) itemCache.get(id);
    }
[...]

this can result in a corruption of a map (eg subsequent accesses may result in a endless loop)."
1,"Node.getWeakReferences throws UnsupportedOperationException if not referenceable. .... while Node.getReferences() doesn't and returns an empty propertyiterator.

From my point of view both methods should behave the same way and i prefer not throwing.

"
1,"Bundle of events may be dropped due to NP.. In [1], if the resolver fails to lookup a node entry, a NP is thrown. This exception will break the loop which forwards the events to the observer in [2].
This will result in an observer not receiving events that he should have.

[1] org.apache.jackrabbit.jcr2spi.hierarchy.HierarchyManagerImpl#lookup(ItemId workspaceItemId)
[2] org.apache.jackrabbit.jcr2spi.WorkspaceManager#onEventReceived(EventBundle[] eventBundles,InternalEventListener[] lstnrs)"
1,"DatabaseJournal: java.lang.IllegalStateException: already in batch mode. Using the database journal (any database) fails with the following stack trace:

java.lang.IllegalStateException: already in batch mode
	at org.apache.jackrabbit.core.util.db.ConnectionHelper.startBatch(ConnectionHelper.java:212)
	at org.apache.jackrabbit.core.journal.DatabaseJournal.doSync(DatabaseJournal.java:449)
	at org.apache.jackrabbit.core.journal.AbstractJournal.lockAndSync(AbstractJournal.java:254)
	at org.apache.jackrabbit.core.journal.DefaultRecordProducer.append(DefaultRecordProducer.java:51)
	at org.apache.jackrabbit.core.cluster.ClusterNode$WorkspaceUpdateChannel.updateCreated(ClusterNode.java:539)
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:559)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:1457)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:1487)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:351)
	at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:354)"
1,"NPE when calling isCurrent() on a ParallellReader. As demonstrated by the test case below, if you call isCurrent() on a ParallelReader it causes an NPE. Fix appears to be to add an isCurrent() to ParallelReader which calls it on the underlying indexes but I'm not sure what other problems may be lurking here. Do methods such as getVersion(), lastModified(), isOptimized() also have to be rewritten or is this a use case where ParallelReader will never mimic IndexReader perfectly? At the very least this behavior should be documented so others know what to expect.


    [junit] Testcase: testIsCurrent(org.apache.lucene.index.TestParallelReader):        Caused an ERROR
    [junit] null
    [junit] java.lang.NullPointerException
    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:502)
    [junit]     at org.apache.lucene.index.SegmentInfos.readCurrentVersion(SegmentInfos.java:336)
    [junit]     at org.apache.lucene.index.IndexReader.isCurrent(IndexReader.java:316)
    [junit]     at org.apache.lucene.index.TestParallelReader.testIsCurrent(TestParallelReader.java:146)



Index: src/test/org/apache/lucene/index/TestParallelReader.java
===================================================================
--- src/test/org/apache/lucene/index/TestParallelReader.java    (revision 518122)
+++ src/test/org/apache/lucene/index/TestParallelReader.java    (working copy)
@@ -135,6 +135,15 @@
       assertEquals(docParallel.get(""f4""), docSingle.get(""f4""));
     }
   }
+  
+  public void testIsCurrent() throws IOException {
+    Directory dir1 = getDir1();
+    Directory dir2 = getDir2();
+    ParallelReader pr = new ParallelReader();
+    pr.add(IndexReader.open(dir1));
+    pr.add(IndexReader.open(dir2));
+    assertTrue(pr.isCurrent());
+  }
 
   // Fiels 1-4 indexed together:
   private Searcher single() throws IOException {
"
1,"wrong charset indication in HttpConstants.getContentString(). Around line 236 in HttpConstants.getConstentString() the charset is wrongly indicated as 
""DEFAULT_CONTENT_CHARSET"" where it should have been indicated as ""charset"" like in the 
getContentBytes function.

            if (LOG.isWarnEnabled()) {
                LOG.warn(""Unsupported encoding: "" 
                    + DEFAULT_CONTENT_CHARSET // <== should be the variable ""charset"" here
                    + "". Default HTTP encoding used"");
            }

Wrong copy/paste I guess :-)

ZC."
1,"TestIndexWriterExceptions random failure: AIOOBE in ByteBlockPool.allocSlice. TestIndexWriterExceptions threw this today, and its reproducable"
1,"Observation events are not triggered for intermediate nodes in version storage. When a new version history is created no observation events are triggered for the intermediate nodes.

E.g. for the VersionHistory d94d4b41-f68e-4586-9e88-96e6790981d8 the following events are triggered (with a node filter applied, property events are not visible):

Node added: /jcr:system/jcr:versionStorage/d9/4d/4b/d94d4b41-f68e-4586-9e88-96e6790981d8
Node added: /jcr:system/jcr:versionStorage/d9/4d/4b/d94d4b41-f68e-4586-9e88-96e6790981d8/jcr:versionLabels
Node added: /jcr:system/jcr:versionStorage/d9/4d/4b/d94d4b41-f68e-4586-9e88-96e6790981d8/jcr:rootVersion
Node added: /jcr:system/jcr:versionStorage/d9/4d/4b/d94d4b41-f68e-4586-9e88-96e6790981d8/jcr:rootVersion/jcr:frozenNode

Observation should also trigger node added events for:
/jcr:system/jcr:versionStorage/d9
/jcr:system/jcr:versionStorage/d9/4d
/jcr:system/jcr:versionStorage/d9/4d/4b"
1,"[PATCH] GermanAnalyzer fails silently + doesn't close files. As mentioned on the developer list, the German analyzer will assume an empty list of 
stopwords if the stopword file isn't found. I'll attach a patch that makes it throw an 
IOException instead. Also the patch makes sure the file readers are closed."
1,"If index has more than Integer.MAX_VALUE terms, seeking can it AIOOBE due to long/int overflow. Tom hit a new long/int overflow case: http://markmail.org/thread/toyl2ujcl4suqvf3

This is a regression, in 3.1, introduced with LUCENE-2075.

Worse, our Test2BTerms failed to catch this, so I've fixed that test to show the failure."
1,"small float underflow detection bug. Underflow detection in small floats has a bug, and can incorrectly result in a byte value of 0 for a non-zero float."
1,"NRTCachingDir has invalid asserts (if same file name is written twice). Normally Lucene is write-once (except for segments.gen file, which NRTCachingDir never caches), but in some tests (TestDoc, TestCrash) we can write the same file more than once.

I don't think NRTCachingDir should have these asserts, and I think on createOutput it should remove any old file if present.

I also found & fixed a possible concurrency issue (if more than one thread syncs at the same time; IndexWriter doesn't ever do this today but it has in the past)."
1,"DefaultHttpParamsFactory.getDefaultParams() is not thread safe. The method getDefaultParams() in 
org.apache.commons.httpclient.params.DefaultHttpParamsFactory is not thread 
safe.  In this code:

    public HttpParams getDefaultParams() {
        if (httpParams == null) {
            httpParams = createParams();
        }

        return httpParams;
    }

it is possible that httpParams will be called by one thread which will set 
httpParams, then a second thread may call it and may find httpParams is 
non-null.  However, under both the old (Java Language Spec chapter 17) and 
new Java Memory Models, the second thread won't necessarily see the values 
the first thread has set in the referenced HttpParams object.

The easiest way to fix this for all JVMs and memory models is by declaring 
getDefaultParams() to be synchronized."
1,"Test case failure for testConnTimeout. [java] There was 1 failure:
     [java] 1)
testConnTimeout(org.apache.commons.httpclient.TestHttpConnection)junit.framework.AssertionFailedError:
Should have timed out
     [java]     at
org.apache.commons.httpclient.TestHttpConnection.testConnTimeout(TestHttpConnection.java:118)
     [java]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
     [java]     at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
     [java]     at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)

This test has been failing for some time.  It is run with the test-local ant target."
1,New BufferedIndexOutput optimization fails to update bufferStart. New BufferIndexOutput optimization of writeBytes fails to update bufferStart under some conditions. Test case and fix attached.
1,"ArrayIndexOutOfBounds thrown on re-index of repository. I encountered a problem with the Lucene NodeIndexer when forcing the repository to re-index itself.

Using the default repository.xml file provided with the examples contribution, I loaded a number of PDF files using the sample application FSImport.  In this utility, the ""encoding"" property is set to the empty string """" for all the files.  The system appeared to index everything properly.  I then stopped the repository, deleted the index files and then restarted the repositoyr.  Re-indexing was initiated and a ""ArrayIndexOutOfBoundsException"" was thrown from the org.apache.jackrabbit.core.query.lucene.NodeIndexer.java

The code in question:

                // jcr:encoding is not mandatory
                String encoding = null;
                if (node.hasPropertyName(JCR_ENCODING)) {
                    PropertyState encodingProp =
                            (PropertyState) stateProvider.getItemState(new PropertyId(node.getUUID(), JCR_ENCODING));
                    encodingProp.getValues()[0].internalValue().toString();


                }

Expects the encodingProperty to be set if the property exists.  However, the node has the property, but the XMLPersistenceManager did not create any entries in the property array.  Either there is a problem in the XMLPersistenceManager (zero length string issues), or the NodeIndexer needs to be altered to verify that there is actually a value for a particular property.

Since the jcr:encoding property is not considered a multi-value property, the requirement to check for an initialized array is probably not the correct route.

Looking at the code for the XMLPersistenceManager readState(DOMWalker walker, PropertyState state) method (line 294), it indicates that if the content length for a property is zero, the property will not have a value added.  However, our encoding property is configured as the empty string and should be created.  Therefore, a suggested alteration is to check if the property is a string, and, even if zero length, add the property value.

        ArrayList values = new ArrayList();
        if (walker.enterElement(VALUES_ELEMENT)) {
            while (walker.iterateElements(VALUE_ELEMENT)) {
                // read serialized value
                String content = walker.getContent();
                if ((content.length() > 0) || (PropertyType.STRING == type)) {   // <==== suggested update
                    if (type == PropertyType.BINARY) {
                        // special handling required for binary value:
                        // the value stores the path to the actual binary file in the blob store
                        try {
                            values.add(InternalValue.create(new FileSystemResource(blobStore, content)));
                        } catch (IOException ioe) {
                            String msg = ""error while reading serialized binary value"";
                            log.debug(msg);
                            throw new ItemStateException(msg, ioe);
                        }
                    } else {
                        values.add(InternalValue.valueOf(content, type));
                    }
                }
            }
            walker.leaveElement();
        }
"
1,"IndexMerger throws null pointer exception without stacktrace. I get the following errors in my log file randomly.  It seems to happen most often when creating the lucene indices, but has happened at other times as well:

[IndexMerger] ERROR - Error while merging indexes: java.lang.NullPointerException

The code at org.apache.jackrabbit.core.query.lucene.IndexMerger line 344 appears to be the point where the error is logged, but no other information is provided because the throwable isn't sent to the log (only the toString() version of the exception).  I haven't been able to tell if any indexes are corrupt when this happens.

I suggest that the logger be changed to determine where the null pointer is coming from first, then resolve the actual issue that is occurring.
"
1,"NullPointerException in ItemState. The following happens quite regularly when multiple threads are adding, retrieving and removing nodes simultaneously. Looking at the code of the pull method, this seems due to under-synchronization somewhere as overlayedState was tested at line 153 for null.

java.lang.NullPointerException
        at org.apache.jackrabbit.core.state.ItemState.pull(ItemState.java:156)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.stateModified(LocalItemStateManager.java:421)
        at org.apache.jackrabbit.core.state.StateChangeDispatcher.notifyStateModified(StateChangeDispatcher.java:85)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.stateModified(SharedItemStateManager.java:434)
        at org.apache.jackrabbit.core.state.ItemState.notifyStateUpdated(ItemState.java:241)
        at org.apache.jackrabbit.core.state.ChangeLog.persisted(ChangeLog.java:271)
        at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:741)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:937)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:327)
        at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:313)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:303)
        at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:307)
        at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1374)"
1,"DefaultAccessManager#hasPrivileges(String,Set,Privilege[]) doesn't close compiled permissions. DefaultAccessManager#hasPrivileges(String,Set,Privilege[]) retrieves the compiled permissions for the specified set of principals
from the ac provider but omit the CompiledPermissions#close() call before returning."
1,"insufficient privileges. HI,
In Jackrabbit  DBStore, On the fly its creating some tables in DB .  But, In our Dev environment we do not have permission for creating tables on the fly. So, I manually inserted all the dll (tables & indexes) before the application start. Although I'm getting the following exception while running application. 

Attached repository.xml.

Below the log.

[11/23/09 11:32:04:405 EST] 0000003a SystemOut     O WARN > org.apache.jackrabbit.core.config.ConfigurationErrorHandler[WebContainer : 3]: Warning parsing the configuration at line 4 using system id file:/usr/local/web/fda/WAS/61x/svdw0047v61fda/installedApps/afda21Network001/osa_registry.ear/osa_registry.war/WEB-INF/cfg/repository.xml: org.xml.sax.SAXParseException: Document root element ""Repository"", must match DOCTYPE root ""null"".
[11/23/09 11:32:04:407 EST] 0000003a SystemOut     O WARN > org.apache.jackrabbit.core.config.ConfigurationErrorHandler[WebContainer : 3]: Warning parsing the configuration at line 4 using system id file:/usr/local/web/fda/WAS/61x/svdw0047v61fda/installedApps/afda21Network001/osa_registry.ear/osa_registry.war/WEB-INF/cfg/repository.xml: org.xml.sax.SAXParseException: Document is invalid: no grammar found.
[11/23/09 11:32:04:977 EST] 0000003a SystemOut     O INFO > org.apache.jackrabbit.core.RepositoryImpl[WebContainer : 3]: Starting repository...
[11/23/09 11:32:05:474 EST] 0000003a SystemOut     O ERROR> org.apache.jackrabbit.core.fs.db.DatabaseFileSystem[WebContainer : 3]: failed to initialize file system
java.sql.SQLException: ORA-01031: insufficient privileges

	at oracle.jdbc.driver.DatabaseError.throwSqlException(DatabaseError.java:112)
	at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:331)
	at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:288)
	at oracle.jdbc.driver.T4C8Oall.receive(T4C8Oall.java:745)
	at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:210)
	at oracle.jdbc.driver.T4CStatement.executeForRows(T4CStatement.java:961)
	at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1190)
	at oracle.jdbc.driver.OracleStatement.executeUpdateInternal(OracleStatement.java:1657)
	at oracle.jdbc.driver.OracleStatement.executeUpdate(OracleStatement.java:1626)
	at org.apache.jackrabbit.core.fs.db.OracleFileSystem.checkSchema(OracleFileSystem.java:211)
	at org.apache.jackrabbit.core.fs.db.DatabaseFileSystem.init(DatabaseFileSystem.java:190)
	at org.apache.jackrabbit.core.fs.db.OracleFileSystem.init(OracleFileSystem.java:137)
	at org.apache.jackrabbit.core.config.RepositoryConfigurationParser$2.getFileSystem(RepositoryConfigurationParser.java:762)
	at org.apache.jackrabbit.core.config.RepositoryConfig.getFileSystem(RepositoryConfig.java:666)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:262)
	at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:621)
	at org.apache.jackrabbit.core.jndi.BindableRepository.createRepository(BindableRepository.java:140)
	at org.apache.jackrabbit.core.jndi.BindableRepository.init(BindableRepository.java:116)
	at org.apache.jackrabbit.core.jndi.BindableRepository.<init>(BindableRepository.java:105)
	at org.apache.jackrabbit.core.jndi.BindableRepositoryFactory.getObjectInstance(BindableRepositoryFactory.java:51)
	at org.apache.jackrabbit.core.jndi.RegistryHelper.registerRepository(RegistryHelper.java:74)
	at com.ssc.soareg.jackrabbit.ContentRepository.<clinit>(ContentRepository.java:71)
	at com.ssc.soareg.jaxr.registry.client.infomodel.ServiceImpl.<init>(ServiceImpl.java:177)
	at com.ssc.soareg.governance.client.SOALifeCycleManagerImpl.saveBusinessServices(SOALifeCycleManagerImpl.java:259)
	at com.ssc.soareg.registry.server.UploadServlet.service(UploadServlet.java:473)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:856)
	at com.ibm.ws.webcontainer.servlet.ServletWrapper.service(ServletWrapper.java:1068)
	at com.ibm.ws.webcontainer.servlet.ServletWrapper.handleRequest(ServletWrapper.java:543)
	at com.ibm.ws.wswebcontainer.servlet.ServletWrapper.handleRequest(ServletWrapper.java:478)
	at com.ibm.ws.webcontainer.webapp.WebApp.handleRequest(WebApp.java:3357)
	at com.ibm.ws.webcontainer.webapp.WebGroup.handleRequest(WebGroup.java:267)
	at com.ibm.ws.webcontainer.WebContainer.handleRequest(WebContainer.java:811)
	at com.ibm.ws.wswebcontainer.WebContainer.handleRequest(WebContainer.java:1455)
	at com.ibm.ws.webcontainer.channel.WCChannelLink.ready(WCChannelLink.java:115)
	at com.ibm.ws.http.channel.inbound.impl.HttpInboundLink.handleDiscrimination(HttpInboundLink.java:454)
	at com.ibm.ws.http.channel.inbound.impl.HttpInboundLink.handleNewInformation(HttpInboundLink.java:383)
	at com.ibm.ws.http.channel.inbound.impl.HttpICLReadCallback.complete(HttpICLReadCallback.java:102)
	at com.ibm.ws.tcp.channel.impl.AioReadCompletionListener.futureCompleted(AioReadCompletionListener.java:165)
	at com.ibm.io.async.AbstractAsyncFuture.invokeCallback(AbstractAsyncFuture.java:217)
	at com.ibm.io.async.AsyncChannelFuture.fireCompletionActions(AsyncChannelFuture.java:161)
	at com.ibm.io.async.AsyncFuture.completed(AsyncFuture.java:136)
	at com.ibm.io.async.ResultHandler.complete(ResultHandler.java:195)
	at com.ibm.io.async.ResultHandler.runEventProcessingLoop(ResultHandler.java:784)
	at com.ibm.io.async.ResultHandler$2.run(ResultHandler.java:873)
	at com.ibm.ws.util.ThreadPool$Worker.run(ThreadPool.java:1473)
"
1,"Constants.LUCENE_MAIN_VERSION is inlined in code compiled against Lucene JAR, so version detection is incorrect. When you compile your own code against the Lucene 2.9 version of the JARs and use the LUCENE_MAIN_VERSION constant and then run the code against the 3.0 JAR, the constant still contains 2.9, because javac inlines primitives and Strings into the class files if they are public static final and are generated by a constant (not method).

The attached fix will fix this by using a ident(String) functions that return the String itsself to prevent this inlining.

Will apply to 2.9, trunk and 2.9 BW branch. No I can also reenable one test I removed because of this."
1,"DirectoryTaxonomyReader.refresh misbehaves with ref counts. DirectoryTaxonomyReader uses the internal IndexReader in order to track its own reference counting. However, when you call refresh(), it reopens the internal IndexReader, and from that point, all previous reference counting gets lost (since the new IndexReader's refCount is 1).

The solution is to track reference counting in DTR itself. I wrote a simple unit test which exposes the bug (will be attached with the patch shortly)."
1,"Bug with textfilters and classloaders. I'm having problems with text filter service. I built the contrib/textfilters package and I included the resulting jackrabbit-textfilters-1.0-SNAPSHOT.jar in my application classpath. The problem is that TextFilterService class is unable to find any filters, even though that a services/org...TextFilterService file is wihin the META-INF jar's directory.

I think that this must to be with Eclipse RCP classloader mechanism, but the fact is that it does not work. I find a little bit strange this way to load services, and as you can see, it seems problematic in some scenarios.

----

Marcel Reutegger 	
<marcel.reutegger@gmx.net> to jackrabbit-dev
	 More options	  11:01 am (55 minutes ago)
Hi Martin,

we had a similar problem with the query languages, but I solved that one
by telling the registry to use a specific classloader. this seemed to work.
I'm not sure this will also work for the text filters, because the jar
file might be in another classloader.

could you please post a jira bug? I'll then change the discovery
mechanism to use good old xml config ;)
"
1,"PayloadTermQuery's explain is broken when span score is not included. When setting includeSpanScore to false with PayloadTermQuery, the explain is broken."
1,"CookieSpec.formatCookie(Cookie) produces an incorrect cookie header value. Consider the following:
----------------------------------------------------------------------
Cookie cookie = new Cookie("".foo.com"", ""name"", ""value"");
cookie.setVersion(1);
cookie.setPath(""/"");
CookieSpec spec = CookiePolicy.getSpecByPolicy(CookiePolicy.RFC2109);
System.out.println(spec.formatCookie(cookie));                
----------------------------------------------------------------------

When calling CookieSpec.formatCookie(Cookie) the resulting output is:

   name=""value""

The Version attribute is not present as required by RFC2109, nor is the path or
domain information included.

It seems that in this case, only Cookie type 0 output is produced."
1,"Digest auth uses incorrect URI. This bug seems to be strongly related to #36918. But in this case, I don't have 
proxy and it is GET method.

The problem is that when a GET request with query parameters is sent to the 
server, uri does not count in the parameters.  The server (Apache 1.3) then 
responds with HTTP/1.1 400 Bad Request. In the error log, the server writes:  
Digest: uri mismatch - </query.cgi> does not match request-uri 
</query.cgi?format=advanced&js=1&rememberjs=1>

As far as I can tell, the problem is with the following line of code:
------------ cut DigestScheme.java
public String authenticate(Credentials credentials, HttpMethod method)
....
getParameters().put(""uri"", method.getPath());
....
------------ cut

I am inclined to quick-hack this and add method.getQueryString() to the uri; 
but to make it right it probably needs some refactoring, esp. considering issue 
#36918."
1,"Destination URI should be normalized. WebdavRequestImpl.getHrefLocator tests if the URI passed as parameter starts with the context path, and passes the next segments to the locator factory.
 
There is a potential hole if the parameter contains "".."", because ""http://example.com/dav/../foo"" starts with the context path ""http://example.com/dav"" but represents to ""http://example.com/foo"". Currently, it is up to the locator factory to detect this situation, meaning that every locator factory should implement this check. Additionally, DavLocatorFactory.createResourceLocator cannot throw exceptions, hence it would not fail cleanly (RuntimeException causing a 500 INTERNAL SERVER ERROR response, when a 403 FORBIDDEN status code would have been apropriate)

Note that the Request-URI should have already been normalized by the servlet container, but in COPY/MOVE operations, the Destination-URI is not normalized.

Conformant clients MUST NOT use dot-segments (""."" or "".."")  [RFC 4918, Section 8.3] in Simple-Ref constructions such as the Destination header [RFC 4918, Section 10.3]), but the server should be able to detect this error.

Proposed change in WebdavRequestImpl:193 (in package org.apache.jackrabbit.webdav from webdav/java)
- ref = uri.getRawPath();
+ ref = uri.normalize().getRawPath();

(This causes /dav/../foo to be rejected because it doesn't start with the context path, and accepts dav/foo/../bar because it starts with the context path)"
1,"Data store garbage collection: inUse not correctly synchronized. Access to the fields DbDataStore.inUse and FileDataStore.inUse is not synchronized.
This is a problem when concurrently calling GarbageCollector.deleteUnused() 
and accessing the data store (ConcurrentModificationException is thrown)."
1,"NullPointerException when iterating over properties. Running ConcurrentReadWriteTest (NUM_NODES=5, NUM_THREADS=3, RUN_NUM_SECONDS=120) resulted in a NullPointerException:

Exception in thread ""Thread-11"" java.lang.NullPointerException
	at org.apache.commons.collections.map.AbstractReferenceMap$ReferenceEntry.getValue(AbstractReferenceMap.java:596)
	at org.apache.commons.collections.map.AbstractReferenceMap.containsKey(AbstractReferenceMap.java:204)
	at org.apache.jackrabbit.core.state.ItemStateMap.contains(ItemStateMap.java:66)
	at org.apache.jackrabbit.core.state.ItemStateReferenceCache.isCached(ItemStateReferenceCache.java:91)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.hasItemState(LocalItemStateManager.java:173)
	at org.apache.jackrabbit.core.state.XAItemStateManager.hasItemState(XAItemStateManager.java:252)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:174)
	at org.apache.jackrabbit.core.ItemManager.createItemInstance(ItemManager.java:495)
	at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:326)
	at org.apache.jackrabbit.core.LazyItemIterator.prefetchNext(LazyItemIterator.java:90)
	at org.apache.jackrabbit.core.LazyItemIterator.next(LazyItemIterator.java:203)
	at org.apache.jackrabbit.core.LazyItemIterator.nextProperty(LazyItemIterator.java:118)
	at org.apache.jackrabbit.core.ConcurrentReadWriteTest$1$1.execute(ConcurrentReadWriteTest.java:64)
	at org.apache.jackrabbit.core.AbstractConcurrencyTest$Executor.run(AbstractConcurrencyTest.java:110)
	at java.lang.Thread.run(Thread.java:619)

The cache is not synchronized and is accessed at the same time by the current thread and another thread that notified ItemStates about changes."
1,"Bugs in org.apache.lucene.index.TermVectorsReader.clone(). A couple of things:

- The implementation can return null which is not allowed.  It should throw a CloneNotSupportedException if that's the case.

- Part of the code reads:

    TermVectorsReader clone = null;
    try {
      clone = (TermVectorsReader) super.clone();
    } catch (CloneNotSupportedException e) {}

    clone.tvx = (IndexInput) tvx.clone();

If a CloneNotSupportedException is caught then ""clone"" will be null and the assignment to clone.tvx will fail with a null pointer exception."
1,"NullPointerExc. when indexing empty field with term vectors. Mark Harwood mentioned this on the user's list. Running the attached code 
you'll get this exception: 
 
Exception in thread ""main"" java.lang.NullPointerException 
	at 
org.apache.lucene.index.TermVectorsReader.clone(TermVectorsReader.java:303) 
	at 
org.apache.lucene.index.SegmentReader.getTermVectorsReader(SegmentReader.java:473) 
	at 
org.apache.lucene.index.SegmentReader.getTermFreqVectors(SegmentReader.java:507) 
	at 
org.apache.lucene.index.SegmentMerger.mergeVectors(SegmentMerger.java:204) 
	at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:94) 
	at 
org.apache.lucene.index.IndexWriter.mergeSegments(IndexWriter.java:618) 
	at 
org.apache.lucene.index.IndexWriter.flushRamSegments(IndexWriter.java:571) 
	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:339) 
	at TVBug.main(TVBug.java:16)"
1,"Deadlock due different Thread access while prepare and commit in same Transaction. Since we have configured a j2c resource adapter any modification to the repository ends
with a deadlock."
1,"EnhancementsPayloadIterator.getCategoryData(CategoryEnhancement) problematic usage of Object.equals(). EnhancementsPayloadIterator has an internal list of category enhancemnets, and in getCategoryData(CategoryEnhancement) there is a lookup of the given CategoryEnhancement in the list. In order to make sure this lookup works, CategoryEnhancement must override Object.equals(Object)."
1,"FastVectorHighlighter: AIOOBE occurs if one PhraseQuery is contained by another PhraseQuery. I'm very sorry but this is another one. If q=""a b c d"" OR ""b c"", then ArrayIndexOutOfBoundsException occurs in FieldQuery.checkOverlap(). I'm working on this and fix with test case soon to be posted.
Thank you for your patient!
"
1,Incorrect excerpt for index aggregates. Incorrect excerpts may be created when the relevant node has an index aggregate configured and the nodes have properties configured for the node scope index with some of them excluded for use in excerpts.
1,"Unsynchronized NameFactoryImpl initialization. org.apache.jackrabbit.spi.commons.name.NameFactoryImpl uses an unsafe pattern when initializing:

    private static NameFactory FACTORY;
    private NameFactoryImpl() {};
    public static NameFactory getInstance() {
        if (FACTORY == null) {
            FACTORY = new NameFactoryImpl();
        }
        return FACTORY;
    }

This is bad in a multi-threaded environment (see http://www.ibm.com/developerworks/library/j-dcl.html for details)."
1,FineGrainedISMLocking problems. The FineGrainedISMLocking strategy suffers from the same deadlock issue as was reported in JCR-2753 against DefaultISMLocking. Additionally the FineGrainedISMLocking class will also fail to function properly with XA transactions since it uses the current thread instead of the current transaction id to track re-entrancy.
1,"SSLSocketFactory.createSSLContext does not process trust store. org.apache.http.conn.ssl.SSLSocketFactory.createSSLContext() does not process a provided trust store.
Only the default (cacerts) is processed. An additional provided trust store is ignored.
Adding the ""trusted"" certificate to the keystore, the peer is authenticated.

Eventually
        tmfactory.init(keystore);
needs to be
        tmfactory.init(truststore);

"
1,"Error logged when repository is shut down. This only happens with the bundle DerbyPersistenceManager.

In DerbyPersistenceManager.close() the embedded derby database is shut down and then super.close() is called. There the ConnectionRecoveryManager is closed, which tries to operate on a connection to the already shut down derby database. The log contains entries like:

25.03.2008 13:49:29 *ERROR* [Thread-5] ConnectionRecoveryManager: failed to close connection, reason: No current connection., state/code: 08003/40000 (ConnectionRecoveryManager.java, line 453)"
1,"MOVE method returns error 412 Precondition Failed. Hi, I was trying MacOS X 10.5 Finder's WebDAV client to do testing on Jackrabbit 1.4 which is hosted on Tomcat 5.5.25 on a Windows XP SP2 computer on a LAN. I encounter an error while doing remote editing, I was able to open the text document, but the problem is I couldn't save it.

I tried to find some log on Tomcat but sadly Jackrabbit didn't produces any log files regarding of my problem. So I used Ethereal 0.99.0 to check the packets from the Windows XP computer. The below trace is a summary from the exported text file of the packet analyzer where the problem occur:-

line 11818:-
No.     Time        Source                Destination           Protocol Info
4352 27.629257   10.60.1.90            10.60.1.187           HTTP     MOVE /jackrabbit-webapp-1.4/repository/default/.TemporaryItems/folders.501/TemporaryItems/(A%20Document%20Being%20Saved%20By%20TextEdit)/Copy%20of%20Request_for_GAMS_User_Account.rtf HTTP/1.1

Frame 4352 (592 bytes on wire, 592 bytes captured)
Ethernet II, Src: AppleCom_72:c3:5e (00:0d:93:72:c3:5e), Dst: 00:19:d1:a0:34:f7 (00:19:d1:a0:34:f7)
Internet Protocol, Src: 10.60.1.90 (10.60.1.90), Dst: 10.60.1.187 (10.60.1.187)
Transmission Control Protocol, Src Port: 64970 (64970), Dst Port: 8080 (8080), Seq: 69060, Ack: 90475, Len: 526
    Source port: 64970 (64970)
    Destination port: 8080 (8080)
    Sequence number: 69060    (relative sequence number)
    Next sequence number: 69586    (relative sequence number)
    Acknowledgement number: 90475    (relative ack number)
    Header length: 32 bytes
    Flags: 0x0018 (PSH, ACK)
    Window size: 524280 (scaled)
    Checksum: 0xd4f9 [correct]
    Options: (12 bytes)
Hypertext Transfer Protocol
    MOVE /jackrabbit-webapp-1.4/repository/default/.TemporaryItems/folders.501/TemporaryItems/(A%20Document%20Being%20Saved%20By%20TextEdit)/Copy%20of%20Request_for_GAMS_User_Account.rtf HTTP/1.1\r\n
        Request Method: MOVE
        Request URI: /jackrabbit-webapp-1.4/repository/default/.TemporaryItems/folders.501/TemporaryItems/(A%20Document%20Being%20Saved%20By%20TextEdit)/Copy%20of%20Request_for_GAMS_User_Account.rtf
        Request Version: HTTP/1.1
    User-Agent: WebDAVFS/1.5 (01508000) Darwin/9.1.0 (Power Macintosh)\r\n
    Accept: */*\r\n
    Destination: http://10.60.1.187:8080/jackrabbit-webapp-1.4/repository/default/au/gov/arc/www/rtf/Copy%20of%20Request_for_GAMS_User_Account.rtf\r\n
    Authorization: Basic YWRtaW46YWRtaW4=\r\n
        Credentials: admin:admin
    Content-Length: 0\r\n
    Connection: keep-alive\r\n
    Host: 10.60.1.187:8080\r\n
    \r\n

line 11850 -
No.     Time        Source                Destination           Protocol Info
4353 27.630345   10.60.1.187           10.60.1.90            HTTP     HTTP/1.1 412 Precondition Failed (text/html)

Frame 4353 (1191 bytes on wire, 1191 bytes captured)
Ethernet II, Src: 00:19:d1:a0:34:f7 (00:19:d1:a0:34:f7), Dst: AppleCom_72:c3:5e (00:0d:93:72:c3:5e)
Internet Protocol, Src: 10.60.1.187 (10.60.1.187), Dst: 10.60.1.90 (10.60.1.90)
Transmission Control Protocol, Src Port: 8080 (8080), Dst Port: 64970 (64970), Seq: 90475, Ack: 69586, Len: 1125
    Source port: 8080 (8080)
    Destination port: 64970 (64970)
    Sequence number: 90475    (relative sequence number)
    Next sequence number: 91600    (relative sequence number)
    Acknowledgement number: 69586    (relative ack number)
    Header length: 32 bytes
    Flags: 0x0018 (PSH, ACK)
    Window size: 65535
    Checksum: 0x1c18 [incorrect, should be 0xa2f0]
    Options: (12 bytes)
Hypertext Transfer Protocol
    HTTP/1.1 412 Precondition Failed\r\n
        Request Version: HTTP/1.1
        Response Code: 412
    Server: Apache-Coyote/1.1\r\n
    Content-Type: text/html;charset=utf-8\r\n
    Content-Length: 965\r\n
    Date: Fri, 29 Feb 2008 02:31:01 GMT\r\n
    \r\n
Line-based text data: text/html
    <html><head><title>Apache Tomcat/5.5.25 - Error report</title><style><!--H1 {font-family:Tahoma,Arial,sans-serif;color:white;background-color:#525D76;font-size:22px;} H2 {font-family:Tahoma,Arial,sans-serif;color:white;background-color:#52
"
1,"Deadlock  on concurrent read & transactional write operations. Isuue has been introduced by resolving JCR-1755 (Transaction-safe versioning). This fixed changed sequence of commits, but at the same time order of acquiring locks has been disturbed.


"
1,"on disk full during close, FSIndexOutput fails to close descriptor. The close method just does this:

{code}
      if (isOpen) {
        super.close();
        file.close();
        isOpen = false;
      }
{code}

But super.close = BufferedIndexOutput.close, which tries to flush its buffer.  If disk is full (or something else is wrong) then we hit an exception and don't actually close the descriptor.

I will put a try/finally in so we always close, taking care to preserve the original exception. I'll commit shortly & backport to 2.3.2"
1,"minor/nitpick TermInfoReader bug ?. Some code flagged by a bytecode static analyzer - I guess a nitpick, but we should just drop the null check in the if? If its null it will fall to the below code and then throw a NullPointer exception anyway. Keeping the nullpointer check implies we expect its possible - but then we don't handle it correctly.

{code}
  /** Returns the nth term in the set. */
  final Term get(int position) throws IOException {
    if (size == 0) return null;

    SegmentTermEnum enumerator = getThreadResources().termEnum;
    if (enumerator != null && enumerator.term() != null &&
        position >= enumerator.position &&
	position < (enumerator.position + totalIndexInterval))
      return scanEnum(enumerator, position);      // can avoid seek

    seekEnum(enumerator, position/totalIndexInterval); // must seek
    return scanEnum(enumerator, position);
  }

{code}"
1,"Simple Webdav: dir with same name as repository has incorrect behavior. Using a default repository named ""default"" for example, creating a directory named ""default"" in that repository will have issues.  The directory will not behave correctly. 

Viewing the dir in Jetty http://localhost:8080/jackrabbit/repository/default/default/ will show the contents of the ""default"" directory.  However, clicking on any of the contents will go to an incorrect URL.  E.g. if a directory named ""test"" was created, then the URL for test will be ""http://localhost:8080/jackrabbit/repository/default/test/""  instead of ""http://localhost:8080/jackrabbit/repository/default/default/test/"".

Notice that there is only one ""default"" in the path provided by jackrabbit.  

This causes the contents of such directories to be inaccessible"
1,"Node merge method doesnt seems to recurse thru childs of the right source node. I checked the NodeImpl.merge(...)

it seems the way it process the childs nodes is wrong
as it calls the merge on the childs of the src node that come from the source workspace.
plus in the case srcNode is null it would end on a NullPointerException as

it does  NodeIterator ni = srcNode.getNodes(); in the second statment of the if condition
"
1,"HttpClient does not properly handle 'application/x-www-form-urlencoded' encoding. As always I'd like to pass on my thanks, I'm finding HttpClient really useful.

The problem occurs because I use Struts map based ActionForm and these generate 
request parameters of the form:

<input type=""text"" name=""searchSelection(c)"">

When this is submitted using the PostMethod class the generateRequestBody() is 
called and in turn this calls the URI.encode() method with a BitSet of the 
acceptable characters. In this case the '(' and ')' characters are marked as 
acceptable.

The problem is that this does not work correctly when I submit it to my remote 
server. If however I issue the request directly (from a webpage rather than 
using HttpClient) it works and when I examine the request input stream I can see 
that the parameter has been re-written so that 'select(c)' is displayed as 
'select%28c%29'.

This may be my error because of encoding problems or the fact I am not setting 
the content type etc. correctly. Or it could be a bug. I'm afraid my HTTP 
knowledge is not good enough.

Chris Mein"
1,"Highligter fails to include non-token at end of string to be highlighted. The following code extract show the problem


		TermQuery query= new TermQuery( new Term( ""data"", ""help"" )); 
		Highlighter hg = new Highlighter(new SimpleHTMLFormatter(), new QueryScorer( query ));
		hg.setTextFragmenter( new NullFragmenter() );
		
		String match = null;
		try {
			match = hg.getBestFragment( new StandardAnalyzer(), ""data"", ""help me [54-65]"" );
		} catch (IOException e) {
			e.printStackTrace();
		}
		System.out.println( match );


The sytsem outputs 

<B>help</B> me [54-65


would expect 

<B>help</B> me [54-65]



"
1,bad route computed for redirected requests. BasicRouteDirector appears to miscalculate complex routes. Example to follow. 
1,"Error when registering node types on virgin repository. When a node type is registered on a repository that has never been started before an error is written to the log:

26.10.2006 10:36:02 *ERROR* [main] VirtualNodeTypeStateManager: Unable to index new nodetype: javax.jcr.ItemNotFoundException (VirtualNodeTypeStateManager.java, line 159)

Steps to reproduce:

> maven test:clean
> cp applications/test/repository/namespaces/ns_re.properties.install applications/test/repository/namespaces/ns_re.properties
> cp applications/test/repository/nodetypes/custom_nodetypes.xml.install applications/test/repository/nodetypes/custom_nodetypes.xml

Run test case o.a.j.init.NodeTypeData

It seems that the workspace initialization creates node states for jcr:system and a child node jcr:nodeTypes. The latter however is overlayed by a virtual node state provided by the VirtualNodeTypeStateProvider.

In case of an initial startup, the jcr:nodeTypes node is cached on creation and is not overlayed by the virtual twin from the VirtualNodeTypeStateProvider.

IMO the jcr:nodeTypes node state shouldn't have been created in the first place because it is overlayed anyway. The workspace initialization routine should only create a child node entry in the jcr:system node state but no actual child node state for that entry.

Opinions?"
1,"Can no longer set a Date property using a Long value. Attempting to set a Date property with a Long value throws a javax.jcr.nodetype.ConstraintViolationException. This worked in Jackrabbit 1.6.2.

To reproduce:
  Node node = session.getItem(""/"");
  node = node.addNode(""dummy"", ""nt:resource"");
  ValueFactory vf = session.getValueFactory();
  Value = vf.createValue(""1234"", 3); // Create a LongValue
  node.setProperty(""jcr:lastModified"", value);
  System.out.println(node.getProperty(""jcr:lastModified""));

Expected result:
- A date around 1970 is printed to System.out

Actual result:
  javax.jcr.nodetype.ConstraintViolationException: no matching property definition found for {http://www.jcp.org/jcr/1.0}lastModified
       at org.apache.jackrabbit.core.nodetype.EffectiveNodeType.getApplicablePropertyDef(EffectiveNodeType.java:770)
       at org.apache.jackrabbit.core.NodeImpl.getApplicablePropertyDefinition(NodeImpl.java:911)
       at org.apache.jackrabbit.core.ItemManager.getDefinition(ItemManager.java:224)
       at org.apache.jackrabbit.core.ItemData.getDefinition(ItemData.java:97)
       at org.apache.jackrabbit.core.PropertyData.getPropertyDefinition(PropertyData.java:53)
       at org.apache.jackrabbit.core.PropertyImpl.getDefinition(PropertyImpl.java:729)
       at org.apache.jackrabbit.core.NodeImpl.setProperty(NodeImpl.java:2512)

According to Jukka Zitting [1], this might be a side-effect of JCR-2170.

[1] Mail thread from dev@jackrabbit.apache.org: http://markmail.org/message/hn3snufsogjvldad"
1,"Inconsistent state when removing mix:lockable from a locked Node. when the lock holder removes mix:lockable from a locked node, the lock related properties get removed.
however, the lock still is live and present on the node.

i would have expected that either

- removing mix:lockable was not allowed or
- the lock was automatically released

test code:

    public void testRemoveMixLockableFromLockedNode() throws RepositoryException {

        Node n = testRootNode.addNode(nodeName1);
        n.addMixin(mixLockable);
        testRootNode.save();

        Lock l = n.lock(true, true);
        n.removeMixin(mixLockable);
        n.save();

        assertFalse(n.isNodeType(mixLockable));                                <===== ok
        assertFalse(l.isLive());                                                                    <===== lock is still live
        assertFalse(n.isLocked());                                                            <=====  node is still locked
        List tokens = Arrays.asList(superuser.getLockTokens());
        assertFalse(tokens.contains(l.getLockToken()));                    <=====  session contains the token

        assertFalse(n.hasProperty(jcrLockOwner));                             <=====  ok. prop got removed.
        assertFalse(n.hasProperty(jcrlockIsDeep));                             <=====  ok. prop got removed.
        n.unlock();                                                                                         <===== LockException (node not lockable)
    }"
1,"trunk TestRollingUpdates.testRollingUpdates seed failure. trunk r1152892
reproducable: always

{code}
junit-sequential:
    [junit] Testsuite: org.apache.lucene.index.TestRollingUpdates
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 1.168 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestRollingUpdates -Dtestmethod=testRollingUpdates -Dtests.seed=-5322802004404580273:-4001225075726350391
    [junit] WARNING: test method: 'testRollingUpdates' left thread running: merge thread: _c(4.0):cv3/2 _h(4.0):cv3 into _k
    [junit] RESOURCE LEAK: test method: 'testRollingUpdates' left 1 thread(s) running
    [junit] NOTE: test params are: codec=RandomCodecProvider: {docid=Standard, body=SimpleText, title=MockSep, titleTokenized=Pulsing(freqCutoff=20), date=MockFixedIntBlock(blockSize=1474)}, locale=lv_LV, timezone=Pacific/Fiji
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestRollingUpdates]
    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_26 (64-bit)/cpus=8,threads=1,free=128782656,total=158400512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testRollingUpdates(org.apache.lucene.index.TestRollingUpdates):   FAILED
    [junit] expected:<20> but was:<21>
    [junit] junit.framework.AssertionFailedError: expected:<20> but was:<21>
    [junit]     at org.apache.lucene.index.TestRollingUpdates.testRollingUpdates(TestRollingUpdates.java:76)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1522)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1427)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.index.TestRollingUpdates FAILED

{code}"
1,"Issues with compiled permissions of ACL provider. - should not use search for infrastructure checks
- event listener never discarded."
1,"IndexReader.setTermInfosIndexDivisor doesn't carry over to reopened readers. When you reopen a reader, some segments are shared (and thus properly inherit the index divisor) but others are newly opened and use the default index divisor.  You then have no way to change the index divisor of those newly opened ones.  The only workaround is to not use reopen (always open a new reader).

I'd like to make termInfosDivisor an up-front param to IndexReader, anyway, for LUCENE-1609, so likely I'll fix both of these issues there."
1,"Minor RFC 2109 / 2965 violation. Hi all,

we received this bug report for the debian commons-httpclient
package:

<debian_bugreport>
The following bug is present in upstream, 2.0.2 and 3.0RC3, at least as far
as I can tell by testing.

The specification grammar for the Cookie and Cookie2 HTTP headers
(specified by RFC 2109 section 4.3.4, and RFC 2965 section 3.3.4,
respectively) require that the ordering of pairs is ""Version, NAME, path,
domain"" (and, in RFC 2965, ""port"" after ""domain""). However, HTTPClient
produces a cookie string with the domain pair appearing before, rather
than after, the path pair. The RFCs specifically *do not* use either the
grammar or the clarifying text (""can occur in any order"") that occurs in
the sections that define the Set-Cookie and Set-Cookie2 headers (4.2.2 and
3.2.2, respectively).

Since the sections in question do not, in fact, discuss the issue of pair
ordering in Set-Cookie/Set-Cookie2 at all (other than in using a grammar
that clearly expresses the requirement), and since the complimentary
header explicitly permits them to occur in any order, it seems likely
that HTTPClient is not the only client with this issue, and that most
servers will accomodate this situation (in fact, for it to have gone
unnoticed for this long, it seems likely that either I'm badly misreading
the specification, or no major server has a problem coping with this).
</debian_bugreport>

For your reference the debian bug number:
http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=329245

Regards,

Wolfgang"
1,"Changes from Session.move() to a top-level node aren't seen in a second session. I'll attach a test case, but basically...

* Create two sessions
* Create a top-level node in the first session and save it.
* Move the top-level node using the first session
* In the second session, try itemExists() for the path of the node. It returns true when it should be false."
1,"QueryParser escaping/parsin issue with strings starting/ending with ||. There is a problem with query parser when search string starts/ends with ||.  When string contains || in the middle like 'something || something' everything runs without a problem.

Part of code: 
  searchText = QueryParser.escape(searchText);
  QueryParser parser = null;
  parser = new QueryParser(fieldName, new CustomAnalyser());
  parser.parse(searchText);

CustomAnalyser class extends Analyser. Here is the only redefined method: 

    @Override
    public TokenStream tokenStream(String fieldName, Reader reader) {
      return new PorterStemFilter( (new StopAnalyzer()).tokenStream(fieldName, reader));
    }

I have tested this on Lucene 2.1 and latest source I have checked-out from SVN (Revision 538867) and in both cases parsing exception was thrown.

Part of Stack Trace (Lucene - SVN checkout - Revision 538867):
Cannot parse 'someting ||': Encountered ""<EOF>"" at line 1, column 11.
Was expecting one of:
    <NOT> ...
    ""+"" ...
    ""-"" ...
    ""("" ...
    ""*"" ...
    <QUOTED> ...
    <TERM> ...
    <PREFIXTERM> ...
    <WILDTERM> ...
    ""["" ...
    ""{"" ...
    <NUMBER> ...
    
 org.apache.lucene.queryParser.ParseException: Cannot parse 'someting ||': Encountered ""<EOF>"" at line 1, column 11.
Was expecting one of:
    <NOT> ...
    ""+"" ...
    ""-"" ...
    ""("" ...
    ""*"" ...
    <QUOTED> ...
    <TERM> ...
    <PREFIXTERM> ...
    <WILDTERM> ...
    ""["" ...
    ""{"" ...
    <NUMBER> ...
    
        at org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:150)


Part of Stack Trace (Lucene 2.1):
Cannot parse 'something ||': Encountered ""<EOF>"" at line 1, column 12.
Was expecting one of:
    <NOT> ...
    ""+"" ...
    ""-"" ...
    ""("" ...
    ""*"" ...
    <QUOTED> ...
    <TERM> ...
    <PREFIXTERM> ...
    <WILDTERM> ...
    ""["" ...
    ""{"" ...
    <NUMBER> ...
    
 org.apache.lucene.queryParser.ParseException: Cannot parse 'something ||': Encountered ""<EOF>"" at line 1, column 12.
Was expecting one of:
    <NOT> ...
    ""+"" ...
    ""-"" ...
    ""("" ...
    ""*"" ...
    <QUOTED> ...
    <TERM> ...
    <PREFIXTERM> ...
    <WILDTERM> ...
    ""["" ...
    ""{"" ...
    <NUMBER> ...
    
        at org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:149)


"
1,"QueryImpl result offSet must be considered after security class grant the item.. ackrabbit version is 1.4 (jackrabbit-core - 1.4.5).
I use searches with result limit and offset but it is working some wrong for my case.
Lets suppose the total of nodes that will return with the search:

NAME      - GRANTACCESS -   OFFSET
node1     -     true                    - 0
node2     -     false                  -  1
node3     -     true                    - 2
node4     -     true                    - 3
node5     -     false                  -  4

My page must have 2 records, so first I do a count for the search and get size of 3 records (after filtered by my security class invoked automatically by jackrabbit), so I have 2 pages to show to the user. The first page must return 2 records, of course, and the second must return 1 record.

In the first search I do set:
QueryImp.setOffset(0);
QueryImpl.setLimit(2);

So, I get the nodes 1 and 3, thats correct.

In the second same search (for second page), I do set:
QueryImp.setOffset(2);
QueryImpl.setLimit(2);

This way I pretend to get two records, starting from the record nro 3, which would be only the node4.
But, the result I got is node3 (again) and node4, because the offset worked not according to the grantacess (provided by the security class), but according to the sequence of the raw result.

This offset have to start in the correct position, counting only the granted nodes returned by the security class.
Hope this make sense for you.

Thanks.
Helio."
1,"ItemStates in the ChangeLog can not be retrieved in the sequence they were created/modified/deleted. The itemstates are ordered by the hash code.
It's an issue with PersistenceManagers that check referencial integrity (e.g. rdbms)."
1,"Invalid cookie causing IllegalArgumentException. The bug reported by Oliver KÃ¶ll <listen at quimby.de> on HttpClient mailing list

<quote>
I'm dealing with a site that serves invalid Cookies in various kind of  
ways. In some cases the Cookie values contain "","" characters, which  
really confuses the Header/Cookie parsers and eventually leads to  
IllegalArgumentExceptions thrown by the Cookie constructor:

java.lang.IllegalArgumentException: Cookie name may not be blank
   at org.apache.commons.httpclient.Cookie.<init>(Cookie.java:142)
   at  
org.apache.commons.httpclient.cookie.CookieSpecBase.parse(CookieSpecBase 
.java:192)
   at  
org.apache.commons.httpclient.cookie.CookieSpecBase.parse(CookieSpecBase 
.java:256)
   at  
org.apache.commons.httpclient.HttpMethodBase.processResponseHeaders(Http 
MethodBase.java:1826)
   at  
org.apache.commons.httpclient.HttpMethodBase.readResponse(HttpMethodBase 
.java:1939)
   at  
org.apache.commons.httpclient.HttpMethodBase.processRequest(HttpMethodBa 
se.java:2631)
   at  
org.apache.commons.httpclient.HttpMethodBase.execute(HttpMethodBase.java 
:1085)
   at  
org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:6 
74)
   at  
org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:5 
29)
   at my.code.Test.getHttp(Test.java:114)

What bothers me, is that these IllegalArgumentExceptions are never  
caught in the HttpClient code, making it effectivily impossible to  
handle these responses.

</quote>"
1,"Node.restore() may throw InvalidItemStateException. It seems that ItemManager cache is not maintained correctly. I'm getting InvalidItemStateException: 'propertyId' has been modified externally tryin restore/checkout versionable nodes in single thread.

ItemState should be evicted from ItemStateManager cache when modified, it seems that status of ItemState is changed to MODIFIED, but itemState remains in the cache."
1,"IndexDeletionPolicy.delete behaves incorrectly when deleting latest generation . I have been looking to provide the ability to rollback committed transactions and encountered some issues.
I appreciate IndexDeletionPolicy's main motivation is to handle cleaning away OLD commit points but it does not explicitly state that it can or cannot be used to clean NEW commit points.

If this is not supported then the documentation should ideally state this. If the intention is to support this behaviour then read on .......

There seem to be 2 issues so far:
1) The first attempt to call IndexCommit.delete on the latest commit point fails to remove any contents. The subsequent call succeeds however
2) Deleting the latest commit point fails to update the segments.gen file to point to segments_N-1. New IndexReaders that are opened are then misdirected to open segments_N which has been deleted

Junit test to follow...

"
1,"DateValue.equals() relies on Calendar.equals(). JSR170 states regarding Date values:
""The text format of dates must follow the following ISO 8601:2000-compliant format"".

While DateValue.valueOf(String) and DateValue.getString() both rely on the functionality provided by the org.apache.jackrabbit.util.ISO8601, DateValue.equals() compares the equality of the internal Calendar object (DateValue line 89). This may return false even if the Iso-format of both values are equal.

In other words: Creating a new DateValue using the ValueFactory from the String representation of an existing DateValue will return an object, that is not equal to the original DateValue. The reason for this is, that the String does not contain all infomation, that is used during Calendar.equals.

regards
angela

"
1,"NRT reader/writer over RAMDirectory memory leak. with NRT reader/writer, emptying an index using:
writer.deleteAll()
writer.commit()
doesn't release all allocated memory.

for example the following code will generate a memory leak:

/**
	 * Reveals a memory leak in NRT reader/writer<br>
	 * 
	 * The following main() does 10K cycles of:
	 * <ul>
	 * <li>Add 10K empty documents to index writer</li>
	 * <li>commit()</li>
	 * <li>open NRT reader over the writer, and immediately close it</li>
	 * <li>delete all documents from the writer</li>
	 * <li>commit changes to the writer</li>
	 * </ul>
	 * 
	 * Running with -Xmx256M results in an OOME after ~2600 cycles
	 */
	public static void main(String[] args) throws Exception {
		RAMDirectory d = new RAMDirectory();
		IndexWriter w = new IndexWriter(d, new IndexWriterConfig(Version.LUCENE_33, new KeywordAnalyzer()));
		Document doc = new Document();
		
		for(int i = 0; i < 10000; i++) {
			for(int j = 0; j < 10000; ++j) {
				w.addDocument(doc);
			}
			w.commit();
			IndexReader.open(w, true).close();

			w.deleteAll();
			w.commit();
		}
		
		w.close();
		d.close();
	}	"
1,"Possible NPE in HttpHost. HttpHost line 167 says:
        if (this.port != this.protocol.getDefaultPort()) {

However, a few lines above, protocol is checked for null.

Line 167 should probably read:

        if (this.protocol != null && this.port != this.protocol.getDefaultPort()) {
"
1,"ShingleMatrixFilter eaily throws StackOverFlow as the complexity of a matrix grows. ShingleMatrixFilter#next makes a recursive function invocation when the current permutation iterator is exhausted or if the current state of the permutation iterator already has produced an identical shingle. In a not too complex matrix this will require a gigabyte sized stack per thread.

My solution is to avoid the recursive invocation by refactoring like this:

{code:java}
public Token next(final Token reusableToken) throws IOException {
    assert reusableToken != null;
    if (matrix == null) {
      matrix = new Matrix();
      // fill matrix with maximumShingleSize columns
      while (matrix.columns.size() < maximumShingleSize && readColumn()) {
        // this loop looks ugly
      }
    }

    // this loop exists in order to avoid recursive calls to the next method
    // as the complexity of a large matrix
    // then would require a multi gigabyte sized stack.
    Token token;
    do {
      token = produceNextToken(reusableToken);
    } while (token == request_next_token);
    return token;
  }

  
  private static final Token request_next_token = new Token();

  /**
   * This method exists in order to avoid reursive calls to the method
   * as the complexity of a fairlt small matrix then easily would require
   * a gigabyte sized stack per thread.
   *
   * @param reusableToken
   * @return null if exhausted, instance request_next_token if one more call is required for an answer, or instance parameter resuableToken.
   * @throws IOException
   */
  private Token produceNextToken(final Token reusableToken) throws IOException {

{code}

"
1,"XMLparser drops user boosting. The lucene XML parser seems to convert user defined boosting back to default 1.0 and thus boosting value is dropped from the query...

e.g.

{code:xml}
<BooleanQuery>
	<Clause occurs=""must"">
		<BooleanQuery>
			<Clause occurs=""should"">
				<UserQuery fieldName=""Vehicle.Colour"">red^66 blue~^8</UserQuery>
			</Clause>
		</BooleanQuery>
	</Clause>
	<Clause occurs=""should"">
		<BooleanQuery>
			<Clause occurs=""should"">
				<UserQuery fieldName=""Vehicle.Colour"">black^0.01</UserQuery>
			</Clause>
		</BooleanQuery>
	</Clause>
</BooleanQuery>
{code}

produces a lucene query: +( ( Vehicle.Colour:red^66 Vehicle.Colour:blue~0.5^8 ) ) ( Vehicle.Colour:black )

The expected query : +( ( Vehicle.Colour:red^66 Vehicle.Colour:blue~0.5^8 ) ) ( Vehicle.Colour:black^0.01 )

I have developed a work around by modifying line 77 of UserInputQueryBuilder.java 

from:

{code:java}
q.setBoost(DOMUtils.getAttribute(e,""boost"",1.0f));
{code}

to:

{code:java}
q.setBoost( DOMUtils.getAttribute( e, ""boost"", q.getBoost() ) );
{code}

"
1,"SampleComparable doesn't work well in contrib/remote tests. As discovered in LUCENE-1749, when using identical instances of a SortComparator you get multiple entries in the FieldCache.

demonstrating this bug currently requires the patches in LUCENE-1749.

See markmiller's comment here...
https://issues.apache.org/jira/browse/LUCENE-1749?focusedCommentId=12735190#action_12735190"
1,LuceneQueryFactory should call QueryHits.close() after running a query. LuceneQueryFactory which is responsible for the JCR_SQL2 implementation does not close QueryHits after running a query.
1,"ClassCastException org.apache.xerces.parsers.XIncludeAwareParserConfiguration cannot be cast to org.apache.xerces.xni.parser.XMLParserConfiguration when deploying in JBoss 5.1. I tried to follow the steps given on http://wiki.apache.org/jackrabbit/JackrabbitOnJBoss
To get over an exception I had to use jcr-2.0.jar (instead of jcr-1.0.jar)
The following exception happens when the jboss server is started.
=======================================================================

2009-10-02 11:49:05,630 ERROR [org.jboss.web.tomcat.service.deployers.JBossContextConfig] (main) XML error parsing: context.xml
org.jboss.xb.binding.JBossXBRuntimeException: Failed to create a new SAX parser
	at org.jboss.xb.binding.UnmarshallerFactory$UnmarshallerFactoryImpl.newUnmarshaller(UnmarshallerFactory.java:100)
	at org.jboss.web.tomcat.service.deployers.JBossContextConfig.processContextConfig(JBossContextConfig.java:549)
	at org.jboss.web.tomcat.service.deployers.JBossContextConfig.init(JBossContextConfig.java:536)
	at org.apache.catalina.startup.ContextConfig.lifecycleEvent(ContextConfig.java:279)
	at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:117)
	at org.apache.catalina.core.StandardContext.init(StandardContext.java:5436)
	at org.apache.catalina.core.StandardContext.start(StandardContext.java:4148)
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeployInternal(TomcatDeployment.java:310)
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeploy(TomcatDeployment.java:142)
	at org.jboss.web.deployers.AbstractWarDeployment.start(AbstractWarDeployment.java:461)
	at org.jboss.web.deployers.WebModule.startModule(WebModule.java:118)
	at org.jboss.web.deployers.WebModule.start(WebModule.java:97)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java:157)
	at org.jboss.mx.server.Invocation.dispatch(Invocation.java:96)
	at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)
	at org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:264)
	at org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:668)
	at org.jboss.system.microcontainer.ServiceProxy.invoke(ServiceProxy.java:206)
	at $Proxy38.start(Unknown Source)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:42)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:37)
	at org.jboss.dependency.plugins.action.SimpleControllerContextAction.simpleInstallAction(SimpleControllerContextAction.java:62)
	at org.jboss.dependency.plugins.action.AccessControllerContextAction.install(AccessControllerContextAction.java:71)
	at org.jboss.dependency.plugins.AbstractControllerContextActions.install(AbstractControllerContextActions.java:51)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.system.microcontainer.ServiceControllerContext.install(ServiceControllerContext.java:286)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.ServiceController.doChange(ServiceController.java:688)
	at org.jboss.system.ServiceController.start(ServiceController.java:460)
	at org.jboss.system.deployers.ServiceDeployer.start(ServiceDeployer.java:163)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:99)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:46)
	at org.jboss.deployers.spi.deployer.helpers.AbstractSimpleRealDeployer.internalDeploy(AbstractSimpleRealDeployer.java:62)
	at org.jboss.deployers.spi.deployer.helpers.AbstractRealDeployer.deploy(AbstractRealDeployer.java:50)
	at org.jboss.deployers.plugins.deployers.DeployerWrapper.deploy(DeployerWrapper.java:171)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doDeploy(DeployersImpl.java:1439)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1157)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1178)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.install(DeployersImpl.java:1098)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.process(DeployersImpl.java:781)
	at org.jboss.deployers.plugins.main.MainDeployerImpl.process(MainDeployerImpl.java:702)
	at org.jboss.system.server.profileservice.repository.MainDeployerAdapter.process(MainDeployerAdapter.java:117)
	at org.jboss.system.server.profileservice.repository.ProfileDeployAction.install(ProfileDeployAction.java:70)
	at org.jboss.system.server.profileservice.repository.AbstractProfileAction.install(AbstractProfileAction.java:53)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.install(AbstractProfileService.java:361)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.activateProfile(AbstractProfileService.java:306)
	at org.jboss.system.server.profileservice.ProfileServiceBootstrap.start(ProfileServiceBootstrap.java:271)
	at org.jboss.bootstrap.AbstractServerImpl.start(AbstractServerImpl.java:461)
	at org.jboss.Main.boot(Main.java:221)
	at org.jboss.Main$1.run(Main.java:556)
	at java.lang.Thread.run(Thread.java:619)
Caused by: org.jboss.xb.binding.JBossXBException: Failed to create a new SAX parser
	at org.jboss.xb.binding.parser.sax.SaxJBossXBParser.<init>(SaxJBossXBParser.java:97)
	at org.jboss.xb.binding.UnmarshallerImpl.<init>(UnmarshallerImpl.java:56)
	at org.jboss.xb.binding.UnmarshallerFactory$UnmarshallerFactoryImpl.newUnmarshaller(UnmarshallerFactory.java:96)
	... 73 more
Caused by: java.lang.ClassCastException: org.apache.xerces.parsers.XIncludeAwareParserConfiguration cannot be cast to org.apache.xerces.xni.parser.XMLParserConfiguration
	at org.apache.xerces.parsers.SAXParser.<init>(Unknown Source)
	at org.apache.xerces.parsers.SAXParser.<init>(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser.<init>(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserImpl.<init>(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserFactoryImpl.newSAXParser(Unknown Source)
	at org.jboss.xb.binding.parser.sax.SaxJBossXBParser.<init>(SaxJBossXBParser.java:92)
	... 75 more
2009-10-02 11:49:05,645 ERROR [org.jboss.web.tomcat.service.deployers.JBossContextConfig] (main) XML error parsing: jboss.web/localhost/context.xml.default
org.jboss.xb.binding.JBossXBRuntimeException: Failed to create a new SAX parser
	at org.jboss.xb.binding.UnmarshallerFactory$UnmarshallerFactoryImpl.newUnmarshaller(UnmarshallerFactory.java:100)
	at org.jboss.web.tomcat.service.deployers.JBossContextConfig.processContextConfig(JBossContextConfig.java:549)
	at org.jboss.web.tomcat.service.deployers.JBossContextConfig.init(JBossContextConfig.java:537)
	at org.apache.catalina.startup.ContextConfig.lifecycleEvent(ContextConfig.java:279)
	at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:117)
	at org.apache.catalina.core.StandardContext.init(StandardContext.java:5436)
	at org.apache.catalina.core.StandardContext.start(StandardContext.java:4148)
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeployInternal(TomcatDeployment.java:310)
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeploy(TomcatDeployment.java:142)
	at org.jboss.web.deployers.AbstractWarDeployment.start(AbstractWarDeployment.java:461)
	at org.jboss.web.deployers.WebModule.startModule(WebModule.java:118)
	at org.jboss.web.deployers.WebModule.start(WebModule.java:97)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java:157)
	at org.jboss.mx.server.Invocation.dispatch(Invocation.java:96)
	at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)
	at org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:264)
	at org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:668)
	at org.jboss.system.microcontainer.ServiceProxy.invoke(ServiceProxy.java:206)
	at $Proxy38.start(Unknown Source)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:42)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:37)
	at org.jboss.dependency.plugins.action.SimpleControllerContextAction.simpleInstallAction(SimpleControllerContextAction.java:62)
	at org.jboss.dependency.plugins.action.AccessControllerContextAction.install(AccessControllerContextAction.java:71)
	at org.jboss.dependency.plugins.AbstractControllerContextActions.install(AbstractControllerContextActions.java:51)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.system.microcontainer.ServiceControllerContext.install(ServiceControllerContext.java:286)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.ServiceController.doChange(ServiceController.java:688)
	at org.jboss.system.ServiceController.start(ServiceController.java:460)
	at org.jboss.system.deployers.ServiceDeployer.start(ServiceDeployer.java:163)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:99)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:46)
	at org.jboss.deployers.spi.deployer.helpers.AbstractSimpleRealDeployer.internalDeploy(AbstractSimpleRealDeployer.java:62)
	at org.jboss.deployers.spi.deployer.helpers.AbstractRealDeployer.deploy(AbstractRealDeployer.java:50)
	at org.jboss.deployers.plugins.deployers.DeployerWrapper.deploy(DeployerWrapper.java:171)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doDeploy(DeployersImpl.java:1439)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1157)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1178)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.install(DeployersImpl.java:1098)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.process(DeployersImpl.java:781)
	at org.jboss.deployers.plugins.main.MainDeployerImpl.process(MainDeployerImpl.java:702)
	at org.jboss.system.server.profileservice.repository.MainDeployerAdapter.process(MainDeployerAdapter.java:117)
	at org.jboss.system.server.profileservice.repository.ProfileDeployAction.install(ProfileDeployAction.java:70)
	at org.jboss.system.server.profileservice.repository.AbstractProfileAction.install(AbstractProfileAction.java:53)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.install(AbstractProfileService.java:361)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.activateProfile(AbstractProfileService.java:306)
	at org.jboss.system.server.profileservice.ProfileServiceBootstrap.start(ProfileServiceBootstrap.java:271)
	at org.jboss.bootstrap.AbstractServerImpl.start(AbstractServerImpl.java:461)
	at org.jboss.Main.boot(Main.java:221)
	at org.jboss.Main$1.run(Main.java:556)
	at java.lang.Thread.run(Thread.java:619)
Caused by: org.jboss.xb.binding.JBossXBException: Failed to create a new SAX parser
	at org.jboss.xb.binding.parser.sax.SaxJBossXBParser.<init>(SaxJBossXBParser.java:97)
	at org.jboss.xb.binding.UnmarshallerImpl.<init>(UnmarshallerImpl.java:56)
	at org.jboss.xb.binding.UnmarshallerFactory$UnmarshallerFactoryImpl.newUnmarshaller(UnmarshallerFactory.java:96)
	... 73 more
Caused by: java.lang.ClassCastException: org.apache.xerces.parsers.XIncludeAwareParserConfiguration cannot be cast to org.apache.xerces.xni.parser.XMLParserConfiguration
	at org.apache.xerces.parsers.SAXParser.<init>(Unknown Source)
	at org.apache.xerces.parsers.SAXParser.<init>(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser.<init>(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserImpl.<init>(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserFactoryImpl.newSAXParser(Unknown Source)
	at org.jboss.xb.binding.parser.sax.SaxJBossXBParser.<init>(SaxJBossXBParser.java:92)
	... 75 more
2009-10-02 11:49:05,645 ERROR [org.jboss.web.tomcat.service.deployers.JBossContextConfig] (main) XML error parsing: WEB-INF/context.xml
org.jboss.xb.binding.JBossXBRuntimeException: Failed to create a new SAX parser
	at org.jboss.xb.binding.UnmarshallerFactory$UnmarshallerFactoryImpl.newUnmarshaller(UnmarshallerFactory.java:100)
	at org.jboss.web.tomcat.service.deployers.JBossContextConfig.processContextConfig(JBossContextConfig.java:549)
	at org.jboss.web.tomcat.service.deployers.JBossContextConfig.init(JBossContextConfig.java:540)
	at org.apache.catalina.startup.ContextConfig.lifecycleEvent(ContextConfig.java:279)
	at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:117)
	at org.apache.catalina.core.StandardContext.init(StandardContext.java:5436)
	at org.apache.catalina.core.StandardContext.start(StandardContext.java:4148)
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeployInternal(TomcatDeployment.java:310)
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeploy(TomcatDeployment.java:142)
	at org.jboss.web.deployers.AbstractWarDeployment.start(AbstractWarDeployment.java:461)
	at org.jboss.web.deployers.WebModule.startModule(WebModule.java:118)
	at org.jboss.web.deployers.WebModule.start(WebModule.java:97)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java:157)
	at org.jboss.mx.server.Invocation.dispatch(Invocation.java:96)
	at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)
	at org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:264)
	at org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:668)
	at org.jboss.system.microcontainer.ServiceProxy.invoke(ServiceProxy.java:206)
	at $Proxy38.start(Unknown Source)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:42)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:37)
	at org.jboss.dependency.plugins.action.SimpleControllerContextAction.simpleInstallAction(SimpleControllerContextAction.java:62)
	at org.jboss.dependency.plugins.action.AccessControllerContextAction.install(AccessControllerContextAction.java:71)
	at org.jboss.dependency.plugins.AbstractControllerContextActions.install(AbstractControllerContextActions.java:51)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.system.microcontainer.ServiceControllerContext.install(ServiceControllerContext.java:286)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.ServiceController.doChange(ServiceController.java:688)
	at org.jboss.system.ServiceController.start(ServiceController.java:460)
	at org.jboss.system.deployers.ServiceDeployer.start(ServiceDeployer.java:163)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:99)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:46)
	at org.jboss.deployers.spi.deployer.helpers.AbstractSimpleRealDeployer.internalDeploy(AbstractSimpleRealDeployer.java:62)
	at org.jboss.deployers.spi.deployer.helpers.AbstractRealDeployer.deploy(AbstractRealDeployer.java:50)
	at org.jboss.deployers.plugins.deployers.DeployerWrapper.deploy(DeployerWrapper.java:171)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doDeploy(DeployersImpl.java:1439)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1157)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1178)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.install(DeployersImpl.java:1098)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.process(DeployersImpl.java:781)
	at org.jboss.deployers.plugins.main.MainDeployerImpl.process(MainDeployerImpl.java:702)
	at org.jboss.system.server.profileservice.repository.MainDeployerAdapter.process(MainDeployerAdapter.java:117)
	at org.jboss.system.server.profileservice.repository.ProfileDeployAction.install(ProfileDeployAction.java:70)
	at org.jboss.system.server.profileservice.repository.AbstractProfileAction.install(AbstractProfileAction.java:53)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.install(AbstractProfileService.java:361)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.activateProfile(AbstractProfileService.java:306)
	at org.jboss.system.server.profileservice.ProfileServiceBootstrap.start(ProfileServiceBootstrap.java:271)
	at org.jboss.bootstrap.AbstractServerImpl.start(AbstractServerImpl.java:461)
	at org.jboss.Main.boot(Main.java:221)
	at org.jboss.Main$1.run(Main.java:556)
	at java.lang.Thread.run(Thread.java:619)
Caused by: org.jboss.xb.binding.JBossXBException: Failed to create a new SAX parser
	at org.jboss.xb.binding.parser.sax.SaxJBossXBParser.<init>(SaxJBossXBParser.java:97)
	at org.jboss.xb.binding.UnmarshallerImpl.<init>(UnmarshallerImpl.java:56)
	at org.jboss.xb.binding.UnmarshallerFactory$UnmarshallerFactoryImpl.newUnmarshaller(UnmarshallerFactory.java:96)
	... 73 more
Caused by: java.lang.ClassCastException: org.apache.xerces.parsers.XIncludeAwareParserConfiguration cannot be cast to org.apache.xerces.xni.parser.XMLParserConfiguration
	at org.apache.xerces.parsers.SAXParser.<init>(Unknown Source)
	at org.apache.xerces.parsers.SAXParser.<init>(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser.<init>(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserImpl.<init>(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserFactoryImpl.newSAXParser(Unknown Source)
	at org.jboss.xb.binding.parser.sax.SaxJBossXBParser.<init>(SaxJBossXBParser.java:92)
	... 75 more
2009-10-02 11:49:05,786 ERROR [org.apache.catalina.startup.ContextConfig] (main) Marking this application unavailable due to previous error(s)
2009-10-02 11:49:05,786 ERROR [org.apache.catalina.core.StandardContext] (main) Context [/jackrabbit-webapp-2.0-alpha9] startup failed due to previous errors
2009-10-02 11:49:06,473 ERROR [org.jboss.kernel.plugins.dependency.AbstractKernelController] (main) Error installing to Start: name=jboss.web.deployment:war=/jackrabbit-webapp-2.0-alpha9 state=Create mode=Manual requiredState=Installed
org.jboss.deployers.spi.DeploymentException: URL file:/C:/applications/jboss-5.1.0.GA/server/default/tmp/ahn1p-6tv4p6-g0bacatm-1-g0bageil-9t/jackrabbit-webapp-2.0-alpha9.war/ deployment failed
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeployInternal(TomcatDeployment.java:331)
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeploy(TomcatDeployment.java:142)
	at org.jboss.web.deployers.AbstractWarDeployment.start(AbstractWarDeployment.java:461)
	at org.jboss.web.deployers.WebModule.startModule(WebModule.java:118)
	at org.jboss.web.deployers.WebModule.start(WebModule.java:97)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java:157)
	at org.jboss.mx.server.Invocation.dispatch(Invocation.java:96)
	at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)
	at org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:264)
	at org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:668)
	at org.jboss.system.microcontainer.ServiceProxy.invoke(ServiceProxy.java:206)
	at $Proxy38.start(Unknown Source)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:42)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:37)
	at org.jboss.dependency.plugins.action.SimpleControllerContextAction.simpleInstallAction(SimpleControllerContextAction.java:62)
	at org.jboss.dependency.plugins.action.AccessControllerContextAction.install(AccessControllerContextAction.java:71)
	at org.jboss.dependency.plugins.AbstractControllerContextActions.install(AbstractControllerContextActions.java:51)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.system.microcontainer.ServiceControllerContext.install(ServiceControllerContext.java:286)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.ServiceController.doChange(ServiceController.java:688)
	at org.jboss.system.ServiceController.start(ServiceController.java:460)
	at org.jboss.system.deployers.ServiceDeployer.start(ServiceDeployer.java:163)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:99)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:46)
	at org.jboss.deployers.spi.deployer.helpers.AbstractSimpleRealDeployer.internalDeploy(AbstractSimpleRealDeployer.java:62)
	at org.jboss.deployers.spi.deployer.helpers.AbstractRealDeployer.deploy(AbstractRealDeployer.java:50)
	at org.jboss.deployers.plugins.deployers.DeployerWrapper.deploy(DeployerWrapper.java:171)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doDeploy(DeployersImpl.java:1439)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1157)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1178)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.install(DeployersImpl.java:1098)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.process(DeployersImpl.java:781)
	at org.jboss.deployers.plugins.main.MainDeployerImpl.process(MainDeployerImpl.java:702)
	at org.jboss.system.server.profileservice.repository.MainDeployerAdapter.process(MainDeployerAdapter.java:117)
	at org.jboss.system.server.profileservice.repository.ProfileDeployAction.install(ProfileDeployAction.java:70)
	at org.jboss.system.server.profileservice.repository.AbstractProfileAction.install(AbstractProfileAction.java:53)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.install(AbstractProfileService.java:361)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.activateProfile(AbstractProfileService.java:306)
	at org.jboss.system.server.profileservice.ProfileServiceBootstrap.start(ProfileServiceBootstrap.java:271)
	at org.jboss.bootstrap.AbstractServerImpl.start(AbstractServerImpl.java:461)
	at org.jboss.Main.boot(Main.java:221)
	at org.jboss.Main$1.run(Main.java:556)
	at java.lang.Thread.run(Thread.java:619)
2009-10-02 11:49:06,598 ERROR [org.jboss.kernel.plugins.dependency.AbstractKernelController] (main) Error installing to Real: name=vfszip:/C:/applications/jboss-5.1.0.GA/server/default/deploy/jackrabbit-webapp-2.0-alpha9.war/ state=PreReal mode=Manual requiredState=Real
org.jboss.deployers.spi.DeploymentException: URL file:/C:/applications/jboss-5.1.0.GA/server/default/tmp/ahn1p-6tv4p6-g0bacatm-1-g0bageil-9t/jackrabbit-webapp-2.0-alpha9.war/ deployment failed
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeployInternal(TomcatDeployment.java:331)
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeploy(TomcatDeployment.java:142)
	at org.jboss.web.deployers.AbstractWarDeployment.start(AbstractWarDeployment.java:461)
	at org.jboss.web.deployers.WebModule.startModule(WebModule.java:118)
	at org.jboss.web.deployers.WebModule.start(WebModule.java:97)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java:157)
	at org.jboss.mx.server.Invocation.dispatch(Invocation.java:96)
	at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)
	at org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:264)
	at org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:668)
	at org.jboss.system.microcontainer.ServiceProxy.invoke(ServiceProxy.java:206)
	at $Proxy38.start(Unknown Source)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:42)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:37)
	at org.jboss.dependency.plugins.action.SimpleControllerContextAction.simpleInstallAction(SimpleControllerContextAction.java:62)
	at org.jboss.dependency.plugins.action.AccessControllerContextAction.install(AccessControllerContextAction.java:71)
	at org.jboss.dependency.plugins.AbstractControllerContextActions.install(AbstractControllerContextActions.java:51)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.system.microcontainer.ServiceControllerContext.install(ServiceControllerContext.java:286)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.ServiceController.doChange(ServiceController.java:688)
	at org.jboss.system.ServiceController.start(ServiceController.java:460)
	at org.jboss.system.deployers.ServiceDeployer.start(ServiceDeployer.java:163)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:99)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:46)
	at org.jboss.deployers.spi.deployer.helpers.AbstractSimpleRealDeployer.internalDeploy(AbstractSimpleRealDeployer.java:62)
	at org.jboss.deployers.spi.deployer.helpers.AbstractRealDeployer.deploy(AbstractRealDeployer.java:50)
	at org.jboss.deployers.plugins.deployers.DeployerWrapper.deploy(DeployerWrapper.java:171)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doDeploy(DeployersImpl.java:1439)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1157)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1178)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.install(DeployersImpl.java:1098)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.process(DeployersImpl.java:781)
	at org.jboss.deployers.plugins.main.MainDeployerImpl.process(MainDeployerImpl.java:702)
	at org.jboss.system.server.profileservice.repository.MainDeployerAdapter.process(MainDeployerAdapter.java:117)
	at org.jboss.system.server.profileservice.repository.ProfileDeployAction.install(ProfileDeployAction.java:70)
	at org.jboss.system.server.profileservice.repository.AbstractProfileAction.install(AbstractProfileAction.java:53)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.install(AbstractProfileService.java:361)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.activateProfile(AbstractProfileService.java:306)
	at org.jboss.system.server.profileservice.ProfileServiceBootstrap.start(ProfileServiceBootstrap.java:271)
	at org.jboss.bootstrap.AbstractServerImpl.start(AbstractServerImpl.java:461)
	at org.jboss.Main.boot(Main.java:221)
	at org.jboss.Main$1.run(Main.java:556)
	at java.lang.Thread.run(Thread.java:619)
"
1,Search results not ordered. The query statements in search.jsp do not have an order by.
1,FALSE predicate always returns true. org.apache.jackrabbit.spi.commons.iterator.Predicates..FALSE always returns true instead of false.
1,"Redirect and Kerberos authentication in conflict. We are using the HttpClient to connect to a Website that uses Kerberos-Authentication.

Beware this trigger word: Kerberos! I think this is *not* the problem, but please read on.

Here is the sequence of events:

Client: GET /
Server: Unauthorized.
Client: GET / and includes authentication.
Server: 302 to /something on the same host (this shows that in principle authentication works)
Client: GET /something,  does not include authentication
Server: Unauthorized

Client quits with 401-Unauthorized.

I would have expected one of the following instead:

1) Client immediately sends authorization information with the redirected GET /something
2) Client re-requests the /something with authorization after 401-Unauthorized.

We could get around the problem by setting the ConnectionReuseStrategy to a constant false.

It would be great if someone could tell me if HttpClient works as expected or whether there is a bug or misconfiguration lurking.

Thanks,
Harald.
"
1,"NPE Exception Thrown By FileJournal During Commit Operation. The following exception stack traces appearing repeatedly during a performance test of a JCR cluster at a customer site. 

ERROR - Unexpected error while preparing log entry.
java.lang.NullPointerException
	at org.apache.jackrabbit.core.cluster.FileRevision.unlock(FileRevision.java:117)
	at org.apache.jackrabbit.core.cluster.FileRevision.get(FileRevision.java:146)
	at org.apache.jackrabbit.core.cluster.FileJournal.sync(FileJournal.java:296)
	at org.apache.jackrabbit.core.cluster.FileJournal.begin(FileJournal.java:435)
	at org.apache.jackrabbit.core.cluster.ClusterNode.updatePrepared(ClusterNode.java:399)
	at org.apache.jackrabbit.core.cluster.ClusterNode.access$000(ClusterNode.java:40)
	at org.apache.jackrabbit.core.cluster.ClusterNode$WorkspaceUpdateChannel.updatePrepared(ClusterNode.java:559)
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:647)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:778)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:808)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:326)
	at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:313)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:302)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:295)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1204)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:821)
	
ERROR - Unexpected error while committing log entry.
java.lang.NullPointerException
	at org.apache.jackrabbit.core.cluster.FileJournal.commit(FileJournal.java:660)
	at org.apache.jackrabbit.core.cluster.ClusterNode.updateCommitted(ClusterNode.java:425)
	at org.apache.jackrabbit.core.cluster.ClusterNode$WorkspaceUpdateChannel.updateCommitted(ClusterNode.java:566)
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:712)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:808)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:326)
	at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:313)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:302)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:295)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1204)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:821)
	"
1,Aggregate include ignored if no primaryType set. If the include element of an aggregate definition does not have a primaryType attribute then the include is never matched.
1,"MSSqlFileSystem - JNDI & several configuration issues. there are several configuration issues using the org.apache.jackrabbit.core.fs.db.MSSqlFileSystem
my (working) configuration (repository.xml) looks like:

<FileSystem class=""org.apache.jackrabbit.core.fs.db.MSSqlFileSystem"">
 <param name=""driver"" value=""javax.naming.InitialContext""/>
 <param name=""url"" value=""java:MYDatasource""/>
 <param name=""schema"" value=""mssql""/>
 <param name=""schemaObjectPrefix"" value=""MYPREFIX_""/>
 <param name=""user"" value=""MYUSERNAME""/> 
 <param name=""password"" value=""MYPASSWORD""/>
 <param name=""tableSpace"" value=""""/>
</FileSystem>

i have to unnecessarily specify username & password, because the MSSqlFileSystem presets them to an empty string instead of null. funnily enough  the tableSpace is preset to null, which leads to a nullpointer in createSchemaSql



"
1,FilterIndexReader should overwrite isOptimized(). A call of FilterIndexReader.isOptimized() results in a NPE because FilterIndexReader does not overwrite isOptimized().
1,"NameSet does not implement equals(Object) and hashCode() methods. The merge context uses the NameSet.equals(NameSet) method to compare two sets; however, the NameSet class does not override the default Object.equals(Object) method, and does not inherit from AbstractSet<E>.  Therefore, the merge check fails, even though the mixin sets are the same.  Object instance equivalence is being performed as opposed to set equivalence.  Behavior is observed when more than one thread is checking the ISM at a given time.  Demonstration code available upon request.

From NodeStateMerger, line 83:
                // mixin types
                if (!state.getMixinTypeNames().equals(overlayedState.getMixinTypeNames())) {
                    // the mixins have been modified but by just looking at the diff we
                    // can't determine where the change happened since the diffs of either
                    // removing a mixin from the overlayed or adding a mixin to the
                    // transient state would look identical...
                    return false;
                }

Proposed solution:
- Implement NameSet.equals(...) method:
	public boolean equals(Object obj) {
		if (obj != null && obj instanceof NameSet) {
			NameSet oo = (NameSet) obj;
			return oo.names.equals(this.names);
		}
		return false;
	}"
1,"IndexWriter memory leak when large docs are indexed. Spinoff from the java-user thread ""IndexWriter and memory usage""...

IndexWriter has had a long standing memory leak, since LUCENE-843.

When the byte/char/int blocks are recycled to the common pool, the
per-thread DW classes incorrectly still hold a reference to them.

This normally is not a problem, since these buffers will be re-used
again.

But, if you index a massive document, causing IW to allocate more than
the RAM buffer allocated to it, then the leak happens.  So you could
have a 16 MB RAM buffer set, but if a huge doc required allocation of
200 MB worth of arrays, those 200 MB are never freed (well, until you
close the IW and deref it from the app).

It's even worse if you use multiple threads: if each thread has ever
had to index a massive document, then that thread incorrectly holds
onto the extra arrays.
"
1,"System properties does not get replaced in a Cluster configuration. Since JCR-1304 has been added to jackrabbit 1.4 I guess this should be reported as a bug...

Still not debugged deeply, but if I try to configure a Cluster using:
<Cluster id=""${server}"" syncDelay=""10"">

after setting a ""server"" system property I expect to have the cluster initialized properly using the value of such property... I just realized that my cluster node gets initialized with the final value of ""${server}"" instead :(

Cluster config is a very good place where to use system properties, since all the configuration is usually identical between cluster nodes while the ""id"" property must be different...

Is there anything I missed/did wrong in my configuration?
"
1,"JCR-SQL2 query with multiple columns in result only returns last column when using Row.getValues(). When running a query like below on an in-process repository (via TransientRepository) or via RMI access, a call to Row.getValues() only returns the last column selected:

       SELECT property1, property2 FROM [nodetype]

QueryResult.getColumnNames() returns the right set of columns.

Stepping through the code shows that org.apache.jackrabbit.core.query.lucene.join.AbstractRow has the implementation of getValues() - this creates a new Values array, then overwrites it multiple times in a for loop that iterates once per column. That doesn't sound like the desired behaviour.

Getting values via individual calls to Row.getValue(""property1"") gives the correct results.

"
1,"BytesRef reuse bugs in QueryParser and analysis.jsp. Some code uses BytesRef as if it were a ""String"", in this case consumers of TermToBytesRefAttribute.
The thing is, while our general implementation works on char[] and then populates the consumers BytesRef,
not all TermToBytesRefAttribute implementations do this, specifically ICU collation, it reuses the bytes and simply sets the pointers:
{noformat}
  @Override
  public int toBytesRef(BytesRef target) {
    collator.getRawCollationKey(toString(), key);
    target.bytes = key.bytes;
    target.offset = 0;
    target.length = key.size;
    return target.hashCode();
  }
{noformat}

Most of the blame falls on me as I added this to the queryparser in LUCENE-2514.

Attached is a patch so that these consumers re-use a 'spare' and copy the bytes when they are going to make a long lasting object such as a Term.
"
1,deadlock on concurrent commit/locking. there can happen an iterlock between the dispatching part of the shared item state manager and the lockmanager.
1,"TestTermsEnum.testIntersectRandom fail. {noformat}
    [junit] Testsuite: org.apache.lucene.index.TestTermsEnum
    [junit] Testcase: testIntersectRandom(org.apache.lucene.index.TestTermsEnum):	FAILED
    [junit] (null)
    [junit] junit.framework.AssertionFailedError
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1530)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1432)
    [junit] 	at org.apache.lucene.index.codecs.BlockTreeTermsReader$FieldReader$IntersectEnum.getState(BlockTreeTermsReader.java:894)
    [junit] 	at org.apache.lucene.index.codecs.BlockTreeTermsReader$FieldReader$IntersectEnum.seekToStartTerm(BlockTreeTermsReader.java:969)
    [junit] 	at org.apache.lucene.index.codecs.BlockTreeTermsReader$FieldReader$IntersectEnum.<init>(BlockTreeTermsReader.java:786)
    [junit] 	at org.apache.lucene.index.codecs.BlockTreeTermsReader$FieldReader.intersect(BlockTreeTermsReader.java:483)
    [junit] 	at org.apache.lucene.index.TestTermsEnum.testIntersectRandom(TestTermsEnum.java:293)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1530)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1432)
    [junit] 
    [junit] 
    [junit] Tests run: 6, Failures: 1, Errors: 0, Time elapsed: 14.762 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestTermsEnum -Dtestmethod=testIntersectRandom -Dtests.seed=320d0949741fc6b1:3cabdb9b04d0d243:-4b7c80572775ed92 -Dtests.multiplier=3
{noformat}"
1,"RMI published Repository using the jcr-rmi library gets lost over time. The jcr-server/webapp project contains a servlet - RepositoryStartupServlet - which may be used in a web app to start a repository and optionally register the repository with JNDI and RMI. To register the repository with JNDI, the jcr-rmi library is used to create a Remote repository instance, which is registered with the RMI registry. Inside the RMI implementation mechanisms based on stub classes created by the RMI compiler are created to make the remote repository available remotely. This includes creating a object table to map remote references to local objects. This table stores references to the local object as weak references to support distributed garbage collection.

Over time, it may now be that this remote repository instance is actually collected and the object table cannot access it anymore thus preventing the repository from being accessed remotely. To prevent this from happening, the RepositoryStartupServlet must keep a strong reference to the remote repository and drop this reference when the servlet is destroyed and the repository unregistered.

*NOTE:* This is an issue to all long running applications which publish repository instances over RMI using the jcr-rmi library."
1,"Resource Leakage when loading keystore in AuthSSLProtocolSocketFactory. Opened stream not closed after keystore is loaded, resulting in resource leakage:

private static KeyStore createKeyStore(final URL url, final String password) 
        throws KeyStoreException, NoSuchAlgorithmException, CertificateException, IOException
    {
        if (url == null) {
            throw new IllegalArgumentException(""Keystore url may not be null"");
        }
        LOG.debug(""Initializing key store"");
        KeyStore keystore  = KeyStore.getInstance(""jks"");
        keystore.load(url.openStream(), password != null ? password.toCharArray(): null);
        return keystore;
    }

Should be changed to something like:

private static KeyStore createKeyStore(final URL url, final String password) 
        throws KeyStoreException, NoSuchAlgorithmException, CertificateException, IOException
    {
        if (url == null) {
            throw new IllegalArgumentException(""Keystore url may not be null"");
        }
        LOG.debug(""Initializing key store"");
        KeyStore keystore  = KeyStore.getInstance(""jks"");
        InputStream is = ulr.openStream();
        try {
          keystore.load(is, password != null ? password.toCharArray(): null);
        } finally {
           is.close();
        }
        return keystore;
    }"
1,"Use of java.net.URI.resolve() is buggy. The use of java.net.URI.resolve() is buggy (see <http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4708535>). Affected class: org.apache.http.impl.client.DefaultRedirectHandler.

Proposed solution: Create a resolve(URI, String) method in o.a.h.client.utils.URLUtils."
1,"DBFileSystem MySQL DDL not compatible with pre-5.0 versions. The packaged ddl for mysql index sizes is too large for 4.x versions of MySQL. As the sum-total of the index sizes may only reach 500.

So, 

create unique index ${schemaObjectPrefix}FSENTRY_IDX on ${schemaObjectPrefix}FSENTRY (FSENTRY_PATH(745), FSENTRY_NAME)

will not work. I would suggest shortening the FSENTRY_PATH index value to 245, as FSENTRY_NAME is already set to 255.

create unique index ${schemaObjectPrefix}FSENTRY_IDX on ${schemaObjectPrefix}FSENTRY (FSENTRY_PATH(245), FSENTRY_NAME)"
1,"Event filtering by path not working as specified. When filtering node events by path, the event filter doesn't compare using the ""associated parent path"", see JSR-170, 8.3.3:

""The set of events can be filtered by specifying restrictions based on characteristics of the associated parent node of the event. The associated parent node of an event is the parent node of the item at (or formerly at) the path returned by Event.getPath. The following restrictions are available:

• absPath, isDeep: Only events whose associated parent node is at absPath (or within its subtree, if isDeep is true) will be received. It is permissible to register a listener for a path where no node currently exists.""

(for property events, filtering is correct)

To fix this, the special handling of node events in EventFilter.blocks() simply needs to be removed.
"
1,"XMLPersistenceManager incorrectly handles properties. JCR Property instances are written by the XMLPersistenceManager as java.util.Properties files and loaded through the loadPropertyState() method as Properties files. Unfortunately the reload() method tries to re-load the Property states from XML files, which is not possible."
1,"Wrong creation of AuthScope object. Class Name: org.apache.http.client.protocol.RequestAuthCache
Line #: 118-119

Issue: Want to create a new Object of AuthScope by passing host name, port and scheme name but due to incorrect constructor call, Getting a object with realm name as scheme name.
Current Code: Credentials creds = credsProvider.getCredentials(new AuthScope(host.getHostName(), host.getPort(), null, schemeName));"
1,"ArrayIndexOutOfBounds Exception on invalid content-length. If the server returns an invalid (not parsable to int) content legnth the method
protected int getResponseContentLength() in HttpMethodBase walks off the
end of the Header[] array and throws the ArrayIndexOutOfBoundsException.

The loop at line 687 in HttpMethodBase.java:

   for (int i = headers.length - 1; i >= 0; i++) {

starts at the end of the array, but uses ++ intead of -- and so walks off the
end of the array on the next line if the header is invalid.  If the header is
valid the return statement in the try block succeeds so there is no error.

The fix is simply to change the line to be

   for (int i = headers.length -1; i>=0; i--) {"
1,"TestNRTThreads hangs in nightly 3.x builds. Maybe we have a problem, maybe its a bug in the test.

But its strange that lately the 3.x nightlies have been hanging here."
1,"leading wildcard's don't work with trailing wildcard. As reported by Antony Bowesman, leading wildcards don't work when there is a trailing wildcard character -- instead a PrefixQuery is constructed.


http://www.nabble.com/QueryParser-bug--tf3270956.html"
1,"jcr-server: DefaultItemCollection#unlock does not call DavSession#removeReference. DefaultItemCollection#unlock does not remove the token-reference from the DavSession that has been added
before upon creating the lock. This causes pending lock-references thus the cache entries in JCRWebdavServer
will not be cleared filling up the cache although the locks have been properly released. 
"
1,"add checks/asserts if you search across a closed reader. if you try to search across a closed reader (and/or searcher too),
there are no checks, not even assertions statements.

this results in crazy scary stacktraces deep inside places like FSTs/various term dictionary implementations etc.

In some situations, depending on codec, you wont even get an error (i'm sure its fun when you try to retrieve the stored fields!)
"
1,"NoSuchItemStateException on checkin after removeVersion in XA Environment. After removing a version, a checkin on the same node in a different transaction (with a different session) fails.
The NoSuchItemStateException refer to the uuid of the previously removed version. 
I'll attach a test demonstrating the problem. "
1,"HttpClient loops endlessly while trying to retrieve status line. When fed with the wrong URL, for example http://localhost:19/ (chargen port),
HttpClient will loop endlessly while attempting to read the status line.

This is caused by a bug in HttpMethodBase.readStatusLine(HttpState, HttpConnection)

(while loop without any exceptional abort condition).

wire log excerpt:

2003/11/10 12:33:04:085 CET [DEBUG] HttpMethodDirector - -Execute loop try 1
2003/11/10 12:33:04:312 CET [DEBUG] wire - ->> ""GET / HTTP/1.1[\r][\n]""
2003/11/10 12:33:04:351 CET [DEBUG] HttpMethodBase - -Adding Host request header
2003/11/10 12:33:04:532 CET [DEBUG] wire - ->> ""User-Agent: Jakarta
Commons-HttpClient[\r][\n]""
2003/11/10 12:33:04:554 CET [DEBUG] wire - ->> ""Host: localhost:19[\r][\n]""
2003/11/10 12:33:04:559 CET [DEBUG] wire - ->> ""[\r][\n]""
2003/11/10 12:33:04:639 CET [DEBUG] wire - -<<
""!""#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefgh[\r][\n]""
2003/11/10 12:33:04:669 CET [DEBUG] wire - -<<
""""#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghi[\r][\n]""
2003/11/10 12:33:04:673 CET [DEBUG] wire - -<<
""#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghij[\r][\n]""
2003/11/10 12:33:04:692 CET [DEBUG] wire - -<<
""$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijk[\r][\n]""
2003/11/10 12:33:04:698 CET [DEBUG] wire - -<<
""%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijkl[\r][\n]""
2003/11/10 12:33:04:703 CET [DEBUG] wire - -<<
""&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklm[\r][\n]""
<snip>"
1,"False assertion of >0 position delta in StandardPostingsWriterImpl. StandardPostingsWriterImpl line 159 is:
{code:java}
    assert delta > 0 || position == 0 || position == -1: ""position="" + position + "" lastPosition="" + lastPosition;            // not quite right (if pos=0 is repeated twice we don't catch it)
{code}

I enable assertions when I run my unit tests and I've found this assertion to fail when delta is 0 which occurs when the same position value is sent in twice in arrow.  Once I added RemoveDuplicatesTokenFilter, this problem went away.  Should I really be forced to add this filter?  I think delta >= 0 would be a better assertion."
1,"Empty response body is not properly handled when chunked encoding is used. IIS 5.0 server, when returning no content in response to an HTTP/1.1 request,
still includes ""Transfer-Encoding: chunked"" response header. As HttpClient
always expects chunk-encoded stream to be properly terminated, an
HttpRecoverableException exception results, when no content is sent back

=====================================================================

POST /someurl.aspx HTTP/1.1
Content-Length: 1132
Host: xxx.xxx.xxx.xxx
User-Agent: Jakarta Commons-HttpClient/2.0alpha2
Content-Type: multipart/form-data; boundary=----------------314159265358979323846

------------------314159265358979323846
Content-Disposition: form-data; name=""nmFile""; filename=""xxxxxxxxx.xml""
Content-Type: application/octet-stream

<... content removed ...>

------------------314159265358979323846--

HTTP/1.1 200 OK
Server: Microsoft-IIS/5.0
Date: Sat, 08 Feb 2003 15:22:26 GMT
Transfer-Encoding: chunked
Cache-Control: private
Content-Type: text/html

=====================================================================

Bug reported by Jim Crossley"
1,"ICU collator thread-safety issues. The ICU Collators (unlike the JDK ones) aren't thread safe: http://userguide.icu-project.org/collation/architecture , a little non-obvious since its not mentioned
in the javadocs, and its not clear if the docs apply to only the C code, but i looked
at the source and there is all kinds of internal state.

So in my opinion, we should clone the icu collators (which are passed in from the outside) 
when creating a new TokenStream/AttributeImpl to prevent problems. This shouldn't be a big
deal since everything uses reusableTokenStream anyway.
"
1,"highlighter problems with overlapping tokens. The lucene highlighter has problems when tokens that overlap are generated.

For example, if analysis of iPod generates the tokens ""i"", ""pod"", ""ipod"" (with pod and ipod in the same position),
then the highlighter will output this as iipod, regardless of if any of those tokens are highlighted.

Discovered via http://issues.apache.org/jira/browse/SOLR-24
"
1,"BitVector.isSparse is sometimes wrong. In working on LUCENE-3246, I found a few problems with
BitVector.isSparse:

  * Its math can overflow int, such that if there are enough deleted
    docs and maxDoc() is largish, isSparse may incorrectly return true

  * It over-estimates the size of the sparse file, since when
    estimating number of bytes for the vInt dgaps it uses bits.length
    instead of bits.length divided by number of set bits (ie, the
    ""average"" gap between set bits)

This is relatively harmless (just affects performance / size of .del
file on disk, not correctness).
"
1,"close() throws incorrect IllegalStateEx after IndexWriter hit an OOME when autoCommit is true. Spinoff from http://www.nabble.com/IllegalStateEx-thrown-when-calling-close-to20201825.html

When IndexWriter hits an OOME, it records this and then if close() is
called it calls rollback() instead.  This is a defensive measure, in
case the OOME corrupted the internal buffered state (added/deleted
docs).

But there's a bug: if you opened IndexWriter with autoCommit true,
close() then incorrectly throws an IllegalStatException.

This fix is simple: allow rollback to be called even if autoCommit is
true, internally during close.  (External calls to rollback with
autoCommmit true is still not allowed).
"
1,"LuceneTestCase's check for uncaught exceptions in threads causes collateral damage?. Eg see these failures:

    https://hudson.apache.org/hudson/job/Lucene-3.x/214/

Multiple test methods failed in TestIndexWriterOnDiskFull, but, I think only 1 test had a real failure but somehow our ""thread hit exc"" tracking incorrectly blames the other 3 cases?

I'm not sure about this but it seems like something like that is going on...

So, one problem is that LuceneTestCase.tearDown fails on any thread excs, but if CMS had also hit a failure, then fails to clear CMS's thread failures.  I think we should just remove CMS's thread failure tracking?  (It's static so it can definitely bleed across tests).  Ie, just rely on LuceneTestCase's tracking."
1,"jcr2spi: NPE with SessionImporter#checkIncludesMixReferenceable if NodeInfo doesn't contain mixin names. issue reported by tobi:

java.lang.NullPointerException
	at java.util.Arrays$ArrayList.<init>(Arrays.java:2355)
	at java.util.Arrays.asList(Arrays.java:2341)
	at org.apache.jackrabbit.jcr2spi.xml.SessionImporter.checkIncludesMixReferenceable(SessionImporter.java:637)
	at org.apache.jackrabbit.jcr2spi.xml.SessionImporter.startNode(SessionImporter.java:209)

including test case:

    public void testEmptyMixins() throws Exception {
        String xml = ""<?xml version=\""1.0\"" encoding=\""UTF-8\""?>\n"" +
                ""<sv:node xmlns:nt=\""http://www.jcp.org/jcr/nt/1.0\""\n"" +
                ""         xmlns:sv=\""http://www.jcp.org/jcr/sv/1.0\""\n"" +
                ""         xmlns:mix=\""http://www.jcp.org/jcr/mix/1.0\""\n"" +
                ""         xmlns:jcr=\""http://www.jcp.org/jcr/1.0\""\n"" +
                ""         sv:name=\""testnode1\"">\n"" +
                ""    <sv:property sv:name=\""jcr:primaryType\""
sv:type=\""Name\"">\n"" +
                ""        <sv:value>nt:unstructured</sv:value>\n"" +
                ""    </sv:property>\n"" +
                ""    <sv:property sv:name=\""jcr:title\"" sv:type=\""String\"">\n"" +
                ""        <sv:value>Test Node</sv:value>\n"" +
                ""    </sv:property>\n"" +
                ""    <sv:property sv:name=\""jcr:uuid\"" sv:type=\""String\"">\n"" +
                ""        <sv:value>1234</sv:value>\n"" +
                ""    </sv:property>\n"" +
                ""</sv:node>"";

        InputStream in = new ByteArrayInputStream(xml.getBytes());
        session.importXML(""/"", in,
ImportUUIDBehavior.IMPORT_UUID_COLLISION_THROW);
        session.save();
    }"
1,"Deprecated Serializer does not properly delegate method calls.. The deprecated org.apache.jackrabbit.core.state.util.Serializer class does not actually forward method calls to its replacement. Instead it calls itself repeatedly, leading to infinite recursion. The attached test demonstrates this and yields the following trace:

<<
java.lang.StackOverflowError
	at org.apache.jackrabbit.core.state.util.Serializer.serialize(Serializer.java:39)
>>"
1,"Analysis back compat break. Old and new style token streams don't mix well.
"
1,"Incorrect usage of AttributeSource.addAttribute/getAttribute leads to failures when onlyUseNewAPI=true. when seting ""use only new API"" for TokenStream, i received the following exception:

{code}
   [junit] Caused by: java.lang.IllegalArgumentException: This AttributeSource does not have the attribute 'interface org.apache.lucene.analysis.tokenattributes.TermAttribute'.
    [junit] 	at org.apache.lucene.util.AttributeSource.getAttribute(AttributeSource.java:249)
    [junit] 	at org.apache.lucene.index.TermsHashPerField.start(TermsHashPerField.java:252)
    [junit] 	at org.apache.lucene.index.DocInverterPerField.processFields(DocInverterPerField.java:145)
    [junit] 	at org.apache.lucene.index.DocFieldProcessorPerThread.processDocument(DocFieldProcessorPerThread.java:244)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:772)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:755)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:2613)
{code}

However, i can't actually see the culprit that caused this exception

suggest that the IllegalArgumentException include ""getClass().getName()"" in order to be able to identify which TokenStream implementation actually caused this
"
1,"Few issues with CachingCollector. CachingCollector (introduced in LUCENE-1421) has few issues:
# Since the wrapped Collector may support out-of-order collection, the document IDs cached may be out-of-order (depends on the Query) and thus replay(Collector) will forward document IDs out-of-order to a Collector that may not support it.
# It does not clear cachedScores + cachedSegs upon exceeding RAM limits
# I think that instead of comparing curScores to null, in order to determine if scores are requested, we should have a specific boolean - for clarity
# This check ""if (base + nextLength > maxDocsToCache)"" (line 168) can be relaxed? E.g., what if nextLength is, say, 512K, and I cannot satisfy the maxDocsToCache constraint, but if it was 10K I would? Wouldn't we still want to try and cache them?

Also:
* The TODO in line 64 (having Collector specify needsScores()) -- why do we need that if CachingCollector ctor already takes a boolean ""cacheScores""? I think it's better defined explicitly than implicitly?

* Let's introduce a factory method for creating a specialized version if scoring is requested / not (i.e., impl the TODO in line 189)

* I think it's a useful collector, which stands on its own and not specific to grouping. Can we move it to core?

* How about using OpenBitSet instead of int[] for doc IDs?
** If the number of hits is big, we'd gain some RAM back, and be able to cache more entries
** NOTE: OpenBitSet can only be used for in-order collection only. So we can use that if the wrapped Collector does not support out-of-order

* Do you think we can modify this Collector to not necessarily wrap another Collector? We have such Collector which stores (in-memory) all matching doc IDs + scores (if required). Those are later fed into several processes that operate on them (e.g. fetch more info from the index etc.). I am thinking, we can make CachingCollector *optionally* wrap another Collector and then someone can reuse it by setting RAM limit to unlimited (we should have a constant for that) in order to simply collect all matching docs + scores.

* I think a set of dedicated unit tests for this class alone would be good.

That's it so far. Perhaps, if we do all of the above, more things will pop up."
1,"DbDataStore: garbage collection deadlock. Sometimes, the unit tests hangs with the following threads blocked. It looks like a database level deadlock caused by the DbDataStore implementation. The database used is Apache Derby.

org.apache.jackrabbit.core.data.db.DbDataStore.addRecord line=298
org.apache.jackrabbit.core.value.BLOBInDataStore.getInstance line=120
org.apache.jackrabbit.core.value.InternalValue.getBLOBFileValue line=644
org.apache.jackrabbit.core.value.InternalValue.create line=123
org.apache.jackrabbit.core.PropertyImpl.setValue line=609
org.apache.jackrabbit.core.PropertyImpl.setValue line=525
org.apache.jackrabbit.core.NodeImpl.setProperty line=2312
org.apache.jackrabbit.core.data.CopyValueTest.doTestCopy line=64
org.apache.jackrabbit.core.data.CopyValueTest.testCopyStream line=45

org.apache.jackrabbit.core.data.db.DbDataStore.updateLastModifiedDate line=641
org.apache.jackrabbit.core.data.db.DbDataStore.touch line=631
org.apache.jackrabbit.core.data.db.DbDataStore.getRecord line=484
org.apache.jackrabbit.core.value.BLOBInDataStore.getDataRecord line=136
org.apache.jackrabbit.core.value.BLOBInDataStore.getLength line=92
org.apache.jackrabbit.core.data.GarbageCollector.scanPersistenceManagers
org.apache.jackrabbit.core.data.GarbageCollector.scan line=161
org.apache.jackrabbit.core.data.GCThread.run line=52"
1,"add/remove dispatchers from DelegatingObservationDispatcher is not synchronized. the 'dispatchers' hashset in DelegatingObservationDispatcher is not synchronized and can lead to errors, when a workspace goes offline or is creating during event dispatching."
1,"IndexWriter.synced  field accumulates data leading to a Memory Leak. I am running into a strange OutOfMemoryError. My small test application does
index and delete some few files. This is repeated for 60k times. Optimization
is run from every 2k times a file is indexed. Index size is 50KB. I did analyze
the HeapDumpFile and realized that IndexWriter.synced field occupied more than
half of the heap. That field is a private HashSet without a getter. Its task is
to hold files which have been synced already.

There are two calls to addAll and one call to add on synced but no remove or
clear throughout the lifecycle of the IndexWriter instance.

According to the Eclipse Memory Analyzer synced contains 32618 entries which
look like file names ""_e065_1.del"" or ""_e067.cfs""

The index directory contains 10 files only.

I guess synced is holding obsolete data "
1,InstantiatedIndexReader does not implement getFieldNames properly. Causes error in org.apache.lucene.index.SegmentMerger.mergeFields
1,"session.setNamespacePrefix() creates ambiguous mappings. 1.) assume the following initial global mappings in the NamespaceRegistry 
(prefixes in lowercase, URIs in uppercase):

a  <-> A
b  <-> B
c  <-> C

2.) locally remap  the namespaces in a session using the following code:

            session.setNamespacePrefix(""x"", ""B"");
            session.setNamespacePrefix(""b"", ""C"");
            session.setNamespacePrefix(""c"", ""B"");

this results in the following session-local mappings:

a  <-> A
c  <-> B
b  <-> C

3.) now the following stmt:

            session.setNamespacePrefix(""b"", ""A"");

produces this ambiguous mapping:

b  <-> A
c  <?> B
c  <?> C

"
1,"Non-versionable children of a versionable node should not be updated when a merge fails. The JCR specification (JSR-170) includes a merge algorithm that is inconsistent with the functionality described elsewhere in the JCR specification. Specifically from JSR-170 section 8.2.10 Merge:

""In either case, (regardless of whether bestEffort is true or false) for each non-versionable node (including both referenceable and non-referenceable), if the merge result of its nearest versionable ancestor is update, or if it has no versionable ancestor, then it is updated to reflect the state of its corresponding node. Otherwise, it is left unchanged.""

The algorithm presented in 8.2.10.1 of the specification goes against the above statement as it does not take into consideration the merge result of the nearest versionable ancestor.

One solution would be to have the doLeave(n) call that dofail(n, v') calls altered to only perform a merge on the versionable children rather than all of the children. The merging of all children (versionable and non-versionable) should only be done if the nearest parent is not in a failed merge state regardless of whether the failure occurred from the current merge operation or a previous merge operation.

I will attach a patch file that makes what I think is the required change.
"
1,"I/O exception in DocsWriter add or updateDocument may not delete unreferenced files. If an I/O exception is thrown in DocumentsWriter#addDocument or #updateDocument, the stored fields files may not be cleaned up."
1,"Missing Content-Length header causes a SocketException. Essentially, we have an invalid HTTP server (Stellent CMS actually and we will file a bug with them), 
which is returning headers like:

HTTP/1.1 401 Unauthorized
WWW-Authenticate: Basic ""Secure Realm""
Connection: keep-alive

Which is clearly missing the Content-Length header.  Now, previously HttpClient handled this 
perfectly by reading until the end of the connection (ie: treating it like it was a Connection: close), 
however for some reason a socket exception is being thrown and the invalid connection is added 
back into the connection pool and then every connection to the server after that thows an 
exception.

See the thread ""SocketException with invalid server"" for the full discussion of the issue.

I'll attach a patch that fixes the problem.  The biggest thing to consider is the changes to the 
duplicate Connection header test cases which resolves around the question: if Connection: keep-
alive is present but no Content-Length is provided, should the connection be closed?  The patch 
requires the answer to be yes and I really can't see any other way to do it."
1,"Missing equals and hashcode preventing the re-use of SharedFieldSortComparator. As briefly mentioned in the dev email list, improperly implemented (i.e., missing - using the default Object implementation) equals and hashcode in SearchIndex.java prevents the reuse of a SharedFieldSortComparator between different queries when nothing has changed in the repository.  In tests, this appears to have a fairly significant negative performance impact.

Please see the following for the correct code:

http://svn.apache.org/viewvc?view=rev&revision=506908
"
1,CachingHttpClient.execute() does not catch the IOException thrown by HttpCache.getCacheEntry(). The IOException caused by the HttpCache is not caught and thus the whole http request fails. I would expect the response to be retrieved from the backend when the cache fails for some reason.
1,"Item.isSame() may return true for 2 nodes from different workspaces.. the code in ItemImpl.isSame() only compares the item id, but not the source workspace."
1,"InstantiatedTermEnum#skipTo(Term) throws ArrayIndexOutOfBoundsException on empty index. {code}
java.lang.ArrayIndexOutOfBoundsException: 0
	at org.apache.lucene.store.instantiated.InstantiatedTermEnum.skipTo(InstantiatedTermEnum.java:105)
	at org.apache.lucene.store.instantiated.TestEmptyIndex.termEnumTest(TestEmptyIndex.java:73)
	at org.apache.lucene.store.instantiated.TestEmptyIndex.testTermEnum(TestEmptyIndex.java:54)
{code}"
1,"Authentication fails with proxied SSL Connections. When connecting through a proxy, using SSL and authentication HttpClient winds 
up sending a GET request to the proxy after the initial auth required response, 
the proxy then obviously responds with a not implemented response since it 
can't handle a GET request to an SSL URL.  In essence the following is 
happening:

1. HttpClient sends Connect response.
2. Proxy responds 200 Connect OK
3. HttpClient uses SSL connection to send the request to the web server.
4. Web server responds with not authorized and closes the connection.
5. HttpClient opens a new connection to the proxy and issues a GET request for 
the SSL URL.
6. Proxy returns 501 not implemented.

I'll attach a full log to this bug.

This is likely to be hard to fix since the retry is performed in HttpMethodBase 
but the Connect method is executed by HttpClient so a fix for this may be best 
waiting for 2.1.  This looks very similar to HTTPCLIENT-195 except that that bug is 
marked as fixed and this one still doesn't work, this also applies to 
authentication schemes other than NTLM (testing NTLM and basic).

My best evaluation is that the web server returns Connection: close when it 
rejects the authorization attempt and then HttpMethodBase is incapable of 
creating a new SSL connection through the proxy.  The only thing I can think of 
that could be done prior to 2.1 to fix this is to send a Connection: keep-alive 
as well as the Proxy-Connection: Keep-Alive we're already sending with the 
original request."
1,"TCK: Test root path not escaped when used in XPath queries. A repository implementation might use a test root path that contains names that need _xXXXX_ escaping when used in XPath queries. Currently the TCK just uses the test path as-is when constructing queries. Even though this only affects few repositories (I've heard of one legacy connector to run into this problem), it would be good to add the proper escaping."
1,"Unreleased 2.3 version of IndexWriter.optimize()  consistly throws java.lang.IllegalArgumentException out-of-the-box. Since the upcoming 2.3 version of Lucene has support for the setRAMBufferSizeMB() method in Index Writer,  I thought I would test its performance.   So, using my application that was built upon (and worked with) Lucene 2.2,  I downloaded the nightly build 2007-10-26_03-16-46 and rebuilt my application with new code setting setRAMBufferSizeMB() from a properties file.   My test data resides in a database table of 30 columns holding 1.25 million records.   The good news is that performance is superior to Lucene 2.2.  The indexing completes in roughly 1/3 the time.   The bad news is the Index Writer.optimize() step now throws an java.lang.IllegalArgumentException.
I also run tests against various other tables.  Indexing smaller amounts of data did not throw the exception.  Indexing largers amounts of data did throw the exception.  Note, I also tested nightly builds dating back to 2007-10-05.

...
INFO:  SEIndexThread.commitCheck...
INFO:    ----Commit point reached:  1200000
INFO:  SEIndexThread.commitCheck...
INFO:    ----Commit point reached:  1225000
INFO:  SEIndexThread.commitCheck...
INFO:    ----Commit point reached:  1250000
INFO:  SEIndexThread.closeIndex()...
INFO:    ----commit point reached:  1250659
INFO:    ----optimize index
INFO: SEIndexThread():  java.lang.IllegalArgumentException

java.lang.IllegalArgumentException
        at java.lang.Thread.setPriority(Thread.java(Compiled Code))
        at org.apache.lucene.index.ConcurrentMergeScheduler.merge(ConcurrentMerg
eScheduler.java(Compiled Code))
        at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1750)
        at org.apache.lucene.index.IndexWriter.optimize(IndexWriter.java:1686)
        at org.apache.lucene.index.IndexWriter.optimize(IndexWriter.java:1652)
        at LuceneSearchEngine.optimizeIndex(LuceneSearchEngine.java:643)
        at LuceneSearchEngine.optimizeIndex(LuceneSearchEngine.java:636)
        at SEIndexThread.closeIndex(SEIndexThread.java:674)
        at SEIndexThread.processSearchObject(SEIndexThread.java:487)
        at SEIndexThread.prepareIndex(SEIndexThread.java:391)
        at SEIndexThread.run(SEIndexThread.java:41)

"
1,"Data Store: UTFDataFormatException when using large minRecordLength. If using a value larger than 33000 for minRecordLength, and then trying to store a value with 33000 bytes, the following exception is thrown: UTFDataFormatException. The reason is that values are serialized using DataOutputStream.writeUTF. There is size limitation of 65 K when using this method. Small entries are hex encoded, and there is a prefix, so the limitation for minRecordLength should be 32000.

This is a problem for both FileDataStore and DbDataStore.
"
1,"Deadlocks in ConcurrentVersioningWithTransactionsTest. Patch follows for a ConcurrentVersioningWithTransactionsTest, based on the existing ConcurrentVersioningTest but using transactions around the versioning operations.

On my macbook, running the test with CONCURRENCY = 100 and NUM_OPERATIONS = 100 causes a deadlock after a few seconds, thread dumps follow.

Note that I had to ignore StaleItemStateException (which is probably justified, due to not locking stuff IIUC) to let the threads run long enough to show the problem.

Running the test a few times showed the same locking pattern several times: some threads are locked at line 87 (session.save(), no transaction) while others are at line 93 (transaction.commit()), in testConcurrentCheckinInTransaction():

    80    public void testConcurrentCheckinInTransaction() throws RepositoryException {
    81      runTask(new Task() {
    82        public void execute(Session session, Node test) throws RepositoryException {
    83          int i = 0;
    84          try {
    85            Node n = test.addNode(""test"");
    86            n.addMixin(mixVersionable);
    87            session.save();
    88            for (i = 0; i < NUM_OPERATIONS / CONCURRENCY; i++) {
    89              final UserTransaction utx = new UserTransactionImpl(test.getSession());
    90              utx.begin();
    91              n.checkout();
    92              n.checkin();
    93              utx.commit();
    94            }
    95            n.checkout();
    96          } catch (Exception e) {
    97            final String threadName = Thread.currentThread().getName();
    98            final Throwable deepCause = getLevel2Cause(e);
    99            if(deepCause!=null && deepCause instanceof StaleItemStateException) {
   100              // ignore 
   101            } else {
   102              throw new RepositoryException(threadName + "", i="" + i + "":"" + e.getClass().getName(), e);
   103            }
   104          }
   105        }
   106      }, CONCURRENCY);
   107    }"
1,"Workspace is shut down while creating initial index. This only happens when a maxIdleTime is configured for the workspaces in the repository.xml and the workspace to index is not the default workspace.

The idle check considers a workspace as idle when there only a system session is open and the configured idle time elapsed. This is also the case when the workspace is initializing.

The repository should either check if a workspace is still initializing or we need to move the search manager initialization into the WorkspaceInfo.doInitialize() method.

"
1,"trectopicsreader doesn't properly read descriptions or narratives. TrecTopicsReader does not read these fields correctly, as demonstrated by the test case.
"
1,"Jackrabbit thread contention issue due to fat lock. Hello,

We are running jackrabbit 1.4.5 using a persistent file data store within a weblogic container and encountering a variety of thread locking issues. To get around the problem, we are forced synchronize thread access to the JCR repository or reduce thread worker count to 1 which has a heavy performance impact on our application. I'm not exactly sure what the problem is and was wondering someone is looking into this issue and if there is a workaround/fix planned?

<Oct 30, 2008 10:45:32 AM PDT> <Error> <WebLogicServer> <BEA-000337> <[STUCK] ExecuteThread: '5' for queue: 'weblogic.kernel.Default (self-tuning)' has been busy for ""1,863"" seconds working on the request ""weblogic.work.j2ee.J2EEWorkManager$WorkWithListener@2117cc9"", which is more than the configured time (StuckThreadMaxTime) of ""1,800"" seconds. Stack trace:
Thread-94 ""[STUCK] ExecuteThread: '5' for queue: 'weblogic.kernel.Default (self-tuning)'"" <alive, in native, suspended, waiting, priority=1, DAEMON> {
    -- Waiting for notification on: EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock@152c384[fat lock]
    java.lang.Object.wait(Object.java:???)
    java.lang.Object.wait(Object.java:474)
    EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock.acquire(Unknown Source)
    org.apache.jackrabbit.core.journal.AbstractJournal.lockAndSync(AbstractJournal.java:235)
    org.apache.jackrabbit.core.journal.DefaultRecordProducer.append(DefaultRecordProducer.java:49)

}

>
<Oct 30, 2008 10:45:32 AM PDT> <Error> <WebLogicServer> <BEA-000337> <[STUCK] ExecuteThread: '2' for queue: 'weblogic.kernel.Default (self-tuning)' has been busy for ""1,916"" seconds working on the request ""weblogic.work.j2ee.J2EEWorkManager$WorkWithListener@227b6d4"", which is more than the configured time (StuckThreadMaxTime) of ""1,800"" seconds. Stack trace:
Thread-25 ""[STUCK] ExecuteThread: '2' for queue: 'weblogic.kernel.Default (self-tuning)'"" <alive, in native, suspended, waiting, priority=1, DAEMON> {
    -- Waiting for notification on: EDU.oswego.cs.dl.util.concurrent.LinkedNode@42d58e0[fat lock]
    java.lang.Object.wait(Object.java:???)
    java.lang.Object.wait(Object.java:474)
    EDU.oswego.cs.dl.util.concurrent.SynchronousChannel.put(Unknown Source)
    EDU.oswego.cs.dl.util.concurrent.PooledExecutor$WaitWhenBlocked.blockedAction(Unknown Source)
    EDU.oswego.cs.dl.util.concurrent.PooledExecutor.execute(Unknown Source)
...
    org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:334)
    org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:307)
    org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:317)
    org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1072)
    ^-- Holding lock: org.apache.jackrabbit.core.query.lucene.VolatileIndex@3eb0f41[thin lock]
    org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:895)
    org.apache.jackrabbit.jca.JCASessionHandle.save(JCASessionHandle.java:178)
    com.qpass.inventory.ingestion.IngestionServiceImpl$1.doInJCR(IngestionServiceImpl.java:124)
    com.qpass.inventory.model.JCRTemplate.execute(JCRTemplate.java:17)
    com.qpass.inventory.ingestion.IngestionServiceImpl.ingestProduct(IngestionServiceImpl.java:93)
    com.qpass.inventory.ingestion.bulk.AbstractBulkIngester.ingestProduct(AbstractBulkIngester.java:42)
    com.qpass.inventory.ingestion.bulk.ZipFileBulkIngester.processFile(ZipFileBulkIngester.java:35)
    com.qpass.inventory.ingestion.bulk.IngestionWorker.processFile(IngestionWorker.java:26)
    com.qpass.inventory.ingestion.bulk.IngestionWorker$1.run(IngestionWorker.java:64)
    org.springframework.scheduling.commonj.DelegatingWork.run(DelegatingWork.java:61)
    weblogic.work.j2ee.J2EEWorkManager$WorkWithListener.run(J2EEWorkManager.java:245)
    weblogic.work.ExecuteThread.execute(ExecuteThread.java:206)
    weblogic.work.ExecuteThread.run(ExecuteThread.java:173)
}

>
<Oct 30, 2008 10:45:32 AM PDT> <Error> <WebLogicServer> <BEA-000337> <[STUCK] ExecuteThread: '1' for queue: 'weblogic.kernel.Default (self-tuning)' has been busy for ""1,891"" seconds working on the request ""weblogic.work.j2ee.J2EEWorkManager$WorkWithListener@2117c83"", which is more than the configured time (StuckThreadMaxTime) of ""1,800"" seconds. Stack trace:
Thread-24 ""[STUCK] ExecuteThread: '1' for queue: 'weblogic.kernel.Default (self-tuning)'"" <alive, in native, suspended, waiting, priority=1, DAEMON> {
    -- Waiting for notification on: EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock@152c384[fat lock]
    java.lang.Object.wait(Object.java:???)
    java.lang.Object.wait(Object.java:474)
    EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock.acquire(Unknown Source)
    org.apache.jackrabbit.core.journal.AbstractJournal.lockAndSync(AbstractJournal.java:235)
    org.apache.jackrabbit.core.journal.DefaultRecordProducer.append(DefaultRecordProducer.java:49)
    org.apache.jackrabbit.core.cluster.ClusterNode$WorkspaceUpdateChannel.updateCreated(ClusterNode.java:556)
...


<Oct 30, 2008 11:21:30 AM PDT> <Warning> <netuix> <BEA-423420> <Redirect is executed in begin or refresh action. Redirect url is /console/console.portal?_nfpb=true&_pageLabel=HomePage1.>
<Oct 30, 2008 11:44:32 AM PDT> <Error> <WebLogicServer> <BEA-000337> <[STUCK] ExecuteThread: '4' for queue: 'weblogic.kernel.Default (self-tuning)' has been busy for ""1,803"" seconds working on the request ""Http Request: /inventory/rpc/searchService"", which is more than the configured time (StuckThreadMaxTime) of ""1,800"" seconds. Stack trace:
Thread-51 ""[STUCK] ExecuteThread: '4' for queue: 'weblogic.kernel.Default (self-tuning)'"" <alive, in native, suspended, waiting, priority=1, DAEMON> {
    -- Waiting for notification on: java.lang.Object@1569e04[fat lock]
    java.lang.Object.wait(Object.java:???)
    java.lang.Object.wait(Object.java:474)
    org.apache.jackrabbit.core.query.lucene.MultiIndex.getIndexReader(MultiIndex.java:694)
    org.apache.jackrabbit.core.query.lucene.SearchIndex.getIndexReader(SearchIndex.java:825)
    org.apache.jackrabbit.core.query.lucene.SearchIndex.executeQuery(SearchIndex.java:682)
    org.apache.jackrabbit.core.query.lucene.QueryResultImpl.executeQuery(QueryResultImpl.java:242)
    org.apache.jackrabbit.core.query.lucene.QueryResultImpl.getResults(QueryResultImpl.java:271)
    org.apache.jackrabbit.core.query.lucene.QueryResultImpl.<init>(QueryResultImpl.java:177)
    org.apache.jackrabbit.core.query.lucene.QueryImpl.execute(QueryImpl.java:105)
    org.apache.jackrabbit.core.query.QueryImpl.execute(QueryImpl.java:174)
    com.qpass.inventory.service.QueryProfiler.execute(QueryProfiler.java:20)
    com.qpass.inventory.service.SearchServiceImpl$1.doInJCR(SearchServiceImpl.java:59)
    com.qpass.inventory.model.JCRTemplate.execute(JCRTemplate.java:17)
    com.qpass.inventory.service.SearchServiceImpl.doSearch(SearchServiceImpl.java:54)
    com.qpass.inventory.ui.impl.SearchUIServiceImpl.search(SearchUIServiceImpl.java:48)
    sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:???)
    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:27)
    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    java.lang.reflect.Method.invoke(Method.java:570)
    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:309)
    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:182)
    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:148)
    org.acegisecurity.intercept.method.aopalliance.MethodSecurityInterceptor.invoke(MethodSecurityInterceptor.java:62)
    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:148)
    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:151)
    $Proxy74.search(Unknown Source)
    sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:???)
    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:27)
    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    java.lang.reflect.Method.invoke(Method.java:570)
    org.gwtwidgets.server.spring.GWTRPCServiceExporter.invokeMethodOnService(GWTRPCServiceExporter.java:157)
    org.gwtwidgets.server.spring.GWTRPCServiceExporter.processCall(GWTRPCServiceExporter.java:295)
    com.google.gwt.user.server.rpc.RemoteServiceServlet.doPost(RemoteServiceServlet.java:173)
    org.gwtwidgets.server.spring.GWTRPCServiceExporter.handleRequest(GWTRPCServiceExporter.java:361)
    com.qpass.base.ui.security.GWTServiceExporter.handleRequest(GWTServiceExporter.java:45)
    org.springframework.web.servlet.mvc.HttpRequestHandlerAdapter.handle(HttpRequestHandlerAdapter.java:49)
    org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:831)
    org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:781)
    org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:567)
    org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:511)
    javax.servlet.http.HttpServlet.service(HttpServlet.java:736)
    javax.servlet.http.HttpServlet.service(HttpServlet.java:851)
    weblogic.servlet.internal.StubSecurityHelper$ServletServiceAction.run(StubSecurityHelper.java:224)
    weblogic.servlet.internal.StubSecurityHelper.invokeServlet(StubSecurityHelper.java:108)
    weblogic.servlet.internal.ServletStubImpl.execute(ServletStubImpl.java:198)
    weblogic.servlet.internal.TailFilter.doFilter(TailFilter.java:26)
    weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:41)
    org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:259)
    org.acegisecurity.intercept.web.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:93)
    org.acegisecurity.intercept.web.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:71)
    org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:259)
    com.qpass.usersecurity.auth.UpdatePermissionsOnContextChangeFilter.doFilter(UpdatePermissionsOnContextChangeFilter.java:44)
    org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:259)
    org.acegisecurity.ui.AbstractProcessingFilter.doFilter(AbstractProcessingFilter.java:191)
    org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:259)
    org.acegisecurity.context.HttpSessionContextIntegrationFilter.doFilter(HttpSessionContextIntegrationFilter.java:195)
    org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:259)
    org.acegisecurity.util.FilterChainProxy.doFilter(FilterChainProxy.java:122)
    org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:236)
    org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:154)
    weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:41)
    com.qpass.base.applicationcontext.RequestContextFilter.doFilter(RequestContextFilter.java:103)
    org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:236)
    org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:154)
    weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:41)
    org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:90)
    org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:61)
    weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:41)
    weblogic.servlet.internal.RequestEventsFilter.doFilter(RequestEventsFilter.java:24)
    weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:41)
    weblogic.servlet.internal.WebAppServletContext$ServletInvocationAction.run(WebAppServletContext.java:3214)
    weblogic.security.acl.internal.AuthenticatedSubject.doAs(AuthenticatedSubject.java:308)
    weblogic.security.service.SecurityManager.runAs(SecurityManager.java:117)
    weblogic.servlet.internal.WebAppServletContext.securedExecute(WebAppServletContext.java:1946)
    weblogic.servlet.internal.WebAppServletContext.execute(WebAppServletContext.java:1868)
    weblogic.servlet.internal.ServletRequestImpl.run(ServletRequestImpl.java:1331)
    weblogic.work.ExecuteThread.execute(ExecuteThread.java:206)
    weblogic.work.ExecuteThread.run(ExecuteThread.java:173)
}




<Oct 2, 2008 2:09:36 PM PDT> <Error> <WebLogicServer> <BEA-000337> <[STUCK] ExecuteThread: '1' for queue: 'weblogic.kernel.Default (self-tuning)' has been busy for ""696"" seconds working on the request ""weblogic.work.j2ee.J2EEWorkManager$WorkWithListener@863e564"", which is more than the configured time (StuckThreadMaxTime) of ""600"" seconds. Stack trace:
Thread-21 ""[STUCK] ExecuteThread: '1' for queue: 'weblogic.kernel.Default (self-tuning)'"" <alive, in native, suspended, priority=1, DAEMON> {
    java.io.FileOutputStream.writeBytes(FileOutputStream.java:???)
    java.io.FileOutputStream.write(FileOutputStream.java:260)
    java.io.BufferedOutputStream.write(BufferedOutputStream.java:100)
    ^-- Holding lock: java.io.BufferedOutputStream@39d70a5[thin lock]
    org.apache.jackrabbit.core.persistence.util.FileSystemBLOBStore.put(FileSystemBLOBStore.java:88)
    org.apache.jackrabbit.core.persistence.bundle.util.BundleBinding.writeState(BundleBinding.java:561)
    org.apache.jackrabbit.core.persistence.bundle.util.BundleBinding.writeBundle(BundleBinding.java:245)
    org.apache.jackrabbit.core.persistence.bundle.Oracle9PersistenceManager.storeBundle(Oracle9PersistenceManager.java:114)
    ^-- Holding lock: org.apache.jackrabbit.core.persistence.bundle.Oracle9PersistenceManager@140f7b9[thin lock]
    org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.putBundle(AbstractBundlePersistenceManager.java:703)
    org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.store(AbstractBundlePersistenceManager.java:526)
    ^-- Holding lock: org.apache.jackrabbit.core.persistence.bundle.Oracle9PersistenceManager@140f7b9[thin lock]
    org.apache.jackrabbit.core.persistence.bundle.BundleDbPersistenceManager.store(BundleDbPersistenceManager.java:517)
    ^-- Holding lock: org.apache.jackrabbit.core.persistence.bundle.Oracle9PersistenceManager@140f7b9[thin lock]
    org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:699)
    org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:873)
    org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:334)
    org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:334)
    org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:307)
    org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:317)
    org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1072)
    ^-- Holding lock: org.apache.jackrabbit.core.XASessionImpl@1f2653b[thin lock]
    org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:895)
    org.apache.jackrabbit.jca.JCASessionHandle.save(JCASessionHandle.java:178)
    com.qpass.inventory.ingestion.IngestionServiceImpl$1.doInJCR(IngestionServiceImpl.java:140)
    com.qpass.inventory.model.JCRTemplate.execute(JCRTemplate.java:17)
    com.qpass.inventory.ingestion.IngestionServiceImpl.ingestProduct(IngestionServiceImpl.java:112)
    ^-- Holding lock: java.lang.Object@849ca9e[fat lock]
    com.qpass.inventory.ingestion.bulk.AbstractBulkIngester.ingestProduct(AbstractBulkIngester.java:42)
    com.qpass.inventory.ingestion.bulk.ZipFileBulkIngester.processFile(ZipFileBulkIngester.java:35)
    com.qpass.inventory.ingestion.bulk.IngestionWorker.processFile(IngestionWorker.java:26)
    com.qpass.inventory.ingestion.bulk.IngestionWorker$1.run(IngestionWorker.java:64)
    org.springframework.scheduling.commonj.DelegatingWork.run(DelegatingWork.java:61)
    weblogic.work.j2ee.J2EEWorkManager$WorkWithListener.run(J2EEWorkManager.java:245)
    weblogic.work.ExecuteThread.execute(ExecuteThread.java:206)
    weblogic.work.ExecuteThread.run(ExecuteThread.java:173)
"
1,"RMI: Property.getValue() fails with EOFException after many reads. When reading binary properties via RMI it can happen that it fails throwing an EOFException. This is caused by a server sided ""Too many open files"" bacause BinaryValue.writeObject() does not close the underlying value InputStream."
1,"benchmark cannot parse highlight-vs-vector-highlight.alg, but only on 3.x?!. A new test (TestPerfTasksParse.testParseExamples) was added in LUCENE-3768 that 
guarantees all .alg files in the conf/ directory can actually be parsed...

But highlight-vs-vector-highlight.alg cannot be parsed on 3.x (NumberFormatException), 
however it works fine on trunk... and the .alg is exactly the same in both cases.

{noformat}
    [junit] ------------- Standard Error -----------------
    [junit] java.lang.NumberFormatException: For input string: ""maxFrags[3.0],fields[body]""
    [junit] 	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1222)
    [junit] 	at java.lang.Float.parseFloat(Float.java:422)
    [junit] 	at org.apache.lucene.benchmark.byTask.tasks.SearchTravTask.setParams(SearchTravTask.java:76)
    [junit] 	at org.apache.lucene.benchmark.byTask.tasks.SearchTravRetVectorHighlightTask.setParams(SearchTravRetVectorHighlightTask.java:124)
    [junit] 	at org.apache.lucene.benchmark.byTask.utils.Algorithm.<init>(Algorithm.java:112)
    [junit] 	at org.apache.lucene.benchmark.byTask.TestPerfTasksParse.testParseExamples(TestPerfTasksParse.java:132)
{noformat}
"
1,"FileSwitchDirectory should uniqueify the String file names returned by listAll. Right now we blindly concatenate what's returned from primary & secondary.

But a legit use of FSD is pointing to the same underlying FSDir but w/ different impls for opening the inputs/outputs.

I have simple patch that just uniqueifies using Set<String>."
1,"File leak when IOException occurs during index optimization.. I am not sure if this issue requires a fix due to the nature of its occurrence, or if it exists in other versions of Lucene.

I am using Lucene Java 3.0.3 on a SUSE Linux machine with Java 6 and have noticed there are a number of file handles that are not being released from my java application. There are IOExceptions in my log regarding disk full, which causes a merge and the optimization to fail. The index is not currupt upon encountering the IOException. I am using CFS for my index format, so 3X my largest index size during optimization certainly consumes all of my available disk. 

I realize that I need to add more disk space to my machine, but I investigated how to clean up the leaking file handles. After failing to find a misuse of Lucene's IndexWriter in the code I have wrapping Lucene, I did a quick search for close() being invoked in the Lucene Jave source code. I found a number of source files that attempt to close more than one object within the same close() method. I think a try/catch should be put around each of these close() attempts to avoid skipping a subsequent closes. The catch may be able to ignore a caught exception to avoid masking the original exception like done in SimpleFSDirectory.close().

Locations in Lucene Java source where I suggest a try/catch should be used:
- org.apache.lucene.index.FormatPostingFieldsWriter.finish()
- org.apache.lucene.index.TermInfosWriter.close()
- org.apache.lucene.index.SegmentTermPositions.close()
- org.apache.lucene.index.SegmentMergeInfo.close()
- org.apache.lucene.index.SegmentMerger.mergeTerms() (The finally block)
- org.apache.lucene.index.DirectoryReader.close()
- org.apache.lucene.index.FieldsReader.close()
- org.apache.lucene.index.MultiLevelSkipListReader.close()
- org.apache.lucene.index.MultipleTermPositions.close()
- org.apache.lucene.index.SegmentMergeQueue.close()
- org.apache.lucene.index.SegmentMergeDocs.close()
- org.apache.lucene.index.TermInfosReader.close()"
1,"Duplicate hits and missing hits in sorted search. If using a searcher that subclasses from IndexSearcher I get different result sets (besides the ordering of course). The problem only occurrs if the searcher is wrapped by (Parallel)MultiSearcher and the index is not too small. The number of hits returned by un unsorted and a sorted search are identical but the hits are referencing different documents. A closer look at the result sets revealed that the sorted search returns duplicate hits.

I created test cases for Lucene 1.4.3 as well as for the head release. The problem showed up for both, the number of duplicates beeing bigger for the head realease. The test cases are written for package org.apache.lucene.search. There are messages describing the problem written to the console. In order to see all those hints the asserts are commented out. So dont't be confused if junit reports no errors. (Sorry, beeing a novice user of the bug tracker I don't see any means to attach the test cases on this screen. Let's see.)"
1,"Extend mimetype list of text extractors. Do you think it would be possible to extend the mimetype list of the
MsPowerpoint and MsExcel textextractors with ""application/powerpoint"" and
""application/excel""? 

It just took me half an hour to figure out why my
documents didn't turn up in a jackrabbit fulltext-search and maybe other
users might run into the same problem...

I'm not sure if there is some kind of standard which lists the possible
default mimetypes but after a quick google search it seems to me that they
are not that uncommon.
"
1,3.1 fileformats out of date. The 3.1 fileformats is missing the change from LUCENE-2811
1,"Auto Reconnect for RMI Repository. If i bind the RepositoryAccessServlet to a RMI Repository and then reboot the Repository i get a
NullpointerException. 
Stack :
java.lang.NullPointerException
	at org.apache.jackrabbit.webdav.jcr.JcrDavException.<init>(JcrDavException.java:111)
	at org.apache.jackrabbit.webdav.simple.DavSessionProviderImpl.attachSession(DavSessionProviderImpl.java:99)
	at org.apache.jackrabbit.server.AbstractWebdavServlet.service(AbstractWebdavServlet.java:181)

If i deploy jackrabbit in a Model 3 Environment this Situation can happen very often.
thanks
claus"
1,"NPE in spi2dav when server does not send all headers. The ValueLoader may throw a NPE if the desired headers are not present in the response:

org.apache.jackrabbit.spi2davex.ValueLoader:

    public Map<String, String> loadHeaders(String uri, String[] headerNames) throws IOException, RepositoryException {
    ....
                for (String name : headerNames) {
--->                headers.put(name, method.getResponseHeader(name).getValue());
                }
    .....
    }

In my case, the server does not return the ETag response header, but the 'loadHeaders' is indirectly called by the QValueFactoryImpl:

                        this.preInitialize(new String[] {HEADER_ETAG, HEADER_LAST_MODIFIED});
"
1,"WorkspaceInfo.dispose() does not deregister SharedItemStateManager from virtual item state providers. Automatic disposal of idle workspaces frees unused workspaces but corresponding SharedItemStateManager (and releated PersistenceManager) is still kept in memory referenced by virtual item state providers,  this can lead to memory leaks."
1,Disallow unregistering of node types still (possibly) in use. 
1,"NullPointerException in ServerRow. A NullPointerException occurs in ServerRow.getValues() when the underlying Value array contains a null reference. See http://www.nabble.com/exception-after-calling-webdav-search-command-tf2826750.html for the details.

java.lang.NullPointerException
     at org.apache.jackrabbit.rmi.value.StatefulValueAdapter.getType(StatefulValueAdapter.java:98)
     at org.apache.jackrabbit.rmi.value.SerialValue.<init>(SerialValue.java:65)
     at org.apache.jackrabbit.rmi.value.SerialValueFactory.makeSerialValue(SerialValueFactory.java:100)
     at org.apache.jackrabbit.rmi.value.SerialValueFactory.makeSerialValueArray(SerialValueFactory.java:77)
     at org.apache.jackrabbit.rmi.server.ServerRow.getValues(ServerRow.java:58)

The best solution would be to explicitly handle nulls in SerialValueFactory.makeSerialValue()."
1,"MMapDirectory is missing newly added openInput method to FSDirectory. This issue was caused by the optimizations in LUCENE-888.  The new
openInput(String name, int bufferSize) added to FSDirectory was not
also overridden by MMapDirectory.
"
1,"InvalidItemStateException when attempting concurrent, non conflicting writes. I'm having some problems doing concurrent addition of nodes to a parent node in Jackrabbit.  I've attached a simple test which starts up a bunch of threads which add nodes to a parent node concurrently. If I add in locks I can get this to work, however according to the mailing list this should work without locks. However, the test always fails with this:

javax.jcr.InvalidItemStateException: Item cannot be saved because it has been modified externally: node /testParent
	at org.apache.jackrabbit.core.ItemImpl.getTransientStates(ItemImpl.java:281)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:939)
	at org.mule.galaxy.impl.JackrabbitConcurrentWriteTest$1.run(JackrabbitConcurrentWriteTest.java:71)

I'm using Jackrabbit 1.6.1. Here is my (verbose) node type:

  <nodeType name=""galaxy:noSiblings"" 
    isMixin=""false"" 
    hasOrderableChildNodes=""false""
    primaryItemName="""">
    <propertyDefinition name=""*"" requiredType=""undefined"" onParentVersion=""COPY"" />
    <propertyDefinition name=""*"" requiredType=""undefined"" onParentVersion=""COPY"" multiple=""true""/>
    <childNodeDefinition name=""*"" defaultPrimaryType=""nt:unstructured"" onParentVersion=""COPY"" sameNameSiblings=""false"" />
    <supertypes>
        <supertype>nt:base</supertype>
        <supertype>mix:referenceable</supertype>
        <supertype>mix:lockable</supertype>
    </supertypes>
  </nodeType>

And my test:    

package org.mule.galaxy.impl;

import java.io.File;
import java.io.IOException;
import java.io.InputStream;
import java.util.ArrayList;
import java.util.List;
import java.util.UUID;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.TimeUnit;

import javax.jcr.LoginException;
import javax.jcr.Node;
import javax.jcr.Repository;
import javax.jcr.RepositoryException;
import javax.jcr.Session;
import javax.jcr.SimpleCredentials;

import junit.framework.TestCase;

import org.apache.commons.io.FileUtils;
import org.apache.jackrabbit.api.JackrabbitNodeTypeManager;
import org.apache.jackrabbit.core.RepositoryImpl;
import org.apache.jackrabbit.core.TransientRepository;
import org.apache.jackrabbit.core.config.RepositoryConfig;

public class JackrabbitConcurrentWriteTest extends TestCase {
    
    private Repository repository;
    private Session session;
    private String parentUUID;
    private boolean continueLoop = true;
    
    public void setUp() throws Exception {
        FileUtils.deleteDirectory(new File(""repository""));
        File repoDir = new File(""repository"");
        repoDir.mkdirs();
        RepositoryConfig config = RepositoryConfig.create(new File(""src/test/resources/META-INF/jackrabbit-repo-test.xml""), repoDir);
        repository = RepositoryImpl.create(config);
        session = createSession();
        
        createCustomNodeTypes(session);
        
        parentUUID = session.getRootNode().addNode(""testParent"", ""galaxy:noSiblings"").getUUID();
        session.save();
        session.logout();
    }

    private Session createSession() throws LoginException, RepositoryException {
        return repository.login(new SimpleCredentials(""username"", ""password"".toCharArray()));
    }
    
    public void testConcurrency() throws Exception {
        final List<Exception> exceptions = new ArrayList<Exception>();
        int threadCount = 20;
        final CountDownLatch latch = new CountDownLatch(threadCount);
        
        for (int i = 0; i < threadCount; i++) {
            Thread thread = new Thread() {

                @Override
                public void run() {
                    try {
                        while (continueLoop) {
                            Session session = createSession();
                            try {
                                Node node = session.getNodeByUUID(parentUUID);
                                node.addNode(UUID.randomUUID().toString());
                                node.save();
                                session.save();
                            } finally {
                                session.logout();
                            }   
                        }
                    } catch (RepositoryException e) {
                        exceptions.add(e);
                        continueLoop = false;
                    }
                    latch.countDown();
                }
                
            };
            thread.start();
        }
        
        latch.await(10, TimeUnit.SECONDS);
        continueLoop = false;
        
        for (Exception e : exceptions) {
            e.printStackTrace();
        }
        assertEquals(0, exceptions.size());
    }
    
    public void createCustomNodeTypes(Session session) throws RepositoryException, IOException {
        // Get the JackrabbitNodeTypeManager from the Workspace.
        // Note that it must be cast from the generic JCR NodeTypeManager to
        // the Jackrabbit-specific implementation.
        // (see: http://jackrabbit.apache.org/node-types.html)
        JackrabbitNodeTypeManager manager = (JackrabbitNodeTypeManager) session.getWorkspace().getNodeTypeManager();

        // Register the custom node types defined in the CND file
        InputStream is = Thread.currentThread().getContextClassLoader()
                .getResourceAsStream(""org/mule/galaxy/impl/jcr/nodeTypes.xml"");
        manager.registerNodeTypes(is, JackrabbitNodeTypeManager.TEXT_XML);
    }
}"
1,"automaton termsenum bug when running with multithreaded search. This one popped in hudson (with a test that runs the same query against fieldcache, and with a filter rewrite, and compares results)

However, its actually worse and unrelated to the fieldcache: you can set both to filter rewrite and it will still fail.
"
1,"Query Builder and jcr:deref problem. Can't add predicate after jcr:deref. Cannot add a predicate (like [@property = 'value'] after a jcr:deref function.
The query builder throws an ""InvalidQueryException: Unsupported location for jcr:deref()"".

So for example, the query :

//element(*,nt:category)[@member]/jcr:deref(@member, '*')[@property='value'] 

is invalid and it should be valid.

"
1,"SSLSocketFactory.connectSocket(...) possible NPE.     public Socket connectSocket(
            final Socket sock,
            final InetSocketAddress remoteAddress,
            final InetSocketAddress localAddress,
            final HttpParams params) throws IOException, UnknownHostException, ConnectTimeoutException {
...

        SSLSocket sslsock = (SSLSocket) (sock != null ? sock : createSocket()); // ==> sock may be null
        if (localAddress != null) {
            sock.setReuseAddress(HttpConnectionParams.getSoReuseaddr(params)); // ==> NPE if sock is null
            sslsock.bind(localAddress);
        }

Should sock.setReuseAddress be sslsock.setReuseAddress?
"
1,"XML serialization in JDK 1.4 broken (mostly for WebDAV). WebDAV uses XmlRequestEntity for serializing XML, which in turn uses org.apache.jackrabbit.commons.xml.SerializingContentHandler to work around the JDK 1.4 problem (serializing in absence of explicit namespace declarations).

The following test fails under JDK 1.4, but passed with newer JDKs:

    public void testXmlSerialization() throws ParserConfigurationException, IOException, SAXException {
        
        DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();
        dbf.setNamespaceAware(true);
        DocumentBuilder db = dbf.newDocumentBuilder();
        
        Document doc = db.newDocument();
        doc.appendChild(doc.createElementNS(""DAV:"", ""propfind""));
        
        XmlRequestEntity xmlent = new XmlRequestEntity(doc);
        ByteArrayOutputStream bos = new ByteArrayOutputStream();
        xmlent.writeRequest(bos);
        
        Document doc2 = db.parse(new ByteArrayInputStream(bos.toByteArray()));
        Element docelem = doc2.getDocumentElement();
        assertEquals(""DAV:"", docelem.getNamespaceURI());
    }"
1,"NPE in RepositoryServiceImpl.getPropertyInfo(). under unknown conditions, i get a NPE in get property info, such as the 'getValue()' of the getstring dav property is null:

            } else if (props.contains(JCR_GET_STRING)) {
                // single valued non-binary property
                String str = props.get(JCR_GET_STRING).getValue().toString();
                QValue qValue = ValueFormat.getQValue(str, propertyType, getNamePathResolver(sessionInfo), getQValueFactory(sessionInfo));
                return new PropertyInfoImpl(propertyId, p, propertyType, qValue);
            } else {

the other properties in the propset are:
 - getstring: null
 - type: String
 - length: 0

the property in question is the last property of a node and it's an empty string. the error only occurs on certain usage patterns, but consistently. maybe depending on the fetch-depth or internal cache.

extending the check to:
            } else if (props.contains(JCR_GET_STRING) && props.get(JCR_GET_STRING).getValue() != null) {

solves the problem.
 
"
1,"o.a.j.core.integration.PrepareTestRepository fails on 2nd and every subsequent invocation. console output: 

-------------------------------------------------------------------------------
Test set: org.apache.jackrabbit.core.integration.PrepareTestRepository
-------------------------------------------------------------------------------
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 3.428 sec <<< FAILURE!
testPrepareTestRepository(org.apache.jackrabbit.core.integration.PrepareTestRepository)  Time elapsed: 3.397 sec  <<< ERROR!
javax.jcr.RepositoryException: Invalid node type definition: {http://www.apache.org/jackrabbit/test}versionable already exists: {http://www.apache.org/jackrabbit/test}versionable already exists
	at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.registerNodeTypes(NodeTypeManagerImpl.java:308)
	at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.registerNodeTypes(NodeTypeManagerImpl.java:488)
	at org.apache.jackrabbit.core.integration.PrepareTestRepository.testPrepareTestRepository(PrepareTestRepository.java:49)
"
1,"Unreferenced sessions should get garbage collected. If an application opens many sessions and doesn't close them, they are never garbage collected. After some time, the virtual machine will run out of memory. This code will run out of memory after a few thousand logins:

Repository rep = new TransientRepository();
for (int i = 0; ; i++) {
  rep.login(new SimpleCredentials("""", new char[0]));
}

Using a finalizer to close SessionImpl doesn't work, because it seems there are references from the (hard referenced part of the cache) to the SessionImpl objects. Maybe it is possible to remove those references, or change them to weak references.
"
1,"import of multivalue properties with single value results in incorrect property creation. When importing a file exported with system view, a value of a multivalued property is stored as a singlevalue property. The bug seems to be that for some reason, even if PropDef.isMultiple() is true for a given property, no ValueFormatException is thrown when setting the property as single value.

Workaround:

It works if I change PropInfo.apply() line 136 to 

if (va.length == 1 && !def.isMultiple()) {
...

"
1,"Binary throws NullPointerException . Precondition: repository with datastore disabled!

Steps to reproduce:

1) create binary from stream
2) set binary on property
3) dispose binary
4) get binary from property and dispose it immediately
5) go to 4)

Binary.dispose() will throw a NullPointerException when 4) is executed the second time.

The exception is not thrown if the property is saved after 2).

See also attached test."
1,"AnalyzingQueryParser can't work with leading wildcards.. The getWildcardQuery mehtod in AnalyzingQueryParser.java need the following changes to accept leading wildcards:

	protected Query getWildcardQuery(String field, String termStr) throws ParseException
	{
		String useTermStr = termStr;
		String leadingWildcard = null;
		if (""*"".equals(field))
		{
			if (""*"".equals(useTermStr))
				return new MatchAllDocsQuery();
		}
		boolean hasLeadingWildcard = (useTermStr.startsWith(""*"") || useTermStr.startsWith(""?"")) ? true : false;

		if (!getAllowLeadingWildcard() && hasLeadingWildcard)
			throw new ParseException(""'*' or '?' not allowed as first character in WildcardQuery"");

		if (getLowercaseExpandedTerms())
		{
			useTermStr = useTermStr.toLowerCase();
		}

		if (hasLeadingWildcard)
		{
			leadingWildcard = useTermStr.substring(0, 1);
			useTermStr = useTermStr.substring(1);
		}

		List tlist = new ArrayList();
		List wlist = new ArrayList();
		/*
		 * somewhat a hack: find/store wildcard chars in order to put them back
		 * after analyzing
		 */
		boolean isWithinToken = (!useTermStr.startsWith(""?"") && !useTermStr.startsWith(""*""));
		isWithinToken = true;
		StringBuffer tmpBuffer = new StringBuffer();
		char[] chars = useTermStr.toCharArray();
		for (int i = 0; i < useTermStr.length(); i++)
		{
			if (chars[i] == '?' || chars[i] == '*')
			{
				if (isWithinToken)
				{
					tlist.add(tmpBuffer.toString());
					tmpBuffer.setLength(0);
				}
				isWithinToken = false;
			}
			else
			{
				if (!isWithinToken)
				{
					wlist.add(tmpBuffer.toString());
					tmpBuffer.setLength(0);
				}
				isWithinToken = true;
			}
			tmpBuffer.append(chars[i]);
		}
		if (isWithinToken)
		{
			tlist.add(tmpBuffer.toString());
		}
		else
		{
			wlist.add(tmpBuffer.toString());
		}

		// get Analyzer from superclass and tokenize the term
		TokenStream source = getAnalyzer().tokenStream(field, new StringReader(useTermStr));
		org.apache.lucene.analysis.Token t;

		int countTokens = 0;
		while (true)
		{
			try
			{
				t = source.next();
			}
			catch (IOException e)
			{
				t = null;
			}
			if (t == null)
			{
				break;
			}
			if (!"""".equals(t.termText()))
			{
				try
				{
					tlist.set(countTokens++, t.termText());
				}
				catch (IndexOutOfBoundsException ioobe)
				{
					countTokens = -1;
				}
			}
		}
		try
		{
			source.close();
		}
		catch (IOException e)
		{
			// ignore
		}

		if (countTokens != tlist.size())
		{
			/*
			 * this means that the analyzer used either added or consumed
			 * (common for a stemmer) tokens, and we can't build a WildcardQuery
			 */
			throw new ParseException(""Cannot build WildcardQuery with analyzer "" + getAnalyzer().getClass()
					+ "" - tokens added or lost"");
		}

		if (tlist.size() == 0)
		{
			return null;
		}
		else if (tlist.size() == 1)
		{
			if (wlist.size() == 1)
			{
				/*
				 * if wlist contains one wildcard, it must be at the end,
				 * because: 1) wildcards at 1st position of a term by
				 * QueryParser where truncated 2) if wildcard was *not* in end,
				 * there would be *two* or more tokens
				 */
				StringBuffer sb = new StringBuffer();
				if (hasLeadingWildcard)
				{
					// adding leadingWildcard
					sb.append(leadingWildcard);
				}
				sb.append((String) tlist.get(0));
				sb.append(wlist.get(0).toString());
				return super.getWildcardQuery(field, sb.toString());
			}
			else if (wlist.size() == 0 && hasLeadingWildcard)
			{
				/*
				 * if wlist contains no wildcard, it must be at 1st position
				 */
				StringBuffer sb = new StringBuffer();
				if (hasLeadingWildcard)
				{
					// adding leadingWildcard
					sb.append(leadingWildcard);
				}
				sb.append((String) tlist.get(0));
				sb.append(wlist.get(0).toString());
				return super.getWildcardQuery(field, sb.toString());
			}
			else
			{
				/*
				 * we should never get here! if so, this method was called with
				 * a termStr containing no wildcard ...
				 */
				throw new IllegalArgumentException(""getWildcardQuery called without wildcard"");
			}
		}
		else
		{
			/*
			 * the term was tokenized, let's rebuild to one token with wildcards
			 * put back in postion
			 */
			StringBuffer sb = new StringBuffer();
			if (hasLeadingWildcard)
			{
				// adding leadingWildcard
				sb.append(leadingWildcard);
			}
			for (int i = 0; i < tlist.size(); i++)
			{
				sb.append((String) tlist.get(i));
				if (wlist != null && wlist.size() > i)
				{
					sb.append((String) wlist.get(i));
				}
			}
			return super.getWildcardQuery(field, sb.toString());
		}
	}
"
1,"MockRandomCodec loads termsIndex even if termsIndexDivisor is set to -1. When working on LUCENE-2891 (on trunk), I found out that if MockRandomCodec is used, then setting IWC.readerTermsIndexDivisor to -1 allows seeking e.g., termDocs, when it shouldn't. Other Codecs fail to seek, as expected by the test. We need to find out why MockRandomCodec does not fail as expected.

To verify that, run ""ant test-core -Dtestcase=TestIndexWriterReader -Dtestmethod=testNoTermsIndex -Dtests.codec=MockRandom"", but comment out the line which adds MockRandom to the list of illegal codecs in the test."
1,"QueryParser.getFieldQuery(String,String) doesn't set default slop on MultiPhraseQuery. there seems to have been an oversight in calling mph.setSlop(phraseSlop) in QueryParser.getFieldQuery(String,String).  The result being that in some cases, the ""default slop"" value doesnt' get set right (sometimes, ... see below).

when i tried amending TestMultiAnalyzer to demonstrate the problem, I discovered that the grammer aparently always calls getFieldQuery(String,String,int) -- even if no ""~slop"" was specified in the text being parsed, in which case it passes the default as if it were specified.
(just to clarify: i haven't comfirmed this from a detailed reading of the grammer/code, it's just what i've deduced based on observation of the test)

The problem isn't entirely obvious unless you have a subclasses of QueryParser and try to call getFieldQuery(String,String) directly.   

In my case, I had overridden getFieldQuery(String,String) to call super.getFieldQuery(String,String) and wrap the result in a DisjunctionMaxQuery ... I don't care about supporting the ~slop syntax, but i do care about the default slop and i wasn't getting lucky the way QueryParser does, because getFieldQuery(String,String,int) wasn't getting back something it could call setSlop() with the (default) value it got from the javacc generated code.

My description may not make much sense, but hopefull the test patch i'm about to attach will.  The fix is also in the patch, and is fairly trivial.

(disclaimer: i don't have javacc installed, so I tested this patch by manually making the change to both QueryParser.java ... it should only be commited by someone with javacc who can regen the java file and confirm that my jj change doesn't have some weird bug in it)



"
1,"Embedded Derby fails under JBoss because of JMX-related conflicts. JBoss fails to start due to a bug in Derby-10.4.2.0. The dependency should be agains derby-10.4.2.1 which seems to has this bug fixed. More info at https://issues.apache.org/jira/browse/DERBY-3887

Please, include this fix in the upcoming 1.6.3"
1,"Highlighter throws StringIndexOutOfBoundsException. Using the canonical Solr example (ant run-example) I added this document (using exampledocs/post.sh):

<add><doc>
  <field name=""id"">Test for Highlighting StringIndexOutOfBoundsExcdption</field>
  <field name=""name"">Some Name</field>
  <field name=""manu"">Acme, Inc.</field>
  <field name=""features"">Description of the features, mentioning various things</field>
  <field name=""features"">Features also is multivalued</field>
  <field name=""popularity"">6</field>
  <field name=""inStock"">true</field>
</doc></add>

and then the URL http://localhost:8983/solr/select/?q=features&hl=true&hl.fl=features caused the exception.

I have a patch.  I don't know if it is completely correct, but it avoids this exception.
"
1,NPE in ConsolidatingChangeLog for id base NodeId. ConsolidatingChangeLog does not guard guard against null in the path value of a NodeId. 
1,"save() might create new transient properties. It seems that when a new node is saved through the parent node, new properties might get created, which are not saved. To persist those properties the new node must be saved again.

Example:

(Consider a mixin type ""extVer"" extending the standard type mix:versionable.)

      Node node = parent.addNode(""newNode"", ""nt:base"");
      node.addMixin(""extVer"");
      // ""mix:versionable"" properties do not exist here
      
      // save the new node
      parent.save();

      // now ""mix:versionable"" properties like ""jcr:isCheckedOut""
      // exist in the ""node"" but:
      //    node.getProperty(""jcr:isCheckedOut"").isNew() == true
      // fix:
      node.save();

If the last node.save() opertation would not be done, a RepositoryException would result if a node.checkIn() would be done immediately after parent.save().

This seems counterintuitive and seems like an error. I wonder whether the properties should not be added upon ""node.addMixin"" ? At least ""parent.save()"" should (or might I say must ?) not only add the properties but also save them."
1,"BLOBFileValue.read(byte[] b, long pos) ignores return value of InputStream.skip. InputStream.skip(long n) returns a long, which may be different from the parameter n (possibly lower).
Currently in BLOBFileValue.read(byte[] b, long pos) the return value is ignored."
1,"AbstractLoginModule must not call abort() in commit(). AbstractLoginModule.commit() currently may call abort() when it detects that the login did not succeed. abort() will reset any state in the login module, including state shared between multiple login modules like Principals in the Subject. When there actually are multiple module, this will delete shared state that was set by other login modules. Moreover, the method commit() is only called when the overall authentication succeeded. Thus, it seems strange to call abort() from within commit().
"
1,"Node.setProperty(String, String) does not convert values. when setting the value of a defined property via the Node.setProperty(String, String) method, a ConstraintViolationException is thrown. but the string value should be converted, or a ValueFormatException must be thrown.

"
1,"Invalid node type definitions with test_nodetypes.xml. an attempt to register the node types defined with 
core/src/test/resources/org/apache/jackrabbit/core/nodetype/xml/test_nodetypes.xml
will fail:

- invalid reference constraint
- autocreated prop-def without default values
- invalid required type with autocreated prop-def (undefined is not allowed)
- invalid required primary type (non existing)
- invalid default primary type (non existing)

"
1,"Support updateDocument() with DWPTs. With separate DocumentsWriterPerThreads (DWPT) it can currently happen that the delete part of an updateDocument() is flushed and committed separately from the corresponding new document.

We need to make sure that updateDocument() is always an atomic operation from a IW.commit() and IW.getReader() perspective.  See LUCENE-2324 for more details."
1,"Removal of first version throws javax.jcr.ReferentialIntegrityException. A ReferentialIntegrityException occurs when I delete the first version succeeding the root version. Deleting other versions works fine. Here is the stack:

javax.jcr.ReferentialIntegrityException: Unable to remove version. At least once referenced.
        at org.apache.jackrabbit.core.version.InternalVersionHistoryImpl.removeVersion(InternalVersionHistoryImpl.java:379)
        at org.apache.jackrabbit.core.version.InternalVersionManagerBase.internalRemoveVersion(InternalVersionManagerBase.java:684)
        at org.apache.jackrabbit.core.version.InternalVersionManagerImpl$5.run(InternalVersionManagerImpl.java:495)
        at org.apache.jackrabbit.core.version.InternalVersionManagerImpl$DynamicESCFactory.doSourced(InternalVersionManagerImpl.java:760)
        at org.apache.jackrabbit.core.version.InternalVersionManagerImpl.removeVersion(InternalVersionManagerImpl.java:493)
        at org.apache.jackrabbit.core.version.InternalXAVersionManager.removeVersion(InternalXAVersionManager.java:264)
        at org.apache.jackrabbit.core.version.VersionHistoryImpl.removeVersion(VersionHistoryImpl.java:253)

The code is simple:

VersionHistory vh = session.getWorkspace().getVersionManager().getVersionHistory(path);
vh.removeVersion(version); // where version is the first version succeeding the root version

"
1,"Warning while building DAV:parent-set for root-node resource. the following warning is generated when calculating DAV:parent-set for the resource representing the root-node:

05.12.2008 11:49:50 *WARN * DavResourceImpl: unable to calculate parent set (DavResourceImpl.java, line 955)
javax.jcr.ItemNotFoundException: root node doesn't have a parent
        at org.apache.jackrabbit.core.NodeImpl.getParent(NodeImpl.java:2078)
        at org.apache.jackrabbit.webdav.simple.DavResourceImpl.getParentElements(DavResourceImpl.java:949)
        at org.apache.jackrabbit.webdav.simple.DavResourceImpl.initProperties(DavResourceImpl.java:393)

this could simply be avoided by slightly modifying the method getParentElements (starting at line 937) and adding a test asserting that node.getParent() is not called for the root node.

"
1,"Removed version is not invalidated. when a version is removed, it's internal represenation is not evicted from the cache. this can leed to unexpected behaviours. XATest.removeVersion() tests this. this also happens in a non-transactional environment."
1,EdgeNgrams creates invalid offsets. A user reported this because it was causing his highlighting to throw an error.
1,"Property.setValue(InputStream) closes stream. Currently the Property.setValue(InputStream) - actually all methods setting a property value from an InputStream - method closes the stream when it has completely been read. While this might be a nice-to-have in some situations, it is IMHO not standard behaviour for stream consumers to close the stream when done.

My special use case is unpacking the contents of a ZIP file (ZIPInputStream). After streaming the contents of the first ZIP file entry into a property, the ZIPInputStream is closed by Jackrabbit and the rest of the file cannot be read.

Workaround: Instead of giving the original InputStream to the method, create a FileInputStream wrapper overwriting the close method to do nothing."
1,"cache module produces improperly formatted Warning header when revalidation fails. The warning header currently attached to a stale response by the caching module when validation with the origin server fails is not a properly-formatted Warning header.

http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.46"
1,"Observation tests fail. Path returned by Event.getPath() is wrong. It always returns the path to the parent node connected to the event. That is, if e.g. a node /foo/bar is created the path /foo is returned instead of /foo/bar.

This issue had been introduced with changed from api version 0.14 to 0.15."
1,"Creating and saving a mix:versionable node creates two VersionHistory nodes. Steps:
   - Create a new mix:versionable node
      [ This creates a new VersionHistory node below jcr:persistentVersionStore
        and sets the new node's versionHistory property to the UUID of this
        VersionHistory node. ]
   - Save the session (or alternatively save the parent of the new node)
      [ This creates a new VersionHistory node below jcr:persistentVersionStore
        and sets the node's versionHistory property to the UUID of this
        VersionHistory node. ]

As you can see, you end up with two VersionHistory nodes for the same node, of which the first VersionHistory node is never used again, because the second VersionHistory node is used from now on."
1,"SpellChecker does not work properly on calling indexDictionary after clearIndex. We have to call clearIndex and indexDictionary to rebuild dictionary from fresh. The call to SpellChecker clearIndex() function does not reset the searcher. Hence, when we call indexDictionary after that, many entries that are already in the stale searcher will not be indexed.

Also, I see that IndexReader reader is used for the sole purpose of obtaining the docFreq of a given term in exist() function. This functionality can also be obtained by using just the searcher by calling searcher.docFreq. Thus, can we get away completely with reader and code associated with it like
      if (IndexReader.isLocked(spellIndex)){
	IndexReader.unlock(spellIndex);
      }
and the reader related code in finalize?

"
1,"Document with no term vectors mixed with ones that have term vectors cause EOFException during merge. Another spinoff from here:

  http://www.gossamer-threads.com/lists/lucene/java-dev/53306

Thank you to Andi Vajda for capturing the issue in a compact test!

This is the same logical error from LUCENE-1008, but in this case the
bug is in TermVectorsWriter: we are failing to write the ""0"" field
count to the tvd file when the document has no vectors.  I have a unit
test showing the issue & simple fix.
"
1,"intermittent failure in TestIndexWriter. testExceptionDuringSync . {code}
common.test:

    [mkdir] Created dir: C:\Projects\lucene\trunk-full1\build\test

    [junit] Testsuite: org.apache.lucene.index.TestIndexWriter

    [junit] Tests run: 102, Failures: 0, Errors: 1, Time elapsed: 100,297sec

    [junit]

    [junit] Testcase: testExceptionDuringSync(org.apache.lucene.index.TestIndexWriter): Caused an ERROR

    [junit] _a.fnm

    [junit] java.io.FileNotFoundException: _a.fnm

    [junit]     at org.apache.lucene.store.MockRAMDirectory.openInput(MockRAMDirectory.java:226)

    [junit]     at org.apache.lucene.index.FieldInfos.<init>(FieldInfos.java:68)

    [junit]     at org.apache.lucene.index.SegmentReader$CoreReaders.<init>(SegmentReader.java:116)

    [junit]     at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:620)

    [junit]     at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:590)

    [junit]     at org.apache.lucene.index.DirectoryReader.<init>(DirectoryReader.java:104)

    [junit]     at org.apache.lucene.index.ReadOnlyDirectoryReader.<init>(ReadOnlyDirectoryReader.java:27)

    [junit]     at org.apache.lucene.index.DirectoryReader$1.doBody(DirectoryReader.java:74)

    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:704)

    [junit]     at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:69)

    [junit]     at org.apache.lucene.index.IndexReader.open(IndexReader.java:307)

    [junit]     at org.apache.lucene.index.IndexReader.open(IndexReader.java:193)

    [junit]     at org.apache.lucene.index.TestIndexWriter.testExceptionDuringSync(TestIndexWriter.java:2723)

    [junit]     at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:206)

    [junit]

    [junit]

    [junit] Test org.apache.lucene.index.TestIndexWriter FAILED
{code}"
1,"Concurrent add/remove child node operations in a cluster may corrupt repository.. Concurrent add/remove child node operations in a cluster may store an inconsistent list of child node entries, i.e. an entry in the list may appear that has no associated node. This eventually results in an ItemNotFoundException, the next time one of these bogus entries is accessed."
1,"Typo in the deploy/jboss/4.x/jcr-ds.xml file. 
The datasource descriptor in the jboss 4.x example xml file has a type in this line:

<config-property name=""bindSessionToTrasaction"" type=""java.lang.Boolean"">true</config-property>

bindSessionToTrasaction ought to be bindSessionToTransaction - there is an 'n' missing from Transaction.

Found this on the tagged release in the subversion repo."
1,"PerFieldCodecWrapper causes crashes if not all per field codes have been used. If a PerFieldCodecWrapper is used an SegmentMerger tries to merge two segments where one segment only has a subset of the field PerFieldCodecWrapper defines SegmentMerger tries to open non-existing files since Codec#files(Directory, SegmentInfo, Set<String>) blindly copies the expected files into the given set. This also hits exceptions in CheckIndex and addIndexes(). 
The reason for this is that PerFieldCodecWrapper simply iterates over the codecs it knows and adds all files without checking if they are present in the given Directory. We need to have some mechnanism that check if the ""required"" files for a codec are present and only add the files to the set if that field is really there.

"
1,"Restoring a deleted version does not work (throws Exception). java.lang.IllegalStateException: Not in edit mode
	at org.apache.jackrabbit.core.state.LocalItemStateManager.createNew(LocalItemStateManager.java:258)
	at org.apache.jackrabbit.core.version.NodeStateEx.createChildNode(NodeStateEx.java:561)
	at org.apache.jackrabbit.core.version.NodeStateEx.addNode(NodeStateEx.java:525)
	at org.apache.jackrabbit.core.version.NodeStateEx.addNode(NodeStateEx.java:505)
	at org.apache.jackrabbit.core.version.VersionManagerImplRestore.restore(VersionManagerImplRestore.java:201)
	at org.apache.jackrabbit.core.VersionManagerImpl.restore(VersionManagerImpl.java:240)
	at org.apache.jackrabbit.core.NodeImpl.restore(NodeImpl.java:3379)
	at org.apache.jackrabbit.test.api.version.RestoreTest.testRestoreRemoved(RestoreTest.java:812)"
1,"Path is not indexed when inserting a new node with SNS. Using Jackrabbit OCM, when inserting two nodes with the same path, the second node's path is not indexed. 
Both nodes have the same path, and a search by path retrieves the first node only. 

The node mapping included the following annotations:

@Node(jcrMixinTypes=""mix:referenceable,mix:lockable,mix:versionable"") 
public class Article { 

        @Field(uuid=true) 
        private String id = null; 
        
        @Field(path=true) 
        private String path = null; 

        ....
}"
1,"TestScoredDocIDsUtils.testWithDeletions test failure. ant test -Dtestcase=TestScoredDocIDsUtils -Dtestmethod=testWithDeletions -Dtests.seed=-2216133137948616963:2693740419732273624 -Dtests.multiplier=5

In general, on both 3.x and trunk, if you run this test with -Dtests.iter=100 it tends to fail 2% of the time.

"
1,Scorer.skipTo() doesn't always work if called before next(). skipTo() doesn't work for all scorers if called before next().
1,"Cluster: Node type register/unregister deadlock. A deadlock can occur when two cluster nodes concurrently register or unregister node types.

Reason: 

NodeTypeRegistry.registerNodeTypes is synchronized, and calls eventChannel.registered(ntDefs), which calls AbstractJournal.lockAndSync(), which tries to lock AbstractJournal.rwLock.

On the other hand, AbstractJournal.sync() locks AbstractJournal.rwLock, then calls NodeTypeRecord.process, which calls NodeTypeRegistry.unregisterNodeTypes, which is also synchronized.

Possible solutions: Either 

- NodeTypeRegistry doesn't synchronize on the object when calling a eventChannel method,

- or NodeTypeRegistry locks AbstractJournal.rwLock before synchronizing.

There might be other solutions."
1,Lucene queries are not properly rewritten. Some of the jackrabbit internal lucene queries are not properly rewritten and may lead to UnsupportedOperationException when terms are extracted from the lucene query.
1,queries with zero boosts don't work. Queries consisting of only zero boosts result in incorrect results.
1,"HttpState.clearCookies() should be synchronized. The HttpState class has a clearCookies method that is not synchronized but
should be considering it modifies an ArrayList (which is unsynchronized). All
other methods which modify or read from the ArrayList are synchronized except
the clearCookies method. 

I stumbled upon this fact because a webapp I am working on that uses HttpClient
threw an IllegalArgumentException indicating that one of the cookies in the
array returned from HttpState.getCookies() was null, which shouldn't be
possible.  Upon further inspection and testing, the only possible option is that
the threadsafety hole left by the unsynchronized clearCookies method caused the
issue."
1,"IndexWriter should never pool readers for external segments. EG when addIndexes is called, it enrolls external segment infos, which are then merged.  But merging will simply ask the pool for the readers, and if writer is pooling (NRT reader has been pooled) it incorrectly pools these readers.

It shouldn't break anything but it's a waste because these readers are only used for merging, once, and they are not opened by NRT reader."
1,"NumericUtils.floatToSortableInt/doubleToSortableLong does not sort certain NaN ranges correctly and NumericRangeQuery produces wrong results for NaNs with half-open ranges. The current implementation of floatToSortableInt does not account for different NaN ranges which may result in NaNs sorted before -Infinity and after +Infinity. The default Java ordering is: all NaNs after Infinity.

A possible fix is to make all NaNs canonic ""quiet NaN"" as in:
{code}
// Canonicalize NaN ranges. I assume this check will be faster here than 
// (v == v) == false on the FPU? We don't distinguish between different
// flavors of NaNs here (see http://en.wikipedia.org/wiki/NaN). I guess
// in Java this doesn't matter much anyway.
if ((v & 0x7fffffff) > 0x7f800000) {
  // Apply the logic below to a canonical ""quiet NaN""
  return 0x7fc00000 ^ 0x80000000;
}
{code}

I don't commit because I don't know how much of the existing stuff relies on this (nobody should be keeping different NaNs  in their indexes, but who knows...)."
1,"ocm fails with NPE when a ClassDescriptor isn't found. ObjectConverterImpl#getObject(Session session, Class clazz, String path) should validate whether there's a suitable mapping in order to throw a more descriptive exception as ObjectConverterImpl#getObject(Session session, String path) does.

"
1,Broken handling of outer join results over davex. The davex join support added in JCR-3089 only works correctly when the join returns at least one row and none of the returned rows contain null values for any of the selectors. This should be reasonably straightforward to fix.
1,"InputStream not being explicitly closed. After deploying a j2ee artifact that uses jackrabbit and org.apache.jackrabbit.core.persistence.pool.MySqlPersistenceManager, Glassfish starts complaining there are input streams without being explicitly closed.
The specific inputStream mey be found at org.apache.jackrabbit.core.persistence.pool.BundleDbPersistenceManager.createCheckSchemaOperation(BundleDbPersistenceManager.java:584).

I've checked the code and in BundleDbPersistenceManager:530 the run method is invoked to the object CheckSchemaOperation.
In run method CheckSchemaOperation:78, the finally block that calls IOUtils.closeQuietly(ddl); to close the stream is inside the condition if (!conHelper.tableExists(table)) (CheckSchemaOperation:79).
So, if this condition is false, the inputStream will not be explicitly closed.

In my opinion, there are two fix alternatives:
The most robust should be:
1 - insert a finalize() method:
    @Override
    protected void finalize() throws Throwable {
	    if (ddl!=null){
            IOUtils.closeQuietly(ddl);
	    }
		super.finalize();
	}

Another alternative:
2 - Put the condition if (!conHelper.tableExists(table)) inside try-finally block.


StackTrace:
[#|2011-05-05T11:43:28.087-0300|WARNING|glassfish3.1|javax.enterprise.system.core.classloading.com.sun.enterprise.loader|_ThreadID=1233;_ThreadName=Thread-1;|Input stream has been finalized or forced closed without being explicitly closed; stream instantiation reported in following stack trace
java.lang.Throwable
        at com.sun.enterprise.loader.ASURLClassLoader$SentinelInputStream.<init>(ASURLClassLoader.java:1230)
        at com.sun.enterprise.loader.ASURLClassLoader$InternalJarURLConnection.getInputStream(ASURLClassLoader.java:1338)
        at java.net.URL.openStream(URL.java:1010)
        at java.lang.ClassLoader.getResourceAsStream(ClassLoader.java:1195)
        at com.sun.enterprise.loader.ASURLClassLoader.getResourceAsStream(ASURLClassLoader.java:872)
        at java.lang.Class.getResourceAsStream(Class.java:2030)
        at org.apache.jackrabbit.core.persistence.pool.BundleDbPersistenceManager.createCheckSchemaOperation(BundleDbPersistenceManager.java:584)
        at org.apache.jackrabbit.core.persistence.pool.BundleDbPersistenceManager.init(BundleDbPersistenceManager.java:530)
        at org.apache.jackrabbit.core.persistence.pool.MySqlPersistenceManager.init(MySqlPersistenceManager.java:51)
        at org.apache.jackrabbit.core.RepositoryImpl.createPersistenceManager(RepositoryImpl.java:1353)"
1,"IndexWriter retains references to Readers used in Fields (memory leak). As described in [1] IndexWriter retains references to Reader used in Fields and that can lead to big memory leaks when using tika's ParsingReaders (as those can take 1MB per ParsingReader). 

[2] shows a screenshot of the reference chain to the Reader from the IndexWriter taken with Eclipse MAT (Memory Analysis Tool) . The chain is the following:

IndexWriter -> DocumentsWriter -> DocumentsWriterThreadState -> DocFieldProcessorPerThread  -> DocFieldProcessorPerField -> Fieldable -> Field (fieldsData) 


-------------
[1] http://markmail.org/thread/ndmcgffg2mnwjo47
[2] http://skitch.com/ecerulm/n7643/eclipse-memory-analyzer

"
1,"Cannot version the root node. After making the root node versionable, the checkin method fails with the following exception. 

java.lang.ArrayIndexOutOfBoundsException: 0
    at org.apache.jackrabbit.core.version.persistence.PersistentNode.copyFrom(PersistentNode.java:589)
    at org.apache.jackrabbit.core.version.persistence.InternalFrozenNodeImpl.checkin(InternalFrozenNodeImpl.java:277)
    at org.apache.jackrabbit.core.version.persistence.InternalFrozenNodeImpl.checkin(InternalFrozenNodeImpl.java:307)
    at org.apache.jackrabbit.core.version.persistence.InternalFrozenNodeImpl.checkin(InternalFrozenNodeImpl.java:307)
    at org.apache.jackrabbit.core.version.persistence.InternalVersionHistoryImpl.checkin(InternalVersionHistoryImpl.java:354)
    at org.apache.jackrabbit.core.version.persistence.NativePVM.checkin(NativePVM.java:506)
    at org.apache.jackrabbit.core.version.VersionManagerImpl.checkin(VersionManagerImpl.java:212)
    at org.apache.jackrabbit.core.NodeImpl.checkin(NodeImpl.java:2184)
    at com.gtnet.jcr.VersionedNodeTest.testVersionRootNode(VersionedNodeTest.java:218)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at com.intellij.rt.execution.junit2.JUnitStarter.main(JUnitStarter.java:31)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:78)
"
1,"Cached Item could be lost in cache. since items and itemstate are cache in week/soft reference maps, they can disappear from the caches after a GC cycle, and code like this:

if (isCached(id)) {
  return retrieveItem(id);
}

has potential to fail. log entries like:

failed to build path of c2eeecbe-6126-45a2-a38a-002361095107: 3334d748-2790-4004-8bfa-09463624c7c4 has no child entry for 4897c961-f36f-4d46-87bd-f24f152138a4

are the result."
1,"PulsingTermState.clone leaks memory. I looked at the heap dump from the OOME this morning (thank you Uwe
for turning this on!), and I think it's a real memory leak.

Well, not really a leak; rather, the cloned PulsingTermState, which we
cache in the terms dict cache, is hanging onto large byte[]
unnecessarily.
"
1,"Indexing rules inheritance doesn't work. Indexing rules are supposed to be inherited by children node types.
In org.apache.jackrabbit.core.query.lucene.IndexingConfigurationImpl.init, rules are registered for the declared node type and all its children. However, as the rule's node type is still the original one, the rule gets rejected in org.apache.jackrabbit.core.query.lucene.IndexingConfigurationImpl$IndexingRule.appliesTo.

One simple solution would be to register not the original rule, but a copy where the original node type has been replaced by the child one.

Please find corrected class attached.

Sincerely,

Stéphane"
1,"docvalues FNFE. I created a test for LUCENE-3335, and it found an unrelated bug in docvalues."
1,"The getOutputStream of the MemoryFileSystem class can replace a folder with a newly created file. It seems that if the filePath parameter passed to the getOutputStream method of the MemoryFileSystem class points to an  existing folder and not to a file - the folder will be replaced with a newly created file.
The function should probably check whether the passed path points to a file and throw an exception if it points to a folder."
1,"JCR-Server: Allow header misses colong (RootCollection, WorkspaceResourceImpl). List of supported dav methods misses some colons."
1,"HttpConnectionParams.setConnectionTimeout(int) has no effect if host unreachable. I have just modified MultiThreadedExample.java by adding
httpClient.getHttpConnectionManager().getParams().setConnectionTimeout(5000); in
order to set a connection timeout on the client side. Then I have added a LAN
url to urisToGet array. The ip of this url (""http://192.168.254.1/"") is not
assigned to any computer.

After running the client, I get the expected message ( error:
org.apache.commons.httpclient.ConnectTimeoutException: The host did not accept
the connection within timeout of 5000 ms) but only after 20 seconds.

I use java version ""1.5.0_04"". This is not a JVM bug since normal connection
procedure times out after 5 seconds as expected:
        SocketAddress addr = new InetSocketAddress(""192.168.254.1"", 80);
        try {
            
            SocketChannel channel = SocketChannel.open();
            channel.socket().connect(addr, 5000);            
            System.out.println(""connected"");
            
        } catch (Exception e) {
            e.printStackTrace();
        }"
1,"RMIRemoteBindingServlet fails to initialize if the RMI registry is not available. If the RMI registry is not available, the RMIRemoteBindingServlet in jcr-rmi will throw an exception in the init() method and prevent the servlet from being loaded.

The same servlet can however also be mapped to the normal HTTP URL space as an alternative mechanism of making the RMI endpoint available to clients. Thus it would be better if the init() method just logged a warning instead of failing completely."
1,"FileDataStore: garbage collection can delete files that are still needed. It looks like the FileDataStore garbage collection (both regular scan and persistence manager scan) can delete files that are still needed.

Currently it looks like the reason is the last access time resolution of the operating system. This is 2 seconds for FAT and Mac OS X, NTFS 100 ns, and 1 second for other file systems. That means file that are scanned at the very beginning are sometimes deleted, because they have a later last modified time then when the scan was started."
1,"UsernamePasswordCredentials.equals(null) throws NPE. Steps to reproduce:
1. new UsernamePasswordCredentials().equals(null);

Observed:
NullPointerException is thrown

Expected:
equals() returns false"
1,"Index changes are lost if you call prepareCommit() then close(). You are supposed to call commit() after calling prepareCommit(), but... if you forget, and call close() after prepareCommit() without calling commit(), then any changes done after the prepareCommit() are silently lost (including adding/deleting docs, but also any completed merges).

Spinoff from java-user thread ""lots of .cfs (compound files) in the index directory"" from Tim Bogaert.

I think to fix this, IW.close should throw an IllegalStateException if prepareCommit() was called with no matching call to commit()."
1,"Fuzzy query scoring issues. Queries which automatically produce multiple terms (wildcard, range, prefix, 
fuzzy etc)currently suffer from two problems:

1) Scores for matching documents are significantly smaller than term queries 
because of the volume of terms introduced (A match on query Foo~ is 0.1 
whereas a match on query Foo is 1).
2) The rarer forms of expanded terms are favoured over those of more common 
forms because of the IDF. When using Fuzzy queries for example, rare mis-
spellings typically appear in results before the more common correct spellings.


I will attach a patch that corrects the issues identified above by 
1) Overriding Similarity.coord to counteract the downplaying of scores 
introduced by expanding terms.
2) Taking the IDF factor of the most common form of expanded terms as the 
basis of scoring all other expanded terms."
1,Instantiating SimpleFSLockFactory by its String param constructor throws an IllegalStateException. 
1,"[contrib-bdb] initialization fails if directory doesn't exist. BerkeleyDBPersistenceManager initialization fails if the directory configured doesn't exist (this doesn't happen with other PMs).
This can easily be fixed in the persistence manager, by making it create all the directories in the path (actually it only creates the last -db- directory).

The trivial patch is to replace envDir.mkdir() to envDir.mkdirs() (note the final ""s"") at BerkeleyDBPersistenceManager  line 73:
        if (!envDir.exists())
            envDir.mkdir();
should be:
        if (!envDir.exists())
            envDir.mkdirs();

(I am not submitting any svn diff since the manual fix sounds so trivial, it's easier to change it manually)
"
1,"MultiPhraseQuery assigns different scores to identical docs when using 0 pos-incr. If you have two identical docs with tokens a b c all zero pos-incr (ie
they occur on the same position), and you run a MultiPhraseQuery with
[a, b] and [c] (all pos incr 0)... then the two docs will get
different scores despite being identical.

Admittedly it's a strange query... but I think the scorer ought to
count the phrase as having tf=1 for each doc.

The problem is that we are missing a tie-breaker for the PhraseQuery
used by ExactPhraseScorer, and so the PQ ends up flip/flopping such
that every other document gets the same score.  Ie, even docIDs all
get one score and odd docIDs all get another score.

Once I added the hard tie-breaker (ord) the scores are the same.

However... there's a separate bug, that can over-count the tf, such
that if I create the MPQ like this:
{noformat}
  mpq.add(new Term[] {new Term(""field"", ""a"")}, 0);
  mpq.add(new Term[] {new Term(""field"", ""b""), new Term(""field"", ""c"")}, 0);
{noformat}

I get tf=2 per doc, but if I create it like this:

{noformat}
  mpq.add(new Term[] {new Term(""field"", ""b""), new Term(""field"", ""c"")}, 0);
  mpq.add(new Term[] {new Term(""field"", ""a"")}, 0);
{noformat}

I get tf=1 (which I think is correct?).

This happens because MultipleTermPositions freely returns the same
position more than once: it just unions the positions of the two
streams, so when both have their term at pos=0, you'll get pos=0
twice, which is not good and leads to over-counting tf.

Unfortunately, I don't see a performant way to fix that... and I'm not
sure that it really matters that much in practice.




"
1,"Version 1.3 reports IOException when re-creating an index. Version: Lucene 1.3 final 
Error reported when I am (re-)doing an initialization on the index created 
previously:
java.io.IOException: couldn't delete _26a.f1

The problem disappearred after a re-start of the jvm, some files may be locked 
after the index writer action !
Problem does not appear in Version 1.2."
1,"Clustering: race condition may cause duplicate entries in search index. There seems to be a race condition that may cause duplicate search index entries. It is reproducible as follows (Jackrabbit 1.3):
1) Start clusternode 1 that just adds a single node of node type clustering:test.
2) Shutdown clusternode 1.
3) Start clusternode 2 with an empty search index.
4) Execute the query  //element(*, clustering:test).
4) Print the result of the query (UUIDs of nodes in the result set).

When I just run clusternode 2, then there is one node in the resultset, as expected. However, when I debug clusternode 2 and have a breakpoint (i.e., a pause of a few seconds at line 306 of RepositoryImpl.java - just before the clusternode is started), then the resultset contains two results, both with the same UUID.
"
1,"Lock tokens reains in session after unlock. I do the followin steps:

* node.lock()
* session.getLockTokens() -> This show the lock token generated by the
previous lock.
* node.unlock()
* session.getLockTokens() -> Still show me the generated token ¿?¿?

If I unlock a node, the token should'n be delete from the session?"
1,"Version.getReferences() does not work correctly. since the /jcr:system/jcr:versionStorage is shared among all workspaces, referes of versions and version histories need to be workspace sensitive. for example can a workspace W1 contain a versionable node N1. Its respective version history VH is visible in the jcr:versionStorage. calling VH.getReferences() should return the jcr:versionHistory property of that node N1. If accessing the repository using another workspace, W2, which does not have the node N2 (that corresponds to N1), calling VH.getReferences() should return an empty set. The same is true for version nodes referenced by jcr:baseVersion and jcr:predecessors properties.


see also spec chapter 8.2.2.1 (jcr:versionStorage):
The full set of version histories in the version storage, though stored in a single location in the repository, must be reflected in each workspace as a subtree below the node /jcr:system/jcr:versionStorage. 
[...]"
1,Session holds LockToken after removeLockToken in XA Environment. 
1,"Crashes when it gets a redirect. I get the following crash when VFS (not my code) calls HttpClient. This code 
worked with some older version of HttpClient (is my belief) but doesn't appear 
to work with CVS HEAD, hence this posting.

Note: I'm sorry, but I don't know which Method it was calling, but hopefully a 
redirect is a redirect and the bug stands irrespective of that.

This is major to me (and Ruper) 'cos it is the first thing it does before 
attempting to read the contents of that location.

regards,

Adam

java.lang.NullPointerException
	at 
org.apache.commons.httpclient.HttpMethodDirector.processRedirectResponse
(HttpMethodDirector.java:454)
	at org.apache.commons.httpclient.HttpMethodDirector.isRetryNeeded
(HttpMethodDirector.java:639)
	at org.apache.commons.httpclient.HttpMethodDirector.executeMethod
(HttpMethodDirector.java:145)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:378)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:268"
1,"same named child nodes disappear on restore. When restoring a versionable node which has several (non-versionable) child nodes with the same name, some child nodes disappear. 

            Node node = session.getRootNode().addNode(""myNode"");
            node.addMixin(""mix:versionable"");
            for (int i = 1; i < 6; i++) {
                Node child = node.addNode(""child"");
                child.setProperty(""name"", ""child_""+i);
            }
            session.save();
            VersionManager versionManager = session.getWorkspace().getVersionManager();
            versionManager.checkin(node.getPath());
            System.out.println(""number of child nodes: "" + node.getNodes().getSize());

            versionManager.checkout(node.getPath());
            node.getNode(""child"").setProperty(""name"", ""modified"");
            session.save();
            Version baseVersion = versionManager.getBaseVersion(node.getPath());
            versionManager.restore(baseVersion, true);
            System.out.println(""number of child nodes in restored node: ""+node.getNodes().getSize());


produces the following output:

number of child nodes: 5
number of child nodes in restored node: 3

Giving unique names or adding the mixin versionable to the child nodes solves the problem.
"
1,"JCR Server has concurrency issues on JcrWebdavServer.SessionCache internal HashMap caches. After doing the davex remoting performance work outlined in JCR-3026, the increased concurrency on my jcr server exposed a lot of errors related to getting and putting from the JcrWebdavServer.SessionCache's internal HashMap's.  This problem with HashMap's is a well known concurrency error and was easily fixed by upgrading these maps to ConcurrentHashMaps.  Performance seems dramatically better.  

The fix includes exposure of a tuning parameter that allows the user to set the expected concurrency level.  This is the number of concurrent requests you expect the server to be handling.  In the typical davex remoting scenario, this means you should tune this server side value to match the total max connections of all clients pointed at the server.  See JCR-3026. 

USAGE:  Set the 'concurrency-level' init param for the JcrRemotingServlet, via the web.xml of the jackabbit-webapp component.  Default value is 50.  Or you can intervene in a lower level api if appropriate."
1,"DateTools needs to use UTC for correct collation,. If your local timezone is Europe/London then the times Sun, 30 Oct 2005 00:00:00 +0000 and exactly one hour later are both converted to 200530010000 by DateTools.dateToString() with minute resolution.   The Linux date command is useful in seeing why:

    $ date --date ""Sun, 30 Oct 2005 00:00:00 +0000""
    Sun Oct 30 01:00:00 BST 2005

    $ date --date ""Sun, 30 Oct 2005 01:00:00 +0000""
    Sun Oct 30 01:00:00 GMT 2005

Both times are 1am in the morning, but one is when DST is in force, the other isn't.   Of course, these are actually different times!

Of course, if dates are stored in the index with implicit timezone information then not only do we get problems when the clocks go back at the end of summer, but we also have problems crossing timezones.   If a database is created in California and used in Paris then the times are going to be badly skewed (there's a nine hour time difference most of the year).
"
1,"Jcr-Server: ValuesProperty missing property type information. JCR specific dav-property ValuesProperty does not reveal the PropertyType of the value, which is therefore lost during (de)serialization. 

Solution: 
- Pass type of the JCR-value as attribute to the xml-element containing the value."
1,"URI path resolution problems.. URI does not completely conform to RFC 2396.  In particular it does not handle the following 
relative URIs correctly:

../../../g
../../../../g"
1,"IndexReader.setNorms is no op if one of the field instances omits norms. If I add two documents to an index w/ same field, and one of them omit norms, then IndexReader.setNorms is no-op. I'll attach a patch w/ test case"
1,"TestGrouping failure. {noformat}
ant test -Dtestcase=TestGrouping -Dtestmethod=testRandom -Dtests.seed=295cdb78b4a442d4:-4c5d64ef4d698c27:-425d4c1eb87211ba
{noformat}

fails with this on current trunk:

{noformat}

    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestGrouping -Dtestmethod=testRandom -Dtests.seed=295cdb78b4a442d4:-4c5d64ef4d698c27:-425d4c1eb87211ba
    [junit] NOTE: test params are: codec=RandomCodecProvider: {id=MockRandom, content=MockSep, sort2=SimpleText, groupend=Pulsing(freqCutoff=3 minBlockSize=65 maxBlockSize=132), sort1=Memory, group=Memory}, sim=RandomSimilarityProvider(queryNorm=true,coord=false): {id=DFR I(F)L2, content=DFR BeL3(800.0), sort2=DFR GL3(800.0), groupend=DFR G2, sort1=DFR GB3(800.0), group=LM Jelinek-Mercer(0.700000)}, locale=zh_TW, timezone=America/Indiana/Indianapolis
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestGrouping]
    [junit] NOTE: Linux 2.6.33.6-147.fc13.x86_64 amd64/Sun Microsystems Inc. 1.6.0_21 (64-bit)/cpus=24,threads=1,free=143246344,total=281804800
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testRandom(org.apache.lucene.search.grouping.TestGrouping):	FAILED
    [junit] expected:<11> but was:<7>
    [junit] junit.framework.AssertionFailedError: expected:<11> but was:<7>
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:148)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:50)
    [junit] 	at org.apache.lucene.search.grouping.TestGrouping.assertEquals(TestGrouping.java:980)
    [junit] 	at org.apache.lucene.search.grouping.TestGrouping.testRandom(TestGrouping.java:865)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$2$1.evaluate(LuceneTestCase.java:611)
    [junit] 
    [junit] 
{noformat}

I dug for a while... the test is a bit sneaky because it compares sorted docs (by score) across 2 indexes.  Index #1 has no deletions; Index #2 has same docs, but organized into doc blocks by group, and has some deletions.  In theory (I think) even though the deletions will cause scores to differ across the two indices, it should not alter the sort order of the docs.  Here is the explain output of the docs that sorted differently:

{noformat}
#1: top hit in the ""has deletes doc-block"" index (id=239):

explain: 2.394486 = (MATCH) weight(content:real1 in 292)
[DFRSimilarity], result of:
 2.394486 = score(DFRSimilarity, doc=292, freq=1.0), computed from:
   1.0 = termFreq=1
   41.944084 = NormalizationH3, computed from:
     1.0 = tf
     5.3102274 = avgFieldLength
     2.56 = len
   102.829 = BasicModelBE, computed from:
     41.944084 = tfn
     880.0 = numberOfDocuments
     239.0 = totalTermFreq
   0.023286095 = AfterEffectL, computed from:
     41.944084 = tfn


#2: hit in the ""no deletes normal index"" (id=229)

ID=229 explain=2.382285 = (MATCH) weight(content:real1 in 225)
[DFRSimilarity], result of:
 2.382285 = score(DFRSimilarity, doc=225, freq=1.0), computed from:
   1.0 = termFreq=1
   41.765594 = NormalizationH3, computed from:
     1.0 = tf
     5.3218827 = avgFieldLength
     10.24 = len
   101.879845 = BasicModelBE, computed from:
     41.765594 = tfn
     786.0 = numberOfDocuments
     215.0 = totalTermFreq
   0.023383282 = AfterEffectL, computed from:
     41.765594 = tfn

Then I went and called explain on the ""no deletes normal index"" for
the top doc (id=239):

explain: 2.3822558 = (MATCH) weight(content:real1 in 17)
[DFRSimilarity], result of:
 2.3822558 = score(DFRSimilarity, doc=17, freq=1.0), computed from:
   1.0 = termFreq=1
   42.165264 = NormalizationH3, computed from:
     1.0 = tf
     5.3218827 = avgFieldLength
     2.56 = len
   102.8307 = BasicModelBE, computed from:
     42.165264 = tfn
     786.0 = numberOfDocuments
     215.0 = totalTermFreq
   0.023166776 = AfterEffectL, computed from:
     42.165264 = tfn
{noformat}"
1,"If you pass Integer.MAX_VALUE as 2nd param to search(Query, int) you hit unexpected NegativeArraySizeException. Note that this is a nonsense value to pass in, since our PQ impl allocates the array up front.

It's because PQ takes 1+ this value (which wraps to -1), and attempts to allocate that.  We should bounds check it, and drop PQ size by one in this case.

Better, maybe: in IndexSearcher, if that n is ever > maxDoc(), set it to maxDoc().

This trips users up fairly often because they assume our PQ doesn't statically pre-allocate (a reasonable assumption...)."
1,"PathFactoryImpl creates illegal Path objects. it is currently possible to create illegal/inconsistent paths using the default path factory.
Path objects are expected to represent syntactically correct paths.

some examples:

            PathFactory pf = PathFactoryImpl.getInstance();
            Path.Element re = pf.getRootElement();
            Path illegalPath = pf.create(new Path.Element[]{re, re});
            
            Path.Element pe = pf.getParentElement();
            Path nonNormalizedPath = pf.create(new Path.Element[]{pe, pe});    // ""../..""
            assertFalse(nonNormalizedPath.isNormalized());

"
1,"fix assertions/checks that use File.length() to use getFilePointer(). This came up on this thread ""Getting RuntimeException: after flush: fdx size mismatch while Indexing"" 
(http://www.lucidimagination.com/search/document/a8db01a220f0a126)

In trunk, a side effect of the codec refactoring is that these assertions were pushed into codecs as finish() before close().
they check getFilePointer() instead in this computation, which checks that lucene did its part (instead of falsely tripping if directory metadata is stale).

I think we should fix these checks/asserts on 3.x too
"
1,"Security of token base authentication. Token based authentication as implemented with JCR-2851 seems to exhibit a security issue: the token returned by the server consists of the identifier of a (newly created) node in the repository. An attacker who is able to guess (or acquire by other means i.e. via log files) that identifier will be granted access to the repository. Worse yet, JCR-2857 introduces sequential node ids. Guessing is a piece of cake in such a setup.

I think we should decouple authentication secrets from node ids. A simple solution would be to store the secret in a token attribute and delegate generation of the secret to a dedicated handler. Such a handler can then use a secure random generator, private/public key encryption or whatever other method that is deemed appropriate to generate the authentication secret. 

Initial discussion see: http://markmail.org/thread/aspetgvmj2qud25a"
1,"Workspace.clone throws ItemNotFoundException on a referenceable node with children. An ItemNotFoundException is thrown when a referenceable node with children is cloned, this happens after the first time the node is cloned.
            
Example:

            Node root = session.getRootNode();   
            Node parent = root.addNode(""parent"");
            parent.addMixin(""mix:referenceable"");
            session.save();
            
// clone parent
            WS2.clone(""default"", ""/parent"", ""/parent"", true);
            
            Node child = parent.addNode(""child"");
// add child
            child.addMixin(""mix:referenceable"");
            session.save();

// clone parent with child            
            WS2.clone(""default"", ""/parent"", ""/parent"", true); 

// clone parent again,   ItemNotFoundException - from now on can't clone parent node.
            WS2.clone(""default"", ""/parent"", ""/parent"", true);


Stacktrace:
javax.jcr.ItemNotFoundException: failed to build path of 229083e5-5f24-4102-b007-785f43be983a: cafebabe-cafe-babe-cafe-babecafebabe has no child entry for 229083e5-5f24-4102-b007-785f43be983a
	at org.apache.jackrabbit.core.HierarchyManagerImpl.buildPath(HierarchyManagerImpl.java:308)
	at org.apache.jackrabbit.core.CachingHierarchyManager.buildPath(CachingHierarchyManager.java:159)
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getPath(HierarchyManagerImpl.java:357)
	at org.apache.jackrabbit.core.CachingHierarchyManager.getPath(CachingHierarchyManager.java:221)
	at org.apache.jackrabbit.core.BatchedItemOperations.checkRemoveNode(BatchedItemOperations.java:700)
	at org.apache.jackrabbit.core.BatchedItemOperations.recursiveRemoveNodeState(BatchedItemOperations.java:1514)
	at org.apache.jackrabbit.core.BatchedItemOperations.removeNodeState(BatchedItemOperations.java:1216)
	at org.apache.jackrabbit.core.BatchedItemOperations.copyNodeState(BatchedItemOperations.java:1642)
	at org.apache.jackrabbit.core.BatchedItemOperations.copy(BatchedItemOperations.java:311)
	at org.apache.jackrabbit.core.WorkspaceImpl.internalCopy(WorkspaceImpl.java:294)
	at org.apache.jackrabbit.core.WorkspaceImpl.clone(WorkspaceImpl.java:401)
	at test.CloneTest.main(CloneTest.java:64)

            "
1,"Error releasing chunked connections with no response body.. HttpMethodBase.releaseConnection() does not successfully release the connection
if closing the response stream throws an exception."
1,"[PATCH] When locks are disabled, IndexWriter.close() throws NullPointerException. If locks are disabled (via setting the System property 'disableLuceneLocks' to
true), IndexWriter throws a NullPointerException on closing. The reason is that
the attempt to call writeLock.release() fails because writeLock is null.
To correct this, just check for this case before releasing. A (trivial) patch is
attached."
1,"Flexible QueryParser fails with local different from en_US. I get the following error during the mentioned testcases on my computer, if I use the Locale de_DE (windows 32):

{code}
    [junit] Testsuite: org.apache.lucene.queryParser.standard.TestQPHelper
    [junit] Tests run: 29, Failures: 1, Errors: 0, Time elapsed: 1,156 sec
    [junit]
    [junit] ------------- Standard Output ---------------
    [junit] Result: (fieldX:xxxxx fieldy:xxxxxxxx)^2.0
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testLocalDateFormat(org.apache.lucene.queryParser.standard.TestQPHelper): FAILED
    [junit] expected:<1> but was:<0>
    [junit] junit.framework.AssertionFailedError: expected:<1> but was:<0>
    [junit]     at org.apache.lucene.queryParser.standard.TestQPHelper.assertHits(TestQPHelper.java:1148)
    [junit]     at org.apache.lucene.queryParser.standard.TestQPHelper.testLocalDateFormat(TestQPHelper.java:1005)
    [junit]     at org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:201)
    [junit]
    [junit]
    [junit] Test org.apache.lucene.queryParser.standard.TestQPHelper FAILED
    [junit] Testsuite: org.apache.lucene.queryParser.standard.TestQueryParserWrapper
    [junit] Tests run: 27, Failures: 1, Errors: 0, Time elapsed: 1,219 sec
    [junit]
    [junit] ------------- Standard Output ---------------
    [junit] Result: (fieldX:xxxxx fieldy:xxxxxxxx)^2.0
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testLocalDateFormat(org.apache.lucene.queryParser.standard.TestQueryParserWrapper):       FAILED
    [junit] expected:<1> but was:<0>
    [junit] junit.framework.AssertionFailedError: expected:<1> but was:<0>
    [junit]     at org.apache.lucene.queryParser.standard.TestQueryParserWrapper.assertHits(TestQueryParserWrapper.java:1120)
    [junit]     at org.apache.lucene.queryParser.standard.TestQueryParserWrapper.testLocalDateFormat(TestQueryParserWrapper.java:985)
    [junit]     at org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:201)
    [junit]
    [junit]
    [junit] Test org.apache.lucene.queryParser.standard.TestQueryParserWrapper FAILED
{code}

With en_US as locale it works."
1,"Dirty Internal State on Transaction-Rollback during Global Transaction (container managed transaction). Running the following code inside an Global Transaction (JTA, container managed transaction) causes problems.

Session session = getRepsoitorySession(); 
      Node rootNode = session.getRootNode(); 

      Node test = rootNode.addNode(""test""); 
      test.addMixin(CTVRepositoryKonstanten.NODE_MIX_TYP_VERSION); 
      session.save(); 
      throw new RuntimeException(""testException"");

Everythink is fine, but if we execute it a second time we get an org.apache.jackrabbit.core.state.NoSuchItemStateException

org.apache.jackrabbit.core.state.NoSuchItemStateException: b36d91bc-8687-428c-a767-2e087b13191a 
at org.apache.jackrabbit.core.state.SharedItemStateManager.getItemState(SharedItemStateManager.java:270) 
at org.apache.jackrabbit.core.state.LocalItemStateManager.getNodeState(LocalItemStateManager.java:107) 
at org.apache.jackrabbit.core.state.LocalItemStateManager.getItemState(LocalItemStateManager.java:172) 
at org.apache.jackrabbit.core.state.XAItemStateManager.getItemState(XAItemStateManager.java:260) 
at org.apache.jackrabbit.core.version.NodeStateEx.store(NodeStateEx.java:519) 
at org.apache.jackrabbit.core.version.NodeStateEx.store(NodeStateEx.java:489) 
at org.apache.jackrabbit.core.version.AbstractVersionManager.getParentNode(AbstractVersionManager.java:414) 
at org.apache.jackrabbit.core.version.AbstractVersionManager.createVersionHistory(AbstractVersionManager.java:357) 
at org.apache.jackrabbit.core.version.XAVersionManager.createVersionHistory(XAVersionManager.java:148) 
at org.apache.jackrabbit.core.version.AbstractVersionManager.getVersionHistory(AbstractVersionManager.java:273) 
at org.apache.jackrabbit.core.ItemImpl.initVersionHistories(ItemImpl.java:738) 
at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1097) 
at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:915) 
at org.apache.jackrabbit.jca.JCASessionHandle.save(JCASessionHandle.java:180) 
at de.continentale.repo.CTVRepository.erstelleDokument(CTVRepository.java:2267)

We think that there is some internal state that is not cleaned up on rollback.
Restarting the runtime (Application Server) ""solved"" this.

May be there are some same causes like in: JCR-2503, JCR-2613

"
1,"ReadTask ignores traversalSize. The ReadTask doLogic() method ignores the value of the traversalSize and loops over hits.length() instead, thus falsely reporting the desired number of iterations through the hit list.

The fix is relatively trivial since we already calculate 
{code}
int traversalSize = Math.min(hits.length(), traversalSize());
{code}
so we just need to use this value in the loop condition."
1,"[PATCH] Problem with Sort logic on tokenized fields. When you set s SortField to a Text field which gets tokenized
FieldCacheImpl uses the term to do the sort, but then sorting is off 
especially with more then one word in the field. I think it is much 
more logical to sort by field's string value if the sort field is Tokenized and
stored. This way you'll get the CORRECT sort order"
1,"Writers on two machines over NFS can hit FNFE due to stale NFS client caching. Issue spawned from this thread:

  http://www.gossamer-threads.com/lists/lucene/java-user/50680

When IndexFileDeleter lists the directory, looking for segments_X
files to load, if it hits a FNFE on opening such a file it should
catch this and treat it as if the file does not exist.

On NFS (and possibly other file systems), a directory listing is not
guaranteed to be ""current""/coherent.  Specifically, if machine #1 has
just removed file ""segments_n"" and shortly thereafer machine #2 does a
dir listing, it's possible (likely?) that the dir listing will still
show that segments_n exists.

I think the fix is simple: catch the FNFE and just handle it as if the
segments_n does not in fact exist.

"
1,Path.equals does not work for other Path implementations. PathImpl.equals does not take other path implementations into account (likely a typo).
1,"incorrect definition of built-in node type nt:hierarchyNode. the property jcr:created of nt:hierarchyNode should be non-mandatory according to the specification (jcr 1.0 and jcr 1.0.1).
both the definition of the built-in node type and the related test case should be fixed accordingly."
1,"IndexWriter.numDocs doesn't take into account applied but not flushed deletes. The javadoc states that buffered deletes are not taken into account and so you must call commit first.

But, if you do that, and you're using CMS, and you're unlucky enough to have a background merge commit just after you call commit but before you call .numDocs, you can still get a wrong count back.

The fix is trivial -- numDocs should also consult any pooled readers for their current del count.

This is causing an intermittent failure in the new TestNRTThreads.
"
1,"Using MultiSearcher and ParallelMultiSearcher can change the sort order.. When using multiple sort criteria the first criterium that indicates a difference should be used.
When a field does not exist for a given document, special rules apply.
From what I see in the code, it is sorted as 0 for integer and float fields, and null Strings are sorted before others.

This works correctly in both Lucene 1.4.3 and in trunk as long as you use a single IndexSearcher (except perhaps in special cases, see other bug reports like LUCENE-374).

However, in MultiSearcher and ParallelMultiSearcher, the results of the separate IndexSearchers are merged and there an error occurs.
The bug is located in FieldDocSortedHitQueue.

It can even be demonstrated by passing a single indexSearcher to a MultiSearcher.

TestCase and patch follow."
1,Creating QValue from stream: stream not closed. QValueFactoryImpl.create(InputStream) does not close the input stream as mandated by the contract. 
1,"SearchIndex class contains garbled String. Somehow during the switch to SL4J also a String literal in the SearchIndex class got garbled.

See:
http://svn.apache.org/viewcvs.cgi/incubator/jackrabbit/trunk/jackrabbit/src/main/java/org/apache/jackrabbit/core/query/lucene/SearchIndex.java?rev=385280&r1=378221&r2=385280

Since this is a low risk change I would like to get this included into the 1.0 branch."
1,HTMLStripCharFilter produces invalid final offset. Nightly build found this... I boiled it down to a small test case that doesn't require the big line file docs.
1,"HttpMultiClient reuses closed connections. If a socket times out while sitting in the connection pool, 
HttpConnectionManager still attempts to reuse it resulting in an IOException 
being thrown when writing to the socket.  I believe this is a problem with both 
server side and client side timeouts (ie: we try to reuse a connection that we 
timed out) though am not certain of that.  At the very least server side 
timeouts cause the issue.

As yet I can't see how to fix this.  With the current code there doesn't even 
appear to be a suitable workaround because when the exception is thrown, the 
connection is added back into the pool to be reused (even though it is closed) 
which causes the next attempt to fail as well.

I can't see any reliable way to tell whether or not a connection is open, so 
would suggest the following as a fix:

1. In HttpMultiClient.executeMethod, close the connection if an exception is 
thrown (optionally, only if an IOException is thrown instead of an 
HttpException, but generally exceptions tend to leave things in an unknown 
state).

2. (optional) Add a retry loop to executeMethod to retry if an exception occurs 
(possibly only if an IOException is thrown, depending on exactly when a 
HttpException is thrown).

I'll attach a patch which does both of this to help clarify."
1,"Invalid behavior of StandardTokenizerImpl. The following code prints the output of StandardAnalyzer:

        Analyzer analyzer = new StandardAnalyzer();
        TokenStream ts = analyzer.tokenStream(""content"", new StringReader(""<some text>""));
        Token t;
        while ((t = ts.next()) != null) {
            System.out.println(t);
        }

If you pass ""www.abc.com"", the output is (www.abc.com,0,11,type=<HOST>) (which is correct in my opinion).
However, if you pass ""www.abc.com."" (notice the extra '.' at the end), the output is (wwwabccom,0,12,type=<ACRONYM>).

I think the behavior in the second case is incorrect for several reasons:
1. It recognizes the string incorrectly (no argue on that).
2. It kind of prevents you from putting URLs at the end of a sentence, which is perfectly legal.
3. An ACRONYM, at least to the best of my understanding, is of the form A.B.C. and not ABC.DEF.

I looked at StandardTokenizerImpl.jflex and I think the problem comes from this definition:
// acronyms: U.S.A., I.B.M., etc.
// use a post-filter to remove dots
ACRONYM    =  {ALPHA} ""."" ({ALPHA} ""."")+

Notice how the comment relates to acronym as U.S.A., I.B.M. and not something else. I changed the definition to
ACRONYM    =  {LETTER} ""."" ({LETTER} ""."")+
and it solved the problem.

This was also reported here:
http://www.nabble.com/Inconsistent-StandardTokenizer-behaviour-tf596059.html#a1593383
http://www.nabble.com/Standard-Analyzer---Host-and-Acronym-tf3620533.html#a10109926
"
1,"Cluster Node ID should be trimmed. If the cluster node ID is not configured in repository.xml, it is read from the file cluster_node.id instead. In case this file is edited by hand, some editors (e.g. vi) insert a trailing newline character (""\n""). This leads to the cluster node ID to contain a blank character. While I don't expect this to cause any issues, it is inconvenient for debugging and also introduces line-breaks in log files. I suggest to trim the cluster node ID, so only non-blank characters are used."
1,"BasicClientCookie.toString() contains 'name' instead of 'value' when writing out cookie value. {noformat}
buffer.append(""[name: "");
buffer.append(this.value);
{noformat}

should be

{noformat}
buffer.append(""[value: "");
buffer.append(this.value);
{noformat}

Will provide a patch soon."
1,"ConnectException not handled in DefaultHttpMethodRetryHandler. Copied from my mailing list post, Oleg suggested I post it to JIRA for 4.0 fix.

i am using commons-httpclient.3.0.1 and I am sending some requests
through https protocol. I have a problem with a long creation of
connection if ip address of remote service is not existing. I think
problem is in the situation when https connection is not created and
ConnectException is thrown after connection timeout. This exception is
catched in HttpMethodDirector.java in method executeWithRetry. Then
the DefaultHttpMethodRetryHandler is called to recognize whether
connection creation will be repeated or not.
I think, that special handling for ConnectException is missing in
retryMethod of DefaultHttpMethodRetryHandler, because exception is not
recognized and connetions are created again.
On the other hand, ConnectTimeoutException is thrown after connection
timeout for HTTP. This exception is handled in
DefaultHttpMethodRetryHandler and call is stopped.

These lines of code handle ConnectTimeoutException in retryMethod of
DefaultHttpMethodRetryHandler:
if (exception instanceof InterruptedIOException) {
            // Timeout
            return false;
        }

Probably this is missing for ConnectException:
if (exception instanceof InterruptedIOException || exception
instanceof ConnectException) {
            // Timeout
            return false;
        }

"
1,"Cookies with null value are not formatted correctly. I have a server that sets a bunch of empty cookies:

2003/03/06 16:28:52:055 PST [DEBUG] wire - -<< ""Set-Cookie: list%2ESince=; 
path=/[\r][\n]""
2003/03/06 16:28:52:055 PST [DEBUG] wire - -<< ""Set-Cookie: search%2EPhoneSDA=; 
path=/[\r][\n]""


  On subsequent requests, httpclient attaches these cookies thusly:

2003/03/06 16:28:55:480 PST [DEBUG] wire - ->> ""Cookie: $Version=0; list%
2ESince=null; $Path=/[\r][\n]""
2003/03/06 16:28:55:480 PST [DEBUG] wire - ->> ""Cookie: $Version=0; search%
2EPhoneSDA=null; $Path=/[\r][\n]""


  I'm not sure how to read this portion of wirelog, but seems that actual
values containing the string ""null"" are being sent as part of the request.
In the response to my request, the server now echos cookies with ""null""
values back to me.


2003/03/06 16:28:55:660 PST [DEBUG] wire - -<< ""Set-Cookie: search%
2EPhoneSDA=null; path=/[\r][\n]""
2003/03/06 16:28:55:660 PST [DEBUG] wire - -<< ""Set-Cookie: list%2ESince=null; 
path=/[\r][\n]""


  This isn't good.  Basically, the list.Since= cookie is being
converted to list.Since=null.  This causes the server's script to
crash:

<p>Microsoft VBScript runtime </font> <font face=""Arial"" 
size=2>error '800a000d'</font>
<p>
<font face=""Arial"" size=2>Type mismatch: 'CINT'</font>
<p>
<font face=""Arial"" size=2>/listCust.asp</font><font face=""Arial"" size=2>, line 
283</font> 


  I guess the script tries to assign the string ""null"" to an integer, and
dies.

Reported by Tom Samplonius <tom@sdf.com>"
1,NOT_ANALYZED fields can double-count offsets. If the same field name has 2 NOT_ANALYZED field instances then the offsets are double-counted.
1,"QueryParser doesn't accept empty string. foo:"""" currently throws a parse exception
foo: bar is also parsed as foo:bar (not serious since it's arguably illegal syntax)"
1,"Workspace.move() and Session.move()  allow moves to an invalid path. When calling Workspace.move() with an invalid destination path, which is a child of the source path, e.g. move(""/path"", ""/path/path2""), results in the source path nodes being removed. When calling Session.move() the failure is a StackOverflowError when save is called.

"
1,"testIWondiskfull checkindex failure. looks like charlie cron created a corrupt index on disk full.. can't reproduce with the seed on this machine, i can try on that VM with the same environment and see if i have better luck."
1,"NullPointerException in LuceneQueryBuilder. after setting up the following query:

//mycoreclass[@ID= 'ArchNachl_class_003']//label[@* = 'A']

I get a NullPointerException:


java.lang.NullPointerException
        at org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder.visit(LuceneQueryBuilder.java:553)
        at org.apache.jackrabbit.core.query.RelationQueryNode.accept(RelationQueryNode.java:157)
        at org.apache.jackrabbit.core.query.NAryQueryNode.acceptOperands(NAryQueryNode.java:131)
        at org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder.visit(LuceneQueryBuilder.java:421)
        at org.apache.jackrabbit.core.query.LocationStepQueryNode.accept(LocationStepQueryNode.java:156)
        at org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder.visit(LuceneQueryBuilder.java:400)
        at org.apache.jackrabbit.core.query.PathQueryNode.accept(PathQueryNode.java:47)
        at org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder.visit(LuceneQueryBuilder.java:200)
        at org.apache.jackrabbit.core.query.QueryRootNode.accept(QueryRootNode.java:112)
        at org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder.createLuceneQuery(LuceneQueryBuilder.java:190)
        at org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder.createQuery(LuceneQueryBuilder.java:172)
        at org.apache.jackrabbit.core.query.lucene.QueryImpl.execute(QueryImpl.java:152)
        at org.apache.jackrabbit.core.query.QueryImpl.execute(QueryImpl.java:132)

I don't really know if it's a valid query. I should search for every label that has a property value (what ever name the property may have) that is  somewhere under a specific ""mycoreclass"" node. Either way Jackrabbit should of cause not exit with a NullPointerExeption here.

I'm using the very current svn snapshot of jackrabbit. For the records line 550-557 of LuceneQueryBuilder.java look like this now:

        String field = """";
        try {
            field = node.getProperty().toJCRName(nsMappings);
        } catch (NoPrefixDeclaredException e) {
            // should never happen
            exceptions.add(e);
        }

"
1,"XML import should not access external entities. With current Jackrabbit the following XML document can not be imported:

    <!DOCTYPE foo SYSTEM ""http://invalid.address/""><foo/>

Even if the DTD address (or some other external resource referenced in the XML document) is correct, I don't think importXML() should even try resolving those references."
1,"LazyField use of IndexInput not thread safe. Hypothetical problem: IndexInput.clone() of an active IndexInput could result in a corrupt copy.
LazyField clones the FieldsReader.fieldsStream, which could be in use via IndexReader.document()"
1,"Bug in duplicate mapping check. There is a bug in the MappingDescriptor for checking if a mapping for a node type is already available. The following patch solves this problem:

Index: /Users/cziegeler/Developer/workspaces/default/jackrabbit/jackrabbit-ocm/src/main/java/org/apache/jackrabbit/ocm/mapper/model/MappingDescriptor.java
===================================================================
--- /Users/cziegeler/Developer/workspaces/default/jackrabbit/jackrabbit-ocm/src/main/java/org/apache/jackrabbit/ocm/mapper/model/MappingDescriptor.java	(revision 614136)
+++ /Users/cziegeler/Developer/workspaces/default/jackrabbit/jackrabbit-ocm/src/main/java/org/apache/jackrabbit/ocm/mapper/model/MappingDescriptor.java	(working copy)
@@ -75,7 +75,7 @@
         if (null != classDescriptor.getJcrType() && !  """".equals(classDescriptor.getJcrType()) && 
         		 ! ManagerConstant.NT_UNSTRUCTURED.equals(classDescriptor.getJcrType()))
         {
-        	if ((classDescriptorsByNodeType.get(classDescriptor.getClassName()) != null) &&
+        	if ((classDescriptorsByNodeType.get(classDescriptor.getJcrType()) != null) &&
         		classDescriptor.usesNodeTypePerConcreteClassStrategy()	)
         	{
         	    log.warn(""Duplicate classdescriptor for node type : "" + classDescriptor.getJcrType());	
"
1,"Spellchecker's dictionary iterator misbehaves. In LuceneDictionary, the LuceneIterator.hasNext() method has two issues that makes it misbehave:

1) If hasNext is called more than once, items are skipped
2) Much more seriously, when comparing fieldnames it is done with != rather than .equals() with the potential result that nothing is indexed
"
1,"NRT can temporarily lose deletions at high indexing rates. OK, I found a sneaky case where NRT can temporarily lose deletions.
The deletions aren't permanently lost - they are seen on the next
opened NRT reader.

It happens like this (in IW.getReader):

  * First flush() is called, to flush added docs & materialize the
    deletes.

  * The very next statement enters a sync'd block to open the reader,
    but, if indexing rate is very high, and threads get scheduled
    ""appropriately"", a ""natural"" flush (due to RAM buffer being full
    or flush doc count being reached) could be hit before the sync
    block is entered, in which case that 2nd flush won't materialize
    the deletes associated with it, and the returned NRT reader will
    only see its adds until it's next reopened.

The fix is simple -- we should materialize deletes inside the sync
block, not during the flush.
"
1,"HttpClient throws NPE on Invalid Port when used with MultiThreadedHttpConnectionManager. The HttpClient throws NullPointerException in the main thread when an invalid port (like 80001) is used in the URL. An IllegalArgumentException is thrown in TimeoutGuard thread.
 
Exception in thread ""Timeout guard"" java.lang.IllegalArgumentException: port out of range:80001
	at java.net.InetSocketAddress.<init>(InetSocketAddress.java:118)
	at java.net.Socket.<init>(Socket.java:240)
	at org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory.createSocket(DefaultProtocolSocketFactory.java:80)
	at org.apache.commons.httpclient.protocol.ControllerThreadSocketFactory$1.doit(ControllerThreadSocketFactory.java:91)
	at org.apache.commons.httpclient.protocol.ControllerThreadSocketFactory$SocketTask.run(ControllerThreadSocketFactory.java:158)
	at java.lang.Thread.run(Thread.java:613)
Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.commons.httpclient.HttpConnection.open(HttpConnection.java:721)
	at org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpConnectionAdapter.open(MultiThreadedHttpConnectionManager.java:1361)
	at org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:387)
	at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:171)
	at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397)
	at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:323)
	at com.aol.test.HttpTest$PoolingHttpConnector.doGet(HttpTest.java:47)
	at com.aol.test.HttpTest.main(HttpTest.java:17)

It should throw a checked exception in main thread so caller can handle the error condition more gracefully.

The test program is attached. This is caused by a race condition and it's not always reproducible. Running in debugger shows a different behavior.

package com.aol.test;

import java.io.IOException;

import org.apache.commons.httpclient.HttpClient;
import org.apache.commons.httpclient.HttpStatus;
import org.apache.commons.httpclient.MultiThreadedHttpConnectionManager;
import org.apache.commons.httpclient.methods.GetMethod;
import org.apache.commons.httpclient.params.HttpConnectionManagerParams;

public class HttpTest {
	
	public static void main(String[] args) {
		PoolingHttpConnector conn = new PoolingHttpConnector();
		
		try {
			String response = conn.doGet(""http://www.aol.com:80001"");
			System.out.println(""Response='"" + response + ""'"");
		} catch (IOException e) {
			e.printStackTrace();
		}
	}


	static class PoolingHttpConnector {
		
		public static final int MAX_TOTAL_CONNECTIONS = 16;
		public static final int MAX_CONNECTIONS_PER_HOST = 8;
		public static final int CONNECT_TIMEOUT = 5000;
		public static final int SOCKET_TIMEOUT = 5000;
		public static final boolean TCP_NO_DELAY = true;
		
	    private static MultiThreadedHttpConnectionManager poolManager;
	    private static HttpConnectionManagerParams httpParams;
	    private static HttpClient httpClient;
	    private static boolean initialized = false;
	    
		public PoolingHttpConnector() 
		{
			initialize();
		}

		public String doGet(String url) throws IOException {
			GetMethod method = new GetMethod(url);
					
			try {
	            int status = httpClient.executeMethod(method);	            
		        String response = new String(method.getResponseBody());
	            
	            if (status != HttpStatus.SC_OK)
	            	throw new IOException(""HTTP error: "" + response);
	            
	            return response;
	            
			} finally {
	            method.releaseConnection();
			}
	 	} 	
	
		private synchronized void initialize() {	
			if (initialized)
				return;
			
	        poolManager = new MultiThreadedHttpConnectionManager();
	        httpParams = new HttpConnectionManagerParams();
	        
	        httpParams.setMaxTotalConnections(MAX_TOTAL_CONNECTIONS);
	        httpParams.setDefaultMaxConnectionsPerHost(MAX_CONNECTIONS_PER_HOST);
	        httpParams.setTcpNoDelay(TCP_NO_DELAY);
	        httpParams.setSoTimeout(SOCKET_TIMEOUT);
	        httpParams.setConnectionTimeout(CONNECT_TIMEOUT);
	        
	        poolManager.setParams(httpParams);
	        httpClient = new HttpClient(poolManager);

			initialized = true;
		}
		
	}
}



"
1,"autoCreate attribute of PropDef instances not serialized. When a property of custom node type is defined to be autoCreate=true, this fact is not serialized to the custom_nodetypes.xml file. Digging into the source revealse the NodeTypeDefStore.writeDef method to not write the autoCreate attribute:

   // autoCreate
   String autoCreate = elem.getAttributeValue(AUTOCREATE_ATTRIBUTE);
   if (autoCreate != null && autoCreate.length() > 0) {
     pd.setAutoCreate(Boolean.valueOf(autoCreate).booleanValue());
   }

This seems to be a remains of a copy-paste procedure :-) with the correct code most probably being something like

   // autoCreate
   elem.setAttribute(AUTOCREATE_ATTRIBUTE, Boolean.toString(pd.isAutoCreate()));
"
1,"BundleDBPersistenceManager does not free blobStore resources. When removing binary property from node or removing node containing binary property, resources occupied by binary property are not freed (orphaned records remains in associated ${schemaObjectPrefix}BINVAL table)."
1,"MatchAllDocsQuery.toString(String field) does not honor the javadoc contract. Should be 

public String toString(String field){
  return ""*:*"";
}

QueryParser needs to be able to parse the String form of this query."
1,"IndexWriter.addIndexes(IndexReader[]) fails to create compound files. Even if no exception is thrown while writing the compound file at the end of the 
addIndexes() call, the transaction is rolled back and the successfully written cfs 
file deleted. The fix is simple: There is just the 
{code:java}
success = true;
{code}
statement missing at the end of the try{} clause.

All tests pass. I'll commit this soon to trunk and 2.3.2."
1,"DefaultClientConnectionOperator doesn't update socket after call to connectSocket(...). In the DefaultClientConnectionOperator function openConnection(...) it calls SocketFactory.connectSocket(...). The documentation for connectSocket(...) says that it returns:
   ""the connected socket. The returned object may be different from
the sock argument if this factory supports a layered protocol. ""

A quick peek at the source showed:
In org.apache.http.impl.conn.DefaultClientConnectionOperator:

117         final SocketFactory sf = schm.getSocketFactory();
118
119         Socket sock = sf.createSocket();
120         conn.opening(sock, target);
121
122         try {
123             sock = sf.connectSocket(sock, target.getHostName(),
124                     schm.resolvePort(target.getPort()),
125                     local, 0, params);
126         } catch (ConnectException ex) {
127             throw new HttpHostConnectException(target, ex);
128         }
129         prepareSocket(sock, context, params);
130         conn.openCompleted(sf.isSecure(sock), params);

So DefaultClientConnectionOperator never updates conn with the new version of sock that may have been returned from connectSocket(...).

adding:
        130         conn.openCompleted(sf.isSecure(sock), params);
+++ 131         conn.update(sock, target, sf.isSecure(sock), params);
appears to fix the issue.
"
1,"Preemtive Auth fails whithout credentials. The preemtive authorization causes a HttpException to be thrown in teh
Authenticator if no credentials were provided at all. This case should be
handled quietly. A test case should be added."
1,"PathElement.equals doesn't handle INDEX_UNDEFINED. PathElement (and therefore Path) comparisons fail when INDEX_UNDEFINED is used (it's treated differently from INDEX_DEFAULT).
"
1,"Node.restore() throws java.lang.ClassCastException. I'm trying to upgrade to 1.5 using existing 1.3.x repository. Restore of versionable node throws ClassCastException.

Caused by: java.lang.ClassCastException: org.apache.jackrabbit.uuid.UUID
	at org.apache.jackrabbit.core.value.InternalValue.getString(InternalValue.java:436)
	at org.apache.jackrabbit.core.version.InternalFrozenNodeImpl.<init>(InternalFrozenNodeImpl.java:113)
	at org.apache.jackrabbit.core.version.AbstractVersionManager.createInternalVersionItem(AbstractVersionManager.java:576)
	at org.apache.jackrabbit.core.version.VersionManagerImpl.getItem(VersionManagerImpl.java:258)
	at org.apache.jackrabbit.core.version.InternalVersionImpl.getFrozenNode(InternalVersionImpl.java:111)
	at org.apache.jackrabbit.core.version.VersionImpl.getFrozenNode(VersionImpl.java:120)
	at org.apache.jackrabbit.core.NodeImpl.internalRestore(NodeImpl.java:4180)
	at org.apache.jackrabbit.core.NodeImpl.internalRestore(NodeImpl.java:4141)
	at org.apache.jackrabbit.core.NodeImpl.restore(NodeImpl.java:3429)

It seems that bug has been introduced already in 1.4 as part of JCR-926 (InternalValue cleanup).

Index: C:/data/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/version/InternalFrozenNodeImpl.java
===================================================================
--- C:/data/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/version/InternalFrozenNodeImpl.java	(revision 549117)
+++ C:/data/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/version/InternalFrozenNodeImpl.java	(working copy)
@@ -109,10 +109,10 @@
             PropertyState prop = props[i];
             if (prop.getName().equals(QName.JCR_FROZENUUID)) {
                 // special property
-                frozenUUID = UUID.fromString(node.getPropertyValue(QName.JCR_FROZENUUID).internalValue().toString());
+                frozenUUID = UUID.fromString(node.getPropertyValue(QName.JCR_FROZENUUID).getString());

Probably one of the assumptions made was wrong :
- The type of QName.JCR_FROZENUUID is STRING (Object.toString() was used before)."
1,"ArrayIndexOutOfBoundsException in HttpStatus.getStatusText(508). Try the following:

    System.out.println(""Status text = "" + HttpStatus.getStatusText(507));
    try {
      System.out.println(""Status text = "" + HttpStatus.getStatusText(508));
    }
    catch (Exception ex) {
      System.err.println(""Exception! msg = "" + ex.getMessage());
      ex.printStackTrace();
    }
    System.out.println(""Status text = "" + HttpStatus.getStatusText(509));

507 -> returns a message as expected
508 -> ArrayIndexOutOfBoundsException
509 -> null as expected"
1,"Removal of versions throws javax.jcr.ReferentialIntegrityException. From the following thread : http://www.mail-archive.com/jackrabbit-dev%40incubator.apache.org/msg03483.html

When trying to remove a version of a Node  the VersionHistory.removeVersion() method throws : ""javax.jcr.ReferentialIntegrityException: Unable to remove version. At least once referenced."".

Secton 8.2.2.10 (Removal of Versions) of the specification indicates that the version graph should be automatically repaired upon removal. Then, VersionHistory.removeVersion() should take care of references. (In fact, a user cannot alter the references (jcr:predecessors and jcr:successors), since they are protected properties.)

Here's the example (*updated) :

Node root1 = session.getRootNode() ;
Node test1 = root1.addNode(""test"") ;
test1.addMixin(""mix:versionable"");
test1.setProperty(""test"", ""1"");
session.save();
test1.checkin();

test1.checkout();
test1.setProperty(""test"", ""2"");
session.save();
test1.checkin();

test1.checkout();
test1.setProperty(""test"", ""3"");
session.save();
test1.checkin();

String baseVersion = test1.getBaseVersion().getName();
System.out.println(""Base version name: "" + baseVersion);

VersionHistory vh = test1.getVersionHistory();
for (VersionIterator vi = vh.getAllVersions(); vi.hasNext(); ) {
    Version currenVersion = vi.nextVersion();
    String versionName = currenVersion.getName();
    if (!versionName.equals(""jcr:rootVersion"") && !versionName.equals(baseVersion)) { 
        String propertyValue = currenVersion.getNode(""jcr:frozenNode"").getProperty(""test"").getString();
        System.out.println(""Removing version : "" + versionName + "" with value: "" + propertyValue);
        vh.removeVersion(versionName);
    }
}

Regards, 

Nicolas"
1,"ManageableCollectionUtil doesn't support Maps. ManageableCollectionUtil has two getManageableCollection methods, which do not currently return a ManageableCollection which wraps Maps. 

ManagedHashMap already exists in the codebase which I assume was created for this purpose, so both getManageableCollection methods could be modified so that they do something like:

            if (object instanceof Map){
                return new ManagedHashMap((Map)object);
            }


An alternative solution might be to modify the JCR mapping to support explicitly defining the 'ManagedXXX' class."
1,"HttpClient should always override the host of HostConfiguration if an absolute request URI is given. This bug most likely occurs on all patforms and OS's, but I have only tested it
on WinXP.

The HttpClient.executeMethod(HostConfiguration,HttpMethod,HttpState) will
receive and throw an IllegalArgumentException stating that ""host parameter is
null"" when a  HostConfiguration object is passed in that ONLY has a proxy set
(via HostConfiguration.setProxy(String, int)). Details to reproduce follow--the
bug can be easily reproduced by using the Apache Axis 1.2 CommonsHTTPSender
class (with JVM system props http.proxyHost, http.proxyPort set):

There is a bug in the Apache Commons HTTP Client 3.0rc2 that does not set the
hostname property
in the <code>HostConfiguration</code> object if the following two steps
are performed:<br>
1. You call
<code>HttpClient.executeMethod(HostConfiguration,HttpMethod,HttpState)</code>
with a <code>HostConfiguration</code> object and an <code>HttpMethod</code> object
(created using the HttpMethod(String uri) constructor).This method 
is called in this exact way in the Apache Axis 1.2 client
(CommonsHTTPSender.java lines 132 and 186).<br>
2. That <code>HostConfiguration</code> object only has a proxy set (using
setProxy(String, int)). This method 
is called in this exact way in the Apache Axis 1.2 client
(CommonsHTTPSender.java line 389).<br>

Apache Axis 1.2rc3 CommonsHTTPSender.java did not expose this bug in Commons
HTTP Client 3.0rc2 because
it set the <code>HostConfiguration</code> in a different manner, as follows:<br>
1. Call <code>HttpClient.setHostConfiguration(HostConfiguration)</code> first.
Again,
The <code>HostConfiguration</code> object must only have a proxy set and no host
name.<br>
2. Then call <code>HttpClient.executeMethod(HttpMethod)</code>.<br>

Using the above steps (as in Axis 1.2rc3 CommonsHTTPSender.java, invoke()
method), line 379 in HttpClient.java evaluates to true
because the argument <code>hostConfiguration</code> is null (see line 324 in
HttpClient.java) and the local 
variable <code>defaultHostConfiguration</code> ==
<code>HttpClient.setHostConfiguration(HostConfiguration)</code>
which was set in item #1 above. The hostname then gets set in the
<code>HostConfiguration</code>
object in line 384 of HttpClient.java."
1,"Deadlock on concurrent commits. As reported in the followup to JCR-1979, there's a case where two transactions may be concurrently inside a commit. This is bad as it breaks the main assumption in http://jackrabbit.apache.org/concurrency-control.html about all transactions first acquiring the versioning write lock.

Looking deeper into this I find that the versioning write lock is only acquired if the transaction being committed contains versioning operations. This is incorrect as all transactions in any case need to access the version store when checking for references."
1,"httpClient failed to reconnect after keep-alive connection timed out. Description:

When using httpClient with https tunnelling througha proxy server, after keep-
alive connection timed out on server side.  The httpClient code was unable to 
establish the connection again.

Cause:

The HttpMethodBase.processRequest's retry loop retries the connection without 
going through the ""CONNECT"" request to the proxy server.  Our proxy server 
returns 407 error code.  In case of tunnelling connection, proper reconnect 
should be done by first doing the ""CONNECT"" sequence to get authenticated 
throught the proxy.

Temp fix and Work around:

We implemented some work around to do the retry from the application layer.  In 
order to detect the situation, we have to rely on the error message contained 
in the HttpRecoverableException.  We are checking the text ""Connection aborted 
by peer: socket write error"".  We also have to modify the HttpMethodBase code 
to throw the HttpRecoverableException out to the application."
1,"HttpClient incorrectly handles Transfer-Encoding header. RFC2616, section 4.4 item 3 states:
     If a Content-Length header field (section 14.13) is present, its
     decimal value in OCTETs represents both the entity-length and the
     transfer-length. The Content-Length header field MUST NOT be sent
     if these two lengths are different (i.e., if a Transfer-Encoding
     header field is present). If a message is received with both a
     Transfer-Encoding header field and a Content-Length header field,
     the latter MUST be ignored.

This is not handled correctly in the case that a noncompliant HTTP server
returns both a Transfer-Encoding header and a Content-Length header.

I gave up on writing a TestCase for this as it would require a reliably
noncompliant HTTP Server."
1,"JNDI Referencable Issues. I'm questioning the use of Referencable in the BindableResource and BindableResourceFactory classes for the JNDI lookup process. Reason for this is because Referencable needs the Addrs to be in the EXACT order in order for it to be considered the same. (see http://java.sun.com/j2se/1.4.2/docs/api/javax/naming/Reference.html#equals(java.lang.Object) )

In order for me to get the JNDI reference to be found correctly I had to change the BindableResource.getReference method to swap the order the StringReferences were added to match up what was being passed in by glassfish. This seems EXTREMELY fragile to me as I don't know what order, say JBoss, would pass the StringRefences in in the Reference object for the Factory method.

Also, another problem is that getReference is binding the class name to BindableRepository class implementation and not javax.jcr.Repository. This again causes them not to match if you follow the example on the wiki on setting up the JNDI reference and use javax.jcr.Repository as the type. This can either be fixed by changing the JNDI reference to use the BindableRepository class or the change the BindableRepository class to set that to the Repository interface. Not sure which would be considered 'better'

I have a patch that fixes the first issue (at least for glassfish), but not the second. Again, this seems like a really 'breakable' setup right now and not sure what would be better to make sure this is avoided."
1,"Directory#copy leaks file handles. Directory#copy doesn't close the target directories output stream if sourceDir.openInput(srcFile) throws an Exception. Before LUCENE-3218 Directory#copy wasn't used extensively so this wasn't likely to happen during tests. Today we had a failure on the 3.x branch that is likely caused by this bug:

{noformat}
[junit] Testsuite: org.apache.lucene.index.TestAddIndexes
    [junit] Testcase: testAddIndexesWithRollback(org.apache.lucene.index.TestAddIndexes):	Caused an ERROR
    [junit] MockDirectoryWrapper: cannot close: there are still open files: {_co.cfs=1}
    [junit] java.lang.RuntimeException: MockDirectoryWrapper: cannot close: there are still open files: {_co.cfs=1}
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:483)
    [junit] 	at org.apache.lucene.index.TestAddIndexes$RunAddIndexesThreads.closeDir(TestAddIndexes.java:693)
    [junit] 	at org.apache.lucene.index.TestAddIndexes.testAddIndexesWithRollback(TestAddIndexes.java:924)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1277)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1195)
    [junit] Caused by: java.lang.RuntimeException: unclosed IndexOutput: _co.cfs
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.addFileHandle(MockDirectoryWrapper.java:410)
    [junit] 	at org.apache.lucene.store.MockCompoundFileDirectoryWrapper.<init>(MockCompoundFileDirectoryWrapper.java:39)
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.createCompoundOutput(MockDirectoryWrapper.java:439)
    [junit] 	at org.apache.lucene.index.SegmentMerger.createCompoundFile(SegmentMerger.java:128)
    [junit] 	at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:3101)
    [junit] 	at org.apache.lucene.index.TestAddIndexes$CommitAndAddIndexes3.doBody(TestAddIndexes.java:839)
    [junit] 	at org.apache.lucene.index.TestAddIndexes$RunAddIndexesThreads$1.run(TestAddIndexes.java:667)
    [junit] 
    [junit] 
    [junit] Tests run: 18, Failures: 0, Errors: 1, Time elapsed: 9.034 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] java.lang.IllegalStateException: CFS has pending open files
    [junit] 	at org.apache.lucene.store.CompoundFileWriter.close(CompoundFileWriter.java:143)
    [junit] 	at org.apache.lucene.store.CompoundFileDirectory.close(CompoundFileDirectory.java:181)
    [junit] 	at org.apache.lucene.store.DefaultCompoundFileDirectory.close(DefaultCompoundFileDirectory.java:58)
    [junit] 	at org.apache.lucene.store.MockCompoundFileDirectoryWrapper.close(MockCompoundFileDirectoryWrapper.java:55)
    [junit] 	at org.apache.lucene.index.SegmentMerger.createCompoundFile(SegmentMerger.java:139)
    [junit] 	at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:3101)
    [junit] 	at org.apache.lucene.index.TestAddIndexes$CommitAndAddIndexes3.doBody(TestAddIndexes.java:839)
    [junit] 	at org.apache.lucene.index.TestAddIndexes$RunAddIndexesThreads$1.run(TestAddIndexes.java:667)
{noformat}"
1,"Deadlock for some Query objects in the equals method (f.ex. PhraseQuery) in a concurrent environment. Some Query objects in lucene 2.3.2 (and previous versions) have internal variables using Vector.   These variables are used during the call to the equals method.   In a concurrent environment a deadlock might occur.    The attached code example shows this happening in lucene 2.3.2, but the patch in LUCENE-1346 fixes this issue (though that doesn't seem to be the intention of that patch according to the description :-)"
1,"add workaround for jre breakiterator bugs. on some inputs, the java breakiterator support will internally crash.

for example: ant test -Dtestcase=TestThaiAnalyzer -Dtestmethod=testRandomStrings -Dtests.seed=-8005471002120855329:-2517344653287596566 -Dtests.multiplier=3"
1,"fix more position corrumptions in 4.0 codecs. Spinoff of LUCENE-3876.

Some codecs have invalid asserts, wrong shift operators etc.

If a position exceeds Integer.MAX_VALUE/2 and then also has a payload,
it will produce corrumpt indexes or other strange errors.

Easiest way to trigger the bugs is to sometimes add a payload to the test from LUCENE-3876."
1,"IdleConnectionHandler can leave closed connections in a inconsistent state. IdleConnectionHandler when shutting down 'stale' connection does not update the state of AbstractPoolEntry thus causing an inconsistency between the state of the connection (closed) and that of the pool entry (still assumed open). The problem is mitigated by the fact that the pooling manager usually evicts closed connections almost immediately. There is a small window of time in the ThreadSafeClientConnManager#closeIdleConnection method, at which a connection can be closed by the IdleConnectionHandler and immediately leased from the pool by another thread in an inconsistent state before the main thread gets a chance to re-acquire the pool lock and clean out closed connections.

For 4.0.x the problem can be worked around by retaining the pool lock for the entire span of the #closeIdleConnection. For the 4.1 branch a better solution should be devised. This probably means a complete rewrite or deprecation of IdleConnectionHandler."
1,"If you ""flush by RAM usage"" then IndexWriter may over-merge. I think a good way to maximize performance of Lucene's indexing for a
given amount of RAM is to flush (writer.flush()) the added documents
whenever the RAM usage (writer.ramSizeInBytes()) has crossed the max
RAM you can afford.

But, this can confuse the merge policy and cause over-merging, unless
you set maxBufferedDocs properly.

This is because the merge policy looks at the current maxBufferedDocs
to figure out which segments are level 0 (first flushed) or level 1
(merged from <mergeFactor> level 0 segments).

I'm not sure how to fix this.  Maybe we can look at net size (bytes)
of a segment and ""infer"" level from this?  Still we would have to be
resilient to the application suddenly increasing the RAM allowed.

The good news is to workaround this bug I think you just need to
ensure that your maxBufferedDocs is less than mergeFactor *
typical-number-of-docs-flushed.
"
1,"ContentEncodingHttpClient.execute(HttpGet, ResponseHandler<T>) throws IOException when reading compressed response. The following snippet:

    String url = ""http://yahoo.com"";
    HttpClient httpClient = new ContentEncodingHttpClient();
    HttpGet get = new HttpGet(url);
    String content = httpClient.execute(get, new BasicResponseHandler());

throws:

java.io.IOException: Attempted read from closed stream.
	at org.apache.http.impl.io.ChunkedInputStream.read(ChunkedInputStream.java:126)
	at java.util.zip.CheckedInputStream.read(CheckedInputStream.java:42)
	at java.util.zip.GZIPInputStream.readUByte(GZIPInputStream.java:205)
	at java.util.zip.GZIPInputStream.readUShort(GZIPInputStream.java:197)
	at java.util.zip.GZIPInputStream.readHeader(GZIPInputStream.java:136)
	at java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:58)
	at java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:68)
	at org.apache.http.client.entity.GzipDecompressingEntity.getContent(GzipDecompressingEntity.java:63)
	at org.apache.http.conn.BasicManagedEntity.getContent(BasicManagedEntity.java:88)
	at org.apache.http.util.EntityUtils.consume(EntityUtils.java:65)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:974)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:919)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:910)
	at tv.adap.service.HttpPoolTest.testChunkedGzip(HttpPoolTest.java:41)

whereas the following snippet runs fine:

    String url = ""http://yahoo.com"";
    HttpClient httpClient = new ContentEncodingHttpClient();
    HttpGet get = new HttpGet(url);
    HttpResponse response = httpClient.execute(get);
    HttpEntity entity = response.getEntity();
    String content = EntityUtils.toString(entity);

These two snippets should be functionally the same (putting the entity body into content). Creating a JIRA per the recommendation of Oleg from httpclient-users."
1,"jcr2spi NodeEntryImpl.getPath() blows stack due to getIndex() calling itself. The jcr2spi NodeEntryImpl class contains logic that causes getIndex() to call itself.

Calling code:

    Session sess = repo.login(creds);
    Node inboxNode = sess.getRootNode().getNode(""Inbox"");
    inboxNode.getPath(); <== blows stack

Tracing reveals:

    1. NodeEntryImpl.getPath() ultimately calls getIndex()
    2. getIndex() calls NodeState.getDefinition()
    3. which calls ItemDefinitionProviderImpl.getQNodeDefinition(...)
    4. which catches a RepositoryException then calls NodeEntryImpl.getWorkspaceId()
    5. which calls NodeEntryImpl.getWorkspaceIndex()
    6. which calls getIndex() (back to step 2, ad infinitum)

Configuration:
    1. A configuration is loaded specifying in-memory persist manager
    2. Config is wrapped in TransientRepository
    3. that's wrapped in spi2jcr's RepositoryService using default BatchReadConfig
    4. a jcr2spi provider is instantiated that directly couples to spi2jcr
    5. Node in question is created as follows:

    Session sess = repo.login(creds);
    sess.getRootNode().addNode(""Inbox"", ""nt:folder"");
    sess.save();

I guess that's about it.
David"
1,"ArrayIndexOutOfBoundsException when using MultiFieldQueryParser. We get the following exception:

Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: -1
        at java.util.Vector.elementAt(Vector.java:434)
        at org.apache.lucene.queryParser.QueryParser.addClause(QueryParser.java:181)
        at org.apache.lucene.queryParser.QueryParser.Query(QueryParser.java:529)
        at org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:108)
        at org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:87)
        at
org.apache.lucene.queryParser.MultiFieldQueryParser.parse(MultiFieldQueryParser.java:77)
        at idx.Mquery.main(Mquery.java:64)


We are using a query with 'AND' like 'bla AND blo' on 5 fields.
One of the fields has a Tokenizer which returns no token
at all on this query, and this together with the AND
triggers the exception."
0,"Collapse nested OR expressions. Executing a query with multiple OR expressions in a predicate leads to score values that depend on the order of the operands.

For example, the following query:

//*[jcr:contains(@prop1, 'foo') or jcr:contains(@prop2, 'foo') or jcr:contains(@prop3, 'foo')] order by @jcr:score descending

will return a slightly different result compared to:

//*[jcr:contains(@prop3, 'foo') or jcr:contains(@prop1, 'foo') or jcr:contains(@prop2, 'foo')] order by @jcr:score descending

Internally jackrabbit parses the predicate of the first query into a tree:

orExpr(orExpr(contains(prop1, 'foo'), contains(prop2, 'foo')), contains(prop3, 'foo'))

Lucene will calculate the score for the inner OR expression first and then for the outer, which is not equivalent with a nested expression that has property names in a different sequence.

The query should be translated internally into a single OR expression with three operands. That way, the score value is always the same, irrespective of the order of the operands."
0,Use the remote-resources-plugin to add LICENSE and NOTICE files to binaries. Since JCRSITE-13 the remote resources plugin is configured to automatically add LICENSE and NOTICE files to all of our binary artifacts (including -sources and -javadoc jars). We should adapt the configuration so that these files get to include all the correct licensing metadata we currently maintain in src/main/resources/META-INF.
0,[API Doc] HttpClient tutorial update. Bring the tutorial up to date with the latest best practices
0,"[PATCH] simplify conversion of strings to primitives by using parseXXX, not valueOf(xxx).xxxValue(). Code converts strings to primitives using a two step process, eg

Boolean.valueOf(myString).booleanValue();

can be simplified to 

Boolean.parseBoolean(myString);

true of Float, Double, Int etc. 

In some cases, this avoids allocating temporary boxed objects

patch fixes this."
0,"Improve ArrayUtil/CollectionUtil.*Sort() methods to early-reaturn on empty or one-element lists/arrays. It might be a good idea to make CollectionUtil or ArrayUtil return early if the passed-in list or array's length <= 1 because sorting is unneeded then. This improves maybe automaton or other places, as for empty or one-element lists no SorterTermplate is created."
0,"Remove recursion in NumericRangeTermEnum. The current FilteredTermEnum in NRQ uses setEnum() which itsself calls next(). This may lead to a recursion that can overflow stack, if the index is empty and a large range with low precStep is used. With 64 bit numbers and precStep == 1 there may be 127 recursions, as each sub-range would hit no term on empty index and the setEnum call would then call next() which itsself calls setEnum again. This leads to recursion depth of 256.

Attached is a patch that converts to iterative approach. setEnum is now unused and throws UOE (like endEnum())."
0,"Visibility of Scorer.score(Collector, int, int) is wrong. The method for scoring subsets in Scorer has wrong visibility, its marked protected, but protected methods should not be called from other classes. Protected methods are intended for methods that should be overridden by subclasses and are called by (often) final methods of the same class. They should never be called from foreign classes.

This method is called from another class out-of-scope: BooleanScorer(2) - so it must be public, but it's protected. This does not lead to a compiler error because BS(2) is in same package, but may lead to problems if subclasses from other packages override it. When implementing LUCENE-2838 I hit a trap, as I thought tis method should only be called from the class or Scorer itsself, but in fact its called from outside, leading to bugs, because I had not overridden it. As ConstantScorer did not use it I have overridden it with throw UOE and suddenly BooleanQuery was broken, which made it clear that it's called from outside (which is not the intention of protected methods).

We cannot fix this in 3.x, as it would break backwards for classes that overwrite this method, but we can fix visibility in trunk.
"
0,"Add set/getLocalAddress methods to HostConfiguration. On clustered or multi-homed systems, there's a need to specify the local bind
address of sockets, to ensure that they're created on the right interface.  To
do this, the local address needs to be passed to the 4-argument version of
ProtcolSocketFactory.createSocket.

After discussion on the mailing list, the best approach for this seems to be
adding the local address as a property on HostConfiguration and HttpConnection.  

I've attached a patch which does the following:
- Add public set/getLocalAddress methods to HostConfiguration and HttpConnection.
- HttpConnection uses the local address when opening connections.
- Modify HostConfiguration.equals and hostEquals to compare the local address too.
- SimpleHttpConnectionManager uses the local address from the provided config. 
I also cleaned up its getConnection method a bit.
- HttpClient.executeMethod uses the local address from its default
HostConfiguration if the method's config doesn't specify one."
0,"jackrabbit-webapp facelift. Still before 1.4, I meant to make the jackrabbit-webapp look a bit nicer. I'm taking the skin from JCR-1236 and applying it to jackrabbit-webapp."
0,JFlex-based HTMLStripCharFilter replacement. A JFlex-based HTMLStripCharFilter replacement would be more performant and easier to understand and maintain.
0,"BindableRepositoryFactory requires exact resource type. The org.apache.jackrabbit.jndi.BindableRepositoryFactory class requires the exact class name org.apache.jackrabbit.jndi.BindableRepository to be specified for the JNDI resource that the factory is responsible for. However the current deployment model 2 howto document suggest that the more generic interface name javax.jcr.Repository be used instead. Currently this suggested configuration results in a null JNDI resource .

This issue should be fixed by either fixing the documentation (I can do this) or by relaxing the code in BindableRepositoryFactory. I'll attach a patch that does the latter, please comment on what you think is the best solution.

This issue was detected during the mailing list thread
http://thread.gmane.org/gmane.comp.apache.jackrabbit.devel/2303"
0,"[PATCH] documentation typo. Just a small patch that fixes a typo and changes the first sentence, as that 
one is used by Javadoc as a kind of summary so it should be something more 
useful than ""The Jakarta Lucene API is divided into several packages."""
0,"More utility methods in JcrUtils. I'd like to add at least the following utility methods to JcrUtils:

For logging:

    // Utility method to simplify log messages and debug prints:
    // Node -> ""name [type]""
    // Property -> ""@name = value(s)""
    String toString(Item item)

For making sure that a node exists:

    // Returns the identified child node. If the child does not already exist,
    // it is added using the default node type from the parent.
    Node setNode(Node parent, String name)

    // Same as above, but ensures that isNodeType(type) is true for the
    // returned node, using addNode(name, type) or setPrimaryType(type)
    // if needed.
    Node setNode(Node parent, String name, String type)

For adding (or setting, see above) nt:folder nodes:

    // Adds a new nt:folder node with the given name
    Node addFolder(Node parent, String name)

    // Ensures that an nt:folder node with the given name exists
    Node setFolder(Node parent, String name)

For adding (or setting) nt:file nodes:

    // Adds a new nt:file/nt:resource structure
    // If the mime type contains a charset parameter, then the jcr:encoding property is also set
    Node addFile(Node parent, String name, String mime, InputStream data)
    Node addFile(Node parent, String name, String mime, Calendar date, InputStream data)

    // Ensures that an nt:file/nt:resource structure exists with the given data.
    // Note that the type of a potential existing jcr:content node is not modified
    Node setFile(Node parent, String name, String mime, InputStream data)
    Node setFile(Node parent, String name, String mime, Calendar date, InputStream data)
"
0,Javadoc mistake in SegmentMerger. 
0,"Create a jackrabbit-site subproject. The Jackrabbit web site sources should be moved to a new ""jackrabbit-site"" subproject so that the top-level Jackrabbit POM would only be used for the multimodule build setup and generic project metadata. This way the web site could be built more easily without invoking all the subprojects, and Eclipse would also be happier since the site project wouldn't ""contain"" the other Jackrabbit modules."
0,"add -Dtests.codecprovider. Currently to test a codec (or set of codecs) you have to add them to lucene's core and edit a couple of arrays here and there...

It would be nice if when using the test-framework you could instead specify a codecprovider by classname (possibly containing your own set of huper-duper codecs).

For example I made the following little codecprovider in contrib:
{noformat}
public class AppendingCodecProvider extends CodecProvider {
  public AppendingCodecProvider() {
    register(new AppendingCodec());
    register(new SimpleTextCodec());
  }
}
{noformat}

Then, I'm able to run tests with 'ant -lib build/contrib/misc/lucene-misc-4.0-SNAPSHOT.jar test-core -Dtests.codecprovider=org.apache.lucene.index.codecs.appending.AppendingCodecProvider', and it always picks from my set of  codecs (in this case Appending and SimpleText), and I can set -Dtests.codec=Appending if i want to set just one.

"
0,"TermOrdVal/DocValuesComparator does too much work in compareBottom. We now have logic to fall back to by-value comparison, when the bottom
slot is not from the current reader.

But this is silly, because if the bottom slot is from a different
reader, it means the tie-break case is not possible (since the current
reader didn't have the bottom value), so when the incoming ord equals
the bottom ord we should always return x > 0.

I added a new random string sort test case to TestSort...

I also renamed DocValues.SortedSource.getByValue -> getOrdByValue and
cleaned up some whitespace.
"
0,"Remove commons-collections and slf4j-api dependencies from jcr-commons. As noted in JCR-1615 and discussed on the mailing list [1] it would be good if jackrabbit-jcr-commons didn't come with extra dependencies beyond the standard Java class libraries and the JCR API.

Currently jackrabbit-jcr-commons depends on both commons-collections and slf4j-api, but both dependencies are relatively isolated and could be dropped with relatively little effort. Both dependency changes may be backwards incompatible with existing clients, but since the impact is reasonably small and easy to resolve I'd be OK doing this in 1.5.

[1] http://markmail.org/message/724ruk4l7b5rjtan"
0,JSR 283: Workspace Management. 
0,"search.function - (1) score based on field value, (2) simple score customizability. FunctionQuery can return a score based on a field's value or on it's ordinal value.

FunctionFactory subclasses define the details of the function.  There is currently a LinearFloatFunction (a line specified by slope and intercept).

Field values are typically obtained from FieldValueSourceFactory.  Implementations include FloatFieldSource, IntFieldSource, and OrdFieldSource."
0,"Merge UUID to NodeId. The current NodeId class is mostly just a wrapper around UUID, which causes two objects to be instantiated for each node identifier that the system uses. The memory and processing overhead is quite small, but given that there are tons of NodeId instances it would be good to eliminate that overhead.

There is also lots of code that just converts UUIDs to NodeIds and vice versa. We could simplify such code if we just used NodeId everywhere.

Also, we might want to open up the possibility of using non-UUID node identifiers at some point in future, so it would make a lot of sense to remove the NodeId.getUUID method and rely directly on NodeId and it's equals(), hashCode(), and toString() methods in many places where we currently use UUIDs."
0,"FST should offer lookup-by-output API when output strictly increases. Spinoff from ""FST and FieldCache"" java-dev thread http://lucene.markmail.org/thread/swoawlv3fq4dntvl

FST is able to associate arbitrary outputs with the sorted input keys, but in the special (and, common) case where the function is strictly monotonic (each output only ""increases"" vs prior outputs), such as mapping to term ords or mapping to file offsets in the terms dict, we should offer a lookup-by-output API that efficiently walks the FST and locates input key (exact or floor or ceil) matching that output.
"
0,"Errors in character entities in Javadoc for HttpVersion. There are some errors in the Javadoc for the HttpVersion class. This is the
class comment:

 *  <p>HTTP version, as specified in RFC 2616.</p>
 *  <p>
 *  HTTP uses a ""&ltmajor&gt.&ltminor&gt"" numbering scheme to indicate versions
 *  of the protocol. The protocol versioning policy is intended to allow
 *  the sender to indicate the format of a message and its capacity for
 *  understanding further HTTP communication, rather than the features
 *  obtained via that communication. No change is made to the version
 *  number for the addition of message components which do not affect
 *  communication behavior or which only add to extensible field values.
 *  The &ltminor&gt number is incremented when the changes made to the
 *  protocol add features which do not change the general message parsing
 *  algorithm, but which may add to the message semantics and imply
 *  additional capabilities of the sender. The &ltmajor&gt number is
 *  incremented when the format of a message within the protocol is
 *  changed. See RFC 2145 [36] for a fuller explanation.
 *  </p>
 *  <p>
 *  The version of an HTTP message is indicated by an HTTP-Version field
 *  in the first line of the message.
 *  </p>
 *  <pre>
 *     HTTP-Version   = ""HTTP"" ""/"" 1*DIGIT ""."" 1*DIGIT
 *  </pre>
 *  <p>
 *   Note that the major and minor numbers MUST be treated as separate
 *   integers and that each MAY be incremented higher than a single digit.
 *   Thus, HTTP/2.4 is a lower version than HTTP/2.13, which in turn is
 *   lower than HTTP/12.3. Leading zeros MUST be ignored by recipients and
 *   MUST NOT be sent.
 *  </p>

Note that the character entities for less-than and greater-than are not properly
ended with a semi-colon.

I will attach a proposed fix."
0,"TokenWrapperAttributeFactory, CachingWrapperFilterHelper implements equals and so should also implement hashCode. its part of the contract of Object 

bq. If two objects are equal according to the equals(Object) method, then calling the hashCode method on each of the two objects must produce the same integer result."
0,"Add open ended range query syntax to QueryParser. The QueryParser fails to generate open ended range queries.
Parsing e.g. ""date:[1990 TO *]""  gives zero results,
but
ConstantRangeQuery(""date"",""1990"",null,true,true)
does produce the expected results.

""date:[* TO 1990]"" gives the same results as ConstantRangeQuery(""date"",null,""1990"",true,true)."
0,"""reproduce with"" on test failure isn't right if you manually overrided anything. If you run a test with eg -Dtests.codec=SimpleText...

If it fails, the ""reproduce with"" fails to include that manual override (-Dtests.codec=SimpleText), ie it only includes the seed / test class / test method.  So it won't actually reproduce the fail, in general.

We just need to fix the ""reproduce with"" to add any manual overrides...."
0,"Demo HTML parser doesn't work for international documents. Javacc assumes ASCII so it won't work with, say, japanese documents. Ideally it would read the charset from the HTML markup, but that can by tricky. For now assuming unicode would do the trick:

Add the following line marked with a + to HTMLParser.jj:

options {
  STATIC = false;
  OPTIMIZE_TOKEN_MANAGER = true;
  //DEBUG_LOOKAHEAD = true;
  //DEBUG_TOKEN_MANAGER = true;
+  UNICODE_INPUT = true;
}
"
0,Add option to make sorting in user/group query case insensitive. Sorting on string properties is currently case sensitive in the user/group search. There should be a way to specify whether sorting shuld be case (in)sensitive.
0,"TopDocsCollector should have bounded generic <T extends ScoreDoc>. TopDocsCollector was changed to be TopDocsCollector<T>. However it has methods which specifically assume the PQ stores ScoreDoc. Therefore, if someone extends it and defines a type which is not ScoreDoc, things will break.

We shouldn't put <T> on TopDocsCollector at all, but rather change its ctor to *protected TopDocsCollector(PriorityQueue<? extends ScoreDoc> pq)*. TopDocsCollector should handle ScoreDoc types. If we do this, we'll need to change FieldValueHitQueue's Entry to extend ScoreDoc as well."
0,"3.x backwards tests are using Version.LUCENE_CURRENT: aren't testing backwards!. The 3.x backwards tests are mostly all using Version.LUCENE_CURRENT, therefore they don't always test the behavior as they should.

I added TEST_VERSION_CURRENT = 3.0 to the backwards/LuceneTestCase, and I think we should fix all backwards tests to use TEST_VERSION_CURRENT instead."
0,changes-to-html: fixes and improvements. The [Lucene Hudson Changes.html|http://hudson.zones.apache.org/hudson/job/Lucene-trunk/lastSuccessfulBuild/artifact/lucene/build/docs/changes/Changes.html] looks bad because changes2html.pl doesn't properly handle some new usages in CHANGES.txt.
0,"jcr-server-webapp: RMI Registration unstable. Registration of the repository to a RMI registry in RepositoryStartupServlet.registerRMI uses web application parameters inconsistently and may not always succeed registering the repository.

Today, the registerRMI uses these parameters for registration to RMI:

    rmi-host : The name of the host on which the registry is running
    rmi-port : The port on which the registry is running
    rmi-uri : An RMI URI to use for registration
    repository-name : The name to bind the repository to

The problem is, that rmi-port is used to try to create the registry to make sure a registry is running on the local host. The rmi-uri is used to register the repository using the static Naming.bind method. If the rmi-uri is not configured, the URI is created from rmi-host, rmi-port and repository-name.

This may now create a bunch of problems: If the rmi-port and rmi-uri configurations do not match, registration fails, if rmi-host does not resolve to an IP address to which the registry is bound, registration fails.

I encounter this issue, when trying to register the repository to an RMI registry using default rmi-port configuration (rmi-host and rmi-uri not configured) when running the web app in Jetty."
0,"Add support for Map of referenced beans. OCM should support the mapping of maps of referenced beans.

@Collection(collectionConverter= BeanReferenceCollectionConverterImpl.class)
private java.util.Map<String, ReferencedBean> aMap;

BeanReferenceCollectionConverterImpl (mainly the method doGetCollection) needs to be updated to support the interface ManageableMap interface.
"
0,"Grouping module should allow subclasses to set the group key per document. The new grouping module can only group by a single-valued indexed field.

But, if we make the 'getGroupKey' a method that a subclass could override, then I think we could refactor Solr over to the module, because it could do function queries and normal queries via subclass (I think).

This also makes the impl more extensible to apps that might have their own interesting group values per document."
0,"Broken javadocs->site docs links. See the java-dev mailing list discussion: [http://www.nabble.com/Broken-javadocs-%3Esite-docs-links-to20369092.html].

When the Lucene Java website transitioned to versioning some of the documentation, links from some javadocs were not modified to follow the resources.  I found broken links to gettingstarted.html and queryparsersyntax.html.  Here is one example, to gettingstarted.html (the link text is ""demo""): 

[http://lucene.apache.org/java/2_4_0/api/org/apache/lucene/document/package-summary.html]

The attached patch converts absolute URLs from javadocs to versioned docs to be relative, and modifies the ""javadocs-all"" target in build.xml to add a path element named ""all"", so that both versions of the javadocs (all: core+contrib; and separated: core, contribs) can use the same relative URLs.  Adding a path element to the ""javadocs-all"" target is necessary because currently the ""all"" javadocs have one fewer path element than the separated javadocs.

I left as-is one absolute URL, in the o.a.l.index.SegmentInfos javadocs, to fileformats.html, because SegmentInfos is a package-private class, and the javadocs targets in build.xml only generate javadocs for public classes.
"
0,"Remove noLockHack in SharedItemStateManager. With the increased test coverage, specifically the recently added multi-threaded tests, I'm reasonably confident that the noLockHack in SharedItemStateManager is not needed anymore.

Attached patch removes the hack. All tests still pass, including the daily integration tests."
0,"ArabicAnalyzer: Lowercase before Stopfilter. ArabicAnalyzer lowercases text in case you have some non-Arabic text around.
It also allows you to set a custom stopword list (you might augment the Arabic list with some English ones, for example).

In this case its helpful for these non-Arabic stopwords, to lowercase before stopfilter.
"
0,Upgrade contrib/bdb-persistence to work w/Jackrabbit 1.3. The bdb-persistence PM in contrib should be upgraded to work w/Jackrabbit 1.3.1
0,"Add narrow API for loading stored fields, to replace FieldSelector. I think we should ""invert"" the FieldSelector API, with a ""push"" API
whereby FieldsReader invokes this API once per field in the document
being visited.

Implementations of the API can then do arbitrary things like save away
the field's size, load the field, clone the IndexInput for later lazy
loading, etc.

This very thin API would be a mirror image of the very thin index time
API we now have (IndexableField) and, importantly, it would have no
dependence on our ""user space"" Document/Field/FieldType impl, so apps
are free to do something totally custom.

After we have this, we should build the ""sugar"" API that rebuilds a
Document instance (ie IR.document(int docID)) on top of this new thin
API.  This'll also be a good test that the API is sufficient.

Relevant discussions from IRC this morning at
http://colabti.org/irclogger/irclogger_log/lucene-dev?date=2011-07-13#l76
"
0,"Term improvement. Term is designed for reuse of the supplied filter, to minimize intern().

One of the common use patterns is to create a Term with the txt field being an empty string.

To simplify this pattern and to document it's usefulness, I suggest adding a constructor:
public Term(String fld)
with the obvious implementation
and use it throughout core and contrib as a replacement.

"
0,create configuration on InputStream. RepositoryConfig should be possible to create based on InputStreams (in case of URLs) ; right now it's possible only using String and InputSource. Please update also the JCA connector. 
0,"Make MultiThreadedHttpConnectionManager defaults public statics.. Could the defaults for MultiThreadedHttpConnectionManager be made public
constants? I would do it my self since I have karma as a contributer to [lang]
and [codec] but I do not want to step on anyones toes. ;-)

Patch attached."
0,"Support underscore in domain name, or provide better exception. 
When calling on HttpClient.execute with a url that contain underscore ('_'), you get NullPointerException.
Tracing it down show that java.net.Uri complains that it is illegal name. Which is true according to the RFC.
But it seems that most browser allow it, and some companies support it.

I think HttpClient should either support underscores, or atleast provide a better exception.

"
0,"TCK: TextNodeTest and jcr:xmltext/jcr:xmlcharacters. Test creates jcr:xmltext nodes without jcr:xmlcharacters properties.  Some repositories may require jcr:xmltext nodes to have jcr:xmlcharacters properties, causing this test to fail.

Proposal: add a jcr:xmlcharacters property to each jcr:xmltext node.

--- TextNodeTest.java   (revision 422074)
+++ TextNodeTest.java   (working copy)
@@ -62,6 +62,7 @@
      */
     public void testTextNodeTest() throws RepositoryException {
         Node text1 = testRootNode.addNode(jcrXMLText);
+        text1.setProperty(jcrXMLCharacters, ""foo"");
         testRootNode.save();
         String xpath = ""/"" + jcrRoot + testRoot + ""/text()"";
         executeXPathQuery(superuser, xpath, new Node[]{text1});
@@ -73,7 +74,9 @@
      */
     public void testTextNodeTestMultiNodes() throws RepositoryException {
         Node text1 = testRootNode.addNode(jcrXMLText);
+        text1.setProperty(jcrXMLCharacters, ""foo"");
         Node text2 = testRootNode.addNode(nodeName1, testNodeType).addNode(jcrXMLText);
+        text2.setProperty(jcrXMLCharacters, ""foo"");
         testRootNode.save();
         String xpath = ""/"" + jcrRoot + testRoot + ""//text()"";
         executeXPathQuery(superuser, xpath, new Node[]{text1, text2});
@@ -105,11 +108,13 @@
             throw new NotExecutableException(""Repository does not support position index"");
         }
         Node text1 = testRootNode.addNode(jcrXMLText);
+        text1.setProperty(jcrXMLCharacters, ""foo"");
         if (!text1.getDefinition().allowsSameNameSiblings()) {
             throw new NotExecutableException(""Node at path: "" + testRoot + "" does not allow same name siblings with name: "" + jcrXMLText);
         }
         testRootNode.addNode(nodeName1, testNodeType);
         Node text2 = testRootNode.addNode(jcrXMLText);
+        text1.setProperty(jcrXMLCharacters, ""foo"");
         testRootNode.save();
         String xpath = ""/"" + jcrRoot + testRoot + ""/text()[2]"";
         executeXPathQuery(superuser, xpath, new Node[]{text2});
"
0,"[REFACTORING] FieldSortedHitQueue has too much duplicated code. There's 40LOC duplicated in FieldDocSortedHitQueue::lessThan just to handle 
the reverse sort. It would be more readable to actually do something like 
(YMMV):

if (field.getReverse()) {
    c = -c;
}"
0,"A few new benchmark tasks. Some tasks that would be helpful to see added. Might want some expansion, but here are some basic ones I have been using:

CommitIndexTask
ReopenReaderTask
SearchWithSortTask

I do the sort in a similar way that the highlighting was done, but another method may be better. Just would be great to have sorting.
Also, since there is no great field for sorting (reuters date always appears to be the same) I changed the id field from doc+id to just id. Again maybe not the best solution, but here I am to get the ball rolling :)"
0,Spellchecker should implement java.io.Closable. As the most of the lucene classes implement Closable (IndexWriter) Spellchecker should do too. 
0,"StringBody has incorrect default for characterset. StringBody defaults to Charset.defaultCharset() if the charset is not provided.

This means that the default depends on the current host.

The default should be US-ASCII (as was the case with StringPart in Commons HttpClient 3.1)."
0,"contrib.ssl.HostConfigurationWithHostFactory. I'd like to contribute an example specialized HostConfiguration, to replace the one I contributed in HTTPCLIENT-634."
0,"Referenced derby library behaves buggy on FreeBSD. The derby library referenced on dependencies page (http://jackrabbit.apache.org/dependencies.html) behaves buggy on FreeBSD. This is the same issue like the one with Magnolia CMS: http://jira.magnolia.info/browse/MAGNOLIA-818. Version derby-10.1.3.1 works fine.
"
0,"[PATCH] png, apng, mng text extractor. Text extractor for tEXt chunk for png, apng and mng formats"
0,"Allow workspace creation over cluster. When workspace is created on cluster node A and then node added to that workspace, the proper event is sent to the journal, but other cluster nodes are not able to process it because they don't have the workspace.

It would be nice to have a configuration option for the cluster to create such workspace automatically (instead of just logging an error)"
0,"Fix typos in CHANGES.txt and contrib/CHANGES.txt prior to 2.9 release. I noticed a few typos in CHANGES.txt and contrib/CHANGES.txt.  (Once they make it past a release, they're set in stone...)

Will attach a patch shortly."
0,"update NOTICE.txt. From the java-dev discussion, NOTICE.txt should be up-to-date.

One thing I know, is that the persian stopwords file (analyzers/fa) came from the same source as the arabic stopwords file, and is BSD-licensed. 

There might be others (I think ICU has already been added)"
0,"Jackrabbit Utilites upgrade to Jackrabbit 2.1.0. I'm including a patch for the jcrutil project in the sandbox, for the S3 DataStore to work with 2.1.0, as well as the VFS.  Also using Tika for MIME type resolution.  Please look this over and feel free to improve, this is something I played with but didn't stress test."
0,"NativeFSLockFactory throws an exception on Android 2.2 platform as java.lang.management package is not available on android.. NativeFSLockFactory throws an exception on Android 2.2 platform as java.lang.management package is not available on android.
   Looks like FSDirectory defaults to NativeFSLockFactory, and this class refers to java.lang.management package to generate a unique lock. java.lang.management is not available in Android 2.2 and hence a runtime exception is raised. The workaround is to use another custom LockFactory or SimpleFSLockFactory, but Fixing NativeFSLockFactroy will help.

Thanks,
Surinder"
0,"Work around ThreadLocal's ""leak"". Java's ThreadLocal is dangerous to use because it is able to take a
surprisingly very long time to release references to the values you
store in it.  Even when a ThreadLocal instance itself is GC'd, hard
references to the values you had stored in it are easily kept for
quite some time later.

While this is not technically a ""memory leak"", because eventually
(when the underlying Map that stores the values cleans up its ""stale""
references) the hard reference will be cleared, and GC can proceed,
its end behavior is not different from a memory leak in that under the
right situation you can easily tie up far more memory than you'd
expect, and then hit unexpected OOM error despite allocating an
extremely large heap to your JVM.

Lucene users have hit this many times.  Here's the most recent thread:

  http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200809.mbox/%3C6e3ae6310809091157j7a9fe46bxcc31f6e63305fcdc%40mail.gmail.com%3E

And here's another:

  http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200807.mbox/%3CF5FC94B2-E5C7-40C0-8B73-E12245B91CEE%40mikemccandless.com%3E

And then there's LUCENE-436 and LUCENE-529 at least.

A google search for ""ThreadLocal leak"" yields many compelling hits.

Sun does this for performance reasons, but I think it's a terrible
trap and we should work around it with Lucene."
0,"TCK: ExportDocViewTest.exportValues fails on empty multivalued property. In ExportDocViewTest.exportValues, line 988 fails if the property being exported is empty (array of size 0, which is allowed by the spec) since there is no space to remove. This code should be skipped if the number of values is zero."
0,"Transparent Content Coding support. I would like to see HttpClient features brought up to parity with other libraries, both in Java and other languages. c.f. Python's httplib2 (not yet in the standard library, but many would like to see it in there). That library transparently handles gzip and compress content codings.

This issue is to capture possible solutions to providing this sort of innate functionality in HttpClient, so that users aren't required to know RFC2616 intimately. The HttpClient library should do the right thing and use the network in the most efficient manner possible."
0,"Use covariant clone() return types. *Paul Cowan wrote in LUCENE-1257:*

OK, thought I'd jump in and help out here with one of my Java 5 favourites. Haven't seen anyone discuss this, and don't believe any of the patches address this, so thought I'd throw a patch out there (against SVN HEAD @ revision 827821) which uses Java 5 covariant return types for (almost) all of the Object#clone() implementations in core. 
i.e. this:

public Object clone() {
changes to:
public SpanNotQuery clone() {

which lets us get rid of a whole bunch of now-unnecessary casts, so e.g.

if (clone == null) clone = (SpanNotQuery) this.clone();
becomes
if (clone == null) clone = this.clone();

Almost everything has been done and all downcasts removed, in core, with the exception of

Some SpanQuery stuff, where it's assumed that it's safe to cast the clone() of a SpanQuery to a SpanQuery - this can't be made covariant without declaring ""abstract SpanQuery clone()"" in SpanQuery itself, which breaks those SpanQuerys that don't declare their own clone() 
Some IndexReaders, e.g. DirectoryReader - we can't be more specific than changing .clone() to return IndexReader, because it returns the result of IndexReader.clone(boolean). We could use covariant types for THAT, which would work fine, but that didn't follow the pattern of the others so that could be a later commit. 
Two changes were also made in contrib/, where not making the changes would have broken code by trying to widen IndexInput#clone() back out to returning Object, which is not permitted. contrib/ was otherwise left untouched.

Let me know what you think, or if you have any other questions."
0,"Spellchecker ""Suggest Mode"" Support. This is a spin-off from SOLR-2585.

Currently o.a.l.s.s.SpellChecker and o.a.l.s.s.DirectSpellChecker support two ""Suggest Modes"":
1. Suggest for terms that are not in the index.
2. Suggest ""more popular"" terms.

This issue is to add a third Suggest Mode:
3. Suggest always.

This will assist users in developing context-sensitive spell suggestions and/or did-you-mean suggestions.  See SOLR-2585 for a full discussion.

Note that o.a.l.s.s.SpellChecker already can support this functionality, if the user passes in a NULL term & IndexReader.  This, however, is not intutive.  o.a.l.s.s.DirectSpellChecker currently has no support for this third Suggest Mode."
0,'ant javacc' in root project should also properly create contrib/surround Java files. For consistency after LUCENE-1829 which did the same for contrib/queryparser
0,"maven 2 poms contain variables in dependency versions that are never resolved, breaking transitive dependency resolution. There are problems with the dependencies declared in jackrabbit-server-1.0 and other poms using variables for dependency versions. These variables are never resolved and break the transitive resolution of these dependencies"
0,"Flexibility to turn on/off any flush triggers. See discussion at http://www.gossamer-threads.com/lists/lucene/java-dev/53186

Provide the flexibility to turn on/off any flush triggers - ramBufferSize, maxBufferedDocs and maxBufferedDeleteTerms. One of ramBufferSize and maxBufferedDocs must be enabled."
0,"Indexing performance tests with realtime branch. We should run indexing performance tests with the DWPT changes and compare to trunk.

We need to test both single-threaded and multi-threaded performance.

NOTE:  flush by RAM isn't implemented just yet, so either we wait with the tests or flush by doc count."
0,"Tokenizers (which are the source of Tokens) should call AttributeSource.clearAttributes() first. This is a followup for LUCENE-1796:
{quote}
Token.clear() used to be called by the consumer... but then it was switched to the producer here: LUCENE-1101 
I don't know if all of the Tokenizers in lucene were ever changed, but in any case it looks like at least some of these bugs were introduced with the switch to the attribute API - for example StandardTokenizer did clear it's reusableToken... and now it doesn't.
{quote}

As alternative to changing all core/contrib Tokenizers to call clearAttributes first, we could do this in the indexer, what would be a overhead for old token streams that itsself clear their reusable token. This issue should also update the Javadocs, to clearly state inside Tokenizer.java, that the source TokenStream (normally the Tokenizer) should clear *all* Attributes. If it does not do it and e.g. the positionIncrement is changed to 0 by any TokenFilter, but the filter does not change it back to 1, the TokenStream would stay with 0. If the TokenFilter would call PositionIncrementAttribute.clear() (because he is responsible), it could also break the TokenStream, because clear() is a general method for the whole attribute instance. If e.g. Token is used as AttributeImpl, a call to clear() would also clear offsets and termLength, which is not wanted. So the source of the Tokenization should rest the attributes to default values.

LUCENE-1796 removed the iterator creation cost, so clearAttributes should run fast, but is an additional cost during Tokenization, as it was not done consistently before, so a small speed degradion is caused by this, but has nothing to do with the new TokenStream API."
0,"SQL2 parser: improved error message for ambiguous properties in joins. For queries of the form:

select id from parent inner join child on parent.id=child.parentid

the SQL2 parser currently only returns a generic error message ""This query result contains more than one selector"". 

The error message should point to the problematic token: 

Query: select id(*)from parent inner join child on parent.id=child.parentid; expected: Need to specify the selector name for ""id"" because the query contains more than one selector.
"
0,"Summer of Code GDATA Server  --Project structure and simple version to start with--. This is the initial issue for the GDATA - Server project (Google Summer of Code). 
The purpose of the issue is to create the project structure in the svn repository to kick off the project. The source e.g. the project will be located at URL: http://svn.apache.org/repos/asf/lucene/java/trunk/contrib
The attachment includes the diff text file and the jar files included in the lib directory as a tar.gz file.
To get some information about the project see http://wiki.apache.org/general/SimonWillnauer/SummerOfCode2006"
0,Improve behaviour when 'too many open files' occurs. The MultiIndex may leave unused directories that were created in an attempt to create an index segment. The directory is not removed again when an error occurs (e.g. 'too many open files').
0,"Reintegrate flex branch into trunk. This issue is for reintegrating the flex branch into current trunk. I will post the patch here for review and commit, when all contributors to flex have reviewed the patch.

Before committing, I will tag both trunk and flex."
0,"Mating Collector and Scorer on doc Id orderness. This is a spin off of LUCENE-1593. This issue proposes to expose appropriate API on Scorer and Collector such that one can create an optimized Collector based on a given Scorer's doc-id orderness and vice versa. Copied from LUCENE-1593, here is the list of changes:

# Deprecate Weight and create QueryWeight (abstract class) with a new scorer(reader, scoreDocsInOrder), replacing the current scorer(reader) method. QueryWeight implements Weight, while score(reader) calls score(reader, false /* out-of-order */) and scorer(reader, scoreDocsInOrder) is defined abstract.
#* Also add QueryWeightWrapper to wrap a given Weight implementation. This one will also be deprecated, as well as package-private.
#* Add to Query variants of createWeight and weight which return QueryWeight. For now, I prefer to add a default impl which wraps the Weight variant instead of overriding in all Query extensions, and in 3.0 when we remove the Weight variants - override in all extending classes.
# Add to Scorer isOutOfOrder with a default to false, and override in BS to true.
# Modify BooleanWeight to extend QueryWeight and implement the new scorer method to return BS2 or BS based on the number of required scorers and setAllowOutOfOrder.
# Add to Collector an abstract _acceptsDocsOutOfOrder_ which returns true/false.
#* Use it in IndexSearcher.search methods, that accept a Collector, in order to create the appropriate Scorer, using the new QueryWeight.
#* Provide a static create method to TFC and TSDC which accept this as an argument and creates the proper instance.
#* Wherever we create a Collector (TSDC or TFC), always ask for out-of-order Scorer and check on the resulting Scorer isOutOfOrder(), so that we can create the optimized Collector instance.
# Modify IndexSearcher to use all of the above logic.

The only class I'm worried about, and would like to verify with you, is Searchable. If we want to deprecate all the search methods on IndexSearcher, Searcher and Searchable which accept Weight and add new ones which accept QueryWeight, we must do the following:
* Deprecate Searchable in favor of Searcher.
* Add to Searcher the new QueryWeight variants. Here we have two choices: (1) break back-compat and add them as abstract (like we've done with the new Collector method) or (2) add them with a default impl to call the Weight versions, documenting these will become abstract in 3.0.
* Have Searcher extend UnicastRemoteObject and have RemoteSearchable extend Searcher. That's the part I'm a little bit worried about - Searchable implements java.rmi.Remote, which means there could be an implementation out there which implements Searchable and extends something different than UnicastRemoteObject, like Activeable. I think there is very small chance this has actually happened, but would like to confirm with you guys first.
* Add a deprecated, package-private, SearchableWrapper which extends Searcher and delegates all calls to the Searchable member.
* Deprecate all uses of Searchable and add Searcher instead, defaulting the old ones to use SearchableWrapper.
* Make all the necessary changes to IndexSearcher, MultiSearcher etc. regarding overriding these new methods.

One other optimization that was discussed in LUCENE-1593 is to expose a topScorer() API (on Weight) which returns a Scorer that its score(Collector) will be called, and additionally add a start() method to DISI. That will allow Scorers to initialize either on start() or score(Collector). This was proposed mainly because of BS and BS2 which check if they are initialized in every call to next(), skipTo() and score(). Personally I prefer to see that in a separate issue, following that one (as it might add methods to QueryWeight)."
0,"CacheManager interval between recalculation of cache sizes should be configurable. Currently interval between recaluclation of cahce size is hard coded to 1000 ms. Resizing/recalculation of cache size is quite expensive method (especially getMemoryUsed on MLRUItemStateCache is time consuming)

Depending on the configuration, we realized that under some load up to 10-15% percent of CPU time (profiler metrics) could be spend doing such recalculations. It does not seem to be needed to resize cache every second. Best this interval should be configurable in external config. file with other cache settings (like memory sizes)."
0,Add toString() methods to QOM tree classes. Having the QOM tree classes render themselves to SQL2 in their toString() methods would make debugging the QOM code quite a bit easier.
0,AbstractLockTest.testLockExpiration fails intermittently . This seems to be a timing issue. I propose to wait a bit longer for the lock to expire. 
0,"Improvement to UndefinedTypeConverterImpl to map super types effectively. Improvement to org.apache.jackrabbit.ocm.manager.atomictypeconverter.impl.UndefinedTypeConverterImpl's implementation of 
public Value getValue(ValueFactory valueFactory, Object propValue) , used equality check of class names to decide whether Object propValue is worthy of any attempt to map to an apropriate property.  Since the purpose of the class is to provide a 'best effort' attempt to map an Object of type java.lang.Object it will be better to use 'instanceof'.  This approach will convert the specific class as well as any inherited objects.  For example using instanceof will let us map a BufferedInputStream, and any other sub classes of InputStream to a Binary Property."
0,"Convert PrecedenceQueryParser to new TokenStream API. Adriano Crestani provided a patch, that updates the PQP to use the new TokenStream API...all tests still pass. 
I hope this helps to keep the PQP 
"
0,"Deprecate all String/File ctors/opens in IndexReader/IndexWriter/IndexSearcher. During investigation of LUCENE-1658, I found out, that even LUCENE-1453 is not completely fixed.
As 1658 deprecates all FSDirectory.getDirectory() static factories, we should not use them anymore. As the user is now free to choose the correct directory implementation using direct instantiation or using FSDir.open() he should no longer use all ctors/methods in IndexWriter/IndexReader/IndexSearcher & Co. that simply take path names as String or File and always instantiate the Directory himself.

LUCENE-1453 currently works for the cached directory implementations from FSDir.getDirectory, but not with uncached, non refcounting FSDirs. Sometime reopen() closes the directory (as far as I see, when a SegmentReader changes to a MultiSegmentReader and/or deletes apply). This is hard to track. In Lucene 3.0 we then can remove the whole bunch of closeDirectory parameters/fields in these classes and simply do not care anymore about closing directories.

To remove this closeDirectory parameter now (before 3.0) and also fix 1453 correctly, an additional idea would be to change these factories that take the File/String to return the IndexReader wrapped by a FilteredIndexReader, that keeps track on closing the underlying directory after close and reopen. This is simplier than passing this boolean between different DirectoryIndexReader instances. The small performance impact by wrapping with FilterIndexReader should not be so bad, because the method is deprecated and we can state, that it is better to user the factory method with Directory parameter."
0,"Remove/Uncommit SegmentingTokenizerBase. I added this class in LUCENE-3305 to support analyzers like Kuromoji,
but Kuromoji no longer needs it as of LUCENE-3767. So now nothing uses it.

I think we should uncommit before releasing, svn doesn't forget so
we can add this back if we want to refactor something like Thai or Smartcn
to use it."
0,"JCAManagedConnectionFactory should chain cause exception. In JCAManagedConnectionFactory, methods openSession and createRepository both throw ResourceException without setting the cause exception.  This can result in the actual error being swallowed silently, and only stepping through the running code at this point will reveal the actual error (eg: Persistent Store configuration error will appear as a pool exception).

Jukka Zitting on 12-Oct-2010 said:

This constructor is not available in J2EE version 1.3, so for now
we've been using the ResourceException.setLinkedException() method for
this (see JCR-761). To address your need we could either upgrade the
platform requirement to J2EE 1.4 or start using the J2SE method
Exception.initCause() instead of setLinkedException(). Can you file an
improvement issue in Jira about this?"
0,"Enforce TokenStream impl / Analyzer finalness by an assertion. As noted in LUCENE-1753 and other issues, TokenStream and Analyzers are based on the decorator pattern. At least all TokenStream and Analyzer implementations in Lucene and Solr should be final.

The attached patch adds an assertion to the ctors of both classes that does the corresponding checks:
- Analyzers must be final or private classes or anonymous inner classes
- TokenStreams must be final or private classes or anonymous inner classes or have a final incrementToken()

I will commit this after robert have fixed solr streams."
0,"OCM:Add the ability to specify name of a Collection Element through XML Mapping files.. Collection elements get mapped to a node ""collection-element"" when the mappings are specified through XML config files.  We need the ability to control this name through configuration.  Without that feature querying object structures is painful.  For example I have structure as below :

class Foo{
String id;
 List<Foo> children
 List<Foo> friends
}

And I have a need to query a Foo with id : 100 .  If I am interested only in child nodes with id = 110 , I could specify through the Filter that look at only node names , ""childFoo"" ; If I have the flexibility of adding a child node name."
0,"Provide More of Lucene For Maven. Please provide javadoc & source jars for lucene-core.  Also, please provide the rest of lucene (the jars inside of ""contrib"" in the download bundle) if possible."
0,"Improve handling of concurrent node modifications. consider the following scenario:
- session1 modifies node /a by adding a child node b
- session2 modifies node /a by adding a child node c
- session2 saves its changes 
- session1 tries to save its changes which results in a InvalidItemStateException

this behavior is in accordance with the spec. the spec however doesn't prevent a 
more lenient handling of this scenario.

if the concurrent modifications are non-conflicting and trivial the changes could
be silently merged in session1's transient state of node /a.

examples for trivial non-conflicting changes:
- s1 adds child node a, s2 removes child node b
- s1 adds child node a, s2 adds child node b
- s1 adds child node a, s2 adds mixin type

examples for non-trivial and/or conflicting changes:
- s1 removes child node a, s2 removes child node a
- s1 adds child node a, s2 adds child node a
- s1 adds sns child node a (-> a[3]), s2 adds sns child node a (-> a[3])
- s1 adds sns child node a (-> a[3]), s2 removes sns child node a[1]
- s1 removes sns child node a[2], s2 reorders child nodes affecting sns nodes a

"
0,Typo in message logged upon startup when repository is already in use. As per subject
0,"Change contrib QP API that uses CharSequence as string identifier. There are some API methods on contrib queryparser that expects CharSequence as identifier. This is wrong, since it may lead to incorrect or mislead behavior, as shown on LUCENE-2855. To avoid this problem, these APIs will be changed and enforce the use of String instead of CharSequence on version 4. This patch already deprecate the old API methods and add new substitute methods that uses only String."
0,"Remove unused LuceneQueryBuilder.createQuery() method. The following method is not used anymore in Jackrabbit and can be removed:

    public static Query createQuery(QueryRootNode root,
                                    SessionImpl session,
                                    ItemStateManager sharedItemMgr,
                                    NamespaceMappings nsMappings,
                                    Analyzer analyzer,
                                    PropertyTypeRegistry propReg)
            throws RepositoryException;"
0,"Cleanup XML QueryParser Code. Before I move the XML QueryParser to the queryparser module, I want to pass over it and bring it up to module standards."
0,"better handling of files inside/outside CFS by codec. Since norms and deletes were moved under Codec (LUCENE-3606, LUCENE-3661),
we never really properly addressed the issue of how Codec.files() should work,
considering these files are always stored outside of CFS.

LUCENE-3606 added a hack, LUCENE-3661 cleaned up the hack a little bit more,
but its still a hack.

Currently the logic in SegmentInfo.files() is:
{code}
clearCache()

if (compoundFile) {
  // don't call Codec.files(), hardcoded CFS extensions, etc
} else {
  Codec.files()
}

// always add files stored outside CFS regardless of CFS setting
Codec.separateFiles()

if (sharedDocStores) {
  // hardcoded shared doc store extensions, etc
}
{code}

Also various codec methods take a Directory parameter, but its inconsistent
what this Directory is in the case of CFS: for some parts of the index its
the CFS directory, for others (deletes, separate norms) its not.

I wonder if instead we could restructure this so that SegmentInfo.files() logic is:
{code}
clearCache()
Codec.files()
{code}

and so that Codec is instead responsible.

instead Codec.files logic by default would do the if (compoundFile) thing, and
Lucene3x codec itself would only have the if (sharedDocStores) thing, and any
part of the codec that wants to put stuff always outside of CFS (e.g. Lucene3x separate norms, deletes) 
could just use SegmentInfo.dir. Directory parameters in the case of CFS would always
consistently be the CFSDirectory.

I haven't totally tested if this will work but there is definitely some cleanups 
we can do either way, and I think it would be a good step to try to clean this up
and simplify it.
"
0,"[PATCH] Differently configured Lucene 'instances' in same JVM. Currently Lucene can be configured using system properties. When running multiple 'instances' of Lucene for different purposes in the same JVM, it is not possible to use different settings for each 'instance'.

I made changes to some Lucene classes so you can pass a configuration to that class. The Lucene 'instance' will use the settings from that configuration. The changes do not effect the API and/or the current behavior so are backwards compatible.

In addition to the changes above I also made the SegmentReader and SegmentTermDocs extensible outside of their package. I would appreciate the inclusion of these changes but don't mind creating a separate issue for them.

"
0,SPNEGO authentication scheme. Consider integrating the SPNEGO auth scheme from Commons HttpClient contrib package into HttpClient 4.0
0,Privilege content representation should be of property type NAME. the content representation of jcr privileges should reflect that fact that privilege names changed from simple string to JCR name.
0,"terminology: source uses ""protected  property"" for something that only only indirectly has to do with that term. Documentation and method names (DavProperty) use ""protected"" as pseudonym for ""return upon PROPFIND/allprop"". This isn't really accurate, because the live properties defined in RFC2518/4918 *are* protected, but are returned with PROPFIND/allprop nevertheless.

Proposal: update documentation and method names to say something like ""visibleInAllprop"".
"
0,"jUnit test-cases: success of some DocumentViewImportTest tests depends on Xerxes version being used. if e.g. xerxes v. 2.4.0 is used instead of v. 2.6.2 as specified in project.xml certain tests in DocumentViewImportTest fail.

e.g. 

Testcase: testWorkspaceImportXml(org.apache.jackrabbit.test.api.DocumentViewImportTest):	FAILED
Xml text is not correctly stored. expected:<......> but was:<...
       ...>
junit.framework.ComparisonFailure: Xml text is not correctly stored. expected:<......> but was:<...
       ...>
	at org.apache.jackrabbit.test.api.DocumentViewImportTest.checkXmlTextNode(DocumentViewImportTest.java:240)
	at org.apache.jackrabbit.test.api.DocumentViewImportTest.checkImportSimpleXMLTree(DocumentViewImportTest.java:174)
	at org.apache.jackrabbit.test.api.DocumentViewImportTest.performTests(DocumentViewImportTest.java:143)
	at org.apache.jackrabbit.test.api.DocumentViewImportTest.doTestImportXML(DocumentViewImportTest.java:115)
	at org.apache.jackrabbit.test.api.DocumentViewImportTest.testWorkspaceImportXml(DocumentViewImportTest.java:73)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at org.apache.jackrabbit.test.AbstractJCRTest.run(AbstractJCRTest.java:401)
"
0,"https should check CN of x509 cert. https should check CN of x509 cert

Since we're essentially rolling our own ""HttpsURLConnection"",  the checking provided by ""javax.net.ssl.HostnameVerifier"" is no longer in place.

I have a patch I'm about to attach which caused both createSocket() methods on o.a.h.conn.ssl.SSLSocketFactory to blowup:

test1: javax.net.ssl.SSLException: hostname in certificate didn't match: <vancity.com> != <www.vancity.com>
test2: javax.net.ssl.SSLException: hostname in certificate didn't match: <vancity.com> != <www.vancity.com>

Hopefully people agree that this is desirable.
"
0,"Update namespace uri for prefix fn. The SearchManager class still uses an outdated namespace uri for the 'fn' prefix: http://www.w3.org/2004/10/xpath-functions

The prefix should be re-mapped to the now offical namespace: http://www.w3.org/2005/xpath-functions

See: http://www.w3.org/TR/xquery-operators/#namespace-prefixes

To keep a minimum of backward compatibility, the existing namespace uri should still exist in the namespace registry, but refer to another prefix. E.g. fn_old."
0,"DirListingExportHandler: Should not implement PropertyHandler. issue found by Roland Porath:

if the DirListingExportHandler is used with some other collection nodetype that nt:folder (that may allow other properties) the list of dav properties obtained upon PROPFIND (being delegated to PropertyHandler) results in an imcomplete list.

since the only benefit of the DirListingExportHandler is to display something nice(?) upon a GET to a folder, i'd suggest to remove the implementation of PropertyHandler from the DirListingExportHandler.

angela"
0,"RFC 2965 Support (Port sensitive cookies). RFC 2109 doesn't consider port numbers when matching and filtering cookies. RFC
2965 does. Modify the Cookie class so that it (optionally?) supports RFC 2965,
while maintaining support for RFC 2109-style (portless) cookies."
0,"ACL with glob restrictions does not work on '/'. i tried to define a ACL on '/' that would allow 'read' on '/' itself, but not for the nodes underneath. i tried ""*"", ""/*"", ""./*"" but none of them seem to do the desired effect.

eg:
everyone,allow,jcr:read, '/'
everyone,deny,jcr:read, '/', glob=""/*""

the same works for a non-root node.

"
0,"httpclient doesn't read and parse response from certain types of proxy servers when POST method is used. It was determined that when sending post data to server via Squid proxy server
of version 2.4.STABLE2 and Squid responds 
with ""407 proxy authentication required"" response, httpclient doesn't read this
response in order to parse, but rather
fails with soket exception ""java.net.SocketException: Software caused connection
abort: recv failed"".

This behaviour is reproduced with the latest nigtly build of httpclient version
3.0. (from 9 of February 2005) as
well as 3.0. RC1, 2.0.2 and 2.0.

This is the piece of code that sends post data using httpclient:

try
{
	HttpClientParams httpClientParams = new HttpClientParams();
	HttpClient client = new HttpClient(httpClientParams);

	HostConfiguration hostconfig = new HostConfiguration();
	hostconfig.setProxy(""db00-devl.eps.agfa.be"", 3128); // SQUID proxy server
version 2.4.STABLE2
	client.setHostConfiguration(hostconfig);
	PostMethod postMethod = new
PostMethod(""http://brugge.eps.agfa.be/portal03/servlet/selectFiles"");

	postMethod.addParameter(""data"", ""some data"");
	int status = client.executeMethod(postMethod);
	System.out.println(""status = "" + status);
	if (status == HttpStatus.SC_OK)
		System.out.println(""Ok"");
	else if (status == HttpStatus.SC_PROXY_AUTHENTICATION_REQUIRED)
		System.out.println(""Proxy authentication required."");
}
catch (Exception e)
{
	System.out.println(""Socket exception."");
	e.printStackTrace();
}

Look at ""debug log of the problem"" attachment to see all output from httpclient
and mentioned piece of code.
In ""problem_request_response_interaction"" attacment it is possible to see
interaction beetween httpclient and Squid proxy server: httpclient sends initial
request and headers, then squid responds with ""proxy authentication required""
response and afterwards httpclient tries to send post data(without reading the
response) but fails because connection is already closed.

For more details look at ""ethereal_problem"" attachment for all network traffic
during running of mentioned piece of code:
Ethereal protocol analyzer can be used to open this file(http://www.ethereal.com/).

Most likely this particular version of Squid closes connection after it sends
proxy athentication response back,
which causes httpclient to fail while sending post data.

Let's have a look at what writeRequest(...) method of HttpMethodBase class does:

1) sends request line and headers to server,
2) handles 'Expect: 100-continue' handshake if needed,
3) sends post data to server.

My question is should HTTPClient send initial request and headers before data
even if it is not going to read 
a response from the server(proxy server), or this should be done only in case of
'Expect: 100-continue' handshake 
(this seems the only case when HTTPClient is going to listen to server
in-between of steps 1 and 3)?

My understanding is that the command

        // make sure the status line and headers have been sent
        conn.flushRequestOutputStream();
        
which actually splits sending of data in two parts are needed only for 'Expect:
100-continue' handshake case.
Just by moving ""flush"" command to appropriate place inside 'Expect:
100-continue' handshake case:
		.....
                try {
	            conn.flushRequestOutputStream(); // moved
                    conn.setSocketTimeout(RESPONSE_WAIT_TIME_MS);
		.....
it is posible to solve described problem.

I created PostMethodEx that overrides writeRequest(...) method of
HttpMethodBase(look at ""PostMethodEx"" attachment) 
and for all cases but the 'Expect: 100-continue' handshake it sends request
line, headers and post data to server 
at once.

When mentioned piece of code(with PostMethod changed to PostMethodEx) is
executed everyting works fine:
look at ""debug log of the fix"", ""fix_request_response_interaction"" and
""ethereal_fix""(all network trafic) 
attachments.

According to mentioned logs httpclient sends all post data at once and then
reads and parses ""proxy authentication required"" 
response from squid and sets status code to 407. Correct."
0,"Connection timeout logic redesign. Changelog:

* CreateSocket method with timeout parameter added to the ProtocolSocketFactory
interface

* TimeoutController related code factored out of HttpConnection class and moved
into ControllerThreadSocketFactory helper class

* ReflectionSocketFactory helper class added. This factory encapsulates
reflection code to call JDK 1.4 Socket#connect method if supported

* All protocol socket factories now attempt to initially use
ReflectionSocketFactory if required to create a socket within a given limit of
time. If reflection fails protocol socket factories fall back onto the good ol'
ControllerThreadSocketFactory

Benefits:

* HttpConnection code got a lot cleaner
* When running in modern JREs expensive timeout controller thread per connection
attempt is no longer needed
* Ugly code intended to work around limitations of the older JREs is now
confined to a few helper classes that can be easily thrown away once we move
onto Java 1.4

Let me know what you think

Oleg"
0,"bad assumptions on QueryResult.getIterator() semantics in QueryResultNodeIteratorTest.testSkip(). testSkip() assumes that calling getIterator() a second time will return a new iterator of the same size. JSR-170 is silent on this. Forcing a server to implement this essantially means that the query result must be cached until there's no reference to QueryResult anymore.

As this is a test of skip(), not getIterator(), the test should really refetch a new QueryResult in order to obtain a new iterator.

(Note: The issue of the semantics of QueryResult.getIterator should be discussed by the JCR EG.)
"
0,"Improved XML export handling. As mentioned in JCR-1574, the current XML export functionality is generating workarounds like the new PropertyWrapper class. I'd like to refactor and clean up the XML export stuff so that such workarounds wouldn't be needed.

An additional bonus would be to make both core and jcr2spi use the same XML export mechanism. For example the one in core already supports JSR 283 shareable nodes, but the one in jcr2spi does not."
0,"Test class in the main source tree. org.apache.jackrabbit.core.TestRepository is in the main source folder
(src/main/java) instead of the test folder (src/test/java).

The build of jackrabbit-core is successful even if I move the class
to the test folder, so it looks like it was just a mistake."
0,"cache does not allow client to override origin-specified freshness using max-stale. According to the RFC, the default freshness lifetime is supposed to be the LEAST restrictive of that specified by the origin, the client, and the cache. Right now, a client can't use 'max-stale' to relax the freshness constraints to get a cache hit without validation occuring first.
"
0,"Parallelize tests. As mentioned on the mailing list I'd like to parallelize test execution.

There will be a pool of RepositoryHelper instances, each represents a distinct repository. This ensures that test cases that run in parallel do not interfere with each other. I suggest we start with a pool of two repositories, but we can later extend this setup."
0,"it is impossible to use a custom dictionary for SmartChineseAnalyzer. it is not possible to use a custom dictionary, even though there is a lot of code and javadocs to allow this.

This is because the custom dictionary is only loaded if it cannot load the built-in one (which is of course, in the jar file and should load)
{code}
public synchronized static WordDictionary getInstance() {
    if (singleInstance == null) {
      singleInstance = new WordDictionary(); // load from jar file
      try {
        singleInstance.load();
      } catch (IOException e) { // loading from jar file must fail before it checks the AnalyzerProfile (where this can be configured)
        String wordDictRoot = AnalyzerProfile.ANALYSIS_DATA_DIR;
        singleInstance.load(wordDictRoot);
      } catch (ClassNotFoundException e) {
        throw new RuntimeException(e);
      }
    }
    return singleInstance;
  }
{code}

I think we should either correct this, document this, or disable custom dictionary support..."
0,"Document the temporary free space requirements of IndexWriter methods. Just opening an issue to track fixes to javadocs around Directory
space usage of optimize(), addIndexes(*), addDocument.

This came out of a recent thread on the users list around unexpectedly
high temporary disk usage during optimize():

  http://www.gossamer-threads.com/lists/lucene/java-user/43475

"
0,"Simple toString() for BooleanFilter. While working with BooleanFilter I wanted a basic toString() for debugging.

This is what I came up.  It works ok for me."
0,"Stats for the PersistenceManager. Statistics for the PersistenceManager impl that cover: 
 - bundle cache access count, 
 - bundle cache miss count, 
 - bundle cache miss avg duration (this avg includes the penalty of having to load from the underlying storage / can be interpreted as avg read latency as there is no cache involved) 
 - bundle writes per second

What it doesn't cover is :
 - number of bundles
 - size of workspace
as these are values that are expensive to compute on demand, and caching them would imply being able to store the values (which is not possible currently)."
0,"bogus javadocs for FieldValueHitQuery.fillFields. FieldValueHitQuery.fillFields has javadocs that seem to be left over from a completely different method...

{code}
  /**
   * Given a FieldDoc object, stores the values used to sort the given document.
   * These values are not the raw values out of the index, but the internal
   * representation of them. This is so the given search hit can be collated by
   * a MultiSearcher with other search hits.
   * 
   * @param doc
   *          The FieldDoc to store sort values into.
   * @return The same FieldDoc passed in.
   * @see Searchable#search(Weight,Filter,int,Sort)
   */
  FieldDoc fillFields(final Entry entry) {
    final int n = comparators.length;
    final Comparable[] fields = new Comparable[n];
    for (int i = 0; i < n; ++i) {
      fields[i] = comparators[i].value(entry.slot);
    }
    //if (maxscore > 1.0f) doc.score /= maxscore;   // normalize scores
    return new FieldDoc(entry.docID, entry.score, fields);
  }

{code}"
0,"JSR 283: new methods on NodeType. JSR 283 adds the methods

  NodeType.getSubtypes

and

  NodeType.getDeclaredSubtypes"
0,"Move 'good' contrib/queries classes to Queries module. With the Queries module now filled with the FunctionQuery stuff, we should look at closing down contrib/queries.  While not a huge contrib, it contains a number of pretty useful classes and some that should go elsewhere.

Heres my proposed plan:

- similar.* -> suggest module
- regex.* -> queries module
- BooleanFilter -> queries module under .filters package
- BoostingQuery -> queries module
- ChainedFilter -> queries module under .filters package
- DuplicateFilter -> queries module under .filters package
- FieldCacheRewriteMethod -> This doesn't belong in this contrib or the queries module.  I think we should push it to contrib/misc for the time being.  It seems to have quite a few constraints on when its useful.  If indeed CONSTANT_SCORE_AUTO rewrite is better, then I dont see a purpose for it.
- FilterClause -> class inside BooleanFilter
- FuzzyLikeThisQuery -> suggest module. This class seems a mess with its Similarity hardcoded.  With all that said, it does seem to do what it claims and with some cleanup, it could be good.
- TermsFilter -> queries module under .filters package
- SlowCollated* -> They can stay in the module till we have a better place to nuke them.

One of the implications of the above moves, is that the xml-query-parser, which supports many of the queries, will need to have a dependency on the queries module.  But that seems unavoidable at this stage.



"
0,Improve Memory Consumption for merging DocValues SortedBytes variants. Currently SortedBytes are loaded into memory during merge which could be a potential trap. Instead of loading them into Heap memory we can merge those sorted values with much smaller memory and without loading all values into ram.
0,"jcr-commons: Add utility to translate a string to a AuthorizableQuery and execute it on the user manager . it would be convenient if jackrabbit-jcr-commons would provide a utility to generate authorizable
queries from a string."
0,"Avoid item state reads during Session.logout(). This is a follow up issue for JCR-2231. There's a second CachingHierarchyManager attached to the LocalItemStateManager, which it unregistered too late and causes reads on the SharedItemStateManager on Session logout. The hierarchy manager should be unregistered as listener before the state manager is disposed."
0,"add Galician analyzer. Adds analyzer for Galician, based upon [""Regras do lematizador para o galego""|http://bvg.udc.es/recursos_lingua/stemming.jsp] , and a set of stopwords created in the usual fashion.

This is really just an adaptation of the Portuguese [RSLP|http://www.inf.ufrgs.br/~viviane/rslp/index.htm], so I added that too, and modified our existing hand-coded RSLP-S (RSLP's plural-only step) to just be a plural-only flow of RSLP.
"
0,"pass liveDocs Bits down in scorercontext, instead of Weights pulling from the reader . Spinoff from LUCENE-1536, this would allow filters to work in a more flexible way (besides just cleaning up)"
0,"wrong assumptions in test cases about lock tokens. Several test cases assume that Lock.getLockToken has to return null for locks not attached to the current session. However, this is optional. Citing the Javadoc for getLockToken:

     * May return the lock token for this lock. If this lock is open-scoped and
     * the current session either holds the lock token for this lock, or the
     * repository chooses to expose the lock token to the current session, then
     * this method will return that lock token. Otherwise this method will
     * return <code>null</code>."
0,"LargeDocHighlighter - another span highlighter optimized for large documents. The existing Highlighter API is rich and well designed, but the approach taken is not very efficient for large documents.

I believe that this is because the current Highlighter rebuilds the document by running through and scoring every every token in the tokenstream.

With a break in the current API, an alternate approach can be taken: rebuild the document by running through the query terms by using their offsets. The benefit is clear - a large doc will have a large tokenstream, but a query will likely be very small in comparison.

I expect this approach to be quite a bit faster for very large documents, while still supporting Phrase and Span queries.

First rough patch to follow shortly."
0,"migrate to commons-codec Base64. Commons Codec is now the authoritative source for Base64 functionality.  The
Base64 in HttpClient is now deprecated and should be removed in 2.1.  This will
also add a new dependancy for HttpClient on the commons-codec package."
0,"Track total term freq per term. Right now we track docFreq for each term (how many docs have the
term), but the totalTermFreq (total number of occurrences of this
term, ie sum of freq() for each doc that has the term) is also a
useful stat (for flex scoring, PulsingCodec, etc.).
"
0,Add a filtered RangeIterator. It would be useful to have a FilteredRangeIterator utility class that can be used to apply arbitrary filters on other RangeIterators.
0,"Remove getSafeJCRPath methods in HierarchyManagerImpl. The getSafeJCRPath utility methods in the HierarchyManagerImpl class have not been used since revision 485720, but their presence still causes the hierarchy managers to depend on namespace mapping information. I'll remove the methods to simplify things."
0,"Index nodes in parallel. CPUs with multiple cores are now standard and Jackrabbit should make use of it where it makes sense. Analyzing content while a node is indexed is quite costly, but can be broken easily into task that can be executed in parallel. E.g. index multiple nodes in parallel."
0,"Allow to plug in a Cache Eviction Listener to IndexReader to eagerly clean custom caches that use the IndexReader (getFieldCacheKey). Allow to plug in a Cache Eviction Listener to IndexReader to eagerly clean custom caches that use the IndexReader (getFieldCacheKey).

A spin of: https://issues.apache.org/jira/browse/LUCENE-2468. Basically, its make a lot of sense to cache things based on IndexReader#getFieldCacheKey, even Lucene itself uses it, for example, with the CachingWrapperFilter. FieldCache enjoys being called explicitly to purge its cache when possible (which is tricky to know from the ""outside"", especially when using NRT - reader attack of the clones).

The provided patch allows to plug a CacheEvictionListener which will be called when the cache should be purged for an IndexReader."
0,"RepositoryLock does not work on NFS sometimes. The RepositoryLock mechanism currently used in Jackrabbit uses FileLock. This doesn't work on some NFS file system. It looks like only NFS version 4 and newer supports locking. Older implementations may throw a IOException ""No locks available"", which means the NFS does not support byte-range locking.

I propose to add a second locking mechanism, and add a configuration option to use it. For example: <FileLocking class=""acme"" />. This second locking mechanism is a cooperative locking protocol that uses a background (watchdog) thread and only uses regular file operations.

"
0,Add user manager performance tests . We should add some performance tests for validating JCR-2710 and related. That is we should measure performance for creating users and groups and adding/removing users to/from groups. This should be done for both repository configurations: one with the old content model (group membership in property) and one with the new content model introduced with JCR-2710 (group membership in b-tree like node structure). 
0,"Use of google-code-prettify for Lucene/Solr Javadoc. My company, RONDHUIT uses google-code-prettify (Apache License 2.0) in Javadoc for syntax highlighting:

http://www.rondhuit-demo.com/RCSS/api/com/rondhuit/solr/analysis/JaReadingSynonymFilterFactory.html

I think we can use it for Lucene javadoc (java sample code in overview.html etc) and Solr javadoc (Analyzer Factories etc) to improve or simplify our life."
0,"Test cases for all FileSystem implementations. Currently we only have test cases for MemoryFileSystem, but those tests could easily be generalized to cover also the other FileSystem implementations."
0,"support for DB2 in BundleDbPersistenceManager. BundleDbPersistenceManager doesn't work with DB2, db2.ddl file is missing.I've created the database scheme for DB2."
0,"Move Solr's FunctionQuery impls to Queries Module. Now that we have the main interfaces in the Queries module, we can move the actual impls over.

Impls that won't be moved are:

function/distance/* (to be moved to a spatial module)
function/FileFloatSource.java (depends on Solr's Schema, data directories and exposes a RequestHandler)"
0,"Numeric range searching with large value sets. I have a set of enhancements that build on the numeric sorting cache introduced
by Tim Jones and that provide integer and floating point range searches over
numeric ranges that are far too large to be implemented via the current term
range rewrite mechanism.  I'm new to Apache and trying to find out how to attach
the source files for the changes for your consideration."
0,"HTTP Version configuration and tracking. HTTP version tracking is currently oversimplified with a single http11 boolean. 
Extend this to handle any http version simply, and efficiently.

Possible suggestion:
> get rid of setHttp11() an isHttp11
> void setHttpVersion(String version)
> String getHttpVersion()
> boolean isHttpVersion(String version)"
0,"Make Explanation include information about match/non-match. As discussed, I'm looking into the possibility of improving the Explanation class to include some basic info about the ""match"" status of the Explanation -- independent of the value...

http://www.nabble.com/BooleanWeight.normalize%28float%29-doesn%27t-normalize-prohibited-clauses--t1596471.html#a4347644

This is neccesary to deal with things like LUCENE-451"
0,"FastVectorHighlighter: add a method to set an arbitrary char that is used when concatenating multiValued data. If the following multiValued names are in authors field:

* Michael McCandless
* Erik Hatcher
* Otis Gospodnetić

Since FragmentsBuilder concatenates multiValued data with a space in BaseFragmentsBuilder.getFragmentSource():

{code}
while( buffer.length() < endOffset && index[0] < values.length ){
  if( index[0] > 0 && values[index[0]].isTokenized() && values[index[0]].stringValue().length() > 0 )
    buffer.append( ' ' );
  buffer.append( values[index[0]++].stringValue() );
}
{code}

an entire field snippet (using LUCENE-2464) will be ""Michael McCandless Erik Hatcher Otis Gospodnetić"". There is a requirement an arbitrary char (e.g. '/') can be set so that client can separate the snippet easily. i.e. ""Michael McCandless/Erik Hatcher/Otis Gospodnetić"""
0,"investigate solr test failures using flex. We have a branch of Solr located here: https://svn.apache.org/repos/asf/lucene/solr/branches/solr

Currently all the tests pass with lucene trunk jars.

I plopped in the flex jars and they do not, so I thought these might be interesting to look at.
"
0,"Remove superfluous comment in MMapDirectory.java. See title, and I prefer my name to be removed from the source code."
0,"Remove/deprecate Tokenizer's default ctor. I was working on a new Tokenizer... and I accidentally forgot to call super(input) (and super.reset(input) from my reset method)... which then meant my correctOffset() calls were silently a no-op; this is very trappy.

Fortunately the awesome BaseTokenStreamTestCase caught this (I hit failures because the offsets were not in fact being corrected).

One minimal thing we can do (but it sounds like from Robert there may be reasons why we can't) is add {{assert input != null}} in Tokenizer.correctOffset:

{noformat}
Index: lucene/core/src/java/org/apache/lucene/analysis/Tokenizer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/analysis/Tokenizer.java	(revision 1242316)
+++ lucene/core/src/java/org/apache/lucene/analysis/Tokenizer.java	(working copy)
@@ -82,6 +82,7 @@
    * @see CharStream#correctOffset
    */
   protected final int correctOffset(int currentOff) {
+    assert input != null: ""subclass failed to call super(Reader) or super.reset(Reader)"";
     return (input instanceof CharStream) ? ((CharStream) input).correctOffset(currentOff) : currentOff;
   }
{noformat}

But best would be to remove the default ctor that leaves input null..."
0,"Improve reusability of AbstractRepositoryService and AbstractReadableRepositoryService. Much of the functionality in AbstractReadableRepositoryService is not specific to reading but rather applies to any implementation (node types, name spaces, descriptors). I suggest to pull this functionality up from AbstractReadableRepositoryService to AbstractRepositoryService"
0,"Manage Lucene FieldCaches per index segment. Jackrabbit uses an IndexSearcher which searches on a single IndexReader which is most likely to be an instance of CachingMultiReader. On every search that does sorting or range queries a FieldCache is populated and associated with this instance of a CachingMultiReader. On successive queries which operate on this CachingMultiReader you will get a tremendous speedup for queries which can reuse  those associated FieldCache instances.
The problem is that Jackrabbit creates a new CachingMultiReader _everytime_ one of the underlying indexes are modified. This means if you just change _one_ item in the repository you will need to rebuild all those FieldCaches because the existing FieldCaches are associated with the old instance of CachingMultiReader.
This does not only lead to slow search response times for queries which contains range queries or are sorted by a field but also leads to massive memory consumption (depending on the size of your indexes) because there might be multiple instances of CachingMultiReaders in use if you have a scenario where a lot of queries and item modifications are executed concurrently.
The goal is to keep those FieldCaches as long as possible."
0,"Path should not be encoded in HttpMethodBase. I suggest to change the protocol or add a new method for this one

protected static String generateRequestLine(HttpConnection connection,
	String name, String reqPath,
	String qString, String protocol);
so that we can choose to use URIUtil.encode(reqPath, URIUtil.pathSafe()) or not

The reason is that after the encoding process, some server cannot recognize this
Actually, I am handling a project of the Method Propfind(for getting mail from 
Hotmail) and I find that the restriction of Hotmail server is quite high, and 
if the address is encoded, it does not work."
0,New QueryParser should not allow leading wildcard by default. The current QueryParser disallows leading wildcard characters by default.
0,"Allow reuse of Q*DefinitionBuilder in QItemDefinitionsBuilder. It would be nice to reuse the builder implementations in QItemDefinitionsBuilder elsewhere when Q*Definitions need to be built.

I will extract the relevant classes so they can be used independently and make QItemDefinitionsBuilder use them."
0,Change all contrib TokenStreams/Filters to use the new TokenStream API. Now that we have the new TokenStream API (LUCENE-1422) we should change all contrib modules to use it.
0,"Javadoc of TokenStream.end() somehow confusing. The Javadocs of TokenStream.end() are somehow confusing, because they also refer to the old TokenStream API (""after next() returned null""). But one who implements his TokenStream with the old API cannot make use of the end() feature, as he would not use attributes and so cannot update the end offsets (he could, but then he should rewrite the whole TokenStream). To be conform to the old API, there must be an end(Token) method, which we will not add.

I would drop the old API from this docs."
0,"Remove redundant RepositoryService.executeQuery() method . There are currently two executeQuery() methods on RepositoryService. For simplicity we should remove the one that assumes default values for limit, offset and bind variable values."
0,"web site for download of 3.1 fails. Down loading version 3 is not possible.
Documentation for 4. does not match 3."
0,"[PATCH] remove code stutter. Method calls getQName for no reason


public String getName() throws RepositoryException {
        checkStatus();
        Name qName = getQName();
        return session.getNameResolver().getJCRName(getQName());
    }

patch fixes it."
0,"Demo HTML parser gives incorrect summaries when title is repeated as a heading. If you have an html document where the title is repeated as a heading at the top of the document, the HTMLParser will return the title as the summary, ignoring everything else that was added to the summary. Instead, it should keep the rest of the summary and chop off the title part at the beginning (essentially the opposite). I don't see any benefit to repeating the title in the summary for any case.

In HTMLParser.jj's getSummary():

    String sum = summary.toString().trim();
    String tit = getTitle();
    if (sum.startsWith(tit) || sum.equals(""""))
      return tit;
    else
      return sum;

change it to: (* denotes a line that has changed)

    String sum = summary.toString().trim();
    String tit = getTitle();
*    if (sum.startsWith(tit))             // don't repeat title in summary
*      return sum.substring(tit.length()).trim();
    else
      return sum;
"
0,"Change DateTools to not create a Calendar in every call to dateToString or timeToString. DateTools creates a Calendar instance on every call to dateToString and timeToString. Specifically:

# timeToString calls Calendar.getInstance on every call.
# dateToString calls timeToString(date.getTime()), which then instantiates a new Date(). I think we should change the order of the calls, or not have each call the other.
# round(), which is called from timeToString (after creating a Calendar instance) creates another (!) Calendar instance ...

Seems that if we synchronize the methods and create the Calendar instance once (static), it should solve it."
0,"DirectoryTaxonomyWriter should throw a proper exception if it was closed. DirTaxoWriter may throw random exceptions (NPE, Already Closed - depend on what API you call) after it has been closed/rollback. We should detect up front that it is already closed, and throw AlreadyClosedException.

Also, on LUCENE-3573 Doron pointed out a problem with DTW.rollback -- it should call close() rather than refreshReader. I will fix that as well in this issue."
0,"Allow setting arbitrary objects on PerfRunData. PerfRunData is used as the intermediary objects between PerfRunTasks. Just like we can set IndexReader/Writer on it, it will be good if it allows setting other arbitrary objects that are e.g. created by one task and used by another.

A recent example is the enhancement to the benchmark package following the addition of the facet module. We had to add TaxoReader/Writer.

The proposal is to add a HashMap<String, Object> that custom PerfTasks can set()/get(). I do not propose to move IR/IW/TR/TW etc. into that map. If however people think that we should, I can do that as well."
0,Replace TrackingInpuStream with Commons IO. The TrackingInputStream class in jackrabbit-core implements essentially the same functionality as the Commons IO class CountingInputStream.
0,"Disallow setBoost() on StringField, throw exception if boosts are set if norms are omitted. Occasionally users are confused why index-time boosts are not applied to their norms-omitted fields.

This is because we silently discard the boost: there is no reason for this!

The most absurd part: in 4.0 you can make a StringField and call setBoost and nothing complains... (more reasons to remove StringField totally in my opinion)"
0,"Complete parallelizaton of ParallelMultiSearcher. ParallelMultiSearcher is parallel only for the method signatures of 'search'.

Part of a query process calls the method docFreq(). There was a TODO comment to parallelize this. Parallelizing this method actually increases the performance of a query on multiple indexes, especially remotely.

"
0,"FST.BYTE2 should save as fixed 2 byte not as vInt. We currently write BYTE1 as a single byte, but BYTE2/4 as vInt, but I think that's confusing.  Also, for the FST for the new Kuromoji analyzer (LUCENE-3305), writing as 2 bytes instead shrank the FST and ran faster, presumably because more values were >= 16384 than were < 128.

Separately the whole INPUT_TYPE is very confusing... really all it's doing is ""declaring"" the allowed range of the characters of the input alphabet, and then the only thing that uses that is the write/readLabel methods (well and some confusing sugar methods in Builder!).  Not sure how to fix that yet...

It's a simple change but it changes the FST binary format so any users w/ FSTs out there will have to rebuild (FST is marked experimental...).
"
0,"Upgrade benchmark from commons-compress-1.0 to commons-compress-1.1 for 15 times faster gzip decompression. In LUCENE-1540 TrecContentSource moved from Java's GZipInputStream to common-compress 1.0. 
This slowed down gzip decompression by a factor of 15. 
Upgrading to 1.1 solves this problem.
I verified that the problem is only in GZIP, not in BZIP.
On the way, as 1.1 introduced constants for the compression methods, the code can be made a bit nicer."
0,"Demo targets for running the demo. Now that the demo build targets are working and build the jar/war, it may be useful for users to also be able to run the demo with something like 'ant run-demo'. This complements existing docs/demo.html."
0,"Highlighter should support all MultiTermQuery subclasses without casts. In order to support MultiTermQuery subclasses the Highlighter component applies instanceof checks for concrete classes from the lucene core. This prevents classes like RegexQuery in contrib from being supported. Introducing dependencies on other contribs is not feasible just for being supported by the highlighter.

While the instanceof checks and subsequent casts might hopefully go somehow away  in the future but for supporting more multterm queries I have a alternative approach using a fake IndexReader that uses a RewriteMethod to force the MTQ to pass the field name to the given reader without doing any real work. It is easier to explain once you see the patch - I will upload shortly.
"
0,SPI: prefer 'Iterator' instead of specialized subclasses. in the F2F we agreed that the SPI should rather use 'Iterator' instead of specialized subclassed (or RangeIterator).
0,"discrepancy in getTermFreqVector-methods . getTermFreqVector(int, TermVectorMapper) never calls the mapper if there is no term vector, consitent with all the other getTermFreqVector methods that returns null. 

getTermFreqVector(int, String, TermVectorMapper) throws an IOException when a field does not contain the term vector.

My suggestion:

{code}
Index: src/java/org/apache/lucene/index/SegmentReader.java
===================================================================
--- src/java/org/apache/lucene/index/SegmentReader.java (revision 590149)
+++ src/java/org/apache/lucene/index/SegmentReader.java (working copy)
@@ -648,7 +648,7 @@
     ensureOpen();
     FieldInfo fi = fieldInfos.fieldInfo(field);
     if (fi == null || !fi.storeTermVector || termVectorsReaderOrig == null)
-      throw new IOException(""field does not contain term vectors"");
+      return; 
{code}"
0,Add a ton of missing license headers throughout test/demo/contrib. 
0,"DocId.UUIDDocId should not have a string attr uuid. After JCR-1213 will be solved, lots of DocId.UUIDDocId can be cached, and not being cleaned after every gc(). The number of cached UUIDDocId can grow very large, depending on the size of the repository.  Therefor, instead of storing the private String uuid; we can make it more memory efficient by storing 2 long's, the lsb and msb of the uuid.  Storing 1.000.000 of parent UUIDDocId might differ about 100Mb of memory. 

I even did test by removing the entire uuid string, and not use msb or lsb, because, when everything works properly (with references to index reader segments (See JCR-1213)), the uuid is never needed again: in 

UUIDDocId getDocumentNumber(IndexReader reader) throws IOException {

we could set uuid = null just before the return. It works perfectly well, because when an index reader is recreated, the CachingIndexReader will be recreated, hence DocId[] parents will be recreated. 

So, IMO, I think we might be able to remove the uuid entirely when the docNumber is found in DocId.UUIDDocId (obviously after JCR-1213)

WDOT?

"
0,"IntelliJ IDEA and Eclipse setup. Setting up Lucene/Solr in IntelliJ IDEA or Eclipse can be time-consuming.

The attached patches add a new top level directory {{dev-tools/}} with sub-dirs {{idea/}} and {{eclipse/}} containing basic setup files for trunk, as well as top-level ant targets named ""idea"" and ""eclipse"" that copy these files into the proper locations.  This arrangement avoids the messiness attendant to in-place project configuration files directly checked into source control.

The IDEA configuration includes modules for Lucene and Solr, each Lucene and Solr contrib, and each analysis module.  A JUnit run configuration per module is included.

The Eclipse configuration includes a source entry for each source/test/resource location and classpath setup: a library entry for each jar.

For IDEA, once {{ant idea}} has been run, the only configuration that must be performed manually is configuring the project-level JDK.  For Eclipse, once {{ant eclipse}} has been run, the user has to refresh the project (right-click on the project and choose Refresh).

If these patches is committed, Subversion svn:ignore properties should be added/modified to ignore the destination IDEA and Eclipse configuration locations.

Iam Jambour has written up on the Lucene wiki a detailed set of instructions for applying the 3.X branch patch for IDEA: http://wiki.apache.org/lucene-java/HowtoConfigureIntelliJ"
0,JSR 283 Property Types. 
0,"Add SimpleFragListBuilder constructor with margin parameter. {{SimpleFragListBuilder}} would benefit from an additional constructor that takes in {{margin}}. Currently, the margin is defined as a constant, so to ""implement"" a {{FragListBuilder}} with a different margin, one has no choice but to copy and paste {{SimpleFragListBuilder}} into a new class that must be placed in the {{org.apache.lucene.search.vectorhighlight}} package due to accesses of package-protected fields in other classes.

If this change were made, the precondition check of the constructor's {{fragCharSize}} should probably be altered to ensure that it's less than {{max(1, margin*3)}} to allow for a margin of 0."
0,"Add ability to open prior commits to IndexReader. If you use a customized DeletionPolicy, which keeps multiple commits
around (instead of the default which is to only preserve the most
recent commit), it's useful to be able to list all such commits and
then open a reader against one of these commits.

I've added this API to list commits:

  public static Collection IndexReader.listCommits(Directory)

and these two new open methods to IndexReader to open a specific commit:

  public static IndexReader open(IndexCommit)
  public static IndexReader open(IndexCommit, IndexDeletionPolicy)

Spinoff from here:

  http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200806.mbox/%3c85d3c3b60806161735o207a3238sa2e6c415171a8019@mail.gmail.com%3e

"
0,"crank up faceting module tests. The faceting module has a large set of good tests.

lets switch them over to use all of our test infra (randomindexwriter, random iwconfig, mockanalyzer, newDirectory, ...)
I don't want to address multipliers and atLeast() etc on this issue, I think we should follow up with that on a separate issue, that also looks at speed and making sure the nightly build is exhaustive.

for now, lets just get the coverage in, it will be good to do before any refactoring.
"
0,"implement PERSIST events for the EventJournal. See <http://www.day.com/specs/jcr/2.0/12_Observation.html#12.6.3%20Event%20Bundling%20in%20Journaled%20Observation>

"
0,"New utility: Journal walker for journal files. Given the cluster record access provided by JCR-1789, add a journal walker utiltity that provides descriptive information about the contents of journal records."
0,"HttpRoutePlanner based on ProxySelector. Since we now require Java 5, we should have a route planner that uses the standard Java ProxySelector. That would allow us to automatically pick up proxy settings from system properties or the browser running an applet.
"
0,"wrong class name in statemgmt.xml. ""BasicClientCookie"" should read ""BasicCookieStore"", see the patch for details"
0,"NodeImpl.checkout() calls save() two times. Similar to JCR-975, The version related properties on a versionable node that is checked out are saved individually. There is no need to save them individually because checkd in node must not have pending changes and save() can be called safely on the node itself."
0,"Use common base classes in jackrabbit-core and jcr2spi. As part of JCR-742 I've implemented a number of generic JCR base classes and adapters in org.apache.jackrabbit.commons. These classes are based on existing code in jackrabbit-core.

To encourage code reuse across jackrabbit-core and jcr2spi, I'd like to make both components use these generic base classes.

"
0,"Add @Override annotations. During removal of deprecated APIs, mostly the problem was, to not only remove the method in the (abstract) base class (e.g. Scorer.explain()), but also remove it in sub classes that override it. You can easily forget that (especially, if the method was not marked deprecated in the subclass). By adding @Override annotations everywhere in Lucene, such removals are simple, because the compiler throws out an error message in all subclasses which then no longer override the method.

Also it helps preventing the well-known traps like overriding hashcode() instead of hashCode().

The patch was generated automatically, and is rather large. Should I apply it, or would it break too many patches (but I think, trunk has changed so much, that this is only a minimum of additional work to merge)?"
0,"FieldCache introspection API. FieldCache should expose an Expert level API for runtime introspection of the FieldCache to provide info about what is in the FieldCache at any given moment.  We should also provide utility methods for sanity checking that the FieldCache doesn't contain anything ""odd""...
   * entries for the same reader/field with different types/parsers
   * entries for the same field/type/parser in a reader and it's subreader(s)
   * etc...


"
0,"French elision filter should use CharArraySet. French elision filter creates new strings, lowercases them, etc just to check against a Set<String>.
trivial patch to use chararrayset instead."
0,"Unnecessary parsing of Name value. InternalValue.toJCRValue(NamePathResolver) formats the Name value and constructs a NameValue using the formatted String. The implementation of NameValue.valueOf(String) again checks the format, which is quite expensive and unnecessary in this case."
0,"PhraseQuery/TermQuery/SpanQuery use IndexReader specific stats in their explains. PhraseQuery uses IndexReader in explainfor top level stats - as mentioned by Mike McCandless in LUCENE-1837.
TermQuery uses IndexReader in explain for top level stats

Always been a bug with MultiSearcher, but per segment search makes it worse.

"
0,"Change default value for respectDocumentOrder. The current default value for the search index configuration parameter respectDocumentOrder is true. Almost all applications are not interested in document order, while this default adds significant overhead to query execution because document order information is present in the index but has to be calculated over the complete result set.

I propose to change the default value to false and document this change in the 1.4 release notes. If an application relies on document order one can still explicitly set the parameter in the configuration to true."
0,"nightly build failed. javadoc tasked failed due to new project structure in contrib/gdata-server
added correct package structure to java/trunk/build.xml

javadoc creation successful.

Patch added as attachment.

regards simon"
0," RAMDirectory(Directory dir, boolean closeDir)  constructor uses memory inefficiently.. recently I found that  RAMDirectory(Directory dir, boolean closeDir)  constructor uses memory inefficiently.
files from source index are read entirely intro memory as single byte array which is after all is thrown away. And if I want to load my 200M optimized, compound format index to memory for faster search I should give JVM at least 400Mb memory limit. For larger indexes this can be an issue.

I've attached patch how to solve this problem."
0,"Build environment configuration: Mavenize the build process. - ease of building. HttpClient should be buildable without the user needing to
go away and download extra jars. Maven does a good job of this.
- automated site and build on a nightly basis to pick up changes
- move to j2sdk1.4"
0,"Update copyright years in READMEs and NOTICEs. The README.txt files of Jackrabbit components contain copyright lines like this:

    Collective work: Copyright 2007 The Apache Software Foundation.

The year should be updated."
0,"specialize payload processing from of DocsAndPositionsEnum. In LUCENE-2760 i started working to try to improve the speed of a few spanqueries.
In general the trick there is to avoid processing positions if you dont have to.

But, we can improve queries that read lots of positions further by cleaning up SegmentDocsAndPositionsEnum, 
in nextPosition() this has no less than 3 payloads-related checks.

however, a large majority of users/fields have no payloads at all.
I think we should specialize this case into a separate implementation and speed up the common case.

edit: dyslexia with the jira issue number."
0,"TCK: NodeTest#testMixinTypesProtected incorrectly fails. The test calls addMixin to add mix:referenceable to a node.  This step is not required to test that jcr:mixinTypes is protected, yet may fail if the node is already mix:referenceable or cannot be mix:referenceable.

Proposal: remove the call to addMixin.

The test attempts to set jcr:mixinTypes to a scalar value, but reports a failure if the implementation throws ValueFormatException instead of ConstraintViolationException.

Proposal: set jcr:mixinTypes to an array of length 1.

--- NodeTest.java       (revision 422074)
+++ NodeTest.java       (working copy)
@@ -1130,10 +1142,9 @@
         Node defaultRootNode = (Node) superuser.getItem(testRootNode.getPath());
  
         Node testNode = defaultRootNode.addNode(nodeName1, testNodeType);
-        testNode.addMixin(mixReferenceable);
  
         try {
-            testNode.setProperty(jcrMixinTypes,mixLockable);
+            testNode.setProperty(jcrMixinTypes, new String[] { mixLockable });
             fail(""Manually setting jcr:mixinTypes should throw a ConstraintViolationException"");
         }
         catch (ConstraintViolationException success) {
"
0,"Move repository home directory into target directory. Currently the spi test client uses the repository home directory from the jackrabbit-core module. Instead it should use its own repository home, preferably in the target directory which can be reset using the maven clean goal."
0,"Remove BitSet caching from QueryFilter. Since caching is built into the public BitSet bits(IndexReader reader)  method, I don't see a way to deprecate that, which means I'll just cut it out and document it in CHANGES.txt.  Anyone who wants QueryFilter caching will be able to get the caching back by wrapping the QueryFilter in the CachingWrapperFilter."
0,"TCK: SerializationTest.helpTestSaxException casts ContentHandler to DefaultHandler. the JSR170 defines import with ContentHandler (see Session.getImportContentHandler, Workspace.getImportContentHandler)

the mentioned helper method within the TCK casts the ContentHandler returned by those methods to DefaultHandler without testing if the contenthandler is a DefaultHandler (line 273)"
0,"Exclude system index for queries that restrict the result set to nodetypes not availble in the ""jcr:system"" subtree. We already have code that is able to decide whether the system index needs to be included in a search or not (see JCR-967). If I execute a query like ""my:app//element(*, my:doc)"" this will only search the workspace index. Unfortunately this is slower than ""//element(*, my:doc)"", since the first query can not be optimized as the second. In our case both queries return the same result set because we use application specific node types. Even though the second query includes the system index it is still faster than the first one. But it could be even faster because it doesn't need to search the system index because nodes with the application specific node type can't be added to the ""jcr:system""-tree and are therefore are added never to the system index (am I right?)."
0,"Allow basic regexp in namespace prefix of index-rule. Currently a regular expression is limited to the local name, which makes fallback declarations that should match everything else difficult to write. I.e. you have to write a line per namespace in the node type registry, which bloats the index-rule unnecessarily.

Currently:

<property isRegexp=""true"">.*</property>

will only match properties with the empty namespace URI.

I propose we allow a basic regular expression in the prefix. That is the match all pattern: '.*' (dot star).

The following would match any property, including any namespace:

<property isRegexp=""true"">.*:.*</property>
"
0,"DocValues cleanup: constructor & getInnerArray(). DocValues constructor taking a numDocs parameter is not very clean.
Get rid of this.

Also, it's optional getInnerArray() method is not very clean.
This is necessary for better testing, but currently tests will fail if it is not implemented.
Modify it to throw UnSupportedOp exception (rather than returning an empty array).
Modify tests to not fail but just warn if the tested iml does not override it.

These changes should make it easier to implement DocValues for other ValueSource's, e.g. above payloads, with or without caching.
"
0,"Build fails on system without X. The failing test is: testFileContains(org.apache.jackrabbit.core.query.FulltextQueryTest)

caused by:

java.lang.InternalError: Can't connect to X11 window server using ':0.0' as the value of the DISPLAY variable.
	at sun.awt.X11GraphicsEnvironment.initDisplay(Native Method)
	at sun.awt.X11GraphicsEnvironment.access$000(X11GraphicsEnvironment.java:53)
	at sun.awt.X11GraphicsEnvironment$1.run(X11GraphicsEnvironment.java:142)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.awt.X11GraphicsEnvironment.<clinit>(X11GraphicsEnvironment.java:131)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:164)
	at java.awt.GraphicsEnvironment.getLocalGraphicsEnvironment(GraphicsEnvironment.java:68)
	at sun.awt.X11.XToolkit.<clinit>(XToolkit.java:96)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:164)
	at java.awt.Toolkit$2.run(Toolkit.java:821)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.awt.Toolkit.getDefaultToolkit(Toolkit.java:804)
	at java.awt.Toolkit.getEventQueue(Toolkit.java:1592)
	at java.awt.EventQueue.isDispatchThread(EventQueue.java:666)
	at javax.swing.SwingUtilities.isEventDispatchThread(SwingUtilities.java:1270)
	at javax.swing.text.StyleContext.reclaim(StyleContext.java:437)
	at javax.swing.text.StyleContext.addAttribute(StyleContext.java:294)
	at javax.swing.text.StyleContext$NamedStyle.addAttribute(StyleContext.java:1486)
	at javax.swing.text.StyleContext$NamedStyle.setName(StyleContext.java:1296)
	at javax.swing.text.StyleContext$NamedStyle.<init>(StyleContext.java:1244)
	at javax.swing.text.StyleContext.addStyle(StyleContext.java:90)
	at javax.swing.text.StyleContext.<init>(StyleContext.java:70)
	at javax.swing.text.DefaultStyledDocument.<init>(DefaultStyledDocument.java:88)
	at org.apache.tika.parser.rtf.RTFParser.parse(RTFParser.java:42)
"
0,Add basic I/O counters to query handler. There should be a couple of simple counters that track the number of I/O operations that are performed during a query execution. This will help debug query performance issues.
0,"Remove usage of deprecated method Document.fields(). The classes DocumentWriter, FieldsWriter, and ParallelReader use the deprecated method Document.fields(). This simple patch changes these three classes to use Document.getFields() instead.

All unit tests pass."
0,"Allow access to journal inside ClusterNode. JCR-757 added support multiple consumers/producers to be attached to the same journal. In order to access this journal, however, o.a.j.core.cluster.ClusterNode has to allow access to the journal it has created."
0,"RFC4918 feature: PROPFIND/include. RFC4918, Section 14.8 (<http://greenbytes.de/tech/webdav/rfc4918.html#rfc.section.14.8>) defines an extension to PROPFIND that allows clients to retrieve all RFC2518 properties, the dead properties, plus a set of additional live properties. This can help avoiding a second roundtrip to retrieve really all properties.
"
0,"Add a method public boolean hasNodeType(String name) in NodeTypeManagerImpl. As seen in the ML, we plan to add this method and update this class and the interface JackrabbitNodeTypeManager"
0,"optional norms. For applications with many indexed fields, the norms cause memory problems both during indexing and querying.
This patch makes norms optional on a per-field basis, in the same way that term vectors are optional per-field.

Overview of changes:
 - Field.omitNorms that defaults to false
 - backward compatible lucene file format change: FieldInfos.FieldBits has a bit for omitNorms
 - IndexReader.hasNorms() method
 - During merging, if any segment includes norms, then norms are included.
 - methods to get norms return the equivalent 1.0f array for backward compatibility

The patch was designed for backward compatibility:
 - all current unit tests pass w/o any modifications required
 - compatible with old indexes since the default is omitNorms=false
 - compatible with older/custom subclasses of IndexReader since a default hasNorms() is provided
 - compatible with older/custom users of IndexReader such as Weight/Scorer/explain since a norm array is produced on demand, even if norms were not stored

If this patch is accepted (or if the direction is acceptable), performance for scoring  could be improved by assuming 1.0f when hasNorms(field)==false.
"
0,"Payloads. This patch adds the possibility to store arbitrary metadata (payloads) together with each position of a term in its posting lists. A while ago this was discussed on the dev mailing list, where I proposed an initial design. This patch has a much improved design with modifications, that make this new feature easier to use and more efficient.

A payload is an array of bytes that can be stored inline in the ProxFile (.prx). Therefore this patch provides low-level APIs to simply store and retrieve byte arrays in the posting lists in an efficient way. 

API and Usage
------------------------------   
The new class index.Payload is basically just a wrapper around a byte[] array together with int variables for offset and length. So a user does not have to create a byte array for every payload, but can rather allocate one array for all payloads of a document and provide offset and length information. This reduces object allocations on the application side.

In order to store payloads in the posting lists one has to provide a TokenStream or TokenFilter that produces Tokens with payloads. I added the following two methods to the Token class:
  /** Sets this Token's payload. */
  public void setPayload(Payload payload);
  
  /** Returns this Token's payload. */
  public Payload getPayload();

In order to retrieve the data from the index the interface TermPositions now offers two new methods:
  /** Returns the payload length of the current term position.
   *  This is invalid until {@link #nextPosition()} is called for
   *  the first time.
   * 
   * @return length of the current payload in number of bytes
   */
  int getPayloadLength();
  
  /** Returns the payload data of the current term position.
   * This is invalid until {@link #nextPosition()} is called for
   * the first time.
   * This method must not be called more than once after each call
   * of {@link #nextPosition()}. However, payloads are loaded lazily,
   * so if the payload data for the current position is not needed,
   * this method may not be called at all for performance reasons.
   * 
   * @param data the array into which the data of this payload is to be
   *             stored, if it is big enough; otherwise, a new byte[] array
   *             is allocated for this purpose. 
   * @param offset the offset in the array into which the data of this payload
   *               is to be stored.
   * @return a byte[] array containing the data of this payload
   * @throws IOException
   */
  byte[] getPayload(byte[] data, int offset) throws IOException;

Furthermore, this patch indroduces the new method IndexOutput.writeBytes(byte[] b, int offset, int length). So far there was only a writeBytes()-method without an offset argument. 

Implementation details
------------------------------
- One field bit in FieldInfos is used to indicate if payloads are enabled for a field. The user does not have to enable payloads for a field, this is done automatically:
   * The DocumentWriter enables payloads for a field, if one ore more Tokens carry payloads.
   * The SegmentMerger enables payloads for a field during a merge, if payloads are enabled for that field in one or more segments.
- Backwards compatible: If payloads are not used, then the formats of the ProxFile and FreqFile don't change
- Payloads are stored inline in the posting list of a term in the ProxFile. A payload of a term occurrence is stored right after its PositionDelta.
- Same-length compression: If payloads are enabled for a field, then the PositionDelta is shifted one bit. The lowest bit is used to indicate whether the length of the following payload is stored explicitly. If not, i. e. the bit is false, then the payload has the same length as the payload of the previous term occurrence.
- In order to support skipping on the ProxFile the length of the payload at every skip point has to be known. Therefore the payload length is also stored in the skip list located in the FreqFile. Here the same-length compression is also used: The lowest bit of DocSkip is used to indicate if the payload length is stored for a SkipDatum or if the length is the same as in the last SkipDatum.
- Payloads are loaded lazily. When a user calls TermPositions.nextPosition() then only the position and the payload length is loaded from the ProxFile. If the user calls getPayload() then the payload is actually loaded. If getPayload() is not called before nextPosition() is called again, then the payload data is just skipped.
  
Changes of file formats
------------------------------
- FieldInfos (.fnm)
The format of the .fnm file does not change. The only change is the use of the sixth lowest-order bit (0x20) of the FieldBits. If this bit is set, then payloads are enabled for the corresponding field. 

- ProxFile (.prx)
ProxFile (.prx) -->  <TermPositions>^TermCount
TermPositions   --> <Positions>^DocFreq
Positions       --> <PositionDelta, Payload?>^Freq
Payload         --> <PayloadLength?, PayloadData>
PositionDelta   --> VInt
PayloadLength   --> VInt 
PayloadData     --> byte^PayloadLength

For payloads disabled (unchanged):
PositionDelta is the difference between the position of the current occurrence in the document and the previous occurrence (or zero, if this is the first   occurrence in this document).
  
For Payloads enabled:
PositionDelta/2 is the difference between the position of the current occurrence in the document and the previous occurrence. If PositionDelta is odd, then PayloadLength is stored. If PositionDelta is even, then the length of the current payload equals the length of the previous payload and thus PayloadLength is omitted.

- FreqFile (.frq)

SkipDatum     --> DocSkip, PayloadLength?, FreqSkip, ProxSkip
PayloadLength --> VInt

For payloads disabled (unchanged):
DocSkip records the document number before every SkipInterval th document in TermFreqs. Document numbers are represented as differences from the previous value in the sequence.

For payloads enabled:
DocSkip/2 records the document number before every SkipInterval th  document in TermFreqs. If DocSkip is odd, then PayloadLength follows. If DocSkip is even, then the length of the payload at the current skip point equals the length of the payload at the last skip point and thus PayloadLength is omitted.


This encoding is space efficient for different use cases:
   * If only some fields of an index have payloads, then there's no space overhead for the fields with payloads disabled.
   * If the payloads of consecutive term positions have the same length, then the length only has to be stored once for every term. This should be a common case, because users probably use the same format for all payloads.
   * If only a few terms of a field have payloads, then we don't waste much space because we benefit again from the same-length-compression since we only have to store the length zero for the empty payloads once per term.

All unit tests pass."
0,"Avoid Maven 3 warnings. Jackrabbit trunk builds fine with the Maven 3 alpha releases, but there are some warnings about deprecated ${pom....} variables and unspecified reporting plugin versions that we might want to fix."
0,"add LuceneTestCase.rarely()/LuceneTestCase.atLeast(). in LUCENE-3175, the tests were sped up a lot by using reasonable number of iterations normally, but cranking up for NIGHTLY.
we also do crazy things more 'rarely' for normal builds (e.g. simpletext, payloads, crazy merge params, etc)
also, we found some bugs by doing this, because in general our parameters are too fixed.

however, it made the code look messy... I propose some new methods:
instead of some crazy code in your test like:
{code}
int numdocs = (TEST_NIGHTLY ? 1000 : 100) * RANDOM_MULTIPLIER;
{code}

you use:
{code}
int numdocs = atLeast(100);
{code}

this will apply the multiplier, also factor in nightly, and finally add some random fudge... so e.g. in local runs its sometimes 127 docs, sometimes 113 docs, etc.

additionally instead of code like:
{code}
if ((TEST_NIGHTLY && random.nextBoolean()) || (random.nextInt(20) == 17)) {
{code}

you do
{code}
if (rarely()) {
{code}

which applies NIGHTLY and also the multiplier (logarithmic growth).
"
0,NodeCanAddMixinTest.testCheckedIn() has wrong option check. NodeCanAddMixinTest.testCheckedIn() checks for locking option instead of versioning option.
0,"Graduate appendingcodec from contrib/misc. * All tests pass with this codec (at least once, maybe we don't test that two-phase commit stuff very well!)
* It doesn't require special client side configuration anymore to work (just set it on indexwriter and go)
* it now works with the compound file format.

I don't think it needs to live in contrib anymore."
0,Allow pseudo properties in query relation. The XPath query parser does not allow using a function name as part of a relation in a query.
0,"Remove code duplication from Token class, just extend TermAttributeImpl. This issue removes the code duplication from Token, as it shares the whole char[] buffer handling code with TermAttributeImpl. This issue removes this duplication by just extending TermAttributeImpl.

When the parent issue LUCENE-2302 will extend TermAttribute to support CharSequence and Appendable and also the new BytesRefAttribute gets added, Token will automatically provide this too, so no further code duplication.

This code should also be committed to trunk, as it has nothing to do with flex."
0,[PATCH] don't use the reflective form of {Collection}.toArray. Passing a prototype array into {Collection}.toArray that is too small makes the toArray call expend alot of effort using reflection to do it's job. It is more performant to just pass in a correctly sized prototype. This patch does this.
0,"Convert Lucene Core tests over to a simple MockQueryParser. Most tests use Lucene Core's QueryParser for convenience.  We want to consolidate it into a QP module which we can't have as a dependency.  We should add a simple MockQueryParser which does String.split() on the query string, analyzers the terms and builds a BooleanQuery if necessary.  Any more complex Queries (such as phrases) should be done programmatically. "
0,"NGramFilter -- construct n-grams from a TokenStream. This filter constructs n-grams (token combinations up to a fixed size, sometimes
called ""shingles"") from a token stream.

The filter sets start offsets, end offsets and position increments, so
highlighting and phrase queries should work.

Position increments > 1 in the input stream are replaced by filler tokens
(tokens with termText ""_"" and endOffset - startOffset = 0) in the output
n-grams. (Position increments > 1 in the input stream are usually caused by
removing some tokens, eg. stopwords, from a stream.)

The filter uses CircularFifoBuffer and UnboundedFifoBuffer from Apache
Commons-Collections.

Filter, test case and an analyzer are attached."
0,"Improve PhraseQuery.toString(). PhraseQuery.toString() is overly simplistic, in that it doesn't correctly show phrases with gaps or overlapping terms. This may be misleading when presenting phrase queries built using complex analyzers and filters."
0,Benchmark: Improve transparency of test results. as discussed in JCR-1501.
0,"Extend apache parent pom for Apache wide configuration. Apache wide config is published in the apache parent pom, please use"
0,"Open up org.apache.commons.httpclient.Base64 please. I have had several problems lately where I needed to truck backwards and
forwards between bytes and base64. As I am using HttpClient, I know I have the
code in my proect, but I need to duplicate it into my own heirarchy to get
access rights. 

Please make appropriate changes to Base64 (can be as simple as marking the
encode and decode methods public) to allow outside use of Base64.

Would be nice to extend Base64 to deal with multi-line Base64 content too - but
I know this is outside of Base64 original intended use. But it would be useful. :)"
0,Add HTMLStripReader and WordDelimiterFilter from SOLR. SOLR has two classes HTMLStripReader and WordDelimiterFilter which are very useful for a wide variety of use cases.  It would be good to place them into core Lucene.
0,"URI uses  sun.security.action.GetPropertyAction. URI uses a sun.* class but should not.  Use of this class should be removed.

Reported my Mark Wilcox"
0,"Make CMS smarter about thread priorities. Spinoff from LUCENE-2161...

The hard throttling CMS does (blocking the incoming thread that wants
to launch a new merge) can be devastating when it strikes during NRT
reopen.

It can easily happen if a huge merge is off and running, but then a
tiny merge is needed to clean up recently created segments due to
frequent reopens.

I think a small change to CMS, whereby it assigns a higher thread
priority to tiny merges than big merges, should allow us to increase
the max merge thread count again, and greatly reduce the chance that
NRT's reopen would hit this.
"
0,"[PATCH] HttpClient#getHost & HttpClient#getPort methods are misleading. HttpClient#getHost & HttpClient#getPort methods are misleading, accompanied by
obsolete, factually wrong javadocs and as such should be deprecated.

Oleg"
0,"TCK: testSaveMovedRefNode. NodeUUIDTest.testSaveMovedRefNode
SessionUUIDTest.testSaveMovedRefNode

makes the assumption, that moving a referenceable node with session A is visible as 'move' operation within session B as well."
0,Add new JSR283 features to CND reader/writer. the current CND parser(s) and writers do not support the new options specified by JSR283
0,"Configure the maven build for IDE project generation for IDEA and Eclipse. Can we add a plugin configuration for the maven-idea-plugin and maven-eclipse-plugin, with JDK version set for IDEA and configured source download of dependencies?

Simplifies project regeneration and working with IDEA or Eclipse.

I'll add a patch.

Thanks!


 "
0,"Allow controllable printing of the hits. Adds ""print.hits.field"" property to the alg.  If set, then the hits retrieved by Search* tasks are printed, along with the value of the specified field, for each doc."
0,Don't throw TooManyClauses exception. I wonder if it would make sense to fall back to a ConstantScoreQuery instead of throwing a TooManyClauses exception?
0,"Cookie Strict Mode independent of regular Strict Mode. Hi,

I'm having a problem where a web site I'm trying to access is using strict 
cookies (on one line) and a 302 redirect that fails in strict mode.  So I 
cannot access this website because in strict mode it fails because of the 302 
redirect and in non-strict mode the website doesn't recognize the Cookies on 
separate lines.

I'd love to see this added for the next release candidate.

Thanks,

Brent"
0,"Optimize TermsEnum.seek when caller doesn't need next term. Some codecs are able to save CPU if the caller is only interested in
exact matches.  EG, Memory codec and SimpleText can do more efficient
FSTEnum lookup if they know the caller doesn't need to know the term
following the seek term.

We have cases like this in Lucene, eg when IW deletes documents by
Term, if the term is not found in a given segment then it doesn't need
to know the ceiling term.  Likewise when TermQuery looks up the term
in each segment.

I had done this change as part of LUCENE-3030, which is a new terms
index that's able to save seeking for exact-only lookups, but now that
we have Memory codec that can also save CPU I think we should commit
this today.

The change adds a ""boolean onlyExact"" param to seek(BytesRef).
"
0,"Error when registering nodetype with same propdef as supertype. error in check:

                                if (pd.getRequiredType() == epd.getRequiredType()
                                        && pd.isMultiple() == epd.isMultiple()) {
                                    // conflict
                                    String msg = ""The property definition for '""
                                            + name + ""' in node type '""
                                            + def.getDeclaringNodeType()
                                            + ""' conflicts with node type '""
                                            + existingDef.getDeclaringNodeType()
                                            + ""': ambiguous property definition"";
                                    log.debug(msg);
                                    throw new NodeTypeConflictException(msg);
                                }

if needs to be inverted."
0,"Eliminate synchronization contention on initial index reading in TermInfosReader ensureIndexIsRead . synchronized method ensureIndexIsRead in TermInfosReader causes contention under heavy load

Simple to reproduce: e.g. Under Solr, with all caches turned off, do a simple range search e.g. id:[0 TO 999999] on even a small index (in my case 28K docs) and under a load/stress test application, and later, examining the Thread dump (kill -3) , many threads are blocked on 'waiting for monitor entry' to this method.

Rather than using Double-Checked Locking which is known to have issues, this implementation uses a state pattern, where only one thread can move the object from IndexNotRead state to IndexRead, and in doing so alters the objects behavior, i.e. once the index is loaded, the index nolonger needs a synchronized method. 

In my particular test, this uncreased throughput at least 30 times.

"
0,"Provide means for exception handling for QueryNodeVisitor implementations. Currently the methods of QueryNodeVisitor do not declare any exceptions. Even though the query tree might be syntactically correct, an implementation might reach a point where it cannot continue (i.e. if it does not support one of the optional query features). For such cases there are currently two solution: 1. throw an unchecked exception or 2. communicate the error state through the visitor using the data object passed along. 
While I don't like 2. it is still an option. For 1. I'm not sure if this is the right way to go. It might be better to actually throw a checked exception. I therefore created a patch which declares RepositoryException on all visit methods of QueryNodeVisitor. Although the necessary changes in classes using QueryNodeVisitor are trivial, there are quite many of them. 

Any opinions on checked exception with probably breaking (trivially) existing code vs. using not checked exceptions?


"
0,"warning: unmappable character for encoding UTF8. There are a few non-ASCII characters in the Jackrabbit source files that cause warnings at least in my environment. It seems that all the warnings are caused by ""smart quote"" characters.

The exact warnings are: 

/home/hukka/workspace/Jackrabbit/src/java/org/apache/jackrabbit/core/NodeImpl.java:3068: warning: unmappable character for encoding UTF8
            // &#65533;newer&#65533; than N and therefore N should be updated to reflect N'.
               ^
/home/hukka/workspace/Jackrabbit/src/java/org/apache/jackrabbit/core/NodeImpl.java:3068: warning: unmappable character for encoding UTF8
            // &#65533;newer&#65533; than N and therefore N should be updated to reflect N'.
                     ^
/home/hukka/workspace/Jackrabbit/src/java/org/apache/jackrabbit/core/NodeImpl.java:3074: warning: unmappable character for encoding UTF8
            // N' is &#65533;older&#65533; or the &#65533;same age&#65533; as N and therefore N should be left alone.
                     ^
/home/hukka/workspace/Jackrabbit/src/java/org/apache/jackrabbit/core/NodeImpl.java:3074: warning: unmappable character for encoding UTF8
            // N' is &#65533;older&#65533; or the &#65533;same age&#65533; as N and therefore N should be left alone.
                           ^
/home/hukka/workspace/Jackrabbit/src/java/org/apache/jackrabbit/core/NodeImpl.java:3074: warning: unmappable character for encoding UTF8
            // N' is &#65533;older&#65533; or the &#65533;same age&#65533; as N and therefore N should be left alone.
                                    ^
/home/hukka/workspace/Jackrabbit/src/java/org/apache/jackrabbit/core/NodeImpl.java:3074: warning: unmappable character for encoding UTF8
            // N' is &#65533;older&#65533; or the &#65533;same age&#65533; as N and therefore N should be left alone.
                                             ^
/home/hukka/workspace/Jackrabbit/src/java/org/apache/jackrabbit/core/NodeImpl.java:3376: warning: unmappable character for encoding UTF8
        // 2. N&#65533;s jcr:baseVersion property will be changed to point to V.
               ^
/home/hukka/workspace/Jackrabbit/src/java/org/apache/jackrabbit/core/NodeImpl.java:3382: warning: unmappable character for encoding UTF8
        // 3. N&#65533;s jcr:isCheckedOut property is set to false.

/home/hukka/workspace/Jackrabbit/src/test/org/apache/jackrabbit/test/api/version/CheckinTest.java:80: warning: unmappable character for encoding UTF8
        assertEquals(""The versionable checked-out node&#65533;s jcr:predecessors property is copied to the new version on checkin."", Arrays.asList(nPredecessorsValue), Arrays.asList(vPredecessorsValue));
                                                      ^
/home/hukka/workspace/Jackrabbit/src/test/org/apache/jackrabbit/init/NodeTestData.java:95: warning: unmappable character for encoding UTF8
        writer.write(""Hello w&#65533;rld."");
"
0,"Add optional state attribute to managed client connections. Provide an optional state attribute to managed client connections. The connection state can represent a user identify in case of connection based authentication schemes such as NTLM or SSL, thus allowing for connection re-use on a per user identity basis."
0,"Little improvement for SimpleHTMLEncoder. The SimpleHTMLEncoder could be improved slightly: all characters with code >=
128 should be encoded as character entities. The reason is, that the encoder
does not know the encoding that is used for the response. Therefore it is safer
to encode all characters beyond ASCII as character entities.

Here is the necessary modification of SimpleHTMLEncoder:

       default:
         if (c < 128) {
           result.append(c);
         } else {
           result.append(""&#"").append((int)c).append("";"");
         }"
0,"introduce QValue.getCalendar(). Introduce QValue.getCalendar() in order to avoid unnecessary conversions from/to string format.
"
0,"Add isDeclaredMember() method to Group. I suggest to add a method for checking whether an authorizable is a declared member of a group. Currently there is only a method for checking membership (which includes indirect memberships). 

"
0,"CharFilter - normalize characters before tokenizer. This proposes to import CharFilter that has been introduced in Solr 1.4.

Please see for the details:
- SOLR-822
- http://www.nabble.com/Proposal-for-introducing-CharFilter-to20327007.html"
0,Remove deprecated Filter.bits() and make Filter.getDocIdSet() abstract.. 
0,"shutdown of MultiThreadedHttpConnectionManager. - declare 'shutdown' attributes volatile
- interrupt cleanup thread to avoid polling
- don't use iterator on WeakHashMap, ConcurrentModificationException
  might be triggered by garbage collection

patch follows
"
0,"Avoidable synchronization bottleneck in MatchAlldocsQuery$MatchAllScorer. The isDeleted() method on IndexReader has been mentioned a number of times as a potential synchronization bottleneck. However, the reason this  bottleneck occurs is actually at a higher level that wasn't focused on (at least in the threads I read).

In every case I saw where a stack trace was provided to show the lock/block, higher in the stack you see the MatchAllScorer.next() method. In Solr paricularly, this scorer is used for ""NOT"" queries. We saw incredibly poor performance (order of magnitude) on our load tests for NOT queries, due to this bottleneck. The problem is that every single document is run through this isDeleted() method, which is synchronized. Having an optimized index exacerbates this issues, as there is only a single SegmentReader to synchronize on, causing a major thread pileup waiting for the lock.

By simply having the MatchAllScorer see if there have been any deletions in the reader, much of this can be avoided. Especially in a read-only environment for production where you have slaves doing all the high load searching.

I modified line 67 in the MatchAllDocsQuery
FROM:
  if (!reader.isDeleted(id)) {
TO:
  if (!reader.hasDeletions() || !reader.isDeleted(id)) {

In our micro load test for NOT queries only, this was a major performance improvement.  We also got the same query results. I don't believe this will improve the situation for indexes that have deletions. 

Please consider making this adjustment for a future bug fix release.




"
0,"Developer documentation. Provide more example code in CVS and give a clear link on the website.  A
walkthrough of the API.  Documenntation suitable for new users."
0,Login performance drop when using DefaultAccessManager. JCR-2700 caused a drop in login performance when using DefaultAccessManager. The drop is caused by the initialization of the LRU map used in CompiledPermissionsImpl. 
0,"Proxies improvement. 1) Improvement :
I need to be able to detect when a bean is an OCM proxy and if it has already been loaded. This kind of functionnality is for example on Hibernate with Hibernate.isInitialized(Object proxy).
I have developped something similar : I have modified ProxyManagerImpl so it uses an InvocationHandler instead of a LazyLoader. This way, I make my proxies implement a special interface whose methods are intercepted.

2) Bug :
If a BeanConverter is specified, ObjectConverterImpl should pass it to the proxy CallBack instead letting BeanLazyLoader use the default ObjectConverter. I think this is a bug, as the behavior is different is bean property is proxified or not.

3) Improvement :
If a jcrType mapped on a java type is specified, ObjectConverterImpl should make a proxy of this type, and not use the the bean property type. This is particularly useful when the bean property type is an interface.

Sorry for reporting this as a bundle instead of seperate items, but I developped my patch as a whole.  
Let me know if you need help on the enclosing patch.

Sincerely,

Stéphane Landelle"
0,"Support for MaxDB / SapSB Databases. I admit that MaxDB / SapSB are a bit exotic but support is easy to achieve when providing the correct ddls. 
"
0,"PropertyReadMethodsTest.testIsSame leaks session. The test case obtains a second session but doesn't logout().
"
0,"Remove references to older versions of Lucene in ""per-release"" documentation. Some of the documentation that is ""per release"" contains references to older versions, which is often confusing.  This is most noticeable in the file formats docs, but there might be other places too."
0,"PrefixQuery is missing the equals() method. The PrefixQuery is inheriting the java.lang.Object's object default equals method. This makes it hard to have test working of PrefixFilter or any other task requiring equals to work proerply (insertion in Set, etc.). The equal method should be very similar, not to say identical except for class casting, to the equals() of TermQuery. "
0,[API DOC] Authentication guide update: alternate authentication. Add a section on alternate authentication.
0,"Support unicode escapes in QueryParser. As suggested by Yonik in http://issues.apache.org/jira/browse/LUCENE-573 the QueryParser should be able to handle unicode escapes, i. e. \uXXXX.

I have already working and tested code. It is based on the patch i submitted for LUCENE-573, so once this is (hopefully ;-)) committed, I will submit another patch here."
0,"Make it possible to adjust MaxTotalConnections parameter dynamicaly. Make it possible to adjust MaxTotalConnections parameter at run time. Document behaviour of MaxTotalConnections and MaxConnectionsPerRoute behaviour (latter cannot be changed for allocated pools)

Oleg"
0,"Use parallel arrays instead of PostingList objects. This is Mike's idea that was discussed in LUCENE-2293 and LUCENE-2324.

In order to avoid having very many long-living PostingList objects in TermsHashPerField we want to switch to parallel arrays.  The termsHash will simply be a int[] which maps each term to dense termIDs.

All data that the PostingList classes currently hold will then we placed in parallel arrays, where the termID is the index into the arrays.  This will avoid the need for object pooling, will remove the overhead of object initialization and garbage collection.  Especially garbage collection should benefit significantly when the JVM runs out of memory, because in such a situation the gc mark times can get very long if there is a big number of long-living objects in memory.

Another benefit could be to build more efficient TermVectors.  We could avoid the need of having to store the term string per document in the TermVector.  Instead we could just store the segment-wide termIDs.  This would reduce the size and also make it easier to implement efficient algorithms that use TermVectors, because no term mapping across documents in a segment would be necessary.  Though this improvement we can make with a separate jira issue."
0,"Review pck names in the others ocm subprojects. Review package structure and graffito references in the other OCM subprojects : jcr-nodemanagement & spring. 
"
0,"Multi-level skipping on posting lists. To accelerate posting list skips (TermDocs.skipTo(int)) Lucene uses skip lists. 
The default skip interval is set to 16. If we want to skip e. g. 100 documents, 
then it is not necessary to read 100 entries from the posting list, but only 
100/16 = 6 skip list entries plus 100%16 = 4 entries from the posting list. This 
speeds up conjunction (AND) and phrase queries significantly.

However, the skip interval is always a compromise. If you have a very big index 
with huge posting lists and you want to skip over lets say 100k documents, then 
it is still necessary to read 100k/16 = 6250 entries from the skip list. For big 
indexes the skip interval could be set to a higher value, but then after a big 
skip a long scan to the target doc might be necessary.

A solution for this compromise is to have multi-level skip lists that guarantee a 
logarithmic amount of skips to any target in the posting list. This patch 
implements such an approach in the following way:

  Example for skipInterval = 3:
                                                      c            (skip level 2)
                  c                 c                 c            (skip level 1) 
      x     x     x     x     x     x     x     x     x     x      (skip level 0)
  d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d  (posting list)
      3     6     9     12    15    18    21    24    27    30     (df)
 
  d - document
  x - skip data
  c - skip data with child pointer
 
Skip level i contains every skipInterval-th entry from skip level i-1. Therefore the 
number of entries on level i is: floor(df / ((skipInterval ^ (i + 1))).
 
Each skip entry on a level i>0 contains a pointer to the corresponding skip entry in 
list i-1. This guarantees a logarithmic amount of skips to find the target document.


Implementations details:

   * I factored the skipping code out of SegmentMerger and SegmentTermDocs to 
     simplify those classes. The two new classes AbstractSkipListReader and 
	 AbstractSkipListWriter implement the skipping functionality.
   * While AbstractSkipListReader and Writer take care of writing and reading the 
     multiple skip levels, they do not implement an actual skip data format. The two 
	 new subclasses DefaultSkipListReader and Writer implement the skip data format 
	 that is currently used in Lucene (with two file pointers for the freq and prox 
	 file and with payload length information). I added this extra layer to be 
	 prepared for flexible indexing and different posting list formats. 
      
   
File format changes: 

   * I added the new parameter 'maxSkipLevels' to the term dictionary and increased the
     version of this file. If maxSkipLevels is set to one, then the format of the freq 
	 file does not change at all, because we only have one skip level as before. For 
	 backwards compatibility maxSkipLevels is set to one automatically if an index 
	 without the new parameter is read. 
   * In case maxSkipLevels > 1, then the frq file changes as follows:
     FreqFile (.frq) --> <TermFreqs, SkipData>^TermCount
	 SkipData        --> <<SkipLevelLength, SkipLevel>^(Min(maxSkipLevels, 
	                       floor(log(DocFreq/log(skipInterval))) - 1)>, SkipLevel>
	 SkipLevel       --> <SkipDatum>^DocFreq/(SkipInterval^(Level + 1))

	 Remark: The length of the SkipLevel is not stored for level 0, because 1) it is not 
	 needed, and 2) the format of this file does not change for maxSkipLevels=1 then.
	 
	 
All unit tests pass with this patch."
0,"Incorrect decodedAttributeValue in AbstractImportXmlTest. The string literal is not correctly escaped.

Later on the decoded attribute value should be used to check the imported value. There is currently an odd test that checks the encoded attribute value twice."
0,"Stats for Queries continued. This is to track the last missing item on the Query Stats list: Top Queries.
Also some needed refactoring."
0,"HostnameVerifier shouldn't shadow simple name of implemented interface. public interface HostnameVerifier extends javax.net.ssl.HostnameVerifier.

As Findbugs says:

Class names shouldn't shadow simple name of implemented interface

This class/interface has a simple name that is identical to that of an implemented/extended interface, except that the interface is in a different package (e.g., alpha.Foo extends beta.Foo). This can be exceptionally confusing, create lots of situations in which you have to look at import statements to resolve references and creates many opportunities to accidently define methods that do not override methods in their superclasses. 
"
0,"remove contrib deprecations. there aren't too many deprecations in contrib to remove for 3.0, but we should get rid of them."
0,"Update commons-io dependency from versiom 1.4 to 2.0.1 . Jackrabbit may be used as JackRabbit-JCA. In many application servers we need to change the classloading policy to ""global"" or ""flat"". These classloading policies result in many conflicts between versions of own server implementation or version of applications. It would be diminished if Jackrabbit dependences are keep ""up-to-date"".

<dependency>
	<groupId>commons-io</groupId>
	<artifactId>commons-io</artifactId>
	<version>2.0.1</version>
</dependency>"
0,"Change contrib/spatial to use trie's NumericUtils, and remove NumberUtils. Currently spatial contrib includes a copy of NumberUtils from solr (otherwise it would depend on solr)

Once LUCENE-1496 is sorted out, this copy should be removed."
0,"Intermittent failure in TestThreadedOptimize. Failure looks like this:

{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestThreadedOptimize
    [junit] Testcase: testThreadedOptimize(org.apache.lucene.index.TestThreadedOptimize):	FAILED
    [junit] null
    [junit] junit.framework.AssertionFailedError: null
    [junit] 	at org.apache.lucene.index.TestThreadedOptimize.runTest(TestThreadedOptimize.java:125)
    [junit] 	at org.apache.lucene.index.TestThreadedOptimize.testThreadedOptimize(TestThreadedOptimize.java:149)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:253)
{noformat}

I just committed some verbosity so next time it strikes we'll have more details."
0,"need a way to set time out when using HttpClient and HttpMultiClient. When using class HttpClient or HttpMultiClient, there is no way to set the time 
out value. Because the setTimeout method is in HttpConnection and HttpClient or 
HttpMultiClient doesn't expose the HttpConnection object. One option is to add 
a method setTimeout in HttpClient and HttpMultiClient. Another option is to add 
such a method in HttpMethod."
0,"loadURI compile error with Maven 1.0.2. As reported on the mailing list by Ashley Martens:

----
C:\apache\jackrabbit-contrib\nt-ns-util>maven
 __  __
|  \/  |__ _Apache__ ___
| |\/| / _` \ V / -_) ' \  ~ intelligent projects ~
|_|  |_\__,_|\_/\___|_||_|  v. 1.0.2

Attempting to download jackrabbit-1.0-SNAPSHOT.jar.
Artifact /org.apache.jackrabbit/jars/jackrabbit-1.0-SNAPSHOT.jar doesn't exists in remote repository, but it exists locally
Attempting to download jackrabbit-commons-1.0-SNAPSHOT.jar.
Artifact /org.apache.jackrabbit/jars/jackrabbit-commons-1.0-SNAPSHOT.jar doesn't exists in remote repository, but it exists locally
build:start:

java:prepare-filesystem:

java:compile:
   [echo] Compiling to C:\apache\jackrabbit-contrib\nt-ns-util/target/classes
   [javac] Compiling 1 source file to C:\apache\jackrabbit-contrib\nt-ns-util\target\classes
C:\apache\jackrabbit-contrib\nt-ns-util\src\main\java\org\apache\jackrabbit\util\nodetype\SchemaConverter.java:71: cannot resolve symbol
symbol  : method loadURI (java.lang.String)
location: class org.apache.xerces.impl.xs.XMLSchemaLoader
       XSModel xsModel = loader.loadURI(uri);
                               ^
1 error

BUILD FAILED
File...... C:\Documents and Settings\ashleym\.maven\cache\maven-java-plugin-1.5\plugin.jelly
Element... ant:javac
Line...... 63
Column.... 48
Compile failed; see the compiler error output for details.
Total time: 8 seconds
Finished at: Mon Jan 02 10:40:47 EST 2006
----

Peeter Piegaze found out the problem:

----
I was able to build it without a problem using maven-1.1-beta-2 and JDK 1.4.2.

However, it sounds to me like in your case maven has set up its
on-build classpath so that it sees the older xerces-2.4.0.jar before
the new xerxesImpl.-2.6.2.jar. Maven seems to download the old
xerces-2.4.0 into its repository for internal use, while my code uses
the newer xerxesImpl-2.6.2.jar. The old jar overlaps class-wise with
the new one, but the new one implements the additional loadURI method
(among others).

I am not sure exactly why your maven build process is looking in the
wrong jar. But that is what is doing, almost certainly.
----
"
0,"Allow access to registered cookie policies. It would be useful for JMeter (and perhaps other applications) to have access to the list of registered Cookie policy names.

[If this is acceptable, let me know if you want me to provide a patch.]"
0,"Enable FileSystem unit tests. The FileSystem tests are implemented, but not actually run, because the TestAll class is missing.
Also, there is a bug in the tests that causes the tests to fail."
0,"FastVectorHighlighter: support for additional queries. I am using fastvectorhighlighter for some strange languages and it is working well! 

One thing i noticed immediately is that many query types are not highlighted (multitermquery, multiphrasequery, etc)
Here is one thing Michael M posted in the original ticket:

{quote}
I think a nice [eventual] model would be if we could simply re-run the
scorer on the single document (using InstantiatedIndex maybe, or
simply some sort of wrapper on the term vectors which are already a
mini-inverted-index for a single doc), but extend the scorer API to
tell us the exact term occurrences that participated in a match (which
I don't think is exposed today).
{quote}

Due to strange requirements I am using something similar to this (but specialized to our case).
I am doing strange things like forcing multitermqueries to rewrite into boolean queries so they will be highlighted,
and flattening multiphrasequeries into boolean or'ed phrasequeries.
I do not think these things would be 'fast', but i had a few ideas that might help:

* looking at contrib/highlighter, you can support FilteredQuery in flatten() by calling getQuery() right?
* maybe as a last resort, try Query.extractTerms() ?
"
0,"Public Suffix List. Hi,

I just found this useful list: http://publicsuffix.org/
and thought it would be nice to validate cookie domains against it, basically serving as a black list of domain for which never to set any cookies. What do you think about the attached patch? The download/parsing of the list is of course not part of the implementation.

Ortwin"
0,"Make WordDelimiterFilter's instantiation more readable. Currently WordDelimiterFilter's constructor is:

{code}
public WordDelimiterFilter(TokenStream in,
	                             byte[] charTypeTable,
	                             int generateWordParts,
	                             int generateNumberParts,
	                             int catenateWords,
	                             int catenateNumbers,
	                             int catenateAll,
	                             int splitOnCaseChange,
	                             int preserveOriginal,
	                             int splitOnNumerics,
	                             int stemEnglishPossessive,
	                             CharArraySet protWords) {
{code}

which means its instantiation is an unreadable combination of 1s and 0s.  

We should improve this by either using a Builder, 'int flags' or an EnumSet."
0,"Degrade gracefully when reading invalid date values. As noted in JCR-1996, it is possible for an old version of Jackrabbit to store date invalid date values in the repository. Currently such values cause exceptions when the repository attempts to read them. A better approach would be to automatically detect such dates and map them instead to string values to avoid losing any information. A client could then access the information as a string through the normal JCR API, and would only get a ValueFormatException when trying to read the value as a date, i.e. using the getDate() method."
0,"SISM blocks the item state cache when loading a new item. The SharedItemStateManager.getNonVirtualItemState() method contains a loadItemState() call within a ""synchronized (cache)"" block. This prevents all item state cache access while a new item is being loaded from the persistence manager. I have at least one case where this has caused a serious performance drop, essentially synchronizing repository access for all readers."
0,"Handle conditional requests in cache. Return 304 if incoming request has ""If-None-Match"" or ""If-Modified-Since"" headers and can be served from cache.  Currently we return a 200 which is correct but not optimal."
0,"TermScorer caches values unnecessarily. TermScorer aggressively caches the doc and freq of 32 documents at a time for each term scored.  When querying for a lot of terms, this causes a lot of garbage to be created that's unnecessary.  The SegmentTermDocs from which it retrieves its information doesn't have any optimizations for bulk loading, and it's unnecessary.

In addition, it has a SCORE_CACHE, that's of limited benefit.  It's caching the result of a sqrt that should be placed in DefaultSimilarity, and if you're only scoring a few documents that contain those terms, there's no need to precalculate the SQRT, especially on modern VMs.

Enclosed is a patch that replaces TermScorer with a version that does not cache the docs or feqs.  In the case of a lot of queries, that saves 196 bytes/term, the unnecessary disk IO, and extra SQRTs which adds up."
0,"AttributeSource/TokenStream API improvements. This patch makes the following improvements to AttributeSource and
TokenStream/Filter:

- introduces interfaces for all Attributes. The corresponding
  implementations have the postfix 'Impl', e.g. TermAttribute and
  TermAttributeImpl. AttributeSource now has a factory for creating
  the Attribute instances; the default implementation looks for
  implementing classes with the postfix 'Impl'. Token now implements
  all 6 TokenAttribute interfaces.

- new method added to AttributeSource:
  addAttributeImpl(AttributeImpl). Using reflection it walks up in the
  class hierarchy of the passed in object and finds all interfaces
  that the class or superclasses implement and that extend the
  Attribute interface. It then adds the interface->instance mappings
  to the attribute map for each of the found interfaces.

- removes the set/getUseNewAPI() methods (including the standard
  ones). Instead it is now enough to only implement the new API,
  if one old TokenStream implements still the old API (next()/next(Token)),
  it is wrapped automatically. The delegation path is determined via
  reflection (the patch determines, which of the three methods was
  overridden).

- Token is no longer deprecated, instead it implements all 6 standard
  token interfaces (see above). The wrapper for next() and next(Token)
  uses this, to automatically map all attribute interfaces to one
  TokenWrapper instance (implementing all 6 interfaces), that contains
  a Token instance. next() and next(Token) exchange the inner Token
  instance as needed. For the new incrementToken(), only one
  TokenWrapper instance is visible, delegating to the currect reusable
  Token. This API also preserves custom Token subclasses, that maybe
  created by very special token streams (see example in Backwards-Test).

- AttributeImpl now has a default implementation of toString that uses
  reflection to print out the values of the attributes in a default
  formatting. This makes it a bit easier to implement AttributeImpl,
  because toString() was declared abstract before.

- Cloning is now done much more efficiently in
  captureState. The method figures out which unique AttributeImpl
  instances are contained as values in the attributes map, because
  those are the ones that need to be cloned. It creates a single
  linked list that supports deep cloning (in the inner class
  AttributeSource.State). AttributeSource keeps track of when this
  state changes, i.e. whenever new attributes are added to the
  AttributeSource. Only in that case will captureState recompute the
  state, otherwise it will simply clone the precomputed state and
  return the clone. restoreState(AttributeSource.State) walks the
  linked list and uses the copyTo() method of AttributeImpl to copy
  all values over into the attribute that the source stream
  (e.g. SinkTokenizer) uses. 

- Tee- and SinkTokenizer were deprecated, because they use
Token instances for caching. This is not compatible to the new API
using AttributeSource.State objects. You can still use the old
deprecated ones, but new features provided by new Attribute types
may get lost in the chain. A replacement is a new TeeSinkTokenFilter,
which has a factory to create new Sink instances, that have compatible
attributes. Sink instances created by one Tee can also be added to
another Tee, as long as the attribute implementations are compatible
(it is not possible to add a sink from a tee using one Token instance
to a tee using the six separate attribute impls). In this case UOE is thrown.

The cloning performance can be greatly improved if not multiple
AttributeImpl instances are used in one TokenStream. A user can
e.g. simply add a Token instance to the stream instead of the individual
attributes. Or the user could implement a subclass of AttributeImpl that
implements exactly the Attribute interfaces needed. I think this
should be considered an expert API (addAttributeImpl), as this manual
optimization is only needed if cloning performance is crucial. I ran
some quick performance tests using Tee/Sink tokenizers (which do
cloning) and the performance was roughly 20% faster with the new
API. I'll run some more performance tests and post more numbers then.

Note also that when we add serialization to the Attributes, e.g. for
supporting storing serialized TokenStreams in the index, then the
serialization should benefit even significantly more from the new API
than cloning. 

This issue contains one backwards-compatibility break:
TokenStreams/Filters/Tokenizers should normally be final
(see LUCENE-1753 for the explaination). Some of these core classes are 
not final and so one could override the next() or next(Token) methods.
In this case, the backwards-wrapper would automatically use
incrementToken(), because it is implemented, so the overridden
method is never called. To prevent users from errors not visible
during compilation or testing (the streams just behave wrong),
this patch makes all implementation methods final
(next(), next(Token), incrementToken()), whenever the class
itsself is not final. This is a BW break, but users will clearly see,
that they have done something unsupoorted and should better
create a custom TokenFilter with their additional implementation
(instead of extending a core implementation).

For further changing contrib token streams the following procedere should be used:

    *  rewrite and replace next(Token)/next() implementations by new API
    * if the class is final, no next(Token)/next() methods needed (must be removed!!!)
    * if the class is non-final add the following methods to the class:
{code:java}
      /** @deprecated Will be removed in Lucene 3.0. This method is final, as it should
       * not be overridden. Delegates to the backwards compatibility layer. */
      public final Token next(final Token reusableToken) throws java.io.IOException {
        return super.next(reusableToken);
      }

      /** @deprecated Will be removed in Lucene 3.0. This method is final, as it should
       * not be overridden. Delegates to the backwards compatibility layer. */
      public final Token next() throws java.io.IOException {
        return super.next();
      }
{code}
Also the incrementToken() method must be final in this case
(and the new method end() of LUCENE-1448)
"
0,"Build problem with StrictSSLProtocolSocketFactory. StrictSSLProtocolSocketFactory requires jcert.jar to be in compile.classpath of
build.xml.  Here is a patch that will fix it:

Index: build.xml
===================================================================
RCS file: /home/cvspublic/jakarta-commons/httpclient/build.xml,v
retrieving revision 1.27
diff -u -r1.27 build.xml
--- build.xml   23 May 2003 02:49:01 -0000      1.27
+++ build.xml   26 May 2003 04:23:50 -0000
@@ -98,6 +98,7 @@
     <pathelement location=""${build.home}/classes""/>
     <pathelement location=""${junit.jar}""/>
     <pathelement location=""${jsse.jar}""/>
+    <pathelement location=""${jcert.jar}""/>
     <pathelement location=""${jce.jar}""/>
     <pathelement location=""${jnet.jar}""/>
     <pathelement location=""${commons-logging.jar}""/>"
0,extensibility changes to SimpleWebdavServlet. the new SimpleWebdavServlet class lost some of the extensibility of the old WebdavServlet that allowed subclasses to ignore the dependency on RepositoryAccessServlet and provide their own support objects. attached is a patch that adds back these qualities.
0,"Log level for message should be debug instead of error.. In method org.apache.commons.httpclient.HttpMethodBase.getResponseBody() Log
message should be logged as debug instead of error. 

717             } catch (IOException e) {
718                 LOG.error(""I/O failure reading response body"", e);
719                 this.responseBody = null;
720             }

According to HTTPCLIENT-57:
2) Only/always log exception stack traces at the debug level
        } catch (Exception ex) {
            log.debug"
0,"Add User#changePassword(String newPw, String oldPw). ... where the oldPw must match in order to have the password of the user successfully changed.

while this could be done by applications with quite some effort, the implementation can easily achieve this
as the functionality required is already present."
0,"Rename IndexReader.reopen to make it clear that reopen may not happen. Spinoff from LUCENE-3454 where Shai noted this inconsistency.

IR.reopen sounds like an unconditional operation, which has trapped users in the past into always closing the old reader instead of only closing it if the returned reader is new.

I think this hidden maybe-ness is trappy and we should rename it (maybeReopen?  reopenIfNeeded?).

In addition, instead of returning ""this"" when the reopen didn't happen, I think we should return null to enforce proper usage of the maybe-ness of this API."
0,"Speedup CharArraySet if set is empty. CharArraySet#contains(...) always creates a HashCode of the String, Char[] or CharSequence even if the set is empty. 
contains should return false if set it empty"
0,Faster packaging of the standalone jar. Currently the standalone jar is created by first unpacking all the dependencies to target/classes and then packaging the resulting directory tree into the resulting jar file. This takes quite a while as all the writing and reading of uncompressed class files requires lots of disk IO. We could avoid these extra copies by using the bundle or assembly plugin to build the standalone jar.
0,"IfHeader: Incorrect test for parsing keyword ""not"" in parseIfList. The test for the ""not"" keyword tests the last character as

     (not !='t' || not != 'T')

which always yields true (not cannot be t and T at the same time). The correct test would be

     (not !='t' && not != 'T')"
0,"NoLockFactory should have a private constructor. NoLockFactory documents in its javadocs that one should use the static getNoLockFactory() method. However the class does not declare a private empty constructor, which breaks its Singleton purpose. We cannot add the empty private constructor because that'd break break-compat (even though I think it's very low chance someone actually instantiates the class), therefore we'll add a @deprecated warning to the class about this, and add the method in 4.0. I personally prefer to add an empty constructor w/ the @deprecated method, but am fine either way.

Don't know if a patch is needed, as this is a trivial change. "
0,Use executor service from repository for index merging. The index merger currently starts its own threads for index merges. Using the repository wide executor service would simplify things and make configuration easier.
0,"move core module to a subdirectory. Actually the jackrabbit svn holds the code for the main module in the top level dir
http://svn.apache.org/repos/asf/incubator/jackrabbit/trunk/
and all the subprojects in subdirectories
http://svn.apache.org/repos/asf/incubator/jackrabbit/trunk/contrib/

given this layout is not possible to checkout from svn only the main module (if you get trunk, you get all), and you can't work to different modules using any IDE which doesn't support nested projects (namely Eclipse).

I would like to request moving the main module (that means moving all the files and directories in trunk except ""contrib"") from 
http://svn.apache.org/repos/asf/incubator/jackrabbit/trunk/
to 
http://svn.apache.org/repos/asf/incubator/jackrabbit/trunk/jackrabbit
following the usual organization of maven-based projects and solving these problems...


"
0,"Typo in API_CHANGES_3_0.txt. DigestSheme should be DigestScheme :)  Also, why is there not an httpclient
component in the list?"
0,"Incorrect/incomplete product name in META-INF/NOTICE file. The NOTICE file in the HttpClient jars is incorrect.

It states:

=====

HttpClient
Copyright 1999-2009 Apache Software Foundation
<snip/>
======

The leading blank line should be deleted, and ""HttpClient"" should be ""Apache HttpComponents Client - HttpClient""  (or similar) as is the case for the source archive.

Similarly for HttpMime"
0,"Handling sub-domain cookies.. I noticed a difference in behaviour between httpclient and most common browsers 
(IE/Mozilla). If a web site sets a cookie for ""beta.gamma.com"", this cookie is 
not sent in requests to ""alpha.beta.gamma.com"". 
  I am not sure what the cookie specs say, but Mozilla, IE and even 
HTTP::Cookies module in LWP seem to behave differently from HttpClient. 
HttpClient seems to rely on the leading dot in the domain name 
(like "".beta.gamma.com"")."
0,"Default KuromojiAnalyzer to use search mode. Kuromoji supports an option to segment text in a way more suitable for search,
by preventing long compound nouns as indexing terms.

In general 'how you segment' can be important depending on the application 
(see http://nlp.stanford.edu/pubs/acl-wmt08-cws.pdf for some studies on this in chinese)

The current algorithm punishes the cost based on some parameters (SEARCH_MODE_PENALTY, SEARCH_MODE_LENGTH, etc)
for long runs of kanji.

Some questions (these can be separate future issues if any useful ideas come out):
* should these parameters continue to be static-final, or configurable?
* should POS also play a role in the algorithm (can/should we refine exactly what we decompound)?
* is the Tokenizer the best place to do this, or should we do it in a tokenfilter? or both?
  with a tokenfilter, one idea would be to also preserve the original indexing term, overlapping it: e.g. ABCD -> AB, CD, ABCD(posInc=0)
  from my understanding this tends to help with noun compounds in other languages, because IDF of the original term boosts 'exact' compound matches.
  but does a tokenfilter provide the segmenter enough 'context' to do this properly?

Either way, I think as a start we should turn on what we have by default: its likely a very easy win.
"
0,"Add simple query method to ObjectContentManager. As discussed in [1], I suggest a new method 

    ObejctContentManager.getObjectIterator(String query, String language)

to easily query the repository for objects using a predefined query. (I chose getObjectIterator instead of getObjects as I intend the method to return an Iterator and not a Collection)

[1] http://www.mail-archive.com/dev%40jackrabbit.apache.org/msg07475.html"
0,"FST Builder methods need fixing,documentation,or improved type safety. Its confusing the way an FST Builder has 4 add() methods, and you get assertion errors (what happens if assertions are disabled?) if you use the wrong one:

For reference we have 3 FST input types:
* BYTE1 (byte)
* BYTE2 (char)
* BYTE4 (int)

For the builder add() method signatures we have:
* add(BytesRef)
* add(char[], int offset, int len)
* add(CharSequence)
* add(IntsRef)

But certain methods only work with certain FST input types, and these mappings are not the ones you think. 

For example, you would think that if you have a char-based FST you should use add(char[]) or add(CharSequence), but this is not the case: those add methods actually only work with int-based FST (they use codePointAt() to extract codepoints). Instead, you have to use add(IntsRef) for the char-based one.

The worst is if you use the wrong one, you get an assertion error, but i'm not sure what happens if assertions are disabled.

Maybe the ultimate solution is to parameterize FST's generics on input too (FST<input,output>) and just require BytesRef/CharsRef/IntsRef as the parameter? Then you could just have add(), and this might clean up FSTEnum too (it would no longer need that InputOutput class but maybe could use Map.Entry<input,output> or something?
 
I think the documentation is improving but i still notice add(BytesRef) has no javadoc at all, and it only works with BYTE1, so I think we still have some work to do even if we want to just pursue a documentation fix.
"
0,"Database Data Store: clean up the code. There is some unnecessary code in the DbDataStore that should be removed.
Also, some more tests should be added."
0,"Improve org.apache.lucene.search.Filter Documentation and Tests to reflect per segment readers. Filter Javadoc does not mention that the Reader passed to getDocIDSet(Reader) could be on a per-segment basis.
This caused confusion on the users-list -- see http://lucene.markmail.org/message/6knz2mkqbpxjz5po?q=date:200912+list:org.apache.lucene.java-user&page=1
We should improve the javadoc and also add a testcase that reflects filtering on a per-segment basis."
0,"JSR 283: new Property Types. the new property types are

- WEAKREFERENCE
- URI
- DECIMAL"
0,"IndexModifier has incomplete Javadocs. A lot of public and protected members of org.apache.lucene.index.IndexModifier 
don't have javadocs."
0,"NodeTypeManagerImpl.hasNodeType should allow unknown prefixes. The current implementation of NodeTypeImpl.hasNodeType(String) throws an exception if the given name uses an unknown prefix. A better alternative would be to just return false, as by definition a node type in an unknown namespace can not exist."
0,"Explicit management of public API. I'd like to start using the Clirr Maven plugin [1] to make sure that we don't accidentally break backwards compatibility in our public APIs, most notably in jackrabbit-api and jackrabbit-jcr-commons.

Also, we should start explicitly managing the API versions exposed as a part of the OSGi package metadata. Currently all our public packages simply get the latest project version as their version number, but it would be better if the version was explicitly managed and only updated if the API actually changes. To do this I propose we use @Version annotations from the bnd tool on the package-info.java files in all packages considered a part of our public API.

The Clirr plugin should flag all changes made in the API, so we have an easy way to tell which packages need to have their version numbers updated.

[1] http://mojo.codehaus.org/clirr-maven-plugin/"
0,"Using deprecated class javax.servlet.http.HttpUtils. [javac]
test-webapp/src/org/apache/commons/httpclient/RedirectServlet.java:74: warning:
javax.servlet.http.HttpUtils in javax.servlet.http has been deprecated
    [javac]             to =
HttpUtils.getRequestURL(request).append(""?"").append(request.getQueryString()).toString();

The javax.servlet.http.HttpUtils class is deprecated in Tomcat 4.1.18 and should
not be used."
0,"Too many open files when merging large index segments. When large index segments are merged it may happen that lots of smaller index segments are created but have to wait until the large
index merge has completed. This may lead to a 'too many open files' exception on some system.

We should find a solution where large index merges are better decoupled from regular index operations."
0,"GCJ build fails with JDK 1.5. The build.xml doesn't specify a target VM version. Using JDK 1.5, this means the compiled .class files 
are automatically made for 1.5, with java.lang.StringBuilder insidiously used for string concatenation. 
GCJ doesn't seem to include this class yet, so when it gets to the gcj build it dies trying to read the class 
files.

Steps to reproduce:
1. Install Sun JDK 1.5 for a Java compiler
2. Check out Lucene from svn
3. 'ant gcj'

Expected behavior:
Should build Lucene to .class files and .jar with the JDK compiler and then compile an .a with GCJ.

Actual behavior:
The GCJ build fails, complaining of being unable to find java.lang.StringBuilder.

Suggested fix:
Adding source=""1.3"" target=""1.3"" to the <javac> tasks seems to take care of this. Patch to be attached.

Additional notes:
Using Lucene from SVN and GCJ pulled from GCC CVS circa 2005-04-19. Ant 1.6.2."
0,Javadoc improvements for Payload class. Some methods in org.apache.lucene.index.Payload don't have javadocs
0,"date encoding limitation removing. currently there is some limitation to date encoding in lucene. I think it's 
because dates should preserve lexicografical ordering, i.e. if one date precedes 
another date then encoded values should keep same ordering.

I know that it can be difficult to integrate it into existing version but there 
is way to remove this limitation.
Date milliseconds can be encoded as unsigned values with prefix that indicates 
positive or negative value.

In more details:
I used hex encoding and prefix &#8216;p&#8217; and &#8216;n&#8217; for positive and negative values. I 
got following results:

Value -10000 is encoded with nffffffffffffd8f0, 
-100	- nffffffffffffff9c
0	- p0000000000000000
100	- p0000000000000064
10000	- p0000000000002710

This preserves ordering between values and theirs encoding.

Also hex encoding can be replaced with Character.MAX_RADIX encoding.

Part of code that do this work:
   final static char[] digits = {
	'0' , '1' , '2' , '3' , '4' , '5' ,
	'6' , '7' , '8' , '9' , 'a' , 'b' ,
	'c' , 'd' , 'e' , 'f' , 'g' , 'h' ,
	'i' , 'j' , 'k' , 'l' , 'm' , 'n' ,
	'o' , 'p' , 'q' , 'r' , 's' , 't' ,
	'u' , 'v' , 'w' , 'x' , 'y' , 'z'
    };


    char prefix;
    if (time >= 0) {
      prefix = 'p';
    } else {
      prefix = 'n';
    }

    char[] chars = new char[DATE_LEN + 1];
    int index = DATE_LEN;
    while (time != 0) {
      int b = (int) (time & 0x0F);
      chars[index--] = digits[b];
      time = time >>> 4;
    }

    while (index >= 0) {
      chars[index--] = '0';
    }
    chars[0] = prefix;

    return new String(chars);"
0,"Use the Jackrabbit RMI extensions by default in jackrabbit-webapp. Using the Jackrabbit RMI extensions by default in jackrabbit-webapp

Ref :  http://www.nabble.com/Custom-node-types-with-RMI-tf3728625.html

"
0,"Optimize copies between IndexInput and Output. We've created an optimized copy of files from Directory to Directory. We've also optimized copyBytes recently. However, we're missing the opposite side of the copy - from IndexInput to Output. I'd like to mimic the FileChannel API by having copyTo on IndexInput and copyFrom on IndexOutput. That way, both sides can optimize the copy process, depending on the type of the IndexInput/Output that they need to copy to/from.

FSIndexInput/Output can use FileChannel if the two are FS types. RAMInput/OutputStream can copy to/from the buffers directly, w/o going through intermediate ones. Actually, for RAMIn/Out this might be a big win, because it doesn't care about the type of IndexInput/Output given - it just needs to copy to its buffer directly.

If we do this, I think we can consolidate all Dir.copy() impls down to one (in Directory), and rely on the In/Out ones to do the optimized copy. Plus, it will enable someone to do optimized copies between In/Out outside the scope of Directory.

If this somehow turns out to be impossible, or won't make sense, then I'd like to optimize RAMDirectory.copy(Dir, src, dest) to not use an intermediate buffer."
0,"Allow servlet filters to specify custom session providers. In order to integrate the Jackrabbit davex server functionality with their custom authentication logic, the Sling project currently needs to embed and subclass the davex servlet classes. It would be cleaner if such tight coupling wasn't needed.

One way to achieve something like that would be to allow external components to provide a custom SessionProvider instance as an extra request attribute. This way for example a servlet filter that implements such custom authentication logic could easily make its functionality available to the standard davex servlet in Jackrabbit."
0,"SpecialOperations.isFinite can have TERRIBLE TERRIBLE runtime in certain situations. in an application of mine, i experienced some very slow query times with finite automata (all the DFAs are acyclic)

It turned out, the slowdown is some terrible runtime in SpecialOperations.isFinite <-- this is used to determine if the DFA is acyclic or not.

(in this case I am talking about even up to minutes of cpu).
"
0,"When resolving deletes, IW should resolve in term sort order. See java-dev thread ""IndexWriter.updateDocument performance improvement""."
0,Use java.util.UUID. Replace the use of org.apache.jackrabbit.uuid.UUID with the new java.util.UUID class introduced in Java 5.
0,"Consolidate compare behaviour for Value(s) and Comparable(s). There are 2 different implementations of Value comparison (ValueComparator and Util). With the introduction of JCR-2906 which introduces arrays into the mix, I'd like to refactor all of them into one place, namely o.a.j.core.query.lucene.Util.

This will also allow for a wider scope of comparison for Value[], marked as TODO in the ValueComparator class.

Will attach patch shortly"
0,"New JcrUtils utility methods. I'd like to add the following new utility methods to JcrUtils:

    readFile(Node): returns an InputStream for reading file contents
    readFile(Node, OutputStream): writes file contents to the given stream
    getLastModified(Node): returns the jcr:lastModified value
    setLastModified(Node, Calendar): sets the jcr:lastModified value
    getPropertyType(String): like PropertyType.valueFromName(String), but case-insensitive
"
0,Backport JCR-1197: Node.restore() may throw InvalidItemStateException. Backport issue JCR-1197 (Node.restore() may throw InvalidItemStateException) to 1.3 branch for 1.3.4 (separate issue to avoid re-opening JCR-1197 which was already released with 1.4).
0,"warn on invalid set-cookie header. I had a problem on a WS server that comes from some proxy misconfiguration...
resulting in this reponse beeing received by HTTPclient :
17:26:36,489 DEBUG [header] << ""HTTP/1.1 200 OK[\r][\n]""
17:26:36,489 DEBUG [header] << ""Set-Cookie: =f448bb59feedbaaabaee; path=/[\r][\n]""
17:26:36,489 DEBUG [header] << ""Date: Tue, 15 Nov 2005 16:26:36 GMT[\r][\n]""
17:26:36,489 DEBUG [header] << ""Server: Apache[\r][\n]""
17:26:36,489 DEBUG [header] << ""Connection: close[\r][\n]""
17:26:36,489 DEBUG [header] << ""Transfer-Encoding: chunked[\r][\n]""
17:26:36,489 DEBUG [header] << ""Content-Type: text/xml;charset=utf-8[\r][\n]""

The set-cookie header is malformed, as cookie has no name, so the HTTP head may
be considered invalid.

This results in an error when building the NEXT request. I'd expect httpclient
to WARN on malformed header and drop it."
0,"RepositoryStub implementation in jackrabbit-core. Currently setting up a Jackrabbit repository for use with the TCK is a relatively complex operation with a large repositoryStubImpl.properties file and lots of specially crafted test content and settings to worry about. This makes it hard to set up new TCK test instances with the various JCR and SPI layers we now have.

To simplify things I'd like to introduce a RepositoryStubImpl class and related configuration files inside src/main/java in jackrabbit-core."
0,"FirstPassGroupingCollector should use pollLast(). Currently FirstPassGroupingCollector uses last and remove method (TreeSet) for replacing a more relevant grouping during grouping.
This can be replaced by pollLast since Lucene trunk is now Java 6. TermFirstPassGroupingCollectorJava6 in Solr can be removed as well."
0,"[JCR-RMI] Have ClientItem.isSame throw RepositoryException. Currently the ClientItem.isSame(Item) method wraps a RepositoryException thrown from the path comparison into a RuntimeException and omits an exception declaration on the method. This contrasts with the API specification which allows for a RepositoryExcption to be thrown.

I suggest, to modify ClientItem.isSame(Item) such that the RepositoryException is declared and thrown."
0,"Move to commons-logging. Commons-logging was derived from httpclient.log, still using the old logging
which should be removed.  Some complaints on mailing list about setting up
commons-logging: should be simple and well documented."
0,"lock token validity. There are several minor issues in the mapping between JCR lock tokens and WebDAV lock tokens:

1) WebDAV lock tokens are supposed to use URI syntax (such as opaquelocktoken: or urn:uuid:)

2) The server currently computes lock tokens for session-scoped locks based on the node id; these are not valid JCR lock tokens though and cause exceptions when they are re-added when they appear in a Lock-Token header or an If header. This will likely cause requests to fail that use both types of locks (yes, maybe academic but should be fixed anyway)

Proposal:

a) Map lock tokens to oqaquelocktoken URIs, using a constant UUID plus a postfix encoding the original lock token
b) Use a syntax that allows to distinguish between tokens for open-scoped locks or session-scoped locks, so that we do not try to add the latter type to the Session (alternatively, handle exceptions doing so gracefully)"
0,"It should be possible to create a non-transient Repository inside the JCARepositoryManager. With JCR-2555 jukka changed the code to create a Repository with the RepositoryFactory mechanism.
It should be possible to create a non-transient Repository
"
0,"Exception messages in AuthorizableImpl include full tracebacks at warn level.. In a number of places in AuthourizableImpl there are log.warn with tracebacks.
This would be fine, however in production with Sling its quite easy to attempt to set a property on the Authorizable and generate the traceback in the logs, which soon fill up. 

We could block all logging from the user manager, but that feels wrong.
Would it be possible to have the log message at warn level put out a single log line and at debug level put out the full traceback ?"
0,Add Amazon S3 authentication support. Add support for the the Amazon S3 authentication scheme as defined by the online document: http://docs.amazonwebservices.com/AmazonS3/latest/index.html?RESTAuthentication.html
0,"JSR 283: adopt CND syntax changes. the CND syntax has changed from Public Review Draft to Public Final Draft.

old and new syntax are incompatible."
0,"HttpClient OSGi Export-Package doesn't specify version. The ""Export-Package"" manifest entry doesn't specify the version of the package being exported.  This means that packages importing it can't specify a version to import."
0,"UUID generation: SecureRandom should be used by default. Currently, the UUID generation used the regular java.util.Random implementation to generate random UUIDs. The seed value of Random is initialized using System.currentTimeMillis(); for Windows, the resolution is about 15 milliseconds. That means two computer that start creating UUIDs with Jackrabbit within the same 15 millisecond interval will generate the same UUIDs. In a clustered environment the nodes could be started automatically at the same time (for example after a backup).

Also, the Random class uses a 48-bit seed, which is much less than the number of random bits in UUID (122). This is not secure. See also:

http://en.wikipedia.org/wiki/UUID
Random UUID probability of duplicates
""The probability [of duplicates] also depends on the quality of the random number generator. A cryptographically secure pseudorandom number generator must be used to generate the values, otherwise the probability of duplicates may be significantly higher.""

Therefore, I suggest to change VersionFourGenerator to use the SecureRandom implementation in by default."
0,javacc ant task for contrib/misc precedence query parser. Add a javacc task in contrib/misc for the precedence query parser.
0,java.lang.Iterable support for RangeIterators. Make javax.jcr.RangeIterator extend java.lang.Iterable in order to enable foreach loops on implementations of RangeIterator.
0,"Add more unit on collection fields. collection fields based on List are  supported not yet tested correctly.
Check if other kind collection are well tested"
0,"Authorizable#getProperty and #setProperty should deal with relativePath . Authorizable#getProperty and #setProperty defines the property to be identified by a name.

The JCR item based implementation could easily deal with relative paths instead and also retrieve or write properties below child
nodes of the rep:Authorizable node."
0,"SQL2 parser may infer type for UncastLiteral from static analysis. The spec says:

""An UncastLiteral is always interpreted as a Value of property type STRING. A CastLiteral, on the other hand, is interpreted as the string form of a Value of the PropertyType indicated.""

There are also two test cases in NodeNameTest that need to be fixed accordingly: testLongLiteral and testBooleanLiteral
"
0,"Extract JDBC Connection Init. An intermediate step to allowing a PM to be easily configurable through JNDI would be to extract the connection init. This will allow system integrators to subclass/wrap and dynamically configure a customized Simple PM. In org.apache.jackrabbit.core.state.db.SimpleDbPersistenceManager:

Replace lines (296-298) with

        initConnection();

Add:
	/**
	* Initialize the JDBC connection
	**/
	protected void initConnection() throws Exception {
            Class.forName(driver);
            con = DriverManager.getConnection(url, user, password);
            con.setAutoCommit(false);
	}"
0,"Hardening of NativeFSLock. NativeFSLock create a test lock file which its name might collide w/ another JVM that is running. Very unlikely, but still it happened a couple of times already, since the tests were parallelized. This may result in a false exception thrown from release(), when the lock file's delete() is called and returns false, because the file does not exist (deleted by another JVM already). In addition, release() should give a second attempt to delete() if it fails, since the file may be held temporarily by another process (like AntiVirus) before it fails. The proposed changes are:

1) Use ManagementFactory.getRuntimeMXBean().getName() as part of the test lock name (should include the process Id)
2) In release(), if delete() fails, check if the file indeed exists. If it is, let's attempt a re-delete() few ms later.
3) If (3) still fails, throw an exception. Alternatively, we can attempt a deleteOnExit.

I'll post a patch later today."
0,"UserManagement: Don't read cached memberships if session has pending (group) changes. for backwards compatibility reasons reading group membership should not access the overall cache in case of pending (group) change.
the current implememation (always reading from cache) caused a regression in test-case we have @day that relied on accurate group
membership information with having unsaved group-member changes."
0,"trappy ignoreCase behavior with StopFilter/ignoreCase. Spinoff from LUCENE-3751:

{code}
* If <code>stopWords</code> is an instance of {@link CharArraySet} (true if
* <code>makeStopSet()</code> was used to construct the set) it will be
* directly used and <code>ignoreCase</code> will be ignored since
* <code>CharArraySet</code> directly controls case sensitivity.
{code}

This is really confusing and trappy... we need to change something here."
0,Limit fields read from index. Reading a lucene document from the index should be limited to only those fields that are necessary.
0,"user need a way to control cookie policy. User need a way to control what cookie can be accepted and what should be 
rejected. It would be nice to provide a cookie filter interface, so the user 
can change the cookie policy by implementing his own filter."
0,"Add Boosting Function Term Query and Some Payload Query refactorings. Similar to the BoostingTermQuery, the BoostingFunctionTermQuery is a SpanTermQuery, but the difference is the payload score for a doc is not the average of all the payloads, but applies a function to them instead.  BoostingTermQuery becomes a BoostingFunctionTermQuery with an AveragePayloadFunction applied to it.

Also add marker interface to indicate PayloadQuery types.  Refactor Similarity.scorePayload to also take in the doc id."
0,"InternalVersion.getFrozenNode confused about root version?. It seems the javadoc for InternalVersion.getFrozenNode  is confused:

     * Returns the frozen node of this version or <code>null</code> if this is
     * the root version.

AFAIU, the frozen node of the root version is always present to capture the node type of the versionable node.

Does anybody recall how this got here? (SVN says it has been there forever)"
0,"Refactor TestIndexWriter. TestIndexWriter is getting a bit unwieldy:
* 5,315 lines of code
* 116 test methods
* runtimes frequently between 1 and 2 minutes.

It starts to be pretty scary to merge changes because its so massive.

A lot of the tests arguably belong somewhere else (e.g. the addIndex* tests belong in TestAddIndexes)

But here is a start:
# Pulls out the *OnDiskFull tests into TestIndexWriterOnDiskFull
# Pulls out the multithreaded tests into TestIndexWriterWithThreads
"
0,"Hudson build doesn't detect Java 5 class references. Due to the fact that the Maven 2 support in Hudson only works on Java 5, our current CI build at http://hudson.zones.apache.org/hudson/job/Jackrabbit-trunk/ doesn't detect references to classes and methods that are only available in the Java 5 class library."
0,"Allow users to disable validation. DigesterMapperImpl leaves validating set to default true when creating a DigesterDescriptorReader.

But as the dtd is not available anywhere (published or in the source), it is usually not declared in mapping files, and DigesterDescriptorReader complains about it.

Could it be possible to leave the user a way to configure the validation? The simpliest way would be to add this constructor to DigesterMapperImpl :

    public DigesterMapperImpl(InputStream[] streams, boolean validate) {
        descriptorReader = new DigesterDescriptorReader(streams);
        DigesterDescriptorReader.class.cast(descriptorReader).setValidating(validate);
        this.buildMapper();
    }

Best regard,

Stephane Landelle"
0,"client cache may be a shared cache but is caching responses to requests with Authorization headers. ""      When a shared cache (see section 13.7) receives a request
      containing an Authorization field, it MUST NOT return the
      corresponding response as a reply to any other request, unless one
      of the following specific exceptions holds:

      1. If the response includes the ""s-maxage"" cache-control
         directive, the cache MAY use that response in replying to a
         subsequent request. But (if the specified maximum age has
         passed) a proxy cache MUST first revalidate it with the origin
         server, using the request-headers from the new request to allow
         the origin server to authenticate the new request. (This is the
         defined behavior for s-maxage.) If the response includes ""s-
         maxage=0"", the proxy MUST always revalidate it before re-using
         it.

      2. If the response includes the ""must-revalidate"" cache-control
         directive, the cache MAY use that response in replying to a
         subsequent request. But if the response is stale, all caches
         MUST first revalidate it with the origin server, using the
         request-headers from the new request to allow the origin server
         to authenticate the new request.

      3. If the response includes the ""public"" cache-control directive,
         it MAY be returned in reply to any subsequent request.""

http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.8

It isn't clear whether the CachingHttpClient is a shared cache or not (it depends on where it gets used), so the conservative compliant behavior is to assume we are a shared cache. The current implementation is caching responses regardless of whether the original requests had Authorization headers or not.

Patch and discussion forthcoming.

"
0,"Clover setup currently has some problems. (tracking as a bug before it get lost in email...
  http://www.nabble.com/Clover-reports-missing-from-hudson--to15510616.html#a15510616
)

The clover setup for Lucene currently has some problems, 3 i think...

1) instrumentation fails on contrib/db/ because it contains java packages the ASF Clover lscence doesn't allow instrumentation of.  i have a patch for this.

2) running instrumented contrib tests for other contribs produce strange errors...

{{monospaced}}
    [junit] Testsuite: org.apache.lucene.analysis.el.GreekAnalyzerTest
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.126 sec
    [junit]
    [junit] ------------- Standard Error -----------------
    [junit] [CLOVER] FATAL ERROR: Clover could not be initialised. Are you sure you have Clover
in the runtime classpath? (class
java.lang.NoClassDefFoundError:com_cenqua_clover/CloverVersionInfo)
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testAnalyzer(org.apache.lucene.analysis.el.GreekAnalyzerTest):    Caused
an ERROR
    [junit] com_cenqua_clover/g
    [junit] java.lang.NoClassDefFoundError: com_cenqua_clover/g
    [junit]     at org.apache.lucene.analysis.el.GreekAnalyzer.<init>(GreekAnalyzer.java:157)
    [junit]     at
org.apache.lucene.analysis.el.GreekAnalyzerTest.testAnalyzer(GreekAnalyzerTest.java:60)
    [junit]
    [junit]
    [junit] Test org.apache.lucene.analysis.el.GreekAnalyzerTest FAILED
{{monospaced}}

...i'm not sure what's going on here.  the error seems to happen both when
trying to run clover on just a single contrib, or when doing the full
build ... i suspect there is an issue with the way the batchtests fork
off, but I can't see why it would only happen to contribs (the regular
tests fork as well)

3) according to Grant...

{{quote}}
...There is also a bit of a change on Hudson during the migration to the new servers that needs to be ironed  out. 
{{quote}}
"
0,"ByteArrayBody as an alternative to InputStreamBody. InputStreamBody can not determine the content length, which in turn causes requests to be sent with a content length of 0, even though the content is there. .NET Servers have trouble dealing with this.

ByteArrayBody provides an alternative that alliviates this limitation.

Source:
 
import java.io.IOException;
import java.io.OutputStream;

import org.apache.http.entity.mime.MIME;
import org.apache.http.entity.mime.content.AbstractContentBody;

/**
 * Body part that is built using a byte array containing a file.
 * 
 * @author Axel Fontaine
 */
public class ByteArrayBody extends AbstractContentBody {
    /**
     * The contents of the file contained in this part.
     */
    private byte[] data;

    /**
     * The name of the file contained in this part.
     */
    private String filename;
    
    /**
     * Creates a new ByteArrayBody.
     * 
     * @param data The contents of the file contained in this part.
     * @param mimeType The mime type of the file contained in this part.
     * @param filename The name of the file contained in this part.
     */
    public ByteArrayBody(final byte[] data, final String mimeType, final String filename) {
        super(mimeType);
        if (data == null) {
            throw new IllegalArgumentException(""byte[] may not be null"");
        }
        this.data = data;
        this.filename = filename;
    }

    /**
     * Creates a new ByteArrayBody.
     * 
     * @param data The contents of the file contained in this part.
     * @param filename The name of the file contained in this part.
     */
    public ByteArrayBody(final byte[] data, final String filename) {
        this(data, ""application/octet-stream"", filename);
    }

    @Override
    public String getFilename() {
        return filename;
    }

    @Override
    public void writeTo(OutputStream out) throws IOException {
        out.write(data);
    }

    @Override
    public String getCharset() {
        return null;
    }

    @Override
    public String getTransferEncoding() {
        return MIME.ENC_BINARY;
    }

    @Override
    public long getContentLength() {
        return data.length;
    }
}
"
0,"add Terms.docCount. spinoff from LUCENE-3290, where yonik mentioned:

{noformat}
Is there currently a way to get the number of documents that have a value in the field?
Then one could compute the average length of a (sparse) field via sumTotalTermFreq(field)/docsWithField(field)
docsWithField(field) would be useful in other contexts that want to know how sparse a field is (automatically selecting faceting algorithms, etc).
{noformat}

I think this is a useful stat to add, in case you have sparse fields for heuristics or scoring."
0,"Populating exception message with InetSocketAddress.getHostName() can take a long time. In the PlainSocketFactory class, when a SocketTimeoutException occurs a call is made to InetSocketAddress.getHostName() when generating the exception message. Unfortunately, this call can take a long time. In my case, the address I am specifying is an IP address, which InetSocketAddress attempts to perform a reverse-lookup on to determine the hostname; however, since  the address does not have a hostname assigned to it, the operation takes a long time to return.

I'm attaching a patch for trunk with my proposed fix. Viewing the source history, it looks like the code used to have the behavior I'm proposing, but it was changed in revision 1070943. Based on the source commits and linked issues, I cannot determine a specific reason for the change. If there is a reason the code needs to be the way it is, then I apologize for inconvenience I have caused."
0,"Release references to JCR items in tearDown. On my 64-Bit environment OS/JVM I tried a ""mvn clean install"" and got an OutOfMemory Exception.
On my 32-Bit environment Mac OSX 10.5 Java 1.5 the tests were all  fine and the IndexMerger was significant faster.

Running org.apache.jackrabbit.test.TestAll
21.11.2007 10:29:51 *INFO * [IndexMerger] IndexMerger: merged 549 documents in 289 ms into _a. (IndexMerger.java, line 304)
21.11.2007 10:29:55 *ERROR* [main] ImportHandler: fatal error encountered at line: 1, column: 10 while parsing XML stream: org.xml.sax.SAXParseException: Attribute name ""is"" associated with an element type ""this"" must be followed by the ' = ' character. (ImportHandler.java, line 116)
21.11.2007 10:29:55 *ERROR* [main] ImportHandler: fatal error encountered at line: 1, column: 10 while parsing XML stream: org.xml.sax.SAXParseException: Attribute name ""is"" associated with an element type ""this"" must be followed by the ' = ' character. (ImportHandler.java, line 104)
21.11.2007 10:29:59 *ERROR* [main] ImportHandler: fatal error encountered at line: -1, column: -1 while parsing XML stream: org.xml.sax.SAXParseException: Premature end of file. (ImportHandler.java, line 104)
21.11.2007 10:29:59 *ERROR* [main] ImportHandler: fatal error encountered at line: -1, column: -1 while parsing XML stream: org.xml.sax.SAXParseException: Premature end of file. (ImportHandler.java, line 116)
21.11.2007 10:30:45 *INFO * [IndexMerger] IndexMerger: merged 555 documents in 2015 ms into _l. (IndexMerger.java, line 304)
21.11.2007 10:33:13 *INFO * [IndexMerger] IndexMerger: merged 412 documents in 25587 ms into _w. (IndexMerger.java, line 304)
Exception in thread ""Timer-1"" java.lang.OutOfMemoryError: Java heap space
        at org.apache.lucene.store.BufferedIndexOutput.<init>(BufferedIndexOutput.java:26)
        at org.apache.lucene.store.FSDirectory$FSIndexOutput.<init>(FSDirectory.java:592)
        at org.apache.lucene.store.FSDirectory.createOutput(FSDirectory.java:435)
        at org.apache.lucene.util.BitVector.write(BitVector.java:122)
        at org.apache.lucene.index.SegmentReader.doCommit(SegmentReader.java:236)
        at org.apache.lucene.index.IndexReader.commit(IndexReader.java:794)
        at org.apache.lucene.index.FilterIndexReader.doCommit(FilterIndexReader.java:190)
        at org.apache.lucene.index.IndexReader.commit(IndexReader.java:825)
        at org.apache.lucene.index.IndexReader.close(IndexReader.java:841)
        at org.apache.jackrabbit.core.query.lucene.AbstractIndex.close(AbstractIndex.java:327)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex$DeleteIndex.execute(MultiIndex.java:1715)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.executeAndLog(MultiIndex.java:936)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.flush(MultiIndex.java:880)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.checkFlush(MultiIndex.java:1110)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.access$100(MultiIndex.java:75)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex$1.run(MultiIndex.java:324)
        at java.util.TimerThread.mainLoop(Timer.java:512)
        at java.util.TimerThread.run(Timer.java:462)
21.11.2007 10:34:37 *ERROR* [main] DatabasePersistenceManager: failed to write node state: cfbffd6d-114d-4738-9383-48da2b5dbc1d (DatabasePersistenceManager.java, line 441)
java.lang.OutOfMemoryError: Java heap space
        at java.util.Properties$LineReader.<init>(Properties.java:346)
        at java.util.Properties.load(Properties.java:284)
"
0,"Exception when missing namespace in CND file should have clearer message. Using the attached CND file, when calling CndImporter.registerNodeTypes(..) the following message is in the returned exception:

""Unable to parse CND Input: Error setting name of sling:resourceType to sling:resourceType (bundle://23.0:0/SLING-INF/nodetypes/types.cnd, line 24)""

The issue with line 24 is that the ""sling"" namespace has not been included.  The message should state that a namespace is missing and what prefix is not understood."
0,"TCK: SetPropertyAssumeTypeTest doesn't allow ValueFormatException upon type conversion failure. SetPropertyAssumeTypeTest# testValuesConstraintVioloationExceptionBecauseOfInvalidTypeParameter

This test should allow an implementation to throw ValueFormatException.  In Section 7.1.5, the Javadoc for setProperty(String, Value[] int) states: ""If the property type of the supplied Value objects is different from that specified, then a best-effort conversion is attempted. If the conversion fails, a ValueFormatException is thrown.""

Proposal: catch and consume ValueFormatException.

--- SetPropertyAssumeTypeTest.java      (revision 422074)
+++ SetPropertyAssumeTypeTest.java      (working copy)
@@ -28,6 +28,7 @@
 import javax.jcr.PropertyType;
 import javax.jcr.RepositoryException;
 import javax.jcr.Property;
+import javax.jcr.ValueFormatException;
 import java.util.Calendar;
 import java.util.Date;
  
@@ -525,6 +526,9 @@
         catch (ConstraintViolationException e) {
             // success
         }
+        catch (ValueFormatException e) {
+            // success
+        }
     }
"
0,"Support http URL inline authentication. If you try to execute a method with the httpClient using a valid url, conform to the schema http://username:password@host:port/ the authentication will fail, and you will get a 401 error."
0,"Improve error messages for index aggregates. In the case where an index aggregate fails because of a node that doesn't exist the logged warn messages contain a full stack-trace.
Besides the fact that this can be misleading (you may think that there's something wrong that you need to fix right away) it is also borderline useless.

The desired behavior would be to just log an ""info"" message mentioning that a certain node was skipped, similar to what the SeachManager does."
0,"QueryParser throws new exceptions even if custom parsing logic threw a better one. We have subclassed QueryParser and have various custom fields.  When these fields contain invalid values, we throw a subclass of ParseException which has a more useful message (and also a localised message.)

Problem is, Lucene's QueryParser is doing this:

{code}
    catch (ParseException tme) {
        // rethrow to include the original query:
        throw new ParseException(""Cannot parse '"" +query+ ""': "" + tme.getMessage());
    }
{code}

Thus, our nice and useful ParseException is thrown away, replaced by one with no information about what's actually wrong with the query (it does append getMessage() but that isn't localised.  And it also throws away the underlying cause for the exception.)

I am about to patch our copy to simply remove these four lines; the caller knows what the query string was (they have to have a copy of it because they are passing it in!) so having it in the error message itself is not useful.  Furthermore, when the query string is very big, what the user wants to know is not that the whole query was bad, but which part of it was bad.

"
0,"API surface of caching module can be reduced. While the caching module can currently be considered functional and useful for folks as-is, there are several near-term enhancements planned that could change the exposed binary API of the caching module (although it is not yet clear whether they would or not). In an effort to allow the 4.1 GA release to go forward while hedging bets against future development, we should consider drastically reducing the exposed binary API of the caching module, and not exposing extension points until someone explicitly asks for them.
"
0,Database Data Store: support database type 'mssql'. MS SQL Server is referred to with the schema name 'mssql' in the persistence managers and the cluster journal. For the DbDataStore it is called 'sqlserver'. This is not consistent.
0,"Add an ""termInfosIndexDivisor"" to IndexReader. The termIndexInterval, set during indexing time, let's you tradeoff
how much RAM is used by a reader to load the indexed terms vs cost of
seeking to the specific term you want to load.

But the downside is you must set it at indexing time.

This issue adds an indexDivisor to TermInfosReader so that on opening
a reader you could further sub-sample the the termIndexInterval to use
less RAM.  EG a setting of 2 means every 2 * termIndexInterval is
loaded into RAM.

This is particularly useful if your index has a great many terms (eg
you accidentally indexed binary terms).

Spinoff from this thread:

  http://www.gossamer-threads.com/lists/lucene/java-dev/54371

"
0,"Remove Hits. LUCENE-1290 removed all references to Hits from core.

Most work to be done here is to remove all references from the contrib modules and some new ones that crept into core after 1290."
0,Remove deprecated methods in CompoundTokenFilters. 
0,"Create a Codec to work with streaming and append-only filesystems. Since early 2.x times Lucene used a skip/seek/write trick to patch the length of the terms dict into a place near the start of the output data file. This however made it impossible to use Lucene with append-only filesystems such as HDFS.

In the post-flex trunk the following code in StandardTermsDictWriter initiates this:
{code}
    // Count indexed fields up front
    CodecUtil.writeHeader(out, CODEC_NAME, VERSION_CURRENT); 

    out.writeLong(0);                             // leave space for end index pointer
{code}
and completes this in close():
{code}
      out.seek(CodecUtil.headerLength(CODEC_NAME));
      out.writeLong(dirStart);
{code}

I propose to change this layout so that this pointer is stored simply at the end of the file. It's always 8 bytes long, and we known the final length of the file from Directory, so it's a single additional seek(length - 8) to read it, which is not much considering the benefits."
0,"Create EMPTY_ARGS constsant in SnowballProgram instead of allocating new Object[0]. Instead of allocating new Object[0] create a proper constant in SnowballProgram. The same (for new Class[0]) is created in Among, although it's less critical because Among is called from static initializers ... Patch will follow shortly."
0,"Rename GenericRepositoryFactory to JndiRepositoryFactory. The GenericRepositoryFactory class introduced in JCR-2360 has since been refactored so that most of its functionality is now distributed among the more implementation-specific RepositoryFactory classes. Now the GenericRepositoryFactory only contains support for looking the repository up in JNDI, so it would be better to rename the class to JndiRepositoryFactory.

The only troublesome part of the rename is the GenericRepositoryFactory.URI constant that was for a while documented on our wiki as a part of the canonical code snippet for accessing a remote repository based on the repository URI. The latest recommendation is to use the JcrUtils.getRepository(String uri) method so the constant is no longer needed in client code, but for backwards compatibility with earlier Jackrabbit 2.0 betas it may be good to keep the deprecated constant for at least the next beta release."
0,"CompoundFileWriter should pre-set its file length. I've read that if you are writing a large file, it's best to pre-set
the size of the file in advance before you write all of its contents.
This in general minimizes fragmentation and improves IO performance
against the file in the future.

I think this makes sense (intuitively) but I haven't done any real
performance testing to verify.

Java has the java.io.File.setLength() method (since 1.2) for this.

We can easily fix CompoundFileWriter to call setLength() on the file
it's writing (and add setLength() method to IndexOutput).  The
CompoundFileWriter knows exactly how large its file will be.

Another good thing is: if you are going run out of disk space, then,
the setLength call should fail up front instead of failing when the
compound file is actually written.  This has two benefits: first, you
find out sooner that you will run out of disk space, and, second, you
don't fill up the disk down to 0 bytes left (always a frustrating
experience!).  Instead you leave what space was available
and throw an IOException.

My one hesitation here is: what if out there there exists a filesystem
that can't handle this call, and it throws an IOException on that
platform?  But this is balanced against possible easy-win improvement
in performance.

Does anyone have any feedback / thoughts / experience relevant to
this?
"
0,"Create org.apache.jackrabbit.api.jsr283 (in jackrabbit-api) and move future jsr283 interfaces and classes there. (as discussed on mailing list)

as the implementation of the new features for jsr283 have started, i
suggest to put the new jsr283 interfaces to jackrabbit-api instead to
core (where possible). this way, we can already figure out
inter-module dependency issues and people can start using experimental
features through an API than rather through casting objects to core
interfaces.

suggestion: use 'org.apache.jackrabbit.api.jsr283' as base package for
the new jsr283 interfaces and classes.

for example, use
org.apache.jackrabbit.api.jsr283.nodetype.NodeDefinitionTemplate
for the future javax.jcr.nodetype.NodeDefinitionTemplate

once jcr2.0 is released, we will drop the interim interfaces.

"
0,"Change default value for maxMergeDocs. This is actually a left over from the time before JCR-197 was implemented. Back then index merges were performed with the client thread and would hold up execution for a long time if a large number of nodes were merged. The default value for maxMergeDocs limited this to 100'000 nodes, resulting in a couple of seconds for the merge operation.

This default value does not make sense anymore because index merges are performed in a background thread and may take a long time without an effect on regular workspace operations. If a workspace grows large it may cause performance degradation because the number of index segments increases linearly when there are more than 100'000 nodes.

I propose to set the new default to Integer.MAX_VALUE"
0,"jcr2spi: use jcr names and path for log and exception message. in a couple of places jcr2spi adds the string representation of Path, Name and ItemId to the exception/error message. it would be convenient to convert them to jcr names and jcr path where ever possible."
0,"need tests to guarantee transparency of caching module on end-to-end headers. ""A transparent proxy SHOULD NOT modify an end-to-end header unless the definition of that header requires or specifically allows that.""

http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.5.2

This is already true of our implementation, but we should have tests to preserve that behavior.
"
0,"OSGi bundle symbolic names changed due to changes in maven-bundle-plugin. See FELIX-1886 - I have noticed this with jackrabbit-jcr-commons 2.0-beta3 but I assume all modules which use maven-bundle-plugin are affected.

Having bundle symbolic name changes is problematic as OSGi frameworks then consider the old a new bundles to be different components, instead of different versions of the same component.

The simplest way to go back to the previous symbolic names is probably to use the maven-bundle-plugin config workaround described in FELIX-1886, in the jackrabbit parent pom (or whever that plugin is configured).

I'll try that and supply a patch."
0,"Rename package core.search to core.query. The package org.apache.jackrabbit.core.search should be renamed to org.apache.jackrabbit.core.query for consistency with the JCR api. All other packages in Jackrabbit already use the package names as suggested by the JCR api (javax.jcr.<subpackage>).

If there are other implementations than the Jackrabbit one of the org.apache.jackrabbit.core.search.QueryHandler interface they need to be adapted to use the new package name!"
0,"Field tokenStream should be usable with stored fields.. Field.tokenStream should be usable for indexing even for stored values.  Useful for many types of pre-analyzed values (text/numbers, etc)
http://search.lucidimagination.com/search/document/902bad4eae20bdb8/field_tokenstreamvalue"
0,"Unnecessary parsing of Name value. When a Name value is created for a call like Property.getValue() the internal QName if formatted, parsed and formatted again."
0,"Create jackrabbit-api(.jar) and the respective jackrabbit-rmi extensions. currently some of the management functions not covered in jcr, like notetype management and workspace creation, are not exposed via any specific api and therfor not accessible via rmi.

create a jackrabbit api and the respective rmi extension."
0,"Persistence: support property databaseType. In persistence managers and cluster journal the term 'schema' is used to mean 'database type'. 

Using the term 'schema' for that is actually quite confusing (in my view):

The definition of schema http://en.wikipedia.org/wiki/Database_schema is ""the schema defines the tables, the fields in each table, and the relationships between fields and tables.""

Additionally in most databases a schema is is a name space within a database: http://java.sun.com/j2se/1.4.2/docs/api/java/sql/DatabaseMetaData.html#getSchemas() .

I suggest to support the property 'databaseType' in addition to 'schema' for the persistence managers and the cluster journal.

"
0,"Remove unnecessary TestAll classes in jcr-commons. The module jackrabbit-jcr-commons uses the default test configuration, which means the TestAll test suites are not necessary. They actually cause all tests to be executed twice."
0,Add timing information to event delivery. There should be debug messages that contain information on how long event listeners spend iterating over the delivered events.
0,"Upgrade to SLF4J 1.3. Version 1.1 of the SLF4J logging facade was recently released. It contains no functional improvements that we'd need, but is split to a separate slf4j-api library and a set of backend-specic logging adapters. This would allow us to avoid exposing log4j as a transitive dependency for projects that depend on Jackrabbit."
0,"better error for unknown date formats. This small patch improves the exception message you get when the date format is unknown.
"
0,"explore morfologik integration. Dawid Weiss mentioned on LUCENE-2298 that there is another Polish stemmer available:
http://sourceforge.net/projects/morfologik/

This works differently than LUCENE-2298, and ideally would be another option for users.
"
0,"make Query.createWeight public (or add back Query.createQueryWeight()). Now that the QueryWeight class has been removed, the public QueryWeight createQueryWeight() method on Query was also removed

i have cases where i want to create a weight for a sub query (outside of the org.apache.lucene.search package) and i don't want the weight normalized (think BooleanQuery outside of the o.a.l.search package)

in order to do this, i have to create a static Utils class inside o.a.l.search, pass in the Query and searcher, and have the static method call the protected createWeight method
this should not be necessary

This could be fixed in one of 2 ways:
1. make createWeight() public on Query (breaks back compat)
2. add the following method:
{code}
public Weight createQueryWeight(Searcher searcher) throws IOException {
  return createWeight(searcher);
}
{code}

createWeight(Searcher) should then be deprectated in favor of the publicly accessible method
"
0,"Make DirectoryTaxonomyWriter's indexWriter member private. DirectoryTaxonomyWriter has a protected indexWriter member. As far as I can tell, for two reasons:

# protected openIndexWriter method which lets you open your own IW (e.g. with a custom IndexWriterConfig).
# protected closeIndexWriter which is a hook for letting you close the IW you opened in the previous one.

The fixes are trivial IMO:
# Modify the method to return IW, and have the calling code set DTW's indexWriter member
# Eliminate closeIW. DTW already has a protected closeResources() which lets you clean other resources you've allocated, so I think that's enough.

I'll post a patch shortly."
0,"reorganize contrib modules. it would be nice to reorganize contrib modules, so that they are bundled together by functionality.

For example:
* the wikipedia contrib is a tokenizer, i think really belongs in contrib/analyzers
* there are two highlighters, i think could be one highlighters package.
* there are many queryparsers and queries in different places in contrib
"
0,"Deploy JCA JAR file to maven repository. Please deploy the JCA JAR file to the maven repository (ibiblio) whenever deploying the RAR artifact.

The JAR is need for non managed usage of the Jackrabbit JCA, eg. for embedding the resource adapter in your application with Spring JCA in order to use XA for Jackrabbit.

It would be nice if this could be done starting at the current 1.3 version (and for future versions, too).

Thanks!"
0,"Redesign NodeInfo.getReferences(). The method returns an array of PropertyIds. When there are lots of references this may become an problem. As with any other return value that potentially is large we should return an iterator.

I suggest to redesign the handling of references in line with recent discussions how child infos are handled.

- A NodeInfo implementation must either return the complete list of PropertyIds or null if it does not want to return the PropertyIds at that time.
- Introduce a new method: Iterator<PropertyId> RepositoryService.getReferences(SessionInfo, NodeId)

This has the following advantages:

- loading of references can be delayed until it is really needed
- large collections of references can be streamed through the SPI"
0,"Merge CharTermAttribute and deprecations to stable. This should be merged to trunk until flex lands, so the analyzers can be ported to new api."
0,Random access non RAM resident IndexDocValues (CSF). There should be a way to get specific IndexDocValues by going through the Directory rather than loading all of the values into memory.
0,"Poor performance race condition in FieldCacheImpl. A race condition exists in FieldCacheImpl that causes a significant performance degradation if multiple threads concurrently request a value that is not yet cached. The degradation is particularly noticable in large indexes and when there a many concurent requests for the cached value.

For the full discussion see the mailing list thread 'Poor performance ""race condition"" in FieldSortedHitQueue' (http://www.gossamer-threads.com/lists/lucene/java-user/38717)."
0,"Node.setPrimaryNodeType should only redefine child-definitions that are not covered by the new effective nt. NodeImpl.setPrimaryNodeType changes the primary node type of an node and resets the definition of child items if required. Currently all child items that are not part of the effective node type of the new primary type get their definition reset or are removed in case not matching definition is found.
From my point of view this doesn't properly cope with mixin types present on the node: child items defined by any of the mixin node types present should probably not be touched (or removed).

I run into this while testing the latest 283 security changes and will try to provide a fix along with those changes."
0,"extend security config -> repository-1.5.dtd. along with issue #JCR-1171 i'd like to extend the configuration. 

this requires a new version (repository-1.5.dtd) of the repository dtd to be present in jackrabbit-site/src/site/resources/dtd in order to have the new versions of repository.xml work properly.

see attached diff that shows the difference to the most recent repository-1.4.dtd

if nobody objects i would put the proposed repository-1.5.dtd to the site.
once this is done i can properly adjust the repository.xml files (uncommenting the DOCTYPE tag) and start committing the new security functionality.

angela"
0,"Expose FilteredTermsEnum from MTQ . MTQ#getEnum() is protected and in order to access it you need to be in the o.a.l.search package. 

here is a relevant snipped from the mailing list discussion

{noformat}
getEnum() is protected so it is intended to be called *only* by subclasses (that's the idea behind protected methods). They are also accessible by other classes from the same package, but that's more a Java bug than a feature. The problem with MTQ is that RewriteMethod is a separate *class* and *not a subclass* of MTQ, so the method cannot be called (it can because of the ""java bug"" called from same package). So theoretically it has to be public otherwise you cannot call getEnum().

Another cleaner fix would be to add a protected final method to RewriteMethod that calls this method from MTQ. So anything subclassing RewriteMethod can get the enum from inside the RewriteMethod class which is the ""correct"" way to handle it. Delegating to MTQ is then ""internal"".
{noformat}"
0,"Mark contrib/wikipedia as experimental. I am going to add javadocs to trunk and 2_3 branch that mark the WikipediaTokenizer as experimental.  I think it is fine to release, but I want people to know that the grammar may change in the next release (although I will try to keep it the same)"
0,"some jcr-client tests fail if a server runs on localhost:80 . This is caused by RepositoryFactoryImplTest.testGetSpi2davRepository() and probably RepositoryFactoryImplTest.testGetSpi2davexRepository(). 

To fix this the tests should set up the required webDav servers for the test runs. Currently the tests just try to connect to localhost:80 expecting the connection to fail in a specific way. "
0,"Add link to ""Benchmarking the HttpClient Caching Module"" article from Comcast Interactive Media. I'd like to add a link into the HttpClient docs detailing some benchmarking we did with the HttpClient Cache module."
0,Package names for spring project do not match update ocm packages. The spring package and tests reference the old graffitto package naming scheme.
0,"Fix pulsingcodec to reuse its enums. PulsingCodec currently doesnt always reuse its enums, which could lead to behavior like LUCENE-3515.

The problem is sometimes it returns the 'wrapped' enum, but other times it returns its 'pulsingenum' depending upon
whether terms are pulsed...

we can use the fact that these enums allow attributes to keep the reuse information for both so it can reuse when stepping through terms.
"
0,Split the wire log into header and content parts..  
0,"refactoring of docvalues params in Codec.java. While working on LUCENE-2621 I am trying to do some cleanup of the Codec APIs, currently Codec.java has a boolean for getDocValuesUseCFS()

I think this is an impl detail that should not be in Codec.java: e.g. i might make a SimpleText impl that uses only 1 file and then the param
is awkward.

So, instead I created Sep impls that dont use CFS (use separate files) and placed them under the sep package, if you don't want to use
CFS you can just use these implementations in your codec."
0,"Similarity.java javadocs and simplifications for 4.0. As part of adding additional scoring systems to lucene, we made a lower-level Similarity
and the existing stuff became e.g. TFIDFSimilarity which extends it.

However, I always feel bad about the complexity introduced here (though I do feel there
are some ""excuses"", that its a difficult challenge).

In order to try to mitigate this, we also exposed an easier API (SimilarityBase) on top of 
it that makes some assumptions (and trades off some performance) to try to provide something 
consumable for e.g. experiments.

Still, we can cleanup a few things with the low-level api: fix outdated documentation and
shoot for better/clearer naming etc.
"
0,"Make the Highlighter use SpanScorer by default. I've always thought this made sense, but frankly, it took me a year to get the SpanScorer included with Lucene at all, so I was pretty much ready to move on after I it got in, rather than push for it as a default.

I think it makes sense as the default in Solr as well, and I mentioned that back when it was put in, but alas, its an option there as well.

The Highlighter package has no back compat req, but custom has been conservative - one reason I havn't pushed for this change before. Might be best to actually make the switch in 3? I could go either way - as is, I know a bunch of people use it, but I'm betting its the large minority. It has never been listed in a changes entry and its not in LIA 1, so you pretty much have to stumble upon it, and figure out what its for.

I'll point out again that its just as fast as the standard scorer for any clause of a query that is not position sensitive. Position sensitive query clauses will obviously be somewhat slower to highlight, but that is because they will be highlighted correctly rather than ignoring position."
0,"synchronize grammar/token types across StandardTokenizer, UAX29EmailURLTokenizer, ICUTokenizer, add CJK types.. I'd like to do LUCENE-2906 (better cjk support for these tokenizers) for a future target such as 3.2

But, in 3.1 I would like to do a little cleanup first, and synchronize all these token types, etc.
"
0,Remove deprecated DocIdSetIterator methods. 
0,standard codec's terms dict seek should only scan if new term is in same index block. TermInfosReader in trunk already optimizes for this case... just need to do the same on flex.
0,"Modified QueryImpl to enable external query builders to read and write JCR expressions in the orderBy Clause. The QueryImpl does not create the JCR expression on the fly. The OrderByExpression does the job. If an external querybuilder class needs to dynamically collect properties against which order by is required, QueryImpl does not support updating the JCR Expression. It can only return the built expression since arrayList is used for collecting the properties. The change enables one to add JCRExpression to the QueryImpl object. A test has been added.

Changed files:
Path
src/main/java/org/apache/jackrabbit/ocm/query/impl/QueryImpl.java
src/test/java/org/apache/jackrabbit/ocm/manager/query/DigesterSimpleQueryTest.java
"
0,"The servlet-api dependency scope should be ""provided"" in jackrabbit-jcr-server. Using the default ""compile"" scope for servlet-api in jackrabbit-jcr-server causes warnings about the scope not being ""provided"" when building jackrabbit-webapp."
0,Jcr2Spi: configuration entry for size of ItemCache. in order to make the size of the ItemCache configurable (see TODO in jcr2spi SessionImpl) i'd like to extend the jcr2spi RepositoryConfig and have a default value being provided with AbstractRepositoryConfig in the tests section.
0,Cut Norms over to DocValues. since IR is now fully R/O and norms are inside codecs we can cut over to use a IDV impl for writing norms. LUCENE-3606 has some [ideas|https://issues.apache.org/jira/browse/LUCENE-3606?focusedCommentId=13160559&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13160559] about how this could be implemented
0,"Reduce memory usage of transient nodes. When adding lots of transient nodes, most of them don't have child nodes because they are leafs. The attached patch initializes NodeState.childNodeEntries with an unmodifiable empty ChildNodeEntries instance and turns it into a modifiable one only when needed.

Running a test with 100k nodes (10 children per node) the memory consumption for child node entries drops from 42MB to 12MB with this patch."
0,"TestCollectionUtil fails on IBM JRE.     [junit] Testcase: testEmptyArraySort(org.apache.lucene.util.TestCollectionUtil):    Caused an ERROR
    [junit] CollectionUtil can only sort random access lists in-place."
0,Rename IOUtils.close methods. The closeSafely methods that take a boolean suppressExceptions are dangerous... I've renamed to .close (no suppression) and .closeWhileHandlingException (suppresses all exceptions).
0,"isHttp11 should have HttpClient scope. -----Original Message-----
From: Kalnichevski, Oleg [mailto:oleg.kalnichevski@bearingpoint.com] 
Sent: Wednesday, January 15, 2003 8:24 AM
To: Commons HttpClient Project
Cc: Rob Owen
Subject: RE: isHttp11 and HTTP/1.0 servers 

Rob
You are basically right hands down. It does make sense for the HTTP version 
flag to have HttpClient scope. We should address this shortcoming as a part of 
the post-2.0-release redesign

Feel free to file a bug report to make sure the issue does not go forgotten

http://nagoya.apache.org/bugzilla/enter_bug.cgi?product=Commons

Many thanks for bring it up

Cheers

Oleg

-----Original Message-----
From: Rob Owen [mailto:Rob.Owen@sas.com]
Sent: Monday, January 13, 2003 18:31
To: Commons HttpClient Project
Subject: isHttp11 and HTTP/1.0 servers 


The boolean variable http11 is set on a method by method basis. For PutMethod, 
decisions (eg. Expect: 100-continue request header) are made prior to 
determining the value for Http11 (chicken and egg problem) and so the default 
(true) is used to produce the request. An HTTP/1.0 server hangs waiting for 
the extra data on the PUT method body. 

For applications that are using HttpClient (ie. they do not manipulate the 
HTTP methods directly and cannot be expected to set the value of Http11 for 
each method instance), shouldn't http11 have HttpClient scope ? This would 
allow an interaction (eg. OPTIONS) to set http11 and all methods thereafter 
would use this setting?
  
------
Rob Owen
SAS Institute Inc.
email: Rob.Owen@sas.com"
0,"Add a way to locate full text extraction problems. Full text indexing of a binary document can fail for various reasons. Currently we just log a generic error message in such cases, which makes it difficult for the user to locate such problems for review and reindexing. We should improve this by making the logs more informative or by adding some other mechanism for locating troublesome documents."
0,"Javadocs for Scorer.java and TermScorer.java. Javadocs for Scorer.java and TermScorer.java 
Also changed build.xml to use package access for the 
javadocs target. That caused some minor error javadoc messages 
in CompoundFileReader.java and FieldInfos.java, which are also fixed. 
 
The patch posted earlier for Weight.java 
(a broken javadoc link) is also included. 
 
The attached patch is for all 5 files against the CVS top directory 
of 28 July 2004. The only dependency is that package access 
is needed for TermScorer.java. 
 
This might be changed by declaring TermScorer as a public class, 
but I preferred to use javadoc package access in build.xml 
over changing java code. 
 
Using package access for javadocs shows some more undocumented 
classes, eg. in the doc page of the search package. This might 
encourage folks to write more javadocs... 
 
Regards, 
Paul"
0,Add path encoding to ISO9075. The utility class ISO9075 only allows you to encode and decode names. It should also have methods that allow you to pass a path. This is useful when a XPath query is created with a path constraint based on e.g. a Node.getPath().
0,"javadocs creation has too many warnings/errors. Currently running 'ant javadocs' creates so many warnings that you have to grep the output to verify that new code did not add more.

While most current errors might be minor, they might hide a few serious ones that we will never know abut until someone complains. 

Best if we fix all of them and keep it always clean..."
0,Update the Highlighter to use the new TokenStream API. 
0,"Provide an convenience AttributeFactory that implements all default attributes with Token. I found some places in contrib tests, where the Token.class was added using addAttributeImpl(). The problem here is, that you cannot be sure, that the attribute is really added and you may fail later (because you only update your local instance). The tests in contrib will partially fail with 3.0 without backwards layer (because the backwards layer uses Token/TokenWrapper internally and copyTo() will work.

The correct way to achieve this is using an AttributeFactory. The AttributeFactory is currently private in SingleTokenTokenStream. I want to move it to Token.java as a static class / static member. In this case the tests can be rewritten.

I also want to mark addAttributeImpl() as EXPERT, because you must really know whats happening and what are the traps."
0,"commons codec not documented as a dependency. Except for some entries on the jdepend-report, commons codec is not documented
as a dependency for using HttpClient 3.0 RC1.  The only dependency documented is
commons logging.

java.lang.NoClassDefFoundError: org/apache/commons/codec/DecoderException
	at org.apache.commons.httpclient.HttpMethodBase.<init>(HttpMethodBase.java:217)
	at
org.apache.commons.httpclient.methods.ExpectContinueMethod.<init>(ExpectContinueMethod.java:92)
	at
org.apache.commons.httpclient.methods.EntityEnclosingMethod.<init>(EntityEnclosingMethod.java:114)
	at org.apache.commons.httpclient.methods.PostMethod.<init>(PostMethod.java:105)"
0,"Consolidate all (Solr's & Lucene's) analyzers into modules/analysis. We've been wanting to do this for quite some time now...  I think, now that Solr/Lucene are merged, and we're looking at opening an unstable line of development for Solr/Lucene, now is the right time to do it.

A standalone module for all analyzers also empowers apps to separately version the analyzers from which version of Solr/Lucene they use, possibly enabling us to remove Version entirely from the analyzers.

We should also do LUCENE-2309 (decouple, as much as possible, indexer from the analysis API), but I don't think that issue needs to block this consolidation.

Once we do this, there is one place where our users can find all the analyzers that Solr/Lucene provide."
0,"Add new top-level projects to the building documentation. The current building.xml file only mentions jackrabbit and contrib as being top level Jackrabbit projects. With the push towards 1.0, there are now several additional projects at the same level as jackrabbit and contrib. The building.xml page should be updated to mention and link to them. It should also provide a link into the subversion repository that is labeled as something like ""Current Jackrabbit project list""; even though the link location is the same as the link to the repository, the label will help readers know that's where to look for the most up-to-date list of Jackrabbit projects."
0,"don't spawn thread statically in FSDirectory on Mac OS X. on the Mac, creating the digester starts up a PKCS11 thread.

I do not think threads should be created statically (I have this same issue with TimeLimitedCollector and also FilterManager).

Uwe discussed removing this md5 digester, I don't care if we remove it or not, just as long as it doesn't create a thread,
and just as long as it doesn't use the system default locale."
0,spi2dav : list of known issues in the pom.xml. 
0,"Reduce Fieldable, AbstractField and Field complexity. In order to move field type like functionality into its own class, we really need to try to tackle the hierarchy of Fieldable, AbstractField and Field.  Currently AbstractField depends on Field, and does not provide much more functionality that storing fields, most of which are being moved over to FieldType.  Therefore it seems ideal to try to deprecate AbstractField (and possible Fieldable), moving much of the functionality into Field and FieldType."
0,"LocalNamespaceMappings does not make use of NameCache in NamespaceRegistryImpl. This basically means that the NameCache in NamespaceRegistryImpl is never used.

The LocalNamespaceMappings should also implement NameCache and forward calls to the NamespaceRegistryImpl for names
with namespace URIs that are not locally remapped. See proposed patch."
0,"remove relative paths assumptions from benchmark code. Also see Eric comments in:
   http://www.nabble.com/forum/ViewPost.jtp?post=14347924&framed=y

Benchmark's config.xml relies on relative paths, more or less like this;
- base-dir
   -- conf-dir
   -- work-dir
       --- docs-dir
       --- indexes-dir

These assumptions are also in the Java code, and so it is inconvenient for
using absolute paths, e.g. for specifying a docs dir that is not under work-dir.

Relax this by modifying in build.xml to replace ""value"" and ""line"" props by 
""location"" and ""file"" and by requiring absolute paths in the Java code."
0,"add FieldInvertState.numUniqueTerms, Terms.sumDocFreq. For scoring systems like lnu.ltc (http://trec.nist.gov/pubs/trec16/papers/ibm-haifa.mq.final.pdf), we need to supply 3 stats:
* average tf within d
* # of unique terms within d
* average number of unique terms across field

If we add FieldInvertState.numUniqueTerms, you can incorporate the first two into your norms/docvalues (once we cut over),
the average tf within d being length / numUniqueTerms.

to compute the average across the field, we can just write the sum of all terms' docfreqs into the terms dictionary header,
and you can then divide this by maxdoc to get the average.
"
0,Configurable Similarity. The similarity implementation for indexing and searching should be configurable.
0,"Change IndexSearcher multisegment searches to search each individual segment using a single HitCollector. This issue changes how an IndexSearcher searches over multiple segments. The current method of searching multiple segments is to use a MultiSegmentReader and treat all of the segments as one. This causes filters and FieldCaches to be keyed to the MultiReader and makes reopen expensive. If only a few segments change, the FieldCache is still loaded for all of them.

This patch changes things by searching each individual segment one at a time, but sharing the HitCollector used across each segment. This allows FieldCaches and Filters to be keyed on individual SegmentReaders, making reopen much cheaper. FieldCache loading over multiple segments can be much faster as well - with the old method, all unique terms for every segment is enumerated against each segment - because of the likely logarithmic change in terms per segment, this can be very wasteful. Searching individual segments avoids this cost. The term/document statistics from the multireader are used to score results for each segment.

When sorting, its more difficult to use a single HitCollector for each sub searcher. Ordinals are not comparable across segments. To account for this, a new field sort enabled HitCollector is introduced that is able to collect and sort across segments (because of its ability to compare ordinals across segments). This TopFieldCollector class will collect the values/ordinals for a given segment, and upon moving to the next segment, translate any ordinals/values so that they can be compared against the values for the new segment. This is done lazily.

All and all, the switch seems to provide numerous performance benefits, in both sorted and non sorted search. We were seeing a good loss on indices with lots of segments (1000?) and certain queue sizes / queries, but the latest results seem to show thats been mostly taken care of (you shouldnt be using such a large queue on such a segmented index anyway).

* Introduces
** MultiReaderHitCollector - a HitCollector that can collect across multiple IndexReaders. Old HitCollectors are wrapped to support multiple IndexReaders.
** TopFieldCollector - a HitCollector that can compare values/ordinals across IndexReaders and sort on fields.
** FieldValueHitQueue - a Priority queue that is part of the TopFieldCollector implementation.
** FieldComparator - a new Comparator class that works across IndexReaders. Part of the TopFieldCollector implementation.
** FieldComparatorSource - new class to allow for custom Comparators.
* Alters
** IndexSearcher uses a single HitCollector to collect hits against each individual SegmentReader. All the other changes stem from this ;)
* Deprecates
** TopFieldDocCollector
** FieldSortedHitQueue
"
0,"RequestEntity, EntityEnclosingMethod have inconsistent Javadocs, use deprecated variables. Robert Manning <Robert.Manning at collabraspace.com> reported a problem on the
httpclient-user list regarding inconsistencies in javadoc of RequestEntity
interface and EntityEnclosingMethod class. This prompted me to review the said
classes. I have discovered several issues that must be dealt with before 3.0
goes final. 

(1) There's virtually no test coverage for the EntityEnclosingMethod class
(2) The code in EntityEnclosingMethod class extensively uses deprecated methods
and variables beyond what is required to maintain backward compatibility with
2.0.x API
(3) Existing code cannot gracefully handle faulty RequestEntity implementations
if the getContentLength method returns a negative value < -2

I have committed additional test cases to cover the most fundamental
functionality of EntityEnclosingMethod:
http://svn.apache.org/repos/asf/jakarta/commons/proper/httpclient/trunk/src/test/org/apache/commons/httpclient/TestEntityEnclosingMethod.java

I will submit a patch addressing issues (2) and (3) shortly

Oleg"
0,"New TokenStream API. This is a very early version of the new TokenStream API that 
we started to discuss here:

http://www.gossamer-threads.com/lists/lucene/java-dev/66227

This implementation is a bit different from what I initially
proposed in the thread above. I introduced a new class called
AttributedToken, which contains the same termBuffer logic 
from Token. In addition it has a lazily-initialized map of
Class<? extends Attribute> -> Attribute. Attribute is also a
new class in a new package, plus several implementations like
PositionIncrementAttribute, PayloadAttribute, etc.

Similar to my initial proposal is the prototypeToken() method
which the consumer (e. g. DocumentsWriter) needs to call.
The token is created by the tokenizer at the end of the chain
and pushed through all filters to the end consumer. The 
tokenizer and also all filters can add Attributes to the 
token and can keep references to the actual types of the
attributes that they need to read of modify. This way, when
boolean nextToken() is called, no casting is necessary.

I added a class called TestNewTokenStreamAPI which is not 
really a test case yet, but has a static demo() method, which
demonstrates how to use the new API.

The reason to not merge Token and TokenStream into one class 
is that we might have caching (or tee/sink) filters in the 
chain that might want to store cloned copies of the tokens
in a cache. I added a new class NewCachingTokenStream that
shows how such a class could work. I also implemented a deep
clone method in AttributedToken and a 
copyFrom(AttributedToken) method, which is needed for the 
caching. Both methods have to iterate over the list of 
attributes. The Attribute subclasses itself also have a
copyFrom(Attribute) method, which unfortunately has to down-
cast to the actual type. I first thought that might be very
inefficient, but it's not so bad. Well, if you add all
Attributes to the AttributedToken that our old Token class
had (like offsets, payload, posIncr), then the performance
of the caching is somewhat slower (~40%). However, if you 
add less attributes, because not all might be needed, then
the performance is even slightly faster than with the old API.
Also the new API is flexible enough so that someone could
implement a custom caching filter that knows all attributes
the token can have, then the caching should be just as 
fast as with the old API.


This patch is not nearly ready, there are lot's of things 
missing:

- unit tests
- change DocumentsWriter to use new API 
  (in backwards-compatible fashion)
- patch is currently java 1.5; need to change before 
  commiting to 2.9
- all TokenStreams and -Filters should be changed to use 
  new API
- javadocs incorrect or missing
- hashcode and equals methods missing in Attributes and 
  AttributedToken
  
I wanted to submit it already for brave people to give me 
early feedback before I spend more time working on this."
0,"revise max-per-host configuration. Max-per-host settings for ThreadSafeClientConnManagers are currently stored in HttpParams, where the parameter value is a map from HttpRoute (formerly HostConfiguration) to Integer. This has several drawbacks:

1) maintaining a map as a value in HttpParams doesn't match my understanding of how params should be used
2) the maximums based on HttpRoute are really specific to the TSCCM implementation and not a generic parameterization

some of the options are:

a) revise to define a more generic parameterization approach
b) revise into an implementation specific parameterization approach
c) define an implementation (TSCCM) specific configuration interface and a default implementation keeping the map as run-time data

cheers,
  Roland
"
0,"rename optimize to a less cool-sounding name. I think users see the name optimize and feel they must do this, because who wants a suboptimal system? but this probably just results in wasted time and resources.

maybe rename to collapseSegments or something?"
0,"Connection pool uses Thread.interrupt(). The connection pool for TSCCM uses Thread.interrupt() to wake up waiting threads.
This interferes with application interrupts.

- expose InterruptedException in interface
- change pool implementation to use wait/notify
"
0,"Document Vector->ArrayList. Document Vector should be changed to ArrayList.
Document is not advertised to be thread safe, and it's doubtful that anyone modifies a Document from multiple threads."
0,"ConjunctionScorer - more tuneup. (See also: #LUCENE-443)
I did some profile testing with the new ConjuctionScorer in 2.1 and discovered a new bottleneck in ConjunctionScorer.sortScorers. The java.utils.Arrays.sort method is cloning the Scorers array on every sort, which is quite expensive on large indexes because of the size of the 'norms' array within, and isn't necessary. 

Here is one possible solution:

  private void sortScorers() {
// squeeze the array down for the sort
//    if (length != scorers.length) {
//      Scorer[] temps = new Scorer[length];
//      System.arraycopy(scorers, 0, temps, 0, length);
//      scorers = temps;
//    }
    insertionSort( scorers,length );
    // note that this comparator is not consistent with equals!
//    Arrays.sort(scorers, new Comparator() {         // sort the array
//        public int compare(Object o1, Object o2) {
//          return ((Scorer)o1).doc() - ((Scorer)o2).doc();
//        }
//      });
  
    first = 0;
    last = length - 1;
  }
  private void insertionSort( Scorer[] scores, int len)
  {
      for (int i=0; i<len; i++) {
          for (int j=i; j>0 && scores[j-1].doc() > scores[j].doc();j-- ) {
              swap (scores, j, j-1);
          }
      }
      return;
  }
  private void swap(Object[] x, int a, int b) {
    Object t = x[a];
    x[a] = x[b];
    x[b] = t;
  }
 
The squeezing of the array is no longer needed. 
We also initialized the Scorers array to 8 (instead of 2) to avoid having to grow the array for common queries, although this probably has less performance impact.

This change added about 3% to query throughput in my testing.

Peter
"
0,"Support lower-/upper-case functions. The query languages should support lower- and upper-case functions when matching property values to string literals.

Example 1: find all nodes with a string property foo with a lower-cased value that equals 'bar':

In XPath that's:

//*[fn:lower-case(@foo) = 'bar']

An in SQL:
SELECT * FROM nt:base WHERE LOWER(foo) = 'bar'

Example 2: find all nodes with a string property foo with an upper-cased value that matches '%JCR%'

XPath: //*[jcr:like(fn:upper-case(@foo), '%JCR%')]

SQL: SELECT * FROM nt:base WHERE UPPPER(foo) LIKE '%JCR%'"
0,"benchmark/stats package is obsolete and unused - remove it. This seems like a leftover from the original benchmark implementation and can thus be removed.
"
0,"RandomAccessFile mode ""w"" is not valid. According to the Java docs for RandomAccessFile, mode must be ""r"" ""rw"" ""rws"" or ""rwd"" - anything else results in an IllegalArgumentException. It seems that Sun/Oracle/OpenJDK's don't document it, but supports ""w"" mode that is equivalent to ""rw"" Android does as the Javadocs say, and throws an IllegalArgumentException when mode ""w"" is passed as HttpClientCache does IOUtils.copyFile() (line 70-71).

This means that HttpClient Cache does not work on Android."
0,"Make DocsEnum subclass of DocIdSetIterator. Spinoff from LUCENE-1458:

One thing I came along long time ago, but now with a new API it get's interesting again: 
DocsEnum should extend DocIdSetIterator, that would make it simplier to use and implement e.g. in MatchAllDocQuery.Scorer, FieldCacheRangeFilter and so on. You could e.g. write a filter for all documents that simply returns the docs enumeration from IndexReader.

So it should be an abstract class that extends DocIdSetIterator. It has the same methods, only some methods must be a little bit renamed. The problem is, because java does not support multiple inheritace, we cannot also extends attributesource  Would DocIdSetIterator be an interface it would work (this is one of the cases where interfaces for really simple patterns can be used, like iterators).

The problem with multiple inheritance could be solved by an additional method attributes() that creates a new AttributeSource on first access then (because constructing an AttributeSource is costly).  The same applies for the other *Enums, it should be separated for lazy init.

DocsEnum could look like this:

{code}
public abstract class DocsEnum extends DocIdSetIterator {
  private AttributeSource atts = null;
  public int freq()
  public DontKnowClassName positions()
  public final AttributeSource attributes() {
   if (atts==null) atts=new AttributeSource();
   return atts;
  }
  ...default impl of the bulk access using the abstract methods from DocIdSetIterator
}
{code}
"
0,Update LICENSE and NOTICE files to match the updated dependencies. We've made quite a few dependency updates since Jackrabbit 1.6 and need to update the license metadata accordingly.
0,"Abstract JCR base classes. Implement and use a set of abstract AbstractSession, AbstractItem, etc. classes that implement as much of the respective JCR interfaces using nothing else but calls to other JCR methods. These would be just like the AbstractMap, etc. classes in java.util.

(See the related discussion at http://thread.gmane.org/gmane.comp.apache.jackrabbit.devel/10583)"
0,"Fix wrong clover analysis because of backwards-tests, upgrade clover to 2.6.3 or better. This is a followup for [http://www.lucidimagination.com/search/document/6248d6eafbe10ef4/build_failed_in_hudson_lucene_trunk_902]

The problem with clover running on hudson is, that it does not instrument all tests ran. The autodetection of clover 1.x is not able to find out which files are the correct tests and only instruments the backwards test. Because of this, the current coverage report is only from the backwards tests running against the current Lucene JAR.

You can see this, if you install clover and start the tests. During test-core no clover data is added to the db, only when backwards-tests begin, new files are created in the clover db folder.

Clover 2.x supports a new ant task, <testsources> that can be used to specify the files, that are the tests. It works here locally with clover 2.4.3 and produces a really nice coverage report, also linking with test files work, it tells which tests failed and so on.

I will attach a patch, that changes common-build.xml to the new clover version (other initialization resource) and tells clover where to find the tests (using the test folder include/exclude properties).

One problem with the current patch: It does *not* instrument the backwards branch, so you see only coverage of the core/contrib tests. Getting the coverage also from the backwards tests is not easy possible because of two things:
- the tag test dir is not easy to find out and add to <testsources> element (there may be only one of them)
- the test names in BW branch are identical to the trunk tests. This completely corrupts the linkage between tests and code in the coverage report.

In principle the best would be to generate a second coverage report for the backwards branch with a separate clover DB. The attached patch does not instrument the bw branch, it only does trunk tests."
0,"light/minimal stemming for euro languages. The snowball stemmers are very aggressive and it would be nice if there were lighter alternatives.

Some applications may want to perform less aggressive stemming, for example:
http://www.lucidimagination.com/search/document/5d16391e21ca6faf/plural_only_stemmer

Good, relevance tested algorithms exist and I think we should provide these alternatives."
0,"Add solr's artifact signing scripts into lucene's build.xml/common-build.xml. Solr has nice artifact signing scripts in its common-build.xml and build.xml.

For me as release manager of 3.0 it would have be good to have them also when building lucene artifacts. I will investigate how to add them to src artifacts and maven artifacts"
0,"Optimize ReadOnlyIndexReader.read(int[] docs, int[] freqs). This method is currently implemented trivially using next(), doc() and freq(). It should read in blocks and filter out deleted docs."
0,"Authorization credentials should be sent pre-emptively. When a web browser receives a <i>401: Unauthorized</i> response code, the
browser prompts for the user and password credentials for the requested
authentication realm.  An Authorization header is then sent for this request. 
HttpClient models this behaviour quite well.

After the web browser has the authentication credentials for a given host, port
and realm, it then sends the Authorization header for subsequent requests
pre-emptively, whithout need for a 401 response.  HttpClient always reqires a
401 response before it will send out the Authorization header.

As <code>HttpClient.startSession()</code> will take a <code>Credentials</code>
object as a parameter as the default credentials, the default credentials should
be sent as part of every request in that session.  Some mechanisim for
over-riding the default credentials should also be provided to be sent
pre-emptively.

The point of this enhancement request is to minimize the number of unnecessisary
401 responses.

It appears that the simple solution might be to modify the logic of when
<code>Authenticator.authenticate()</code> gets called in
<code>HttpMethodBase.addAuthorizationRequestHeader()</code>"
0,"Make WildcardTermEnum#difference() non-final. The method WildcardTermEnum#difference() is declared final. I found it very useful to subclass WildcardTermEnum to implement different scoring for exact vs. partial matches. The change is rather trivial (attached)  but I guess it could make life easier for a couple of users.

I attached two patches:
 - one which contains the single change to make difference() non-final (WildcardTermEnum.patch)
 - one which does also contain some minor cleanup of WildcardTermEnum. I removed unnecessary member initialization and made those final. ( WildcardTermEnum_cleanup.patch)

Thanks simon"
0,"Remove ImportContextImpl#getDetector. the method ImportContextImpl#getDetector refers an interface method on ImportContext that does not exist.
according to jukka that is a leftover. since the method is not used at all i would therefore suggest to remove it altogether, remove the instance field and deprecate the constructor taking the detector param."
0,"Default charset. As defined in RFC2616 the default character set is ISO-8859-1 an not US-ASCII 
as defined in HttpMethodBase. See ""3.7.1 Canonicalization and Text Defaults"" at
RFC 2616"
0,"The native FS lock used in test-framework's o.a.l.util.LuceneJUnitResultFormatter prohibits testing on a multi-user system. {{LuceneJUnitResultFormatter}} uses a lock to buffer test suites' output, so that when run in parallel, they don't interrupt each other when they are displayed on the console.

The current implementation uses a fixed directory ({{lucene_junit_lock/}} in {{java.io.tmpdir}} (by default {{/tmp/}} on Unix/Linux systems) as the location of this lock.  This functionality was introduced on SOLR-1835.

As Shawn Heisey reported on SOLR-2739, some tests fail when run as root, but succeed when run as a non-root user.  

On #lucene IRC today, Shawn wrote:
{quote}
(2:06:07 PM) elyograg: Now that I know I can't run the tests as root, I have discovered /tmp/lucene_junit_lock.  Once you run the tests as user A, you cannot run them again as user B until that directory is deleted, and only root or the original user can do so.
{quote}
"
0,Fix SnowballAnalyzer casing behavior for Turkish Language. LUCENE-2102 added a new TokenFilter to handle Turkish unique casing behavior correctly. We should fix the casing behavior in SnowballAnalyzer too as it supports a TurkishStemmer.
0,"Move Kuromoji to analysis.ja and introduce Japanese* naming. Lucene/Solr 3.6 and 4.0 will get out-of-the-box Japanese language support through {{KuromojiAnalyzer}}, {{KuromojiTokenizer}} and various other filters.  These filters currently live in {{org.apache.lucene.analysis.kuromoji}}.

I'm proposing that we move Kuromoji to a new Japanese package {{org.apache.lucene.analysis.ja}} in line with how other languages are organized.  As part of this, I also think we should rename {{KuromojiAnalyzer}} to {{JapaneseAnalyzer}}, etc. to further align naming to our conventions by making it very clear that these analyzers are for Japanese.  (As much as I like the name ""Kuromoji"", I think ""Japanese"" is more fitting.)

A potential issue I see with this that I'd like to raise and get feedback on, is that end-users in Japan and elsewhere who use lucene-gosen could have issues after an upgrade since lucene-gosen is in fact releasing its analyzers under the {{org.apache.lucene.analysis.ja}} namespace (and we'd have a name clash).

I believe users should have the freedom to choose whichever Japanese analyzer, filter, etc. they'd like to use, and I don't want to propose a name change that just creates unnecessary problems for users, but I think the naming proposed above is most fitting for a Lucene/Solr release.
"
0,"NumericRange support for new query parser. It would be good to specify some type of ""schema"" for the query parser in future, to automatically create NumericRangeQuery for different numeric types? It would then be possible to index a numeric value (double,float,long,int) using NumericField and then the query parser knows, which type of field this is and so it correctly creates a NumericRangeQuery for strings like ""[1.567..*]"" or ""(1.787..19.5]"".

There is currently no way to extract if a field is numeric from the index, so the user will have to configure the FieldConfig objects in the ConfigHandler. But if this is done, it will not be that difficult to implement the rest.

The only difference between the current handling of RangeQuery is then the instantiation of the correct Query type and conversion of the entered numeric values (simple Number.valueOf(...) cast of the user entered numbers). Evenerything else is identical, NumericRangeQuery also supports the MTQ rewrite modes (as it is a MTQ).

Another thing is a change in Date semantics. There are some strange flags in the current parser that tells it how to handle dates."
0,"Deprecate Directory.touchFile. Lucene doesn't use this method, and, FindBugs reports that FSDirectory's impl shouldn't swallow the returned result from File.setLastModified."
0,"Improve how IndexWriter flushes deletes against existing segments. IndexWriter buffers up all deletes (by Term and Query) and only
applies them if 1) commit or NRT getReader() is called, or 2) a merge
is about to kickoff.

We do this because, for a large index, it's very costly to open a
SegmentReader for every segment in the index.  So we defer as long as
we can.  We do it just before merge so that the merge can eliminate
the deleted docs.

But, most merges are small, yet in a big index we apply deletes to all
of the segments, which is really very wasteful.

Instead, we should only apply the buffered deletes to the segments
that are about to be merged, and keep the buffer around for the
remaining segments.

I think it's not so hard to do; we'd have to have generations of
pending deletions, because the newly merged segment doesn't need the
same buffered deletions applied again.  So every time a merge kicks
off, we pinch off the current set of buffered deletions, open a new
set (the next generation), and record which segment was created as of
which generation.

This should be a very sizable gain for large indices that mix
deletes, though, less so in flex since opening the terms index is much
faster.
"
0,"Add way to check for release of connections back into pool. As the documentation emphasizes, its important to clean up HttpEntities after use, so they don't tie up the default very small number of connections in the pool.

However, nothing is provided to HttpClient users with the default classes that allows them to unit test their code to help verify that they are in fact properly releasing the connections under all circumstances. One way this could be done is for the API to expose the number of current leased (taken) connections in the pool, which would be connManager.pool.leasedConnections.size() if not for the necessary fields being protected. If this statistic were published through the API, user unit tests could check that it is zero when they finish.

A workaround is for the user to subclass both the connection manager and ConnPoolByRoute and add getter methods. But its kind of a clunky solution, and I think the API should be written to encourage its users to perform this check."
0,Perform random operation tests. As discussed on the mailing list and in other jira issues it makes sense to execute tests that perform random operations on the repository. This helps us detect concurrency issues in jackrabbit and increase code coverage.
0,change some access-level modifiers to allow for better subclassing. Some of the methods in the core parts of jackrabbit are package protected and do not allow easy subclassing. suggest to make some of the methods 'protected'.
0,"ISOLatin1AccentFilter a bit slow. The ISOLatin1AccentFilter is a bit slow giving 300+ ms responses when used in a highligher for output responses.

Patch to follow"
0,"Improve the documentation of Version. In my opinion, we should elaborate more on the effects of changing the Version parameter.
Particularly, changing this value, even if you recompile your code, likely involves reindexing your data.
I do not think this is adequately clear from the current javadocs.
"
0,"Add VERBOSE to LuceneTestCase and LuceneTestCaseJ4. component-build.xml allows to define tests.verbose as a system property when running tests. Both LuceneTestCase and LuceneTestCaseJ4 don't read that property. It will be useful for overriding tests to access one place for this setting (I believe currently some tests do it on their own). Then (as a separate issue) we can move all tests that don't check the parameter to only print if VERBOSE is true.

I will post a patch soon."
0,"Rethrow exception with cause in BundleDbPersistenceManager. An exception forwarded from SQL should have a cause for better diagnosis.

Index: jackrabbit-core/src/main/java/org/apache/jackrabbit/core/persistence/bundle/BundleDbPersistenceManager.java
===================================================================
--- jackrabbit-core/src/main/java/org/apache/jackrabbit/core/persistence/bundle/BundleDbPersistenceManager.java (revision 585555)
+++ jackrabbit-core/src/main/java/org/apache/jackrabbit/core/persistence/bundle/BundleDbPersistenceManager.java (working copy)
@@ -604,7 +604,7 @@
             }
             return nameIndex;
         } catch (Exception e) {
-            throw new IllegalStateException(""Unable to create nsIndex: "" + e);
+            throw new IllegalStateException(""Unable to create nsIndex"", e);
         }
     }
 "
0,"Absorb NIOFSDirectory into FSDirectory. I think whether one uses java.io.* vs java.nio.* or eventually
java.nio2.*, or some other means, is an under-the-hood implementation
detail of FSDirectory and doesn't merit a whole separate class.

I think FSDirectory should be the core class one uses when one's index
is in the filesystem.

So, I'd like to deprecate NIOFSDirectory, absorbing it into
FSDirectory, and add a setting ""useNIO"" to FSDirectory.  It should
default to ""true"" for non-Windows OSs, because it gives far better
concurrent performance on all platforms but Windows (due to known Sun
JRE issue http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6265734).
"
0,"outdated information in Analyzer javadoc. I'm sure you find more ways to improve the javadoc, so feel free to change and extend my patch."
0,EventImpl should implement toString. This would simplify logging and debugging.
0,"TopDocsCollector should be abstract super class that is the real ""TopDocsCollector"" contract, a subclass should implement the priority-queue logic. e.g. PQTopDocsCollector. TopDocsCollector is both an abstract interface for producing TopDocs as well as a PriorityQueue based implementation.
Not all Collectors that could produce TopDocs must use a PriorityQueue, and it would be advantageous to allow the TopDocsCollector to be an ""interface"" type abstract class, with a PQTopDocsCollector sub-class.
While doing this, it'd be good to clean up the generics uses in these classes. As it's odd to create a TopFieldCollector and have to case the TopDocs object, when this can be fixed with generics."
0,"shouldn't throw exception on bad cookies. Currently, HttpClient throws Exception on bad cookie. This is not expected. The 
user will expect HttpClient to ignore such cookies, but not getting an 
exception. Once exception is throw, user has no way to know if he can continue."
0,"SPI: RepositoryService.obtain should allow to pass null workspaceName indicating the default workspace. improvement suggested by tobi

the contract of

public SessionInfo obtain(Credentials credentials, String workspaceName)
public SessionInfo obtain(Credentials credentials, String workspaceName)

should be changed to allow for null workspaceName.

* @param workspaceName the name of the workspace the <code>SessionInfo</code>
* should be built for. <code>null</code> indicates that the info should be created for the
* default workspace.

consequently we could either deprecate 

RepositoryConfig.getDefaultWorkspaceName()

or allow it to return null as well or remove it altogether.




"
0,"make applying deletes optional when pulling a new NRT reader. Usually when you pull an NRT reader, you want all deletes to be applied.

But in some expert cases you may not need it (eg you just want to validate that the doc was indexed).  Since it's costly to apply deletes, and trivial to add this boolean (we already have a boolean internally), I think we should add it.

The deletes are still buffered, and you can always later pull another reader (for ""real"" searching) with deletes applied."
0,"Remove DocumentBuilder interface from facet module. The facet module contains an interface called DocumentBuilder, which contains a single method, build(Document) (it's a builder API). We use it in my company to standardize how different modules populate a Document object. I've included it with the facet contribution so that things will compile with as few code changes as possible.

Now it's time to do some cleanup and I'd like to start with this interface. If people think that this interface is useful to reside in 'core', then I don't mind moving it there. But otherwise, let's remove it from the code. It has only one impl in the facet module: CategoryDocumentBuilder, and we can certainly do without the interface.

More so, it's under o.a.l package which is inappropriate IMO. If it's moved to 'core', it should be under o.a.l.document.

If people see any problem with that, please speak up. I will do the changes and post a patch here shortly."
0,"Supplementary Character Handling in CharTokenizer. CharTokenizer is an abstract base class for all Tokenizers operating on a character level. Yet, those tokenizers still use char primitives instead of int codepoints. CharTokenizer should operate on codepoints and preserve bw compatibility. "
0,"[PATCH] Better ""lock obtain timed out"" error message. The attached patch prints the complete path and name of the lock file. This 
should simplify debugging (it's actually a wish from the Wiki)."
0,"Avoid String.intern() for UUID terms. Creating Lucene terms is somewhat expensive, because it will usually call String.intern() on the field String. Jackrabbit uses UUID terms quite heavily to resolve hierarchy constraints. Lucene also provides a factory method on a Term that will create a new term instance with a given value and the same field name, avoiding the String.intern(). Jackrabbit should use the factory method whenever it creates a term for a UUID field."
0,"Improve CachingWrapperFilter to optionally also cache acceptDocs, if identical to liveDocs. Spinoff from LUCENE-1536: This issue removed the different cache modes completely and always applies the acceptDocs using BitsFilteredDocIdSet.wrap(), the cache only contains raw DocIdSet without any deletions/acceptDocs. For IndexReaders that are seldom reopened, this might not be as performant as it could be. If the acceptDocs==IR.liveDocs, those DocIdSet could also be cached with liveDocs applied."
0,"add maxtf to fieldinvertstate. the maximum within-document TF is a very useful scoring value, 
we should expose it so that people can use it in scoring

consider the following sim:
{code}
@Override
public float idf(int docFreq, int numDocs) {
  return 1.0F; /* not used */
}

@Override
public float computeNorm(String field, FieldInvertState state) {
  return state.getBoost() / (float) Math.sqrt(state.getMaxTF());
}
{code}

which is surprisingly effective, but more interesting for practical reasons.

"
0,"Improve contrib/benchmark for testing near-real-time search performance. It's not easy to test NRT performance right now w/ contrib/benchmark.
I've made some initial fixes to improve this:

  * Added new '&', that can follow any task within a serial sequence,
    to ""background"" the task (just like a shell).  The test runs in
    the BG, and then at the end of all serial tasks, any still running
    BG tasks are stopped & joined.

  * Added WaitTask that simply waits; useful for controlling how long
    the BG'd tasks get to run.

  * Added RollbackIndex task, which is real handy for using a given
    index for an NRT test, doing a bunch of updates, then reverting it
    all so your next run uses the same starting index

  * Fixed the existing NearRealTimeReaderTask to simply periodically
    open the new reader (previously it was also running a fixed
    search), and removed its own threading (since & can do that
    now). It periodically wakes up, opens the new reader, and swaps it
    into the PerfRunData, at the schedule you specify.  I switched all
    usage of PerfRunData's get/setIndexReader APIs to use ref
    counting.

With these changes you can now make some very simple but powerful
algs, eg:

{code}
OpenIndex
{
  NearRealtimeReader(0.5) &
  # Warm
  Search
  { ""Index1"" AddDoc > : * : 100/sec &
  [ { ""Search"" Search > : * ] : 4 &
  Wait(30.0)
}
CloseReader
RollbackIndex
RepSumByName
{code}

This alg first opens the IndexWriter, then spawns the BG thread to
reopen the NRT reader twice per second, does one warming Search (in
the FG), spans a new thread to index documents at the rate of 100 per
second, then spawns 4 search threads that do as many searches as they
can.  We then wait for 30 seconds, then stop all the threads, revert
the index, and report.

The patch is a work in progress -- it generally works, but there're a
few nocommits, and, we may want to improve reporting (though I think
that's a separate issue).
"
0,EasySimilarity to interpret document length as float. 
0,"Add FieldCache.getTermBytes, to load term data as byte[]. With flex, a term is now an opaque byte[] (typically, utf8 encoded unicode string, but not necessarily), so we need to push this up the search stack.

FieldCache now has getStrings and getStringIndex; we need corresponding methods to load terms as native byte[], since in general they may not be representable as String.  This should be quite a bit more RAM efficient too, for US ascii content since each character would then use 1 byte not 2."
0,"Remove remaining @author references. $ find . -name \*.java | xargs grep '@author' | cut -d':' -f1 | xargs perl -pi -e 's/ \@author.*//'
"
0,"XML text extraction in Jackrabbit 1.x accesses external resources. As discussed on users@, we should add the following code to the ExtractorHandler class:

   public InputSource resolveEntity(String publicId, String systemId) {
       return new InputSource(new ByteArrayInputStream(new byte[0]));
   }
"
0,"[PATCH] Contribution: A QueryParser which passes wildcard and prefix queries to analyzer. Lucenes built-in QueryParser class does not analyze prefix nor wildcard queries.
Attached is a subclass which passes these queries to the analyzer as well."
0,"Tests using Version.LUCENE_CURRENT will produce problems in backwards branch, when development for 3.2 starts. A lot of tests for the most-recent functionality in Lucene use Version.LUCENE_CURRENT, which is fine in trunk, as we use the most recent version without hassle and changing this in later versions.

The problem is, if we copy these tests to backwards branch after 3.1 is out and then start to improve analyzers, we then will have the maintenance hell for backwards tests. And we loose backward compatibility testing for older versions. If we would specify a specific version like LUCENE_31 in our tests, after moving to backwards they must work without any changes!

To not always modify all tests after a new version comes out (e.g. after switching to 3.2 dev), I propose to do the following:
- declare a static final Version TEST_VERSION = Version.LUCENE_CURRENT (or better) Version.LUCENE_31 in LuceneTestCase(4J).
- change all tests that use Version.LUCENE_CURRENT using eclipse refactor to use this constant and remove unneeded import statements.

When we then move the tests to backward we must only change one line, depending on how we define this constant:
- If in trunk LuceneTestCase it's Version.LUCENE_CURRENT, we just change the backwards branch to use the version numer of the released thing.
- If trunk already uses the LUCENE_31 constant (I prefer this), we do not need to change backwards, but instead when switching version numbers we just move trunk forward to the next major version (after added to Version enum)."
0,Remove deprecated query components. Remove the rest of the deprecated query components.
0,"[PATCH] demo HTML parser corrupts foreign characters. We are using HTML parser for parsing English and other NL documents in 
Eclipse.  Post Lucene 1.2 there has been a regression in the parser.  
Characters coming from Reader (obtained from getReader() ) are corrupted.  
Only the characters that can be encoded using the default machine encoding go 
through correctly.  For example, parsing Chinese document on an English 
machine results with all characters, except the few English words, corrupted."
0,Remove some unused code in Surround query parser. 
0,"JSR 283: JCR Path. with jsr 283 the jcr path is defined to consist of a combination of the following segments

•	a name segment, (J, I), where J is a JCR name and I is an integer index (I ≥ 1).
•	an identifier segment, U, where U is a JCR identifier.
•	the root segment.
•	the self segment.
•	the parent segment.

-> the name segment can be in extended or qualified form -> see issue JCR-1712
-> the identifier segment is new for jsr283 and always identifies a node (-> see new method Node.getIdentifier())

Non-standard parts always need to be standardized. Any of the following makes a path non-standard:
- expanded name segments
- trailing /
- index [1]

Identifier-segments
- get resolved upon being passed to any API calls that take path to an existing Node
- don't get resolved when being used to create a PATH value object.

Except for PATH values, all jcr paths returned by the API are normalized and standard, thus never identifier-based.

PATH values in contrast:
- must be converted to standard form
- must NOT be normalized. i.e. redundant segments and identifiers must be preserved.
"
0,"Incorrect Specification-Title headers in MANIFEST.MF. The Specification-Title headers in MANIFEST.MF should all include the full project name, i.e.

Apache HttpComponents ...

The ""HttpComponents"" qualifier is missing; at present the entries are:

Specification-Title: Apache HttpClient
Specification-Title: Apache HttpMime

If present, Implementation-Title should follow the same convention."
0,"The Field ctors that take byte[] shouldn't take Store, since it must be YES. API silliness.  Makes you think you can set Store.NO for binary fields.  This used to be meaningful when we also accepted COMPRESS, but now it's an orphan."
0,"JCR2SPI does not provide actual size on RangeIterator.getSize(). Currently, JCR2SPI always returns -1 on RangeIterator.getSize().

This return value is legal (meaning ""unknown""), but may cause clients to simply iterate through the whole list when what they really want is simply the count.

Use case:

""The use case is to count the number of members of a NT_FOLDER without having to open up the NT_FOLDER and count all the members (and I assume load them into memory) ""

To make this happen we probably need to move away from simple Iterators on the SPI level, and put quite some additional work into JCR2SPI.

Feedback appreciated."
0,"HeaderElement#parse(String) implementation is not optimal. The cookie setted by the LocalDirector 416 Version 4.2.3 has a bug.
It sets for Tuesday and Thursday ""Tues"" and ""Thur"" instead of the 
canonical ""Tue"" and ""Thu"". This break the parsing stage and stop HttpClient to 
work for 2 days a week. Of course I modified the parse method in HeaderElement 
class, but everytime I download a new version, I have to remake the jar....
It's possible to include this into the CVS files ?"
0,"SimpleFieldsHelper emits a lot warnings. The SimpleFieldsHelper.retrieveSimpleField method is used to load JCR properties into simple Java object fields according to the mapping descriptor. If the node does not have the named property, a WARN message is emited.

If the missing property is defined as optional in the node type definition, it is quite normal, that it may be missing. Therefore emitting a WARN message does not seem appropriate. It would be better, to do the following (in order):

   If the missing property is declared to be required in the descriptor, throw an exception
   else if the descriptor has a default value for the missing property, use that value
   else if the property is defined with a default value in the node type definition, use that value
   else emit a DEBUG message and leave the field undefined

Not sure, whether it makes absolute sense to define a property as mandatory in the descriptor but not in the node type definition. Are there any opinions on that ?"
0,"References to old repository-1.x.dtd. Some components still reference old version of the repository-1.x.dtd.
All components should be upgraded to repository-1.6.dtd"
0,"Add SearcherManager, to manage IndexSearcher usage across threads and reopens. This is a simple helper class I wrote for Lucene in Action 2nd ed.
I'd like to commit under Lucene (contrib/misc).

It simplifies using & reopening an IndexSearcher across multiple
threads, by using IndexReader's ref counts to know when it's safe
to close the reader.

In the process I also factored out a test base class for tests that
want to make lots of simultaneous indexing and searching threads, and
fixed TestNRTThreads (core), TestNRTManager (contrib/misc) and the new
TestSearcherManager (contrib/misc) to use this base class.
"
0,"Command line access to remote repositories. A few years ago Edgar Poce implemented a nice command line JCR access tool called jcr-commands. We haven't really been using it much and the code is currently parked in sandbox/inactive.

I'd like to resurrect this codebase and integrate it to jackrabbit-standalone to implement command line access to remote repositories. The idea would be to have an easy-to-use tool for simple testing and administration tasks.
"
0,"Use a separate JFlex generated Unicode 4 by Java 5 compatible StandardTokenizer. The current trunk version of StandardTokenizerImpl was generated by Java 1.4 (according to the warning). In Java 3.0 we switch to Java 1.5, so we should regenerate the file.

After regeneration the Tokenizer behaves different for some characters. Because of that we should only use the new TokenizerImpl when Version.LUCENE_30 or LUCENE_31 is used as matchVersion."
0,"Promote solr's PrefixFilter into Java Lucene's core. Solr's PrefixFilter class is not specific to Solr and seems to be of interest to core lucene users (PyLucene in this case).
Promoting it into the Lucene core would be helpful."
0,"[PATCH] small fixes to the new scoring.html doc. This is an awesome initiative.  We need more docs that cleanly explain the inner workings of Lucene in general... thanks Grant & Steve & others!

I have a few small initial proposed fixes, largely just adding some more description around the components of the formula.  But also a couple typos, another link out to Wikipedia, a missing closing ), etc.  I've only made it through the ""Understanding the Scoring Formula"" section so far."
0,"Jackrabbit does not allow concurrent reads to the data store if copyWhenReading=false. Jackrabbit does not allow concurrent reads to the data store if copyWhenReading=false, even if maxConnections>1.
See JCR-1184 for a test for this problem (run it with copyWhenReading=false).
"
0,"WebDAV: add support for DAV:lockroot. see http://www.webdav.org/specs/rfc4918.html#ELEMENT_lockroot

this element has been added with RFC 4918.
add constant to the DAVConstants and extend ActiveLock interface accordingly."
0,Stop text extraction when the maxFieldLength limit is reached. When indexing large documents the text extraction often takes quite a while and uses lots of memory even if only the first maxFieldLength (by default 10000) tokens are used. I'd like to add a maxExtractLength parameter that can be used to set the maximum number of characters to extract from a binary. The default value of this parameter could be something like ten times the maxFieldLength setting.
0,"Ability to group search results by field. It would be awesome to group search results by specified field. Some functionality was provided for Apache Solr but I think it should be done in Core Lucene. There could be some useful information like total hits about collapsed data like total count and so on.

Thanks,
Artyom"
0,Improve equals for binary values. The current implementation of spi2dav's equals method for binary values is not symmetric. Moreover it forces the values to be loaded instead of trying to determine equality by examining etag and last-modified headers. 
0,Add a testing implementation for DocumentsWriterPerThreadPool. currently we only have one impl for DocumentsWriterPerThreadPool. We should add some more to make sure the interface is sufficient and to beef up tests. For testing I'm working on a randomized impl. selecting and locking states randomly.
0,"Chunked transfer encoding not isolated from application.. Chunked transfer encoding is not being supported transparently by the
HttpMethodBase object, causing chunk data to be embedded in response body data
and forcing the application to handle the HTTP/1.1 implementation of chunked
transfer encoding.

The included patch now properly parses chunk data as per RFC 2068 and provides
body content consistently, regardless of whether chunked transfer encoding was
used by the server or not. This relieves the application from the requirement of
implementing RFC 2068.

Patch sent to mailing list as per guidelines to address this deficiency."
0,"Improvements to contrib.benchmark for TREC collections. The benchmarking utilities for  TREC test collections (http://trec.nist.gov) are quite limited and do not support some of the variations in format of older TREC collections.  

I have been doing some benchmarking work with Lucene and have had to modify the package to support:
* Older TREC document formats, which the current parser fails on due to missing document headers.
* Variations in query format - newlines after <title> tag causing the query parser to get confused.
* Ability to detect and read in uncompressed text collections
* Storage of document numbers by default without storing full text.

I can submit a patch if there is interest, although I will probably want to write unit tests for the new functionality first.

"
0,adding EmptyDocIdSet/Iterator. Adding convenience classes for EmptyDocIdSet and EmptyDocIdSetIterator
0,"Respect Keep-Alive Header. HttpClient currently does not respect the 'Keep-Alive' header tokens (timeout, max, etc..) and continues to use the persistent connection beyond limits the server requests.  This leads to failure and falling back to HttpRequestRetryHandler, when it should instead just use a new connection explicitly."
0,"Improve read/write concurrency. I'd like to set up a few performance tests to help identify our worst bottlenecks for various kinds of concurrent read-only and read-write access patterns.

Once identified, I'm hoping to fix at least some of those bottlenecks."
0,"org.apache.lucene.ant.HtmlDocument added Tidy config file passthrough availability. Parsing HTML documents using the org.apache.lucene.ant.HtmlDocument.Document method resulted in many error messages such as this:

    line 152 column 725 - Error: <as-html> is not recognized!
    This document has errors that must be fixed before
    using HTML Tidy to generate a tidied up version.

The solution is to configure Tidy to accept these abnormal tags by adding the tag name to the ""new-inline-tags"" option in the Tidy config file (or the command line which does not make sense in this context), like so:

    new-inline-tags: as-html

Tidy needs to know where the configuration file is, so a new constructor and Document method can be added.  Here is the code:

{code}
    /**                                                                                                                                                                                            
     *  Constructs an <code>HtmlDocument</code> from a {@link                                                                                                                                      
     *  java.io.File}.                                                                                                                                                                             
     *                                                                                                                                                                                             
     *@param  file             the <code>File</code> containing the                                                                                                                                
     *      HTML to parse                                                                                                                                                                          
     *@param  tidyConfigFile   the <code>String</code> containing                                                                                                                                  
     *      the full path to the Tidy config file                                                                                                                                                  
     *@exception  IOException  if an I/O exception occurs                                                                                                                                          
     */
    public HtmlDocument(File file, String tidyConfigFile) throws IOException {
        Tidy tidy = new Tidy();
        tidy.setConfigurationFromFile(tidyConfigFile);
        tidy.setQuiet(true);
        tidy.setShowWarnings(false);
        org.w3c.dom.Document root =
                tidy.parseDOM(new FileInputStream(file), null);
        rawDoc = root.getDocumentElement();
    }

    /**                                                                                                                                                                                            
     *  Creates a Lucene <code>Document</code> from a {@link                                                                                                                                       
     *  java.io.File}.                                                                                                                                                                             
     *                                                                                                                                                                                             
     *@param  file                                                                                                                                                                                 
     *@param  tidyConfigFile the full path to the Tidy config file                                                                                                                                 
     *@exception  IOException                                                                                                                                                                      
     */
    public static org.apache.lucene.document.Document
        Document(File file, String tidyConfigFile) throws IOException {

        HtmlDocument htmlDoc = new HtmlDocument(file, tidyConfigFile);

        org.apache.lucene.document.Document luceneDoc = new org.apache.lucene.document.Document();

        luceneDoc.add(new Field(""title"", htmlDoc.getTitle(), Field.Store.YES, Field.Index.ANALYZED));
        luceneDoc.add(new Field(""contents"", htmlDoc.getBody(), Field.Store.YES, Field.Index.ANALYZED));

        String contents = null;
        BufferedReader br =
            new BufferedReader(new FileReader(file));
        StringWriter sw = new StringWriter();
        String line = br.readLine();
        while (line != null) {
            sw.write(line);
            line = br.readLine();
        }
        br.close();
        contents = sw.toString();
        sw.close();

        luceneDoc.add(new Field(""rawcontents"", contents, Field.Store.YES, Field.Index.NO));

        return luceneDoc;
    }
{code}

I am using this now and it is working fine.  The configuration file is being passed to Tidy and now I am able to index thousands of HTML pages with no more Tidy tag errors.

"
0,"Data store garbage collection: ScanEventListener not working. The ScanEventListener is currently only called when using the 'scan all nodes recursively' strategy. It is not called when all persistence managers implement IterablePersistenceManager (GarbageCollector.scanPersistenceManagers). The ScanEventListener should be called in every case, otherwise it is not possible to see the progress of the garbage collection.

However there is a problem: IterablePersistenceManager.getAllNodeIds() doesn't return Node objects, and it would make little sense to create real node objects (the performance advantage of scanPersistenceManagers would be lost).

Therefore, I propose a workaround: the ScanEventListener is called using a 'PseudoNode'. This is a class that implements Node but only has meaningful getUUID() and toString() methods. This allows to create a meaningful progress bar (as the UUIDs are returned in order)."
0,"Allow overriding the specification version in MANIFEST.MF. The specification version in MANIFEST.MF should only consist of
digits. When we e. g. build a release candidate with a version like
2.3-rc1 then we have to specify a different specification version.

See related discussion:
http://www.gossamer-threads.com/lists/lucene/java-dev/56611

"
0,"DbDataStore: improve error message when init fails. When initialization of the database data store fails, the error message does not
contain enough data to analyze the problem:

Driver: Oracle JDBC driver / 10.2.0.1.0
could not execute statement, reason: ORA-00902: invalid datatype, state/code: 42000/902
Can not init data store, driver=oracle.jdbc.OracleDriver url=jdbc:oracle:thin:@localhost:1521:orcl user=JACKRABBIT

Additionally the create table statement should be logged, and the table name."
0,"Some tests try to add new nodes without specifying the node type. Some tests try to add new nodes without specifying the node type, which may not be supported by a repository.

In particular:

- NodeAddMixinTest.testAddNonExisting
- NodeCanAddMixinTest.testNonExisting
- ValueFactoryTest.testValueType
- ValueFactoryTest.testValueFormatException

Proposal: update test cases to obtain the node type from the test config.
"
0,"Add doBeforeFlush to IndexWriter. IndexWriter has doAfterFlush which can be overridden by extensions in order to perform operations after flush has been called. Since flush is final, one can only override doAfterFlush. This issue will handle two things:
# Make doAfterFlush protected, instead of package-private, to allow for easier extendability of IW.
# Add doBeforeFlush which will be called by flush before it starts, to allow extensions to perform any operations before flush begings.

Will post a patch shortly.

BTW, any chance to get it out in 3.0.1?"
0,"Add support for encrpted db password in repository.xml. Basically this is same to the issue https://issues.apache.org/jira/browse/JCR-2673. I can not reopen JCR-2673, so I filed a new one instead. 

The reason for this jira is because for a lot of companies it is not allowed to store password in a clear text. 

Sorry, I dont know how this can be implemented yet. But I hope at least the requirement is clear. 

Thanks."
0,"Add setters to Field to allow re-use of Field instances during indexing. If we add setters to Field it makes it possible to re-use Field
instances during indexing which is a sizable performance gain for
small documents.  See here for some discussion:

    http://www.gossamer-threads.com/lists/lucene/java-dev/51041
"
0,"Redesign of HTTP authentication framework. The existing HTTP authentication framework has got a few glaring deficiencies:
- Authentication headers management evolved (or degraded) into a some sort of
black art and proved very error-prone.
- Existing logic intended to deal with authentication failures and
authentication failure recovery is flawed. The resolution of the HTTPCLIENT-213 did
appear possible without a better approach than the one based on AuthScheme#getID.

On top of that authentication logic got quite messy with the series of attempts
to fix breakages in complex authentication schemes (the latest being NTLM proxy
+ basic host fix) 

The patch I am about to attach is an attempt to address all the shortcomings
mentioned above. It builds upon my previous patch that enabled authentication
schemes to maintain authentication state and presents a complete redesign of the
existing HTTP authentication framework.

Basically there's no authentication code left untouched, so please do take a
closer look. Critique, comments, suggestions welcome.

Oleg"
0,"Tablespace (Filegroup) support for MS SQL Server. Tablespace support was added for Oracle database servers in this issue

https://issues.apache.org/jira/browse/JCR-968

We would like tablespace (or filegroup) support for MS SQL Server as well. To address this, we have created a patch using the trunk of JackRabbit that closely follows the patch in the above named issue."
0,Improve aggregate node indexing code. Currently the aggregate nodes indexing code uses a sub-optimal way of copying and sorting the aggregated fields.
0,Use type StaticOperand for fullTextSearchExpression. See: https://jsr-283.dev.java.net/issues/show_bug.cgi?id=691
0,"Field specified norms in MatchAllDocumentsScorer . This patch allows for optionally setting a field to use for norms factoring when scoring a MatchingAllDocumentsQuery.

From the test case:
{code:java}
.
    RAMDirectory dir = new RAMDirectory();
    IndexWriter iw = new IndexWriter(dir, new StandardAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
    iw.setMaxBufferedDocs(2);  // force multi-segment
    addDoc(""one"", iw, 1f);
    addDoc(""two"", iw, 20f);
    addDoc(""three four"", iw, 300f);
    iw.close();

    IndexReader ir = IndexReader.open(dir);
    IndexSearcher is = new IndexSearcher(ir);
    ScoreDoc[] hits;

    // assert with norms scoring turned off

    hits = is.search(new MatchAllDocsQuery(), null, 1000).scoreDocs;
    assertEquals(3, hits.length);
    assertEquals(""one"", ir.document(hits[0].doc).get(""key""));
    assertEquals(""two"", ir.document(hits[1].doc).get(""key""));
    assertEquals(""three four"", ir.document(hits[2].doc).get(""key""));

    // assert with norms scoring turned on

    MatchAllDocsQuery normsQuery = new MatchAllDocsQuery(""key"");
    assertEquals(3, hits.length);
//    is.explain(normsQuery, hits[0].doc);
    hits = is.search(normsQuery, null, 1000).scoreDocs;

    assertEquals(""three four"", ir.document(hits[0].doc).get(""key""));    
    assertEquals(""two"", ir.document(hits[1].doc).get(""key""));
    assertEquals(""one"", ir.document(hits[2].doc).get(""key""));
{code}"
0,EffectiveNodeType#getNamedNodeDefs returns array QItemDefinition instead of QNodeDefinition. ... thus requires unnecessary casting...
0,"explore using automaton for fuzzyquery. we can optimize fuzzyquery by using AutomatonTermsEnum. The idea is to speed up the core FuzzyQuery in similar fashion to Wildcard and Regex speedups, maintaining all backwards compatibility.

The advantages are:
* we can seek to terms that are useful, instead of brute-forcing the entire terms dict
* we can determine matches faster, as true/false from a DFA is array lookup, don't even need to run levenshtein.

We build Levenshtein DFAs in linear time with respect to the length of the word: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.16.652

To implement support for 'prefix' length, we simply concatenate two DFAs, which doesn't require us to do NFA->DFA conversion, as the prefix portion is a singleton. the concatenation is also constant time with respect to the size of the fuzzy DFA, it only need examine its start state.

with this algorithm, parametric tables are precomputed so that DFAs can be constructed very quickly.
if the required number of edits is too large (we don't have a table for it), we use ""dumb mode"" at first (no seeking, no DFA, just brute force like now).

As the priority queue fills up during enumeration, the similarity score required to be a competitive term increases, so, the enum gets faster and faster as this happens. This is because terms in core FuzzyQuery are sorted by boost value, then by term (in lexicographic order).

For a large term dictionary with a low minimal similarity, you will fill the pq very quickly since you will match many terms. 
This not only provides a mechanism to switch to more efficient DFAs (edit distance of 2 -> edit distance of 1 -> edit distance of 0) during enumeration, but also to switch from ""dumb mode"" to ""smart mode"".

With this design, we can add more DFAs at any time by adding additional tables. The tradeoff is the tables get rather large, so for very high K, we would start to increase the size of Lucene's jar file. The idea is we don't have include large tables for very high K, by using the 'competitive boost' attribute of the priority queue.

For more information, see http://en.wikipedia.org/wiki/Levenshtein_automaton"
0,"Credentials ignored if realm specified in preemptive authentication. When you specifiy credentials for a specific realm using preemptive 
authentication, the credentials are ignored during the first try (error 401 
back).

...
HttpClient client = new HttpClient(manager);
client.getState().setCredentials(""myRealm"",""myHost"",
			new UsernamePasswordCredentials(
				""user"",""password""));
client.getState().setAuthenticationPreemptive(true); 
...

""myRealm"" will be ignored in HttpState's matchCredentials() private method 
because during preemptive authentication, it is called with a null realm:

 private static Credentials matchCredentials(HashMap map, String realm, String 
host) {
        HttpAuthRealm entry = new HttpAuthRealm(host, realm);
	// no possible match here, map only contains the version with the realm
        Credentials creds = (Credentials) map.get(entry);
        if (creds == null && host != null && realm != null) {
            entry = new HttpAuthRealm(host, null);
            creds = (Credentials) map.get(entry);
            if (creds == null) {
                entry = new HttpAuthRealm(null, realm);
                creds = (Credentials) map.get(entry);
            }
        }
        if (creds == null) {
            creds = (Credentials) map.get(DEFAULT_AUTH_REALM);
        }
        return creds;
    } 

This is quite logical since the realm comes from the server and you don't 
contact the server first during preemptive authentication.

But, it should not be possible to set a realm when using preemptive mode, or at 
least it should not be silently ignored.

The current workaround is to set the realm to null in setCredential(), no 
elegant but works.

Regards,

Philippe"
0,Optimize queries with relative path in order by clause. This is a follow up to JCR-800 and adds a way to configure relative property paths for aggregates in the indexing configuration. Aggregated properties are handled much more efficiently when used in an order by. The implementation from JCR-800 is used as a fallback when no aggregate is configured. See attached patch for details.
0,"Commit volatile index to disc after some configurable idle time. The query handler keeps the most recent part of the index in memory (volatile index). That index is committed to disc when a certain amount of nodes (config param: minMergeDocs) has been added. Most of the times the volatile index will contain some nodes, causing a redo.log that needs to be applied when the jackrabbit process is killed.

In addition to the size threshold (minMergeDocs) of the volatile index, an idle time limit should force a commit to disc. The volatile index would be written to disc after some configurable idle time. This will result in an persistent index (on dic) which is most of the time in sync with the data, but still minimizes the disc IO during heavy modification activity on the workspace."
0,"Disable SearchManager. In previous versions (e.g. SVN tag 0.1-spec0.14) it was possible to disable the SearchManagers by not configuring a search index path. In the current revision, a NullPointerException is thrown, if the search index configuration is missing, tough the rest of the system would support missing search index configuration as before.

I suggest to extend search index configuration interpretation in WorkspaceCfg.init as follows:

        Element srchConfig = wspElem.getChild(SEARCH_INDEX_ELEMENT);
        if (srchConfig != null) {
            String pathAttr = srchConfig.getAttributeValue(PATH_ATTRIB);
            if (pathAttr != null && pathAttr.length() > 0) {
                searchIndexDir = replaceVars(pathAttr, vars);
            }
        }

This only reads search index configuration if available.

The reason to switch of the SearchManager is, that in my use case enabling the SearchManager yields a performance degradation of a factor of 10 ! Instead of taking around 500ms (which is still too long :-) to save 3 nodes and 15 properties, it would take around 5 seconds to save the same amount of data. And I do not need the SearchManager in my use case."
0,"Missing possibility to supply custom FieldParser when sorting search results. When implementing the new TrieRangeQuery for contrib (LUCENE-1470), I was confronted by the problem that the special trie-encoded values (which are longs in a special encoding) cannot be sorted by Searcher.search() and SortField. The problem is: If you use SortField.LONG, you get NumberFormatExceptions. The trie encoded values may be sorted using SortField.String (as the encoding is in such a way, that they are sortable as Strings), but this is very memory ineffective.

ExtendedFieldCache gives the possibility to specify a custom LongParser when retrieving the cached values. But you cannot use this during searching, because there is no possibility to supply this custom LongParser to the SortField.

I propose a change in the sort classes:
Include a pointer to the parser instance to be used in SortField (if not given use the default). My idea is to create a SortField using a new constructor
{code}SortField(String field, int type, Object parser, boolean reverse){code}

The parser is ""object"" because all current parsers have no super-interface. The ideal solution would be to have:

{code}SortField(String field, int type, FieldCache.Parser parser, boolean reverse){code}

and FieldCache.Parser is a super-interface (just empty, more like a marker-interface) of all other parsers (like LongParser...). The sort implementation then must be changed to respect the given parser (if not NULL), else use the default FieldCache.getXXXX without parser."
0,"DateUtils should cache SimpleDateFormat. DateUtils create a SimpleDateFormat for each invocation of #formatDate and #parseDate. This can be optimized if SimpleDateFormat instances are cached. Since SimpleDateFormat is not threadsafe, the cache must be threadlocal."
0,"Various improvment to Path and PathImpl. There are various issues with Path and PathImpl which the following patch addresses:
- Fixed problem with normalization of some paths in PathImpl. 
- Fixed handling of relative paths in PathImpl. 
- Fixed wrong return value for depth and ancestor count in PathImpl. 
- Added method for determining equivalence of paths in PathImpl.
- Fixed subPath method in PathImpl. 
- Clarified blurry contract for Path.
- Added many new test cases

For many of the fixes credits are due to Angela."
0,"Avoid exceptions thrown in finalize handler of RepositoryImpl constructor. If an exception happens during initialization of the repository, it might be overlayed by an exception thrown in the finalize handler of the RepositoryImpl constructor (see line 382 ff in [1]). The latter exception wins and the original exception is lost (if you don't have a log). This makes it hard to figure out the real problem.

This problem is actually a bit self-enforcing: if something goes wrong during startup, the code in the shutdown() method that is called is actually very prone to fail as it might not expect such a broken-startup state. In my case the overlaying NPE happened in ObservationManagerImpl.getRegisteredEventListeners, where this.dispatcher was unexpectedly null [2].

I think both places should be fixed (NPE guard in ObservationManagerImpl constructor for ""dispatcher"") and a try/catch block in the finalizer, just logging the exception:

    } finally {
        if (!succeeded) {
            try {
                // repository startup failed, clean up...
                shutdown();
            } catch (Throwable t) {
                // shutdown() likely to fail now, as startup was broken...
                log.error(""In addition to startup fail, another problem occurred while shutting down the repository again."", e);
            }
        }
    }


[1] http://svn.apache.org/viewvc/jackrabbit/trunk/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/RepositoryImpl.java?view=markup

[2] Overlaying exception's stacktrace:
Caused by: java.lang.NullPointerException
	at org.apache.jackrabbit.core.observation.ObservationManagerImpl.getRegisteredEventListeners(ObservationManagerImpl.java:143)
	at org.apache.jackrabbit.core.SessionImpl.removeRegisteredEventListeners(SessionImpl.java:1190)
	at org.apache.jackrabbit.core.SessionImpl.logout(SessionImpl.java:1215)
	at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.doDispose(RepositoryImpl.java:2153)
	at com.day.crx.core.CRXRepositoryImpl$CRXWorkspaceInfo.doDispose(CRXRepositoryImpl.java:1095)
	at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.dispose(RepositoryImpl.java:2108)
	at org.apache.jackrabbit.core.RepositoryImpl.doShutdown(RepositoryImpl.java:1146)
	at com.day.crx.core.CRXRepositoryImpl.doShutdown(CRXRepositoryImpl.java:845)
	at org.apache.jackrabbit.core.RepositoryImpl.shutdown(RepositoryImpl.java:1098)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:387)
	at com.day.crx.core.CRXRepositoryImpl.<init>(CRXRepositoryImpl.java:201)
	at com.day.crx.core.CRXRepositoryImpl.create(CRXRepositoryImpl.java:190)
	... 28 more
"
0,"Contrib: another highlighter approach. Mark Harwoods highlighter package is a great contribution to Lucene, I've used it a lot! However, when you have *large* documents (fields), highlighting can be quite time consuming if you increase the number of bytes to analyze with setMaxDocBytesToAnalyze(int). The default value of 50k is often too low for indexed PDFs etcetera, which results in empty highlight strings.

This is an alternative approach using term position vectors only to build fragment info objects. Then a StringReader can read the relevant fragments and skip() between them. This is a lot faster. Also, this method uses the *entire* field for finding the best fragments so you're always guaranteed to get a highlight snippet.

Because this method only works with fields which have term positions stored one can check if this method works for a particular field using following code (taken from TokenSources.java):

        TermFreqVector tfv = (TermFreqVector) reader.getTermFreqVector(docId, field);
        if (tfv != null && tfv instanceof TermPositionVector)
        {
          // use FulltextHighlighter
        }
        else
        {
          // use standard Highlighter
        }

Someone else might find this useful so I'm posting the code here."
0,"Move document type definition out of repository.xml. Hello!

Here at Cognifide, Przemo and I we got a bit confused while trying to solve JCR-202. There was a need to modify repository.xml configuration file and it's DTD, and we have found that there are different repository.xml files within trunk that differs this definition. I think that it is a good idea to extract this definition to a one separate file (and maybe .xsd instead of .dtd) and then link it in other files. It would be also nice to put this file somewhere on the Web and reference it via URL.

I am waiting for your comments.

Regards, Jan"
0,FSDirectory.open should return MMap on 64-bit Solaris. MMap is ~ 30% faster than NIOFS on this platform.
0,Update Spatial Lucene sort to use FieldComparatorSource. Update distance sorting to use FieldComparator sorting as opposed to SortComparator
0,Improve NodeTypeRegistry.effectiveNodeType(). The current getEffectiveNodeType() implementation has a minor bug that prevents from proper caching for certain nodetype combinations. further performance enhancements can be made to the effective node type cache.
0,"Catch Throwables while calling TextExtractors. There are different Exception Handlings in the current TextExtractors.
The Method Signature throws IOException but the internal Handling is different.

For example in the MsExcelTextExtractor there will be RuntimeException's catched but not in all Extractors.
@see JCR-574

I think we should catch Throwables in the NodeIndexer to prevent OutOfMemoryErros while indexing a node."
0,"o.a.h.conn.scheme.PlainSocketFactory is not a true Singleton. The class ""org.apache.http.conn.scheme.PlainSocketFactory"" has a factory method, getSocketFactory(), and clearly indicates in the Javadocs that it expects to be a Singleton; however, the presence of public constructors makes it quite possible that this is not the case.

To protect the Singleton status of the class, the constructors should be private, or, at the very least, default (package) access. This will force access to the single instance through the factory method."
0,serious performance degradation of node operations when node has a large number of child nodes (e.g. > 10k child node entries). 
0,"importXML still depends on Xerces. Przemo Pakulski commented on JCR-367:
> Jackrabbit-core is still dependent on Xerces directly during runtime, SessionImpl.importWorkspace,
> Workspacempl.importWorkspace methods contains folliwng lines :
>
>             XMLReader parser =
>                     XMLReaderFactory.createXMLReader(""org.apache.xerces.parsers.SAXParser"");
>
> It works in maven1 probably because maven1 itself needs xerces to run test goal.
>
> I suggest reopening the issue.

Creating a new issue since JCR-367 is already closed after the 1.1 release."
0,"Replacing mixin type doesn't preserve properties. NodeImpl.setPrimaryType(String) attempts to ""redefine"" nodes and properties that were defined by the previous node type if they also appear in the new type. If there is no matching definition for a node/property in the new type - or value conversion for matched node/property fails - only then are children removed. For example, say I have a node ""harry"", with a primary type ""Human"" that defines a ""bloodgroup"" property. If I set the primary type to be an unrelated type ""Animal"" that has a similar ""bloodgroup"" property, then its property value will survive the call to setPrimaryType(""Animal"").

The same is apparently not possible with mixins. NodeImpl.removeMixin(Name) immediately removes all children that were defined by the mixin (strictly, those that are not present in the effective node type resulting from the mixin being removed). In addition, NodeImpl.addMixin(Name) immediately throws a NodeTypeConflictException if you attempt to add a mixin defining an identically-named property prior to calling removeMixin. For example, say I have a node ""matrix"", with a mixin type ""movie"" that defines a ""title"" property. If I wish to replace the ""movie"" mixin on that node with another ""jcr:title"" mixin type, the existing ""title"" property will be deleted.

This occurs regardless of the order in which removeMixin and addMixin are called, and without session.save() being called between them. One option for coding this is to defer validation (and possible node/property removal) until session.save() is called.

This is not strictly a bug, as JSR-283 seems to leave the details of assigning node types (section 5.5) unspecified. However, it does say for Node.removeMixin(String) that ""Both the semantic change in effective node type and the persistence of the
change to the jcr:mixinTypes property occur on save"" and ideally we could emulate the nice behaviour in NodeImpl.setPrimaryType(String) for mixin types."
0,Include the WebDAV litmus tests in the Jackrabbit integration tests. It would be great to integrate the litmus tests (http://www.webdav.org/neon/litmus/) to our integration test suite.
0,"BrowserCompatHostnameVerifier and StrictHostnameVerifier should handle wildcards in SSL certificates better. I ran into a problem with SSL wildcard certificates in the class BrowserCompatHostnameVerifier. It handles ""*.example.org"" fine but ""server*.example.org"" fails to work correctly. The javadoc claims that it should behave the same way as curl and FireFox. In Firefox an SSL certificate for ""server*.example.org"" works fine for the host ""server.example.org"", using HttpClient it throws an exception.

Here is an example test (JUnit4):

package org.example.hb;

import javax.net.ssl.SSLException;

import org.apache.http.conn.ssl.BrowserCompatHostnameVerifier;
import org.junit.Test;

public class BrowserCompatHostnameVerifierTest {

	/**
	 * Should not throw an exeption in the verify method.
	 * @throws SSLException
	 */
	@Test
	public void testVerifyStringStringArrayStringArray() throws SSLException
	{
		BrowserCompatHostnameVerifier hv = new BrowserCompatHostnameVerifier();
		String host = ""www.example.org"";
		String[] cns = {""www*.example.org""};
		
		hv.verify(host, cns, cns);
	}

}"
0,"Move Query.weight() to IndexSearcher as protected method. We had this issue several times, latest in LUCENE-3207.

The method Query.weight() was left in Query for backwards reasons in Lucene 2.9 when we changed Weight class. This method is only to be called on top-level queries - and this is done by IndexSearcher. This method is just a utility method, that has nothing to do with the query itsself (it just combines the createWeight method and calls the normalization afterwards). 

The problem we have is that any query that wraps other queries (like CustomScore, ConstantScore, Boolean) calls Query.weight() instead of Query.createWeight(), it will do normalization two times, leading to strange bugs.

For 3.3 I will make Query.weight() simply delegate to IndexSearcher's replacement method with a big deprecation warning, so user sees this. In IndexSearcher itsself the method will be protected to only be called by itsself or subclasses of IndexSearcher. Delegation for backwards is no problem, as protected is accessible by classes in same package.

I would suggest the method name to be IndexSearcher.createNormalizedWeight(Query q)"
0,"Enable MultiTermQuery's constant score mode to also use BooleanQuery under the hood. When MultiTermQuery is used (via one of its subclasses, eg
WildcardQuery, PrefixQuery, FuzzyQuery, etc.), you can ask it to use
""constant score mode"", which pre-builds a filter and then wraps that
filter as a ConstantScoreQuery.

If you don't set that, it instead builds a [potentially massive]
BooleanQuery with one SHOULD clause per term.

There are some limitations of this approach:

  * The scores returned by the BooleanQuery are often quite
    meaningless to the app, so, one should be able to use a
    BooleanQuery yet get constant scores back.  (Though I vaguely
    remember at least one example someone raised where the scores were
    useful...).

  * The resulting BooleanQuery can easily have too many clauses,
    throwing an extremely confusing exception to newish users.

  * It'd be better to have the freedom to pick ""build filter up front""
    vs ""build massive BooleanQuery"", when constant scoring is enabled,
    because they have different performance tradeoffs.

  * In constant score mode, an OpenBitSet is always used, yet for
    sparse bit sets this does not give good performance.

I think we could address these issues by giving BooleanQuery a
constant score mode, then empower MultiTermQuery (when in constant
score mode) to pick & choose whether to use BooleanQuery vs up-front
filter, and finally empower MultiTermQuery to pick the best (sparse vs
dense) bit set impl.
"
0,"Add shutdown method to SimpleHttpConnectionManager. It would be useful to be able to close the connection in the
SimpleHttpConnectionManager. This could be achieved by adding a shutdown()
method as per the MultiThreadedConnectionManager.

Ideally this would be added to the HttpConnection interface, but this could
break existing implementations.

To avoid this, perhaps consider introducing a sub-interface with the method in it.

[Could also create an AbstractConnectionManager class - this would make it
easier to add more functions later]"
0,Remove dependency on Xerces. Classloaders in certain J2EE servers do not play well with the Xerces requirement
0,"questionable default value for BufferedOutputStream size in HttpConnection. From the dev list

--

Hi Eric

Thanks for bringing this up. HttpClient 3.0 allows for parameterization
of SO_SNDBUF and SO_RCVBUF settings. For HttpClient 2.0 (as well as for
3.0 when falling back onto the system defaults), however, it would make
sense to set a cap on the size of the send and receive buffers.

Feel free to open a ticket for this issue with Bugzilla

Oleg


On Fri, 2004-07-02 at 18:39, Eric Bloch wrote:

>> Hi httpclient folks,
>> 
>> I've been looking at 2.0 source code and the default value for the 
>> BufferedOutputStream that is used in an HttpConnectionn is coming from 
>> socket.getSendBufferSize().  My hunch, is that, in general, this is 
>> bigger than you'd want.
>> 
>> Most HTTP ""sends"" are less than 1KByte ('cept for big POSTs).
>> The default value I get for socket.getSendBufferSize for this is 8192.
>> I would think a better default for this buffer would be 1K, no?
>> 
>> Also, fyi, if someone happens to dork the system send buffer size hi 
>> (say MB) and you are using the MultiThreadedConnectionManager in 2.0 
>> (dunno about 3.0), you will use up a lot of memory for each connection 
>> since the pool doesn't let idle connections (or their buffers) be gced. 
>>   I just got bit bad by that.
>> 
>> -Eric
>> 
>"
0,"Omit positions but keep termFreq. it would be useful to have an option to discard positional information but still keep the term frequency - currently setOmitTermFreqAndPositions discards both. Even though position-dependent queries wouldn't work in such case, still any other queries would work fine and we would get the right scoring."
0,"add numDocs() and maxDoc() methods to IndexWriter; deprecate docCount(). Spinoff from here:

  http://mail-archives.apache.org/mod_mbox/lucene-java-user/200804.mbox/%3c405706.11550.qm@web65411.mail.ac4.yahoo.com%3e

I think we should add maxDoc() and numDocs() methods to IndexWriter,
and deprecate docCount() in favor of maxDoc().  To do this I think we
should cache the deletion count of each segment in the segments file.

"
0,"[Maven] Migrate checkstyle.properties to XML. Newer Checkstyle versions (used with latest Maven builds) can not use the old
properties configuration file. They work with XML configuration files.
checkstyle.properties must be migrated in order to use a current version of Maven."
0,"MultiFieldQueryParser field boost multiplier. Allows specific boosting per field, e.g. +(name:foo^1 description:foo^0.1).

Went from String[] field to MultiFieldQueryParser.FieldSetting[] field in constructor. "
0,"Jcr2Spi: UpdateTest#testUpdateRemovesExtraProperty and #testUpdateAddsMissingSubtree fail occasionally. issue reported by jukka:

Every now and then I see the following jcr2spi test failures on a
clean checkout:

    Tests in error:
      testUpdateRemovesExtraProperty(org.apache.jackrabbit.jcr2spi.UpdateTest)
      testUpdateAddsMissingSubtree(org.apache.jackrabbit.jcr2spi.UpdateTest)

The problem seems to be caused by the test cases automatically
choosing the ""security"" workspace for the update test. The reason why
the tests only fail occasionally is that currently the ordering of the
string array returned by getAccessibleWorkspaceNames() is not
deterministic.

I can work around the issue by making
RepositoryImpl.getWorkspaceNames() explicitly sort the returned array
of names, but a more proper fix would probably be to ensure that the
workspace selected by UpdateTest is useful for the test.


"
0,"rename expungeDeletes. Similar to optimize(), expungeDeletes() has a misleading name.

We already had problems with this on the user list because TieredMergePolicy
didn't 'expunge' all their deletes.

Also I think expunge is the wrong word, because expunge makes it seem
like you just wrangle up the deletes and kick them out of the party and
that it should be fast.



"
0,"LuceneTestCase's uncaught exceptions handler should check for AssumptionViolatedExceptions and then not trigger test failure. As in single-threaded tests, {{LuceneTestCase}} should not trigger test failures for {{AssumptionViolatedException}}'s when they occur in multi-threaded tests."
0,"ValueFormat should provide method getJCRString. In order to retrieve the JCR String representation of a QValue currently the following calls are required:

ValueFormat.getJCRValue(QValue, NamePathResolver, ValueFactory)
Value.getString()

This could be simplified if the ValueFormat would provide

ValueFormat.getJCRString(QValue, NamePathResolver)

"
0,"Cut over numeric docvalues to fixed straight bytes. Currently numeric docvalues types are encoded and stored individually which creates massive duplication of writing / indexing code. Yet, almost all of them (except packed ints) are essentially a fixed straight bytes variant. "
0,Reorganize Jackrabbit into 'core' 'api' and 'commons'. 
0,"AccessControlImporter does not import repo level ac content. the implementation of the ProtectedNodeImporter responsible for dealing with access control content should be
adjusted such that it can properly cope with repository level access control that may be stored together with
the root node (by using access control API with null path)."
0,"Remove dependency on Jackrabbit-core. We should remove the dependency on Jackrabit core in the OCM subprojects ""jcr-mapping"" and ""annotation"". We can use Jackrabbit core only for the unit tests. 

We can also split the jcr-nodemanagement into several subprojects (one per JCR repo impl).  We will have only one subproject for Jackrabbit but contributions for other JCR repo impl are welcome.  A specific jcr-nodemanagement jar can be produce for each JCR repo impl. When the JCR will support the node creation, we can refactor the jcr-nodemanagement. 

"

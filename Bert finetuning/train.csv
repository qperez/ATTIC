label,summarydescription
1,"DefaultMethodRetryHandler bug. DefaultMethodRetryHandler does not seem to test correctly for the number of
attempts to retry a given method. It seems to bail out one attempt too early:

if (executionCount >= this.retryCount) {
  // Do not retry if over max retry count
  return false;
}

For example, if I set the retryCount to 1, HttpClient does not retry the method
at all. At least that's what I'm seeing when I step through it with a debugger."
1,"Not configuring the adminId, anonymousId, or defaultuserId causes login module to ignore credentials. Using the DefaultLoginModule, DefaultAccessManager, and DefaultSecurityManager and calling Repository.login(Credentials) causes the following stack trace to be thrown.  

javax.jcr.LoginException: LoginModule ignored Credentials: LoginModule ignored Credentials: LoginModule ignored Credentials
	at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1353)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:53)
	at com.cerner.system.configuration.repository.jcr.JackrabbitTest.testLoginWithCredentials(JackrabbitTest.java:23)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at org.junit.internal.runners.TestMethod.invoke(TestMethod.java:59)
	at org.junit.internal.runners.MethodRoadie.runTestMethod(MethodRoadie.java:98)
	at org.junit.internal.runners.MethodRoadie$2.run(MethodRoadie.java:79)
	at org.junit.internal.runners.MethodRoadie.runBeforesThenTestThenAfters(MethodRoadie.java:87)
	at org.junit.internal.runners.MethodRoadie.runTest(MethodRoadie.java:77)
	at org.junit.internal.runners.MethodRoadie.run(MethodRoadie.java:42)
	at org.junit.internal.runners.JUnit4ClassRunner.invokeTestMethod(JUnit4ClassRunner.java:88)
	at org.junit.internal.runners.JUnit4ClassRunner.runMethods(JUnit4ClassRunner.java:51)
	at org.junit.internal.runners.JUnit4ClassRunner$1.run(JUnit4ClassRunner.java:44)
	at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:27)
	at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:37)
	at org.junit.internal.runners.JUnit4ClassRunner.run(JUnit4ClassRunner.java:42)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:45)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)
Caused by: javax.security.auth.login.FailedLoginException: LoginModule ignored Credentials
	at org.apache.jackrabbit.core.security.authentication.LocalAuthContext.login(LocalAuthContext.java:73)
	at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1346)
	... 24 more
javax.security.auth.login.FailedLoginException: LoginModule ignored Credentials
	at org.apache.jackrabbit.core.security.authentication.LocalAuthContext.login(LocalAuthContext.java:73)
	at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1346)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:53)
	at com.cerner.system.configuration.repository.jcr.JackrabbitTest.testLoginWithCredentials(JackrabbitTest.java:23)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at org.junit.internal.runners.TestMethod.invoke(TestMethod.java:59)
	at org.junit.internal.runners.MethodRoadie.runTestMethod(MethodRoadie.java:98)
	at org.junit.internal.runners.MethodRoadie$2.run(MethodRoadie.java:79)
	at org.junit.internal.runners.MethodRoadie.runBeforesThenTestThenAfters(MethodRoadie.java:87)
	at org.junit.internal.runners.MethodRoadie.runTest(MethodRoadie.java:77)
	at org.junit.internal.runners.MethodRoadie.run(MethodRoadie.java:42)
	at org.junit.internal.runners.JUnit4ClassRunner.invokeTestMethod(JUnit4ClassRunner.java:88)
	at org.junit.internal.runners.JUnit4ClassRunner.runMethods(JUnit4ClassRunner.java:51)
	at org.junit.internal.runners.JUnit4ClassRunner$1.run(JUnit4ClassRunner.java:44)
	at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:27)
	at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:37)
	at org.junit.internal.runners.JUnit4ClassRunner.run(JUnit4ClassRunner.java:42)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:45)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)

A testcase and repository.xml file will be attached shortly."
1,"Node removal fails with AccessDeniedException. I have a hierarchy of nodes which are all access controllable. The following hierarchy illustrates the setup for my problem.
root  -  read permissions to everyone
  | - subFolder  -  all permissions to user A
        | - subsubFolder  -  all permissions to user A

The user A has all rights from the node ""subFolder"" downwards.

I tried to remove the node ""subsubFolder"" with the user A. Clearly A has enough permissions to remove the node. But as soon as I call Session.save() an AccessDeniedException is thrown.

I did a lot of debugging and found a possible cause for this fault. It led me to the function ACLProvider.AclPermissions.buildResult(). All line references are based on the source code in the subversion repository found here: http://svn.apache.org/viewvc/jackrabbit/tags/1.6.0/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/security/authorization/acl/ACLProvider.java?view=markup.
On line 458 Jackrabbit collects all access control entries of the node, that I want to remove, and all its parents and puts it in the variable ""entries"". In my example this variable contains three entries:
1. all permissions to user A
2. all permissions to user A
3. read permissions to everyone
On lines 460 - 466 it collects all access control entries of the node, that I want to remove, and puts it in ""localACEs"". This variable contains one entry: all permissions to user A.
If I want to be able to remove ""subsubFolder"", user A needs the permission from the parent node. The permissions of the parent nodes of ""subsubFolder"" are: all permissions to user A and read permissions to everyone. But that's where the access check fails. In line 488 Jackrabbit checks if a permission from ""entries"" is local or not by looking it up in ""localACEs"". If it is in there, the permission is local, else not. Unfortunately it recognizes the permission of the node ""subFolder"" as local. Thus the permissions of the parent nodes of ""subsubFolder"" are: read permissions to everyone. So I cannot remove the node.
The source of the error is the equals check of the access control entries. The permissions of node ""subFolder"" are considered equal to the one of ""subsubFolder"". If I explicitly assign the permission ""remove node permission to user A"" to the node ""subFolder"", it works fine, because it is recognized as parent permission."
1,"SSL verification occurs before setSoTimeout, which can lead to hangs. partial thread dump:

       at java.net.SocketInputStream.socketRead0(Native Method)
       at java.net.SocketInputStream.read(SocketInputStream.java:129)
       at com.sun.net.ssl.internal.ssl.InputRecord.readFully(InputRecord.java:293)
       at com.sun.net.ssl.internal.ssl.InputRecord.read(InputRecord.java:331)
       at com.sun.net.ssl.internal.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:723)
       - locked <0x00002aaab87d9de0> (a java.lang.Object)
       at com.sun.net.ssl.internal.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1030)
       - locked <0x00002aaab87d9dc0> (a java.lang.Object)
       at com.sun.net.ssl.internal.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1057)
       at com.sun.net.ssl.internal.ssl.SSLSocketImpl.getSession(SSLSocketImpl.java:1757)
       at org.apache.http.conn.ssl.AbstractVerifier.verify(AbstractVerifier.java:87)
       at org.apache.http.conn.ssl.SSLSocketFactory.connectSocket(SSLSocketFactory.java:295)
       at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:131)
       at org.apache.http.impl.conn.AbstractPoolEntry.open(AbstractPoolEntry.java:143)
       at org.apache.http.impl.conn.AbstractPooledConnAdapter.open(AbstractPooledConnAdapter.java:120)
       at org.apache.http.impl.client.DefaultClientRequestDirector.execute(DefaultClientRequestDirector.java:286)
       at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:452)
       at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:406)
       at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:365)


... this is because in DefaultClientConnectionOperator, prepareSocket (which sets any configured timeouts) isn't called until after SocketFactory.connectSocket. When using SSLSocketFactory, the default behavior is to verify the hostname, which opens a connection, and can block indefinitely.

Simple workaround is to use the AllowAllHostnameVerifier which doesn't do any verification."
1,"JCR2SPI: VersionHistoryImpl.getQLabels() needs to skip jcr:mixinTypes as well. getQLabels() iterates through the properties on a version labels node to compute the set of labels. Currently it only ignores jcr:primaryType, but it needs to skip jcr:mixinTypes as well.
"
1,"VirtualItemStates of node types definitions not accessible with uuid. The VirtualNodeTypeStateProvider that maps node type definitions into the workspace under /jcr:system/jcr:nodeTypes does not implement the methods:

- internalGetNodeState(NodeId id)
- internalHasNodeState(NodeId id)

This has the effect that ItemStates that reflect node type definitions are not accessible directly with their uuid."
1,"Malformed excerpt if content contains markup and no highlights found. Any markup in content that is used in an excerpt is encoded with corresponding entity references. However, this process is broken when there are no highlights in the excerpt. In this case, the content is provided as is in the excerpt, which may lead to malformed HTML/XML."
1,"Concurrency issues in SegmentInfo.files() could lead to ConcurrentModificationException. The multi-threaded call of the files() in SegmentInfo could lead to the ConcurrentModificationException if one thread is not finished additions to the ArrayList (files) yet while the other thread already obtained it as cached (see below). This is a rare exception, but it would be nice to fix. I see the code is no longer problematic in the trunk (and others ported from flex_1458), looks it was fixed while implementing post 3.x features. The fix to 3.x and 2.9.x branches could be the same - create the files set first and populate it, and then assign to the member variable at the end of the method. This will resolve the issue. I could prepare the patch for 2.9.4 and 3.x, if needed.

--

INFO: [19] webapp= path=/replication params={command=fetchindex&wt=javabin} status=0 QTime=1
Jul 30, 2010 9:13:05 AM org.apache.solr.core.SolrCore execute
INFO: [19] webapp= path=/replication params={command=details&wt=javabin} status=0 QTime=24
Jul 30, 2010 9:13:05 AM org.apache.solr.handler.ReplicationHandler doFetch
SEVERE: SnapPull failed
java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
        at java.util.AbstractList$Itr.next(AbstractList.java:343)
        at java.util.AbstractCollection.addAll(AbstractCollection.java:305)
        at org.apache.lucene.index.SegmentInfos.files(SegmentInfos.java:826)
        at org.apache.lucene.index.DirectoryReader$ReaderCommit.<init>(DirectoryReader.java:916)
        at org.apache.lucene.index.DirectoryReader.getIndexCommit(DirectoryReader.java:856)
        at org.apache.solr.search.SolrIndexReader.getIndexCommit(SolrIndexReader.java:454)
        at org.apache.solr.handler.SnapPuller.fetchLatestIndex(SnapPuller.java:261)
        at org.apache.solr.handler.ReplicationHandler.doFetch(ReplicationHandler.java:264)
        at org.apache.solr.handler.ReplicationHandler$1.run(ReplicationHandler.java:146)
"
1,"EnglishPossessiveFilter should work with Unicode right single quotation mark. The current EnglishPossessiveFilter (used in EnglishAnalyzer) removes possessives using only the '\'' character (plus 's' or 'S'), but some common systems (German?) insert the Unicode ""\u2019"" (RIGHT SINGLE QUOTATION MARK) instead and this is not removed when processing UTF-8 text. I propose to change EnglishPossesiveFilter to support '\u2019' as an alternative to '\''."
1,"Bug in SegmentTermPositions if used for first term in the dictionary. When a SegmentTermPositions object is reset via seek() it does not move
the proxStream to the correct position in case the term is the first one
in the dictionary.

The reason for this behavior is that the skipStream is only moved if
lazySkipPointer is != 0. But 0 is a valid value for the posting list of
the very first term. The fix is easy: We simply have to set lazySkipPointer
to -1 in case no lazy skip has to be performed and then we only move the
skipStream if lazySkipPointer!=-1."
1,"NumericTokenStream.NumericTermAttribute does not support cloning -> Solr analysis request handlers fail. During converting Solr's AnalysisRequestHandlers (LUCENE-2374) I noticed, that the current implementation of NumericTokenStream fails on cloneAttributes(), which is needed to buffer the tokens for structured display.

This issue should fix this by refactoring the inner class."
1,Benchmark deletes.alg fails. Benchmark deletes.alg fails because the index reader defaults to open readonly.  
1,"Problem with IndexWriter.mergeFinish. I'm getting a (very) infrequent assert in IndexWriter.mergeFinish from TestIndexWriter.testAddIndexOnDiskFull. The problem occurs during the rollback when the merge hasn't been registered. I'm not 100% sure this is the correct fix, because it's such an infrequent event. 

{code:java}
  final synchronized void mergeFinish(MergePolicy.OneMerge merge) throws IOException {
    
    // Optimize, addIndexes or finishMerges may be waiting
    // on merges to finish.
    notifyAll();

    if (merge.increfDone)
      decrefMergeSegments(merge);

    assert merge.registerDone;

    final SegmentInfos sourceSegments = merge.segments;
    final int end = sourceSegments.size();
    for(int i=0;i<end;i++)
      mergingSegments.remove(sourceSegments.info(i));
    mergingSegments.remove(merge.info);
    merge.registerDone = false;
  }
{code}

Should  be something like:

{code:java}
  final synchronized void mergeFinish(MergePolicy.OneMerge merge) throws IOException {
    
    // Optimize, addIndexes or finishMerges may be waiting
    // on merges to finish.
    notifyAll();

    if (merge.increfDone)
      decrefMergeSegments(merge);

    if (merge.registerDone) {
      final SegmentInfos sourceSegments = merge.segments;
      final int end = sourceSegments.size();
      for(int i=0;i<end;i++)
        mergingSegments.remove(sourceSegments.info(i));
      mergingSegments.remove(merge.info);
      merge.registerDone = false;
    }
  }
{code}"
1,"ChunkedInputStream incorrectly handles chunksize without semicolon. ChunkedInputStream does not correctly read the chunk size when a semicolon does
not appear in the first line of the chunk.  If whitespace exists between the
chunk size value and the end of line and no semicolon is present, the whitespace
is not removed before parseInt is called resulting in an IOException ""Bad chunk
size""

I can not tell from RFC2616 if whitespace is legal here, but I have received it
from at least one web server.  The relevant section is 3.6.1.

A small patch repairs the problem.  I will attach it immediately."
1,"BlockJoinQuery doesn't implement boost. After reviewing LUCENE-3494, i checked other queries and noticed that BlockJoinQuery currently throws UOE for getBoost and setBoost:
{noformat}
throw new UnsupportedOperationException(""this query cannot support boosting; please use childQuery.setBoost instead"");
{noformat}

I don't think we can safely do that in queries, because other parts of lucene rely upon this working... for example BQs rewrite when
it has a single clause and erases itself.

So I think we should just pass down the boost to the inner weight.
"
1,"Memory leak in MultiThreadedHttpClient caused by bad .equals(). Note: I have '2.0 release candidate 1'; I'm not sure which version this
translates into.  The bug is definitely present in the current source.

MultiThreadedHttpClient uses the following code:

// Look for a list of connections for the given config
HostConnectionPool listConnections = (HostConnectionPool) 
    mapHosts.get(hostConfiguration);
if (listConnections == null) { 
    // First time for this config
    listConnections = new HostConnectionPool();
    listConnections.hostConfiguration = hostConfiguration;
    mapHosts.put(hostConfiguration, listConnections);
}


The hash map relys on HostConfiguration's .equals() to resolve equality &
determine if there is a mapping for the configuration.

HostConfiguration has the following in it's .equals() method:

if (!protocol.equals(config.getProtocol())) {
    return false;
}

. . . and Protocol has:

if (obj instanceof Protocol) {
            
    Protocol p = (Protocol) obj;
            
    return (
        defaultPort == p.getDefaultPort()
        && scheme.equalsIgnoreCase(p.getScheme())
        && secure == p.isSecure()
        && socketFactory.equals(p.getSocketFactory()));

}

However, there is no .equals() method in any of the ProtocolSocketFactory
objects, and there isn't any note in the interface about the necessity of the
.equals() method."
1,"Hop 0 sample app doesn't exit because of on-daemon thread pool-1-thread-1. When starting the sample app Hop 0 (or any other Hop sample app) if there is no ""repository"" directory, then the application doesn't exit because there is a non-daemon thread named ""pool-1-thread-1""."
1,"Fix unexpected behavior of Text.getName(). Text.getName() and variants does return an empty string, if the given path is already a name. eg:

Text.getName(""foo"") returns """" and not ""foo"" as one would expect for relative paths.
suggest to change this."
1,"unable to workspace import XML.. tika detects xml as ""application/xml"" thus breaking the org.apache.jackrabbit.server.io.XmlHandler
which just checks for ""text/xml""."
1,"Session.save() potentially causes endless loop when READ permission is denied on root node. if the current session doesn't have read permission on the root node, calling Session.save() triggers a call to SessionItemStateManager.getIdOfRootTransientNodeState()
in order to find the root of the minimal subtree including all transient states. this might cause an endless loop, depending on the transient changes."
1,"Item.remove fails if a child-item is not visible to the editing session. the following test setup fails:

- a given session is allowed to remove a node
- the node has a policy child node which is not visible to the editing session (missing ac-read permission)
  OR the node has another invisible child item which could - based on the permissions above - be removed by that session.

calling Node.remove however fails with accessdeniedexception because the internal remove
mechanism accesses all child items to mark them removed. however, the access is executed
using the regular itemmgr calls that are used to retrieve the items using the JCR API which
results in accessdenied exception as those child items are not visible to the session.
since the items can be removed i would argue that this is a bug in the internal remove process.
"
1,"Lucene Query Exception: 'attempt to access a deleted document'. Hi,

I am getting an exception when trying to execute a query through the (Spring) JcrTemplate class....using the following code:
QueryManager qMgr = session.getWorkspace().getQueryManager();
QueryResult result = qMgr.createQuery(xpathQuery, Query.XPATH ).execute();

The exception is thrown at the second line and is as follows:

[DEBUG] << ""[0x9]at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:113)[\n]""
[DEBUG] << ""[0x9]at org.apache.lucene.search.Hits.getMoreDocs(Hits.java:74)[\n]""
[DEBUG] << ""[0x9]at org.apache.lucene.search.Hits.&lt;init>(Hits.java:53)[\n]""
[DEBUG] << ""[0x9]at org.apache.lucene.search.Searcher.search(Searcher.java:46)[\n]""
[DEBUG] << ""[0x9]at org.apache.lucene.search.Searcher.search(Searcher.java:38)[\n]""
[DEBUG] << ""[0x9]at org.apache.jackrabbit.core.query.lucene.SearchIndex.executeQuery(SearchIndex.java:660)[\n]""
[DEBUG] << ""[0x9]at org.apache.jackrabbit.core.query.lucene.QueryResultImpl.executeQuery(QueryResultImpl.java:242)[\n]""
[DEBUG] << ""[0x9]at org.apache.jackrabbit.core.query.lucene.QueryResultImpl.getResults(QueryResultImpl.java:290)[\n]""
[DEBUG] << ""[0x9]at org.apache.jackrabbit.core.query.lucene.QueryResultImpl.&lt;init>(QueryResultImpl.java:192)[\n]""
[DEBUG] << ""[0x9]at org.apache.jackrabbit.core.query.lucene.QueryImpl.execute(QueryImpl.java:138)[\n]""
[DEBUG] << ""[0x9]at org.apache.jackrabbit.core.query.QueryImpl.execute(QueryImpl.java:176)[\n]""
[DEBUG] << ""[0x9]at com.intel.cds.cr.jcr.JcrManager$5.doInJcr(JcrManager.java:363)[\n]""
[DEBUG] << ""[0x9]at org.springmodules.jcr.JcrTemplate.execute(JcrTemplate.java:76)[\n]""
[DEBUG] << ""[0x9]at org.springmodules.jcr.JcrTemplate.execute(JcrTemplate.java:108)[\n]""
[DEBUG] << ""[0x9]... 19 more[\n]""
[DEBUG] << ""</Exception></detail></soapenv:Fault></soapenv:Body></soapenv:Envelope>""
org.apache.axis2.AxisFault: attempt to access a deleted document
	at org.apache.axis2.util.Utils.getInboundFaultFromMessageContext(Utils.java:486)
	at org.apache.axis2.description.OutInAxisOperationClient.handleResponse(OutInAxisOperation.java:343)
	at org.apache.axis2.description.OutInAxisOperationClient.send(OutInAxisOperation.java:389)
	at org.apache.axis2.description.OutInAxisOperationClient.executeImpl(OutInAxisOperation.java:211)
	at org.apache.axis2.client.OperationClient.execute(OperationClient.java:163)


My Jackrabbit/Lucene configuration is as follows:

<SearchIndex class=""org.apache.jackrabbit.core.query.lucene.SearchIndex"">
        <param name=""path"" value=""${rep.home}/repository/index""/>
        
        <param name=""useCompoundFile"" value=""false""/>
        <param name=""mergeFactor"" value=""5""/>
        <param name=""cacheSize"" value=""10000""/>
        <param name=""respectDocumentOrder"" value=""false""/>  
  </SearchIndex>

Is this a configuration issue or a bug?

Thanks,
David."
1,"IndexCommit.getFileNames() should not return dups. If the index was created with autoCommit false, and more than 1
segment was flushed during the IndexWriter session, then the shared
doc-store files are incorrectly duplicated in
IndexCommit.getFileNames().  This is because that method is walking
through each SegmentInfo, appending its files to a list.  Since
multiple SegmentInfo's may share the doc store files, this causes dups.

To fix this, I've added a SegmentInfos.files(...) method, and
refactored all places that were computing their files one SegmentInfo
at a time to use this new method instead.
"
1,"NodeTypeDefDiff compares to restrictive. The NodeTypeDefDiff class is used to compare NodeTypeDef instances. Unfortunately this class reports two NodeTypeDef instances which are not equal but have no structural difference as having trivial changes. The correct result would be to have no modification at all.

I suggest to modify the NodeTypeDefDiff.init() method such, that the initial type is ""NONE"" instead of ""TRIVIAL"" and to first compare the ""hasOrderableChildNodes"" first and raise the level to ""TRIVIAL"" if not equal. Next the rest of the current comparisons would follow."
1,Tika configuration may use wrong class loader. The configurable Tika parser construction mechanism added in JCR-2864 constructs the parser instance lazily when the first indexing request is made. This may confuse things as the context class loader used by Tika to load all the available parser classes may not always be the class loader used to create the repository. To avoid this problem the Tika parser should be constructed already during normal repository initialization.
1,"memory leak in MultiThreadedHttpConnectionManager. MultiThreadedHttpConnectionManager.getConnectionsInPool(hostConfiguration) will create HostConnectionPool entries that will not be cleaned up unless they are later used for communication. This should be changed to not create pools in that method, but rather return 0 for a non-existent pool.
"
1,"DocViewSAXEventGenerator produces invalid SAX stream. ISO9075.encode() is called twice in DocViewSAXEventGenerator.leaving(), which produces invalid endElement events.

Faulty block of code (note the encode method called twice):

        // encode node name to make sure it's a valid xml name
        name = ISO9075.encode(name);
        // element name
        String elemName;
        if (node.getDepth() == 0) {
            // root node needs a name
            elemName = jcrRoot;
        } else {
            // encode node name to make sure it's a valid xml name
            elemName = ISO9075.encode(name);
        }"
1,"Missing Content-Length header makes cached entry invalid. A cached entry whose original response didn't carry a Content-Length header, should not be rejected for considered invalid because the length of its cached content is different from the non-existing Content-Length header value. The attached patch only verifies the lengths if the header was originally present."
1,AbstractExcerpt uses wrong logger. It uses DefaultXMLExcerpt instead of AbstractExcerpt.
1,"derelativizing of relative URIs with a scheme is incorrect. URI constructor ""public URI(URI base, URI relative) throws URIException"" assumes that if given 'relative' URI has a scheme, it should provide an authority and complete path to the constructed URI. However, a URI can have a scheme but still be relative, requiring the authority and base path of the 'base' URI. 

Demonstration code:

URI base = new URI(""http://www.example.com/some/page"");
URI rel = new URI(""http:boo"");
URI derel = new URI(base,rel);
derel.toString();
(java.lang.String) http:boo

In fact, derel should be ""http://www.example.com/some/boo"". 

RFC2396 is a little confused about this; section 3.1 states """"Relative URI references are distinguished from absolute URI in that they do not begin with a scheme name."" But, in section 5, there are several sentences talking about relative URIs that begin with schemes (and how this prevents using relative URIs that have leading path segments that look like scheme identifiers). 

RFC3896, which supercedes RFC2396, removes the implication a relative URI cannot begin with a scheme, leaving the other text explcitly discussing relative URIs with schemes. 

Both Firefox (1.5) and IE (6.0) treat ""http:boo"" the same as ""boo"" for purposes of derelativization against an HTTP base URI, which would give the final URI ""http://www.example.com/some/boo"" in the example above. 

Even relative URIs like ""http:../../boo"" are explicitly legal. 

"
1,"Unmatched right parentheses truncates query. The query processor truncates a query when right parentheses are unmatched.
E.g.:

 secret AND illegal) AND access:confidential

will not result in a ParseException instead will run as:

 secret AND illegal"
1,"Registering node type names with spaces fails in clustered environment. Registering a node type name that contains at least one space in a clustered environment will cause a JournalException in cluster nodes trying to read that change back from the journal. The stack trace observed is:

JournalException: Parse error while reading node type definition.
       at AbstractRecord.readNodeTypeDef(AbstractRecord.java:245)
       ...
Caused by: ParseException: Missing '[' delimiter for beginning of node type name ((internal), line 47)
       at Lexer.fail(Lexer.java:148)
       ...

(package names and intermediate frames omitted for brevity)."
1,"inconsistent repository after overlapping node add operations. It seems I can reproduce a sequence of operations that cause the repository to be inconsistent.

The short version: 2 sessions add a same-named child node to the same parent folder (not allowing same-name-siblings). Session 1's save() succeeds. Session 2's save() fails, but succeeds on retry (!).

After the operation, the child node created by session 1 is still present, but the parent doesn't list it as child node anymore.

(will add test case)"
1,"BasicCookieStore.getCookies() returns non-threadsafe collection. BasicCookieStore.getCookies() is a simple method.  It's synchronized, and it returns an unmodifiable wrapper around the underlying cookie list.  If the caller were to then iterate over it as another thread were to manipulate the cookie list via BasicCookieStore, this would create a thread un-safe situation because both threads aren't doing their reading/writing with the same lock (the reader doesn't even have a lock).

I suggest fixing this by using CopyOnWriteArrayList, or by making a defensive copy in getCookies()

This issue might apply to some of the other basic implementations of some of the interfaces but I haven't checked."
1,"DirectIOLinuxDirectory hardwires buffer size and creates files with invalid permissions. TestDemo fails if I use the DirectIOLinuxDirectory (using Robert's new -Dtests.directory=XXX), because when it O_CREATs a file, it fails to specify the mode, so [depending on C stack!] you can get permission denied.

Also, we currently hardwire the buffer size to 1 MB (Mark found this)... I plan to add a ""forcedBufferSize"" to the DirectIOLinuxDir's ctor, to optionally override lucene's default buffer sizes (which are way too small for direct IO to get barely OK performance).  If you pass 0 for this then you get Lucene's default buffer sizes..."
1,"Proxy NTLM Authentication  Redirecting to different address fails saying Proxy Auth Required.. The issue has been discussed in,
http://www.nabble.com/redirect-fails-when-NTLM-authentication-is-used-for-proxy-tt23867531.html

This was found in http client 3.1 release,  where NTLM proxy authentication is must and the server ask the redirect to a new url, in this case, when redirecting, the earlier proxy auth status is not cleared, so, it does not do proxy authentication for the new URL and hence fails.

Target Host Authenticaiton NTLM authentication - redirect also had problem and fixed as said,
http://issues.apache.org/jira/browse/HTTPCLIENT-211
Proxy Authentication - redirect has to be fixed, 

The wire logs for the release https://repository.apache.org/content/repositories/snapshots/org/apache/httpcomponents/httpclient/4.0-beta3-SNAPSHOT/
is given below,

[DEBUG] wire - >> ""GET http://verisign.com HTTP/1.1[EOL]""
[DEBUG] wire - >> ""Host: verisign.com[EOL]""
[DEBUG] wire - >> ""Proxy-Connection: Keep-Alive[EOL]""
[DEBUG] wire - >> ""User-Agent: Apache-HttpClient/UNAVAILABLE (java 1.5)[EOL]""
[DEBUG] wire - >> ""[EOL]""
[DEBUG] wire - << ""HTTP/1.1 407 Proxy Authentication Required ( The ISA Server requires authorization to fulfill the request. Access to the Web Proxy filter is denied.  )[EOL]""
[DEBUG] wire - << ""Via: 1.1 lab1[EOL]""
[DEBUG] wire - << ""Proxy-Authenticate: Negotiate[EOL]""
[DEBUG] wire - << ""Proxy-Authenticate: Kerberos[EOL]""
[DEBUG] wire - << ""Proxy-Authenticate: NTLM[EOL]""
[DEBUG] wire - << ""Proxy-Authenticate: Basic realm=""lab1.""[EOL]""
[DEBUG] wire - << ""Connection: Keep-Alive[EOL]""
[DEBUG] wire - << ""Proxy-Connection: Keep-Alive[EOL]""
[DEBUG] wire - << ""Pragma: no-cache[EOL]""
[DEBUG] wire - << ""Cache-Control: no-cache[EOL]""
[DEBUG] wire - << ""Content-Type: text/html[EOL]""
[DEBUG] wire - << ""Content-Length: 4107  [EOL]""
[DEBUG] wire - << ""[EOL]""
[DEBUG] wire - << ""<!DOCTYPE HTML PUBLIC ""-//W3C//DTD HTML 4.0 Transitional//EN"">[\r][\n]""
[DEBUG] wire - << ""<HTML><HEAD><TITLE>Error Message</TITLE>[\r][\n]""
[DEBUG] wire - << ""<META http-equiv=Content-Type content=""text/html; charset=UTF-8"">[\r][\n]""
[DEBUG] wire - << ""<STYLE id=L_default_1>A {[\r][\n]""
[DEBUG] wire - << ""[0x9]FONT-WEIGHT: bold; FONT-SIZE: 10pt; COLOR: #005a80; FONT-FAMILY: tahoma[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << ""A:hover {[\r][\n]""
[DEBUG] wire - << ""[0x9]FONT-WEIGHT: bold; FONT-SIZE: 10pt; COLOR: #0d3372; FONT-FAMILY: tahoma[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << ""TD {[\r][\n]""
[DEBUG] wire - << ""[0x9]FONT-SIZE: 8pt; FONT-FAMILY: tahoma[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << ""TD.titleBorder {[\r][\n]""
[DEBUG] wire - << ""[0x9]BORDER-RIGHT: #955319 1px solid; BORDER-TOP: #955319 1px solid; PADDING-LEFT: 8px; FONT-WEIGHT: bold; FONT-SIZE: 12pt; VERTICAL-ALIGN: middle; BORDER-LEFT: #955319 0px solid; COLOR: #955319; BORDER-BOTTOM: #955319 1px solid; FONT-FAMILY: tahoma; HEIGHT: 35px; BACKGROUND-COLOR: #d2b87a; TEXT-ALIGN: left[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << ""TD.titleBorder_x {[\r][\n]""
[DEBUG] wire - << ""[0x9]BORDER-RIGHT: #955319 0px solid; BORDER-TOP: #955319 1px solid; PADDING-LEFT: 8px; FONT-WEIGHT: bold; FONT-SIZE: 12pt; VERTICAL-ALIGN: middle; BORDER-LEFT: #955319 1px solid; COLOR: #978c79; BORDER-BOTTOM: #955319 1px solid; FONT-FAMILY: tahoma; HEIGHT: 35px; BACKGROUND-COLOR: #d2b87a; TEXT-ALIGN: left[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << "".TitleDescription {[\r][\n]""
[DEBUG] wire - << ""[0x9]FONT-WEIGHT: bold; FONT-SIZE: 12pt; COLOR: black; FONT-FAMILY: tahoma[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << ""SPAN.explain {[\r][\n]""
[DEBUG] wire - << ""[0x9]FONT-WEIGHT: normal; FONT-SIZE: 10pt; COLOR: #934225[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << ""SPAN.TryThings {[\r][\n]""
[DEBUG] wire - << ""[0x9]FONT-WEIGHT: normal; FONT-SIZE: 10pt; COLOR: #934225[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << "".TryList {[\r][\n]""
[DEBUG] wire - << ""[0x9]MARGIN-TOP: 5px; FONT-WEIGHT: normal; FONT-SIZE: 8pt; COLOR: black; FONT-FAMILY: tahoma[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << "".X {[\r][\n]""
[DEBUG] wire - << ""[0x9]BORDER-RIGHT: #955319 1px solid; BORDER-TOP: #955319 1px solid; FONT-WEIGHT: normal; FONT-SIZE: 12pt; BORDER-LEFT: #955319 1px solid; COLOR: #7b3807; BORDER-BOTTOM: #955319 1px solid; FONT-FAMILY: verdana; BACKGROUND-COLOR: #d1c2b4[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << "".adminList {[\r][\n]""
[DEBUG] wire - << ""[0x9]MARGIN-TOP: 2px[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << ""</STYLE>[\r][\n]""
[DEBUG] wire - << ""<META content=""MSHTML 6.00.2800.1170"" name=GENERATOR></HEAD>[\r][\n]""
[DEBUG] wire - << ""<BODY bgColor=#f3f3ed>[\r][\n]""
[DEBUG] wire - << ""<TABLE cellSpacing=0 cellPadding=0 width=""100%"">[\r][\n]""
[DEBUG] wire - << ""  <TBODY>[\r][\n]""
[DEBUG] wire - << ""  <TR>[\r][\n]""
[DEBUG] wire - << ""    <TD class=titleborder_x width=30>[\r][\n]""
[DEBUG] wire - << ""      <TABLE height=25 cellSpacing=2 cellPadding=0 width=25 bgColor=black>[\r][\n]""
[DEBUG] wire - << ""        <TBODY>[\r][\n]""
[DEBUG] wire - << ""        <TR>[\r][\n]""
[DEBUG] wire - << ""          <TD class=x vAlign=center alig""
[DEBUG] wire - << ""n=middle>X</TD>[\r][\n]""
[DEBUG] wire - << ""        </TR>[\r][\n]""
[DEBUG] wire - << ""        </TBODY>[\r][\n]""
[DEBUG] wire - << ""      </TABLE>[\r][\n]""
[DEBUG] wire - << ""    </TD>[\r][\n]""
[DEBUG] wire - << ""    <TD class=titleBorder id=L_default_2>Network Access Message:<SPAN class=TitleDescription> The page cannot be displayed</SPAN> </TD>[\r][\n]""
[DEBUG] wire - << ""  </TR>[\r][\n]""
[DEBUG] wire - << ""  </TBODY>[\r][\n]""
[DEBUG] wire - << ""</TABLE>[\r][\n]""
[DEBUG] wire - << ""[\r][\n]""
[DEBUG] wire - << ""<TABLE id=spacer>[\r][\n]""
[DEBUG] wire - << ""  <TBODY>[\r][\n]""
[DEBUG] wire - << ""  <TR>[\r][\n]""
[DEBUG] wire - << ""    <TD height=10></TD></TR></TBODY></TABLE>[\r][\n]""
[DEBUG] wire - << ""<TABLE width=400>[\r][\n]""
[DEBUG] wire - << ""  <TBODY>[\r][\n]""
[DEBUG] wire - << ""  <TR>[\r][\n]""
[DEBUG] wire - << ""    <TD noWrap width=25></TD>[\r][\n]""
[DEBUG] wire - << ""    <TD width=400><SPAN class=explain><ID id=L_default_3><B>Explanation:</B></ID></SPAN><ID id=L_default_4> There is a problem with the page you are trying to reach and it cannot be displayed. </ID><BR><BR>[\r][\n]""
[DEBUG] wire - << ""    <B><SPAN class=tryThings><ID id=L_default_5><B>Try the following:</B></ID></SPAN></B> [\r][\n]""
[DEBUG] wire - << ""      <UL class=TryList>[\r][\n]""
[DEBUG] wire - << ""        <LI id=L_default_6><B>Refresh page:</B> Search for the page again by clicking the Refresh button. The timeout may have occurred due to Internet congestion.[\r][\n]""
[DEBUG] wire - << ""<LI id=L_default_7><B>Check spelling:</B> Check that you typed the Web page address correctly. The address may have been mistyped.[\r][\n]""
[DEBUG] wire - << ""<LI id=L_default_8><B>Access from a link:</B> If there is a link to the page you are looking for, try accessing the page from that link.[\r][\n]""
[DEBUG] wire - << ""[\r][\n]""
[DEBUG] wire - << ""      </UL>[\r][\n]""
[DEBUG] wire - << ""<ID id=L_default_9>If you are still not able to view the requested page, try contacting your administrator or Helpdesk.</ID> <BR><BR>[\r][\n]""
[DEBUG] wire - << ""    </TD>[\r][\n]""
[DEBUG] wire - << ""  </TR>[\r][\n]""
[DEBUG] wire - << ""  </TBODY>[\r][\n]""
[DEBUG] wire - << ""</TABLE>[\r][\n]""
[DEBUG] wire - << ""[\r][\n]""
[DEBUG] wire - << ""<TABLE id=spacer><TBODY><TR><TD height=15></TD></TR></TBODY></TABLE>[\r][\n]""
[DEBUG] wire - << ""[\r][\n]""
[DEBUG] wire - << ""<TABLE width=400>[\r][\n]""
[DEBUG] wire - << ""  <TBODY>[\r][\n]""
[DEBUG] wire - << ""  <TR>[\r][\n]""
[DEBUG] wire - << ""    <TD noWrap width=25></TD>[\r][\n]""
[DEBUG] wire - << ""    <TD width=400 id=L_default_10><B>Technical Information (for support personnel)</B> [\r][\n]""
[DEBUG] wire - << ""      <UL class=adminList>[\r][\n]""
[DEBUG] wire - << ""        <LI id=L_default_11>Error Code: 407 Proxy Authentication Required. The ISA Server requires authorization to fulfill the request. Access to the Web Proxy filter is denied. (12209)[\r][\n]""
[DEBUG] wire - << ""<LI id=L_default_12>IP Address: x.x.x.x[\r][\n]""
[DEBUG] wire - << ""<LI id=L_default_13>Date: 6/29/2009 11:15:15 AM [GMT][\r][\n]""
[DEBUG] wire - << ""<LI id=L_default_14>Server: lab1[\r][\n]""
[DEBUG] wire - << ""<LI id=L_default_15>Source: proxy[\r][\n]""
[DEBUG] wire - << ""[\r][\n]""
[DEBUG] wire - << ""      </UL>[\r][\n]""
[DEBUG] wire - << ""    </TD>[\r][\n]""
[DEBUG] wire - << ""  </TR>[\r][\n]""
[DEBUG] wire - << ""  </TBODY>[\r][\n]""
[DEBUG] wire - << ""</TABLE>[\r][\n]""
[DEBUG] wire - << ""[\r][\n]""
[DEBUG] wire - << ""</BODY>[\r][\n]""
[DEBUG] wire - << ""</HTML>[\r][\n]""
[DEBUG] wire - << ""[\r][\n]""
[DEBUG] wire - >> ""GET http://verisign.com HTTP/1.1[EOL]""
[DEBUG] wire - >> ""Host: verisign.com[EOL]""
[DEBUG] wire - >> ""Proxy-Connection: Keep-Alive[EOL]""
[DEBUG] wire - >> ""User-Agent: Apache-HttpClient/UNAVAILABLE (java 1.5)[EOL]""
[DEBUG] wire - >> ""Proxy-Authorization: NTLM TlRMTVNTUAABAAAAATIAAAgACAAgAAAADgAOACgAAABNWURPTUFJTkpDSUZTMjMwXzg2Xzkx[EOL]""
[DEBUG] wire - >> ""[EOL]""
[DEBUG] wire - << ""HTTP/1.1 407 Proxy Authentication Required ( Access is denied.  )[EOL]""
[DEBUG] wire - << ""Via: 1.1 lab1[EOL]""
[DEBUG] wire - << ""Proxy-Authenticate: NTLM TlRMTVNTUAACAAAAAAAAADgAAAABAgACqbXrIWnZ3i4AAAAAAAAAAAAAAAA4AAAABQLODgAAAA8=[EOL]""
[DEBUG] wire - << ""Connection: Keep-Alive[EOL]""
[DEBUG] wire - << ""Proxy-Connection: Keep-Alive[EOL]""
[DEBUG] wire - << ""Pragma: no-cache[EOL]""
[DEBUG] wire - << ""Cache-Control: no-cache[EOL]""
[DEBUG] wire - << ""Content-Type: text/html[EOL]""
[DEBUG] wire - << ""Content-Length: 0     [EOL]""
[DEBUG] wire - << ""[EOL]""
[DEBUG] wire - >> ""GET http://verisign.com HTTP/1.1[EOL]""
[DEBUG] wire - >> ""Host: verisign.com[EOL]""
[DEBUG] wire - >> ""Proxy-Connection: Keep-Alive[EOL]""
[DEBUG] wire - >> ""User-Agent: Apache-HttpClient/UNAVAILABLE (java 1.5)[EOL]""
[DEBUG] wire - >> ""Proxy-Authorization: NTLM TlRMTVNTUAADAAAAGAAYAEAAAAAwADAAWAAAABAAEACIAAAAGgAaAJgAAAAcABwAsgAAAAAAAAAAAAAAAQIAAAXLpW40q7jqh7E6FgFnJqy9529ANaSLqfTiwjyF2BrUP9F8ObYOyYsBAQAAAAAAACDgxRg9+skBRt4mUOFFCs0AAAAAAAAAAE0AWQBEAE8ATQBBAEkATgBBAGQAbQBpAG4AaQBzAHQAcgBhAHQAbwByAEoAQwBJAEYAUwAyADMAMABfADgANgBfADkAMQA=[EOL]""
[DEBUG] wire - >> ""[EOL]""
[DEBUG] wire - << ""HTTP/1.1 301 Unknown reason[EOL]""
[DEBUG] wire - << ""Via: 1.1 lab1[EOL]""
[DEBUG] wire - << ""Connection: Keep-Alive[EOL]""
[DEBUG] wire - << ""Proxy-Connection: Keep-Alive[EOL]""
[DEBUG] wire - << ""Content-length: 0[EOL]""
[DEBUG] wire - << ""Date: Mon, 29 Jun 2009 11:16:50 GMT[EOL]""
[DEBUG] wire - << ""Location: http://www.verisign.com/[EOL]""
[DEBUG] wire - << ""Content-type: text/html[EOL]""
[DEBUG] wire - << ""Server: Netscape-Enterprise/4.1[EOL]""
[DEBUG] wire - << ""[EOL]""
[ERROR] RequestProxyAuthentication - Proxy authentication error: Unexpected state: MSG_TYPE3_GENERATED
[DEBUG] wire - >> ""GET http://www.verisign.com/ HTTP/1.1[EOL]""
[DEBUG] wire - >> ""Host: www.verisign.com[EOL]""
[DEBUG] wire - >> ""Proxy-Connection: Keep-Alive[EOL]""
[DEBUG] wire - >> ""User-Agent: Apache-HttpClient/UNAVAILABLE (java 1.5)[EOL]""
[DEBUG] wire - >> ""[EOL]""
[DEBUG] wire - << ""HTTP/1.1 407 Proxy Authentication Required ( The ISA Server requires authorization to fulfill the request. Access to the Web Proxy filter is denied.  )[EOL]""
[DEBUG] wire - << ""Via: 1.1 lab1[EOL]""
[DEBUG] wire - << ""Proxy-Authenticate: Negotiate[EOL]""
[DEBUG] wire - << ""Proxy-Authenticate: Kerberos[EOL]""
[DEBUG] wire - << ""Proxy-Authenticate: NTLM[EOL]""
[DEBUG] wire - << ""Proxy-Authenticate: Basic realm=""lab1.""[EOL]""
[DEBUG] wire - << ""Connection: Keep-Alive[EOL]""
[DEBUG] wire - << ""Proxy-Connection: Keep-Alive[EOL]""
[DEBUG] wire - << ""Pragma: no-cache[EOL]""
[DEBUG] wire - << ""Cache-Control: no-cache[EOL]""
[DEBUG] wire - << ""Content-Type: text/html[EOL]""
[DEBUG] wire - << ""Content-Length: 4107  [EOL]""
[DEBUG] wire - << ""[EOL]""
----------------------------------------
HTTP/1.1 407 Proxy Authentication Required ( The ISA Server requires authorization to fulfill the request. Access to the Web Proxy filter is denied.  )

Thanks,
Raj





"
1,"MatchAllDocsQueryNode toString() creates invalid XML-Tag. MatchAllDocsQueryNode.toString() returns ""<matchAllDocs field='*' term='*'>"", which is inavlid XML should read ""<matchAllDocs field='*' term='*' />.
"
1,"TransientRepository does not shutdown if first login fails. The TransientRepository.login() method initializes the underlying repository when it is first called (initially or after the repository has previously been shut down) bug doesn't shut down the initialized repository if the login fails. If the application then decides to exit or otherwise not start another session, then the repository remains in an initialized state with no active sessions.

This issue should be fixed by properly handling login failures in the TransientRepository.login() method."
1,"TestIndexWriter.testCommitThreadSafety fails on realtime_search branch. Hudson failed on RT with this error - I wasn't able to reproduce yet....

{noformat}
NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testCommitThreadSafety -Dtests.seed=410261592077577885:-4099127561715488589 -Dtests.multiplier=3
NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testCommitThreadSafety -Dtests.seed=410261592077577885:-4099127561715488589 -Dtests.multiplier=3
The following exceptions were thrown by threads:
*** Thread: Thread-331 ***
java.lang.RuntimeException: java.lang.AssertionError: term=f:2_0; r=DirectoryReader(segments_6 _8(4.0):Cv7) expected:<1> but was:<0>
	at org.apache.lucene.index.TestIndexWriter$5.run(TestIndexWriter.java:2416)
Caused by: java.lang.AssertionError: term=f:2_0; r=DirectoryReader(segments_6 _8(4.0):Cv7) expected:<1> but was:<0>
	at org.junit.Assert.fail(Assert.java:91)
	at org.junit.Assert.failNotEquals(Assert.java:645)
	at org.junit.Assert.assertEquals(Assert.java:126)
	at org.junit.Assert.assertEquals(Assert.java:470)
	at org.apache.lucene.index.TestIndexWriter$5.run(TestIndexWriter.java:2410)
NOTE: test params are: codec=RandomCodecProvider: {=SimpleText, f6=MockVariableIntBlock(baseBlockSize=91), f7=MockFixedIntBlock(blockSize=1289), f8=Standard, f9=MockRandom, f1=MockSep, f0=Pulsing(freqCutoff=15), f3=Pulsing(freqCutoff=15), f2=MockFixedIntBlock(blockSize=1289), f5=MockVariableIntBlock(baseBlockSize=91), f4=MockRandom, f=MockSep, c=MockVariableIntBlock(baseBlockSize=91), termVector=SimpleText, d9=SimpleText, d8=MockSep, d5=MockVariableIntBlock(baseBlockSize=91), d4=MockRandom, d7=Standard, d6=SimpleText, d25=Standard, d0=MockVariableIntBlock(baseBlockSize=91), c29=Standard, d24=SimpleText, d1=MockFixedIntBlock(blockSize=1289), c28=MockFixedIntBlock(blockSize=1289), d23=MockVariableIntBlock(baseBlockSize=91), d2=Standard, c27=MockVariableIntBlock(baseBlockSize=91), d22=MockRandom, d3=MockRandom, d21=MockFixedIntBlock(blockSize=1289), d20=MockVariableIntBlock(baseBlockSize=91), c22=MockVariableIntBlock(baseBlockSize=91), c21=MockRandom, c20=Pulsing(freqCutoff=15), d29=MockVariableIntBlock(baseBlockSize=91), c26=SimpleText, d28=MockRandom, c25=MockSep, d27=Pulsing(freqCutoff=15), c24=MockRandom, d26=MockFixedIntBlock(blockSize=1289), c23=Standard, e9=MockRandom, e8=MockFixedIntBlock(blockSize=1289), e7=MockVariableIntBlock(baseBlockSize=91), e6=MockSep, e5=Pulsing(freqCutoff=15), c17=Standard, e3=MockFixedIntBlock(blockSize=1289), d12=SimpleText, c16=SimpleText, e4=Pulsing(freqCutoff=15), d11=MockSep, c19=MockSep, e1=MockSep, d14=Pulsing(freqCutoff=15), c18=Pulsing(freqCutoff=15), e2=SimpleText, d13=MockFixedIntBlock(blockSize=1289), e0=Standard, d10=Standard, d19=Pulsing(freqCutoff=15), c11=SimpleText, c10=MockSep, d16=MockRandom, c13=MockSep, c12=Pulsing(freqCutoff=15), d15=Standard, d18=SimpleText, c15=MockFixedIntBlock(blockSize=1289), d17=MockSep, c14=MockVariableIntBlock(baseBlockSize=91), b3=MockRandom, b2=Standard, b5=SimpleText, b4=MockSep, b7=MockSep, b6=Pulsing(freqCutoff=15), d50=MockVariableIntBlock(baseBlockSize=91), b9=MockFixedIntBlock(blockSize=1289), b8=MockVariableIntBlock(baseBlockSize=91), d43=Pulsing(freqCutoff=15), d42=MockFixedIntBlock(blockSize=1289), d41=SimpleText, d40=MockSep, d47=MockRandom, d46=Standard, b0=SimpleText, d45=MockFixedIntBlock(blockSize=1289), b1=Standard, d44=MockVariableIntBlock(baseBlockSize=91), d49=MockSep, d48=Pulsing(freqCutoff=15), c6=MockVariableIntBlock(baseBlockSize=91), c5=MockRandom, c4=Pulsing(freqCutoff=15), c3=MockFixedIntBlock(blockSize=1289), c9=MockSep, c8=MockRandom, c7=Standard, d30=MockFixedIntBlock(blockSize=1289), d32=MockRandom, d31=Standard, c1=MockVariableIntBlock(baseBlockSize=91), d34=Standard, c2=MockFixedIntBlock(blockSize=1289), d33=SimpleText, d36=MockSep, c0=MockSep, d35=Pulsing(freqCutoff=15), d38=MockVariableIntBlock(baseBlockSize=91), d37=MockRandom, d39=SimpleText, e92=MockFixedIntBlock(blockSize=1289), e93=Pulsing(freqCutoff=15), e90=MockSep, e91=SimpleText, e89=MockVariableIntBlock(baseBlockSize=91), e88=MockSep, e87=Pulsing(freqCutoff=15), e86=SimpleText, e85=MockSep, e84=MockRandom, e83=Standard, e80=MockFixedIntBlock(blockSize=1289), e81=Standard, e82=MockRandom, e77=MockVariableIntBlock(baseBlockSize=91), e76=MockRandom, e79=Standard, e78=SimpleText, e73=MockSep, e72=Pulsing(freqCutoff=15), e75=MockFixedIntBlock(blockSize=1289), e74=MockVariableIntBlock(baseBlockSize=91), binary=MockVariableIntBlock(baseBlockSize=91), f98=Pulsing(freqCutoff=15), f97=MockFixedIntBlock(blockSize=1289), f99=MockRandom, f94=Standard, f93=SimpleText, f96=MockSep, f95=Pulsing(freqCutoff=15), e95=SimpleText, e94=MockSep, e97=Pulsing(freqCutoff=15), e96=MockFixedIntBlock(blockSize=1289), e99=MockFixedIntBlock(blockSize=1289), e98=MockVariableIntBlock(baseBlockSize=91), id=MockRandom, f34=Standard, f33=SimpleText, f32=MockVariableIntBlock(baseBlockSize=91), f31=MockRandom, f30=MockFixedIntBlock(blockSize=1289), f39=Standard, f38=MockVariableIntBlock(baseBlockSize=91), f37=MockRandom, f36=Pulsing(freqCutoff=15), f35=MockFixedIntBlock(blockSize=1289), f43=MockSep, f42=Pulsing(freqCutoff=15), f45=MockFixedIntBlock(blockSize=1289), f44=MockVariableIntBlock(baseBlockSize=91), f41=SimpleText, f40=MockSep, f47=Standard, f46=SimpleText, f49=MockSep, f48=Pulsing(freqCutoff=15), content=MockSep, e19=SimpleText, e18=MockSep, e17=Standard, f12=SimpleText, e16=SimpleText, f11=MockSep, f10=MockRandom, e15=MockVariableIntBlock(baseBlockSize=91), e14=MockRandom, f16=MockRandom, e13=MockSep, f15=Standard, e12=Pulsing(freqCutoff=15), e11=Standard, f14=MockFixedIntBlock(blockSize=1289), e10=SimpleText, f13=MockVariableIntBlock(baseBlockSize=91), f19=Pulsing(freqCutoff=15), f18=Standard, f17=SimpleText, e29=MockRandom, e26=MockSep, f21=Pulsing(freqCutoff=15), e25=Pulsing(freqCutoff=15), f20=MockFixedIntBlock(blockSize=1289), e28=MockFixedIntBlock(blockSize=1289), f23=MockVariableIntBlock(baseBlockSize=91), e27=MockVariableIntBlock(baseBlockSize=91), f22=MockRandom, f25=SimpleText, e22=MockFixedIntBlock(blockSize=1289), f24=MockSep, e21=MockVariableIntBlock(baseBlockSize=91), f27=Pulsing(freqCutoff=15), e24=MockRandom, f26=MockFixedIntBlock(blockSize=1289), e23=Standard, f29=MockFixedIntBlock(blockSize=1289), f28=MockVariableIntBlock(baseBlockSize=91), e20=Pulsing(freqCutoff=15), field=MockRandom, string=Standard, e30=MockRandom, e31=MockVariableIntBlock(baseBlockSize=91), a98=Standard, e34=MockSep, a99=MockRandom, e35=SimpleText, f79=MockSep, e32=Standard, e33=MockRandom, b97=MockRandom, f77=MockRandom, e38=Standard, b98=MockVariableIntBlock(baseBlockSize=91), f78=MockVariableIntBlock(baseBlockSize=91), e39=MockRandom, b99=SimpleText, f75=MockFixedIntBlock(blockSize=1289), e36=MockVariableIntBlock(baseBlockSize=91), f76=Pulsing(freqCutoff=15), e37=MockFixedIntBlock(blockSize=1289), f73=Pulsing(freqCutoff=15), f74=MockSep, f71=SimpleText, f72=Standard, f81=MockFixedIntBlock(blockSize=1289), f80=MockVariableIntBlock(baseBlockSize=91), e40=Standard, e41=Pulsing(freqCutoff=15), e42=MockSep, e43=MockFixedIntBlock(blockSize=1289), e44=Pulsing(freqCutoff=15), e45=MockRandom, e46=MockVariableIntBlock(baseBlockSize=91), f86=SimpleText, e47=MockSep, f87=Standard, e48=SimpleText, f88=Pulsing(freqCutoff=15), e49=MockFixedIntBlock(blockSize=1289), f89=MockSep, f82=MockVariableIntBlock(baseBlockSize=91), f83=MockFixedIntBlock(blockSize=1289), f84=Standard, f85=MockRandom, f90=MockRandom, f92=SimpleText, f91=MockSep, str=MockSep, a76=SimpleText, e56=SimpleText, f59=MockVariableIntBlock(baseBlockSize=91), a77=Standard, e57=Standard, a78=Pulsing(freqCutoff=15), e54=MockRandom, f57=Pulsing(freqCutoff=15), a79=MockSep, e55=MockVariableIntBlock(baseBlockSize=91), f58=MockSep, e52=MockVariableIntBlock(baseBlockSize=91), e53=MockFixedIntBlock(blockSize=1289), e50=Pulsing(freqCutoff=15), e51=MockSep, f51=MockFixedIntBlock(blockSize=1289), f52=Pulsing(freqCutoff=15), f50=SimpleText, f55=Standard, f56=MockRandom, f53=MockVariableIntBlock(baseBlockSize=91), e58=MockFixedIntBlock(blockSize=1289), f54=MockFixedIntBlock(blockSize=1289), e59=Pulsing(freqCutoff=15), a80=MockRandom, e60=MockRandom, a82=SimpleText, a81=MockSep, a84=MockSep, a83=Pulsing(freqCutoff=15), a86=MockFixedIntBlock(blockSize=1289), a85=MockVariableIntBlock(baseBlockSize=91), a89=Standard, f68=Standard, e65=Pulsing(freqCutoff=15), f69=MockRandom, e66=MockSep, a87=MockVariableIntBlock(baseBlockSize=91), e67=MockVariableIntBlock(baseBlockSize=91), a88=MockFixedIntBlock(blockSize=1289), e68=MockFixedIntBlock(blockSize=1289), e61=Standard, e62=MockRandom, e63=MockSep, e64=SimpleText, f60=MockRandom, f61=MockVariableIntBlock(baseBlockSize=91), f62=SimpleText, f63=Standard, e69=SimpleText, f64=MockSep, f65=SimpleText, f66=MockFixedIntBlock(blockSize=1289), f67=Pulsing(freqCutoff=15), f70=MockSep, a93=MockVariableIntBlock(baseBlockSize=91), a92=MockRandom, a91=Pulsing(freqCutoff=15), e71=Pulsing(freqCutoff=15), a90=MockFixedIntBlock(blockSize=1289), e70=MockFixedIntBlock(blockSize=1289), a97=SimpleText, a96=MockSep, a95=MockRandom, a94=Standard, c58=MockFixedIntBlock(blockSize=1289), a63=MockVariableIntBlock(baseBlockSize=91), a64=MockFixedIntBlock(blockSize=1289), c59=Pulsing(freqCutoff=15), c56=MockSep, d59=MockRandom, a61=Pulsing(freqCutoff=15), c57=SimpleText, a62=MockSep, c54=SimpleText, c55=Standard, a60=SimpleText, c52=MockRandom, c53=MockVariableIntBlock(baseBlockSize=91), d53=Standard, d54=MockRandom, d51=MockVariableIntBlock(baseBlockSize=91), d52=MockFixedIntBlock(blockSize=1289), d57=Pulsing(freqCutoff=15), b62=MockFixedIntBlock(blockSize=1289), d58=MockSep, b63=Pulsing(freqCutoff=15), d55=SimpleText, b60=MockSep, d56=Standard, b61=SimpleText, b56=SimpleText, b55=MockSep, b54=MockRandom, b53=Standard, d61=SimpleText, b59=MockVariableIntBlock(baseBlockSize=91), d60=MockSep, b58=MockSep, b57=Pulsing(freqCutoff=15), c62=MockSep, c61=Pulsing(freqCutoff=15), a59=Pulsing(freqCutoff=15), c60=Standard, a58=MockFixedIntBlock(blockSize=1289), a57=MockSep, a56=Pulsing(freqCutoff=15), a55=Standard, a54=SimpleText, a72=Standard, c67=MockRandom, a73=MockRandom, c68=MockVariableIntBlock(baseBlockSize=91), a74=MockSep, c69=SimpleText, a75=SimpleText, c63=Pulsing(freqCutoff=15), c64=MockSep, a70=MockRandom, c65=MockVariableIntBlock(baseBlockSize=91), a71=MockVariableIntBlock(baseBlockSize=91), c66=MockFixedIntBlock(blockSize=1289), d62=MockSep, d63=SimpleText, d64=MockFixedIntBlock(blockSize=1289), b70=MockFixedIntBlock(blockSize=1289), d65=Pulsing(freqCutoff=15), b71=MockRandom, d66=MockVariableIntBlock(baseBlockSize=91), b72=MockVariableIntBlock(baseBlockSize=91), d67=MockFixedIntBlock(blockSize=1289), b73=SimpleText, d68=Standard, b74=Standard, d69=MockRandom, b65=Pulsing(freqCutoff=15), b64=MockFixedIntBlock(blockSize=1289), b67=MockVariableIntBlock(baseBlockSize=91), b66=MockRandom, d70=Pulsing(freqCutoff=15), b69=MockRandom, b68=Standard, d72=MockVariableIntBlock(baseBlockSize=91), d71=MockRandom, c71=MockFixedIntBlock(blockSize=1289), c70=MockVariableIntBlock(baseBlockSize=91), a69=SimpleText, c73=MockRandom, c72=Standard, a66=MockFixedIntBlock(blockSize=1289), a65=MockVariableIntBlock(baseBlockSize=91), a68=MockRandom, a67=Standard, c32=MockSep, c33=SimpleText, c30=Standard, c31=MockRandom, c36=MockVariableIntBlock(baseBlockSize=91), a41=MockRandom, c37=MockFixedIntBlock(blockSize=1289), a42=MockVariableIntBlock(baseBlockSize=91), a0=SimpleText, c34=Pulsing(freqCutoff=15), c35=MockSep, a40=Pulsing(freqCutoff=15), b84=Pulsing(freqCutoff=15), d79=MockSep, b85=MockSep, b82=SimpleText, d77=Standard, c38=SimpleText, b83=Standard, d78=MockRandom, c39=Standard, b80=Standard, d75=MockRandom, b81=MockRandom, d76=MockVariableIntBlock(baseBlockSize=91), d73=MockFixedIntBlock(blockSize=1289), d74=Pulsing(freqCutoff=15), d83=Standard, a9=MockSep, d82=SimpleText, d81=MockVariableIntBlock(baseBlockSize=91), d80=MockRandom, b79=MockSep, b78=Standard, b77=SimpleText, b76=MockVariableIntBlock(baseBlockSize=91), b75=MockRandom, a1=SimpleText, a35=Pulsing(freqCutoff=15), a2=Standard, a34=MockFixedIntBlock(blockSize=1289), a3=Pulsing(freqCutoff=15), a33=SimpleText, a4=MockSep, a32=MockSep, a5=MockFixedIntBlock(blockSize=1289), a39=MockRandom, c40=Pulsing(freqCutoff=15), a6=Pulsing(freqCutoff=15), a38=Standard, a7=MockRandom, a37=MockFixedIntBlock(blockSize=1289), a8=MockVariableIntBlock(baseBlockSize=91), a36=MockVariableIntBlock(baseBlockSize=91), c41=MockFixedIntBlock(blockSize=1289), c42=Pulsing(freqCutoff=15), c43=MockRandom, c44=MockVariableIntBlock(baseBlockSize=91), c45=Standard, a50=SimpleText, c46=MockRandom, a51=Standard, c47=MockSep, a52=Pulsing(freqCutoff=15), c48=SimpleText, a53=MockSep, b93=MockVariableIntBlock(baseBlockSize=91), d88=MockFixedIntBlock(blockSize=1289), c49=MockVariableIntBlock(baseBlockSize=91), b94=MockFixedIntBlock(blockSize=1289), d89=Pulsing(freqCutoff=15), b95=Standard, b96=MockRandom, d84=SimpleText, b90=SimpleText, d85=Standard, b91=MockFixedIntBlock(blockSize=1289), d86=Pulsing(freqCutoff=15), b92=Pulsing(freqCutoff=15), d87=MockSep, d92=MockSep, d91=Pulsing(freqCutoff=15), d94=MockFixedIntBlock(blockSize=1289), d93=MockVariableIntBlock(baseBlockSize=91), b87=MockSep, b86=Pulsing(freqCutoff=15), d90=SimpleText, b89=MockFixedIntBlock(blockSize=1289), b88=MockVariableIntBlock(baseBlockSize=91), a44=MockVariableIntBlock(baseBlockSize=91), a43=MockRandom, a46=Standard, a45=SimpleText, a48=SimpleText, a47=MockSep, c51=Standard, a49=MockFixedIntBlock(blockSize=1289), c50=SimpleText, d98=MockFixedIntBlock(blockSize=1289), d97=MockVariableIntBlock(baseBlockSize=91), d96=MockSep, d95=Pulsing(freqCutoff=15), d99=MockRandom, a20=MockRandom, c99=MockVariableIntBlock(baseBlockSize=91), c98=MockRandom, c97=Pulsing(freqCutoff=15), c96=MockFixedIntBlock(blockSize=1289), b19=MockVariableIntBlock(baseBlockSize=91), a16=SimpleText, a17=Standard, b17=Pulsing(freqCutoff=15), a14=MockRandom, b18=MockSep, a15=MockVariableIntBlock(baseBlockSize=91), a12=MockVariableIntBlock(baseBlockSize=91), a13=MockFixedIntBlock(blockSize=1289), a10=Pulsing(freqCutoff=15), a11=MockSep, b11=MockFixedIntBlock(blockSize=1289), b12=Pulsing(freqCutoff=15), b10=SimpleText, b15=Standard, b16=MockRandom, a18=MockFixedIntBlock(blockSize=1289), b13=MockVariableIntBlock(baseBlockSize=91), a19=Pulsing(freqCutoff=15), b14=MockFixedIntBlock(blockSize=1289), b30=MockSep, a31=Pulsing(freqCutoff=15), a30=MockFixedIntBlock(blockSize=1289), b28=Standard, a25=Pulsing(freqCutoff=15), b29=MockRandom, a26=MockSep, a27=MockVariableIntBlock(baseBlockSize=91), a28=MockFixedIntBlock(blockSize=1289), a21=Standard, a22=MockRandom, a23=MockSep, a24=SimpleText, b20=MockRandom, b21=MockVariableIntBlock(baseBlockSize=91), b22=SimpleText, b23=Standard, a29=SimpleText, b24=MockSep, b25=SimpleText, b26=MockFixedIntBlock(blockSize=1289), b27=Pulsing(freqCutoff=15), b41=MockFixedIntBlock(blockSize=1289), b40=MockVariableIntBlock(baseBlockSize=91), c77=MockRandom, c76=Standard, c75=MockFixedIntBlock(blockSize=1289), c74=MockVariableIntBlock(baseBlockSize=91), c79=Standard, c78=SimpleText, c80=MockVariableIntBlock(baseBlockSize=91), c83=MockSep, c84=SimpleText, c81=Standard, b39=MockSep, c82=MockRandom, b37=MockRandom, b38=MockVariableIntBlock(baseBlockSize=91), b35=MockFixedIntBlock(blockSize=1289), b36=Pulsing(freqCutoff=15), b33=Pulsing(freqCutoff=15), b34=MockSep, b31=SimpleText, b32=Standard, str2=Pulsing(freqCutoff=15), b50=MockRandom, b52=SimpleText, str3=MockRandom, b51=MockSep, c86=SimpleText, tvtest=MockSep, c85=MockSep, c88=Pulsing(freqCutoff=15), c87=MockFixedIntBlock(blockSize=1289), c89=MockVariableIntBlock(baseBlockSize=91), c90=Pulsing(freqCutoff=15), c91=MockSep, c92=MockFixedIntBlock(blockSize=1289), c93=Pulsing(freqCutoff=15), c94=MockRandom, c95=MockVariableIntBlock(baseBlockSize=91), content1=MockSep, b46=SimpleText, b47=Standard, content3=Standard, b48=Pulsing(freqCutoff=15), content4=SimpleText, b49=MockSep, content5=MockRandom, b42=MockVariableIntBlock(baseBlockSize=91), b43=MockFixedIntBlock(blockSize=1289), b44=Standard, b45=MockRandom}, locale=lv_LV, timezone=Australia/Lindeman
NOTE: all tests run in this JVM:
[TestNumericTokenStream, TestIndexFileDeleter, TestIndexInput, TestIndexReaderCloneNorms, TestIndexReaderReopen, TestIndexWriter]
NOTE: FreeBSD 8.2-RELEASE amd64/Sun Microsystems Inc. 1.6.0 (64-bit)/cpus=16,threads=1,free=44228576,total=213778432
{noformat}"
1,"Unable to delete a non session-scoped locked node in XA Environment. You must first add a valid lockToken to the Session and then try to remove this node in a XA Environment.
This will resulting in a NoSuchItemStateException: State has been marked destroyed.
The  problem is that the unlock Operation will be done after that the node has been marked for destroyed.
"
1,"NodeState and NodeStateListener deadlock. 

Java stack information for the threads listed above:
===================================================
""jmssecondaryApplnJobExecutor-8"":
	at org.apache.jackrabbit.core.state.NodeState.getChildNodeEntry(NodeState.java:300)
	- waiting to lock <0x9e6c6d08> (a org.apache.jackrabbit.core.state.NodeState)
	at org.apache.jackrabbit.core.CachingHierarchyManager.nodeModified(CachingHierarchyManager.java:316)
	- locked <0xa09882a8> (a java.lang.Object)
	at org.apache.jackrabbit.core.CachingHierarchyManager.stateModified(CachingHierarchyManager.java:293)
	at org.apache.jackrabbit.core.state.StateChangeDispatcher.notifyStateModified(StateChangeDispatcher.java:111)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.stateModified(SessionItemStateManager.java:889)
	at org.apache.jackrabbit.core.state.StateChangeDispatcher.notifyStateModified(StateChangeDispatcher.java:111)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.stateModified(LocalItemStateManager.java:452)
	at org.apache.jackrabbit.core.state.XAItemStateManager.stateModified(XAItemStateManager.java:602)
	at org.apache.jackrabbit.core.state.StateChangeDispatcher.notifyStateModified(StateChangeDispatcher.java:111)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.stateModified(SharedItemStateManager.java:400)
	at org.apache.jackrabbit.core.state.ItemState.notifyStateUpdated(ItemState.java:244)
	at org.apache.jackrabbit.core.state.ChangeLog.persisted(ChangeLog.java:297)
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:749)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:1115)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:351)
	at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:354)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:326)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:325)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1111)
	- locked <0x9b1b0be0> (a org.apache.jackrabbit.core.XASessionImpl)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:915)
	at org.apache.jackrabbit.jca.JCASessionHandle.save(JCASessionHandle.java:180)
        ...
	at sun.reflect.GeneratedMethodAccessor1067.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:36)
	at sun.reflect.GeneratedMethodAccessor110.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:243)
	at javax.management.modelmbean.RequiredModelMBean.invokeMethod(RequiredModelMBean.java:1074)
	at javax.management.modelmbean.RequiredModelMBean.invoke(RequiredModelMBean.java:955)
	at org.springframework.jmx.export.SpringModelMBean.invoke(SpringModelMBean.java:88)
	at org.jboss.mx.server.RawDynamicInvoker.invoke(RawDynamicInvoker.java:164)
	at org.jboss.mx.modelmbean.RequiredModelMBeanInvoker.invoke(RequiredModelMBeanInvoker.java:127)
	at org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)
	at org.jboss.system.server.jmx.LazyMBeanServer.invoke(LazyMBeanServer.java:291)
	at javax.management.MBeanServerInvocationHandler.invoke(MBeanServerInvocationHandler.java:288)
	at $Proxy692.doDiscoveryNow(Unknown Source)
        ...
	at org.springframework.jms.listener.AbstractMessageListenerContainer.doInvokeListener(AbstractMessageListenerContainer.java:531)
	at org.springframework.jms.listener.AbstractMessageListenerContainer.invokeListener(AbstractMessageListenerContainer.java:466)
	at org.springframework.jms.listener.AbstractMessageListenerContainer.doExecuteListener(AbstractMessageListenerContainer.java:435)
	at org.springframework.jms.listener.AbstractPollingMessageListenerContainer.doReceiveAndExecute(AbstractPollingMessageListenerContainer.java:322)
	at org.springframework.jms.listener.AbstractPollingMessageListenerContainer.receiveAndExecute(AbstractPollingMessageListenerContainer.java:260)
	at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.invokeListener(DefaultMessageListenerContainer.java:944)
	at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.run(DefaultMessageListenerContainer.java:868)
	at java.lang.Thread.run(Thread.java:619)
""jmssecondaryApplnJobExecutor-7"":
	at org.apache.jackrabbit.core.CachingHierarchyManager.nodeAdded(CachingHierarchyManager.java:362)
	- waiting to lock <0xa09882a8> (a java.lang.Object)
	at org.apache.jackrabbit.core.state.StateChangeDispatcher.notifyNodeAdded(StateChangeDispatcher.java:159)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.nodeAdded(SessionItemStateManager.java:947)
	at org.apache.jackrabbit.core.state.NodeState.notifyNodeAdded(NodeState.java:882)
	at org.apache.jackrabbit.core.state.NodeState.addChildNodeEntry(NodeState.java:351)
	- locked <0x9e6c6d08> (a org.apache.jackrabbit.core.state.NodeState)
	at org.apache.jackrabbit.core.NodeImpl.createChildNode(NodeImpl.java:541)
	- locked <0xa00619a8> (a org.apache.jackrabbit.core.NodeImpl)
	at org.apache.jackrabbit.core.NodeImpl.internalAddChildNode(NodeImpl.java:802)
	at org.apache.jackrabbit.core.NodeImpl.internalAddNode(NodeImpl.java:735)
	at org.apache.jackrabbit.core.NodeImpl.addNodeWithUuid(NodeImpl.java:2200)
	- locked <0xa00619f8> (a org.apache.jackrabbit.core.NodeImpl)
	at org.apache.jackrabbit.core.NodeImpl.addNode(NodeImpl.java:2133)
	- locked <0xa00619f8> (a org.apache.jackrabbit.core.NodeImpl)
        ...
	at sun.reflect.GeneratedMethodAccessor1067.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:36)
	at sun.reflect.GeneratedMethodAccessor110.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:243)
	at javax.management.modelmbean.RequiredModelMBean.invokeMethod(RequiredModelMBean.java:1074)
	at javax.management.modelmbean.RequiredModelMBean.invoke(RequiredModelMBean.java:955)
	at org.springframework.jmx.export.SpringModelMBean.invoke(SpringModelMBean.java:88)
	at org.jboss.mx.server.RawDynamicInvoker.invoke(RawDynamicInvoker.java:164)
	at org.jboss.mx.modelmbean.RequiredModelMBeanInvoker.invoke(RequiredModelMBeanInvoker.java:127)
	at org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)
	at org.jboss.system.server.jmx.LazyMBeanServer.invoke(LazyMBeanServer.java:291)
	at javax.management.MBeanServerInvocationHandler.invoke(MBeanServerInvocationHandler.java:288)
	at $Proxy692.doDiscoveryNow(Unknown Source)
        ...
	at org.springframework.jms.listener.AbstractMessageListenerContainer.doInvokeListener(AbstractMessageListenerContainer.java:531)
	at org.springframework.jms.listener.AbstractMessageListenerContainer.invokeListener(AbstractMessageListenerContainer.java:466)
	at org.springframework.jms.listener.AbstractMessageListenerContainer.doExecuteListener(AbstractMessageListenerContainer.java:435)
	at org.springframework.jms.listener.AbstractPollingMessageListenerContainer.doReceiveAndExecute(AbstractPollingMessageListenerContainer.java:322)
	at org.springframework.jms.listener.AbstractPollingMessageListenerContainer.receiveAndExecute(AbstractPollingMessageListenerContainer.java:260)
	at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.invokeListener(DefaultMessageListenerContainer.java:944)
	at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.run(DefaultMessageListenerContainer.java:868)
	at java.lang.Thread.run(Thread.java:619)

Found 1 deadlock.

"
1,"spi2davex: Batch fails to create/modify properties with non-ascii characters names. the spi2davex batch implementation fails upon creation/modification of all property types that have their value sent as
separate stringpart or binarypart AND contain non-ascii characters in their property name.

from what i've seen this is due to a limitation in HttpClient 3.x Part#sendDispositionHeader that always writes the part name
as ascii-bytes. in a related discussion [1] specification compliance and usability were addressed.

looking at the server-side part revealed that org.apache.commons.fileupload.FileUploadBase#FileItemIteratorImpl
is prepared to receive non-ascii characters in a header value.
a simple test also showed that curl is perfectly able to send utf-8 part names.

based on this information and given the fact that spi2dav and the server-sided part are intended to communicate
with one other rather than with any kind of custom clients, i suggest to add a simple fix by patching the parts used
within spi2davex.

btw: in HttpClient 4.x there seems to be a workaround for this problem [2]

[1] http://www.mail-archive.com/httpclient-dev@jakarta.apache.org/msg04637.html
[2] https://issues.apache.org/jira/browse/HTTPCLIENT-293"
1,"SetCookie / DateParser failing to parse non-standard date format. I'm receiving the following expiration date in SetCookie which DateParser 
doesn't handle:

expires=Sat,19-Apr-03 04:28:07 GMT

The lack of a space between ',' and '19' is causing the problem. Is it possible 
to add the following lines to DatePattern?

""EEE,dd-MMM-yy HH:mm:ss z""
""EEE,dd-MMM-yyyy HH:mm:ss z"""
1,Add delete term and query need to more precisely record the bytes used. DocumentsWriter's add delete query and add delete term add to the number of bytes used regardless of the query or term already existing in the respective map.
1,"PROPPATCH doesn't respect document order. PROPPATCH is currently implemented in terms of DavResource.alterProperties(...), which takes a set of properties to be set and a set of properties to be removed. This is not sufficient to model WebDAV's method semantics, as the order in which set/remove instructions appear is supposed to be relevant.

I have submitted a patch to the Litmus mailing list checking this (see <http://mailman.webdav.org/pipermail/litmus/2006-April/000196.html>).

In jcr-server, alterProperties probably should be changed to take an (ordered) list of set/remove instructions instead. The simplest approach for that would probably be to use a List containing either DavProperty (set) or DavPropertyName (remove) objects.
"
1,Auth state is not correctly maintained if a successful NTLM authentication results in a redirect. HttpClient fails to update the auth state correctly if a successful NTLM authentication results in a redirect response. Reported by Valentin Popov <valentin.po at gmail.com>
1,"ISO8601 uses default DecimalFormat constructor using locale specific digits. ISO8601.java uses the default DecimalFormat constructor which uses locale specific DecimalFormatSymbols. Runnning Jackrabbit in an Indian locale the format() produces a date using DEVANAGARI numeric digits. The saved version (UTF-8) encoded is much longer than usual and is not transportable. On parsing, DecimalFormat works, but TimeZone.getTimeZone(""GMT+09:30"") (with Indian numeric digits) fails and null is returned from ISO8601. Later this traceback occurs.

2010-02-22 15:14:04,059[http-0.0.0.0-8080-16] ERROR org.apache.jackrabbit.core.persistence.bundle.BundleFsPersistenceManager - failed to write bundle: ff629488-ebb9-4300-a63b-341553cc1140
java.lang.IllegalArgumentException: argument can not be null
	at org.apache.jackrabbit.util.ISO8601.format(ISO8601.java:217)
	at org.apache.jackrabbit.core.value.InternalValue.toString(InternalValue.java:531)
	at org.apache.jackrabbit.core.persistence.bundle.util.BundleBinding.writeState(BundleBinding.java:689)
	at org.apache.jackrabbit.core.persistence.bundle.util.BundleBinding.writeBundle(BundleBinding.java:273)
	at org.apache.jackrabbit.core.persistence.bundle.BundleFsPersistenceManager.storeBundle(BundleFsPersistenceManager.java:664)
	at org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.putBundle(AbstractBundlePersistenceManager.java:703)
	at org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.store(AbstractBundlePersistenceManager.java:643)


ISO8601 probably meant the chars to be ASCII, and so the constructor with a fixed locale is more appropriate (and this doesn't encounter the TimeZone issue either).

    private static final DecimalFormat XX_FORMAT = new DecimalFormat(""00"", new DecimalFormatSymbols(Locale.US));
    private static final DecimalFormat XXX_FORMAT = new DecimalFormat(""000"", new DecimalFormatSymbols(Locale.US));
    private static final DecimalFormat XXXX_FORMAT = new DecimalFormat(""0000"", new DecimalFormatSymbols(Locale.US));
 "
1,"Request with DIGEST authentication fails when redirected. Request with DIGEST authentication fails when redirected due to invalid URI
parameter.

-- Client side log ----------------------------------------------------------

[DEBUG] HttpClient - -Java version: 1.2.2
[DEBUG] HttpClient - -Java vendor: Sun Microsystems Inc.
[DEBUG] HttpClient - -Operating system name: Linux
[DEBUG] HttpClient - -Operating system architecture: i386
[DEBUG] HttpClient - -Operating system version: 2.4.20-13.9-ok
[DEBUG] HttpClient - -SUN 1.2: SUN (DSA key/parameter generation; DSA signing;
SHA-1, MD5 digests; SecureRandom; X.509 certificates; JKS keystore)
[DEBUG] HttpClient - -SunJSSE 1.0301: Sun JSSE provider(implements RSA
Signatures, PKCS12, SunX509 key/trust factories, SSLv3, TLSv1)
[DEBUG] HttpConnection - -Creating connection for localhost using protocol http:80
[DEBUG] HttpConnection - -HttpConnection.setSoTimeout(0)
[DEBUG] HttpMethod - -Execute loop try 1
[DEBUG] wire - ->> ""GET /transfer HTTP/1.1[\r][\n]""
[DEBUG] HttpMethod - -Adding Host request header
[DEBUG] wire - ->> ""User-Agent: Jakarta Commons-HttpClient/2.0beta1[\r][\n]""
[DEBUG] wire - ->> ""Host: localhost[\r][\n]""
[DEBUG] wire - ->> ""[\r][\n]""
[DEBUG] wire - -<< ""HTTP/1.1 401 Authorization Required[\r][\n]""
[DEBUG] wire - -<< ""Date: Fri, 20 Jun 2003 08:30:06 GMT[\r][\n]""
[DEBUG] wire - -<< ""Server: Apache/2.0.40 (Red Hat Linux)[\r][\n]""
[DEBUG] wire - -<< ""WWW-Authenticate: Digest realm=""guest realm"",
nonce=""ei+T7oPAAwA=53c8e6d609ff81a8dcbc370b51f8aadec565009a"", algorithm=MD5,
domain=""/transfer"", qop=""auth""[\r][\n]""
[DEBUG] wire - -<< ""Vary: accept-language[\r][\n]""
[DEBUG] wire - -<< ""Accept-Ranges: bytes[\r][\n]""
[DEBUG] wire - -<< ""Content-Length: 1285[\r][\n]""
[DEBUG] wire - -<< ""Content-Type: text/html; charset=ISO-8859-1[\r][\n]""
[DEBUG] HttpMethod - -Authorization required
[DEBUG] HttpAuthenticator - -Using 'guest realm' authentication realm
[DEBUG] HttpMethod - -HttpMethodBase.execute(): Server demanded authentication
credentials, will try again.
...
[DEBUG] HttpMethod - -Resorting to protocol version default close connection policy
[DEBUG] HttpMethod - -Should NOT close connection, using HTTP/1.1.
[DEBUG] HttpMethod - -Execute loop try 2
[DEBUG] wire - ->> ""GET /transfer HTTP/1.1[\r][\n]""
[DEBUG] HttpMethod - -Request to add Host header ignored: header already added
[DEBUG] wire - ->> ""User-Agent: Jakarta Commons-HttpClient/2.0beta1[\r][\n]""
[DEBUG] wire - ->> ""Host: localhost[\r][\n]""
[DEBUG] wire - ->> ""Authorization: Digest username=""guest"", realm=""guest realm"",
nonce=""ei+T7oPAAwA=53c8e6d609ff81a8dcbc370b51f8aadec565009a"", uri=""/transfer"",
qop=""auth"", algorithm=""MD5"", nc=00000001,
cnonce=""81d4b905a4e9def944beaed8daf79283"",
response=""71394edcddf4bcee6237ea4bb50cfaa5""[\r][\n]""
[DEBUG] wire - ->> ""[\r][\n]""
[DEBUG] wire - -<< ""HTTP/1.1 301 Moved Permanently[\r][\n]""
[DEBUG] wire - -<< ""Date: Fri, 20 Jun 2003 08:30:06 GMT[\r][\n]""
[DEBUG] wire - -<< ""Server: Apache/2.0.40 (Red Hat Linux)[\r][\n]""
[DEBUG] wire - -<< ""Location: http://localhost/transfer/[\r][\n]""
[DEBUG] wire - -<< ""Content-Length: 302[\r][\n]""
[DEBUG] wire - -<< ""Content-Type: text/html; charset=iso-8859-1[\r][\n]""
[DEBUG] HttpMethod - -Redirect required
[DEBUG] HttpMethod - -Redirect requested to location 'http://localhost/transfer/'
[DEBUG] HttpMethod - -Redirecting from 'http://localhost:80/transfer' to
'http://localhost/transfer/
...
[DEBUG] HttpMethod - -Resorting to protocol version default close connection policy
[DEBUG] HttpMethod - -Should NOT close connection, using HTTP/1.1.
[DEBUG] HttpMethod - -Execute loop try 3
[DEBUG] wire - ->> ""GET /transfer/ HTTP/1.1[\r][\n]""
[DEBUG] HttpMethod - -Request to add Host header ignored: header already added
[DEBUG] wire - ->> ""User-Agent: Jakarta Commons-HttpClient/2.0beta1[\r][\n]""
[DEBUG] wire - ->> ""Host: localhost[\r][\n]""
[DEBUG] wire - ->> ""Authorization: Digest username=""guest"", realm=""guest realm"",
nonce=""ei+T7oPAAwA=53c8e6d609ff81a8dcbc370b51f8aadec565009a"", uri=""/transfer"",
qop=""auth"", algorithm=""MD5"", nc=00000001,
cnonce=""81d4b905a4e9def944beaed8daf79283"",
response=""71394edcddf4bcee6237ea4bb50cfaa5""[\r][\n]""
[DEBUG] wire - ->> ""[\r][\n]""
[DEBUG] wire - -<< ""HTTP/1.1 400 Bad Request[\r][\n]""
[DEBUG] wire - -<< ""Date: Fri, 20 Jun 2003 08:30:06 GMT[\r][\n]""
[DEBUG] wire - -<< ""Server: Apache/2.0.40 (Red Hat Linux)[\r][\n]""
[DEBUG] wire - -<< ""Vary: accept-language[\r][\n]""
[DEBUG] wire - -<< ""Accept-Ranges: bytes[\r][\n]""
[DEBUG] wire - -<< ""Content-Length: 973[\r][\n]""
[DEBUG] wire - -<< ""Connection: close[\r][\n]""
[DEBUG] wire - -<< ""Content-Type: text/html; charset=ISO-8859-1[\r][\n]""

-- End of client side log -----------------------------------------------------


-- Server side log ------------------------------------------------------------

[Fri Jun 20 10:30:06 2003] [error] [client 127.0.0.1] Digest: uri mismatch -
</transfer> does not match request-uri </transfer/>

-- End of server side log -----------------------------------------------------"
1,Fix exception handling and thread safety in realtime branch. Several tests are currently failing in the realtime branch - most of them due to thread safety problems (often exceptions in ConcurrentMergeScheduler) and in tests that test for aborting and non-aborting exceptions.
1,"MatchAllScorer calculateDocFilter() bug. In MatchAllScorer.calculateDocFilter(), When you have just two nodes, with different properties, like ""myprop"" and ""myprop2"", and you have an xpath String xpath = ""//*[@myprop], you get both nodes back (to be precise, you'll get every node that has a property that startswith ""myprop"")


You can reproduce it by changing the SimpleQueryTest.testIsNotNull() a little:

Change 

bar.setProperty(""text"", ""the quick brown fox jumps over the lazy dog.""); 

to

bar.setProperty(""mytextwhichstartswithmytext"", ""the quick brown fox jumps over the lazy dog."");

Now the test with xpath = ""//*[@jcr:primaryType='nt:unstructured' and @mytext]""; fails because 2 results. I did test for the trunk and tag 1.3.1 and both have the same problem. I have attached MatchAllScorer.java.patch in this mail, or should I create a JIRA issue for this? 

Furthermore I would like to discuss a different implementation for the MatchAllScorer, because IMHO the current calculateDocFilter() becomes slow pretty fast (see bottom email the code part i am referring to: if you have 100.000 docs with ""mytext"" property, and you query  [@mytext] the loop below is executed at least 100.000 times). I think it might be out of scope for the user-list, or is the user-list the place to discuss something like this? 

-----------------------------------------------------------------------

TermEnum terms = reader.terms(new Term(FieldNames.PROPERTIES, field));
        try {
            TermDocs docs = reader.termDocs();
            try {
                while (terms.term() != null
                        && terms.term().field() == FieldNames.PROPERTIES
                        && terms.term().text().startsWith(field)) {
                    docs.seek(terms);
                    while (docs.next()) {
                        docFilter.set(docs.doc());
                    }
                    terms.next();
                }
            } finally {
                docs.close();
            }
        } finally {
            terms.close();
        }

-----------------------------------------------------------------------

"
1,"Benchmark package uses new TopFieldCollector but also still uses AUTO without resolving it - result is, our sort algorithms won't run. AUTO does not work with TopFieldCollector. If you want to use AUTO with TopFieldCollector, we have a convienence method called detectType on SortField, but it is package protected and so cannot be used here as a stop gap or by users if they wanted to mix AUTO with TopFieldCollector. Lucene does still handle this for back compat internally. Solr got bit here when it was switched to use TopFieldCollector - no auto resolution was added (detectType help couldn't have been used due to visibility), and the result was that plugin code that used to be able to use AUTO would now blow up. You shouldn't use AUTO in Solr anyway though.

The Benchmark package got bit as well  when it moved to TopFieldCollector. Sort algorithms allowed auto if you specified it, or if you left off the type. Now our sort algs fail because they didn't specify a type.

I'll change to require the type to be specified to get the algs working again. I was thinking of just putting auto resolution in as a stop gap till 3.0 (when auto is removed), but since detectFieldType is package protected and I don't want to repeat it, disallowing auto seems the best way to go."
1,"Several test cases fail when declaring nt:base / nt:hierarchy node types as 'abstract' . JSR283 introduces a new node type attribute 'abstract' and defines nt:base and nt:hierarchyNode as such.
when changing those nodetypes, the following test cases fail:

Failed tests: 
  testDefinedAndLegalType(org.apache.jackrabbit.test.api.nodetype.CanAddChildNodeCallWithNodeTypeTest)
  testResidualAndLegalType(org.apache.jackrabbit.test.api.nodetype.CanAddChildNodeCallWithNodeTypeTest)

Tests in error: 
  testAddNodeConstraintViolationExceptionUndefinedNodeType(org.apache.jackrabbit.test.api.NodeTest)
  testRemoveMandatoryNode(org.apache.jackrabbit.test.api.NodeTest)
  testCloneNodesConstraintViolationException(org.apache.jackrabbit.test.api.WorkspaceCloneTest)
  testCopyNodesConstraintViolationException(org.apache.jackrabbit.test.api.WorkspaceCopyBetweenWorkspacesTest)
  testCopyNodesConstraintViolationException(org.apache.jackrabbit.test.api.WorkspaceCopyTest)
  testMoveNodesConstraintViolationException(org.apache.jackrabbit.test.api.WorkspaceMoveTest)
  testNodeTypeConstraintViolationWorkspaceWithHandler(org.apache.jackrabbit.test.api.SerializationTest)
  testNodeTypeConstraintViolationSessionWithHandler(org.apache.jackrabbit.test.api.SerializationTest)
  testNodeTypeConstraintViolationWorkspace(org.apache.jackrabbit.test.api.SerializationTest)
  testNodeTypeConstraintViolationSession(org.apache.jackrabbit.test.api.SerializationTest)
  testJoinFilterPrimaryType(org.apache.jackrabbit.test.api.query.SQLJoinTest)
  testElementTest(org.apache.jackrabbit.test.api.query.ElementTest)
  testElementTestAnyNode(org.apache.jackrabbit.test.api.query.ElementTest)
  testElementTestAnyNodeNtBase(org.apache.jackrabbit.test.api.query.ElementTest)
  testElementTestAnyNodeSomeNT(org.apache.jackrabbit.test.api.query.ElementTest)
  testElementTestNameTest(org.apache.jackrabbit.test.api.query.ElementTest)
  testElementTestNameTestNtBase(org.apache.jackrabbit.test.api.query.ElementTest)
  testElementTestNameTestSomeNT(org.apache.jackrabbit.test.api.query.ElementTest)
  testElementTestNameTestSomeNTWithSNS(org.apache.jackrabbit.test.api.query.ElementTest)
  testNodeType(org.apache.jackrabbit.test.api.observation.AddEventListenerTest)


here's a stacktrace of a failing test:

javax.jcr.nodetype.ConstraintViolationException: nt:hierarchyNode: is an abstract node type.
        at org.apache.jackrabbit.core.NodeImpl.internalAddChildNode(NodeImpl.java:768)
        at org.apache.jackrabbit.core.NodeImpl.internalAddNode(NodeImpl.java:737)
        at org.apache.jackrabbit.core.NodeImpl.internalAddNode(NodeImpl.java:691)
        at org.apache.jackrabbit.core.NodeImpl.addNode(NodeImpl.java:2147)
        at org.apache.jackrabbit.test.api.SessionTest.testMoveItemExistsException(SessionTest.java:69)


the failing tests are actually a backwards compatibility issue. nt:base and nt:hierarchyNode were
non-abstract in JCR 1.0, i.e. 

     node.addNode(""foo"", ""nt:base"");

was perfectly legal.

however, as of JCR 2.0, above statement fails. all above mentioned tests fail because they 
create nodes of type nt:base or nt:hierarchyNode."
1,"Escape colon in statement of jcr:contains. The colon is a special character in the lucene query parser and allows to prefix query terms with an optional field name. JCR does not specify such a feature, thus a colon in the fulltext statement should be treated as a regular character. "
1,"invalid groupid for tm-extractors in textfilters project. groupid for tm-extractors should be ""org.textmining"" and not ""textmining"".
The dependency with the correct groupid is available on ibiblio"
1,"Inconsistencies if ""everyone"" Group is created by User Management. currently the 'everyone' principal used to define ACEs that apply for all regular users in the repository is hardcoded in the
principal management. this leads to inconsistencies if a group (or user) is created within the user management that has a principal 
name 'everyone'.


"
1,"Garbage data when reading a compressed, text field, lazily. lazy compressed text fields is a case that was neglected during lazy field implementation.  TestCase and patch provided.

"
1,"Index segments are only committed on close. There is a check in AbstractIndex.commit(), which prevents that deleted documents are committed to the index. Up to lucene version 2.0 the index was locked when there were pending changes. Beginning with lucene 2.1 this is not true anymore. See LUCENE-701.

This is a regression of JCR-788, hence it does not occur in a release but only in trunk."
1,TieredMergePolicy expungeDeletes should not enforce maxMergedSegmentMB. 
1,"JackrabbitIndexReader prevents use of DocNumberCache. The JackrabbitIndexReader was introduced in 1.5 by JCR-1363. Unfortunately it does not overwrite the method termDocs(Term), which means the default implementation in IndexReader is used. This bypasses the DocNumberCache built into CachingIndexReader, which is used for UUID terms that look up individual documents."
1,"MultiIndexDocValues pretends it can merge sorted sources. Nightly build hit this failure:

{noformat}
ant test-core -Dtestcase=TestSort -Dtestmethod=testReverseSort -Dtests.seed=791b126576b0cfab:-48895c7243ecc5d0:743c683d1c9f7768 -Dtests.multiplier=3 -Dargs=""-Dfile.encoding=ISO8859-1""

    [junit] Testcase: testReverseSort(org.apache.lucene.search.TestSort):	Caused an ERROR
    [junit] expected:<[CEGIA]> but was:<[ACEGI]>
    [junit] 	at org.apache.lucene.search.TestSort.assertMatches(TestSort.java:1248)
    [junit] 	at org.apache.lucene.search.TestSort.assertMatches(TestSort.java:1216)
    [junit] 	at org.apache.lucene.search.TestSort.testReverseSort(TestSort.java:759)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$3$1.evaluate(LuceneTestCase.java:523)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:149)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:51)
{noformat}

It's happening in the test for reverse-sort of a string field with DocValues, when the test had gotten SlowMultiReaderWrapper.

I committed a fix to the test to avoid testing this case, but we need a better fix to the underlying bug.

MultiIndexDocValues cannot merge sorted sources (I think?), yet somehow it's pretending it can (in the above test, the three subs had BYTES_FIXED_SORTED type, and the TypePromoter happily claims to merge these to BYTES_FIXED_SORTED; I think MultiIndexDocValues should return null for the sorted source in this case?"
1,"URI.getHost() generates IllegalArgumentException. Hi guys,

I don't know if I'm doing something wrong or not but the following code:

   URI uri = new URI(""mailto:eay@cryptsoft.com"", true);
   System.out.println(uri.getHost());

generates the following exception:

java.lang.IllegalArgumentException: Component array of chars may not be null
	at org.apache.commons.httpclient.URI.decode(URI.java:1722)
	at org.apache.commons.httpclient.URI.getHost(URI.java:2780)

Could you help?

Also, I'm sorry I put the report under version ""3.0 Final"" but I couldn't find 
an entry for ""3.0-RC1"" (which I'm using at the moment).

Thanks a lot!

Bisser"
1,"VersionManagerImplRestore internalRestoreFrozen method has identity versus equals bug. In method protected void internalRestoreFrozen(NodeStateEx state,
                                         InternalFrozenNode freeze,
                                         VersionSelector vsel,
                                         Set<InternalVersion> restored,
                                         boolean removeExisting,
                                         boolean copy)
in the VersionManagerImplRestore class line 557 the code performs an == instead of calling the NodeId.equals() method.  We ran into problems with the code that executes below this (trying to restore a folder node throws an ItemExistsException since same sibling not allowed on folder nodes)"
1,"ItemState constructor throws IllegalArgumentException. When running ConcurrentReadWriteTest it may happen that a reading session gets an IllegalArgumentException:

Exception in thread ""Thread-7"" java.lang.IllegalArgumentException: illegal status: 0
	at org.apache.jackrabbit.core.state.ItemState.<init>(ItemState.java:138)
	at org.apache.jackrabbit.core.state.PropertyState.<init>(PropertyState.java:79)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.getPropertyState(LocalItemStateManager.java:121)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.getItemState(LocalItemStateManager.java:152)
	at org.apache.jackrabbit.core.state.XAItemStateManager.getItemState(XAItemStateManager.java:226)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:175)
	at org.apache.jackrabbit.core.ItemManager.createItemInstance(ItemManager.java:495)
	at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:326)
	at org.apache.jackrabbit.core.LazyItemIterator.prefetchNext(LazyItemIterator.java:90)
	at org.apache.jackrabbit.core.LazyItemIterator.<init>(LazyItemIterator.java:75)
	at org.apache.jackrabbit.core.ItemManager.getChildProperties(ItemManager.java:485)
	at org.apache.jackrabbit.core.NodeImpl.getProperties(NodeImpl.java:2481)
	at org.apache.jackrabbit.core.ConcurrentReadWriteTest$1$1.execute(ConcurrentReadWriteTest.java:61)
	at org.apache.jackrabbit.core.AbstractConcurrencyTest$Executor.run(AbstractConcurrencyTest.java:107)
	at java.lang.Thread.run(Thread.java:595)

Status 0 is STATUS_UNDEFINED. I think the following happens: when the reading session retrieves the ItemState from the SharedItemStateManager it is still valid but a short time later the writing session removes the item and changes the status to STATUS_UNDEFINED. Then the reading session tries to create an overlayed ItemState for the LocalItemStateManager using the changed status.

Adding the STATUS_UNDEFINED to the list of 'valid' status in the ItemState constructor seems to solve the issue, but I'm not sure if that's the right way to do it.

Opinions?"
1,"HTMLTextExtractor modifying UTF-8 encoded String. Trying to extract an HTML that is UTF-8 encoded is modifying the UTF-8 special char (like , , ,  etc).

This cause a wrong search, because lucene use this extractor to index content.

See attachments for an example of the problem."
1,"Jcr-Server: Invalid value for HTTP auth header. At present, DAV Explorer won't log in to the JCR WebDav servlet - it doesn't even ask for a username & password.  (Neither the Microsoft WinXP WebDAV & Novell's NetDrive were as fussy and were happy to log in)
Using Ethereal, I compared the traffic for a valid Slide WebDav login compared to a JCR WebDav login.

I've now found and fixed the problem on my local build, and I've now got DAV Explorer to work with JCR Webdav.  Here's a description of the bugfix:

In jackrabbit/contrib/jcr-server/server/src/java/org/apache/jackrabbit/server/AbstractWebdavServlet.java, there is a public static final String DEFAULT_AUTHENTICATE_HEADER.
This is currently set to ""Basic Realm=Jackrabbit Webdav Server"".

This is not a valid string for use in this context as it is in breach of RFC2617 for 2 reasons:
1) ""Realm"" should be ""realm""
2) ""Jackrabbit Webdav Server"" should be in quotes, i.e. ""\""Jackrabbit Webdav Server\""""

According to http://www.ietf.org/rfc/rfc2617.txt, a valid challenge would be:
   WWW-Authenticate: Basic realm=""WallyWorld""
Note that ""realm"" is not capitalised and ""WallyWorld"" has been enclosed in quotes (the ""WWW-Authenticate: "" string is held elsewhere in the Java code and is correct)


In other words, AbstractWebdavServlet.java line 82, which currently reads:
    public static final String DEFAULT_AUTHENTICATE_HEADER = ""Basic Realm=Jackrabbit Webdav Server"";
should be changed to read
    public static final String DEFAULT_AUTHENTICATE_HEADER = ""Basic realm=\""Jackrabbit Webdav Server\"""";

"
1,"Set_property permission not checked when saving a new node. When a new node is saved, the add_node permission is checked, but not the set_property permission on it's properties in ItemImpl.validateTransientItems(). This is already fixed in trunk where the implementation is slightly different."
1,"Versioned node importXML fails. When importing system-view XML previously exported for a repository, any nodes with a version history cannot be reimported. This appears to be due to the version manager attempting to create a new version history for the node, which fails due to a previous history existing for the same UUID. The behavior occurs with ImportUUIDBehavior.IMPORT_UUID_COLLISION_REPLACE_EXISTING and  ImportUUIDBehavior.IMPORT_UUID_COLLISION_REMOVE_EXISTING, with the following stack trace:

javax.jcr.version.VersionException: History already exists for node a892651d-1688-46cd-bb12-14f2f0b3d886
	at org.apache.jackrabbit.core.version.VersionManagerImpl.createVersionHistory(VersionManagerImpl.java:194)
	at org.apache.jackrabbit.core.ItemImpl.initVersionHistories(ItemImpl.java:900)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1313)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:766)

I am using the 1.0-dev version, revision 209290 obtained on 05 Jul, 2005 at 9:18:02 EST. Attached please find my repository configuration and the test code. Thanks!


"
1,"Data Store: DB2 fails to create the table. DB2 throws an exception(1) when creating the table. The correct SQL sentence to create it is:
createTable=CREATE TABLE ${tablePrefix}${table}(ID VARCHAR(255) PRIMARY KEY NOT NULL, LENGTH BIGINT, LAST_MODIFIED BIGINT, DATA BLOB(1000M))
(1): Sorry but I don't have the exception information since I made this change a few weeks ago."
1,"TestParallelTermEnum fails with Sep codec. reproduceable in the 'preflexfixes' branch (since we test all codecs there) with: ant test-core -Dtestcase=TestParallelTermEnum -Dtests.codec=Sep

But I think there are probably more tests like this that have only been run with Standard and we might find more like this.
I don't think this should block LUCENE-2554.

{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestParallelTermEnum
    [junit] Testcase: test1(org.apache.lucene.index.TestParallelTermEnum):      Caused an ERROR
    [junit] read past EOF
    [junit] java.io.IOException: read past EOF
    [junit]     at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:154)
    [junit]     at org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:39)
    [junit]     at org.apache.lucene.store.DataInput.readVInt(DataInput.java:86)
    [junit]     at org.apache.lucene.index.codecs.sep.SingleIntIndexInput$Reader.next(SingleIntIndexInput.java:64)
    [junit]     at org.apache.lucene.index.codecs.sep.SepPostingsReaderImpl$SepDocsEnum.nextDoc(SepPostingsReaderImpl.java:316)
    [junit]     at org.apache.lucene.index.TestParallelTermEnum.test1(TestParallelTermEnum.java:188)
    [junit]     at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:316)
    [junit]
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.009 sec
    [junit]
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: random codec of testcase 'test1' was: Sep
    [junit] ------------- ---------------- ---------------
{noformat}
"
1,"Merging of compressed string Fields may hit NPE. This bug was introduced with LUCENE-1219 (only present on 2.4).

The bug happens when merging compressed string fields, but only if bulk-merging code does not apply because the FieldInfos for the segment being merged are not congruent.  This test shows the bug:

{code}
  public void testMergeCompressedFields() throws IOException {
    File indexDir = new File(System.getProperty(""tempDir""), ""mergecompressedfields"");
    Directory dir = FSDirectory.getDirectory(indexDir);
    try {
      for(int i=0;i<5;i++) {
        // Must make a new writer & doc each time, w/
        // different fields, so bulk merge of stored fields
        // cannot run:
        IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), i==0, IndexWriter.MaxFieldLength.UNLIMITED);
        w.setMergeFactor(5);
        w.setMergeScheduler(new SerialMergeScheduler());
        Document doc = new Document();
        doc.add(new Field(""test1"", ""this is some data that will be compressed this this this"", Field.Store.COMPRESS, Field.Index.NO));
        doc.add(new Field(""test2"", new byte[20], Field.Store.COMPRESS));
        doc.add(new Field(""field"" + i, ""random field"", Field.Store.NO, Field.Index.TOKENIZED));
        w.addDocument(doc);
        w.close();
      }

      byte[] cmp = new byte[20];

      IndexReader r = IndexReader.open(dir);
      for(int i=0;i<5;i++) {
        Document doc = r.document(i);
        assertEquals(""this is some data that will be compressed this this this"", doc.getField(""test1"").stringValue());
        byte[] b = doc.getField(""test2"").binaryValue();
        assertTrue(Arrays.equals(b, cmp));
      }
    } finally {
      dir.close();
      _TestUtil.rmDir(indexDir);
    }
  }
{code}

It's because in FieldsReader, when we load a field ""for merge"" we create a FieldForMerge instance which subsequently does not return the right values for getBinary{Value,Length,Offset}."
1,"Move operation may turn AC caches stale. the EntryCollector instance associated with a given workspace is listening to any modifications made to the access control content
(add, modification and removal of access control lists). however, due to the structure of the ac content (the ac node being attached to the affected nodes) 
caches may become stale if a node is moved that contains ac information somewhere in the subtree.

in order to circumvent that problem the EntryCollector should in addition collect any kind of move operations
and make sure that the caches are updated accordingly."
1,"Session.checkPermission(""/"", ""add_node"") throws PathNotFoundException instead of AccessControlException. When invoking Session.checkPermission(""/"", ""add_node""), a PathNotFoundException is thrown:

Exception in thread ""main"" javax.jcr.PathNotFoundException: no such ancestor path of degree 1
	at org.apache.jackrabbit.spi.commons.name.PathFactoryImpl$PathImpl.getAncestor(PathFactoryImpl.java:443)
	at org.apache.jackrabbit.core.SessionImpl.checkPermission(SessionImpl.java:710)
	at Test.main(Test.java:35)

i assume that getAncestor(1) used to return null in an earlier version.



"
1,"DateUtil#formatDate uses default locale instead of US. Problem reported by Yannick <yannick at meudal.net> on the httpclient-user list

==================================================================
Hello,

This is a bug report.

I'm using Commons HTTPClient (rc2) for generating HTTP requests. I put in 
headers some specific header, like the If-Modified-Since attribute. 
When I generate the date through DateUtil.formatDate method, I get a 
localized date, in french. Example: 
If-Modified-Since: dim., 10 avr. 2005 05:04:08 CEST

I get problems on my http server during parsing the received date. This is 
not a RFC 2616 compliant date format. It should be:
If-Modified-Since: Sun, 10 Apr 2005 05:04:08 CEST

A patch should be applied, by creating a new SimpleDateFormat(pattern, 
Locale.US) instead of SimpleDateFormat(pattern) (like it is done in the 
parse method, line #159).

org.apache.commons.httpclient.util.DateUtil, line #205:

    public static String formatDate(Date date, String pattern) {
        if (date == null) throw new IllegalArgumentException(""date is 
null"");
        if (pattern == null) throw new IllegalArgumentException(""pattern 
is null"");
 
        SimpleDateFormat formatter = new SimpleDateFormat(pattern, 
Locale.US);
        return formatter.format(date);
    }


Regards,

Yannick."
1,"AbstractClientConnAdapter doesn't ensure that only one of ConnectionReleaseTrigger.abortConnection, .releaseConnection has effect. If HttpUriRequest.abort() is called at about the same time that the request completes, it's possible for an aborted connection to be returned to the pool.  The next time the connection is used, HttpClient.execute fails without retrying, throwing this exception:

java.io.IOException: Connection already shutdown
	at org.apache.http.impl.conn.DefaultClientConnection.opening(DefaultClientConnection.java:112)
	at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:120)
	at org.apache.http.impl.conn.AbstractPoolEntry.open(AbstractPoolEntry.java:147)
	at org.apache.http.impl.conn.AbstractPooledConnAdapter.open(AbstractPooledConnAdapter.java:101)
	at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:381)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:641)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:576)

Steps to reproduce:
1) Set a breakpoint in ThreadSafeClientConnManager.releaseConnection just after ""reusable"" is set (and found to be true).
2) Run to the breakpoint in releaseConnection.
3) Call HttpUriRequest.abort.
4) Let releaseConnection complete.

When the connection is next used, the exception will be thrown.

Snippet from ThreadSafeClientConnManager:
    public void releaseConnection(ManagedClientConnection conn, long validDuration, TimeUnit timeUnit) {
		...
            boolean reusable = hca.isMarkedReusable();
            if (log.isDebugEnabled()) {                             // breakpoint here
                if (reusable) {
                    log.debug(""Released connection is reusable."");
                } else {
                    log.debug(""Released connection is not reusable."");
                }
            }
            hca.detach();
            if (entry != null) {
                connectionPool.freeEntry(entry, reusable, validDuration, timeUnit);
            }
        }
    }


I think that AbstractClientConnAdapter should be modified as follows:

1) Add ""released"" flag:

    /** True if the connection has been released. */
    private boolean released;

2) Modify abortConnection:

    public void abortConnection() {
        synchronized(this) {
            if (aborted || released) {
                return;
            }
            aborted = true;
        }
        unmarkReusable(); // this line and all that follow unchanged

3) Modify releaseConnection:

    public void releaseConnection() {
        synchronized(this) {
            if (aborted || released) {
                return;
            }
            released = true;
        }
        if (connManager != null) {
            connManager.releaseConnection(this, duration, TimeUnit.MILLISECONDS);
        }
    }

"
1,"Value#getBinary() and #getStream() return internal representation for type PATH and NAME. just found a path-related spi2dav test failing that passed some time before jackrabbit 2.0 (BatchTest#testSetPathValue).

i had a quick look at it and it seems to me that the reasons is the internal (Path, Name) value representation 
being exposed when calling Value#getBinary(), Value#getStream() and the corresponding shortcuts on Property.

from my understanding of the specification these methods should always return the standard JCR path (or name) representation as it
is exposed by Value#getString() and Property#getString() as it used to be in previous versions.



"
1,"Deadlock inside XASession on Weblogic. In one of our client deployments on WebLogic 9.2 we observed JackRabbit sessions going stale in a load test. This was observed against release 1.6.1 (to which we migrated due to concurrency related issues JCR-2081 and JCR-2237). Same effect with 2.0.0.
 
I could finally reproduce this issue locally. And it seems to boil down to WLS invoking the sequence of <prepare> ... <release> ... <commit> on one XA session from multiple threads, as it seems breaking assumptions of the thread-bound java.util.concurrent-RWLock based DefaultISMLocking class.
Effectively the setActiveXid(..) method on DefaultISMLocking$RWLock fails as the old active XID was not yet cleared. With the result of more and more sessions deadlocking in below's invocation stack.

{code}
""[ACTIVE] ExecuteThread: '27' for queue: 'weblogic.kernel.Default (self-tuning)'"" daemon prio=1 tid=0x33fc3ec0 nid=0x2324 in Object.wait() [0x2156a000..0x2156beb0] at java.lang.Object.wait(Native Method) - waiting on <0x68a54698> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock) at java.lang.Object.wait(Object.java:474) at EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock.acquire(Unknown Source) - locked <0x68a54698> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock) at org.apache.jackrabbit.core.state.DefaultISMLocking$1.<init>(DefaultISMLocking.java:64) at org.apache.jackrabbit.core.state.DefaultISMLocking.acquireWriteLock(DefaultISMLocking.java:61) at org.apache.jackrabbit.core.version.AbstractVersionManager.acquireWriteLock(AbstractVersionManager.java:146) at org.apache.jackrabbit.core.version.XAVersionManager$1.prepare(XAVersionManager.java:562) at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:154) - locked <0x6dc2ad88> (a org.apache.jackrabbit.core.TransactionContext) at org.apache.jackrabbit.core.XASessionImpl.prepare(XASessionImpl.java:331) at org.apache.jackrabbit.jca.TransactionBoundXAResource.prepare(TransactionBoundXAResource.java:68) at weblogic.connector.security.layer.AdapterLayer.prepare(AdapterLayer.java:397) at weblogic.connector.transaction.outbound.XAWrapper.prepare(XAWrapper.java:297) at weblogic.transaction.internal.XAServerResourceInfo.prepare(XAServerResourceInfo.java:1276) at weblogic.transaction.internal.XAServerResourceInfo.prepare(XAServerResourceInfo.java:499) at weblogic.transaction.internal.ServerSCInfo$1.execute(ServerSCInfo.java:335) at weblogic.kernel.Kernel.executeIfIdle(Kernel.java:243) at weblogic.transaction.internal.ServerSCInfo.startPrepare(ServerSCInfo.java:326) at weblogic.transaction.internal.ServerTransactionImpl.localPrepare(ServerTransactionImpl.java:2516) at weblogic.transaction.internal.ServerTransactionImpl.globalPrepare(ServerTransactionImpl.java:2211) at weblogic.transaction.internal.ServerTransactionImpl.internalCommit(ServerTransactionImpl.java:266) at weblogic.transaction.internal.ServerTransactionImpl.commit(ServerTransactionImpl.java:227) at weblogic.transaction.internal.TransactionManagerImpl.commit(TransactionManagerImpl.java:283) at org.springframework.transaction.jta.JtaTransactionManager.doCommit(JtaTransactionManager.java:1028) at org.springframework.transaction.support.AbstractPlatformTransactionManager.processCommit(AbstractPlatformTransactionManager.java:709) at org.springframework.transaction.support.AbstractPlatformTransactionManager.commit(AbstractPlatformTransactionManager.java:678)
{code}"
1,"Version.isSame(Object) not working. Version interface is implemented (on the frontend) by the VersionImpl class (extending NodeWrapper), which delegates to an internal NodeImpl class, which in turn extends ItemImpl.

Say you have :
      Node node = // at Version 1.0
      Version version = // retrieved as 1.0 for the node
      Version baseVersion = node.getBaseVersion()

You now expect
      baseVersion.isSame(version)
even if
      baseVersion != version

This fails, because VersionImpl delegates the isSame call to its delegatee, thus above call becomes
      ((VersionImpl) baseVersion).delegatee.isSame(version)
where this method is implemented by the ItemImpl class from which the delegatee NodeImpl extends.

That latter implementation ItemImpl.isSame() only returns true if the other is an ItemImpl, too. But this is not the case because VersionImpl is a Version, NodeWrapper, Node but not an ItemImpl.

Probably the best solution would be for NodeImpl.isSame() to check whether the otherItem is a NodeWrapper und use ((NodeWrapper) otherItem).delegatee as the otherItem for the delegatee call.

On another track: ItemImpl.isSame() should probably do a fast check whether the otherItem is actually the same instance to prevent type checks..."
1,"CharArraySet behaves inconsistently in add(Object) and contains(Object). CharArraySet's add(Object) method looks like this:
    if (o instanceof char[]) {
      return add((char[])o);
    } else if (o instanceof String) {
      return add((String)o);
    } else if (o instanceof CharSequence) {
      return add((CharSequence)o);
    } else {
      return add(o.toString());
    }
You'll notice that in the case of an Object (for example, Integer), the o.toString() is added. However, its contains(Object) method looks like this:
    if (o instanceof char[]) {
      char[] text = (char[])o;
      return contains(text, 0, text.length);
    } else if (o instanceof CharSequence) {
      return contains((CharSequence)o);
    }
    return false;
In case of contains(Integer), it always returns false. I've added a simple test to TestCharArraySet, which reproduces the problem:
  public void testObjectContains() {
    CharArraySet set = new CharArraySet(10, true);
    Integer val = new Integer(1);
    set.add(val);
    assertTrue(set.contains(val));
    assertTrue(set.contains(new Integer(1)));
  }
Changing contains(Object) to this, solves the problem:
    if (o instanceof char[]) {
      char[] text = (char[])o;
      return contains(text, 0, text.length);
    } 
    return contains(o.toString());

The patch also includes few minor improvements (which were discussed on the mailing list) such as the removal of the following dead code from getHashCode(CharSequence):
      if (false && text instanceof String) {
        code = text.hashCode();
and simplifying add(Object):
    if (o instanceof char[]) {
      return add((char[])o);
    }
    return add(o.toString());
(which also aligns with the equivalent contains() method).

One thing that's still left open is whether we can avoid the calls to Character.toLowerCase calls in all the char[] array methods, by first converting the char[] to lowercase, and then passing it through the equals() and getHashCode() methods. It works for add(), but fails for contains(char[]) since it modifies the input array."
1,TCK: check for wrong repository descriptor. should be versioning instead of locking. ... at least according to the comment.
1,"URIResolverImpl: use of bitwise instead of logical AND operator. URIResolverImpl, line 111: 

                if (path != null & cache.containsItemId(uuidId)) {
"
1,"AbstractHttpClient.determineTarget does not recognize target host correctly. I am trying to execute an HttpGet with the following URI:
""http://www.foo.foo/doSomething.html?url=http://www.bar.bar/doSomethingElse.html""

This leads to UnknownHostException

Going through the internal code, the problem seems to be in the AbstractHttpClient.determineTarget method:
            String ssp = requestURI.getSchemeSpecificPart();
            ssp = ssp.substring(2, ssp.length()); //remove ""//"" prefix
            int end = ssp.indexOf(':') > 0 ? ssp.indexOf(':') :
                    ssp.indexOf('/') > 0 ? ssp.indexOf('/') :
                    ssp.indexOf('?') > 0 ? ssp.indexOf('?') : ssp.length();
            String host = ssp.substring(0, end);

This code sets the target host to ""www.foo.foo/doSomething.html?url=http"" instead of ""www.foo.foo"". This obviously breaks the execution not far down the line... DefaultClientConnectionOperator.resolveHostname throws an UnknownHostException.

FWIW the AbstractHttpClient.determineTarget method actually has access to the request URI object, which correctly states that the host is ""www.foo.foo"".

So why does it try to extract the host from the scheme specific part anyway?

I hope this is useful... and if there is any workaround please let me know, as I'm stuck on this one.

Marco"
1,"VirtualNodeTypeStateProvider creates PropertyState with type != value(s).getType. VirtualNodeTypeStateProvider creates the item states for the in content representation of the node type definitions and in case of jcr:defaultValues hard codes the type of the property state (thus the jcr property) to PropertyType.STRING.

the nt-definition of nt:propertyDefinition however states that jcr:defaultValues doesn't have a required type, thus the type should rather be determined based on the values themselves.

the current behaviour leads situations where

Property.getType != Property.getValues()[0].getType()

which from my point of view is a bug."
1,"MMapDirectory chunking is buggy. MMapDirectory uses chunking with MultiMMapIndexInput.
 
Because Java's ByteBuffer uses an int to address the
values, it's necessary to access a file >
Integer.MAX_VALUE in size using multiple byte buffers.

But i noticed from the clover report the entire MultiMMapIndexInput class is completely untested: no surprise since all tests make tiny indexes.
"
1,Update monitor is not released. When the timer thread in MultiIndex commits the volatile index after some idle time it does not release / reset the updateInProgress flag. This results in queries that hang until another thread writes to the workspace.
1,NearSpansOrdered.getPayload does not return the payload from the minimum match span. 
1,"Custom similarity is ignored when using MultiSearcher. Symptoms:
I am using Searcher.setSimilarity() to provide a custom similarity that turns off tf() factor. However, somewhere along the way the custom similarity is ignored and the DefaultSimilarity is used. I am using MultiSearcher and BooleanQuery.

Problem analysis:
The problem seems to be in MultiSearcher.createWeight(Query) method. It creates an instance of CachedDfSource but does not set the similarity. As the result CachedDfSource provides DefaultSimilarity to queries that use it.

Potential solution:
Adding the following line:
    cacheSim.setSimilarity(getSimilarity());
after creating an instance of CacheDfSource (line 312) seems to fix the problem. However, I don't understand enough of the inner workings of this class to be absolutely sure that this is the right thing to do.

"
1,"NullPointerException when using HttpHead and Request/Response interceptors. When you try to execute a HttpHead object instead of a HttpGet object while using the add request/response interceptors, you get a nullpointerexception.

I can replicate the exception when using the ClientGZipContentCompression example that can be found at the HttpClient examples. But instead of using the HttpGet object I execute a HttpHead object. When I comment the interceptor parts out, I don't get the exception. 

This is the error stack trace I get when executing the code in netbeans:

Exception in thread ""main"" java.lang.NullPointerException
	at testhttphead.ClientGZipContentCompression$2.process(ClientGZipContentCompression.java:74)
	at org.apache.http.protocol.ImmutableHttpProcessor.process(ImmutableHttpProcessor.java:116)
	at org.apache.http.protocol.HttpRequestExecutor.postProcess(HttpRequestExecutor.java:342)
	at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:472)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:820)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:754)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:732)
	at testhttphead.ClientGZipContentCompression.main(ClientGZipContentCompression.java:92)
Java Result: 1

Here is the code that gives me the error:

package testhttphead;

import java.io.IOException;
import java.io.InputStream;
import java.util.zip.GZIPInputStream;
import org.apache.http.*;
import org.apache.http.client.methods.HttpHead;
import org.apache.http.entity.HttpEntityWrapper;
import org.apache.http.impl.client.DefaultHttpClient;
import org.apache.http.protocol.HttpContext;
import org.apache.http.util.EntityUtils;

/**
 * Demonstration of the use of protocol interceptors to transparently modify
 * properties of HTTP messages sent / received by the HTTP client.
 * <p/>
 * In this particular case HTTP client is made capable of transparent content
 * GZIP compression by adding two protocol interceptors: a request interceptor
 * that adds 'Accept-Encoding: gzip' header to all outgoing requests and a
 * response interceptor that automatically expands compressed response entities
 * by wrapping them with a uncompressing decorator class. The use of protocol
 * interceptors makes content compression completely transparent to the consumer
 * of the {@link org.apache.http.client.HttpClient HttpClient} interface.
 */
public class ClientGZipContentCompression {

    public final static void main(String[] args) throws Exception {
        DefaultHttpClient httpclient = new DefaultHttpClient();

        try {
            httpclient.addRequestInterceptor(new HttpRequestInterceptor() {

                public void process(
                        final HttpRequest request,
                        final HttpContext context) throws HttpException, IOException {
                    if (!request.containsHeader(""Accept-Encoding"")) {
                        request.addHeader(""Accept-Encoding"", ""gzip"");
                    }
                }
            });

            httpclient.addResponseInterceptor(new HttpResponseInterceptor() {

                public void process(
                        final HttpResponse response,
                        final HttpContext context) throws HttpException, IOException {
                    HttpEntity entity = response.getEntity();
                    Header ceheader = entity.getContentEncoding();
                    if (ceheader != null) {
                        HeaderElement[] codecs = ceheader.getElements();
                        for (int i = 0; i < codecs.length; i++) {
                            if (codecs[i].getName().equalsIgnoreCase(""gzip"")) {
                                response.setEntity(
                                        new GzipDecompressingEntity(response.getEntity()));
                                return;
                            }
                        }
                    }
                }
            });

            HttpHead httpHead = new HttpHead(""http://www.howest.be"");

            // Execute HTTP request
            System.out.println(""executing request "" + httpHead.getURI());
            HttpResponse response = httpclient.execute(httpHead);

            System.out.println(""----------------------------------------"");
            System.out.println(response.getStatusLine());
            System.out.println(response.getLastHeader(""Content-Encoding""));
            System.out.println(response.getLastHeader(""Content-Length""));
            System.out.println(""----------------------------------------"");

            HttpEntity entity = response.getEntity();

            if (entity != null) {
                String content = EntityUtils.toString(entity);
                System.out.println(content);
                System.out.println(""----------------------------------------"");
                System.out.println(""Uncompressed size: "" + content.length());
            }

        } finally {
            // When HttpClient instance is no longer needed,
            // shut down the connection manager to ensure
            // immediate deallocation of all system resources
            httpclient.getConnectionManager().shutdown();
        }
    }

    static class GzipDecompressingEntity extends HttpEntityWrapper {

        public GzipDecompressingEntity(final HttpEntity entity) {
            super(entity);
        }

        @Override
        public InputStream getContent()
                throws IOException, IllegalStateException {

            // the wrapped entity's getContent() decides about repeatability
            InputStream wrappedin = wrappedEntity.getContent();

            return new GZIPInputStream(wrappedin);
        }

        @Override
        public long getContentLength() {
            // length of ungzipped content is not known
            return -1;
        }
    }
}

With kind regards,

Peter"
1,"Session.impersonate non-functional. Currently SessionImpl.impersonate simply calls Repository.login with the given credentials and the workspace name of the session. If the credentials are incomplete in that the password is missing, the method throws a ""LoginException: Failed to authenticate userID"", which is actually a misleading text, as the reason is not failure to authenticate userID but at best that the session has not enough access rights to impersonate as userID.

For my application, it is crucial, that Session.impersonate is implemented in the sense that this method allows creation of a session with password-less credentials. I accept this method to fail, but it should fail with a correct message."
1,"Jcr-Server: registration of ReportTypes fails. Registration of ReportType(s) using 

ReportType.register(String localName, Namespace namespace, Class reportClass)  [ReportType]

fails due to wrong evaluation of interfaces implemented by the given class object.
"
1,"Node.getReferences(String) and Node.getWeakReferences(String) issues. Node.getReferences(String) always returns empty iterator.

Node.getWeakReferences() & getWeakReferences(String)  cannot handle multi-valaued reference properties"
1,"MultiFieldQueryParser ignores slop parameter. MultiFieldQueryParser.getFieldQuery(String, String, int) calls super.getFieldQuery(String, String), thus obliterating any slop parameter present in the query.

It should probably be changed to call super.getFieldQuery(String, String, int), except doing only that will result in a recursive loop which is a side-effect of what may be a deeper problem in MultiFieldQueryParser -- getFieldQuery(String, String, int) is documented as delegating to getFieldQuery(String, String), yet what it actually does is the exact opposite.  This also causes problems for subclasses which need to override getFieldQuery(String, String) to provide different behaviour.
"
1,"BindVariable not registered in JCR-SQL2 CONTAINS. The following fails with a ""java.lang.IllegalArgumentException: not a valid variable in this query:""

Query query = qm.createQuery(""SELECT * FROM [my:document] AS document WHERE CONTAINS(document.original, $x)"", Query.JCR_SQL2);
query.bindVariable(""x"", vf.createValue(""moo""));

And query.getBindVariableNames() returns an empty array.

The FullTextSearchExpression _is_ however correctly parsed as a BindVariableValueImpl:
((FullTextSearch) ((QueryObjectModelImpl) query).getConstraint()).getFullTextSearchExpression() instanceof BindVariableValue
"
1,"spi2davex: session-scoped lock tokens not included in if-header. detected while running API lock tests.
org.apache.jackrabbit.test.api.lock.DeepLockTest#testParentChildDeepLock failed though it used to work with spi2dav.

fix is simple: SessionInfoImpl.getAllLockTokens must be used to populate the if-header as it is done in spi2dav."
1,"MMapDirectory can't create new index on Windows. When I set the system property to request the use of the mmap directory, and start building a large index, the process dies with an IOException trying to delete a file. Apparently, Lucene isn't closing down the memory map before deleting the file.
"
1,"RepositoryImpl.acquireRepositoryLock() fails to detect that the file lock is already held by the current process. with java 1.4 and 1.5 on a *nix-based platform it is possible to (concurrently) instantiate 
more than one repository instance in the same jvm based on same/identical configurations.

this is a critical issue since it might lead to data corruption.

the issue only exists with java versions prior to 1.6 and *nix-based platforms (only verified
on mac os-x 10.4).

note that the issue does not exist when the file lock is held by another jvm.

 code snippet to reproduce the issue:

            Repository rep1 = new TransientRepository();
            Session s1 = rep1.login(new SimpleCredentials(""johndoe"", """".toCharArray()));
            Repository rep2 = new TransientRepository();
            Session s2 = rep2.login(new SimpleCredentials(""johndoe"", """".toCharArray()));


the root problem is the incorrect behavior of java.nio.channels.FileChannel#tryLock()
which is demonstrated by the following code snippet:

            try {
                FileLock fl1 = new FileOutputStream(""foo"").getChannel().tryLock();
                System.out.println(""1st lock: "" + fl1);
                FileLock fl2 = new FileOutputStream(""foo"").getChannel().tryLock();
                System.out.println(""2nd lock: "" + fl2);
            } catch (Throwable t) {
                t.printStackTrace();
            }

"
1,SimpleText has a bulk enum buffer reuse bug. testBulkPostingsBufferReuse fails with SimpleText codec.
1,"CachingWrapperFilter throws NPE when Filter.getDocIdSet() returns null. Followup for [http://www.lucidimagination.com/search/document/1014ea92f15677bd/filter_getdocidset_returning_null_and_what_this_means_for_cachingwrapperfilter]:

Daniel Noll is seeing an exception like this:

{noformat}
java.lang.NullPointerException
    at org.apache.lucene.search.CachingWrapperFilter.docIdSetToCache(CachingWrapperFilter.java:84)
    at org.apache.lucene.search.CachingWrapperFilter.getDocIdSet(CachingWrapperFilter.java:112)
    at com.nuix.storage.search.LazyConstantScoreQuery$LazyFilterWrapper.getDocIdSet(SourceFile:91)
    at org.apache.lucene.search.ConstantScoreQuery$ConstantScorer.<init>(ConstantScoreQuery.java:116)
    at org.apache.lucene.search.ConstantScoreQuery$ConstantWeight.scorer(ConstantScoreQuery.java:81)
    at org.apache.lucene.search.BooleanQuery$BooleanWeight.scorer(BooleanQuery.java:297)
    at org.apache.lucene.search.BooleanQuery$BooleanWeight.scorer(BooleanQuery.java:297)
    at org.apache.lucene.search.QueryWrapperFilter$2.iterator(QueryWrapperFilter.java:75)
{noformat}

The class of our own is just an intermediary which delays creating the Filter object...

{code}
@Override
public DocIdSet getDocIdSet(IndexReader reader) throws IOException {
            if (delegate == null) {
                delegate = factory.createFilter();
            }
            return delegate.getDocIdSet(reader);
}
{code}

Tracing through the code in CachingWrapperFilter, I can see that this NPE would occur if getDocIdSet() were to return null.

The Javadoc on Filter says that null will be returned if no documents will be accepted by the filter, but it doesn't seem that Lucene itself is handling null return values correctly, so which is correct?  The code or the Javadoc?  Supposing that null really is OK, does this cause any problems with how CachingWrapperFilter is implementing the caching?  I notice it's calling get() and then comparing against null so it wouldn't appear that it can distinguish ""the entry isn't in the cache"" from ""the entry is in the cache but it's null""."
1,"NumericRangeQuery.NumericRangeTermsEnum sometimes seeks backwards. Subclasses of FilteredTermsEnum are ""supposed to"" seek forwards only (this gives better performance, typically).

However, we don't check for this, so I added an assert to do that (while digging into testing the SimpleText codec) and NumericRangeQuery trips the assert!

Other MTQs seem not to trip it.

I think I know what's happening -- say NRQ has term ranges a-c, e-f to seek to, but then while it's .next()'ing through the first range, the first term after c is f.  At this point NRQ sees the range a-c is done, and then tries to seek to term e which is before f.  Maybe NRQ's accept method should detect this case (where you've accidentally .next()'d into or possibly beyond the next one or more seek ranges)?"
1,"JCR2SPI: potential race condition in event listener registration. There's a potential race condition when the first event listener is registered (ObservationManager.addEventListener). The observation manager should only start listening for events after the new SPI event filter has been created.

(Note there's a related problem when an *additional* event listener is getting registered, while a RepositoryService.getEvents call is already in progress).
"
1,"Xpath query parser accepts ""/a | /b"" and treats it as ""/a/b"". The XPath query parser accepts the query

  /a | /b

and parses it into a query tree corresponging the Xpath query

  /a/b

It should be rejected instead."
1,"Error on query initialization - intermittent. About 1 in ten times, I get the error as shown in the stack trace below. This happens when I run test, or when I start the app. The only way to resolve (when testing) seems to be to blow away the repository. 

It always happens at the point the query manager is accessed (triggering the query subsystem to start up). It DOES NOT cause an exception to be thrown back to the caller, I just noticed it in the logs. Basically the queries return NO data at all (and show up as test failures of course). 

In each case when I startup the system/test, if the repository exists I use it, and (for tests) clean it by deleting the root node of the user content, and then starting again, otherwise there is nothing that exciting.

Please let me know if more info is needed.


ERROR 05-03 15:54:39,386 (LazyQueryResultImpl.java:getResults:266)  -Exception while executing query:
java.io.IOException : No such file or directory
    at java.io.UnixFileSystem.createFileExclusively(Native Method)
    at java.io.File.createNewFile(File.java:850)
    at org.apache.jackrabbit.core.query.lucene.FSDirectory$1.obtain( FSDirectory.java:119)
    at org.apache.lucene.store.Lock.obtain(Lock.java:51)
    at org.apache.lucene.store.Lock$With.run(Lock.java:98)
    at org.apache.lucene.index.IndexReader.open(IndexReader.java:141)
    at org.apache.lucene.index.IndexReader.open(IndexReader.java:136)
    at org.apache.jackrabbit.core.query.lucene.AbstractIndex.getReadOnlyIndexReader(AbstractIndex.java:191)
    at org.apache.jackrabbit.core.query.lucene.MultiIndex.getIndexReader (MultiIndex.java:616)
    at org.apache.jackrabbit.core.query.lucene.SearchIndex.executeQuery(SearchIndex.java:384)
    at org.apache.jackrabbit.core.query.lucene.LazyQueryResultImpl.executeQuery(LazyQueryResultImpl.java :204)
    at org.apache.jackrabbit.core.query.lucene.LazyQueryResultImpl.getResults(LazyQueryResultImpl.java:244)
    at org.apache.jackrabbit.core.query.lucene.LazyQueryResultImpl.<init>(LazyQueryResultImpl.java :161)
    at org.apache.jackrabbit.core.query.lucene.QueryImpl.execute(QueryImpl.java:164)
    at org.apache.jackrabbit.core.query.QueryImpl.execute(QueryImpl.java:142)"
1,"CookieSpecBase.domainMatch() leaks cookies to 3rd party domains. The change committed for #32833
<http://issues.apache.org/bugzilla/show_bug.cgi?id=32833> is buggy; it doesn't
match browser behavior and in fact leaks cookies to third party domains. 

To see, try the following:

CookieSpecBase cspec = new CookieSpecBase();
Cookie cookie = new Cookie("".hotmail.com"",""foo"",""bar"",""/"",Integer.MAX_VALUE,false);
cspec.match(""iwanttostealcookiesfromhotmail.com"",80,""/"",false,cookie);

It will return true. Testing in Firefox1.0.4 and IE6 show no such similar
leakage for similar cases. (Indeed, it'd be a headline-making privacy bug if
they were to do this.)

Those browsers do, in my limited testing, behave as desired by the filer of
#32833: a cookie of domain value '.mydomain.com' will be returned to exact host
'mydomain.com' (. However, the fix that was suggested was overbroad.

I suggest instead for CookieSpecBase.domainMatch():

    public boolean domainMatch(final String host, final String domain) {
// BUGGY: matches a '.service.com' cookie to hosts like 'enemyofservice.com'
//        return host.endsWith(domain)
//            || (domain.startsWith(""."") && host.endsWith(domain.substring(1)));
// BETTER: RFC2109, plus matches a '.service.com' cookie to exact host 'service.com'
        return host.equals(domain)
            || (domain.startsWith(""."") 
                    && (host.endsWith(domain)
                            || host.equals(domain.substring(1))));
    }"
1,"ThaiAnalyzer assumes things about your jre. The ThaiAnalyzer/ThaiWordFilter depends on the fact that BreakIterator.getWordInstance(new Locale(""th"")) returns a dictionary-based break iterator that can segment thai phrases into words (it does not use whitespace).

But this is non-standard that the JRE will specialize this locale in this way, its nice, but you can't depend on it.
For example, if you are running on IBM JRE, this analyzer/wordfilter is completely ""broken"" in the sense it won't do what it claims to do.

At the minimum, we need to document this and suggest users look at ICUTokenizer for thai, which always has this breakiterator and is not jre-dependent.

Better, would be to check statically that the thing actually works.
when creating a new ThaiWordFilter we could clone() the BreakIterator, which is often cheaper than making a new one anyway.
we could throw an exception, if its not supported, and add a boolean so the user knows it works.
and we could refer to this boolean with Assert.assume in its tests.
"
1,"Possible index corruption if crashing while replacing segments file. Lucene's indexing is expected to be reasonably tolerant to computer crashes or the indexing process being killed. By reasonably tolerant, I mean that it is ok to lose a few documents (those currently buffered in memory), or have to repeat some work (e.g., a long merge that was in progress) - but it is not ok for the entire index, or large chunks of it, to become irreversebly corrupt.

The fact that Lucene works by repeated merging of several small segments into a new larger segments, solves most of the crash problems, because until the new segment is fully created, the old segments are still there and fully functional. However, one possibility for corruption remains in the segment replacement code:

After a new segment is created, a new segments file is written as a new file ""segments.new"", and then this file is renamed to ""segments"". The problem is that this renaming is done using Directory.renameFile(), and FSDirectory.renameFile is *NOT* atomic: it first deletes the old file, and then renames the new file. A crash between these stages (or perhaps during Java's rename which also isn't guaranteed to be atomic) will potentially leave us without a working ""segments"" file.

I will post here a patch for this bug shortly.

The patch will also include a change to Directory.renameFile()'s Javadoc. It currently claims ""This replacement should be atomic."", which is false in FSDirectory. Instead it should make a weaker claim, for example
   ""This replacement does not have to be atomic, but must at least obey a weaker guarantee: at any time during the replacement, either the ""from"" file is still available, or the ""to"" file is available with either the new or old content.""
(or, we can just drop the guaranteee altogether, like Java's File.renameTo() provides no atomic-ness guarantees)."
1,"In case of SocketTimeoutException and using HttpRequestRetryHandler the execution is always +1. If my request encounter a SocketTimeoutException, the HttpRequestRetryHandler#retryRequest will be called with an executionCount with a value +1."
1,"NPE in RequestProxyAuthentication on Android. Got a NPE backtrace in RequestProxyAuthentication.process(). 

HttpRoute route = conn.getRoute();
        if (route.isTunnelled()) {      <= line 88, NPE here
            return;
        }

There's no null check on the returned route although getRoute() can return null.
I guess it's not supposed to happen.

In the httpclient code, there's a few more calls to getRoute() without a null check on the returned route.


java.lang.NullPointerException
at com.bubblesoft.org.apache.http.client.protocol.RequestProxyAuthentication.process(SourceFile:88)
at com.bubblesoft.org.apache.http.protocol.ImmutableHttpProcessor.process(SourceFile:108)
at com.bubblesoft.org.apache.http.protocol.HttpRequestExecutor.preProcess(SourceFile:174)
at com.bubblesoft.org.apache.http.impl.client.DefaultRequestDirector.execute(SourceFile:457)
at com.bubblesoft.org.apache.http.impl.client.AbstractHttpClient.createHttpProcessor(SourceFile:821)
                                                                 execute
at com.bubblesoft.org.apache.http.impl.client.AbstractHttpClient.createHttpProcessor(SourceFile:755)
                                                                 execute
at com.bubblesoft.org.apache.http.impl.client.AbstractHttpClient.createHttpProcessor(SourceFile:733)
                                                                 execute
"
1,"RangeQuery equals method does not compare collator property fully. The equals method in the range query has the collator comparison implemented as:
(this.collator != null && ! this.collator.equals(other.collator))

When _this.collator = null_ and _other.collator = someCollator_  this method will incorrectly assume they are equal. 

So adding something like
|| (this.collator == null && other.collator != null)
would fix the problem
"
1,"ShingleFilter skips over trie-shingles if outputUnigram is set to false. Spinoff from http://lucene.markmail.org/message/uq4xdjk26yduvnpa

{quote}
I noticed that if I set outputUnigrams to false it gives me the same output for
maxShingleSize=2 and maxShingleSize=3.

please divide divide this this sentence

when i set maxShingleSize to 4 output is:

please divide please divide this sentence divide this this sentence

I was expecting the output as follows with maxShingleSize=3 and
outputUnigrams=false :

please divide this divide this sentence 
{quote}


"
1,"SearchWithSortTask ignores sorting by Doc. During my work in LUCENE-3912, I found the following code:

{code}
if (field.equals(""doc"")) {
    sortField0 = SortField.FIELD_DOC;
} if (field.equals(""score"")) {
    sortField0 = SortField.FIELD_SCORE;
} ...
{code}

This means the setting of SortField.FIELD_DOC is ignored.  While I don't know much about this code, this seems like a valid setting and obviously just a bug."
1,"BQ provides an explanation on a non-match. Plug in seed -6336594106867842617L into TestExplanations then run TestSimpleExplanationsOfNonMatches and you'll hit this:
{noformat}
    [junit] Testsuite: org.apache.lucene.search.TestSimpleExplanationsOfNonMatches
    [junit] Testcase: testBQ2(org.apache.lucene.search.TestSimpleExplanationsOfNonMatches):	FAILED
    [junit] Explanation of [[+yy +w3]] for #0 doesn't indicate non-match: 0.08778467 = (MATCH) product of:
    [junit]   0.17556934 = (MATCH) sum of:
    [junit]     0.17556934 = (MATCH) weight(field:w3 in 0), product of:
    [junit]       0.5165708 = queryWeight(field:w3), product of:
    [junit]         0.7768564 = idf(docFreq=4, maxDocs=4)
    [junit]         0.6649502 = queryNorm
    [junit]       0.33987468 = (MATCH) fieldWeight(field:w3 in 0), product of:
    [junit]         1.0 = tf(termFreq(field:w3)=1)
    [junit]         0.7768564 = idf(docFreq=4, maxDocs=4)
    [junit]         0.4375 = fieldNorm(field=field, doc=0)
    [junit]   0.5 = coord(1/2)
    [junit]  expected:<0.0> but was:<0.08778467>
    [junit] junit.framework.AssertionFailedError: Explanation of [[+yy +w3]] for #0 doesn't indicate non-match: 0.08778467 = (MATCH) product of:
    [junit]   0.17556934 = (MATCH) sum of:
    [junit]     0.17556934 = (MATCH) weight(field:w3 in 0), product of:
    [junit]       0.5165708 = queryWeight(field:w3), product of:
    [junit]         0.7768564 = idf(docFreq=4, maxDocs=4)
    [junit]         0.6649502 = queryNorm
    [junit]       0.33987468 = (MATCH) fieldWeight(field:w3 in 0), product of:
    [junit]         1.0 = tf(termFreq(field:w3)=1)
    [junit]         0.7768564 = idf(docFreq=4, maxDocs=4)
    [junit]         0.4375 = fieldNorm(field=field, doc=0)
    [junit]   0.5 = coord(1/2)
    [junit]  expected:<0.0> but was:<0.08778467>
    [junit] 	at org.apache.lucene.search.CheckHits.checkNoMatchExplanations(CheckHits.java:60)
    [junit] 	at org.apache.lucene.search.TestSimpleExplanationsOfNonMatches.qtest(TestSimpleExplanationsOfNonMatches.java:36)
    [junit] 	at org.apache.lucene.search.TestExplanations.qtest(TestExplanations.java:101)
    [junit] 	at org.apache.lucene.search.TestSimpleExplanations.testBQ2(TestSimpleExplanations.java:235)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:397)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.run(LuceneTestCase.java:389)
{noformat}

The bug is real -- BQ's explain method fails to properly enforce required clauses when the sub-scorer is null.  Thank you random testing!
"
1,"error handling duplicate connection headers. HttpMethodBase.shouldCloseConnection() does not correctly handle the case when
more than one connection header exists.  Reported by Ross Rankin."
1,"Sorting produces duplicates. If you run the code below the exception will be thrown. I believe that it isn't 
correct behaviour (the duplicities, of course), index id of hits should be 
unique as it is without sort.

Lucene versions:
1.4-final
1.4.1
CVS 1.5-rc1-dev


import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.document.Document;
import org.apache.lucene.document.Field;
import org.apache.lucene.index.IndexReader;
import org.apache.lucene.index.IndexWriter;
import org.apache.lucene.queryParser.ParseException;
import org.apache.lucene.queryParser.QueryParser;
import org.apache.lucene.search.Hits;
import org.apache.lucene.search.IndexSearcher;
import org.apache.lucene.search.Query;
import org.apache.lucene.search.Searcher;
import org.apache.lucene.search.Sort;
import org.apache.lucene.search.SortField;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.RAMDirectory;

import java.io.IOException;
import java.util.HashSet;
import java.util.LinkedList;
import java.util.ListIterator;
import java.util.Set;

/**
 * Run this test with Lucene 1.4 final or 1.4.1
 */
public class DuplicityTest
{
    public static void main(String[] args) throws IOException, ParseException
    {
        Directory directory = create_index();

        search_index(directory);
    }

    private static void search_index(Directory directory) throws IOException, 
ParseException
    {
        IndexReader reader = IndexReader.open(directory);
        Searcher searcher = new IndexSearcher(reader);

        Sort sort = new Sort(new SortField(""co"", SortField.INT, false));

        Query q = QueryParser.parse(""sword"", ""text"", new StandardAnalyzer());

        find_duplicity(searcher.search(q), ""no sort"");

        find_duplicity(searcher.search(q, sort), ""using sort"");

        searcher.close();
        reader.close();
    }

    private static void find_duplicity(Hits hits, String message) throws 
IOException
    {
        System.out.println(message + "" hits size: "" + hits.length());

        Set set = new HashSet();
        for (int i = 0; i < hits.length(); i++) {
//            System.out.println(hits.id(i) + "": "" + hits.doc(i).toString());
            Integer id = new Integer(hits.id(i));
            if (!set.contains(id))
                set.add(id);
            else
                throw new RuntimeException(""duplicity found, index id: "" + id);
        }
        System.out.println(""no duplicity found"");
    }

    private static LinkedList words;

    static {
        words = new LinkedList();

        words.add(""word"");
        words.add(""sword"");
        words.add(""dwarf"");
        words.add(""whale"");
        words.add(""male"");
    }

    private static Directory create_index() throws IOException
    {
        Directory directory = new RAMDirectory();

        ListIterator e_words1 = words.listIterator();
        ListIterator e_words2 = words.listIterator(words.size());

        IndexWriter writer = new IndexWriter(directory, new StandardAnalyzer(), 
true);

        int co = 1;

        for (int i = 0; i < 300; i++) {

            if (!e_words1.hasNext()) {
                e_words1 = words.listIterator();
                e_words1.hasNext();
            }
            String word1 = (String)e_words1.next();
            if (!e_words2.hasPrevious()) {
                e_words2 = words.listIterator(words.size());
                e_words2.hasPrevious();
            }
            String word2 = (String)e_words2.previous();

            Document doc = new Document();

            doc.add(Field.Keyword(""co"", String.valueOf(co)));
            doc.add(Field.Text(""text"", word1 + "" "" + word2));
            writer.addDocument(doc);

            if (i % 20 == 0)
                co++;
        }
        writer.optimize();
        System.err.println(""index size: "" + writer.docCount());
        writer.close();

        return directory;
    }
}"
1,"Backwards problems with CharStream and Tokenizers with custom reset(Reader) method. When reviewing the new CharStream code added to Tokenizers, I found a
serious problem with backwards compatibility and other Tokenizers, that do
not override reset(CharStream).

The problem is, that e.g. CharTokenizer only overrides reset(Reader):

{code}
  public void reset(Reader input) throws IOException {
    super.reset(input);
    bufferIndex = 0;
    offset = 0;
    dataLen = 0;
  }
{code}

If you reset such a Tokenizer with another CharStream (not a Reader), this
method will never be called and breaking the whole Tokenizer.

As CharStream extends Reader, I propose to remove this reset(CharStream
method) and simply do an instanceof check to detect if the supplied Reader
is no CharStream and wrap it. We could also remove the extra ctor (because
most Tokenizers have no support for passing CharStreams). If the ctor also
checks with instanceof and warps as needed the code is backwards compatible
and we do not need to add additional ctors in subclasses.

As this instanceof check is always done in CharReader.get() why not remove
ctor(CharStream) and reset(CharStream) completely?

Any thoughts?

I would like to fix this somehow before RC4, I'm, sorry :(
"
1,"BoostingTermQuery.explain() bugs. There are a couple of minor bugs in BoostingTermQuery.explain().

1. The computation of average payload score produces NaN if no payloads were found. It should probably be:
float avgPayloadScore = super.score() * (payloadsSeen > 0 ? (payloadScore / payloadsSeen) : 1);

2. If the average payload score is zero, the value of the explanation is 0:
result.setValue(nonPayloadExpl.getValue() * avgPayloadScore);
If the query is part of a BooleanClause, this results in:
""no match on required clause...""
""failure to meet condition(s) of required/prohibited clause(s)""

The average payload score can be zero if the field boost = 0.

I've attached a patch to 'TestBoostingTermQuery.java', however, the test 'testNoPayload' fails in 'SpanScorer.score()' because the doc = -1. It looks like 'setFreqCurrentDoc() should have been called before 'score()'. Maybe someone more knowledgable of spans could investigate this.
"
1,"Version.merge() corrupts repository. Version.merge() corrupts repository. somehow the 'protected' flags of the nt:version nodetype is not properly checked.
this happens due to a wrong test case that calls merge on a Version instead of a normal Node.
"
1,"UUID field not populated when saving a new node. In the following 'Article' class, there is are fields set to true for 'path' and 'uuid' jcr properties. 
The mixins for referencing (hence, support for UUID) are declared at the @Node level of the class. 

After saving the node with the ObjectContentManager, the uuid field is not populated as it could be expected

@Node(jcrMixinTypes=""mix:referenceable,mix:lockable,mix:versionable"")
public class Article {

        @Field(uuid=true)
        private String id = null;
       
        @Field(path=true)
        private String path = null;
       
        @Field
        private String content = null;

        ... constructor, getters and setters
} 

The full discussion is here : http://www.nabble.com/OCM-issues-with-path-and-id-fields-%28annotations%29-tt15460625.html#a15460625 "
1,"MatchAllDocsQuery doesn't honor boost or queryNorm. MatchAllDocsQuery doesn't pay attention to either it's own boost, or lucene's query normalization factor."
1,"[PATCH] PerFieldAnalyzerWrapper fails to implement getPositionIncrementGap(). The attached patch causes PerFieldAnalyzerWrapper to delegate calls to getPositionIncrementGap() to the analyzer that is appropriate for the field in question.  The current behavior without this patch is to always use the default value from Analyzer, which is a bug because PerFieldAnalyzerWrapper should behave just as if it was the analyzer for the selected field.
"
1,"PredefinedNodeTypeTest..getNodeTypeSpec handling unknown super types. This method tries to filter out custom super types, but produces a broken spec when *all* super types are custom (in which case it should emit ""[]"", but doesn't).
"
1,"QueryUtils should check that equals properly handles null. Its part of the equals contract, but many classes currently violate"
1,"IndexWriter & ConcurrentMergeScheduler deadlock case if starting a merge hits an exception. If you're using CMS (the default) and mergeInit hits an exception (eg
OOME), we are not properly clearing IndexWriter's internal tracking of
running merges.  This causes IW.close() to hang while it incorrectly
waits for these non-started merges to finish.

"
1,"Crash when querying an index using multiple term positions.. file: MultipleTermPositions.java, line: 201, function: skipTo(int).

This refers to the source that can currently be downloaded from the lucene site,
Lucene v. 1.4.3.

The function peek() returns null (because top() also retruned null). There is no
check for this, as far as I can understand. The function doc() is called on a
null-object, which results in a NullPointerException.

I switched the specified line to this one:

while(_termPositionsQueue.peek() != null && target >
_termPositionsQueue.peek().doc())

This got rid of the crash for me."
1,"rep:excerpt() not working for attribute searches. example: //element(*, nt:unstructured)[jcr:contains(@textProp, 'foobar')]/(rep:excerpt())

produces empty excerpts."
1,"Missing support for lock timeout and ownerHint in jcr-server. trying to set the lock timeout when creating a lock seems not to work over the davex transport. the timeout is always 2147483.

this was my test code:

import javax.jcr.*;
import javax.jcr.lock.*;

import org.apache.jackrabbit.jcr2spi.RepositoryImpl;
import org.apache.jackrabbit.jcr2spi.config.RepositoryConfig;


String url = ""http://localhost:8080/server/"";
String workspace = ""tests"";

RepositoryConfig config = new RepositoryConfigImplTest(repoUrl);
Repository repo = RepositoryImpl.create(config);

Credentials sc = new SimpleCredentials(""admin"",""admin"".toCharArray());
Session s = repo.login(sc,workspace);

Node t;
if (s.getRootNode().hasNode(""test"")) {
    t = s.getRootNode().getNode(""test"");
} else {
    t = s.getRootNode().addNode(""test"", ""nt:unstructured"");
}
t.addMixin(""mix:lockable"");
s.save();
LockManager m = s.getWorkspace().getLockManager();
Lock l = m.lock(t.getPath(), false, true, 10, ""me"");
System.out.println(l.getSecondsRemaining());

and the output is 2147483


the relevant communication fragment is below, i attach the full trace in case i miss something.

LOCK /server/tests/jcr%3aroot/test HTTP/1.1
Timeout: Second-10
Depth: 0
Link: <urn:uuid0c740bb9-042a-4ef2-b019-1a6c52784c29>; rel=""http://www.day.com/jcr/webdav/1.0/session-id""
Authorization: Basic YWRtaW46YWRtaW4=
User-Agent: Jakarta Commons-HttpClient/3.0
Host: localhost:8080
Content-Length: 254
Content-Type: text/xml; charset=UTF-8

<?xml version=""1.0"" encoding=""UTF-8"" standalone=""no""?><D:lockinfo xmlns:D=""DAV:""><D:lockscope><dcr:exclusive-session-scoped xmlns:dcr=""http://www.day.com/jcr/webdav/1.0""/></D:lockscope><D:locktype><D:write/></D:locktype><D:owner>me</D:owner></D:lockinfo>

HTTP/1.1 200 OK
Content-Type: text/xml; charset=utf-8
Content-Length: 450
Lock-Token: <aa724c28-3c24-41e8-a3b4-9fc129adf732>
Server: Jetty(6.1.x)

<?xml version=""1.0"" encoding=""UTF-8"" standalone=""no""?><D:prop xmlns:D=""DAV:""><D:lockdiscovery><D:activelock><D:lockscope><dcr:exclusive-session-scoped xmlns:dcr=""http://www.day.com/jcr/webdav/1.0""/></D:lockscope><D:locktype><D:write/></D:locktype><D:depth>0</D:depth><D:timeout>Second-2147483</D:timeout><D:owner>admin</D:owner><D:locktoken><D:href>aa724c28-3c24-41e8-a3b4-9fc129adf732</D:href></D:locktoken></D:activelock></D:lockdiscovery></D:prop>



by the way: if i do not explicitly logout before the program exits, the lock is also not released even though it is session based. should the session not trigger a logout on destruction?"
1,"SimpleText sumTotalTermFreq is wrong if only positions are omitted. ant test -Dtestcase=TestOmitPositions -Dtestmethod=testBasic -Dtests.seed=-6c9bd4a6197b9463:-71d0d11bc2db9a15:697690b3dff2369 -Dargs=""-Dfile.encoding=UTF-8""
    [junit] java.lang.AssertionError: sumTotalTermFreq=0,sumDocFreq=400
    [junit] 	at org.apache.lucene.search.CollectionStatistics.<init>(CollectionStatistics.java:38)

This assert fails because #of positions for the field is < #of postings, which is impossible.

From memory i think SimpleText calculates sumTotalTermFreq ""one the fly"" by reading the positions from its text file...
In this case it should write the stat explicitly."
1,"Tika regressions in 0.8. There are a few notable problems in Tika 0.8, namely TIKA-548 and TIKA-556, that may adversely affect users of Jackrabbit 2.2.

Since we don't have a Tika 0.9 release available yet, I'll add workarounds for these issues in Jackrabbit."
1,"KeywordTokenizer does not properly set the end offset. KeywordTokenizer sets the Token's term length attribute but appears to omit the end offset. The issue was discovered while using a highlighter with the KeywordAnalyzer. KeywordAnalyzer delegates to KeywordTokenizer propagating the bug. 

Below is a JUnit test (source is also attached) that exercises various analyzers via a Highlighter instance. Every analyzer but the KeywordAnazlyzer successfully wraps the text with the highlight tags, such as ""<b>thetext</b>"". When using KeywordAnalyzer the tags appear before the text, for example: ""<b></b>thetext"". 

Please note NewKeywordAnalyzer and NewKeywordTokenizer classes below. When using NewKeywordAnalyzer the tags are properly placed around the text. The NewKeywordTokenizer overrides the next method of the KeywordTokenizer setting the end offset for the returned Token. NewKeywordAnalyzer utilizes KeywordTokenizer to produce proper token.

Unless there is an objection I will gladly post a patch in the very near future . 

-----------------------------
package lucene;

import java.io.IOException;
import java.io.Reader;

import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.analysis.KeywordAnalyzer;
import org.apache.lucene.analysis.KeywordTokenizer;
import org.apache.lucene.analysis.SimpleAnalyzer;
import org.apache.lucene.analysis.StopAnalyzer;
import org.apache.lucene.analysis.Token;
import org.apache.lucene.analysis.TokenStream;
import org.apache.lucene.analysis.Tokenizer;
import org.apache.lucene.analysis.WhitespaceAnalyzer;
import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.search.highlight.Highlighter;
import org.apache.lucene.search.highlight.QueryScorer;
import org.apache.lucene.search.highlight.SimpleHTMLFormatter;
import org.apache.lucene.search.highlight.WeightedTerm;
import org.junit.Test;
import static org.junit.Assert.*;

public class AnalyzerBug {

	@Test
	public void testWithHighlighting() throws IOException {
		String text = ""thetext"";
		WeightedTerm[] terms = { new WeightedTerm(1.0f, text) };

		Highlighter highlighter = new Highlighter(new SimpleHTMLFormatter(
				""<b>"", ""</b>""), new QueryScorer(terms));

		Analyzer[] analazers = { new StandardAnalyzer(), new SimpleAnalyzer(),
				new StopAnalyzer(), new WhitespaceAnalyzer(),
				new NewKeywordAnalyzer(), new KeywordAnalyzer() };

		// Analyzers pass except KeywordAnalyzer
		for (Analyzer analazer : analazers) {
			String highighted = highlighter.getBestFragment(analazer,
					""CONTENT"", text);
			assertEquals(""Failed for "" + analazer.getClass().getName(), ""<b>""
					+ text + ""</b>"", highighted);
			System.out.println(analazer.getClass().getName()
					+ "" passed, value highlighted: "" + highighted);
		}
	}
}

class NewKeywordAnalyzer extends KeywordAnalyzer {

	@Override
	public TokenStream reusableTokenStream(String fieldName, Reader reader)
			throws IOException {
		Tokenizer tokenizer = (Tokenizer) getPreviousTokenStream();
		if (tokenizer == null) {
			tokenizer = new NewKeywordTokenizer(reader);
			setPreviousTokenStream(tokenizer);
		} else
			tokenizer.reset(reader);
		return tokenizer;
	}

	@Override
	public TokenStream tokenStream(String fieldName, Reader reader) {
		return new NewKeywordTokenizer(reader);
	}
}

class NewKeywordTokenizer extends KeywordTokenizer {
	public NewKeywordTokenizer(Reader input) {
		super(input);
	}

	@Override
	public Token next(Token t) throws IOException {
		Token result = super.next(t);
		if (result != null) {
			result.setEndOffset(result.termLength());
		}
		return result;
	}
}
"
1,NRTCachingDirectory.deleteFile always throws exception. Silly bug.
1,"restore sometime throws error about missing tmp files. Caused by: javax.jcr.RepositoryException: file backing binary value not
found: /server/apache-tomcat-5.5.15/temp/bin4435.tmp (No such file or
directory): /server/apache-tomcat-5.5.15/temp/bin4435.tmp (No such file
or directory)
   at
org.apache.jackrabbit.core.value.BLOBFileValue.getStream(BLOBFileValue.java:454)
   at
org.apache.jackrabbit.core.state.util.Serializer.serialize(Serializer.java:197)"
1,"IndexReader.lastModified - throws NPE. IndexReader.lastModified(String dir) or its variants always return NPE on 2.3, perhaps something to do with SegmentInfo."
1,"cache returns cached responses even if validators not consistent with all conditional headers. This is a MUST-level requirement in the RFC, where if both ETags and Last-Modified dates are used as validators in a conditional request, a cache cannot return a cached response unless it is consistent with all the conditional headers in the request. There is a unit test for this already, but it is incorrect (it uses 'If-Unmodified-Since' instead of 'If-Modified-Since' in the test case).

"
1,"InstantiatedIndexReader.norms called from MultiReader bug. Small bug in InstantiatedIndexReader.norms(String field, byte[] bytes, int offset) where the offset is not applied properly in the System.arraycopy"
1,"Infinite loop on basic authentication. Class org.apache.http.impl.client.DefaultRequestDirector has a bug whereby when Authentication fails if the log is not warnEnabled then you will receive a retry request and end up in an infinite loop retrying requests.. This occurred for me when SL4J was being picked up as the implementation but not properly configured.

In 4.1.2 the line number of the offending code is in the handleResponse method, line 1126, the return null statement requires moving outside of the if statement that checkes if the log is warn enabled."
1,"FieldSortedHitQueue - subsequent String sorts with different locales sort identically. From my own post to the java-user list. I have looked into this further and am sure it's a bug.

---

It seems to me that there's a possible bug in FieldSortedHitQueue, specifically in getCachedComparator(). This is showing up on our 1.4.3 install, but it seems from source code inspection that if it's a bug, it's in 1.9.1 also.

The issue shows up when you need to sort results from a given IndexReader multiple times, using different locales. On line 180 (all line numbers from the 1.9.1 code), we have this:

ScoreDocComparator comparator = lookup (reader, fieldname, type, factory);

Then, if no comparator is found in the cache, a new one is created (line 193) and then stored in the cache (line 202). HOWEVER, both the cache lookup() and store() do NOT take into account locale; if we, on the same index reader, try to do one search sorted by Locale.FRENCH and one by Locale.ITALIAN, the first one will result in a cache miss, a new French comparator will be created, and stored in the cache. Second time through, lookup() finds the cached French comparator -- even though this time, the locale parameter to getCachedComparator() is an Italian locale. Therefore, we don't create a new comparator and we use the wrong one to sort the results.

It looks to me (unless I'm mistaken) that the FieldCacheImpl.Entry class should have an additional property, .locale, to ensure that different locales get different comparators.

---

Patch (well, most of one) to follow immediately."
1,"HttpUrl does not accept unescaped passwords. - Taken from an email from Gustav Munkby posted to the HttpClient dev mailing list -

If I do:

HTTPUrl url = new HTTPUrl(""kurt"", ""nicepass#"", hostname, 80, path);

throws a URIException with message ""port number invalid"".

First of all the message is wrong...

Next attempt was to urlencode the password, which resulted in the above line working, but the 
password was sent url-encoded to the destination, which can hardly be the desired behaviour?"
1,"UserManager throws javax.jcr.query.InvalidQueryException on createUser. The UserManager method createUser(String userID, String password) throws an exception (javax.jcr.query.InvalidQueryException) if the user name contains a '@' character.

Stack trace:
Exception in thread ""main"" javax.jcr.query.InvalidQueryException: Lexical error at line 1, column 76.  Encountered: ""@"" (64), after : """" for statement: for $v in /jcr:root/rep:security/rep:authorizables/rep:groups//element(test@example.com,rep:Group) return $v: Lexical error at line 1, column 76.  Encountered: ""@"" (64), after : """": Lexical error at line 1, column 76.  Encountered: ""@"" (64), after : """"
        at org.apache.jackrabbit.spi.commons.query.xpath.XPathQueryBuilder.<init>(XPathQueryBuilder.java:302)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPathQueryBuilder.createQuery(XPathQueryBuilder.java:331)
        at org.apache.jackrabbit.spi.commons.query.xpath.QueryBuilder.createQueryTree(QueryBuilder.java:39)
        at org.apache.jackrabbit.spi.commons.query.QueryParser.parse(QueryParser.java:57)
        at org.apache.jackrabbit.core.query.lucene.QueryImpl.<init>(QueryImpl.java:91)
        at org.apache.jackrabbit.core.query.lucene.SearchIndex.createExecutableQuery(SearchIndex.java:615)
        at org.apache.jackrabbit.core.query.QueryImpl.init(QueryImpl.java:128)
        at org.apache.jackrabbit.core.SearchManager.createQuery(SearchManager.java:282)
        at org.apache.jackrabbit.core.query.QueryManagerImpl.createQuery(QueryManagerImpl.java:102)
        at org.apache.jackrabbit.core.security.user.IndexNodeResolver.buildQuery(IndexNodeResolver.java:105)
        at org.apache.jackrabbit.core.security.user.IndexNodeResolver.findNode(IndexNodeResolver.java:50)
        at org.apache.jackrabbit.core.security.user.UserManagerImpl.getAuthorizable(UserManagerImpl.java:93)
        at org.apache.jackrabbit.core.security.user.UserManagerImpl.createUser(UserManagerImpl.java:177)
        at org.apache.jackrabbit.core.security.user.UserManagerImpl.createUser(UserManagerImpl.java:158)
        at FirstHop.main(FirstHop.java:20)
Caused by: org.apache.jackrabbit.spi.commons.query.xpath.TokenMgrError: Lexical error at line 1, column 76.  Encountered: ""@"" (64), after : """"
        at org.apache.jackrabbit.spi.commons.query.xpath.XPathTokenManager.getNextToken(XPathTokenManager.java:13263)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.jj_ntk(XPath.java:9187)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ElementTest(XPath.java:8745)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.KindTest(XPath.java:8120)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.NodeTest(XPath.java:5041)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.AbbrevForwardStep(XPath.java:4891)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ForwardStep(XPath.java:4747)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.AxisStep(XPath.java:4692)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.StepExpr(XPath.java:4597)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.RelativePathExpr(XPath.java:4547)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.PathExpr(XPath.java:4396)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ValueExpr(XPath.java:4125)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.UnaryExpr(XPath.java:4032)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.CastExpr(XPath.java:3935)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.CastableExpr(XPath.java:3898)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.TreatExpr(XPath.java:3861)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.InstanceofExpr(XPath.java:3824)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.IntersectExceptExpr(XPath.java:3748)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.UnionExpr(XPath.java:3672)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.MultiplicativeExpr(XPath.java:3586)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.AdditiveExpr(XPath.java:3510)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.RangeExpr(XPath.java:3451)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ComparisonExpr(XPath.java:3353)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.AndExpr(XPath.java:3290)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.OrExpr(XPath.java:3227)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ExprSingle(XPath.java:2214)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ForClause(XPath.java:2337)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.FLWORExpr(XPath.java:2233)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ExprSingle(XPath.java:2133)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.Expr(XPath.java:2094)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.QueryBody(XPath.java:2066)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.MainModule(XPath.java:512)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.Module(XPath.java:387)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.QueryList(XPath.java:151)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.XPath2(XPath.java:118)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPathQueryBuilder.<init>(XPathQueryBuilder.java:295)
        ... 14 more
org.apache.jackrabbit.spi.commons.query.xpath.TokenMgrError: Lexical error at line 1, column 76.  Encountered: ""@"" (64), after : """"
        at org.apache.jackrabbit.spi.commons.query.xpath.XPathTokenManager.getNextToken(XPathTokenManager.java:13263)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.jj_ntk(XPath.java:9187)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ElementTest(XPath.java:8745)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.KindTest(XPath.java:8120)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.NodeTest(XPath.java:5041)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.AbbrevForwardStep(XPath.java:4891)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ForwardStep(XPath.java:4747)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.AxisStep(XPath.java:4692)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.StepExpr(XPath.java:4597)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.RelativePathExpr(XPath.java:4547)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.PathExpr(XPath.java:4396)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ValueExpr(XPath.java:4125)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.UnaryExpr(XPath.java:4032)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.CastExpr(XPath.java:3935)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.CastableExpr(XPath.java:3898)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.TreatExpr(XPath.java:3861)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.InstanceofExpr(XPath.java:3824)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.IntersectExceptExpr(XPath.java:3748)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.UnionExpr(XPath.java:3672)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.MultiplicativeExpr(XPath.java:3586)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.AdditiveExpr(XPath.java:3510)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.RangeExpr(XPath.java:3451)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ComparisonExpr(XPath.java:3353)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.AndExpr(XPath.java:3290)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.OrExpr(XPath.java:3227)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ExprSingle(XPath.java:2214)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ForClause(XPath.java:2337)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.FLWORExpr(XPath.java:2233)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ExprSingle(XPath.java:2133)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.Expr(XPath.java:2094)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.QueryBody(XPath.java:2066)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.MainModule(XPath.java:512)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.Module(XPath.java:387)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.QueryList(XPath.java:151)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.XPath2(XPath.java:118)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPathQueryBuilder.<init>(XPathQueryBuilder.java:295)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPathQueryBuilder.createQuery(XPathQueryBuilder.java:331)
        at org.apache.jackrabbit.spi.commons.query.xpath.QueryBuilder.createQueryTree(QueryBuilder.java:39)
        at org.apache.jackrabbit.spi.commons.query.QueryParser.parse(QueryParser.java:57)
        at org.apache.jackrabbit.core.query.lucene.QueryImpl.<init>(QueryImpl.java:91)
        at org.apache.jackrabbit.core.query.lucene.SearchIndex.createExecutableQuery(SearchIndex.java:615)
        at org.apache.jackrabbit.core.query.QueryImpl.init(QueryImpl.java:128)
        at org.apache.jackrabbit.core.SearchManager.createQuery(SearchManager.java:282)
        at org.apache.jackrabbit.core.query.QueryManagerImpl.createQuery(QueryManagerImpl.java:102)
        at org.apache.jackrabbit.core.security.user.IndexNodeResolver.buildQuery(IndexNodeResolver.java:105)
        at org.apache.jackrabbit.core.security.user.IndexNodeResolver.findNode(IndexNodeResolver.java:50)
        at org.apache.jackrabbit.core.security.user.UserManagerImpl.getAuthorizable(UserManagerImpl.java:93)
        at org.apache.jackrabbit.core.security.user.UserManagerImpl.createUser(UserManagerImpl.java:177)
        at org.apache.jackrabbit.core.security.user.UserManagerImpl.createUser(UserManagerImpl.java:158)
        at FirstHop.main(FirstHop.java:20)

"
1,"Repository lock file is not removed on shutdown. The repository lock file is not removed when Jackrabbit runs on a windows platform:

*ERROR* [Thread-4] RepositoryImpl: Unable to release repository lock (RepositoryImpl.java, line 283)

I assume this problem does not occur on unix based platforms, because they allow to delete a file while another process still uses it."
1,"Crash when adding node to cluster with big journal on PSQL DB. When adding a new node to a cluster with big journal on PSQL database application runs out of memory on the new node and crashes (no exception, application exits with code 137).
It's because with PSQL, when no fetchSize is specified, all the results of query are loaded into memory before being passed to application. Furthermore, specification of fetchSize only works in transactional mode and have no effect if autoCommit is true. (these are configured in ConnectionHelper)"
1,"{XML|Object}PersistenceManager.destroy(*) may fail. The destroy methods of the ObjectPersistenceManager class try to delete their files without checking for their existence. This may result in a FileSystemException being thrown because according to the specification of FileSystem.deleteFile() a FileSystemException is thrown ""if this path does not denote a file or if another error occurs.""

While the Jackrabbit LocalFileSystem implementation silently ignores a request to delete a non-existing file, our internal implementation of the interface throws a FileSystemException in this case, which cause destroy to fail.

I suggest all destroy methods should be extended to first check for the existence of the file to prevent from being thrown.

Note: This not only applies to ObjectPersistenceManager but also to XMLPersistenceManager."
1,"DocValues merging is not associative, leading to different results depending upon how merges execute. recently I cranked up TestDuelingCodecs to actually test docvalues (previously it wasn't testing it at all).

This test is simple, it indexes the same random content with 2 different indexwriters, it just allows them
to use different codecs with different indexwriterconfigs.

then it asserts the indexes are equal.

Sometimes, always on BYTES_FIXED_DEREF type, we end out with one reader that has a zero-filled byte[] for a doc,
but that same document in the other reader has no docvalues at all.
"
1,toString() causes StackOverflowError. further regressions of JCR-2763...
1,"encode/decode. As I mention in my email executing <code>ISO9075.decode(""StringWith$inside"")</code> leads to exception:
java.lang.StringIndexOutOfBoundsException: String index out of range: 1
	at java.lang.String.charAt(String.java:444)
	at java.util.regex.Matcher.appendReplacement(Matcher.java:559)
	at com.day.crx.domino.util.NameEncoderDecoder.decode(NameEncoderDecoder.java:117)
	at integration.query.QueryTest.testQuery(QueryTest.java:49)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:324)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:128)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)

The problem is in Matcher.appendReplacement() method, because it didn't correctly interpret '$' and '\' sign. Both have to be escaped with '\' sign."
1,"TestIndexWriterOnDiskFull.testAddIndexOnDiskFull fails with java.lang.IllegalStateException: CFS has pending open files . {noformat}
 Testsuite: org.apache.lucene.index.TestIndexWriterOnDiskFull
    [junit] Testcase: testAddIndexOnDiskFull(org.apache.lucene.index.TestIndexWriterOnDiskFull):	Caused an ERROR
    [junit] CFS has pending open files
    [junit] java.lang.IllegalStateException: CFS has pending open files
    [junit] 	at org.apache.lucene.store.CompoundFileWriter.close(CompoundFileWriter.java:162)
    [junit] 	at org.apache.lucene.store.CompoundFileDirectory.close(CompoundFileDirectory.java:206)
    [junit] 	at org.apache.lucene.index.IndexWriter.createCompoundFile(IndexWriter.java:4099)
    [junit] 	at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3661)
    [junit] 	at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3260)
    [junit] 	at org.apache.lucene.index.SerialMergeScheduler.merge(SerialMergeScheduler.java:37)
    [junit] 	at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1902)
    [junit] 	at org.apache.lucene.index.IndexWriter.forceMerge(IndexWriter.java:1716)
    [junit] 	at org.apache.lucene.index.IndexWriter.forceMerge(IndexWriter.java:1670)
    [junit] 	at org.apache.lucene.index.TestIndexWriterOnDiskFull.testAddIndexOnDiskFull(TestIndexWriterOnDiskFull.java:304)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$3$1.evaluate(LuceneTestCase.java:529)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:165)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)
    [junit] 
    [junit] 
    [junit] Tests run: 4, Failures: 0, Errors: 1, Time elapsed: 31.96 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterOnDiskFull -Dtestmethod=testAddIndexOnDiskFull -Dtests.seed=-7dd066d256827211:127c018cbf5b0975:20481cd18a7d8b6e -Dtests.multiplier=3 -Dtests.nightly=true -Dargs=""-Dfile.encoding=ISO8859-1""
    [junit] NOTE: test params are: codec=SimpleText, sim=RandomSimilarityProvider(queryNorm=true,coord=false): {field=DFR GB1, id=DFR I(F)L1, content=IB SPL-D3(800.0), f=DFR G2}, locale=de_AT, timezone=America/Cambridge_Bay
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestAssertions, TestSearchForDuplicates, TestMockAnalyzer, TestDocValues, TestPerFieldPostingsFormat, TestDocument, TestAddIndexes, TestConcurrentMergeScheduler, TestCrashCausesCorruptIndex, TestDocCount, TestDocumentsWriterDeleteQueue, TestFieldInfos, TestFilterIndexReader, TestFlex, TestIndexInput, TestIndexWriter, TestIndexWriterMergePolicy, TestIndexWriterMerging, TestIndexWriterNRTIsCurrent, TestIndexWriterOnDiskFull]
    [junit] NOTE: FreeBSD 8.2-RELEASE amd64/Sun Microsystems Inc. 1.6.0 (64-bit)/cpus=16,threads=1,free=39156976,total=180748288
{noformat}"
1,"Workspace.clone() fails the second time, if cloning referenceables. the following testcode fails with the 2nd clone. please note, that if the 'folder' node is not made
referenceable, the test passes (copied an adapted from test in WorkspaceCloneTest).

    public void testCloneNodesTwice() throws RepositoryException {
        // clone referenceable node below non-referenceable node
        String dstAbsPath = node2W2.getPath() + ""/"" + node1.getName();

        Node folder = node1.addNode(""folder"");
        folder.addMixin(""mix:referenceable"");
        node1.save();
        workspaceW2.clone(workspace.getName(), node1.getPath(), dstAbsPath, true);
        workspaceW2.clone(workspace.getName(), node1.getPath(), dstAbsPath, true);

        // there should not be any pending changes after clone
        assertFalse(superuserW2.hasPendingChanges());
    }

"
1,"spi2dav: avoid reusing the same document in repositoryserviceimpl. ... instead each call should create it's own document (credits due to jukka :)
that seems to avoid that odd npe in DomUtil."
1,"Problems with maxMergeDocs parameter. I found two possible problems regarding IndexWriter's maxMergeDocs value. I'm using the following code to test maxMergeDocs:

{code:java} 
  public void testMaxMergeDocs() throws IOException {
    final int maxMergeDocs = 50;
    final int numSegments = 40;
    
    MockRAMDirectory dir = new MockRAMDirectory();
    IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true);      
    writer.setMergePolicy(new LogDocMergePolicy());
    writer.setMaxMergeDocs(maxMergeDocs);

    Document doc = new Document();
    doc.add(new Field(""field"", ""aaa"", Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
    for (int i = 0; i < numSegments * maxMergeDocs; i++) {
      writer.addDocument(doc);
      //writer.flush();      // uncomment to avoid the DocumentsWriter bug
    }
    writer.close();
    
    new SegmentInfos.FindSegmentsFile(dir) {

      protected Object doBody(String segmentFileName) throws CorruptIndexException, IOException {

        SegmentInfos infos = new SegmentInfos();
        infos.read(directory, segmentFileName);
        for (int i = 0; i < infos.size(); i++) {
          assertTrue(infos.info(i).docCount <= maxMergeDocs);
        }
        return null;
      }
    }.run();
  }
{code} 
  
- It seems that DocumentsWriter does not obey the maxMergeDocs parameter. If I don't flush manually, then the index only contains one segment at the end and the test fails.

- If I flush manually after each addDocument() call, then the index contains more segments. But still, there are segments that contain more docs than maxMergeDocs, e. g. 55 vs. 50. The javadoc in IndexWriter says:
{code:java}
   /**
   * Returns the largest number of documents allowed in a
   * single segment.
   *
   * @see #setMaxMergeDocs
   */
  public int getMaxMergeDocs() {
    return getLogDocMergePolicy().getMaxMergeDocs();
  }
{code}"
1,"Term's equals() throws ClassCastException if passed something other than a Term. Term.equals(Object) does a cast to Term without checking if the other object is a Term.

It's unlikely that this would ever crop up but it violates the implied contract of Object.equals()."
1,"Brazilian Analyzer doesn't remove stopwords when uppercase is given. The order of filters matter here, just need to apply lowercase token filter before removing stopwords

	result = new StopFilter( result, stoptable );
		result = new BrazilianStemFilter( result, excltable );
		// Convert to lowercase after stemming!
		result = new LowerCaseFilter( result );

Lowercase must come before BrazilianStemFilter

At the end of day I'll attach a patch, it's straightforward"
1,"jcr:mixinTypes property inconsitent, if addMixin() throws exception. If Node.addMixin() throws an exception, that was not due to validation checks but rather internal, the jcr:mixinTypes still contains the newly added nodetype. a subsequent call to Item.save() will store that property. The effective nodetype is not affected though."
1,"Memory is not freed up when jackrabbit-server war is redeployed in tomcat. This bug was introduced with the new CacheManager feature. See JCR-619.

The CacheManager starts a new background thread which optimizes memory distribution every second accross the various caches. When a jackrabbit repository is shutdown, this background thread is still running and prevents the GC from collecting the classloader when jackrabbit is deployed in a web application.

Steps to reproduce:
1) build jackrabbit and jcr-server from trunk and deploy into a tomcat
2) touch the web.xml file of the jcr-server web app (this will force a redeployment)

After step 2 two things may happen. Either:
- The memory consumption increases because the CacheManager thread is not shutdown
or
- The CacheManager thread dies unexpectedly with a NullPointerException:

Exception in thread ""org.apache.jackrabbit.core.state.CacheManager"" java.lang.NullPointerException
        at org.apache.jackrabbit.core.state.CacheManager.run(CacheManager.java:90)
        at java.lang.Thread.run(Unknown Source)"
1,"getScheme() and getPort() return wrong defaults for HttpsURL. getScheme(), if called on an instance of HttpsURL, wrongly returns http instead
of https. That's because dynamic data binding doesn't work for final static
fields (see DEFAULT_SCHEME)."
1,"XML import using MacOS X WebDAV client does not work. when trying to import a xml file via a webdav mount this does not work.

this is mainly because the client first tries to create a 0-sized file, which fails with the xml importer. after the file is created, it will lock it and put the xml body. a second problem might be the ""dot-underscore"" files mac tries to create. "
1,"No entry created for this pool.. Followup to https://issues.apache.org/jira/browse/HTTPCLIENT-741, as reported by Sam Berlin:

java.lang.IllegalStateException: No entry created for this pool. HttpRoute[{}->http://74.160.66.42:14561]
    at org.apache.http.impl.conn.tsccm.RouteSpecificPool.freeEntry(RouteSpecificPool.java:137)
    at org.apache.http.impl.conn.tsccm.ConnPoolByRoute.freeEntry(ConnPoolByRoute.java:337)
    at org.apache.http.impl.conn.tsccm.ThreadSafeClientConnManager.releaseConnection(ThreadSafeClientConnManager.java:230)
    at org.apache.http.impl.client.DefaultClientRequestDirector.execute(DefaultClientRequestDirector.java:427)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:500)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:455)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:421)
    at com.limegroup.gnutella.http.DefaultHttpExecutor.performRequest(DefaultHttpExecutor.java:97)
    at com.limegroup.gnutella.http.DefaultHttpExecutor.access$000(DefaultHttpExecutor.java:26)
    at com.limegroup.gnutella.http.DefaultHttpExecutor$MultiRequestor.run(DefaultHttpExecutor.java:139)
    at org.limewire.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1006)
    at org.limewire.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:549)
    at java.lang.Thread.run(Thread.java:613)
---

DefaultHttpExecutor$MultiRequestor basically is just a Runnable / Cancellable [exposes a cancel() method] that can be cancelled from any thread. cancel just calls abort() on the current AbortableHttpRequest, but is called on a thread other than the one that's doing the client.execute(request).

The last one is the most common exception, and seems to happen with some regularity. The other two we've only seen once, so may just be a memory quirk (we've seen some crazy bugs, including recursive NPEs while constructing an NPE.)
"
1,"PROPPATCH does not send multistatus after revision 397835. After changes to use alterProperties for PROPPATCH in revision 397835, it returns a status code 200 and doesn't return a multistatus body. Patch below...

Index: C:/jprojects/eclipse/jackrabbit/jcr-server/server/src/java/org/apache/jackrabbit/server/AbstractWebdavServlet.java
===================================================================
--- C:/jprojects/eclipse/jackrabbit/jcr-server/server/src/java/org/apache/jackrabbit/server/AbstractWebdavServlet.java	(revision 398580)
+++ C:/jprojects/eclipse/jackrabbit/jcr-server/server/src/java/org/apache/jackrabbit/server/AbstractWebdavServlet.java	(working copy)
@@ -448,6 +448,7 @@
         MultiStatus ms = new MultiStatus();
         MultiStatusResponse msr = resource.alterProperties(changeList);
         ms.addResponse(msr);
+        response.sendMultiStatus(ms);
     }
 
     /**
"
1,"fix some more locale problems in lucene/solr. set ANT_ARGS=""-Dargs=-Duser.language=tr -Duser.country=TR""
ant clean test

We should make sure this works across all of lucene/solr"
1,"ChangeLog serialization causes cache inconsistencies. The ordering of actions is taken into account when a ChangeLog is built through session manipulations (see, for instance,  ChangeLog.deleted(ItemState state)). When it is serialized in ClusterNode.write(Record record, ChangeLog changeLog, EventStateCollection esc), however, this implicit ordering might be changed. As a consequence,  the deserialization in ClusterNode.consume(Record record) might produce a different ChangeLog with the effect that the local caches get out-of-sync with the persistent state of the repository.

The issue should be reproducable as follows:
- Setup a clustered environment with two Jackrabbit instances, say A and B.
- On instance A add a property ""P"" with value ""x"" to some node and save the session.
- On instance B read property ""P"" -> it will have value ""x"".
- On instance A delete property P and then add it again with value ""y"" and save the session.
- On instance B read property ""P"" -> it will still have value ""x"" after the cluster sync..."
1,"UserManager: concurrent user creation using same intermediate path fails. concurrently creating users using same intermediate path fails with ""node ... has been modified externally"".

the problem is the intermediate path. if it doesn't exist multiple threads try to create it concurrently: 

o.a.jackrabbit.core.security.user.UserManagerImpl, line 1310ff:


            String[] segmts = defaultPath.split(""/"");
            NodeImpl folder = (NodeImpl) session.getRootNode();
            String authRoot = (isGroup) ? groupsPath : usersPath;

            for (String segment : segmts) {
                if (segment.length() < 1) {
                    continue;
                }
                if (folder.hasNode(segment)) {
                    folder = (NodeImpl) folder.getNode(segment);
                    if (Text.isDescendantOrEqual(authRoot, folder.getPath()) &&
                            !folder.isNodeType(NT_REP_AUTHORIZABLE_FOLDER)) {
                        throw new ConstraintViolationException(""Invalid intermediate path. Must be of type rep:AuthorizableFolder."");
                    }
                } else {
                    Node parent = folder;
                    folder = addNode(folder, session.getQName(segment), NT_REP_AUTHORIZABLE_FOLDER);
                }
            }

the attached test case illustrates this issue/"
1,"FileDataStore ignores return code from setLastModified. Garbage collection depends on the file modification date being successfully updated when records are ""touched"" during the mark phase. The result of a silent failure is the catastrophic loss of the file in the sweep phase.

FileDataStore.getRecordIfStored does not, however, check the return code from setLastModified.

I believe I was bitten by this when my dev deployment ran out of disk space. A substantial portion of my datastore was deleted, and the best explanation I can come up with is that the setLastModified calls started (silently) failing, leading to massive overkill in the sweep.

There is also a call to setLastModified in FileDataStore.addRecord which is not strictly correct in the face of GC (i.e. it needs the resolution offset, and also must succeed if the file is writable or risk incorrect collection).

Patch to follow."
1,"nonce-count in digest auth should not be quoted. In 3.0rc3 nonce-count (nc) is enclosed in quote marks. According to rfc2617 this
is wrong, nonce-count shouldn't be enclosed in quote marks.

> 3.2.2 The Authorization Request Header
> 
>    The client is expected to retry the request, passing an Authorization
>    header line, which is defined according to the framework above,
>    utilized as follows.
> 
>        credentials      = ""Digest"" digest-response
>        digest-response  = 1#( username | realm | nonce | digest-uri
>                        | response | [ algorithm ] | [cnonce] |
>                        [opaque] | [message-qop] |
>                            [nonce-count]  | [auth-param] )
> 
>        username         = ""username"" ""="" username-value
>        username-value   = quoted-string
>        digest-uri       = ""uri"" ""="" digest-uri-value
>        digest-uri-value = request-uri   ; As specified by HTTP/1.1
>        message-qop      = ""qop"" ""="" qop-value
>        cnonce           = ""cnonce"" ""="" cnonce-value
>        cnonce-value     = nonce-value
>        nonce-count      = ""nc"" ""="" nc-value
>        nc-value         = 8LHEX
>        response         = ""response"" ""="" request-digest
>        request-digest = <""> 32LHEX <"">
>        LHEX             =  ""0"" | ""1"" | ""2"" | ""3"" |
>                            ""4"" | ""5"" | ""6"" | ""7"" |
>                            ""8"" | ""9"" | ""a"" | ""b"" |
>                            ""c"" | ""d"" | ""e"" | ""f"""
1,o.a.j.spi.commons.nodetype.NodeTypeDefinitionFactory does not set required type. NodeTypeDefinitionFactory does not set required type for property definitions.
1,"ArrayHits does not end properly when skipTo doesn't find document. If skipTo(target) does not find a document that that has a higher value than the target, it falls out of the loop and calls next() possibly returning a previously found document. The patch makes sure that -1 is returned in this case, otherwise confusing results might occur.

Index: src/main/java/org/apache/jackrabbit/core/query/lucene/hits/ArrayHits.java
===================================================================
--- src/main/java/org/apache/jackrabbit/core/query/lucene/hits/ArrayHits.java	(revision 608900)
+++ src/main/java/org/apache/jackrabbit/core/query/lucene/hits/ArrayHits.java	(working copy)
@@ -87,9 +87,9 @@
             int nextDocValue = hits[i];
             if (nextDocValue >= target) {
                 index = i;
-                break;
+                return next();
             }
         }
-        return next();
+        return -1;
     }
 }
"
1,"NodeTypeDefDiff.PropDefDiff.init() constraints change check bugs. Two bugs have been found in NodeTypeDefDiff.PropDefDiff.init() when try to modify property constraints of an already registered node type:

1) according to the java doc it should be possible to remove all constraints from a property, but it is not (marked as a MAJOR change).
 
2) it's allowed (TRIVIAL) to set a constraint to a property that had no constraint at all before, which is wrong, because it could affect the consistency of existing repository content."
1,CLONE -Aggregate include ignored if no primaryType set. If the include element of an aggregate definition does not have a primaryType attribute then the include is never matched.
1,"QueryNodeImpl.containsTag(String) should lowercase the tag key. QueryNodeImpl.containsTag(String key): tag keys are  supposed to be case insensitive, however QueryNodeImpl.containsTag method is considering the case when looking up for tag.

*Bug found by Karsten Fissmer"
1,"FrenchAnalyzer's tokenStream method does not honour the contract of Analyzer. In {{Analyzer}} :
{code}
/** Creates a TokenStream which tokenizes all the text in the provided
    Reader.  Default implementation forwards to tokenStream(Reader) for 
    compatibility with older version.  Override to allow Analyzer to choose 
    strategy based on document and/or field.  Must be able to handle null
    field name for backward compatibility. */
  public abstract TokenStream tokenStream(String fieldName, Reader reader);
{code}


and in {{FrenchAnalyzer}}

{code}
public final TokenStream tokenStream(String fieldName, Reader reader) {

    if (fieldName == null) throw new IllegalArgumentException(""fieldName must not be null"");
    if (reader == null) throw new IllegalArgumentException(""reader must not be null"");
{code}"
1,"Registering NodeType from templates throws exception about invalid decl. node type.. when adding PropertyDefinitionTemplates to NodeTypeTemplates, the internal declaredNodeType field is not set and causes the registration fail with:

org.apache.jackrabbit.api.jsr283.nodetype.InvalidNodeTypeDefinitionException: [{}foo#{}test] invalid declaring node type specified
	at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.registerNodeTypes(NodeTypeManagerImpl.java:695)
	at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.registerNodeType(NodeTypeManagerImpl.java:615)"
1,"FieldBoostMapAttribute in contrib/qp is broken.. While looking for more SuppressWarnings in lucene, i came across two of them in contrib/queryparser.

even worse, i found these revolved around using maps with CharSequence as key.

From the javadocs for CharSequence:

This interface does not refine the general contracts of the equals and hashCode methods. The result of comparing two objects that implement CharSequence is therefore, in general, undefined. Each object may be implemented by a different class, and there is no guarantee that each class will be capable of testing its instances for equality with those of the other. {color:red} It is therefore inappropriate to use arbitrary CharSequence instances as elements in a set or as keys in a map. {color}

"
1,"WebDAV LocatorFactoryImpl$Locator.getHref() constructs root resource URLs incorrectly. cadaver was reporting an error when i tried to open / in my repository's default workspace at <https://localhost:8443/webdav/).

in tracking down the problem, i saw something strange - the multistatus response's href had an extra ""/"" tacked onto the end:

  <D:multistatus xmlns:D=""DAV:"">
    <D:response>
      <D:href>https://localhost:8443/webdav//</D:href>

WebdavServlet (rather, my subclass of it) is mapped as the default servlet of a webapp mounted at /webdav. i've configured the WebdavServlet with a resource path prefix of """" (incidentally, i'm not sure what that's meant to be used for - i see that when the value is not empty, it's appended to the response's href, but i don't know in what circumstance that would be useful).

when i requested a child node such as <https://localhost:8443/webdav/bcm>, the response's href was formed as expected:

  <D:multistatus xmlns:D=""DAV:"">
    <D:response>
      <D:href>https://localhost:8443/webdav/bcm/</D:href>

i found that LocatorFactoryImpl$Locator.getHref() was adding the extra ""/"" since the requested resource was a collection. i patched the method to not add the character when itemPath == ""/"", and cadaver stopped complaining. all is well.

i also patched WebdavServlet to default to an empty resource path prefix if one is not specified as a servlet init parameter.
"
1,"TokenStream.next(Token) reuse 'policy': calling Token.clear() should be responsibility of producer.. Tokenizers which implement the reuse form of the next method:
    next(Token result) 
should reset the postionIncrement of the returned token to 1."
1,"BasicCookieStore treats cookies of the same name from the same host as duplicates, even if they have different paths. The DefaultHttpClient is not handling cookies correctly when a single host returns multiple cookies of the same name but with separate paths.  For example, if a single instance of the client is used to access two different webapps on the same server, it may receive two different JSESSIONID cookies:

Cookie: [version: 0][name: JSESSIONID][value: F832C01D23F501CE5EEB296B602700C1][domain: lglom139.example.com][path: /msa-adrenalina][expiry: null]
Cookie: [version: 0][name: JSESSIONID][value: 0FC660347391B93267168F84F2B520F5][domain: lglom139.example.com][path: /maps][expiry: null]

Because the CookieIdentityComparator class does not test the cookie path when determining equality, each new JSESSIONID received replaces the previous one instead of adding a new cookie to the store.  This results in ""disconnecting"" the client from its sessions on the prior webapps.

I've confirmed that adding a path test to CookieIdentityComparator resolves this problem."
1,"QueryObjectModel does not generate the corresponding SQL2 Query when dealing with spaces in the path. This is the original issue:
----------
I tried to get the childnodes of a node names ""/a b"" using the following code
  QueryManager queryManager=session.getWorkspace().getQueryManager();
  QueryObjectModelFactory qomf=queryManager.getQOMFactory();
  Source source1=qomf.selector(NodeType.NT_BASE, ""selector_0"");
  Column[] columns = new Column[]{qomf.column(""selector_0"", null, null)};
  Constraint constraint2 = qomf.childNode(""selector_0"", ""/a b"");
  QueryObjectModel qom = qomf.createQuery(source1, constraint2 , null, columns);

This is not giving any result when the session is acquired through webdav. But when connected using JNDI it is giving the child nodes. 

The sql statement getting created is 
SELECT selector_0.* FROM [nt:base] AS selector_0 WHERE ISCHILDNODE(selector_0, 
[/a b]).

When using webdav If i give this SQL2 query directly along with quotes around 
the path i.e. ['/a b'] then it is working as expected.
----------

this doesn't have anything to do with webdav. the problem is the QueryObjectModel generates an SQL2 query that is not 100% equivalent, it fails to escape paths that have spaces in them.
this way, in the case of davex remoting, the jr client will use the statement generated instead, which is not escaped, and will fail to return the expected nodes. 

This can be seen easily if we do a System.out.println(qom.getStatement())


"
1,"PathParser accepts illegal paths containing curly brackets. o.a.jackrabbit.spi.commons.conversion.PathParser accepts the following path:

""/public/.{.}/private""

the normalized resulting Path object represents ""/private"" 

that's a potential security risk."
1,"Node's childNodes out of sync after unsuccessful save(). If node.save() failes due to an exception in PersistanceManager.store(ChangeLog) a successive call to node.getNodes() will still contain a reference to the node which failed to be persisted.
This is the case even after a call to refresh(false).
You have to restart Jackrabbit in order to get rid of this reference


UseCase in kind of dummy-code:

node.addNode(""new"", ""nt:unstructured"");
node.save();
=> ItemStateException from persistance

node.refresh(false);

Iterator itr = node.getNodes();
while(itr.hasNext()) {
   child = itr.nextNode();
}
=> Exception: ""Failed to build path to ""new""

"
1,"Possible concurrency bug with Workspace.copy() . Hi,

Enclosed below is a test case that can be used to reproduce a
concurrency bug. This test case uses two con-current threads to
execute Workspace.copy() to copy a node to same destination. The
parent node has set its allowSameNameSiblings to false. According to
the javadoc of Workspace.copy(String srcAbsPath, String destAbsPath) :
""This method copies the node at srcAbsPath to the new location at
destAbsPath. If successful, the change is persisted immediately, there
is no need to call save."".  ""An ItemExistException is thrown if a
property already exists at destAbsPath or a node already exist there,
and same name siblings are not allowed. ""

However in reality this is not the case.  The test case can end up
with two child nodes with same names. Please note, not every run can
reproduce the problem, but generally I can get the problem within 3 to
10 iterations. I also got an InvalidItemStateException once (only
once).  Can someone kindly help to confirm if this is a bug in
Jackrabbit or maybe I am using JackRabbit in a wrong way? The test
case has been tested on Jackrabbit 1.6 branch
(http://svn.apache.org/repos/asf/jackrabbit/tags/1.6.0), Windows
Vista, JDK 1.5.0_14.

The test case is also attached for your convenience.

Thanks,
Jervis Liu

package org.apache.jackrabbit.core;

import org.apache.jackrabbit.test.AbstractJCRTest;
import javax.jcr.ItemExistsException;
import javax.jcr.Node;
import javax.jcr.Session;
import javax.jcr.Value;
import javax.jcr.NodeIterator;
import java.util.Random;
import java.util.ArrayList;
import java.util.Iterator;
import javax.jcr.nodetype.NodeType;

import org.apache.jackrabbit.test.NotExecutableException;
import javax.jcr.RepositoryException;
import javax.jcr.nodetype.NodeTypeManager;


public class ConcurrentCopyTest extends AbstractJCRTest {

    private static final int NUM_ITERATIONS = 40;
    private static final int NUM_SESSIONS = 2;

    String sourcePath;
    String destPath;

    public void testConcurrentCopy() throws Exception {
        for (int n = 0; n < NUM_ITERATIONS; n++) {
            System.out.println(""---Iteration---- "" + n);

            // clean up testRoot first
            if (testRootNode.hasNode(""ConcurrentCopyTestNode"")) {
                Node testNode = testRootNode.getNode(""ConcurrentCopyTestNode"");
                testNode.remove();
                testRootNode.save();
                System.out.println(""---old node removed---"");
            }

            // create a parent node where allowSameNameSiblings is set to false
            Node snsfNode = testRootNode.addNode(""ConcurrentCopyTestNode"",
                    ""nt:folder"");
            testRootNode.save();
            sourcePath = snsfNode.getPath();
            destPath = sourcePath + ""/"" + ""CopiedFromConcurrentCopyTestNode"";
            System.out.println(""---sourcePath-----------------"" + sourcePath);
            System.out.println(""---destPath-----------------"" + destPath);

            // firstly we verify it works with single thread.
            Session rootSession = helper.getSuperuserSession();
            rootSession.getWorkspace().copy(sourcePath, destPath + ""test"");

            // copy again to same destPath, expect an ItemExistsException
            try {
                rootSession.getWorkspace().copy(sourcePath, destPath + ""test"");
                fail(""Node exists below '"" + destPath + ""'. Test should fail."");
            } catch (ItemExistsException e) {
            }

            Thread[] threads = new Thread[NUM_SESSIONS];
            for (int i = 0; i < threads.length; i++) {
                // create new session
                Session session = helper.getSuperuserSession();
                TestSession ts = new TestSession(""s"" + i, session);
                Thread t = new Thread(ts);
                t.setName((NUM_ITERATIONS - n) + ""-s"" + i);
                t.start();
                log.println(""Thread#"" + i + "" started"");
                threads[i] = t;
                // Thread.yield();
                // Thread.sleep(100);
            }
            for (int i = 0; i < threads.length; i++) {
                threads[i].join();
            }

            NodeIterator results = testRootNode.getNode(
                    ""ConcurrentCopyTestNode"").getNodes(
                    ""CopiedFromConcurrentCopyTestNode"");
            while (results.hasNext()) {
                Node node = results.nextNode();
                System.out.println(""--result node- "" + node.getName());
            }

            assertEquals(1, results.getSize());
        }
    }

    // --------------------------------------------------------< inner classes >
    class TestSession implements Runnable {

        Session session;
        String identity;
        Random r;

        TestSession(String identity, Session s) {
            session = s;
            this.identity = identity;
            r = new Random();
        }

        private void randomSleep() {
            long l = r.nextInt(90) + 20;
            try {
                Thread.sleep(l);
            } catch (InterruptedException ie) {
            }
        }

        public void run() {

            log.println(""started."");
            String state = """";
            try {
                this.session.getWorkspace().copy(sourcePath, destPath);
                session.save();
                Node newNode =
testRootNode.getNode(""ConcurrentCopyTestNode/CopiedFromConcurrentCopyTestNode"");
                System.out.println(""--Added node- "" + newNode.getName());

                session.save();
                randomSleep();
            } catch (Exception e) {
                log.println(""Exception while "" + state + "": "" + e.getMessage());
                e.printStackTrace();
            } finally {
                session.logout();
            }

            log.println(""ended."");
        }
    }

}

"
1,"NTLM Authentication No Longer Working In Latest Release. Our application has been working fine using NTLM auth with HttpClient for 3 years.   We were most recently on 4.0.3.    Upon upgrading to 4.1.2, NTLM stopped working.

I tried both the new for 4.1 built-in NTLM and the ""old way"" of using JCIFS: client.getAuthSchemes().register(""ntlm"", new NTLMSchemeFactory()); 

Using wireshark I can see that NTLM auth is not even attempted using 4.1.2.    Rolling back to 4.0.3 immediately resolved this problem."
1,In case of ConnectTimeoutException : HttpRequestRetryHandler is not used.. 
1,"In NRT mode, and CFS enabled, IndexWriter incorrectly ties up disk space. Spinoff of java-user thread titled ""searching while optimize""...

If IndexWriter is in NRT mode (you've called getReader() at least
once), and CFS is enabled, then internally the writer pools readers.
However, after a merge completes, it opens the reader against het
non-CFS segment files, and pools that.  It then builds the CFS file,
as well, thus tying up the storage for that segment twice.

Functionally the bug is harmless (it's only a disk space issue).
Also, when the segment is merged, the disk space is released again
(though the newly merged segment will also be double-tied-up).

Simple workaround is to use non-CFS mode, or, don't use getReader."
1,Using the WeightedTerms option in the Highlighter can cause fragments to be supressed for indexes with deletes. An index with a few documents and many deletes can report a lower total docs than docFreq for a term - total docs will account for deletes while docFreq will not - this causes the idf to be negative and the fragment to score < 0.
1,"Enabling wire logging changes isEof/isStale behavior. If you enable wire logging, DefaultClientConnection wraps the SocketInputBuffer with a LoggingSessionInputBuffer. This hides the EofSensor interface implemented by SocketInputBuffer (but not LoggingSessionInputBuffer), which makes at least AbstractHttpClientConnection.isEof() and isStale() methods behave differently.

(That is, stale connection checks won't really work as intended if wire logging is enabled. Which makes it a bit difficult to debug problems related to stale connections...)

Proposed fix: implement EofSensor interface in LoggingSessionInputBuffer (delegating it to wrapped buffer).
"
1,"EchoHandler:104 possible NPE. Line 104 of EchoHandler is

    bae.setContentType(entity.getContentType());

a few lines previously, entity is checked for null, so it appears that entity can be null."
1,"HostConfiguration socketFactory is ignored. HostConfiguration doesn't use its host.protocol to execute an HttpMethod with an absolute URL.  It should, if the Protocol's scheme is the same as the method's URL scheme.

This bug makes it difficult to integrate a specialized SSL connection algorithm (in a SecureProtocolSocketFactory) with a module implemented on top of HttpClient.  The latter module must not execute methods with absolute URLs.  Of course, this is difficult when one doesn't control that module.  For example, I recently tried to integrate SSL certificate-based client authentication with XFire.  XFire provides a reasonable API for replacing its HttpClient, but one must hack its source code to prevent it from executing methods with absolute URLs.

Protocol.registerProtocol is a possible answer, but it can't support two or more SSL connection algorithms for one HTTPS host and port."
1,"ResidualProperties Converter uses wrong AtomicType Converter on update. When writing back data, the ResidualPropertiesCollectionConverterImpl.internalSetProperties method looks at the type of the Java object
to find the atomic type converter instead of getting the converter according to the collection descriptor.

This may lead to NullPointerExceptions in case the concrete type is an extension (or implementation) of the declared type.

I am currently working on a patch to attache to this bug."
1,"TestUTF32ToUTF8 fails on IBM's JRE. This is because AutomatonTestUtil.RandomAcceptedString is returning an invalid UTF32 int[] -- it has an unpaired surrogate, and IBM's JRE handles this differently than Oracle's."
1,"deadlock in TestIndexWriterExceptions.     [junit] 2012-01-18 18:18:16
    [junit] Full thread dump Java HotSpot(TM) 64-Bit Server VM (19.1-b02 mixed mode):
    [junit] 
    [junit] ""Indexer 3"" prio=10 tid=0x0000000041b9b800 nid=0x6291 waiting for monitor entry [0x00007f7e8868f000]
    [junit]    java.lang.Thread.State: BLOCKED (on object monitor)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushQueue.innerPurge(DocumentsWriterFlushQueue.java:118)
    [junit] 	- waiting to lock <0x00000000e40ff2a8> (a org.apache.lucene.index.DocumentsWriterFlushQueue)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushQueue.tryPurge(DocumentsWriterFlushQueue.java:141)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:439)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:317)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:390)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1534)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1506)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread.run(TestIndexWriterExceptions.java:187)
    [junit] 
    [junit] ""Indexer 2"" prio=10 tid=0x0000000041b9b000 nid=0x6290 waiting on condition [0x00007f7e8838c000]
    [junit]    java.lang.Thread.State: WAITING (parking)
    [junit] 	at sun.misc.Unsafe.park(Native Method)
    [junit] 	- parking to wait for  <0x00000000e4103100> (a org.apache.lucene.index.DocumentsWriterStallControl$Sync)
    [junit] 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    [junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)
    [junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireShared(AbstractQueuedSynchronizer.java:941)
    [junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireShared(AbstractQueuedSynchronizer.java:1261)
    [junit] 	at org.apache.lucene.index.DocumentsWriterStallControl.waitIfStalled(DocumentsWriterStallControl.java:115)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushControl.waitIfStalled(DocumentsWriterFlushControl.java:591)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.preUpdate(DocumentsWriter.java:302)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:362)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1534)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1506)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread.run(TestIndexWriterExceptions.java:187)
    [junit] 
    [junit] ""Indexer 1"" prio=10 tid=0x0000000042500000 nid=0x628f waiting for monitor entry [0x00007f7e8858e000]
    [junit]    java.lang.Thread.State: BLOCKED (on object monitor)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushQueue.addSegment(DocumentsWriterFlushQueue.java:84)
    [junit] 	- waiting to lock <0x00000000e40ff2a8> (a org.apache.lucene.index.DocumentsWriterFlushQueue)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:424)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:317)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:390)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1534)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1506)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread.run(TestIndexWriterExceptions.java:187)
    [junit] 
    [junit] ""Indexer 0"" prio=10 tid=0x0000000041508000 nid=0x628d waiting on condition [0x00007f7e8848d000]
    [junit]    java.lang.Thread.State: WAITING (parking)
    [junit] 	at sun.misc.Unsafe.park(Native Method)
    [junit] 	- parking to wait for  <0x00000000e414b408> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
    [junit] 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    [junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)
    [junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:842)
    [junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1178)
    [junit] 	at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:186)
    [junit] 	at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:262)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushQueue.forcePurge(DocumentsWriterFlushQueue.java:130)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushQueue.addDeletesAndPurge(DocumentsWriterFlushQueue.java:50)
    [junit] 	- locked <0x00000000e40ff2a8> (a org.apache.lucene.index.DocumentsWriterFlushQueue)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.applyAllDeletes(DocumentsWriter.java:179)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:460)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:317)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:390)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1534)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1506)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread.run(TestIndexWriterExceptions.java:187)
    [junit] 
    [junit] ""Low Memory Detector"" daemon prio=10 tid=0x00007f7e84025800 nid=0x6003 runnable [0x0000000000000000]
    [junit]    java.lang.Thread.State: RUNNABLE
    [junit] 
    [junit] ""CompilerThread1"" daemon prio=10 tid=0x00007f7e84022800 nid=0x6002 waiting on condition [0x0000000000000000]
    [junit]    java.lang.Thread.State: RUNNABLE
    [junit] 
    [junit] ""CompilerThread0"" daemon prio=10 tid=0x00007f7e8401f800 nid=0x6001 waiting on condition [0x0000000000000000]
    [junit]    java.lang.Thread.State: RUNNABLE
    [junit] 
    [junit] ""Signal Dispatcher"" daemon prio=10 tid=0x00007f7e8401d800 nid=0x6000 waiting on condition [0x0000000000000000]
    [junit]    java.lang.Thread.State: RUNNABLE
    [junit] 
    [junit] ""Finalizer"" daemon prio=10 tid=0x00007f7e84001000 nid=0x5ffa in Object.wait() [0x00007f7e8961b000]
    [junit]    java.lang.Thread.State: WAITING (on object monitor)
    [junit] 	at java.lang.Object.wait(Native Method)
    [junit] 	- waiting on <0x00000000e2ece380> (a java.lang.ref.ReferenceQueue$Lock)
    [junit] 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:118)
    [junit] 	- locked <0x00000000e2ece380> (a java.lang.ref.ReferenceQueue$Lock)
    [junit] 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:134)
    [junit] 	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:159)
    [junit] 
    [junit] ""Reference Handler"" daemon prio=10 tid=0x000000004101e000 nid=0x5ff9 in Object.wait() [0x00007f7e8971c000]
    [junit]    java.lang.Thread.State: WAITING (on object monitor)
    [junit] 	at java.lang.Object.wait(Native Method)
    [junit] 	- waiting on <0x00000000e2ece318> (a java.lang.ref.Reference$Lock)
    [junit] 	at java.lang.Object.wait(Object.java:485)
    [junit] 	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:116)
    [junit] 	- locked <0x00000000e2ece318> (a java.lang.ref.Reference$Lock)
    [junit] 
    [junit] ""main"" prio=10 tid=0x0000000040fb2000 nid=0x5fe2 in Object.wait() [0x00007f7e8ecc1000]
    [junit]    java.lang.Thread.State: WAITING (on object monitor)
    [junit] 	at java.lang.Object.wait(Native Method)
    [junit] 	- waiting on <0x00000000e4105080> (a org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread)
    [junit] 	at java.lang.Thread.join(Thread.java:1186)
    [junit] 	- locked <0x00000000e4105080> (a org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread)
    [junit] 	at java.lang.Thread.join(Thread.java:1239)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions.testRandomExceptionsThreads(TestIndexWriterExceptions.java:286)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit] 	at java.lang.reflect.Method.invoke(Method.java:597)
    [junit] 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit] 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit] 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit] 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit] 	at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$3$1.evaluate(LuceneTestCase.java:528)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:165)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)
    [junit] 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    [junit] 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    [junit] 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    [junit] 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    [junit] 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    [junit] 	at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:743)
    [junit] 
    [junit] ""VM Thread"" prio=10 tid=0x0000000041017800 nid=0x5fef runnable 
    [junit] 
    [junit] ""GC task thread#0 (ParallelGC)"" prio=10 tid=0x0000000040fc5000 nid=0x5fe3 runnable 
    [junit] 
    [junit] ""GC task thread#1 (ParallelGC)"" prio=10 tid=0x0000000040fc7000 nid=0x5fe4 runnable 
    [junit] 
    [junit] ""GC task thread#2 (ParallelGC)"" prio=10 tid=0x0000000040fc9000 nid=0x5fe5 runnable 
    [junit] 
    [junit] ""GC task thread#3 (ParallelGC)"" prio=10 tid=0x0000000040fca800 nid=0x5fe6 runnable 
    [junit] 
    [junit] ""GC task thread#4 (ParallelGC)"" prio=10 tid=0x0000000040fcc800 nid=0x5fe7 runnable 
    [junit] 
    [junit] ""GC task thread#5 (ParallelGC)"" prio=10 tid=0x0000000040fce800 nid=0x5fe8 runnable 
    [junit] 
    [junit] ""GC task thread#6 (ParallelGC)"" prio=10 tid=0x0000000040fd0000 nid=0x5fe9 runnable 
    [junit] 
    [junit] ""GC task thread#7 (ParallelGC)"" prio=10 tid=0x0000000040fd2000 nid=0x5fea runnable 
    [junit] 
    [junit] ""VM Periodic Task Thread"" prio=10 tid=0x00007f7e84030000 nid=0x6004 waiting on condition 
    [junit] 
    [junit] JNI global references: 1578
    [junit] 
    [junit] 
    [junit] Found one Java-level deadlock:
    [junit] =============================
    [junit] ""Indexer 3"":
    [junit]   waiting to lock monitor 0x0000000041477498 (object 0x00000000e40ff2a8, a org.apache.lucene.index.DocumentsWriterFlushQueue),
    [junit]   which is held by ""Indexer 0""
    [junit] ""Indexer 0"":
    [junit]   waiting for ownable synchronizer 0x00000000e414b408, (a java.util.concurrent.locks.ReentrantLock$NonfairSync),
    [junit]   which is held by ""Indexer 3""
    [junit] 
    [junit] Java stack information for the threads listed above:
    [junit] ===================================================
    [junit] ""Indexer 3"":
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushQueue.innerPurge(DocumentsWriterFlushQueue.java:118)
    [junit] 	- waiting to lock <0x00000000e40ff2a8> (a org.apache.lucene.index.DocumentsWriterFlushQueue)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushQueue.tryPurge(DocumentsWriterFlushQueue.java:141)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:439)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:317)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:390)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1534)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1506)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread.run(TestIndexWriterExceptions.java:187)
    [junit] ""Indexer 0"":
    [junit] 	at sun.misc.Unsafe.park(Native Method)
    [junit] 	- parking to wait for  <0x00000000e414b408> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
    [junit] 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    [junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)
    [junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:842)
    [junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1178)
    [junit] 	at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:186)
    [junit] 	at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:262)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushQueue.forcePurge(DocumentsWriterFlushQueue.java:130)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushQueue.addDeletesAndPurge(DocumentsWriterFlushQueue.java:50)
    [junit] 	- locked <0x00000000e40ff2a8> (a org.apache.lucene.index.DocumentsWriterFlushQueue)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.applyAllDeletes(DocumentsWriter.java:179)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:460)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:317)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:390)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1534)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1506)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread.run(TestIndexWriterExceptions.java:187)
    [junit] 
    [junit] Found 1 deadlock.
    [junit] 
    [junit] Heap
    [junit]  PSYoungGen      total 67136K, used 4647K [0x00000000f5560000, 0x00000000fbc60000, 0x0000000100000000)
    [junit]   eden space 65792K, 5% used [0x00000000f5560000,0x00000000f58a5e10,0x00000000f95a0000)
    [junit]   from space 1344K, 96% used [0x00000000f9740000,0x00000000f98840a0,0x00000000f9890000)
    [junit]   to   space 19840K, 0% used [0x00000000fa900000,0x00000000fa900000,0x00000000fbc60000)
    [junit]  PSOldGen        total 171392K, used 66868K [0x00000000e0000000, 0x00000000ea760000, 0x00000000f5560000)
    [junit]   object space 171392K, 39% used [0x00000000e0000000,0x00000000e414d080,0x00000000ea760000)
    [junit]  PSPermGen       total 21248K, used 14733K [0x00000000dae00000, 0x00000000dc2c0000, 0x00000000e0000000)
    [junit]   object space 21248K, 69% used [0x00000000dae00000,0x00000000dbc635a8,0x00000000dc2c0000)
    [junit] 
"
1,"Exception in DocumentsWriter.ThreadState.init leads to corruption. If an exception is hit in the init method, DocumentsWriter incorrectly
increments numDocsInRAM when in fact the document is not added.

Spinoff of this thread:

  http://markmail.org/message/e76hgkgldxhakuaa

The root cause that led to the exception in init was actually due to
incorrect use of Lucene's APIs (one thread still modifying the
Document while IndexWriter.addDocument is adding it) but still we
should protect against any exceptions coming out of init.

"
1,"DefaultProtectedPropertyImporter masks several fields from parent, causing potential derived classes to not perform correctly. The fields session, resolver, and referenceTracker are duplicated in DefaultProtectedPropertyImporter from DefaultProtectedItemImporter, and thus future derived classes will not function correctly if they attempt to use those fields, as they will be null.

I Plan to remove them from DefaultProtectedPropertyImporter"
1,"CacheBehaviour Observation broken. While trying to fix JCR-2293 I discovered that CacheBehaviour Observation is broken:

- HierarchyEventListener.onEvent ignores local event (despite the comment saying otherwise). Not sure which way it should be. However with local events being ignored, JCR-2293 will most probably also occur with CacheBehaviour Observation. 

- NodeEntryImpl.refresh(Event) does not set its child node entries to incomplete when a node/property was added.

- After tentatively fixing above issues, I discovered that NodeEntryImpl.refresh(Event) and my own event listener operate on different NodeEntryImpl and ChildNodeEntryImpl instances. That is, even though I set childNodeEntries.complete to false in NodeEntryImpl.refresh(Event), when my own event listener retrieves that node (entry), it gets a different instance which has childNodeEntries.complete still set to true.
"
1,"TestAddIndexes#testAddIndexesWithThreads fails on Realtime. Selckin reported two failures on LUCENE-3023 which I can unfortunately not reproduce at all. here are the traces

{noformat}
  [junit] Testsuite: org.apache.lucene.index.TestAddIndexes
    [junit] Testcase: testAddIndexesWithThreads(org.apache.lucene.index.TestAddIndexes):	FAILED
    [junit] expected:<3160> but was:<3060>
    [junit] junit.framework.AssertionFailedError: expected:<3160> but was:<3060>
    [junit] 	at org.apache.lucene.index.TestAddIndexes.testAddIndexesWithThreads(TestAddIndexes.java:783)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1226)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1154)
    [junit] 
    [junit] 
    [junit] Tests run: 18, Failures: 1, Errors: 0, Time elapsed: 14.272 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestAddIndexes -Dtestmethod=testAddIndexesWithThreads -Dtests.seed=6128854208955988865:2552774338676281184
    [junit] NOTE: test params are: codec=PreFlex, locale=no_NO_NY, timezone=America/Edmonton
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestToken, TestDateTools, Test2BTerms, TestAddIndexes]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=8,threads=1,free=84731792,total=258080768
    [junit] ------------- ---------------- ---------------
{noformat}
and 
{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestAddIndexes
    [junit] Testcase: testAddIndexesWithThreads(org.apache.lucene.index.TestAddIndexes):	FAILED
    [junit] expected:<3160> but was:<3060>
    [junit] junit.framework.AssertionFailedError: expected:<3160> but was:<3060>
    [junit] 	at org.apache.lucene.index.TestAddIndexes.testAddIndexesWithThreads(TestAddIndexes.java:783)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1226)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1154)
    [junit] 
    [junit] 
    [junit] Tests run: 18, Failures: 1, Errors: 0, Time elapsed: 14.841 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestAddIndexes -Dtestmethod=testAddIndexesWithThreads -Dtests.seed=4502815121171887759:-6764285049309266272
    [junit] NOTE: test params are: codec=PreFlex, locale=tr_TR, timezone=Mexico/BajaNorte
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestToken, TestDateTools, Test2BTerms, TestAddIndexes]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=8,threads=1,free=163663416,total=243335168
    [junit] ------------- ---------------- ---------------
{noformat}"
1,"cache module generates exceptions for non-compliant responses without consuming response bodies. In the ResponseProtocolCompliance class, the caching module checks the incoming origin response to attempt to make it compliant with RFC2616. However, if there are instances where this is not possible, it currently throws an exception without consuming the origin response body; this causes a connection leak if the general try..catch..finally pattern documented on the HttpClient interface Javadoc is followed.
"
1,"thaiwordfilter uses attributesource.copyTo incorrectly. The bug can be seen by https://builds.apache.org/hudson/job/Lucene-Solr-tests-only-3.x/7367/

It looks like the issue is this lazy initialization of the cloned token: if the tokenstream is reused
and the consumer is interested in a different set of attributes, it could be a problem.

one probably-probably-not-totally-correct fix would be to add 'clonedToken = null;' to reset(), at 
least it would solve this case?"
1,"Charset omitted from UrlEncodedFormEntity Content-Type header. UrlEncodedFormEntity sets the Content-Type header to:
   ""application/x-www-form-urlencoded""

It should set the header to:
   ""application/x-www-form-urlencoded; charset="" + charset

As a result, content can be misinterpreted by the recipient (e.g. if the entity content includes multibyte Unicode characters encoded with the ""UTF-8"" charset).

For a correct example of specifying the charset in the Content-Type header, see StringEntity.java.

Here's the fix:

    public UrlEncodedFormEntity (
        final List <? extends NameValuePair> parameters, 
        final String encoding) throws UnsupportedEncodingException {
        super(URLEncodedUtils.format(parameters, encoding),  encoding);
-        setContentType(URLEncodedUtils.CONTENT_TYPE);
+        setContentType(URLEncodedUtils.CONTENT_TYPE + HTTP.CHARSET_PARAM +
+            (encoding != null ? encoding : HTTP.DEFAULT_CONTENT_CHARSET));
    }

    public UrlEncodedFormEntity (
        final List <? extends NameValuePair> parameters) throws UnsupportedEncodingException {
-        super(URLEncodedUtils.format(parameters, HTTP.DEFAULT_CONTENT_CHARSET), 
-            HTTP.DEFAULT_CONTENT_CHARSET);
-        setContentType(URLEncodedUtils.CONTENT_TYPE);
+        this(parameters, HTTP.DEFAULT_CONTENT_CHARSET);
    }
"
1,"JCR-Server: respect maximal value for timeout. RFC 2518 states:

""The timeout value for TimeType ""Second"" MUST NOT be greater than 2^32-1.""

->> adjust constant according.

BTW: sending 'Infinite' timeout in case of maximal value causes problems with microsoft builtin client, that will never unlock that resource."
1,"PathNotFoundException but item exists. The following test case (for jcr2spi) throws a PathNotFoundException for an item which exists. It does not throw if the marked line below is commented out. 

public void testBug24687() throws RepositoryException {
    String parentPath = testNode.getPath();
    String folderName = ""folder_"" + System.currentTimeMillis();
    Session session = getHelper().getReadWriteSession();

    Session session2 = getHelper().getReadOnlySession();
    session2.getItem(parentPath);  // removing this line makes the failure go away

    Node parent = (Node) session.getItem(parentPath);
    Node toDelete = parent.addNode(folderName, ""nt:folder"");
    parent.save();

    try {
        Item item2 = session2.getItem(parentPath + ""/"" + folderName);  // wrongly throws PathNotFoundException
        assertEquals(parentPath + ""/"" + folderName, item2.getPath());
    }
    finally {
        toDelete.remove();
        parent.save();
        assertFalse(parent.hasNode(folderName));
    }
}
"
1,DbDataStore keeps ResultSets open. The DbDataStore does not always close the ResultSet which can lead to memory leaks and/or large memory usage. It seems that  this already has been fixed in trunk and 1.5.
1,"Error instantiating lucene search index in Turkish Regional Setting. There is an issue when changing regional setting to Turkish. 
It fails when starting a repository, instantiating the lucene search index due to the following issue :

org.apache.jackrabbit.core.config.ConfigurationException: Configured class org.apache.jackrabbit.core.query.lucene.SearchIndex does not contain a property named indexingConfiguration
	at
org.apache.jackrabbit.core.config.BeanConfig.newInstance(BeanConfig.java
:205)
	at
org.apache.jackrabbit.core.config.RepositoryConfigurationParser$1.getQue
ryHandler(RepositoryConfigurationParser.java:631)
	at
org.apache.jackrabbit.core.config.RepositoryConfig.getQueryHandler(Repos
itoryConfig.java:1013)

This issue is known in java world, due to lower case conversion of 'I' character (in Turkish locale). JackRabbit source code try to instantiate the indexing configuration during the repository starting and is accessing indexingConfiguration property. It instantiates a setter for this property with a bad 'i' character.
"
1,"smartcn analyzer throw NullPointer exception when the length of analysed text over 32767. That's all because of org.apache.lucene.analysis.cn.smart.hhmm.SegGraph's makeIndex() method:
  public List<SegToken> makeIndex() {
    List<SegToken> result = new ArrayList<SegToken>();
    int s = -1, count = 0, size = tokenListTable.size();
    List<SegToken> tokenList;
    short index = 0;
    while (count < size) {
      if (isStartExist(s)) {
        tokenList = tokenListTable.get(s);
        for (SegToken st : tokenList) {
          st.index = index;
          result.add(st);
          index++;
        }
        count++;
      }
      s++;
    }
    return result;
  }

here 'short index = 0;' should be 'int index = 0;'. And that's reported here http://code.google.com/p/imdict-chinese-analyzer/issues/detail?id=2 and http://code.google.com/p/imdict-chinese-analyzer/issues/detail?id=11, the author XiaoPingGao have already fixed this bug:http://code.google.com/p/imdict-chinese-analyzer/source/browse/trunk/src/org/apache/lucene/analysis/cn/smart/hhmm/SegGraph.java"
1,"URIUtils.extractHost(...) throws a NumberFormatException line 310. Original Jboss-seam-wicket-booking application in Jboss-4.2.3.GA started, post a login request thanks httpclient, then NumberFormatException.



regarding this page :
http://hc.apache.org/httpcomponents-client-dev/httpclient/clover/org/apache/http/client/utils/URIUtils.html

305 	   	// Extract the port suffix, if present
306 	   	if (host != null) {
307 	  	   int colon = host.indexOf(':');
308 	   	   if (colon >= 0) {
309 	   	      if (colon+1 < host.length()) {
310 	   	          port = Integer.parseInt(host.substring(colon+1));
311 	   	      }
312 	  	   host = host.substring(0,colon);
313 	   	   }
314 	   	}

resolving the port throw a NumberFormatException

java.lang.NumberFormatException: For input string: ""8080;jsessionid=9E9EDA0B6E1CDD499A0A15C4A8F212D8""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)
	at java.lang.Integer.parseInt(Integer.java:458)
	at java.lang.Integer.parseInt(Integer.java:499)
	at org.apache.http.client.utils.URIUtils.extractHost(URIUtils.java:310)
	at org.apache.http.impl.client.AbstractHttpClient.determineTarget(AbstractHttpClient.java:764)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:754)
	at org.tagbrowser.api.TagBrowser.request(TagBrowser.java:109)


another case of this problem canbe found hier :
https://gitorious.org/yacy/rc1/commit/8b0920b0b5eb67ae17eec24c1bf3a059543cb6e8/diffs"
1,"Overwriting a reference property with different type corrupts rep. - create node n1
- create node n2
- n2.setProperty(""prop"", n1)
- save()
- n2.setProperty(""prop"", ""hello, world."")
- save()
- n1.remove()
- save() --> exception

see also ReferencesTest case

btw: removing the property or overwriting with a different reference works."
1,"Field names can be wrong for stored fields / term vectors after merging. The good news is this bug only exists in trunk... the bad news is it's
been here for some time (created by accident in LUCENE-2881).  But the
good news is it should strike fairly rarely.

SegmentMerger sometimes incorrectly thinks it can bulk-copy TVs/stored
fields when it cannot (because field numbers don't map to the same
names across segments).

I think it happens only with addIndexes, or indexes that have
pre-trunk segments, and then SM falsely thinks it can bulk-merge only
when the last field number has the same field name across segments.
"
1,"The ""jackrabbit-pool-"" thread prevents the process from stopping. If the repository is not closed, and a session is still logged in, then the process doesn't terminate because of a non-daemon thread named ""jackrabbit-pool-<n>"". Test case:

public class TestThreadPreventsExit {
    public static void main(String... a) throws Exception {
        new TransientRepository().login(
                new SimpleCredentials("""", new char[0]));
    }
}

This program doesn't stop.

The non-daemon thread was introduces as part of https://issues.apache.org/jira/browse/JCR-2465

The fix is to use a daemon thread."
1,"IW.optimize() can do too many merges at the very end. This was fixed on trunk in LUCENE-1044 but I'd like to separately
backport it to 2.3.

With ConcurrentMergeScheduler there is a bug, only when CFS is on,
whereby after the final merge of an optimize has finished and while
it's building its CFS, the merge policy may incorrectly ask for
another merge to collapse that segment into a compound file.  The net
effect is optimize can spend many extra iterations unecessarily
merging a single segment to collapse it to compound file.

I believe the case is rare (hard to hit), and maybe only if you have
multiple threads calling optimize at once (the TestThreadedOptimize
test can hit it), but it's a low-risk fix so I plan to commit to 2.3
shortly.

"
1,NPE in ConsolidatingChangeLog. The hasSNS(NodeId) method in ConsolidatingChangeLog throws an NPE when nodeId is null. It should rather return false. 
1,"Concurrent Session.move() operations failure. Performing concurrent move operations may cause failures similar to the following:

javax.jcr.RepositoryException: Unable to update item: node /
       at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1147)
       at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:920)
       at ConcurrentMoveTest$1.execute(ConcurrentMoveTest.java:30)
       at org.apache.jackrabbit.core.AbstractConcurrencyTest$Executor.run(AbstractConcurrencyTest.java:209)
       at java.lang.Thread.run(Thread.java:637)
Caused by: org.apache.jackrabbit.core.state.ItemStateException: Unable
to resolve path for item: 79a0fbdb-49fd-4830-a842-5ab11842cd17
       at org.apache.jackrabbit.core.observation.EventStateCollection.getPath(EventStateCollection.java:683)
       at org.apache.jackrabbit.core.observation.EventStateCollection.createEventStates(EventStateCollection.java:268)
       at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:702)
       at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:1110)
       at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:1140)
       at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:351)
       at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:354)
       at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:326)
       at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:328)
       at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1141)
       ... 4 more
Caused by: javax.jcr.ItemNotFoundException: failed to build path of
79a0fbdb-49fd-4830-a842-5ab11842cd17:
826f0c19-9956-402a-9c0d-93089eedcc1c has no child entry for
79a0fbdb-49fd-4830-a842-5ab11842cd17
       at org.apache.jackrabbit.core.HierarchyManagerImpl.buildPath(HierarchyManagerImpl.java:291)
       at org.apache.jackrabbit.core.CachingHierarchyManager.buildPath(CachingHierarchyManager.java:198)
       at org.apache.jackrabbit.core.HierarchyManagerImpl.getPath(HierarchyManagerImpl.java:395)
       at org.apache.jackrabbit.core.CachingHierarchyManager.getPath(CachingHierarchyManager.java:232)
       at org.apache.jackrabbit.core.observation.EventStateCollection.getPath(EventStateCollection.java:678)
       ... 13 more
"
1,"[jcr-rmi] workspace.copy doesn't work. swapped parameters in org/apache/jackrabbit/rmi/client/ClientWorkspace/ClientWorkspace. 
"
1,"SO_TIMEOUT parameter on the method level has no effect. This bug has been reported on the HttpClient user list by Ilya Kharmatsky <ilyak
-at- mainsoft.com>"
1,"Two consecutive score() calls return different scores for Boolean Queries.. Two consecutive calls to score() return different scores (no next() or skipTo() calls in between). 
Background in LUCENE-912 .
"
1,"SystemSessions created for GarbageCollector are not logged out of. I have a simple garbage collection task that runs periodically. After upgrading to 1.5.5 it started logging a warning shortly after each run:

2009-05-09 03:44:45,480 WARN [org.apache.jackrabbit.core.SessionImpl] - <Unclosed session detected. The session was opened here: >
java.lang.Exception: Stack Trace
	at org.apache.jackrabbit.core.SessionImpl.<init>(SessionImpl.java:239)
	at org.apache.jackrabbit.core.SystemSession.<init>(SystemSession.java:76)
	at org.apache.jackrabbit.core.SystemSession.create(SystemSession.java:64)
	at org.apache.jackrabbit.core.SessionImpl.createDataStoreGarbageCollector(SessionImpl.java:649)

So it's not my session, but an internally created SystemSession.


Code I'm using:
            getTemplate().execute(new JcrCallback()
            {
                public Object doInJcr(Session session)
                    throws IOException, RepositoryException {
                    SessionImpl sessionImpl = (SessionImpl)session;
                    GarbageCollector gc = sessionImpl.createDataStoreGarbageCollector();
                    gc.scan();
                    gc.stopScan();
                    gc.deleteUnused();
                    return null;
                }
            }, true);
"
1,"TestAddIndexes fails (norms file not found). ant test-core -Dtestcase=TestAddIndexes -Dtestmethod=testAddIndexesWithRollback -Dtests.seed=2f55291b308dc34b:-4d72bfad34f3f357:4bc5ec894269c041 -Dargs=""-Dfile.encoding=UTF-8"" -Dtests.iter=100

fails about 4 or 5 times out of 100."
1,"Startup fails if clustered jackrabbit is upgrade from 1.4.4 to 1.5. This is closely related to JCR-1087

The call to checkLocalRevisionSchema() is too late because preapreStatements() already uses the LOCAL_REVISIONS table.

checkLocalRevisionSchema() should be called in checkSchema()"
1,"Repository is corrupt after concurrent changes with the same session. After concurrent write operations using the same session, the repository can get corrupt, meaning a ItemNotFoundException is thrown when trying to remove a node.

Concurrent write operations are not supported, however I believe the persistent state of the repository should not be get corrupt.

One way to solve this problem is to synchronize on the session internally."
1,"Memory leak in UUIDDocId. The hierarchy cache of the search index uses DocId's to reference the parent of a Node. One implementation of a DocId is the UUIDDocId which may hold a reference to the IndexReader that was used to calculate the id. UUIDDocId's are invalidated on a lazy basis. That is the referenced IndexReader may still be present even though the IndexReader instance has long been closed.

-> UUIDDocId should only hold a weak reference to the IndexReader."
1,"Jcr-server: Parsing NodeTypeProperty not compliant with definition. Creating a new NodeTypeProperty from an existing DavProperty fails, since assumptions made are not compliant with definition:

a) nodetype name is always enclosed in a 'nodetypename' element
b) nodetype property may be empty, thus contain no 'nodetype' element."
1,"KeywordTokenizer/Analyzer cannot be re-used. 
The new reusableTokenStream API in KeywordAnalyzer fails to reset the tokenizer when it re-uses it.

This issue came from this thread:

    http://www.gossamer-threads.com/lists/lucene/java-dev/55929

Thanks to Hideaki Takahashi for finding this!"
1,Session#move doesn't trigger rebuild of parent node aggregation. The summary says it all.
1,"XPathQueryBuilder may not handle multiple jcr:deref correctly. If you have the following tree (inspired from DerefTest) :
+ people
   + carl (worksfor -> company/microsoft)
   + frank (worksfor -> company/microsoft)
+ company
    + microsoft (eotm -> carl)

The following queries will be translated to :

testroot/people/frank/jcr:deref(@worksfor, '*')/jcr:deref(@eotm, '*')
+ Root node
+ Select properties: *
  + PathQueryNode
    + LocationStepQueryNode:  NodeTest={}testroot Descendants=false Index=NONE
    + LocationStepQueryNode:  NodeTest={}people Descendants=false Index=NONE
    + LocationStepQueryNode:  NodeTest={}frank Descendants=false Index=NONE
    + DerefQueryNode:  NodeTest=* Descendants=false Index=NONE
    + DerefQueryNode:  NodeTest=* Descendants=false Index=NONE
=> Matching carl node

testroot/people/frank/jcr:deref(@worksfor, '*')/jcr:deref(@eotm, '*')[@jcr:uuid]
+ Root node
+ Select properties: *
  + PathQueryNode
    + LocationStepQueryNode:  NodeTest={}testroot Descendants=false Index=NONE
    + LocationStepQueryNode:  NodeTest={}people Descendants=false Index=NONE
    + LocationStepQueryNode:  NodeTest={}frank Descendants=false Index=NONE
    + LocationStepQueryNode:  NodeTest=* Descendants=false Index=NONE
    + DerefQueryNode:  NodeTest=* Descendants=false Index=NONE
      + RelationQueryNode: Op: NOT NULL Prop=@{http://www.jcp.org/jcr/1.0}uuid
=> Not matching carl node

testroot/people/frank/jcr:deref(@worksfor, '*')[@jcr:uuid]/jcr:deref(@eotm, '*')[@jcr:uuid]
+ Root node
+ Select properties: *
  + PathQueryNode
    + LocationStepQueryNode:  NodeTest={}testroot Descendants=false Index=NONE
    + LocationStepQueryNode:  NodeTest={}people Descendants=false Index=NONE
    + LocationStepQueryNode:  NodeTest={}frank Descendants=false Index=NONE
    + DerefQueryNode:  NodeTest=* Descendants=false Index=NONE
      + RelationQueryNode: Op: NOT NULL Prop=@{http://www.jcp.org/jcr/1.0}uuid
    + DerefQueryNode:  NodeTest=* Descendants=false Index=NONE
      + RelationQueryNode: Op: NOT NULL Prop=@{http://www.jcp.org/jcr/1.0}uuid
=> Matching carl node

testroot/people/frank/jcr:deref(@worksfor, '*')[@jcr:uuid]/jcr:deref(@eotm, '*')
+ Root node
+ Select properties: *
  + PathQueryNode
    + LocationStepQueryNode:  NodeTest={}testroot Descendants=false Index=NONE
    + LocationStepQueryNode:  NodeTest={}people Descendants=false Index=NONE
    + LocationStepQueryNode:  NodeTest={}frank Descendants=false Index=NONE
    + DerefQueryNode:  NodeTest=* Descendants=false Index=NONE
      + RelationQueryNode: Op: NOT NULL Prop=@{http://www.jcp.org/jcr/1.0}uuid
    + DerefQueryNode:  NodeTest=* Descendants=false Index=NONE
=> Matching carl node

This is because XPathQueryBuilder calls NAryQueryNode#removeOperand(QueryNode)
in order to replace current LocationStepQueryNode with a DerefQueryNode.

NAryQueryNode#removeOperand(QueryNode) uses internally a List and thus
relies on Object#equals(Object) for retrieving the object to remove.

But the equals method is redefined for every QueryNode with a different semantic.

Then, the call to NAryQueryNode#removeOperand(QueryNode) will not remove the
wanted operand but the first operand returning true after calling equals in
ArrayList#remove(Object)."
1,"WorkspaceImporter throws exception. sessio.getWorkspace().getImportContentHandler() throws 
java.lang.UnsupportedOperationException: Workspace-Import of protected nodes: Not yet implement.

suggest to issue warning instead of throwing."
1,"TestIndexWriterExceptions fails (reproducible). {noformat}
ant test -Dtestcase=TestIndexWriterExceptions -Dtestmethod=testIllegalPositions -Dtests.seed=-228094d3d2f35cf2:-496e33eec9bbd57c:36a1c54f4e1bb32 -Dargs=""-Dfile.encoding=UTF-8""

    [junit] junit.framework.AssertionFailedError: position=-2 lastPosition=0
    [junit]     at org.apache.lucene.codecs.lucene40.Lucene40PostingsWriter.addPosition(Lucene40PostingsWriter.java:215)
    [junit]     at org.apache.lucene.index.FreqProxTermsWriterPerField.flush(FreqProxTermsWriterPerField.java:519)
    [junit]     at org.apache.lucene.index.FreqProxTermsWriter.flush(FreqProxTermsWriter.java:92)
    [junit]     at org.apache.lucene.index.TermsHash.flush(TermsHash.java:117)
    [junit]     at org.apache.lucene.index.DocInverter.flush(DocInverter.java:53)
    [junit]     at org.apache.lucene.index.DocFieldProcessor.flush(DocFieldProcessor.java:81)
    [junit]     at org.apache.lucene.index.DocumentsWriterPerThread.flush(DocumentsWriterPerThread.java:475)
    [junit]     at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:422)
    [junit]     at org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:553)
    [junit]     at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:2640)
    [junit]     at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:2616)
    [junit]     at org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:851)
    [junit]     at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:810)
    [junit]     at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:774)
    [junit]     at org.apache.lucene.index.TestIndexWriterExceptions.testIllegalPositions(TestIndexWriterExceptions.java:1517)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)
    [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
    [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
    [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
    [junit]     at org.apache.lucene.util.LuceneTestCase$SubclassSetupTeardownRule$1.evaluate(LuceneTestCase.java:729)
    [junit]     at org.apache.lucene.util.LuceneTestCase$InternalSetupTeardownRule$1.evaluate(LuceneTestCase.java:645)
    [junit]     at org.apache.lucene.util.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:22)
    [junit]     at org.apache.lucene.util.LuceneTestCase$TestResultInterceptorRule$1.evaluate(LuceneTestCase.java:556)
    [junit]     at org.apache.lucene.util.UncaughtExceptionsRule$1.evaluate(UncaughtExceptionsRule.java:51)
    [junit]     at org.apache.lucene.util.LuceneTestCase$RememberThreadRule$1.evaluate(LuceneTestCase.java:618)
    [junit]     at org.junit.rules.RunRules.evaluate(RunRules.java:18)
    [junit]     at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
    [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
    [junit]     at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:164)
    [junit]     at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)
    [junit]     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
    [junit]     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
    [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
    [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
    [junit]     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
    [junit]     at org.apache.lucene.util.UncaughtExceptionsRule$1.evaluate(UncaughtExceptionsRule.java:51)
    [junit]     at org.apache.lucene.util.StoreClassNameRule$1.evaluate(StoreClassNameRule.java:21)
    [junit]     at org.apache.lucene.util.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:22)
    [junit]     at org.junit.rules.RunRules.evaluate(RunRules.java:18)
    [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
    [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:518)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1052)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:906)
    [junit] 
{noformat}"
1,"HttpConnection.isResponseAvailable() calls setSoTimeout() but does not catch IOException. HttpConnection.isResponseAvailable() can throw an IOException when setting the
soTimeout but should probably just return false in this case.

<http://marc.theaimsgroup.com/?t=106268485100002&r=1&w=2>"
1,"Incorrect sort by Numeric values for documents missing the sorting field. While sorting results over a numeric field, documents which do not contain a value for the sorting field seem to get 0 (ZERO) value in the sort. (Tested against Double, Float, Int & Long numeric fields ascending and descending order).
This behavior is unexpected, as zero is ""comparable"" to the rest of the values. A better solution would either be allowing the user to define such a ""non-value"" default, or always bring those document results as the last ones.

Example scenario:
Adding 3 documents, 1st with value 3.5d, 2nd with -10d, and 3rd without any value.
Searching with MatchAllDocsQuery, with sort over that field in descending order yields the docid results of 0, 2, 1.

Asking for the top 2 documents brings the document without any value as the 2nd result - which seems as a bug?"
1,Basic Authentification fails with non-ASCII username/password characters. http://marc.theaimsgroup.com/?t=106866959500001&r=1&w=2
1,"SamplingWrapperTest failure with certain test seed. Build: https://builds.apache.org/job/Lucene-Solr-tests-only-trunk/12231/

1 tests failed.
REGRESSION:  org.apache.lucene.facet.search.SamplingWrapperTest.testCountUsingSamping

Error Message:
Results are not the same!

Stack Trace:
org.apache.lucene.facet.FacetTestBase$NotSameResultError: Results are not the same!
       at org.apache.lucene.facet.FacetTestBase.assertSameResults(FacetTestBase.java:333)
       at org.apache.lucene.facet.search.sampling.BaseSampleTestTopK.assertSampling(BaseSampleTestTopK.java:104)
       at org.apache.lucene.facet.search.sampling.BaseSampleTestTopK.testCountUsingSamping(BaseSampleTestTopK.java:82)
       at org.apache.lucene.util.LuceneTestCase$3$1.evaluate(LuceneTestCase.java:529)
       at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:165)
       at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)

NOTE: reproduce with: ant test -Dtestcase=SamplingWrapperTest -Dtestmethod=testCountUsingSamping -Dtests.seed=4a5994491f79fc80:-18509d134c89c159:-34f6ecbb32e930f7 -Dtests.multiplier=3 -Dargs=""-Dfile.encoding=UTF-8""
NOTE: test params are: codec=Lucene40: {$facets=PostingsFormat(name=MockRandom), $full_path$=PostingsFormat(name=MockSep), content=Pulsing40(freqCutoff=19 minBlockSize=65 maxBlockSize=209), $payloads$=PostingsFormat(name=Lucene40WithOrds)}, sim=RandomSimilarityProvider(queryNorm=true,coord=true): {$facets=LM Jelinek-Mercer(0.700000), content=DFR I(n)B3(800.0)}, locale=bg, timezone=Asia/Manila
"
1,"BundleDbPersistenceManager.checkConsistency() only fixes inconsistency if consistencyFix is enabled in configuration. The method has a parameter that explicitly tells whether an inconsistency should be fixed, thus the configuration parameter should be ignored.

Suggested patch:

Index: BundleDbPersistenceManager.java
===================================================================
--- BundleDbPersistenceManager.java	(revision 648657)
+++ BundleDbPersistenceManager.java	(working copy)
@@ -864,7 +864,7 @@
         }
 
         // repair collected broken bundles
-        if (consistencyFix && !modifications.isEmpty()) {
+        if (fix && !modifications.isEmpty()) {
             log.info(name + "": Fixing "" + modifications.size() + "" inconsistent bundle(s)..."");
             Iterator iterator = modifications.iterator();
             while (iterator.hasNext()) {
"
1,"Garbage collection deletes temporary files in FileDataStore. In FileDataStore.addRecord(InputStream), a temporary file is created. The data is written to the file and then it is moved to its final location (based on the contents hash).

If the garbage collector runs whilst this temp file is present, it deletes it (on Solaris 10 at least), and the addRecord fails at the attempt to rename the now non-existent temp file.

I am attaching a minimal patch that prevents these temp files being deleted by deleteOlderRecursive(..), regardless of their lastModified() value.

I have made this a Minor priority, since there is the obvious workaround of disabling the GC.
"
1,"jcr2spi:  Remove sanityCheck() from ItemImpl.getSession(). same as JCR-911 for jcr2spi.

the check was responsible for the failure of ActivitiesTest#testActivitiesRelation"
1,"recovery tool does not recover when version history can be instantiated, but root version can not. JCR-2551 introduced a recovery mode which tries to instantiate the version history, and if this fails, disconnects the VH (version history) and makes the node unversioned.

However, it appears it can happen that the persistence is damaged such as getting the VH does indeed work, but subsequent operations fail due to other problems. One problem that has been seen is a missing frozenNode property of the root version (or a missing frozenNode itself).

As a quick fix, we may want to change the checker so that it actually also tries to get the rootVersion and it's frozenNode. Long term, depending on how frequent this problem is, we may have to think about a less drastic recovery than disconnecting the VH."
1,"Exception in HttpConnection because of unchecked buffer size. From the httpclient-dev mailing list:

Date: Tue, 8 Mar 2005 19:08:35 +0100
Subject: Error with multiple connections

Hello,

 

I am having some problems while trying multiple connections over a
HttpClient object with a MultiThreadedHttpConnectionManager. I am
launching 10 threads and each thread executes some GetMethods using this
HttpClient object.

 

Some times I got an error like this:

 

java.lang.IllegalArgumentException: Buffer size <= 0

      at java.io.BufferedInputStream.<init>(Unknown Source)

      at
org.apache.commons.httpclient.HttpConnection.open(HttpConnection.java:70
3)

      at
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpCon
nectionAdapter.open(MultiThreadedHttpConnectionManager.java:1170)

      at
org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:6
28)

      at
org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:4
97)

      at Main$Hilo.run(Main.java:58)

 

Does anybody have any idea? 

 

Thanks in advance,

Jorge"
1,"NTLM Authentication Fails. NTLM Authentication requires multiple request/responses for the authentication
to succeed.  Since HttpMethodBase is now using just the host, port and realm to
identify whether or not authentication has been attempted the second pass for
NTLM authentication is never performed."
1,"problem with edgengramtokenfilter and highlighter. i ran into a problem while using the edgengramtokenfilter, it seems to report incorrect offsets when generating tokens, more specifically all the tokens have offset 0 and term length as start and end, this leads to goofy highlighting behavior when creating edge grams for tokens beyond the first one, i created a small patch that takes into account the start of the original token and adds that to the reported start/end offsets.

"
1,"You cannot sort on fields that don't exist. While it's possible to search for fields that don't exist (you'll get 0 hits),  
you'll get an exception if you try to sort by a field that has no values. The  
exception is this:  
  
if (termEnum.term() == null) {  
  throw new RuntimeException (""no terms in field "" + field);  
}  
  
I'll attach a change suggested by Yonik Seeley that removes this exception. 
 
Also, the if-condition above is incomplete anyway, so currently the exception 
is not always thrown (as termEnum .term() might well be != null but point to a 
term in a different field already)"
1,"MinPayloadFunction returns 0 when only one payload is present. In some experiments with payload scoring through PayloadTermQuery, I'm seeing 0 returned when using MinPayloadFunction.  I believe there is a bug there.  No time at the moment to flesh out a unit test, but wanted to report it for tracking."
1,"incorrect snippet returned with SpanScorer. This problem was reported by my customer. They are using Solr 1.3 and uni-gram, but it can be reproduced with Lucene 2.9 and WhitespaceAnalyzer.

{panel:title=Query}
(f1:""a b c d"" OR f2:""a b c d"") AND (f1:""b c g"" OR f2:""b c g"")
{panel}

The snippet we expected is:
{panel}
x y z <B>a</B> <B>b</B> <B>c</B> <B>d</B> e f g <B>b</B> <B>c</B> <B>g</B>
{panel}

but we got:
{panel}
x y z <B>a</B> b c <B>d</B> e f g <B>b</B> <B>c</B> <B>g</B>
{panel}

Program to reproduce the problem:
{code}
public class TestHighlighter {

  static final String CONTENT = ""x y z a b c d e f g b c g"";
  static final String PH1 = ""\""a b c d\"""";
  static final String PH2 = ""\""b c g\"""";
  static final String F1 = ""f1"";
  static final String F2 = ""f2"";
  static final String F1C = F1 + "":"";
  static final String F2C = F2 + "":"";
  static final String QUERY_STRING =
    ""("" + F1C + PH1 + "" OR "" + F2C + PH1 + "") AND (""
    + F1C + PH2 + "" OR "" + F2C + PH2 + "")"";
  static Analyzer analyzer = new WhitespaceAnalyzer();
  
  public static void main(String[] args) throws Exception {
    QueryParser qp = new QueryParser( F1, analyzer );
    Query query = qp.parse( QUERY_STRING );
    CachingTokenFilter stream = new CachingTokenFilter( analyzer.tokenStream( F1, new StringReader( CONTENT ) ) );
    Scorer scorer = new SpanScorer( query, F1, stream, false );
    Highlighter h = new Highlighter( scorer );
    System.out.println( ""query : "" + QUERY_STRING );
    System.out.println( h.getBestFragment( analyzer, F1,  CONTENT ) );
  }
}
{code}
"
1,ChangeLogRecord throws NullPointerException. Happens when the userId of the session that created the events is null.
1,"Text.unescape(""%"") throws a StringIndexOutOfBoundsException. You get the following exception:

java.lang.StringIndexOutOfBoundsException: String index out of range: 3
	at java.lang.String.substring(String.java:1935)
	at org.apache.jackrabbit.util.Text.unescape(Text.java:407)
	at org.apache.jackrabbit.util.Text.unescape(Text.java:438)

It would be better if it failed with IllegalArgumentException."
1,"cannot PUT changes to a resource in the simple webdav server. when using the simple webdav server to PUT a resource, the ""versionable"" mixin node type is assigned to the new node without regard to whether the node type is already assigned to the node. this causes PUT requests that change existing resources to fail with 403 errors.

the fix is to augment AddMixinCommand to not try to add the mixin node type if the node already has it.
"
1,"isCurrent() and getVersion() on an NRT reader are broken. Right now isCurrent() will always return true for an NRT reader and getVersion() will always return the version of the last commit.  This is because the NRT reader holds the live segmentInfos.

I think isCurrent() should return ""false"" when any further changes have occurred with the writer, else true.   This is actually fairly easy to determine, since the writer tracks how many docs & deletions are buffered in RAM and these counters only increase with each change.

getVersion should return the version as of when the reader was created."
1,"Deadlock with MultiThreadedHttpConnectionManager. I'm getting a dealock with the MultiThreadedHttpConnectionManager. Usually, it
works fine, but when a web page is redirected, it blocks. 

Ludovic.

[ERROR] Redirect to http://sourceforge.net/
Full thread dump Java HotSpot(TM) Client VM (1.4.2_03-b02 mixed mode):

""MultiThreadedHttpConnectionManager cleanup"" daemon prio=5 tid=0x02d566f0
nid=0xe14 in Object.wait() [2e9f000..2e9fd8c]
        at java.lang.Object.wait(Native Method)
        - waiting on <0x10513be8> (a java.lang.ref.ReferenceQueue$Lock)
        at java.lang.ref.ReferenceQueue.remove(Unknown Source)
        - locked <0x10513be8> (a java.lang.ref.ReferenceQueue$Lock)
        at java.lang.ref.ReferenceQueue.remove(Unknown Source)
        at
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$ReferenceQueueThread.run(MultiThreadedHttpConnectionManager.java:805)

""Signal Dispatcher"" daemon prio=10 tid=0x0003da00 nid=0xd44 waiting on condition
[0..0]

""Finalizer"" daemon prio=9 tid=0x009bca30 nid=0xce8 in Object.wait()
[2b5f000..2b5fd8c]
        at java.lang.Object.wait(Native Method)
        - waiting on <0x10504b80> (a java.lang.ref.ReferenceQueue$Lock)
        at java.lang.ref.ReferenceQueue.remove(Unknown Source)
        - locked <0x10504b80> (a java.lang.ref.ReferenceQueue$Lock)
        at java.lang.ref.ReferenceQueue.remove(Unknown Source)
        at java.lang.ref.Finalizer$FinalizerThread.run(Unknown Source)

""Reference Handler"" daemon prio=10 tid=0x009bb600 nid=0xfa4 in Object.wait()
[2b1f000..2b1fd8c]
        at java.lang.Object.wait(Native Method)
        - waiting on <0x10504be8> (a java.lang.ref.Reference$Lock)
        at java.lang.Object.wait(Unknown Source)
        at java.lang.ref.Reference$ReferenceHandler.run(Unknown Source)
        - locked <0x10504be8> (a java.lang.ref.Reference$Lock)

""main"" prio=5 tid=0x00035e28 nid=0xf68 in Object.wait() [7f000..7fc3c]
        at java.lang.Object.wait(Native Method)
        - waiting on <0x105170e8> (a
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$ConnectionPool)
        at
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager.doGetConnection(MultiThreadedHttpConnectionManager.java:388)
        - locked <0x105170e8> (a
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$ConnectionPool)
        at
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager.getConnection(MultiThreadedHttpConnectionManager.java:296)
        at
org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:645)
        at
org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:529)
        at net.sourceforge.cvsgrab.WebBrowser.executeMethod(WebBrowser.java:201)
        at net.sourceforge.cvsgrab.WebBrowser.getResponse(WebBrowser.java:257)
        at net.sourceforge.cvsgrab.WebBrowser.getDocument(WebBrowser.java:295)
        at
net.sourceforge.cvsgrab.CvsWebInterface.loadDocument(CvsWebInterface.java:111)
        at
net.sourceforge.cvsgrab.CvsWebInterface.getDocumentForDetect(CvsWebInterface.java:216)
        at
net.sourceforge.cvsgrab.CvsWebInterface.findInterface(CvsWebInterface.java:86)
        at net.sourceforge.cvsgrab.CVSGrab.detectWebInterface(CVSGrab.java:688)
        at net.sourceforge.cvsgrab.CVSGrab.grabCVSRepository(CVSGrab.java:616)
        at net.sourceforge.cvsgrab.CVSGrab.run(CVSGrab.java:317)
        at net.sourceforge.cvsgrab.CVSGrab.main(CVSGrab.java:206)

""VM Thread"" prio=5 tid=0x009f76d0 nid=0x550 runnable

""VM Periodic Task Thread"" prio=10 tid=0x009f8208 nid=0x560 waiting on condition
""Suspend Checker Thread"" prio=10 tid=0x009bed88 nid=0xe84 runnable"
1,"setProperty access control evaluation does not properly cope with XA transactions. This is another instance of the problems with ACL evaluation within transactions described in https://issues.apache.org/jira/browse/JCR-2999.
In this case PropertyImpl#getParent() called from PropertyImpl#checkSetValue() is trying to check read permissions of the yet uncommited parent and thus fails with an ItemNotFound exception.

The problem is reproducible with the following test:

public void testTransaction() throws Exception {

        // make sure testUser has all privileges
        Privilege[] privileges = privilegesFromName(Privilege.JCR_ALL);
        givePrivileges(path, privileges, getRestrictions(superuser, path));

        // create new node and lock it
        Session s = getTestSession();
        UserTransaction utx = new UserTransactionImpl(s);
        utx.begin();

        // add node and save it
        Node n = s.getNode(childNPath);
        if (n.hasNode(nodeName1)) {
            Node c = n.getNode(nodeName1);
            c.remove();
            s.save();
        }

        // create node and save
        Node n2 = n.addNode(nodeName1);
        s.save(); // -> node is NEW -> no failure

        // set a property on a child node of an uncommited parent
        n2.setProperty(propertyName1, ""testSetProperty"");
        s.save();  // -> fail because PropertyImpl#getParent called from PropertyImpl#checkSetValue
                       //    was checking read permission on the not yet commited parent

        // commit
        utx.commit();
    }"
1,IndexReader.undeleteAll can mess up the deletion count stored in the segments file. Spinoff from LUCENE-1474.  I'll attach a test case showing the issue.
1,"OpenBitSet#hashCode() may return false for identical sets.. OpenBitSet uses an internal buffer of long variables to store set bits and an additional 'wlen' index that points 
to the highest used component inside {@link #bits} buffer.

Unlike in JDK, the wlen field is not continuously maintained (on clearing bits, for example). This leads to a situation when wlen may point
far beyond the last set bit. 

The hashCode implementation iterates over all long components of the bits buffer, rotating the hash even for empty components. This is against the contract of hashCode-equals. The following test case illustrates this:

{code}
// initialize two bitsets with different capacity (bits length).
BitSet bs1 = new BitSet(200);
BitSet bs2 = new BitSet(64);
// set the same bit.
bs1.set(3);
bs2.set(3);
        
// equals returns true (passes).
assertEquals(bs1, bs2);
// hashCode returns false (against contract).
assertEquals(bs1.hashCode(), bs2.hashCode());
{code}

Fix and test case attached."
1,"redefinition of xml-namespace mapping should not be allowed. the following throws an exception, but should work:

// remap xml namespace -> works
Session.setNamespacePrefix(""foobar"", ""http://www.w3.org/XML/1998/namespace"");

// revert mapping -> throws exception
Session.setNamespacePrefix(""xml"", ""http://www.w3.org/XML/1998/namespace"");"
1,"Mixins as supertypes do not appear to be queryable. When creating custom nodetypes that contain mixins as the supertype, nodes of the custom type do not appear to be queryable when using statements of the form: //element(*, mixin). Attached are a relatively simple JUnit test and compact type definition that seem to illustrate the problem."
1,"HttpState.setCookiePolicy() is completely ignored. Though this method is deprecated, it currently has no effect and gives no warning that it does nothing.  
A patch that fixes this problem is coming shortly.

Mike"
1,"UserAccessControlProvider handles users who dont have Jackrabbit managed Principals or User node inconsistently.. JR core 2.0.0
In UserAccessControlProvider.compilePermissions(...), if no principal relating to a user node can be found, then a set or read only compiled permissions is provided. That set gives the session read only access to the entire security workspace regardless of path.

If the user node is found, then an instance of UserAccessControlProvider.CompilePermissions is used and in UserAccessControlProvider.CompilePermissions.buildResult(...) there is a check for no user node. If there is no user node, all permissions are denied regardless of path.

Although the first case will never happen for an installation of Jackrabbit where there are no custom PrincipalManagers, I suspect, based on the impl of UserAccessControlProvider.CompilePermissions.buildResult(...) was to deny all access to the security workspace where there was no corresponding user node in a set of principals.

Since this does not effect JR unless there is an external Principal Manager its a bit hard to produce a compact unit test, the issue was found by looking at the code."
1,"ZombieHierarchyManager can return wrong child node entries for replaced nodes. The ZombieHierarchyManager currently implements the two getChildNodeEntry methods like this:

1) look up child node in old, overlayed state, which might contain removed child nodes
2) if not found, ask the super implementation (ie. get the child node from the up-to-date list)

The purpose of the ZombieHM is to be able to return removed item ids from the attic. However, the behavior above is IMO wrong, as it should first find an existing child node with the given name (or id):

1) look up child node in super implementation (ie. get the child node from the up-to-date list)
2) if not found, look in the old, overlayed state if it might have been removed

I was able to reproduce this issue when replacing a node (but note the custom access manager in 1.4.x used as explained below): create /replaced/subnode structure, save the session, remove the replaced node and add /replaced and then /replaced/subnode again:

        Node rootNode = session.getRootNode();
        
        // 1. create structure /replaced/subnode
        Node test = rootNode.addNode(""replaced"", NT);
        test.addNode(""subnode"", NT);
        // 2. persist changes
        session.save();

        // 3. remove node and recreate it
        test.remove();
        test = rootNode.addNode(""replaced"", NT);
        
        // 4. create previous child with same name
        test.addNode(""subnode"", NT);
        
        // 5. => gives exception
        test.getNode(""subnode"").getNodes();

To complicate things further, this was only triggered by a custom access manager, and all based upon Jackrabbit 1.4.x. Back then (pre-1.5 and new security stuff era), the access manager would get a ZombieHM as its hierarchy manager. If its implementation called resolvePath() on the HM for checking read-access in the final getNodes() call, where the tree will be traversed using the getChildNdeEntry(NodeState, Name, int) method, it would get the old node id and hence fail if it would try to retrieve it from the real item state manager.

Thus with a Jackrabbit >= 1.5 and 2.0 the above code will work fine, because the ZombieHM is not used.

However, we might want to fix it for 1.4.x and also check the other uses of the ZombieHM in the current trunk, which I couldn't test. These are (explicit and implicit): ChangeLogBasedHierarchyMgr, SessionItemStateManager.getDescendantTransientItemStates(NodeId), ItemImpl.validateTransientItems(Iterable<ItemState>, Iterable<ItemState>) and SessionItemStateManager.getDescendantTransientItemStatesInAttic(NodeId).
"
1,ChainedTermEnum omits initial terms. This is a regression caused by JCR-2393.
1,Index recovery may fail when redo log contains nodes that are part of an index aggregate. SearchIndex.mergeAggregatedNodeIndexes() will throw a NullPointerException because index is not yet set. The call is made from the recovery code that is triggered in the MultiIndex constructor.
1,"Security issue - DigestScheme uses constant nonce count value. The nonce count value in DigestScheme is static (set to 00000001) and never changes.  (also seen as comment in said file).

This means that it fails against servers that correctly detect man-in-the-middle or replay attacks, leading to additional 401 requests (every second time), or such servers must be configured to turn such checks off (which is either poor security or poor for performance).

I suggest that at minimum, this count is incremented for every call to DigestScheme#createDigest.  It should also be an instance variable instead of a static, as it really relates to the challenge (assuming cases where instances are cached for reuse).  AtomicInteger is a good choice for implementing this counter.

See RFC 2617 chapters 3.2.2 and 3.2.3"
1,"Failing Node.unlock() might leave inconsistent transient state. Similar to issue JCR-538 but for Node.unlock():

If updating the lock related properties (jcr:lockIsDeep or jcr:lockOwner) fails e.g. due to missing permission, there might be inconsistent transient modifications pending."
1,"Read permission on parent node required to access an item's definition. If a session is granted all permissions on a given item B but lacks permission to read it's parent node A an attempt to
access the definition of B by means of Node.getDefinition or Property.getDefinition will fail with AccessDeniedException.

Similarly, the same session will not be able to modify that item B - e.g. add a child node in case it was a node - since implementation e.g. checks of that
item B isn't protected, which is determined by looking at the definition.

My feeling is, that the item definition should be accessible even if the parent node cannot be read."
1,"QueryStat getPopularQueries doesn't set the proper position. Embarrassing copy/paste error. I was updating the wrong array and the position info was never returned. 

This made any jmx client to fail with: 
at javax.management.openmbean.TabularDataSupport.checkValueAndIndex(TabularDataSupport.java:871) 
at javax.management.openmbean.TabularDataSupport.internalPut(TabularDataSupport.java:331) 
at javax.management.openmbean.TabularDataSupport.put(TabularDataSupport.java:323) 
at org.apache.jackrabbit.core.jmx.QueryStatManager.asTabularData(QueryStatManager.java:103)"
1,"Evict fixed NodePropBundle from cache. The BundleDbPersistenceManager only stores back fixed NodePropBundles in checkConsistency() but does not invalidate the cache, which may potentially contain a cached version of a NodePropBundle."
1,"Highlighter doesn't support NumericRangeQuery or deprecated RangeQuery. Sucks. Will throw a NullPointer exception. 

Only NumericRangeQuery will throw the exception.
RangeQuery just won't highlight."
1,Repository does not release all resources on shutdown. When Jackrabbit is shutdown some java.util.Timer threads are still running in the background even though no tasks are scheduled. This prevents the GC from collecting the classes when Jackrabbit is redeployed within a web application.
1,"SpellChecker file descriptor leak - no way to close the IndexSearcher used by SpellChecker internally. I can't find any way to close the IndexSearcher (and IndexReader) that
is being used by SpellChecker internally.

I've worked around this issue by keeping a single SpellChecker open
for each index, but I'd really like to be able to close it and
reopen it on demand without leaking file descriptors.

Could we add a close() method to SpellChecker that will close the
IndexSearcher and null the reference to it? And perhaps add some code
that reopens the searcher if the reference to it is null? Or would
that break thread safety of SpellChecker?

The attached patch adds a close method but leaves it to the user to
call setSpellIndex to reopen the searcher if desired."
1,"IndexReader.indexExists sometimes returns true when an index isn't present. If you open a writer on a new dir and prepareCommit but don't finish the commit, IndexReader.indexExists incorrectly returns true, because it just checks for whether a segments_N file is present and not whether it can be successfully read."
1,"Multipart post is broken. I tried to do HttpPost request with MultipartEntity, this request was encoded to wire with 3 line separators after header and not processed correctly by http server.
MultipartEntry add 1 extra line separator before write itself to wire. I'm not sure about standards, but it is at least not ""browser compatible"".

"
1,"InputContextImpl: cannot upload file larger than 2GB. If an entity is larger than 2GB, the Content-Length cannot be obtained by using getIntHeader because of integer overflow. One needs to parse the value of the header from string to long. This issue affects InputContextImpl.getContentLength() in org.apache.jackrabbit.webdav.io from webdav/java (the current behavior is that the header is converted from string to int by the servlet API, then from int to long by Jackrabbit)

Testcase: largefile from Litmus. (test 3 - large_put fails when the PUT request is received)"
1,"BooleanScorer2 fails to update this.doc when its the top scorer. When BooleanScorer2 runs the top collection loop (one of its
score(Collector)) methods, it uses a local ""doc"" var, ie:

{code}
public void score(Collector collector) throws IOException {
    collector.setScorer(this);
    int doc;
    while ((doc = countingSumScorer.nextDoc()) != NO_MORE_DOCS) {
      collector.collect(doc);
    }
}
{code}

The problem is, if the child collector calls scorer.doc() it will
always get -1.  Most Collectors don't actually call scorer.doc(), but
one important one that does is ScoreCachingWrapperScorer, as it uses
the doc to know when to invalidate its cache.  Since this always
returns -1, the ScoreCachingWrapperScorer keeps returning score=0.0 to
its caller, thus messing up a SortField.SCORE comparator instance if
it's included in the sort fields.
"
1,"ConcurrentModificationException during registration of nodetypes. During the registration of a set of nodetypes this exception may be encountered:

java.util.ConcurrentModificationException
        at org.apache.commons.collections.map.AbstractReferenceMap$ReferenceEntrySetIterator.checkMod(AbstractReferenceMap.java:761)
        at org.apache.commons.collections.map.AbstractReferenceMap$ReferenceEntrySetIterator.hasNext(AbstractReferenceMap.java:735)
        at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.notifyRegistered(NodeTypeRegistry.java:1750)
        at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.registerNodeTypes(NodeTypeRegistry.java:223)

It seems that the copying of the listeners triggered this exception:

    private void notifyRegistered(QName ntName) {
        // copy listeners to array to avoid ConcurrentModificationException
        NodeTypeRegistryListener[] la =
                new NodeTypeRegistryListener[listeners.size()];
        Iterator iter = listeners.values().iterator();
        int cnt = 0;
1750:   while (iter.hasNext()) {
            la[cnt++] = (NodeTypeRegistryListener) iter.next();
        }
        for (int i = 0; i < la.length; i++) {
            if (la[i] != null) {
                la[i].nodeTypeRegistered(ntName);
            }
        }
    }

The methods ""notifyReRegistered"" and ""notifyUnregistered"" will probably suffer from the same problem.

Reproduction of this exception may be tricky; it only occurred once in our application. It is probably a race condition: another thread might access the listeners during the copy. It may be helpful to use a debugger and set a breakpoint in the middle of the copy giving other threads the opportunity to access the listeners...

We think that a possible solution is the following:

    /**
     * Notify the listeners that a node type <code>ntName</code> has been registered.
     */
    private void notifyRegistered(QName ntName) {
        // copy listeners to array to avoid ConcurrentModificationException
    	NodeTypeRegistryListener[] la;
    	synchronized (listeners) {
            la = (NodeTypeRegistryListener[]) listeners.values().toArray(new NodeTypeRegistryListener[listeners.size()]);
		}

        for (int i = 0; i < la.length; i++) {
            if (la[i] != null) {
                la[i].nodeTypeRegistered(ntName);
            }
        }
    }



"
1,"Adding nodes from concurrently running sessions cause exceptions. Exceptions are thrown when trying to add child nodes to one parent node from different sessions running concurrently. One of the following exceptions is always thrown:

 * Exception in thread ""Thread-8"" java.lang.RuntimeException: javax.jcr.nodetype.ConstraintViolationException: /A/7 needs to be saved as well.
 * Exception in thread ""Thread-8"" java.lang.RuntimeException: javax.jcr.RepositoryException: /A: unable to update item.: Unable to
resolve path for item: 016b885a-64aa-45b9-a990-05cbabb4586f/{http://www.jcp.org/jcr/1.0}primaryType: Unable to resolve path for item: 016b885a-64aa-45b9-a990-05cbabb4586f/{http://www.jcp.org/jcr/1.0}primaryType

According to JCR-584 ""Improve handling of concurrent node modifications"" the following scenario ""session 1 adds or removes child node 'x', session 2 adds or removes child node 'y'"" should run gracefully, but the following test constantly fails:

public void testSync() throws Exception
{
       Node rootNode = getSession ().getRootNode ();
       rootNode.addNode (""A"");
       rootNode.save();

       final Session session1 = getRepository().login (new SimpleCredentials (""userName"", ""password"".toCharArray()));
       final Session session2 = getRepository().login (new SimpleCredentials (""userName"", ""password"".toCharArray()));

       Thread thread1 = new Thread (new Runnable()
       {
               public void run()
               {
                       try
                       {
                               addNodes (""A"", session1, 0);
                       }
                       catch (RepositoryException ex)
                       {
                               throw new RuntimeException (ex);
                       }
               }
       });

       Thread thread2 = new Thread (new Runnable()
       {
               public void run()
               {
                       try
                       {
                               addNodes (""A"", session2, 1001);
                       }
                       catch (RepositoryException ex)
                       {
                               throw new RuntimeException (ex);
                       }
               }
       });

       thread1.start();
       thread2.start();

       thread1.join();
       thread2.join();
}

private void addNodes (String parentName, Session session, int startIndex)
       throws RepositoryException
{
       Node parentNode = session.getRootNode().getNode (parentName);
       for (int i = startIndex; i < startIndex + 100; i++)
       {
               String name = Integer.toString (i);
               parentNode.addNode (name);
               parentNode.save();
       }
}

BTW: exceptions were also thrown when I tried to add nodes from one thread and remove some of them from another one. Each thread used it's own session, each node had unique name.

"
1,"IndexWriter.setMaxMergeDocs gives non-backwards-compatible exception ""out of the box"". Yonik hit this (see details in LUCENE-994): because we have switched
to LogByteSizeMergePolicy by default in IndexWriter, which uses MB to
limit max size of merges (setMaxMergeMB), when an existing app calls
setMaxMergeDocs (or getMaxMergeDocs) it will hit an
IllegalArgumentException on dropping in the new JAR.

I think the simplest solution is to fix LogByteSizeMergePolicy to
allow setting of the max by either MB or by doc count, just like how
in LUCENE-1007 allows flushing by either MB or docCount or both."
1,"Bad request vulnerability . The HttpParser.readRawLine() method below has no guard code against a post without a end-of-line.  A large post of data without ""\n"" will be read into the ByteArray.  If this post is large enough, it will deplete the system of free memory.  A DOS attack could easily be played out by submitting several of these post at once.   readRawLine should decide that its not reading character data (basically because character data should never show up over something like a megabyte a line) and report an error.  

   /**
     * Return byte array from an (unchunked) input stream.
     * Stop reading when <tt>""\n""</tt> terminator encountered 
     * If the stream ends before the line terminator is found,
     * the last part of the string will still be returned. 
     * If no input data available, <code>null</code> is returned.
     *
     * @param inputStream the stream to read from
     *
     * @throws IOException if an I/O problem occurs
     * @return a byte array from the stream
     */
    public static byte[] readRawLine(InputStream inputStream) throws IOException {
        LOG.trace(""enter HttpParser.readRawLine()"");

        ByteArrayOutputStream buf = new ByteArrayOutputStream();
        int ch;
        while ((ch = inputStream.read()) >= 0) {
            buf.write(ch);
            if (ch == '\n') { // be tolerant (RFC-2616 Section 19.3)
                break;
            }
        }
        if (buf.size() == 0) {
            return null;
        }
        return buf.toByteArray();
    }"
1,"NullPointerException from SegmentInfos.FindSegmentsFile.run() if FSDirectory.list() returns NULL . Found this bug while running unit tests to verify an upgrade of our system from 1.4.3 to 2.1.0.  This bug did *not* occur during 1.4.3, it is new to 2.x (I'm pretty sure it's 2.1-only)

If the index directory gets deleted out from under Lucene after the FSDirectory has been created, then attempts to open an IndexWriter or IndexReader will result in an NPE.  Lucene should be throwing an IOException in this case.

Repro:
    1) Create an FSDirectory pointing somewhere in the filesystem (e.g. /foo/index/1)
    2) rm -rf the parent dir (rm -rf /foo/index)
    3) Try to open an IndexReader

Result: NullPointerException on line ""for(int i=0;i<files.length;i++) { "" -- 'files' is NULL.
 
Expect: IOException


....  

This is happening because of a missing NULL check in SegmentInfos$FindSegmentsFile.run():

        if (0 == method) {
          if (directory != null) {
            files = directory.list();
          } else {
            files = fileDirectory.list();
          }

          gen = getCurrentSegmentGeneration(files);

          if (gen == -1) {
            String s = """";
            for(int i=0;i<files.length;i++) { 
              s += "" "" + files[i];
            }
            throw new FileNotFoundException(""no segments* file found: files:"" + s);
          }
        }


The FSDirectory constructor will make sure the index dir exists, but if it is for some reason deleted out from underneath Lucene after the FSDirectory is instantiated, then java.io.File.list() will return NULL.  Probably better to fix FSDirectory.list() to just check for null and return a 0-length array:

(in org/apache/lucene/store/FSDirectory.java)
314c314,317
<         return directory.list(IndexFileNameFilter.getFilter());
---
>     String[] toRet = directory.list(IndexFileNameFilter.getFilter());
>     if (toRet == null)
>         return new String[]{};
>     return toRet;
"
1,Several DocsEnum / DocsAndPositionsEnum return wrong docID when next() / advance(int) return NO_MORE_DOCS. During work on LUCENE-2878 I found some minor problems in PreFlex and Pulsing Codec - they are not returning NO_MORE_DOCS but the last docID instead from DocsEnum#docID() when next() or advance(int) returned NO_MORE_DOCS. The JavaDoc clearly says that it should return NO_MORE_DOCS.
1,"intermittant exceptions in TestConcurrentMergeScheduler. 
The TestConcurrentMergeScheduler throws intermittant exceptions that
do not result in a test failure.

The exception happens in the ""testNoWaitClose()"" test, which repeated
tests closing an IndexWriter with ""false"", meaning abort any
still-running merges.  When a merge is aborted it can hit various
exceptions because the files it is reading and/or writing have been
deleted, so we ignore these exceptions.

The bug was just that we were failing to properly check whether the
running merge was actually aborted because of a scoping issue of the
""merge"" variable in ConcurrentMergeScheduler.  So the exceptions are
actually ""harmless"".  Thanks to Ning for spotting it!

"
1,"TermVectors corruption case when autoCommit=false. I took Yonik's awesome test case (TestStressIndexing2) and extended it to also compare term vectors, and, it's failing.

I still need to track down why, but it seems likely a separate issue."
1,SQL2 ISDESCENDANTNODE can throw BooleanQuery#TooManyClauses if there are too many matching child nodes. Running a query that has a ISDESCENDANTNODE clause can easily go over the max clause limit from lucene's BooleanQuery when there's a bigger hierarchy involved.
1,"SSL does not seem to work at all. Whenever I try to request content via https I get this exception:


Exception in thread ""main"" javax.net.ssl.SSLException: hostname in certificate didn't match: <140.211.11.131> != <*.apache.org>
	at org.apache.http.conn.ssl.AbstractVerifier.verify(AbstractVerifier.java:220)
	at org.apache.http.conn.ssl.BrowserCompatHostnameVerifier.verify(BrowserCompatHostnameVerifier.java:54)
	at org.apache.http.conn.ssl.AbstractVerifier.verify(AbstractVerifier.java:149)
	at org.apache.http.conn.ssl.AbstractVerifier.verify(AbstractVerifier.java:130)
	at org.apache.http.conn.ssl.SSLSocketFactory.createSocket(SSLSocketFactory.java:399)
	at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:143)
	at org.apache.http.impl.conn.AbstractPoolEntry.open(AbstractPoolEntry.java:149)
	at org.apache.http.impl.conn.AbstractPooledConnAdapter.open(AbstractPooledConnAdapter.java:108)
	at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:415)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:641)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:576)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:554)
	at HttpsTest.fails(HttpsTest.java:25)
	at HttpsTest.main(HttpsTest.java:12)


I can reproduce this whith the following code:


import org.apache.http.client.HttpClient;
import org.apache.http.client.methods.HttpGet;
import org.apache.http.impl.client.DefaultHttpClient;

public class HttpsTest {

    public static void main(final String[] args) throws Exception {
        final HttpClient client = new DefaultHttpClient();
        final HttpGet req = new HttpGet(""https://www.apache.org"");
        client.execute(req);
    }

}
"
1,"problems with IR's readerFinishedListener. There are two major problems:
1. The listener api does not really apply all indexreaders. for example segmentreaders dont fire it on close, only segmentcorereaders. this is wrong, a segmentcorereader is *not* an indexreader. Furthermore, if you register it on a top-level reader you get events for anything under the reader tree (sometimes, unless they are segmentreaders as mentioned above, where it doesnt work correctly at all).
2. Furthermore your listener is 'passed along' in a viral fashion from clone() and reopen(). This means for example, if you are trying to listen to readers in NRT search you are just accumulating reader listeners, all potentially keeping references to old indexreaders (because, in order to deal with #1 your listener must 'keep' a reference to the IR it was registered on, so it can check if thats *really* the one).

We should discuss how to fix #1. 

I will create a patch for #2 shortly and commit it, its just plain wrong.
"
1,"JVM bug 4949631 causes BufferOverflowException in HttpMethodBase.getResponseBodyAsString. ava.nio.BufferOverflowException
        at java.nio.charset.CoderResult.throwException(CoderResult.java:259)
        at java.lang.StringCoding$CharsetSD.decode(StringCoding.java:188)
        at java.lang.StringCoding.decode(StringCoding.java:224)
        at java.lang.String.<init>(String.java:320)
        at
org.apache.commons.httpclient.HttpConstants.getContentString(HttpConstants.java:199)
        at
org.apache.commons.httpclient.HttpConstants.getContentString(HttpConstants.java:233)
        at
org.apache.commons.httpclient.HttpMethodBase.getResponseBodyAsString(HttpMethodBase.java:735)


This seems to be caused by a known JVM bug:
http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4949631

Strings over 16Mb can cause the problem.   Some workarounds are listed, the
essence being to split the string and call getBytes on each piece and reassemble
with a ByteBuffer."
1,"always apply position increment gap between values. I'm doing some fancy stuff with span queries that is very sensitive to term positions.  I discovered that the position increment gap on indexing is only applied between values when there are existing terms indexed for the document.  I suspect this logic wasn't deliberate, it's just how its always been for no particular reason.  I think it should always apply the gap between fields.  Reference DocInverterPerField.java line 82:

if (fieldState.length > 0)
          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);

This is checking fieldState.length.  I think the condition should simply be:  if (i > 0).
I don't think this change will affect anyone at all but it will certainly help me.  Presently, I can either change this line in Lucene, or I can put in a hack so that the first value for the document is some dummy value which is wasteful."
1,"Missing synchronization in InternalVersionHistoryImpl. The InternalVersionHistoryImpl objects can be accessed (and modified) concurrently by multiple sessions, which can in some rare cases result in corruption in the internal cache map data structures. Access to these cache maps should be properly synchronized."
1,"registration of new namespace does not respect existing session mappings. consider the following (starting with a default namespace registry):

// remap nt namespace
Session.setNamespacePrefix(""foobar"", ""http://www.jcp.org/jcr/nt/1.0"");

// create new namespace
NamespaceRegistry.registerNamespace(""foobar"", ""http://www.foo.org/bar/1.0"");

now the session used above that remapped the nt namespace has an ambigous namespace mapping:
foobar --> ""http://www.jcp.org/jcr/nt/1.0""
""http://www.jcp.org/jcr/nt/1.0"" --> foobar
""http://www.foo.org/bar/1.0"" --> foobar

i.e. the new foobar namespace is hidden for this session. either the registration should not work, or an automatic prefix is to be defined in all local session mappings.

"
1,"Large file download over webdav causes exception. Downloading a large file (>2GB) from webdav causes an exception.

(Note: uploading the file works ok, when jackrabbit is configured to use the filesystem DataStore.)

When trying to retrieve the file with e.g. ""wget"", we get the following error:

Gozer:Desktop greg$ wget --http-user=xxx --http-passwd=xxx http://localhost:8080/jackrabbit/repository/workbench/pkgs/demo/zip/zips/largetest-1.zip
--08:59:50--  http://localhost:8080/jackrabbit/repository/workbench/pkgs/demo/zip/zips/largetest-1.zip
           => `largetest-1.zip'
Resolving localhost... done.
Connecting to localhost[127.0.0.1]:8080... connected.
HTTP request sent, awaiting response... 500 For input string: ""3156213760""
09:04:53 ERROR 500: For input string: ""3156213760"".

In the server log we see this:

06.03.2009 08:59:50 *INFO * RepositoryImpl: SecurityManager = class org.apache.jackrabbit.core.security.simple.SimpleSecurityManager (RepositoryImpl.java, line 432)
2009-03-06 09:04:53.822::WARN:  /jackrabbit/repository/workbench/pkgs/demo/zip/zips/largetest-1.zip
java.lang.NumberFormatException: For input string: ""3156213760""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)
	at java.lang.Integer.parseInt(Integer.java:459)
	at java.lang.Integer.parseInt(Integer.java:497)
	at org.apache.jackrabbit.webdav.io.OutputContextImpl.setContentLength(OutputContextImpl.java:60)
	at org.apache.jackrabbit.server.io.ExportContextImpl.informCompleted(ExportContextImpl.java:192)
	at org.apache.jackrabbit.server.io.IOManagerImpl.exportContent(IOManagerImpl.java:157)
	at org.apache.jackrabbit.webdav.simple.DavResourceImpl.spool(DavResourceImpl.java:332)
	at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.spoolResource(AbstractWebdavServlet.java:422)
	at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.doGet(AbstractWebdavServlet.java:388)
	at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.execute(AbstractWebdavServlet.java:229)
	at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.service(AbstractWebdavServlet.java:196)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:502)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:363)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:417)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
	at org.mortbay.jetty.handler.HandlerCollection.handle(HandlerCollection.java:114)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:324)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:534)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:864)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:533)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:207)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:403)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:409)
	at org.mortbay.thread.BoundedThreadPool$PoolThread.run(BoundedThreadPool.java:451)


The problem seems to lie in OutputContextImpl.java it makes the mistake of potentially trying to parse a Long as an Integer, here: http://svn.apache.org/repos/asf/jackrabbit/trunk/jackrabbit-webdav/src/main/java/org/apache/jackrabbit/webdav/io/OutputContextImpl.java

in the method setContentLength(long contentLength):

public void setContentLength(long contentLength) {
       int length = Integer.parseInt(contentLength + """");
       if (length >= 0) {
           response.setContentLength(length);
       }
   }

I'm not sure, but a fix might be like this:

public void setContentLength(long contentLength) {
       if(contentLength <= Integer.MAX_VALUE && contentLength >= 0) {
           response.setContentLength((int) contentLength);
       }else if (contentLength >  Integer.MAX_VALUE) {
            response.addHeader(""Content-Length"", Long.toString(contentLength));
       }
   }

This would at least set the Content-Length header, and in some preliminary tests does seem to allow downloading the files."
1,"IndexWriter.addIndexes* can deadlock in rare cases. In somewhat rare cases it's possible for addIndexes to deadlock
because it is a synchronized method.

Normally the merges that are necessary for addIndexes are done
serially (with the primary thread) because they involve segments from
an external directory.  However, if mergeFactor of these merges
complete then a merge becomes necessary for the merged segments, which
are not external, and so it can run in the background.  If too many BG
threads need to run (currently > 4) then the ""pause primary thread""
approach adopted in LUCENE-1164 will deadlock, because the addIndexes
method is holding a lock on IndexWriter.

This was appearing as a intermittant deadlock in the
TestIndexWriterMerging test case.

This issue is not present in 2.3 (it was caused by LUCENE-1164).

The solution is to shrink the scope of synchronization: don't
synchronize on the whole method & wrap synchronized(this) in the right
places inside the methods."
1,"The deprecated constructor of BooleanClause does not set new state. Nick Burch reported this on lucene-user. 
 
Patch will follow. 
 
Regards, 
Paul Elschot"
1,"SpanMultiTermQueryWrapper with Prefix Query issue. If we try to do a search with SpanQuery and a PrefixQuery this message is returned:

""You can only use SpanMultiTermQueryWrapper with a suitable SpanRewriteMethod.""

The problem is in the WildcardQuery rewrite function.

If the wildcard query is a prefix, a new prefix query is created, the rewrite method is set with the SpanRewriteMethod and the prefix query is returned.

But, that's the rewritten prefix query which should be returned:

-      return rewritten;
+      return rewritten.rewrite(reader);

I will attach a patch with a unit test included.



"
1,"When reopen returns a new IndexReader, both IndexReaders may now control the lifecycle of the underlying Directory which is managed by reference counting. Rough summary. Basically, FSDirectory tracks references to FSDirectory and when IndexReader.reopen shares a Directory with a created IndexReader and closeDirectory is true, FSDirectory's ref management will see two decrements for one increment. You can end up getting an AlreadyClosed exception on the Directory when the IndexReader is open.

I have a test I'll put up. A solution seems fairly straightforward (at least in what needs to be accomplished)."
1,CompactNodeTypeDefReader does not recognise MIXIN ORDERABLE sequence. the code in 'doOptions' misses to set the setOrderableChildNodes flag if the order of the tokens is MIXIN ORDERABLE.
1,"Index corruption when using RAMDirectory( Directory) constructor. LUCENE-475 introduced a bug in creating RAMDirectories for large indexes. It truncates the length of the file to an int, from its original long value. Any files that are larger than an int are truncated. Patch to fix is attached."
1,"SingleClientConnectionManager Needs to Recreate UniquePoolEntry. Due to the change yesterday of adding some state into DefaultClientConnection (remembering when shutdown was called & aborting the next opening), SingeClientConnectionManager now breaks when subsequent requests are performed if the first one encountered an exception or was aborted.  

Attaching a patch with the fix + a testcase (that previously failed)."
1,"Stale connection check does not work with IBM JSSE/JRE. OS: Windows/AIX
JRE: IBM JRE 1.4.1
JSSE: IBM's implementation (SSLite?)
HttpClient Library: 2.0.2 release

My code enabled connection pooling feature to gain performance improvement in 
the SSL Handshake area. The code works perfectly on Sun JRE 1.4.2 with a think 
time of 60seconds between requests, but the same code fails on IBM JRE. On IBM 
JRE, the code fails to detech stale connections, thus causing down the stream 
setSoTimeout() call to fail.

Further debugging into the library code revealed difference in the way the 
HTTPConnection.isStale() behaves. With in that method, particularly, the 
inputStream.isAvailable() method returns 0 with Sun JRE but -1 with IBM JRE.

I made a small code change to HttpConnection.isStale() method by moving the try
{}finally{} block outside of the if(inputStream.isAvailable()==0) check in the 
following code and BINGO, everything started working on IBM JVMs. It did not 
break anything on Suns JVM.

============== CODE BEGIN
    protected boolean isStale() {
    	LOG.debug(""##SUBBA## HttpConnection.isStale() got called. soTimeout="" 
+ soTimeout);
        boolean isStale = true;
        if (isOpen) {
        	LOG.debug(""##SUBBA## HttpConnection.isStale() got called. 
isOpen="" + isOpen);        	
            // the connection is open, but now we have to see if we can read it
            // assume the connection is not stale.
            isStale = false;

                try {         
                    if (inputStream.available() == 0) {		// ALWAYS 
RETURNS -1 on IBM JVM  0 on SUN
                    	
		  // try {		// SUBBA  MOVED OUTSIDE IF
	                	socket.setSoTimeout(1);
	                  	LOG.debug(""##SUBBA## HttpConnection.isStale() 
got called. setSoTimeout(1)"");                    	
	                    
	                    inputStream.mark(1);
	                    int byteRead = inputStream.read();
	                	LOG.debug(""##SUBBA## HttpConnection.isStale() 
got called. bytesRead="" + byteRead);                    	
	                    
	                    if (byteRead == -1) {
	                    	LOG.debug(""##SUBBA## HttpConnection.isStale() 
got called. SETTING isStale to TRUE HERE"");                    	
	                    	
	                        // again - if the socket is reporting all data 
read,
	                        // probably stale
	                        isStale = true;
	                    } else {
	                        inputStream.reset();
	                    }
		    // SUBBA  MOVED OUTSIDE IF
                //} finally {
                //	LOG.debug(""##SUBBA## HttpConnection.isStale() got 
called. finally block - BEGIN "" + soTimeout);                    	
                //    socket.setSoTimeout(soTimeout);
                //	LOG.debug(""##SUBBA## HttpConnection.isStale() got 
called. finally block - DONE"");                        
               // }

	
                    }                        
                } finally {
                	LOG.debug(""##SUBBA## HttpConnection.isStale() got 
called. finally block - BEGIN "" + soTimeout);                    	
                    socket.setSoTimeout(soTimeout);
                	LOG.debug(""##SUBBA## HttpConnection.isStale() got 
called. finally block - DONE"");                        
                }
.....
.....
.....
========================== CODE END


I've attached logs captured before and after the change on both the JRE's for 
your review:

==================================
IBMs LOG (after change):
==================================
<Jun 10, 2005 1:26:55 PM EDT> <DEBUG> 
<apache.commons.httpclient.MultiThreadedHttpConnectionManager:700> <Getting 
free connection, hostConfig=HostConfiguration
[host=uatservices30.ilab.fnfismd.com, protocol=https:443, port=443]>
<Jun 10, 2005 1:26:55 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:492> <##SUBBA## 
HttpConnection.isStale() got called. soTimeout=0>
<Jun 10, 2005 1:26:55 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:495> <##SUBBA## 
HttpConnection.isStale() got called. isOpen=true>
<Jun 10, 2005 1:26:55 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:500> <##SUBBA## 
HttpConnection.isStale() got called. [class 
java.io.BufferedInputStream].available=-1>
<Jun 10, 2005 1:26:55 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:523> <##SUBBA## 
HttpConnection.isStale() got called. finally block - BEGIN 0>
<Jun 10, 2005 1:26:55 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:532> <An error occurred while 
reading from the socket, is appears to be stale>
java.net.SocketException: Socket is closed
	at java.net.Socket.setSoTimeout(Socket.java:927)
	at com.ibm.sslite.bf.setSoTimeout(Unknown Source)
	at com.ibm.jsse.bg.setSoTimeout(Unknown Source)
	at org.apache.commons.httpclient.HttpConnection.isStale
(HttpConnection.java:524)
	at org.apache.commons.httpclient.HttpConnection.isOpen
(HttpConnection.java:436)
	at 
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpConnection
Adapter.isOpen(MultiThreadedHttpConnectionManager.java:1122)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:626)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:497)
	at 
com.touchpoint.pia.services.transactions.msp.ApacheHttpClient.invokeRequest
(ApacheHttpClient.java:69)
	at 
com.touchpoint.pia.services.transactions.msp.MsWSManager.invokeRequest
(MsWSManager.java:86)
	at 
com.touchpoint.pia.services.transactions.msp.MsWSManager.invokeRequestWithPaylo
ad(MsWSManager.java:114)
	at com.touchpoint.pia.services.transactions.msp.MsWSManager.main
(MsWSManager.java:179)
<Jun 10, 2005 1:26:55 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:540> <##SUBBA## 
HttpConnection.isStale() return=true>
<Jun 10, 2005 1:26:55 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:437> <Connection is stale, 
closing...>

==================================
IBMs LOG (before change):
==================================
<Jun 10, 2005 1:07:29 PM EDT> <DEBUG> 
<apache.commons.httpclient.MultiThreadedHttpConnectionManager:666> <enter 
HttpConnectionManager.ConnectionPool.getHostPool(HostConfiguration)>
<Jun 10, 2005 1:07:29 PM EDT> <DEBUG> 
<apache.commons.httpclient.MultiThreadedHttpConnectionManager:700> <Getting 
free connection, hostConfig=HostConfiguration
[host=uatservices30.ilab.fnfismd.com, protocol=https:443, port=443]>
<Jun 10, 2005 1:07:29 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:492> <##SUBBA## 
HttpConnection.isStale() got called.>
<Jun 10, 2005 1:07:29 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:495> <##SUBBA## 
HttpConnection.isStale() got called. isOpen=true>
<Jun 10, 2005 1:07:29 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:500> <##SUBBA## 
HttpConnection.isStale() got called. [class 
java.io.BufferedInputStream].available=-1>
<Jun 10, 2005 1:07:29 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:538> <##SUBBA## 
HttpConnection.isStale() return=false>
<Jun 10, 2005 1:07:29 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:599> <HttpConnection.setSoTimeout(0)>
<Jun 10, 2005 1:07:29 PM EDT> <WARN> 
<apache.commons.httpclient.HttpConnection:607> <##SUBBA## Socket Exception>
java.net.SocketException: Socket is closed
	at java.net.Socket.setSoTimeout(Socket.java:927)
	at com.ibm.sslite.bf.setSoTimeout(Unknown Source)
	at com.ibm.jsse.bg.setSoTimeout(Unknown Source)
	at org.apache.commons.httpclient.HttpConnection.setSoTimeout
(HttpConnection.java:603)
	at 
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpConnection
Adapter.setSoTimeout(MultiThreadedHttpConnectionManager.java:1296)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:633)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:497)
	at 
com.touchpoint.pia.services.transactions.msp.ApacheHttpClient.invokeRequest
(ApacheHttpClient.java:69)
	at 
com.touchpoint.pia.services.transactions.msp.MsWSManager.invokeRequest
(MsWSManager.java:86)
	at 
com.touchpoint.pia.services.transactions.msp.MsWSManager.invokeRequestWithPaylo
ad(MsWSManager.java:114)
	at com.touchpoint.pia.services.transactions.msp.MsWSManager.main
(MsWSManager.java:179)
<Jun 10, 2005 1:07:29 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:1151> <enter 
HttpConnection.releaseConnection()>
<Jun 10, 2005 1:07:29 PM EDT> <DEBUG> 
<apache.commons.httpclient.MultiThreadedHttpConnectionManager:513> <enter 
HttpConnectionManager.releaseConnection(HttpConnection)>
<Jun 10, 2005 1:07:29 PM EDT> <DEBUG> 
<apache.commons.httpclient.MultiThreadedHttpConnectionManager:791> <Freeing 
connection, hostConfig=HostConfiguration[host=uatservices30.ilab.fnfismd.com, 
protocol=https:443, port=443]>
<Jun 10, 2005 1:07:29 PM EDT> <DEBUG> 
<apache.commons.httpclient.MultiThreadedHttpConnectionManager:666> <enter 
HttpConnectionManager.ConnectionPool.getHostPool(HostConfiguration)>
<Jun 10, 2005 1:07:29 PM EDT> <DEBUG> 
<apache.commons.httpclient.MultiThreadedHttpConnectionManager:774> <Notifying 
no-one, there are no waiting threads>
java.net.SocketException: Socket is closed
	at java.net.Socket.setSoTimeout(Socket.java:927)
	at com.ibm.sslite.bf.setSoTimeout(Unknown Source)
	at com.ibm.jsse.bg.setSoTimeout(Unknown Source)
	at org.apache.commons.httpclient.HttpConnection.setSoTimeout
(HttpConnection.java:603)
	at 
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpConnection
Adapter.setSoTimeout(MultiThreadedHttpConnectionManager.java:1296)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:633)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:497)
	at 
com.touchpoint.pia.services.transactions.msp.ApacheHttpClient.invokeRequest
(ApacheHttpClient.java:69)
	at 
com.touchpoint.pia.services.transactions.msp.MsWSManager.invokeRequest
(MsWSManager.java:86)
	at 
com.touchpoint.pia.services.transactions.msp.MsWSManager.invokeRequestWithPaylo
ad(MsWSManager.java:114)
	at com.touchpoint.pia.services.transactions.msp.MsWSManager.main
(MsWSManager.java:179)
java.net.SocketException: Socket is closed
	at java.net.Socket.setSoTimeout(Socket.java:927)
	at com.ibm.sslite.bf.setSoTimeout(Unknown Source)
	at com.ibm.jsse.bg.setSoTimeout(Unknown Source)
	at org.apache.commons.httpclient.HttpConnection.setSoTimeout
(HttpConnection.java:603)
	at 
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpConnection
Adapter.setSoTimeout(MultiThreadedHttpConnectionManager.java:1296)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:633)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:497)
	at 
com.touchpoint.pia.services.transactions.msp.ApacheHttpClient.invokeRequest
(ApacheHttpClient.java:69)
	at 
com.touchpoint.pia.services.transactions.msp.MsWSManager.invokeRequest
(MsWSManager.java:86)
	at 
com.touchpoint.pia.services.transactions.msp.MsWSManager.invokeRequestWithPaylo
ad(MsWSManager.java:114)
	at com.touchpoint.pia.services.transactions.msp.MsWSManager.main
(MsWSManager.java:179)
Exception in thread ""main"" java.net.SocketException: Socket is closed
	at java.net.Socket.setSoTimeout(Socket.java:927)
	at com.ibm.sslite.bf.setSoTimeout(Unknown Source)
	at com.ibm.jsse.bg.setSoTimeout(Unknown Source)
	at org.apache.commons.httpclient.HttpConnection.setSoTimeout
(HttpConnection.java:603)
	at 
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpConnection
Adapter.setSoTimeout(MultiThreadedHttpConnectionManager.java:1296)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:633)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:497)
	at 
com.touchpoint.pia.services.transactions.msp.ApacheHttpClient.invokeRequest
(ApacheHttpClient.java:69)
	at 
com.touchpoint.pia.services.transactions.msp.MsWSManager.invokeRequest
(MsWSManager.java:86)
	at 
com.touchpoint.pia.services.transactions.msp.MsWSManager.invokeRequestWithPaylo
ad(MsWSManager.java:114)
	at com.touchpoint.pia.services.transactions.msp.MsWSManager.main
(MsWSManager.java:179)


============================================
**SUNs LOG (after change = before change):
============================================
<Jun 10, 2005 1:25:12 PM EDT> <DEBUG> 
<apache.commons.httpclient.MultiThreadedHttpConnectionManager:700> <Getting 
free connection, hostConfig=HostConfiguration
[host=uatservices30.ilab.fnfismd.com, protocol=https:443, port=443]>
<Jun 10, 2005 1:25:12 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:492> <##SUBBA## 
HttpConnection.isStale() got called. soTimeout=0>
<Jun 10, 2005 1:25:12 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:495> <##SUBBA## 
HttpConnection.isStale() got called. isOpen=true>
<Jun 10, 2005 1:25:12 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:500> <##SUBBA## 
HttpConnection.isStale() got called. [class 
java.io.BufferedInputStream].available=0>
<Jun 10, 2005 1:25:12 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:506> <##SUBBA## 
HttpConnection.isStale() got called. setSoTimeout(1)>
<Jun 10, 2005 1:25:12 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:510> <##SUBBA## 
HttpConnection.isStale() got called. bytesRead=-1>
<Jun 10, 2005 1:25:12 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:513> <##SUBBA## 
HttpConnection.isStale() got called. SETTING isStale to TRUE HERE>
<Jun 10, 2005 1:25:12 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:523> <##SUBBA## 
HttpConnection.isStale() got called. finally block - BEGIN 0>
<Jun 10, 2005 1:25:12 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:532> <An error occurred while 
reading from the socket, is appears to be stale>
java.net.SocketException: Socket Closed
	at java.net.PlainSocketImpl.setOption(PlainSocketImpl.java:177)
	at java.net.Socket.setSoTimeout(Socket.java:924)
	at com.sun.net.ssl.internal.ssl.SSLSocketImpl.setSoTimeout(DashoA12275)
	at org.apache.commons.httpclient.HttpConnection.isStale
(HttpConnection.java:524)
	at org.apache.commons.httpclient.HttpConnection.isOpen
(HttpConnection.java:436)
	at 
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpConnection
Adapter.isOpen(MultiThreadedHttpConnectionManager.java:1122)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:626)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:497)
	at 
com.touchpoint.pia.services.transactions.msp.ApacheHttpClient.invokeRequest
(ApacheHttpClient.java:69)
	at 
com.touchpoint.pia.services.transactions.msp.MsWSManager.invokeRequest
(MsWSManager.java:86)
	at 
com.touchpoint.pia.services.transactions.msp.MsWSManager.invokeRequestWithPaylo
ad(MsWSManager.java:114)
	at com.touchpoint.pia.services.transactions.msp.MsWSManager.main
(MsWSManager.java:179)
<Jun 10, 2005 1:25:12 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:540> <##SUBBA## 
HttpConnection.isStale() return=true>"
1,"IndexWriter.commit()  does not update the index version. IndexWriter.commit() can update the index *version* and *generation* but the update of *version* is lost.
As result added documents are not seen by IndexReader.reopen().
(There might be other side effects that I am not aware of).
The fix is 1 line - update also the version in SegmentsInfo.updateGeneration().
(Finding this line involved more lines though... :-) )
"
1,"Error downloading text file with gzip content encoding. Hello I am getting an exception when I try to download certain files.

I don't have control over the host server, only the client.  Here's my client code:

		HttpParams params = new BasicHttpParams();
		params.setParameter(CoreConnectionPNames.CONNECTION_TIMEOUT, 300000L);
		params.setParameter(ClientPNames.HANDLE_REDIRECTS, true);

		// This client indicates to servers that it will support 'gzip'
		// and 'deflate' compressed responses.
		ContentEncodingHttpClient.setDefaultHttpParams(params);
		ContentEncodingHttpClient client = new ContentEncodingHttpClient();

		if (user != null && password != null) {
			String hostname = url.getHost();
			HttpHost hostHttp = new HttpHost(hostname, 80, ""http"");
			HttpHost hostHttps = new HttpHost(hostname, 443, ""https"");
			client.getCredentialsProvider().setCredentials(
			        new AuthScope(hostname, 80), 
			        new UsernamePasswordCredentials(user, password));
	
			client.getCredentialsProvider().setCredentials(
			        new AuthScope(hostname, 443), 
			        new UsernamePasswordCredentials(user, password));
	
			// Create AuthCache instance
			AuthCache authCache = new BasicAuthCache();
			// Generate BASIC scheme object and add it to the local auth cache
			BasicScheme basicAuth = new BasicScheme();
			authCache.put(hostHttp, basicAuth);
			authCache.put(hostHttps, basicAuth);
	
			// Add AuthCache to the execution context
			BasicHttpContext localcontext = new BasicHttpContext();
			localcontext.setAttribute(ClientContext.AUTH_CACHE, authCache);
		}
		HttpGet httpget = new HttpGet(url.toString());
		httpget.setHeader(""If-Modified-Since"", lastModified);


		HttpResponse response = client.execute(httpget);
		responseCode = response.getStatusLine().getStatusCode();
		HttpEntity entity = response.getEntity();
		if (responseCode == HttpStatus.SC_NOT_MODIFIED) {
			
		} else if (responseCode == HttpStatus.SC_OK && entity != null) {
			outStream = new BufferedOutputStream(new FileOutputStream(outFilename));
			entity.writeTo(outStream);
		}

Here's the log output:

DEBUG [2011-08-02 01:23:01,031] [org.apache.http.impl.conn.SingleClientConnManager:212] Get connection for route HttpRoute[{}->http://<host>]
DEBUG [2011-08-02 01:23:01,036] [org.apache.http.impl.conn.DefaultClientConnectionOperator:145] Connecting to <host>/<IP>:80
DEBUG [2011-08-02 01:23:01,057] [org.apache.http.client.protocol.RequestAddCookies:132] CookieSpec selected: best-match
DEBUG [2011-08-02 01:23:01,057] [org.apache.http.client.protocol.RequestAuthCache:75]   Auth cache not set in the context
DEBUG [2011-08-02 01:23:01,058] [org.apache.http.impl.client.DefaultRequestDirector:631]        Attempt 1 to execute request
DEBUG [2011-08-02 01:23:01,058] [org.apache.http.impl.conn.DefaultClientConnection:264] Sending request: GET <file> HTTP/1.1
DEBUG [2011-08-02 01:23:01,058] [org.apache.http.impl.conn.Wire:63]     >> ""GET <file> HTTP/1.1[\r][\n]""
DEBUG [2011-08-02 01:23:01,058] [org.apache.http.impl.conn.Wire:63]     >> ""If-Modified-Since: Mon, 01 Aug 2011 18:26:09 CEST[\r][\n]""
DEBUG [2011-08-02 01:23:01,059] [org.apache.http.impl.conn.Wire:63]     >> ""Host: <host>[\r][\n]""
DEBUG [2011-08-02 01:23:01,059] [org.apache.http.impl.conn.Wire:63]     >> ""Connection: Keep-Alive[\r][\n]""
DEBUG [2011-08-02 01:23:01,059] [org.apache.http.impl.conn.Wire:63]     >> ""User-Agent: Apache-HttpClient/4.1.1 (java 1.5)[\r][\n]""
DEBUG [2011-08-02 01:23:01,059] [org.apache.http.impl.conn.Wire:63]     >> ""Accept-Encoding: gzip,deflate[\r][\n]""
DEBUG [2011-08-02 01:23:01,060] [org.apache.http.impl.conn.Wire:63]     >> ""[\r][\n]""
DEBUG [2011-08-02 01:23:01,060] [org.apache.http.impl.conn.DefaultClientConnection:268] >> GET <file> HTTP/1.1
DEBUG [2011-08-02 01:23:01,060] [org.apache.http.impl.conn.DefaultClientConnection:271] >> If-Modified-Since: Mon, 01 Aug 2011 18:26:09 CEST
DEBUG [2011-08-02 01:23:01,060] [org.apache.http.impl.conn.DefaultClientConnection:271] >> Host: <host>
DEBUG [2011-08-02 01:23:01,061] [org.apache.http.impl.conn.DefaultClientConnection:271] >> Connection: Keep-Alive
DEBUG [2011-08-02 01:23:01,061] [org.apache.http.impl.conn.DefaultClientConnection:271] >> User-Agent: Apache-HttpClient/4.1.1 (java 1.5)
DEBUG [2011-08-02 01:23:01,061] [org.apache.http.impl.conn.DefaultClientConnection:271] >> Accept-Encoding: gzip,deflate
DEBUG [2011-08-02 01:23:01,085] [org.apache.http.impl.conn.Wire:63]     << ""HTTP/1.1 200 OK[\r][\n]""
DEBUG [2011-08-02 01:23:01,086] [org.apache.http.impl.conn.Wire:63]     << ""Server: nginx/0.8.54[\r][\n]""
DEBUG [2011-08-02 01:23:01,086] [org.apache.http.impl.conn.Wire:63]     << ""Date: Mon, 01 Aug 2011 23:23:01 GMT[\r][\n]""
DEBUG [2011-08-02 01:23:01,086] [org.apache.http.impl.conn.Wire:63]     << ""Content-Type: text/plain[\r][\n]""
DEBUG [2011-08-02 01:23:01,086] [org.apache.http.impl.conn.Wire:63]     << ""Last-Modified: Wed, 20 Jul 2011 14:39:57 GMT[\r][\n]""
DEBUG [2011-08-02 01:23:01,087] [org.apache.http.impl.conn.Wire:63]     << ""Transfer-Encoding: chunked[\r][\n]""
DEBUG [2011-08-02 01:23:01,087] [org.apache.http.impl.conn.Wire:63]     << ""Connection: keep-alive[\r][\n]""
DEBUG [2011-08-02 01:23:01,087] [org.apache.http.impl.conn.Wire:63]     << ""Vary: Accept-Encoding[\r][\n]""
DEBUG [2011-08-02 01:23:01,087] [org.apache.http.impl.conn.Wire:63]     << ""Expires: Wed, 31 Aug 2011 23:23:01 GMT[\r][\n]""
DEBUG [2011-08-02 01:23:01,088] [org.apache.http.impl.conn.Wire:63]     << ""Cache-Control: max-age=2592000[\r][\n]""
DEBUG [2011-08-02 01:23:01,088] [org.apache.http.impl.conn.Wire:63]     << ""Content-Encoding: gzip[\r][\n]""
DEBUG [2011-08-02 01:23:01,088] [org.apache.http.impl.conn.Wire:63]     << ""[\r][\n]""
DEBUG [2011-08-02 01:23:01,088] [org.apache.http.impl.conn.DefaultClientConnection:249] Receiving response: HTTP/1.1 200 OK
DEBUG [2011-08-02 01:23:01,089] [org.apache.http.impl.conn.DefaultClientConnection:252] << HTTP/1.1 200 OK
DEBUG [2011-08-02 01:23:01,089] [org.apache.http.impl.conn.DefaultClientConnection:255] << Server: nginx/0.8.54
DEBUG [2011-08-02 01:23:01,089] [org.apache.http.impl.conn.DefaultClientConnection:255] << Date: Mon, 01 Aug 2011 23:23:01 GMT
DEBUG [2011-08-02 01:23:01,089] [org.apache.http.impl.conn.DefaultClientConnection:255] << Content-Type: text/plain
DEBUG [2011-08-02 01:23:01,089] [org.apache.http.impl.conn.DefaultClientConnection:255] << Last-Modified: Wed, 20 Jul 2011 14:39:57 GMT
DEBUG [2011-08-02 01:23:01,090] [org.apache.http.impl.conn.DefaultClientConnection:255] << Transfer-Encoding: chunked
DEBUG [2011-08-02 01:23:01,090] [org.apache.http.impl.conn.DefaultClientConnection:255] << Connection: keep-alive
DEBUG [2011-08-02 01:23:01,090] [org.apache.http.impl.conn.DefaultClientConnection:255] << Vary: Accept-Encoding
DEBUG [2011-08-02 01:23:01,090] [org.apache.http.impl.conn.DefaultClientConnection:255] << Expires: Wed, 31 Aug 2011 23:23:01 GMT
DEBUG [2011-08-02 01:23:01,090] [org.apache.http.impl.conn.DefaultClientConnection:255] << Cache-Control: max-age=2592000
DEBUG [2011-08-02 01:23:01,091] [org.apache.http.impl.conn.DefaultClientConnection:255] << Content-Encoding: gzip
DEBUG [2011-08-02 01:23:01,091] [org.apache.http.impl.client.DefaultRequestDirector:477]        Connection can be kept alive indefinitely
DEBUG [2011-08-02 01:23:01,131] [org.apache.http.impl.conn.Wire:63]     << ""600a[\r][\n]""
DEBUG [2011-08-02 01:23:01,132] [org.apache.http.impl.conn.Wire:77]     << ""[0x1f]""
DEBUG [2011-08-02 01:23:03,838] [org.apache.http.impl.conn.Wire:63]     << ""[\r][\n]""

.... (Content)

DEBUG [2011-08-02 01:23:03,839] [org.apache.http.impl.conn.SingleClientConnManager:267] Releasing connection org.apache.http.impl.conn.SingleClientConnManager$ConnAdapter@2aa3873
DEBUG [2011-08-02 01:23:03,839] [org.apache.http.impl.conn.SingleClientConnManager:285] Released connection open but not reusable.
DEBUG [2011-08-02 01:23:03,839] [org.apache.http.impl.conn.DefaultClientConnection:152] Connection shut down
ERROR [2011-08-02 01:23:03,840] [app]        Exception downloading file
java.io.EOFException
        at java.util.zip.GZIPInputStream.readUByte(GZIPInputStream.java:224)
        at java.util.zip.GZIPInputStream.readUShort(GZIPInputStream.java:214)
        at java.util.zip.GZIPInputStream.readHeader(GZIPInputStream.java:153)
        at java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:75)
        at java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:85)
        at org.apache.http.client.entity.GzipDecompressingEntity.getContent(GzipDecompressingEntity.java:63)
        at org.apache.http.util.EntityUtils.consume(EntityUtils.java:65)
        at org.apache.http.conn.BasicManagedEntity.ensureConsumed(BasicManagedEntity.java:98)
        at org.apache.http.conn.BasicManagedEntity.writeTo(BasicManagedEntity.java:115)
        at util.FileDownload.download(FileDownload.java:188) <--- my app

Does this happen because the server doesn't specify the content length?"
1,"Multithreading issue with versioning. In a multithreading environment with two or more threads accessing the same version history, inconsistent state may be encountered. Concretely, the first thread is currently checking in the node to which the version history is attached while the second thread walks this same version history by means of a ""self-built"" iterator, which just accesses the successors of each version to get the ""next"" to visit.

At a certain point the second point may encounter an ItemNotFoundException with a stack trace similar to this:

javax.jcr.ItemNotFoundException: c9bd405b-dff4-46ef-845c-d98e073e473a
        at org.apache.jackrabbit.core.ItemManager.createItemInstance(ItemManager.java:354)
        at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:230)
        at org.apache.jackrabbit.core.SessionImpl.getNodeByUUID(SessionImpl.java:494)
        at org.apache.jackrabbit.core.version.VersionImpl.getSuccessors(VersionImpl.java:86)
        ....

It seems that the first thread has already filled the successor of the version, while the node is not yet accessible by the createItemInstance method.

This bug seems to not be enforcible, but it is easily reproducible."
1,"TestFSTs.testRealTerms produces a corrupt index. seems to be prox/skip related: the test passes, but the checkindex upon closing fails.

ant test-core -Dtestcase=TestFSTs -Dtests.seed=-4012305283315171209:0 -Dtests.multiplier=3 -Dtests.nightly=true -Dtests.linedocsfile=c:/data/enwiki.random.lines.txt.gz

Note: to get the enwiki.random.lines.txt.gz you have to fetch it from hudson (warning 1 gigabyte file).
you also have to run the test a few times to trigger it.

ill upload the index this thing makes to this issue.
"
1,"Using transactions still leads to memory leak. This is a result of the way that JCR-395 was fixed. If you look at the code, you'll see that txGlobal.remove(xid) is called as the last statement in both XASessionImpl.commit() and XASessionImpl.rollback(). However, in both methods an exception could be thrown either as a result of calling tx.commit() (or tx.prepare()) and tx.rollback(). 

As a result, the transaction will not be removed from txGlobals whenever the commit or the rollback has failed for any reason. My suggestion would be to move the txGlobal.remove(xid) into a finally block."
1,"OutOfMemoryError When repeat login and the logout many times. When repeat login and the logout many times, I encountered?OutOfMemoryError.

javax.jcr.RepositoryException: Cannot instantiate persistence manager org.apache.jackrabbit.core.state.db.DerbyPersistenceManager: Java exception: 'Java heap space: java.lang.OutOfMemoryError'.: Java exception: 'Java heap space: java.lang.OutOfMemoryError'.
	at org.apache.jackrabbit.core.RepositoryImpl.createPersistenceManager(RepositoryImpl.java:1095)
	at org.apache.jackrabbit.core.RepositoryImpl.createVersionManager(RepositoryImpl.java:300)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:245)
	at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:498)
	at org.apache.jackrabbit.core.TransientRepository$2.getRepository(TransientRepository.java:245)
	at org.apache.jackrabbit.core.TransientRepository.startRepository(TransientRepository.java:265)
	at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:333)
	at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:388)
	at Test.tryLoginAndLogout(Test.java:19)
	at Test.test1(Test.java:13)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:164)
	at junit.framework.TestCase.runBare(TestCase.java:130)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:120)
	at junit.framework.TestSuite.runTest(TestSuite.java:230)
	at junit.framework.TestSuite.run(TestSuite.java:225)
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:128)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)
Caused by: SQL Exception: Java exception: 'Java heap space: java.lang.OutOfMemoryError'.
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.javaException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.<init>(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement20.<init>(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement30.<init>(Unknown Source)
	at org.apache.derby.jdbc.Driver30.newEmbedPreparedStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.prepareStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.prepareStatement(Unknown Source)
	at org.apache.jackrabbit.core.state.db.DatabasePersistenceManager.init(DatabasePersistenceManager.java:224)
	at org.apache.jackrabbit.core.RepositoryImpl.createPersistenceManager(RepositoryImpl.java:1091)
	... 27 more
java.lang.OutOfMemoryError: Java heap space

"
1,"SegmentReader.getFieldNames ignores FieldOption.DOC_VALUES. we use this getFieldNames api in segmentmerger if we merge something that isn't a SegmentReader (e.g. FilterIndexReader)

it looks to me that if you use a FilterIndexReader, call addIndexes(Reader...) the docvalues will be simply dropped.

I dont think its enough to just note that the field has docvalues either right? We need to also set the type 
correctly in the merged field infos? This would imply that instead of FieldOption.DOCVALUES, we need to have a 
FieldOption for each ValueType so that we correctly update the type.

But looking at FI.update/setDocValues, it doesn't look like we 'type-promote' here anyway?
"
1,"SISM.checkAddedChildNodes() prevents merging of concurrent changes. This is a regression caused by JCR-2456. The check method reports false positives and prevents merges of concurrently removed child nodes.

The check is done before the local item states are connected to their shared states, which means getAddedChildNodes() will always return the complete list of local child nodes. In addition the merge attempt is also done after the check, which means it is impossible to handle concurrently removed child nodes."
1,"XATest error: commit from different thread but same XID must not block. I'm seeing the following test error quite often in the CI server at work:

testDistributedThreadAccess(org.apache.jackrabbit.core.XATest)  Time elapsed: 0.213 sec  <<< ERROR!
javax.transaction.SystemException: commit from different thread but same XID must not block
	at org.apache.jackrabbit.core.UserTransactionImpl.commit(UserTransactionImpl.java:147)
	at org.apache.jackrabbit.core.XATest.testDistributedThreadAccess(XATest.java:1637)

It seems to be a system-specific issue, as I've never seen the same error locally or on Hudson."
1,moving locked node removes locked state. when moving a locked node it looses it locked state.
1,DWFlushControl does not take active DWPT out of the loop on fullFlush. We have seen several OOM on TestNRTThreads and all of them are caused by DWFlushControl missing DWPT that are set as flushPending but can't full due to a full flush going on. Yet that means that those DWPT are filling up in the background while they should actually be checked out and blocked until the full flush finishes. Even further we currently stall on the maxNumThreadStates while we should stall on the num of active thread states. I will attach a patch tomorrow.
1,"302 response without location header throws exception. Hi, 

According to HTTP 1.1 Spec : http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html#sec10.3.3
""The temporary URI SHOULD be given by the Location field in the response. Unless the request method was HEAD, the entity of the response SHOULD contain a short hypertext note with a hyperlink to the new URI(s).""

Now, in ""DefaultRedirectStrategy.getLocationURI()"", there's a ProtocolException thrown if location header is null.  

if (locationHeader == null) {
    // got a redirect response, but no location header
    throw new ProtocolException(
        ""Received redirect response "" + response.getStatusLine()
       + "" but no location header"");
 }

The specs says ""SHOULD"" and not ""MUST"". ProtocolException ""signals that an HTTP protocol violation has occurred"", which is not exactly true."
1,"TestStressIndexing2 testMultiConfig failure. trunk: r1134311

reproducible

{code}
    [junit] Testsuite: org.apache.lucene.index.TestStressIndexing2
    [junit] Tests run: 1, Failures: 2, Errors: 0, Time elapsed: 0.882 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] java.lang.AssertionError: ram was 460908 expected: 408216 flush mem: 395100 active: 65808
    [junit]     at org.apache.lucene.index.DocumentsWriterFlushControl.assertMemory(DocumentsWriterFlushControl.java:102)
    [junit]     at org.apache.lucene.index.DocumentsWriterFlushControl.doAfterDocument(DocumentsWriterFlushControl.java:164)
    [junit]     at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:380)
    [junit]     at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1473)
    [junit]     at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1445)
    [junit]     at org.apache.lucene.index.TestStressIndexing2$IndexingThread.indexDoc(TestStressIndexing2.java:723)
    [junit]     at org.apache.lucene.index.TestStressIndexing2$IndexingThread.run(TestStressIndexing2.java:757)
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestStressIndexing2 -Dtestmethod=testMultiConfig -Dtests.seed=2571834029692482827:-8116419692655152763
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestStressIndexing2 -Dtestmethod=testMultiConfig -Dtests.seed=2571834029692482827:-8116419692655152763
    [junit] The following exceptions were thrown by threads:
    [junit] *** Thread: Thread-0 ***
    [junit] junit.framework.AssertionFailedError: java.lang.AssertionError: ram was 460908 expected: 408216 flush mem: 395100 active: 65808
    [junit]     at junit.framework.Assert.fail(Assert.java:47)
    [junit]     at org.apache.lucene.index.TestStressIndexing2$IndexingThread.run(TestStressIndexing2.java:762)
    [junit] NOTE: test params are: codec=RandomCodecProvider: {f33=Standard, f57=MockFixedIntBlock(blockSize=649), f11=Standard, f41=MockRandom, f40=Standard, f62=MockRandom, f75=Standard, f73=MockSep, f29=MockFixedIntBlock(blockSize=649), f83=MockRandom, f66=MockSep, f49=MockVariableIntBlock(baseBlockSize=9), f72=Pulsing(freqCutoff=7), f54=Standard, id=MockFixedIntBlock(blockSize=649), f80=MockRandom, f94=MockSep, f93=Pulsing(freqCutoff=7), f95=Standard}, locale=en_SG, timezone=Pacific/Palau
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestStressIndexing2]
    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=133324528,total=158400512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testMultiConfig(org.apache.lucene.index.TestStressIndexing2):     FAILED
    [junit] r1.numDocs()=17 vs r2.numDocs()=16
    [junit] junit.framework.AssertionFailedError: r1.numDocs()=17 vs r2.numDocs()=16
    [junit]     at org.apache.lucene.index.TestStressIndexing2.verifyEquals(TestStressIndexing2.java:308)
    [junit]     at org.apache.lucene.index.TestStressIndexing2.verifyEquals(TestStressIndexing2.java:278)
    [junit]     at org.apache.lucene.index.TestStressIndexing2.testMultiConfig(TestStressIndexing2.java:124)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1403)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1321)
    [junit] 
    [junit] 
    [junit] Testcase: testMultiConfig(org.apache.lucene.index.TestStressIndexing2):     FAILED
    [junit] Some threads threw uncaught exceptions!
    [junit] junit.framework.AssertionFailedError: Some threads threw uncaught exceptions!
    [junit]     at org.apache.lucene.util.LuceneTestCase.tearDown(LuceneTestCase.java:603)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1403)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1321)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.index.TestStressIndexing2 FAILED
{code}"
1,"IndexWriter commits update documents without corresponding delete. while backporting the testcase from LUCENE-3348 I ran into this thread hazard in the 3.x branch. We actually fixed this issue in LUCENE-3348 for Lucene 4.0 but since DWPT has a slightly different behavior when committing segments I create a new issue to track this down in 3.x. when we prepare a commit we sync on IW flush the DW and apply all deletes then release the lock, maybeMerge and start the commit (IW#startCommit(userdata)). Yet, a new segment could be flushed via getReader and sneak into the SegementInfos which are cloned in IW#startCommit instead of in prepareCommit right after the flush. "
1,CustomScoreQuery calls weight() where it should call createWeight(). Thanks to Uwe for helping me track down this bug after I pulled my hair out for hours on LUCENE-3174.
1,"MultiPhraseQuery has incorrect hashCode() implementation - Leads to Solr Cache misses. I found this while hunting for the cause of Solr Cache misses.

The MultiPhraseQuery class hashCode() implementation is non-deterministic. It uses termArrays.hashCode() in the computation. The contents of that ArrayList are actually arrays themselves, which return there reference ID as a hashCode instead of returning a hashCode which is based on the contents of the array. I would suggest an implementation involving the Arrays.hashCode() method.

I will try to submit a patch soon, off for today."
1,"SortField.AUTO doesn't work with long. This is actually the same as LUCENE-463 but I cannot find a way to re-open that issue. I'm attaching a test case by dragon-fly999 at hotmail com that shows the problem and a patch that seems to fix it.

The problem is that a long (as used for dates) cannot be parsed as an integer, and the next step is then to parse it as a float, which works but which is not correct. With the patch the following parsers are used in this order: int, long, float.
"
1,"Restoring a node which has OPV=Version children fails if they are not versionable. when a node has a OPV=Version childnode which is not versionable itself, restoring of the node fails,
when it tries to read the versionhistory of that childnode."
1,"Deadlock during checkin. Under a load of 3 threads performing checkin and restore operations it's possible for all to become deadlocked in AbstractVersionManager.checkin(). This method attempts to upgrade a read lock to a write lock with the following code

    aquireReadLock();
    ....

    try {
        aquireWriteLock();
        releaseReadLock();
        ...

If 2 or more threads acquire the read lock then neither can acquire the write lock resulting in the deadlock, and after that any other thread that calls this method will block waiting for the write lock. The release of the read lock needs to be done before acquiring the write lock, this is documented Concurrent library javadoc.

There is another area where there is an attempt to upgrade a read lock to write lock, RepositoryImpl.WorkspaceInfo.disposeIfIdle() acquires a read lock and calls dispose() which then acquires a write lock, this maybe ok, as I assume there is only 1 thread that will attempt to dispose of idle workspaces.
"
1,"Workspace.getImportHandler() doesn't handle namespace declarations in document view when they are reported as attributes. XMIDocumentViewImportTest is copy of DocumentViewImportTest EXCEPT that createSimpleDocument is overridden.

New simple document is typical of XMI serializations from Eclipse Modeling Framework (EMF).

Four out of eight tests fail due to bad uri    Trace below:

javax.jcr.NamespaceException: www.apache.org/jackrabbit/test/namespaceImportTest7: is not a registered namespace uri.
	at org.apache.jackrabbit.core.NamespaceRegistryImpl.getPrefix(NamespaceRegistryImpl.java:378)
	at org.apache.jackrabbit.core.LocalNamespaceMappings.getPrefix(LocalNamespaceMappings.java:193)
	at org.apache.jackrabbit.core.SessionImpl.getNamespacePrefix(SessionImpl.java:1307)
	at org.apache.jackrabbit.test.api.XMIDocumentViewImportTest.checkImportSimpleXMLTree(XMIDocumentViewImportTest.java:176)
	at org.apache.jackrabbit.test.api.XMIDocumentViewImportTest.performTests(XMIDocumentViewImportTest.java:154)
	at org.apache.jackrabbit.test.api.XMIDocumentViewImportTest.doTestImportXML(XMIDocumentViewImportTest.java:119)
	at org.apache.jackrabbit.test.api.XMIDocumentViewImportTest.testWorkspaceImportXml(XMIDocumentViewImportTest.java:70)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at org.apache.jackrabbit.test.AbstractJCRTest.run(AbstractJCRTest.java:393)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:128)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)

"
1,"httpclient charset encooding loosing problem. file: org\apache\commons\httpclient\HttpConstants.java 


line: near 261


---------------------------------


public static String getContentString(final byte[] data, String charset) {


        return getContentString(data, 0, data.length);


    }


---------------------------------


must be


---


        return getContentString(data, 0, data.length, charset);


---"
1,"Do not launch new merges if IndexWriter has hit OOME. if IndexWriter has hit OOME, it defends itself by refusing to commit changes to the index, including merges.  But this can lead to infinite merge attempts because we fail to prevent starting a merge.

Spinoff from http://www.nabble.com/semi-infinite-loop-during-merging-td23036156.html."
1,"SpellChecker min score is increased by time. The minimum score, an instance variable, is modified in a search. That is wrong, since it makes it 1. thread unsafe and 2. not working. 

Lucky enought it is only used from the one and same method call, so I simply compied the instance variable to a local method variable.

        float min = this.min; 
"
1,"String properties with invalid XML characters export as invalid XML. As noted in the current JCR 1.0.1 maintenance draft, sections 6.4.1,
6.4.2.6, XML export of string properties that contain invalid XML
characters isn't well-defined currently, since those characters are
not permissible in XML.  The proposed fix is to use base64
encoding for such values in System View.

Most characters below #x20 are examples of this.  Currently, these
are escaped numerically in output (such as (amp)#0; )  but
such escape sequences can't be parsed by the XML
import methods.

The current behavior is particularly problematic, because the user
doesn't know the output is corrupt until later, when they try to import it
and get InvalidSerializedDataException.

If for some reason the base64 option is delayed, it might
make sense, as an interim solution, to fail on export
or to somehow patch import to relax its parsing and allow
these escape codes."
1,"/contrib/orm-persistence/ OJBPersistenceManager. OJBPersistenceManager seems to have the following problems

1. OJBPersistenceBroker inherites from AbstractPersistenceBroker. There's no 
need of using a non transactional implementation as the feature is available in 
jdbc. 

2. A single broker is used and It's not thread-safe. This is not a problem now 
because it inherits from AbstractPersistenceManager, and the store(ChangeLog ) 
method is synchronized.

3. The broker is never closed so it leaves an open connection.

4. There's no pooling with only one broker.

5 Each write method (e.g. store(NodeState state)) starts its own transaction 
but the transaction should start and end in store(ChangeLog log).

6. It never rollbacks, even when an item in the changelog can't be persisted.

7. The mysql example create MyISAM tables which don't support transactions. 
Innodb tables would be more appropriate.

8. jdbc to java type mapping is wrong for 
class: org.apache.jackrabbit.core.state.orm.ORMBlobValue
field: size
Changed from INTEGER to BIGINT

9. When a Blob value is loaded a ArrayStoreException is thrown because in 
load(PropertyId id) BlobFileValues are added to internalValueList instead of 
InternalValue instances.

10. in store(NodeReferences). When storing a NodeReferences which have some (but not all) the references deleted the OJB persistence Manager doesn't delete any one.

Some of this problems are present in the Hibernate implementation."
1,"Suggested Patches to MultiPhraseQuery and QueryTermExtractor (for use with HighLighter). I encountered a problem with the Highlighter, where it was not recognizing MultiPhraseQuery.
To fix this, I developed the following two patches:

=====================================================
1. Addition to org.apache.lucene.search.MultiPhraseQuery:

Add the following method:

/** Returns the set of terms in this phrase. */
public Term[] getTerms() {
  ArrayList allTerms = new ArrayList();
  Iterator iterator = termArrays.iterator();
  while (iterator.hasNext()) {
    Term[] terms = (Term[])iterator.next();
    for (int i = 0, n = terms.length; i < n; ++i) {
      allTerms.add(terms[i]);
    }
  }
  return (Term[])allTerms.toArray(new Term[0]);
}

=====================================================
2. Patch to org.apache.lucene.search.highlight.QueryTermExtractor:

a) Add the following import:
import org.apache.lucene.search.MultiPhraseQuery;

b) Add the following code to the end of the getTerms(...) method:
      else  if(query instanceof MultiPhraseQuery)
              getTermsFromMultiPhraseQuery((MultiPhraseQuery) query, terms, fieldName);
  }

c) Add the following method:
 private static final void getTermsFromMultiPhraseQuery(MultiPhraseQuery query, HashSet terms, String fieldName)
 {
   Term[] queryTerms = query.getTerms();
   int i;

   for (i = 0; i < queryTerms.length; i++)
   {
       if((fieldName==null)||(queryTerms[i].field()==fieldName))
       {
           terms.add(new WeightedTerm(query.getBoost(),queryTerms[i].text()));
       }
   }
 }


=====================================================

Can the team update the repository?

Thanks
Michael Harhen "
1,"LuceneTaxonomyReader .decRef() may close the inner IR, renderring the LTR in a limbo.. TaxonomyReader which supports ref-counting, has a decRef() method which delegates to an inner IndexReader and calls its .decRef(). The latter may close the reader (if the ref is zeroes) but the taxonomy would remain 'open' which will fail many of its method calls.

Also, the LTR's .close() method does not work in the same manner as IndexReader's - which calls decRef(), and leaves the real closing logic to the decRef(). I believe this should be the right approach for the fix."
1,"XML export (stream) doesn't initialize TransformerHandler properly. For instance, in SessionImpl.java:

    public void exportSystemView(String absPath, OutputStream out,
                                 boolean skipBinary, boolean noRecurse)
            throws IOException, PathNotFoundException, RepositoryException {

        SAXTransformerFactory stf = (SAXTransformerFactory) SAXTransformerFactory.newInstance();
        try {
            TransformerHandler th = stf.newTransformerHandler();
            th.setResult(new StreamResult(out));
            th.getTransformer().setParameter(OutputKeys.METHOD, ""xml"");
            th.getTransformer().setParameter(OutputKeys.ENCODING, ""UTF-8"");
            th.getTransformer().setParameter(OutputKeys.INDENT, ""no"");

            exportSystemView(absPath, th, skipBinary, noRecurse);
        } catch (TransformerException te) {
            throw new RepositoryException(te);
        } catch (SAXException se) {
            throw new RepositoryException(se);
        }
    }

(1) It should be ""setOutputProperty()"", not ""setParameter()"",

(2) My tests show that setting the parameters only has an effect when done before calling setResult()

That being said, the effect is minor, as the default settings for the TransformerHandler seem to be correct anway.

"
1,"handling of expanded-form jcr names by node type *Template classes . ItemDefinitionTemplate treats the name as opque string, instead of a JCR Name.

Example: when setting the name to

  ""{http://example.org/}foo""

then getName() needs to return

  ""bar:foo""

which the prefix ""bar"" being mapped to the namesapce ""http://example.org/""."
1,"Issue while loading list of classes at that path itself.. Hi,

I cannot retrieve list of objects that are directly under the path that they were saved in. I did not know where to simulate this issue and hence I have used DigesterSimpleQueryTest. I have attached the path for the newly added test case testObjectListRetrievalAtBasePath. In case the patch is not up to the mark I have attached the modified file too.

Instead of creating Page in /test if I create it in /sample/test and search in /sample/test it returns nothing but if I search in /sample it would return the object.

Another important point here is that it is causing issues while retrieving Page class, the other test cases that are retrieving Paragraph class (embedded inside Page class) are still working fine!

Regards,

Kaizer"
1,"MultiThreadedHttpConnectionManager setMaxTotalConnections() method doesn't work. The deprecated setMaxTotalConnections() method in the
MultiThreadedHttpConnectionManager seems like it has no effect:

Here is the source code in the current version:

    public void setMaxTotalConnections(int maxTotalConnections) {
        this.params.getMaxTotalConnections();
    }

Shouldn't it look more like this?

    public void setMaxTotalConnections(int maxTotalConnections) {
        this.params.setMaxTotalConnections(maxTotalConnections);
    }"
1,"jcr:baseVersion is not updated when the base version is removed from the version history. 
        Session s1 = repo.login(new SimpleCredentials(""user1"", ""pwd1"".toCharArray()));
        Node root1 = s1.getRootNode() ;
        Node test1 = root1.addNode(""test"") ;
        test1.addMixin(""mix:versionable"");
        s1.save() ;
        System.out.println(test1.getProperty(""jcr:baseVersion"").getValue().getString()) ;
        test1.checkin() ;
        System.out.println(test1.getProperty(""jcr:baseVersion"").getValue().getString()) ;
        test1.getVersionHistory().removeVersion(""1.0"") ;
        // the base version wasn't updated :(
        System.out.println(test1.getProperty(""jcr:baseVersion"").getValue().getString()) ;
        // the next line throws ItemNotFoundException :(
        test1.getBaseVersion() ;

javax.jcr.ItemNotFoundException: c33bf049-c7e1-4b34-968a-63ff1b1113b0
	at org.apache.jackrabbit.core.ItemManager.createItemInstance(ItemManager.java:498)
	at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:349)
	at org.apache.jackrabbit.core.PropertyImpl.getNode(PropertyImpl.java:642)
	at org.apache.jackrabbit.core.NodeImpl.getBaseVersion(NodeImpl.java:2960)
	at org.apache.jackrabbit.core.RemoveVersionTest.main(RemoveVersionTest.java:56)


"
1,"Identifier paths for inexistent items throw exception. The following fails with a RepositoryException but it should rather return false:

session.itemExists(""["" + UUID.randomUUID() + ""]"")"
1,"InvalidQueryException thrown for a SQL query using WHERE CONTAINS(., 'someword'). The following SQL query:
SELECT * FROM es:document WHERE CONTAINS(., 'software')
throws an InvalidQueryException exception.

javax.jcr.query.InvalidQueryException: Encountered ""."" at line 1, column 42.
Was expecting one of:
    ""BY"" ...
    ""IN"" ...
    ""OR"" ...
    ""IS"" ...
    ""AND"" ...
    ""LIKE"" ...
    ""NULL"" ...
    ""FROM"" ...
    ""ORDER"" ...
    ""WHERE"" ...
    ""SELECT"" ...
    ""BETWEEN"" ...
    ""*"" ...
    <REGULAR_IDENTIFIER> ...
    <DELIMITED_IDENTIFIER> ...
    
This syntax seems correct according to the latest jcr spec (1.0.1).
Using an asterisk (*) instead of a dot (.) as the first parameter of CONTAINS() works fine."
1,"303 Redirects are not handled properly. When the server spits back a 303 (See Other), the redirect is not handled. 
Looking at the code, I saw that the processRedirectResponse method in
HttpMethodBase does not check for SC_SEE_OTHER in the case statement. 
SC_SEE_OTHER is a redirect and should be handled appropriately.

Here is a trace from the output of the client and server.

GET http://172.30.229.75/CGI/Screenshot HTTP/1.1 
Authorization: Basic c3VwZXJ1c2VyOnJvb3Q= 
Host: 172.30.229.75 
User-Agent: Jakarta Commons-HttpClient/2.0M1 

HTTP/1.1 303 See Other 
Location: http://172.30.229.75/FS/CIP_0_5842
Content-Length: 0 
Server: *snip*"
1,JCR2SPI: VersionManagerImpl.getVersionableNodeEntry uses toString() rather than getString() to obtain property value. VersionManagerImpl.getVersionableNodeEntry uses toString() rather than getString() to obtain property value.
1,"Problem getting the HTTPClient to use HTTP 1.0 with a proxy server. I am using HTTPClient 3.0-rc1.
I am connecting to an HTTPS site through a proxy.

I used HTTPLook to see the HTTP messages between the Proxy and the HTTPClient.
I noticed that it always used HTTP/1.1 and setVersion() on either the 
httpclient and the method do not help. I could not find how to get 
the HTTPClient to use HTTP/1.0 with the proxy.

Looking at the ConnectMethod class, the HTTP1.1 was indeed hardcoded.

Thanks
riad"
1,"Problems with File Copy using WebDAV. When i make a copy of files from one workspace to other (CTRL-C -> CTRL-V). The file isnt copied, but the original file is deleted."
1,"org.apache.lucene.ant.HtmlDocument creates a FileInputStream in its constructor that it doesn't close. A look through the jtidy source code doesn't show a close that i can find in parse (seems to be standard that you close your own streams anyway), so this looks like a small descriptor leak to me."
1,"DisjunctionMaxScorer.skipTo has bug that keeps it from skipping. as reported on the mailing list, DisjunctionMaxScorer.skipTo is broken if called before next in some situations...

http://www.nabble.com/Potential-issue-with-DisjunctionMaxScorer-tf3846366.html#a10894987"
1,"Node.addNode(String, String) doesn't prevent use of mixin types as primary type. "
1,"org.apache.jackrabbit.server.remoting.davex.JsonWriter: wrong value type for ::NodeIteratorSize attribute. the ::NodeIteratorSize attribute is serialized as string value whereas the client expects a long value.
this causes unnecessery server-roundtrips since the client doesn't detect this hint."
1,[PATCH] DbDataStore: Make sure streams are closed. Stream isn't closed on end of use. this patch fixes it.
1,"multitermquery scoring differences between 3x and trunk. try this patch with a test, that applies clean to both 3x and trunk, but fails on trunk.

if you modify the test-data-generator to use TopTerms*BoostOnly* rewrite, then it acts like TestFuzzyQuery2, and passes.

So the problem is in TopTermsScoringBooleanRewrite, or BooleanQuery, or somewhere else.
"
1,"PUT method blocks against older servers. To reproduce, attempt a PUT request against an appropriate servlet under TC3.2
(yes I know that needs an upgrade - sigh)

RFC 2616 says:
""Because of the presence of older implementations, the protocol allows ambiguous
situations in which a client may send ""Expect: 100- continue"" without receiving
either a 417 (Expectation Failed) status or a 100 (Continue) status. Therefore,
when a client sends this header field to an origin server (possibly via a proxy)
from which it has never seen a 100 (Continue) status, the client SHOULD NOT wait
for an indefinite period before sending the request body.""

This isn't how HttpClient behaves. After sending the headers,
PutMethod.writeRequestBody() returns false. HttpMethodBase then calls
readStatusCode(), which blocks waiting for a read (or I guess you could time out
the whole request). Right now this makes it impossible to use HttpClient to PUT
to older Http 1.1 implementations.

A suggested resolution: since the spec allows for clients to avoid waiting if
they know the 100 response will not arrive, why not simply provide a boolean
flag to allow the 'wait for 100' behaviour in PutMethod.writeResponseBody() to
be turned off, on a per-request basis? This solution puts the burden of knowing
""origin server[s]...from which it has never seen a 100 (Continue) status"" on the
user of HttpClient. Less than perfect as you can only find out that this has
happened by trial and error.

A more correct solution, is to maintain a list of servers that ignore the Expect
header in PutMethod, and override PutMethod.readStatusCode() to time out, send
the body, remember this server is buggy, and read the status code again."
1,"JCR-RMI UnmarshalException when calling getProperty(). I've been trying to get the JCR-RMI adaptors going to talk to my noddy test repository. It seems I can successfully login and traverse the nodes (getNode etc..) but whenever I try to get something from a nt:resource (actually I've subclassed nt:resource to add our own properties) I get the following exception:

org.apache.jackrabbit.rmi.client.RemoteRepositoryException: error unmarshalling return; nested exception is: 
	java.io.WriteAbortedException: writing aborted; java.io.NotSerializableException: org.apache.jackrabbit.value.BinaryValue: error unmarshalling return; nested exception is: 
	java.io.WriteAbortedException: writing aborted; java.io.NotSerializableException: org.apache.jackrabbit.value.BinaryValue
	at org.apache.jackrabbit.rmi.client.ClientProperty.getValue(ClientProperty.java:139)
	at org.apache.jackrabbit.rmi.client.ClientProperty.getString(ClientProperty.java:131)
	at ClientTest.main(ClientTest.java:20)
Caused by: java.rmi.UnmarshalException: error unmarshalling return; nested exception is: 
	java.io.WriteAbortedException: writing aborted; java.io.NotSerializableException: org.apache.jackrabbit.value.BinaryValue
	at sun.rmi.server.UnicastRef.invoke(UnicastRef.java:164)
	at org.apache.jackrabbit.rmi.server.ServerProperty_Stub.getValue(Unknown Source)
	at org.apache.jackrabbit.rmi.client.ClientProperty.getValue(ClientProperty.java:137)
	... 2 more

My svn is up-to-date as of this morning 2005/7/5 although the maven builds don't inspire much confidence as it falls over with Jelly exceptions at various points and there's a couple of dozen unit tests that fail. Don't know if this is the norm, the maven reports on the jackrabbit main site reports suggest not? But calling the same code with an in-process repository works fine.

"
1,"Unusual Http status line. The web server at http://alces.med.umn.edu/Candida.html returns the following
status line:

HTTP 200 Document follows

This page loads in the 3 browsers I tried (though Safari actually rendered the
headers).  The current version of HttpClient reads through the whole page
looking for a line that starts with HTTP/.  I don't know how big of a problem
this is, but it's a fairly easy fix.  Patch to follow."
1,"setProperty(""name"", new Value[0], PropertyType.LONG) loses property type. Adding an empty multivalued property with a specific non-STRING type to an unstructured node (i.e. one with an UNDEFINED multivalued property definition) creates an empty multivalued property of type STRING.

In some cases keeping the explicit type information is important, so we should avoid losing it."
1,"ClassCastException in ParallelReader class. ClassCastException in ParalleReader when calling getTermFreqVectors on line 153

Reason : 

 cast of key and value is swapped

Fixed with : 

      IndexReader reader = (IndexReader)e.getValue();
      String field = (String)e.getKey();
"
1,"Jenkins builds hang quite often in TestIndexWriterWithThreads.testCloseWithThreads. Last hung test run: [https://builds.apache.org/job/Lucene-Solr-tests-only-trunk/10638/console]

{noformat}
[junit] ""main"" prio=5 tid=0x0000000801ef3800 nid=0x1965c waiting on condition [0x00007fffffbfd000]
[junit]    java.lang.Thread.State: WAITING (parking)
[junit] 	at sun.misc.Unsafe.park(Native Method)
[junit] 	- parking to wait for  <0x0000000825d853a8> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
[junit] 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
[junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:838)
[junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:871)
[junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1201)
[junit] 	at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)
[junit] 	at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)
[junit] 	at org.apache.lucene.index.DocumentsWriterFlushControl.markForFullFlush(DocumentsWriterFlushControl.java:403)
[junit] 	at org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:557)
[junit] 	- locked <0x0000000825d81998> (a org.apache.lucene.index.DocumentsWriter)
[junit] 	at org.apache.lucene.index.IndexWriter.prepareCommit(IndexWriter.java:2776)
[junit] 	- locked <0x0000000825d7d840> (a java.lang.Object)
[junit] 	at org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:2904)
[junit] 	- locked <0x0000000825d7d830> (a java.lang.Object)
[junit] 	at org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:1156)
[junit] 	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1099)
[junit] 	at org.apache.lucene.index.TestIndexWriterWithThreads.testCloseWithThreads(TestIndexWriterWithThreads.java:200)
[junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
[junit] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[junit] 	at java.lang.reflect.Method.invoke(Method.java:616)
[junit] 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
[junit] 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
[junit] 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
[junit] 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
[junit] 	at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
[junit] 	at org.apache.lucene.util.LuceneTestCase$2$1.evaluate(LuceneTestCase.java:611)
[junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
[junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
[junit] 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
[junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:148)
[junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:50)
[junit] 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
[junit] 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
[junit] 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
[junit] 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
[junit] 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
[junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
[junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
[junit] 	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
[junit] 	at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
[junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
[junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
[junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:743)
{noformat}"
1,"Oracle JNDI DataSource support. When org.apache.jackrabbit.core.persistence.bundle.util.ConnectionFactory tries to get a connection from a JNDI Datasource without login and pasword, if no user/password are specified, they re retrieved as empty strings, not null, so it tries to do a ds.getConnection(user, password), which fails. Please complete the test line 66 as :
if ((user == null || user.length() > 0) && (password == null || password.length() > 0)) {

Sincerely,

Stphane Landelle"
1,"String.toLowerCase() / toUpperCase() should specify Locale.ENGLISH. There are quite a few instances of String.toLowerCase() - and some of toUpperCase() - method calls which don't specify the Locale.

These should probably mostly/all use Locale.ENGLISH, otherwise there may be problems in some Locales.
e.g. Turkey, where ""i"".toUpperCase() is not equal to ""I"" - and vice-versa.

The isSpecialDomain() method in NetscapeDomainHandler is one instance where the code won't always work in Turkey."
1,"HttpGet request not being created when parameter ""url"" is present. 
/*
* @formatter:off
* 
* The following redirect Location results in a Bad Request (404) being made.
* 
* http://www.qpassport.co.uk/passport/register.php?do=signup&who=adult&url=http%3A%2F%2Fwww.qpassport.co.uk%2Fpassport%2F&month=2&year=1947&day=26
* 
* The GET request is made with these headers (Notice the ""Host"" value):
* 
* DEBUG org.apache.http.wire  - >> ""GET http://www.qpassport.co.uk/passport/register.php?do=signup&who=adult&url=http%3A%2F%2Fwww.qpassport.co.uk%2Fpassport%2F&month=2&year=1947&day=26 HTTP/1.1[\r][\n]""
* DEBUG org.apache.http.wire  - >> ""Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*//*;q=0.8[\r][\n]""
* DEBUG org.apache.http.wire  - >> ""Accept-Language: en-us,en;q=0.5[\r][\n]""
* DEBUG org.apache.http.wire  - >> ""Accept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7[\r][\n]""
* DEBUG org.apache.http.wire  - >> ""Proxy-Connection: Keep-Alive[\r][\n]""
* DEBUG org.apache.http.wire  - >> ""Referer: http://www.qpassport.co.uk/passport/register.php?s=b9761dfa820bb55722e3feb6438fa11f&[\r][\n]""
* DEBUG org.apache.http.wire  - >> ""Host: www.qpassport.co.uk/passport/register.php?do=signup&who=adult&url=http[\r][\n]""
* DEBUG org.apache.http.wire  - >> ""User-Agent: Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)[\r][\n]""
*  
* They should be: (Notice the ""Host"" value)
* DEBUG org.apache.http.wire  - >> ""GET http://www.qpassport.co.uk/passport/register.php?do=signup&who=adult&month=2&year=1947&day=26 HTTP/1.1[\r][\n]""
* DEBUG org.apache.http.headers  - >> Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*//*;q=0.8
* DEBUG org.apache.http.headers  - >> Accept-Language: en-us,en;q=0.5
* DEBUG org.apache.http.headers  - >> Accept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7
* DEBUG org.apache.http.headers  - >> Proxy-Connection: Keep-Alive
* DEBUG org.apache.http.headers  - >> Referer: http://www.qpassport.co.uk/passport/register.php?s=005ef80064a0fb2f5aa6f4677a194928&
* DEBUG org.apache.http.headers  - >> Content-Length: 134
* DEBUG org.apache.http.headers  - >> Content-Type: application/x-www-form-urlencoded; charset=UTF-8
* DEBUG org.apache.http.headers  - >> Host: www.qpassport.co.uk
* DEBUG org.apache.http.headers  - >> User-Agent: Opera/9.20 (Windows NT 6.0; U; en)
* 
* The problem appears to be related to the URL parameter in the request
* when it is removed, the request succeeds.
* 
* @formatter:on
*/
"
1,"Cannot clone BasicClientCookie2 without specified ports. The clone method returns a null pointer exception when called on a BasicClientCookie2 that does not use any ports properties.
In other words, it is impossible to clone a BasicClientCookie2 instance without ports specification.

In the clone() method, they are two main instructions :
 - calling clone() method on super
 - calling clone() method on the ports integer array (which is null)

It may be a good idea to check whether the array is null or not

"
1,"addIndexesNoOptimize should not enforce maxMergeDocs/maxMergeSize limit. If you pass an index that has a segment > maxMergeDocs or maxMergeSize
to addIndexesNoOptimize, it throws an IllegalArgumentException.

But this check isn't reasonable because segment merging can easily
produce segments over these sizes since those limits apply to each
segment being merged, not to the final size of the segment produced.

So if you set maxMergeDocs to X, build up and index, then try to add
that index to another index that also has maxMergeDocs X, you can
easily hit the exception.

I think it's being too pedantic; I plan to just remove the checks for
sizes."
1,"trunk:  TestDocumentsWriterDeleteQueue.testStressDeleteQueue seed failure. fails 100% of the time for me, trunk r1152089

{code}
    [junit] Testsuite: org.apache.lucene.index.TestDocumentsWriterDeleteQueue
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.585 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestDocumentsWriterDeleteQueue -Dtestmethod=testStressDeleteQueue -Dtests.seed=724635056932528964:-56
53725200660632980
    [junit] NOTE: test params are: codec=RandomCodecProvider: {}, locale=en_US, timezone=Pacific/Port_Moresby
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestDocumentsWriterDeleteQueue]
    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_26 (64-bit)/cpus=8,threads=1,free=86067624,total=125632512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testStressDeleteQueue(org.apache.lucene.index.TestDocumentsWriterDeleteQueue):    FAILED
{code}"
1,"TCK: Incorrect check of namespace mappings in System View XML export. org.apache.jackrabbit.test.api.SysViewContentHandler. In endDocument(), two issues:

1. line 351: tries to go through a table of prefixes but uses a fixed index inside the loop;
2. The mapping for the 'xml' prefix should be skipped (it must be registered in the Session but must not be registered during export since this is a built-in XML mapping."
1,"NPE if you open IW with CREATE on an index with no segments file. I have a simple test case that hits this NPE:

{noformat}
    [junit] java.lang.NullPointerException
    [junit] 	at java.io.File.<init>(File.java:305)
    [junit] 	at org.apache.lucene.store.NIOFSDirectory.openInput(NIOFSDirectory.java:67)
    [junit] 	at org.apache.lucene.store.FSDirectory.openInput(FSDirectory.java:333)
    [junit] 	at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:213)
    [junit] 	at org.apache.lucene.index.IndexFileDeleter.<init>(IndexFileDeleter.java:218)
    [junit] 	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:1113)
    [junit] 	at org.apache.lucene.index.TestIndexWriter.testNoSegmentFile(TestIndexWriter.java:4975)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:277)
{noformat}

It happens if you have an aborted index, ie, there are segment files in there (*.frq, *.tis, etc.) but no segments_N file, and then you try to open an IW with CREATE on that index."
1,"Proxy-Authorization header received on server side. 
 
 I'm following example
 http://hc.apache.org/httpcomponents-client-ga/examples.html
 Proxy authentication
 
 but it seems that not only proxy is receiving credentials for proxy.
 In log, which is generated at target.host I can see header
 Proxy-Authorization: Basic ....

--------- HEADER
Host:target.host:443
Connection:Keep-Alive
User-Agent:Apache-HttpClient/4.1 (java 1.5)
Proxy-Authorization:Basic Z
--------- POST


Dusan"
1,"SQL2 parser: identifiers should be case sensitive. Currently the SQL2 parser converts the query to uppercase before parsing. However the identifiers should be kept case sensitive.

Instead of converting the query to uppercase, String.equalsIgnoreCase should be used to compare against keywords.
"
1,"reopen on NRT reader should share readers w/ unchanged segments. A repoen on an NRT reader doesn't seem to share readers for those segments that are unchanged.
http://search.lucidimagination.com/search/document/9f0335d480d2e637/nrt_and_caching_based_on_indexreader"
1,Null paths break the compare method. The compare method cannot handle the path being null
1,"java.lang.ArrayIndexOutOfBoundsException while importXML in Java 6. Using:
- Jackrabbit 1.3
- Java:
  java version ""1.6.0_02""
  Java(TM) SE Runtime Environment (build 1.6.0_02-b05)
  Java HotSpot(TM) Client VM (build 1.6.0_02-b05, mixed mode, sharing)

When importing attached XML, I get an exception:
Caused by: java.lang.ArrayIndexOutOfBoundsException
        at java.lang.System.arraycopy(Native Method)
        at org.apache.jackrabbit.core.xml.BufferedStringValue.append(BufferedStringValue.java:201)
        at org.apache.jackrabbit.core.xml.SysViewImportHandler.characters(SysViewImportHandler.java:187)
        at org.apache.jackrabbit.core.xml.ImportHandler.characters(ImportHandler.java:200)
        at com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.characters(AbstractSAXParser.java:538)
        at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:461)
        at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:807)
        at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:737)
        at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:107)
        at com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.parse(AbstractSAXParser.java:1205)
        at com.sun.org.apache.xerces.internal.jaxp.SAXParserImpl$JAXPSAXParser.parse(SAXParserImpl.java:522)
        at javax.xml.parsers.SAXParser.parse(SAXParser.java:395)
        at org.apache.jackrabbit.core.SessionImpl.importXML(SessionImpl.java:1116)
...

If I use Java 1.5, then it works.

java version ""1.5.0_12""
Java(TM) 2 Runtime Environment, Standard Edition (build 1.5.0_12-b04)
Java HotSpot(TM) Client VM (build 1.5.0_12-b04, mixed mode, sharing)
"
1,"ConcurrentModificationException in QueryStatImpl. Running with qurystats enabled the Query#execute can throw ConcurrentModificationException

caused by the iterator which backing collection is changed from another thread

see logQuery method
        Iterator<QueryStatDtoImpl> iterator = popularQueries.iterator();
        while (iterator.hasNext()) {
-->            QueryStatDtoImpl qsdi = iterator.next();
            if (qsdi.equals(qs)) {
                qs.setOccurrenceCount(qsdi.getOccurrenceCount() + 1);
                iterator.remove();
                break;
            }
        }
        popularQueries.offer(qs);
"
1,"While you could use a custom Sort Comparator source with remote searchable before, you can no longer do so with FieldComparatorSource. FieldComparatorSource is not serializable, but can live on a SortField"
1,"search vs explain - score discrepancies. I'm on a mission to demonstrate (and then hopefully fix) any inconsistencies between the score you get for a doc when executing a search, and the score you get when asking for an explanation of the query for that doc."
1,"Cluster information is not persisted to database when connected to case sensitive MS SQL Server 2005. After a call to Session::save, we observed that cluster information was not written to the ${schemaObjectPrefix}JOURNAL and ${schemaObjectPrefix}GLOBAL_REVISION tables. We tested against Oracle 10 database servers and MS Sql Server 2005 servers. The problem was noticed only with MS Sql Server 2005. 

Initially, the problem was masked since the test was written as part of our unit test environment and the exceptions generated by JDBC were not showing up in the logs. A separate test with was carried out as shown by the code below

<pre>
import java.io.FileInputStream;

import javax.jcr.Node;
import javax.jcr.Repository;
import javax.jcr.Session;
import javax.jcr.SimpleCredentials;

import org.apache.jackrabbit.core.TransientRepository;
import org.apache.jackrabbit.core.config.RepositoryConfig;

public class Main
{
    public static void main(String[] args)
        throws Exception
    {
        System.setProperty(""org.apache.jackrabbit.core.cluster.node_id"", ""testid"");
        
        RepositoryConfig config = RepositoryConfig.create(new FileInputStream(""repository.xml""), ""repository"");
        
        Repository repository = new TransientRepository();
        
        Session session = repository.login(new SimpleCredentials(""username"", ""password"".toCharArray()));
        
        Node root = session.getRootNode();
        
        root.addNode(""node1"");
        root.addNode(""node2"");
        root.addNode(""node3"");
        
        session.save();
    }
}
</pre>

The configuration file used to configure the repository is attached.

After debugging this, we obtained the exceptions that were previously not visible. Note that, JackRabbit continues to run (is that because the cluster code is running in a separate thread?) even after this exception. The problem was that the 'revision_id' field did not exist. The mssql.ddl schema file sets up the table names in capitals. However, at least two of the SQL statements in DatabaseJournal use lower case table names. For example:-

<pre>
        updateGlobalStmt = con.prepareStatement(
                ""update "" + schemaObjectPrefix + ""global_revision "" +
                ""set revision_id = revision_id + 1"");
        selectGlobalStmt = con.prepareStatement(
                ""select revision_id "" +
                ""from "" + schemaObjectPrefix + ""global_revision"");
</pre>

An additional error is that the mssql.ddl file is missing the following:

<pre>
# Inserting the one and only revision counter record now helps avoiding race conditions
insert into ${schemaObjectPrefix}GLOBAL_REVISION VALUES(0)
</pre>

Fixing the above two issues, fixed the problem with MS SQL Server 2005."
1,SpanScorer fails when sloppyFreq() returns 0. I think we should fix this for 2.4 (now back to 10)?
1,"DefaultPrincipalProvider#collectGroupMembership puts wrong principal instance into the cache. DefaultPrincipalProvider#collectGroupMembership adds the passed principal instance to the cache. This may cause
inconsistencies as the cache should only contain principals obtained from by the provider."
1,"Setting WebDAV property without value causes NPE in DAVResourceImpl. A WebDAV PROPPATCH of a property without a value <prf:SomeProperty/> causes a NPE in DAVResourceImpl when the value is retrieved and the toString() method called on it. Here is a patch that works around the problem.

Index: C:/jprojects/eclipse/jackrabbit/jcr-server/server/src/java/org/apache/jackrabbit/webdav/simple/DavResourceImpl.java
===================================================================
--- C:/jprojects/eclipse/jackrabbit/jcr-server/server/src/java/org/apache/jackrabbit/webdav/simple/DavResourceImpl.java	(revision 388517)
+++ C:/jprojects/eclipse/jackrabbit/jcr-server/server/src/java/org/apache/jackrabbit/webdav/simple/DavResourceImpl.java	(working copy)
@@ -930,7 +930,10 @@
      */
     private void setJcrProperty(DavProperty property) throws RepositoryException {
         // retrieve value
-        String value = property.getValue().toString();
+        String value = """";
+        if (property.getValue() != null) {
+            value = property.getValue().toString();
+        }
         // set value; since multivalued-properties are not listed in the set
         // of available properties, this extra validation-check is omitted.
         node.setProperty(getJcrName(property.getName()), value);
"
1,webapp doesn't compile (use of enum keyword). AbstractConfig.java and JNDIConfig.java have local variables named 'enum' that aren't allowed when using JDK5 or later compilers.
1,"SpanOrQuery skipTo() doesn't always move forwards. In SpanOrQuery the skipTo() method is improperly implemented if the target doc is less than or equal to the current doc, since skipTo() may not be called for any of the clauses' spans:

    public boolean skipTo(int target) throws IOException {
          if (queue == null) {
            return initSpanQueue(target);
          }

          while (queue.size() != 0 && top().doc() < target) {
            if (top().skipTo(target)) {
              queue.adjustTop();
            } else {
              queue.pop();
            }
          }
          
        	return queue.size() != 0;
        }

This violates the correct behavior (as described in the Spans interface documentation), that skipTo() should always move forwards, in other words the correct implementation would be:

    public boolean skipTo(int target) throws IOException {
          if (queue == null) {
            return initSpanQueue(target);
          }

          boolean skipCalled = false;
          while (queue.size() != 0 && top().doc() < target) {
            if (top().skipTo(target)) {
              queue.adjustTop();
            } else {
              queue.pop();
            }
            skipCalled = true;
          }
          
          if (skipCalled) {
        	return queue.size() != 0;
          }
          return next();
        }"
1,JCARepositoryManager does not close InputStream used to obtain repository config from classpath. 
1,"ArrayIndexOutOfBoundsException in NodeTypeDefDiff. It appears that the code for building diffs in child node definitions loops incorrectly, opening the possibility for an ArrayIndexOutOfBounds exception. The offending portion is in the ""buildChildNodeDefDiffs"" method:

<<
NodeDef[] cnda2 = newDef.getChildNodeDefs();
HashMap defs2 = new HashMap();
for (int i = 0; i < cnda1.length; i++) {
    defs2.put(cnda2[i].getId(), cnda2[i]);
}
>>

It seems like simply changing the length check to be cnda2 (as it is in ""buildPropDefsDiff"") would suffice."
1,"bad assumptions in VersionHistoryTest.testInitallyGetAllVersionsContainsTheRootVersion(). There are two incorrect assumptions in testInitallyGetAllVersionsContainsTheRootVersion:

- getAllVersions() returns versions in a particular order (test assumes root version comes first), and

- Node.equals() is suitable for node comparison

And finally, there's a typo in the test case name.
"
1,"If there is more than 15 seconds between HttpClient.execute() calls using a MultipartEntity, a ProtocolException is thrown complaining about the Content-Length header already being present.. I am not sure if this time-related behaviour is intentional or not (I have only been using this library for a few weeks) , but even if a timeout is to be expected, the exception thrown ought to indicate that there is a time component involved. ""org.apache.http.ProtocolException: Content-Length header already present"" is incredibly misleading. 

A simple-ish compileable program to reproduce the bug is as follows:

import java.nio.charset.Charset;
import org.apache.http.HttpResponse;
import org.apache.http.client.methods.HttpPost;
import org.apache.http.client.params.ClientPNames;
import org.apache.http.client.params.CookiePolicy;
import org.apache.http.entity.mime.MultipartEntity;
import org.apache.http.entity.mime.content.StringBody;
import org.apache.http.impl.client.DefaultHttpClient;
public class Simple {
    static public void main(String [] args)
    {
        try
        {
            DefaultHttpClient client = new DefaultHttpClient();
            client.getParams().setParameter(
                        ClientPNames.COOKIE_POLICY, CookiePolicy.BROWSER_COMPATIBILITY);
            MultipartEntity entity;
            StringBody stringBody;
            HttpPost post;
            HttpResponse response;
            entity = new MultipartEntity();
            stringBody = new StringBody(""field contents"",Charset.forName(""ISO-8859-1""));
            entity.addPart(""field"", stringBody);  
            post = new HttpPost(""http://localhost/simple.php"");
            post.setEntity(entity); 
            response = client.execute(post);
            
            //The exception does not occur if the content is not consumed
            response.getEntity().consumeContent();
            System.out.println(""First post done"");
            
            //The exception does not occur if the time interval between the requests is too short
            Thread.sleep(15000);
            
            //The exception naturally doesn't occur if a new HttpClient is created
            //client = new DefaultHttpClient();

            entity = new MultipartEntity();
            stringBody = new StringBody(""field contents"",Charset.forName(""ISO-8859-1""));
            entity.addPart(""field"", stringBody);  

            post = new HttpPost(""http://localhost/simple.php"");
            post.setEntity(entity); 
            response = client.execute(post); //Will throw the following:
            /*
                org.apache.http.ProtocolException: Content-Length header already present
                at org.apache.http.protocol.RequestContent.process(RequestContent.java:70)
                at org.apache.http.protocol.BasicHttpProcessor.process(BasicHttpProcessor.java:290)
                at org.apache.http.protocol.HttpRequestExecutor.preProcess(HttpRequestExecutor.java:160)
                at org.apache.http.impl.client.DefaultClientRequestDirector.execute(DefaultClientRequestDirector.java:356)
                at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:501)
                at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:456)
                at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:422)
                at test.Simple.main(Simple.java:57)
             */ 
            System.out.println(""Second post done"");
        }
        catch(Exception e)
        {
            System.out.println(e);
            e.printStackTrace();
        }
    }    
}"
1,"EasyX509TrustManager no longer checks cert expiry. EasyX509TrustManager was made even ""easier"" by the last commit:  a socket will
now be created when talking to a server with an expired certificate.

2 commits ago it looked like this (notice ""return false"" on line 107):

102             try {
103                 certificate.checkValidity();
104             }
105             catch (CertificateException e) {
106                 LOG.error(e.toString());
107                 return false;
108             }


Now it looks like this:

102             try {
103                 certificate.checkValidity();
104             }
105             catch (CertificateException e) {
106                 LOG.error(e.toString());
107             }


I'm proposing we just do:

102             certificate.checkValidity();

Now that we're using Java 1.4 in the contrib code, we'll just let the
CertificateException fly up the stack."
1,"Updates to multiple workspaces (e.g. in a transaction) locked in cluster journal. Running a transaction that updates multiple workspaces (e.g. a versioning operation) will be locked, as they all try to acquire a non-reentrant lock in the cluster's journal. Short-term fix is to make the lock re-entrant. In the long run, a transaction context sensitive lock may be more appropriate.

How to reproduce: enable clustering in the test environment and let the test o.a.j.core.XATest.testSetVersionLabel() run. This will result in a deadlock when committing the operation.

This was initially reported by Rafa Kwiecie as a problem when using springmodules and clustering but turned out to be general problem with transactions and clustering. Thanks for reporting it!"
1,"NPE in classes of OJB-PM. NPE occurs while accessing the Id of the parent of the root node which is null.

Patch follows"
1,"TaxonomyReader.refresh() is broken, replace its logic with reopen(), following IR.reopen pattern. When recreating the taxonomy index, TR's assumption that categories are only added does not hold anymore.
As result, calling TR.refresh() will be incorrect at best, but usually throw an AIOOBE."
1,"Uncommitted changes or connection leak with Container Managed Transactions. Apparently the connector doesn't support CMT (container managed transactions). if the jcr session is closed inside a CMT the AS (application server) throws an exception on commit. And if the jcr session is leaved open, the AS commits the TX successfully but it causes a connection leak by leaving the session open."
1,"Writers blocked forever when waiting on update operations  . Thread 1 calls Session.save() and has a write lock.

Thread 2 is in XA prepare() and is waiting on thread 1 in FineGrainedISMLocking.acquireWriteLock().

Thread 1's save calls SharedItemStateManager.Update#end() and performs a write-lock downgrade to a read-lock, then (at the end of Update#end()) it calls readLock.release(). FineGrainedISMLocking.ReadLockImpl#release thinks activeWriterId is of the current transation and does not notify any writers (activeWriterId is not being reset on downgrade in what seems to be a related to JCR-2753).
Thread 1 waits forever."
1,"StringRequestEntity.getContentLength wrong for multibyte chars. When setting up a PostMethod containing a StringRequestEntity with umlauts and
charset UTF-8 the content-length header is wrong. It should be the number
of bytes, but is the number of chars by now.

(e.g.
Content-Type: text/xml; charset=UTF-8
body='')

Bug-location: org.apache.commons.httpclient.methods.StringRequestEntity"
1,"Weird BooleanQuery behavior. Here's a simple OR-connected query.

T:files T:deleting C:thanks C:exists

The query above hits 1 document. But following *same* query only
with parenthesis results nothing.

(T:files T:deleting) (C:thanks C:exists)

Another combinations of MUST and SHOULD.

""T:files T:deleting +C:production +C:optimize"" hits 1 document.
""(T:files T:deleting) (+C:production +C:optimize)"" hits 1 document."
1,"Provider org.apache.xalan.processor.TransformerFactoryImpl not found. ""maven jar"" fails with the following error message on a fresh Jackrabbit source tree:

BUILD FAILED
File...... /home/hukka/tmp/jackrabbit/maven.xml
Element... ant:xslt
Line...... 146
Column.... 25
Provider org.apache.xalan.processor.TransformerFactoryImpl not found
Total time: 4 seconds
Finished at: Sun Feb 13 10:09:03 EET 2005
"
1,"StandardQueryParser ignores AND operator for tokenized query terms. The standard query parser uses the default query operator for query clauses that are created from tokenization in the query parser instead of the actual operator for the source term.

here is an example:
{code}
StandardQueryParser parser = new StandardQueryParser(new StandardAnalyzer(Version.LUCENE_34));
parser.setDefaultOperator(Operator.OR);
System.out.println(((BooleanQuery)parser.parse(""_deleted:true AND title:"", ""f"")));
{code}

this should yield:
+_deleted:true +(title: title:)

as our former core query parser does but actually yields:
+_deleted:true title: title:

seems like a bug to me, looking at the tests seems we don't test for this kind of queries in the standard query parser tests too.
"
1,"it is not possible to register an event listener which listens to mixin nodetypes. it would be a nice enhancement if one could as well define mixin nodetypes to be listened:
...
om.addEventListener(this,
                        Event.PROPERTY_ADDED | Event.PROPERTY_CHANGED | Event.PROPERTY_REMOVED,
                        ""/"",
                        true,
                        null,
                        new String[]{""mix:Custom""},
                        false);
..."
1,"GetMethod.java checks the ""used"" flag which cannot be set at this time. GetMethod.getResponseBodyAsStream calls HttpMethodBase.checkUsed, which asserts
the flag ""used"" is ""true"". But at this time, ""used"" cannot be true, as ""used"" is
set to true in HttpMethodBase.processRequest, two lines after readResponse is
called (which in turn calls readResponseBody / readResponseBodyAsStream).

Maybe ""requestSent"" is the flag which should be checked instead of ""used""?

My stack trace: (fragment)

java.lang.IllegalStateException: Not Used.
        at
org.apache.commons.httpclient.HttpMethodBase.checkUsed(HttpMethodBase.java:1642)
        at
org.apache.commons.httpclient.methods.GetMethod.getResponseBodyAsStream(GetMethod.java:309)
        at
org.apache.commons.httpclient.methods.GetMethod.readResponseBody(GetMethod.java:428)
        at
org.apache.commons.httpclient.HttpMethodBase.readResponse(HttpMethodBase.java:1893)
        at
org.apache.commons.httpclient.HttpMethodBase.processRequest(HttpMethodBase.java:2496)
        at
org.apache.commons.httpclient.HttpMethodBase.execute(HttpMethodBase.java:1062)
        at
org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:599)
        at
org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:497)
        at
org.apache.webdav.lib.WebdavResource.getMethodData(WebdavResource.java:2227)
        at
org.apache.webdav.lib.WebdavResource.getMethodData(WebdavResource.java:2206)
[...]"
1,"Query parser builds invalid parse tree. Calling org.apache.jackrabbit.spi.commons.query.QueryParser.parse on 

SELECT prop1 FROM nt:unstructured WHERE prop1 IS NOT NULL ORDER BY prop1 ASC 

results in the following parse tree

+ Select properties: {}prop1
 + PathQueryNode
   + LocationStepQueryNode:  NodeTest=* Descendants=true Index=NONE
     + RelationQueryNode: Op: NOT NULL Prop=@{}prop1 Type=STRING Value=%
     + NodeTypeQueryNode:  Prop={http://www.jcp.org/jcr/1.0}primaryType Value={http://www.jcp.org/jcr/nt/1.0}unstructured
 + OrderQueryNode
   {}prop1 asc=true

The RelationQueryNode should not have a second operand since the NOT NULL operator is unary.

"
1,"NullPointerException thrown by equals method in SpanOrQuery. Part of our code utilizes the equals method in SpanOrQuery and, in certain cases (details to follow, if necessary), a NullPointerException gets thrown as a result of the String ""field"" being null.  After applying the following patch, the problem disappeared:

Index: src/java/org/apache/lucene/search/spans/SpanOrQuery.java
===================================================================
--- src/java/org/apache/lucene/search/spans/SpanOrQuery.java    (revision 465065)
+++ src/java/org/apache/lucene/search/spans/SpanOrQuery.java    (working copy)
@@ -121,7 +121,8 @@
     final SpanOrQuery that = (SpanOrQuery) o;

     if (!clauses.equals(that.clauses)) return false;
-    if (!field.equals(that.field)) return false;
+    if (field != null && !field.equals(that.field)) return false;
+    if (field == null && that.field != null) return false;

     return getBoost() == that.getBoost();
   }

"
1,"Optimize runs forever if you keep deleting docs at the same time. Because we ""cascade"" merges for an optimize... if you also delete documents while the merges are running, then the merge policy will see the resulting single segment as still not optimized (since it has pending deletes) and do a single-segment merge, and will repeat indefinitely (as long as your app keeps deleting docs)."
1,"Decompounders based on CompoundWordTokenFilterBase cannot be used with custom attributes. The CompoundWordTokenFilterBase.setToken method will call clearAttributes() and then will reset only the default Token attributes (term, position, flags, etc) resulting in any custom attributes losing their value. Commenting out clearAttributes() seems to do the trick, but will fail the TestCompoundWordTokenFilter tests.."
1,"PriorityQueue is inheriently broken if subclass attempts to use ""heap"" w/generic T bound to anything other then ""Object"". as discovered in SOLR-2410 the fact that the protected ""heap"" variable in PriorityQueue is initialized using an Object[] makes it impossible for subclasses of PriorityQueue to exist and access the ""heap"" array unless they bind the generic to Object."
1,document field lengths count analyzer synonym overlays. Using a synonym expansion analyzer to add tokens with zero offset from the substituted token should not extend the length of the field in the document (for scoring purposes)
1,"SegmentReader.hasSeparateNorms always returns false. The loop in that method looks like this: 
 
for(int i = 0; i < 0; i++){ 
 
I guess ""i < 0"" should be replaced by ""i < result.length""?"
1,"iterative removal of same-name sibling nodes might under certain circumstances throw unexpected exceptions. code fragment to reproduce the issue:

            // setup test
            if (root.hasNode(""tmp"")) {
                root.getNode(""tmp"").remove();
                session.save();
            }
            Node tmp = root.addNode(""tmp"");
            for (int i = 0; i < 4; i++) {
                Node a = tmp.addNode(""a"");
                System.out.println(""added "" + a.getPath());
            }
            session.save();

            // iterative removal of same name sibling child nodes
            NodeIterator ni = tmp.getNodes();
            while (ni.hasNext()) {
                Node n = ni.nextNode();
                System.out.println(""removing "" + n.getPath());
                n.remove();
                tmp.save();
            }

console output:

added /tmp/a
added /tmp/a[2]
added /tmp/a[3]
added /tmp/a[4]
removing /tmp/a
removing /tmp/a
removing /
javax.jcr.RepositoryException: /: cannot remove root node
	at org.apache.jackrabbit.core.ItemImpl.internalRemove(ItemImpl.java:766)
	at org.apache.jackrabbit.core.ItemImpl.remove(ItemImpl.java:997)
	at org.apache.jackrabbit.core.Test.main(Test.java:141)


note that the msg of the exception is misleading: the above code did never try to remove
the root node. 

the exception is caused by a bug in CachingHierarchyManager which fails to update
the cache correctly.

btw: if you comment the first logging stmt, i.e. 

                //System.out.println(""added "" + a.getPath());

the problem doesn't occur anymore."
1,"Handling of multiple residual prop defs in EffectiveNodeTypeImpl. org.apache.jackrabbit.jcr2spi.nodetype.EffectiveNodeTypeImpl currently rejects multiple residual property definitions, if they do not differ in getMultiple(). In fact, it should accept all combinations, so differing values for getOnParentVersionAction and other aspects should be accepted as well.

See JSR 170, 6.7.8:

""For purposes of the above, the notion of two definitions having the same name does not apply to two residual definitions. Two (or more) residual property or child node definitions with differing subattributes must be permitted to co-exist in the same effective node type. They are interpreted as disjunctive (ORed) options."""
1,"Host configuration properties not updated when the method is redirected. the above uri:

http://www.adobe.com/cgi-bin/redirect?http://lists.w3.org/Archives/Public/www-xsl-fo

generates two 302 responses:

from the original to http://lists.w3.org/Archives/Public/www-xsl-fo
and from that to http://lists.w3.org/Archives/Public/www-xsl-fo/

the client accepts and follows these redirects (a trace of the process shows it's working well) but when 
you ask the getmethod what uri we ended up at using the getURI() method it returns the bastardised 
result:

http://www.adobe.com/Archives/Public/www-xsl-fo/

instead of the correct 

http://lists.w3.org/Archives/Public/www-xsl-fo/

that the client has actually downloaded.

using cvsup'd copy showing version string "" Jakarta Commons-HttpClient/2.1m1"""
1,"Inconsistent scoring with SpanTermQuery in BooleanQuery. When a SpanTermQuery is added to a BooleanQuery, incorrect results are 
returned.

I am running Lucene 1.9 RC1 on Windows XP.  I have a test case which has 
several tests.  It has an index with 4 identical documents in it.

When two TermQuerys are used in a BooleanQuery, the score looks like this:
  4 hits for search: two term queries
    ID:1 (score:0.54932046)
    ID:2 (score:0.54932046)
    ID:3 (score:0.54932046)
    ID:4 (score:0.54932046)

Notice how it is correctly setting the score to be the same for each document.

When two SpanQuerys are used in a BooleanQuery, the score looks like this:
  2 hits for search: two span queries
    ID:1 (score:0.3884282)
    ID:4 (score:0.1942141)

Notice how it only returned two documents instead of four.  And the two it did 
return have differing scores.

I believe that there is an error in the scoring algorithm that is making the 
other two documents not show up."
1,"When HttpClient-Cache cannot open cache file, should act like miss. Set up HttpClient-Cache like this:
final String cacheDir = ""cachedir"";
HttpClient cachingHttpClient;
final CacheConfig cacheConfig = new CacheConfig();
cacheConfig.setSharedCache(false);
cacheConfig.setMaxObjectSizeBytes(262144); //256kb

if(! new File(cacheDir, ""httpclient-cache"").exists()){
	if(!new File(cacheDir, ""httpclient-cache"").mkdir()){
		throw new RuntimeException(""failed to create httpclient cache directory: "" + new File(cacheDir, ""httpclient-cache"").getAbsolutePath());
	}
}
final ResourceFactory resourceFactory = new FileResourceFactory(new File(cacheDir, ""httpclient-cache""));

final HttpCacheStorage httpCacheStorage = new ManagedHttpCacheStorage(cacheConfig);

cachingHttpClient = new CachingHttpClient(client, resourceFactory, httpCacheStorage, cacheConfig);

Then make a request:
final HttpGet get = new HttpGet(url);
final HttpResponse response = cachingHttpClient.execute(get);
final StatusLine statusLine = response.getStatusLine();
if (statusLine.getStatusCode() >= 300) {
	if(statusLine.getStatusCode() == 404)
		throw new NoResultException();
    throw new HttpResponseException(statusLine.getStatusCode(),
            statusLine.getReasonPhrase());
}
response.getEntity().getContent();

Everything worked as expected.

Now delete the cache directory (""cachedir/httpclient-cache"" in this example).

And make the same request again.

Actual:
 Caused by: java.lang.IllegalStateException: Content has been consumed
	at org.apache.http.entity.BasicHttpEntity.getContent(BasicHttpEntity.java:84)
	at org.apache.http.conn.BasicManagedEntity.getContent(BasicManagedEntity.java:100)

Expected:
HttpClient shouldn't throw an exception - it should just perform the request again acting like a cache miss."
1,"Several Codecs use the same files - PerFieldCodecWrapper can not hold two codec using the same files. Currently we have a rather simple file naming scheme which prevents us from using more than one codec in a segment that relies on the same file.  For instance pulsing and standard codec can not be used together since they both need the .frq .tii .tis etc. To make this work we either need to write distinct per codec files or set a per field / codec file ID. While the first solution seems to be quiet verbose the second one seems to be more flexible too.

One possibility to do that would be to assign a unique id to each SegmentsWriteState when opening the FieldsConsumer and write the IDs into the segments file to eventually load it once the segment is opened. Otherwise our PerFieldCodec feature will not be really flexible nor useful though.  "
1,"Findbugs reports and fixes. Ran findbugs 0.94.rc1 on 3.0RC4. 
Fixed a few of the obvious ones (patches to follow) and made notes on the 
remainder - see the //TODO markers in code.
Also created a findbugs target in build.xml - see appropriate patch file"
1,"infinite loop on 302 redirect with different host in Location: header. Using the CVS version - 2.1 rc I think.

Trying to get the url contents for a url with followRedirects specified and 
StrictMode off and I get a 302 redirect to a different url with a different 
host in the Location header causes it to go into an infinite loop (mercifully 
aborting at 100 redirects).  An example offending url: 

http://www.snowcrest.net/mice/mice.htm

The problem lies in HttpMethodBase.java version 1.177 at line 1225.  It does 
the following:

        if (getRequestHeader(""host"") != null) {
            LOG.debug(
                ""Request to add Host header ignored: header already added"");
            return;
        }

and there is already the Host header from the previous url request so it 
endlessly loops until it aborts at the max redirects (default is 100 I guess).  
I commented this out and it worked fine."
1,"Missing skip()-Method in ContentLengthInputStream. ContentLengthInputStream is missing the skip()-Method.

This causes the internal pos variable to get out of sync with the content 
length. 
We oberseved that closing the stream caused a wait time of about 15 sec in 
routines which use the skip()-method of InputStream.

Here's a possible implementation which should solve the problem:

    public long skip(long len) throws IOException {
        long count = super.skip(len);
        pos += count;
        return count;
    }"
1,"WorkspaceImpl.dispose() might cause ClassNotFoundException. Wenn using Jackrabbit in an environment, where ClassLoaders may get inactivated in the sense, the loading new classes is not possible anymore, shutting down the repository may result in a ClassNotFoundException during WorkspaceImpl.dispose().

Reason for this is, that in the dispose() method, the ObservationManager is asked for all registered event listeners for them to be removed from the ObservationManager one-by-one. Asking for the listeners results in a new EventListenerIteratorImpl object being created.

If now, this class has never been used during the live time of the repository, this would cause a ClassNotFoundException because the class loader is not laoding classes anymore in the specific environment.

The specific environment is Eclipse, where one plugin is managing different Repository instances provided by separate plugins. When now the Jackrabbit provider plugin has already been stopped while the managing plugin tries to shutdown the Jackrabbit repository, the EventListenerIteratorImpl class cannot be loaded anymore and disposing the WorkspaceImpl in a controlled way fails.

I suggest adding an ObservationManagerImpl.dispose() method, which is called by the WorkspaceImpl like :
    WorkspaceImpl.dispose() {
       if (obsMgr != null) {
         obsMgr.dispose();
         obsMgr = null;
        }
    }

As a side effect of not calling getObservationManager[Impl]() the observation manager would also not be created if not existing yet.

As a side effect to having the dispose method is, that the ObservationManagerImpl class could also do other cleanup work in addition to clearing the listener lists."
1,"Incorrect check for replace when importing item with colliding id. When fixing JCR-1128 bug was introduced due to incorrect check for UUID behavior. Current code is:
201 : 	 if (!(existing.getId().equals(id)
202 : 	&& (uuidBehavior == ImportUUIDBehavior.IMPORT_UUID_COLLISION_REMOVE_EXISTING
203 :	|| uuidBehavior == ImportUUIDBehavior.IMPORT_UUID_COLLISION_REMOVE_EXISTING))) {
204 :	throw new ItemExistsException(existing.safeGetJCRPath());
205 :	}

While it should check for ImportUUIDBehavior.IMPORT_UUID_COLLISION_REPLACE_EXISTING in one of the cases (line 202 or 203).
Also it is possible that id of imported item is not known and therefore value of ""id"" variable is null and check will always fail. Would be nice if this case can be handled as well.
"
1,"SQL query with jcr:path LIKE '/foo/%' only selects children. A query like: 

SELECT * FROM nt:base WHERE jcr:path LIKE '/foo/%'

only selects the children of /foo instead off all descendants of /foo."
1,"Session scoped lock not always removed on Session.logout(). Consider the following use case:

      Session s = repo.login(...);
      Node root = s.getRootNode();
      root.lock(true, true); // session-scoped, deep lock
      // modifiy items
      // root.isModified() still is true
      s.logout();

To my understanding, the session scoped locks should be removed (unlocked) and unsaved should be dropped on logout of a session. Unfortunately currently this is not the case, as the lock implementation gets notified by the SessionImpl on the logout situation and just calls Node.unlock() on the lock's node for session scoped locks. This method fails as there are unsaved changes. Hence after logout, the lock on the session is still there and will only be gone when the repository is stopped.
      "
1,"Merging multiple indexes does not maintain document order.. When I merge multiple indexes into a single, empty index, the document addition order is not being maintained.

Self contained test case coming (as soon as I figure out how to attach it)"
1,"Intermittent thread safety issue with EnwikiDocMaker. Intermittent thread safety issue with EnwikiDocMaker

When I run the conf/wikipediaOneRound.alg, sometimes it gets started
OK, other times (about 1/3rd the time) I see this:

     Exception in thread ""Thread-0"" java.lang.RuntimeException: java.io.IOException: Bad file descriptor
     	at org.apache.lucene.benchmark.byTask.feeds.EnwikiDocMaker$Parser.run(EnwikiDocMaker.java:76)
     	at java.lang.Thread.run(Thread.java:595)
     Caused by: java.io.IOException: Bad file descriptor
     	at java.io.FileInputStream.readBytes(Native Method)
     	at java.io.FileInputStream.read(FileInputStream.java:194)
     	at org.apache.xerces.impl.XMLEntityManager$RewindableInputStream.read(Unknown Source)
     	at org.apache.xerces.impl.io.UTF8Reader.read(Unknown Source)
     	at org.apache.xerces.impl.XMLEntityScanner.load(Unknown Source)
     	at org.apache.xerces.impl.XMLEntityScanner.scanQName(Unknown Source)
     	at org.apache.xerces.impl.XMLNSDocumentScannerImpl.scanStartElement(Unknown Source)
     	at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl$FragmentContentDispatcher.dispatch(Unknown Source)
     	at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanDocument(Unknown Source)
     	at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
     	at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
     	at org.apache.xerces.parsers.XMLParser.parse(Unknown Source)
     	at org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source)
     	at org.apache.lucene.benchmark.byTask.feeds.EnwikiDocMaker$Parser.run(EnwikiDocMaker.java:60)
     	... 1 more

The problem is that the thread that pulls the XML docs is started as
soon as EnwikiDocMaker class is instantiated.  When it's started, it
uses the fileIS (FileInputStream) to feed the XML Parser.  But,
openFile is actually called twice on starting the alg, if you use any
task deriving from ResetInputsTask, which closes the original fileIS
that the XML parser may be using.

I changed the thread to instead start on-demand the first time next()
is called.  I also removed a redundant resetInputs() call (which was
opening the file more frequently than needed).  Finally, I added logic
in the thread to detect that the input stream was closed (because
LineDocMaker.resetInputs() was called, eg, if we are not running the
doc maker to exhaustion).

"
1,"Avoid premature publication of XAItemStateManager. The XAItemStateManager constructor calls the super constructor (LocalItemStateManager)  which registers the instance as a listener with the SharedItemStateManager. The construction of the instance has not yet been finished, but it is accessible from the SharedItemStateManager. This can result in strange exceptions like the following:

java.lang.NullPointerException
        at org.apache.jackrabbit.core.state.XAItemStateManager.stateModified(XAItemStateManager.java:580)
        at org.apache.jackrabbit.core.state.StateChangeDispatcher.notifyStateModified(StateChangeDispatcher.java:111)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.stateModified(SharedItemStateManager.java:400)
        at org.apache.jackrabbit.core.virtual.AbstractVISProvider.stateModified(AbstractVISProvider.java:445)
        at org.apache.jackrabbit.core.state.ItemState.notifyStateUpdated(ItemState.java:244) 

The NPE is caused by the commitLogs field being null (it has not yet been initialized to its final value)."
1,"ConcurrentModificationException in IndexMerger. The IndexMerger.start() method can cause the following ConcurrentModificationException to be thrown since it doesn't protect against concurrent access to the busyMergers list. The workers started by the start() method will remove themselves from the busyMergers list, which makes it possible for a quick worker to concurrently modify the list before the start() method is finished iterating through it.

java.util.ConcurrentModificationException
	at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
	at java.util.AbstractList$Itr.next(AbstractList.java:343)
	at org.apache.jackrabbit.core.query.lucene.IndexMerger.start(IndexMerger.java:122)
	at org.apache.jackrabbit.core.query.lucene.MultiIndex.<init>(MultiIndex.java:325)
	at org.apache.jackrabbit.core.query.lucene.SearchIndex.doInit(SearchIndex.java:507)
	at org.apache.jackrabbit.core.query.AbstractQueryHandler.init(AbstractQueryHandler.java:78)
	at org.apache.jackrabbit.core.config.RepositoryConfigurationParser$1.getQueryHandler(RepositoryConfigurationParser.java:630)
	at org.apache.jackrabbit.core.config.WorkspaceConfig.getQueryHandler(WorkspaceConfig.java:215)
	at org.apache.jackrabbit.core.config.WorkspaceConfig.getQueryHandler(WorkspaceConfig.java:215)
	at org.apache.jackrabbit.core.SearchManager.<init>(SearchManager.java:173)
	at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.getSearchManager(RepositoryImpl.java:1868)
	at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.doPostInitialize(RepositoryImpl.java:2077)
	at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.initialize(RepositoryImpl.java:1996)
	at org.apache.jackrabbit.core.RepositoryImpl.initStartupWorkspaces(RepositoryImpl.java:535)
"
1,"Lock expires almost immediately. When a timeoutHint other than Long.MAX_VALUE is given to the javax.jcr.lock.LockManager API:

   lock(String absPath, boolean isDeep, boolean isSessionScoped, long timeoutHint, String ownerInfo)

a timeoutTime in seconds will be computed as follows (o.a.j.core.lock.LockInfo#updateTimeoutTime):

   long now = (System.currentTimeMillis() + 999) / 1000; // round up
   this.timeoutTime = now + timeoutHint;

the TimeoutHandler in o.a.j.core.lock.LockManagerImpl running every second will then check whether the timeout has expired (o.a.j.core.lock.LockInfo#isExpired):

    public boolean isExpired() {
        return timeoutTime != Long.MAX_VALUE
            && timeoutTime * 1000 > System.currentTimeMillis();
    }

Obviously, the latter condition is true from the very beginning. Replacing '>' with '<' or '<=' should do the trick."
1,"in trunk if you switch up omitNorms while indexing, you get a corrumpt norms file. document 1 has 
  body: norms=true
  title: norms=true
document 2 has 
  body: norms=false
  title: norms=true

when seeing 'body' for the first time, normswriterperfield gets 'initial fieldinfo' and 
saves it away, which says norms=true

however, at flush time we dont check, so we write the norms happily anyway.
then SegmentReader reads the norms later: it skips ""body"" since it omits norms
and if you ask for the norms of 'title' it instead returns the bogus ""body"" norms.

asserting that SegmentReader ""plans to"" read the whole .nrm file exposes the bug."
1,"WorkspaceManager.dispose() should wait until change feed thread is stopped. The WorkspaceManager currently only interrupts the change feed thread, but does not wait until it stops."
1,"Preemptive auth flags disregarded during ssl tunnel creation. Using a Squid2.4 proxy, the connection is dropped when trying to connect to a 
ssl site. In order for the connection to remain open, preemptive authorization 
is needed for the proxy. The preemptive authorization flags are not propagated 
down to where the ssl tunnel is created in HttpMethodDirectors executeConnect 
method. A new ConnectMethod object is created for the tunnel but the preemptive 
flags set as parameters are not being set on the new ConnectMethod object.

Here is the code that would replicate the problem using a Squid(2.4) proxy :

HttpClient client = new HttpClient();
client.getHostConfiguration().setProxyHost(new ProxyHost(""someproxy"", 3128));
client.getParams().setAuthenticationPreemptive(true);
client.getState().setProxyCredentials(AuthScope.ANY, new 
UsernamePasswordCredentials(""user"", ""password""));
GetMethod httpget = new GetMethod(""https://www.verisign.com/"");
httpget.getProxyAuthState().setPreemptive();
client.executeMethod(httpget);
httpget.releaseConnection();"
1,"Abort Before Execute & Various Other Times Fails. With svn commit #639506, a few more scenarios become testable & can be fixed.  These are: aborting before HttpClient.execute is called, aborting between setting the connection request for aborting and setting the connection release trigger, and aborting after a redirected route uses a new connection request.  As of r639506, those three scenarios fail to abort correctly."
1,[PATCH] Fix possible Null Ptr exception in ConnectionFactory. code will throw npe if driver string is null - patch fixes this.
1,"RangeQuery - add equals and hashCode methods. I'm attaching a patch with an equals() and hashCode() implementation for 
RangeQuery.java, and a new unit test for TestRangeQuery.java, as per a recent 
message on lucene-user mailing list (subject ""RangeQuery doesn't override 
equals() or hashCode() - intentional?"")

patches to follow"
1,"Kerberos cross-realm support is broken. This issue is basically based on the same facts as this issue https://issues.sonatype.org/browse/AHC-71?focusedCommentId=129559#action_129559 
Since the Kerberos code looks the same, I assume that AHC used your code. The same patch can be applied to fix [this http://hc.apache.org/httpcomponents-client-ga/httpclient/xref/org/apache/http/impl/auth/NegotiateScheme.html#200] defective code."
1,"intermittent failure in TestIndexWriter. testRandomIWReader. Rarely, this test (which was added with LUCENE-1516) fails in MockRAMDirectory.close because some files were not closed, eg:
{code}
   [junit] NOTE: random seed of testcase 'testRandomIWReader' was: -5001333286299627079
   [junit] ------------- ---------------- ---------------
   [junit] Testcase: testRandomIWReader(org.apache.lucene.index.TestStressIndexing2):        Caused an ERROR
   [junit] MockRAMDirectory: cannot close: there are still open files: {_cq.tvx=3, _cq.fdx=3, _cq.tvf=3, _cq.tvd=3, _cq.fdt=3}
   [junit] java.lang.RuntimeException: MockRAMDirectory: cannot close: there are still open files: {_cq.tvx=3, _cq.fdx=3, _cq.tvf=3, _cq.tvd=3, _cq.fdt=3}
   [junit]     at org.apache.lucene.store.MockRAMDirectory.close(MockRAMDirectory.java:292)
   [junit]     at org.apache.lucene.index.TestStressIndexing2.testRandomIWReader(TestStressIndexing2.java:66)
   [junit]     at org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:88)
{code}"
1,"bogus positions create a corrumpt index. Its pretty common that positionIncrement can overflow, this happens really easily 
if people write analyzers that don't clearAttributes().

It used to be the case that if this happened (and perhaps still is in 3.x, i didnt check),
that IW would throw an exception.

But i couldnt find the code checking this, I wrote a test and it makes a corrumpt index..."
1,"DeleteByPercentTask hits NPE. I'm building up Wiki indices for testing search perf across 3.x/4.0, but hit NPE when creating deletions in 4.0 due to flex cutover..."
1,"Deadlock: IndexWriter.addIndexes(IndexReader[]). A deadlock issue occurs under the following circumstances
- IndexWriter.autoCommit == true
- IndexWriter.directory contains multiple segments
- IndexWriter.AddIndex(IndexReader[]) is invoked

I put together a JUnit test that recreates the deadlock, which I've attached.  It is the first test method, 'testAddIndexByIndexReader()'.

In a nutshell, here is what happens:

        // 1) AddIndexes(IndexReader[]) acquires the write lock,
        // then begins optimization of destination index (this is
        // prior to adding any external segments).
        //
        // 2) Main thread starts a ConcurrentMergeScheduler.MergeThread
        // to merge the 2 segments.
        //
        // 3) Merging thread tries to acquire the read lock at
        // IndexWriter.blockAddIndexes(boolean) in
        // IndexWriter.StartCommit(), but cannot as...
        //
        // 4) Main thread still holds the write lock, and is
        // waiting for the IndexWriter.runningMerges data structure
        // to be devoid of merges with their optimize flag
        // set (IndexWriter.optimizeMergesPending()).
"
1,"Property.getValue() throws RepositoryException with internal error. Running ConcurrentReadWriteTest (NUM_NODES=5, NUM_THREADS=3, RUN_NUM_SECONDS=120) resulted in a RepositoryException calling Property.getValue():

javax.jcr.RepositoryException: Internal error while retrieving value of b3fc1ea8-3364-4236-bcc7-dea0baf90640/{}test: null: null

Debugging shows that it is a NullPointerException:

java.lang.NullPointerException
	at org.apache.jackrabbit.core.PropertyImpl.getValue(PropertyImpl.java:481)
	at org.apache.jackrabbit.core.ConcurrentReadWriteTest$1$1.execute(ConcurrentReadWriteTest.java:68)
	at org.apache.jackrabbit.core.AbstractConcurrencyTest$Executor.run(AbstractConcurrencyTest.java:110)
	at java.lang.Thread.run(Thread.java:619)

It's probably the state which has been discarded after the sanityCheck()."
1,"Bundle binding deserialization problem. I'm trying to upgrade from 1.3.x to jackrabbit 1.4.x (branch)  and have problems with existing repostories (probaly the same issue is with 1.5.x)

Caused by: org.apache.jackrabbit.core.state.ItemStateException: failed to read bundle: deadbeef-face-babe-cafe-babecafebabe: java.lang.IllegalArgumentException: invalid namespaceURI specified
 at org.apache.jackrabbit.core.persistence.bundle.BundleDbPersistenceManager.loadBundle(BundleDbPersistenceManager.java:1229)
 at org.apache.jackrabbit.core.persistence.bundle.BundleDbPersistenceManager.loadBundle(BundleDbPersistenceManager.java:1161)

It looks that issue was introduced by resolving JCR-1632"
1,"contrib-spatial java.lang.UnsupportedOperationException on QueryWrapperFilter.getDocIdSet. We use in our Project (which is in the devel phase) the latest Snapshot release of lucene. After i updated to the latest Snapshot a few days ago one of our JUnit tests fails and throws the following error:

java.lang.UnsupportedOperationException
	at org.apache.lucene.search.Query.createWeight(Query.java:91)
	at org.apache.lucene.search.QueryWrapperFilter.getDocIdSet(QueryWrapperFilter.java:72)
	at org.apache.lucene.misc.ChainedFilter.getDISI(ChainedFilter.java:150)
	at org.apache.lucene.misc.ChainedFilter.initialResult(ChainedFilter.java:173)
	at org.apache.lucene.misc.ChainedFilter.getDocIdSet(ChainedFilter.java:211)
	at org.apache.lucene.misc.ChainedFilter.getDocIdSet(ChainedFilter.java:141)
	at org.apache.lucene.search.ConstantScoreQuery$ConstantScorer.<init>(ConstantScoreQuery.java:116)
	at org.apache.lucene.search.ConstantScoreQuery$ConstantWeight.scorer(ConstantScoreQuery.java:81)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:244)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:172)
	at org.apache.lucene.search.Searcher.search(Searcher.java:183)
	at org.hibernate.search.query.QueryHits.updateTopDocs(QueryHits.java:100)
	at org.hibernate.search.query.QueryHits.<init>(QueryHits.java:61)
	at org.hibernate.search.query.QueryHits.<init>(QueryHits.java:51)
	at org.hibernate.search.query.FullTextQueryImpl.getQueryHits(FullTextQueryImpl.java:373)
	at org.hibernate.search.query.FullTextQueryImpl.list(FullTextQueryImpl.java:293)
	...

I think it appeared after the Hudson build 917... and the following commit of the Query.java http://hudson.zones.apache.org/hudson/job/Lucene-trunk/917/changes#detail4 and is in connection with this JIRA issue: LUCENE-1771
I hope i'm at the right place and that you can fix it. Thanks!"
1,"The CredentialsWrapper should use a empty String as userId if custom Credentials are used. If custom Credentials are used we get a IllegalArgumentException from the AbstractQValueFactory while executing SessionItemStateManager.computeSystemGeneratedPropertyValues().
The 2 Properties jcr:createdBy and jcr:lastModified could not be created."
1,"Calling size method of a ManageableArrayList causes NullPointerException. When using the NTCollectionConverterImpl with proxy=""true"" a call on the size () method of a ManageableArrayList causes a NullPointerException if there is no underlying List. LazyCollectionLoader doLoad returns null because there is are no children.

The ManageableArrayList is created because the isNull method of the NTCollectionConverterImpl class always returns false. 
According to the comment line this is done because the getCollectionNodes always returns a list. 
But after the fix for JCR-882 this is not correct anymore.

The attached fix corrects this. 

The only question remaining is how to differ between an empty list and a null-value for the field containing the list."
1,"rep:similar in xpath does not work. This query //*[rep:similar(., '/content/en')] produces an exception:

24.09.2009 16:56:48.156 *ERROR* [0:0:0:0:0:0:0:1%0 [1253804208093] GET /libs/cq/search/content/querydebug.html HTTP/1.1] org.apache.sling.engine.impl.SlingMainServlet service: Uncaught SlingException java.lang.ArrayIndexOutOfBoundsException: -1
	at org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder.visit(LuceneQueryBuilder.java:612)
	at org.apache.jackrabbit.spi.commons.query.RelationQueryNode.accept(RelationQueryNode.java:115)
	at org.apache.jackrabbit.spi.commons.query.NAryQueryNode.acceptOperands(NAryQueryNode.java:143)
	at org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder.visit(LuceneQueryBuilder.java:489)
	at org.apache.jackrabbit.spi.commons.query.LocationStepQueryNode.accept(LocationStepQueryNode.java:166)
	at org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder.visit(LuceneQueryBuilder.java:468)
	at org.apache.jackrabbit.spi.commons.query.PathQueryNode.accept(PathQueryNode.java:74)
	at org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder.visit(LuceneQueryBuilder.java:257)
	at org.apache.jackrabbit.spi.commons.query.QueryRootNode.accept(QueryRootNode.java:115)
	at org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder.createLuceneQuery(LuceneQueryBuilder.java:247)
	at org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder.createQuery(LuceneQueryBuilder.java:227)
	at org.apache.jackrabbit.core.query.lucene.QueryImpl.execute(QueryImpl.java:111)
	at org.apache.jackrabbit.core.query.QueryImpl.execute(QueryImpl.java:127)
"
1,"Possible rare thread hazard in IW.commit. I was seeing a very rare intermittent failure in TestIndexWriter.testCommitThreadSafety.

The issue happens if one thread calls commit while another is flushing, and is exacerbated at high flush rates (eg maxBufferedDocs=2).  The thread doing commit will first flush, and then it syncs the files.  However in between those two, if other threads manage to add enough docs and trigger another flush, a 2nd new segment can sneak into the SegmentInfos before we sync.

This is normally harmless, in that it just means the commit includes a few more docs that had been added by other threads, so it's fine. But, it can mean that a committed segment references the still-open doc store files.  Our tests now catch this (I changed MockDirWrapper to throw an exception in this case), and so testCommitThreadSafety can fail with this exception.  If you hardwire the maxBufferedDocs to 2 it happens quite often.

It's not clear this is really a problem in real apps vs just our anal MockDirWrapper but I think we should fix it..."
1,The move method doesn't remove the source node. Here is a small unit test that demonstrate that the method move doesn't remove the source node. 
1,webapp: troubleshooting.jsp fails. 
1,"Rare thread hazard in IndexWriter.commit(). The nightly build 2 nights ago hit this:

{code}
 NOTE: random seed of testcase 'testAtomicUpdates' was: -5065675995121791051
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testAtomicUpdates(org.apache.lucene.index.TestAtomicUpdate):	FAILED
    [junit] expected:<100> but was:<91>
    [junit] junit.framework.AssertionFailedError: expected:<100> but was:<91>
    [junit] 	at org.apache.lucene.index.TestAtomicUpdate.runTest(TestAtomicUpdate.java:142)
    [junit] 	at org.apache.lucene.index.TestAtomicUpdate.testAtomicUpdates(TestAtomicUpdate.java:194)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:88)
{code}

It's an intermittant failure that only happens when multiple threads
are calling commit() at once.  With autoComit=true and
ConcurrentMergeScheduler, this can happen more often because each
merge thread calls commit after it's done.

The problem happens when one thread has already begun the commit
process, but another two or more threads then come along wanting to
also commit after further changes have happened.  Those two or more
threads would wait until the currently committing thread finished, and
then they'd wake up and do their commit.  The problem was, after
waking up they would fail to check whether they had been superseded,
ie whether another thread had already committed more up-to-date
changes.

The fix is simple -- after waking up, check again if your commit has
been superseded, and skip your commit if so.
"
1,"DBDataStore doesn't support concurrent reads. My understanding is that setting parameter copyWhenReading to true should allow concurrent reads by spooling binary property to temporary file and free database resources (connection) immediately to make it available for other threads.

After applying patch for JCR-1388, DBDataStore doesn't support concurrent reads anymore, resultSet is kept open and db connection is blocked until the stream is read and closed. When copyWhenReading is set to true db connection should be released immediately, this is the reason i guess why temporary file is used."
1,"TermSpans skipTo() doesn't always move forwards. In TermSpans (or the anonymous Spans class returned by SpansTermQuery, depending on the version), the skipTo() method is improperly implemented if the target doc is less than or equal to the current doc:

  public boolean skipTo(int target) throws IOException {
          // are we already at the correct position?
          if (doc >= target) {
            return true;
          }

          ...


This violates the correct behavior (as described in the Spans interface documentation), that skipTo() should always move forwards, in other words the correct implementation would be:

if (doc >= target) {
  return next();
}

This bug causes particular problems if one wants to use the payloads feature - this is because if one loads a payload, then performs a skipTo() to the same document, then tries to load the ""next"" payload, the spans hasn't changed position and it attempts to load the same payload again (which is an error)."
1,"NoSuchItemStateException on removing node (no versioning). I'm using jackrabbit 1.2.1
with no versioning
with a very simple SimpleAccessManager (this try to compute the path of the passed ItemId and verify permissions over that path)

when I remove a node (nt:file or nt:folder),  calling session.save() I obtains the exception reported below.

is it really a bug or am i wrong?
thanks

the following is the code I'm using to build the path
----------------------------------------- CODE START
public String getStringPath(ItemId id) throws ItemNotFoundException, RepositoryException, NoPrefixDeclaredException
	{
		String p = """";
		NamespaceResolver nsResolver = ((HierarchyManagerImpl) hierMgr).getNamespaceResolver();
		Path path = hierMgr.getPath(id);
		PathElement[] pe = path.getElements();
		for (int i = 0; i < pe.length; i++)
		{
			if (pe[i].denotesName())
				p += ""/"" + pe[i].toJCRName(nsResolver);
		}
		return p;
	}
----------------------------------------- CODE END


----------------------------------------- START
javax.jcr.ItemNotFoundException: failed to build path of d688e92f-26ae-4f7c-aba7-aaff1df62c2c: d688e92f-26ae-4f7c-aba7-aaff1df62c2c: d688e92f-26ae-4f7c-aba7-aaff1df62c2c
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getPath(HierarchyManagerImpl.java:362)
	at org.apache.jackrabbit.core.CachingHierarchyManager.getPath(CachingHierarchyManager.java:224)
	at it.unict.faq.jackrabbit.SimpleAccessManager.getStringPath(SimpleAccessManager.java:238)
	at it.unict.faq.jackrabbit.SimpleAccessManager.controllo(SimpleAccessManager.java:215)
	at it.unict.faq.jackrabbit.SimpleAccessManager.isGranted(SimpleAccessManager.java:183)
	at org.apache.jackrabbit.core.ItemImpl.validateTransientItems(ItemImpl.java:645)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1162)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:821)
	at it.unict.faq.driver.manager.impl.DAO.JcrDAO.CancellaNodo(JcrDAO.java:638)
	at it.unict.faq.driver.manager.impl.DocumentServerManager.ds_del(DocumentServerManager.java:58)
	at elearn.portal.action.ds_del_portal.execute(ds_del_portal.java:28)
	at org.apache.struts.action.RequestProcessor.processActionPerform(RequestProcessor.java:421)
	at org.apache.struts.action.RequestProcessor.process(RequestProcessor.java:226)
	at org.apache.struts.action.ActionServlet.process(ActionServlet.java:1158)
	at org.apache.struts.action.ActionServlet.doPost(ActionServlet.java:415)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:709)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:252)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:173)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:264)
	at org.acegisecurity.intercept.web.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:107)
	at org.acegisecurity.intercept.web.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:72)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.ui.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:110)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.providers.anonymous.AnonymousProcessingFilter.doFilter(AnonymousProcessingFilter.java:125)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.ui.rememberme.RememberMeProcessingFilter.doFilter(RememberMeProcessingFilter.java:142)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.wrapper.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:81)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.ui.AbstractProcessingFilter.doFilter(AbstractProcessingFilter.java:217)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.ui.logout.LogoutFilter.doFilter(LogoutFilter.java:108)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.context.HttpSessionContextIntegrationFilter.doFilter(HttpSessionContextIntegrationFilter.java:193)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.util.FilterChainProxy.doFilter(FilterChainProxy.java:148)
	at org.acegisecurity.util.FilterToBeanProxy.doFilter(FilterToBeanProxy.java:98)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:202)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:173)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:213)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:178)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:126)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:105)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:107)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:148)
	at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:869)
	at org.apache.coyote.http11.Http11BaseProtocol$Http11ConnectionHandler.processConnection(Http11BaseProtocol.java:664)
	at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(PoolTcpEndpoint.java:527)
	at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt(LeaderFollowerWorkerThread.java:80)
	at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadPool.java:684)
	at java.lang.Thread.run(Thread.java:595)
Caused by: org.apache.jackrabbit.core.state.NoSuchItemStateException: d688e92f-26ae-4f7c-aba7-aaff1df62c2c
	at org.apache.jackrabbit.core.state.SessionItemStateManager.getTransientItemState(SessionItemStateManager.java:323)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:154)
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getItemState(HierarchyManagerImpl.java:120)
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getPath(HierarchyManagerImpl.java:357)
	... 52 more
org.apache.jackrabbit.core.state.NoSuchItemStateException: d688e92f-26ae-4f7c-aba7-aaff1df62c2c
	at org.apache.jackrabbit.core.state.SessionItemStateManager.getTransientItemState(SessionItemStateManager.java:323)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:154)
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getItemState(HierarchyManagerImpl.java:120)
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getPath(HierarchyManagerImpl.java:357)
	at org.apache.jackrabbit.core.CachingHierarchyManager.getPath(CachingHierarchyManager.java:224)
	at it.unict.faq.jackrabbit.SimpleAccessManager.getStringPath(SimpleAccessManager.java:238)
	at it.unict.faq.jackrabbit.SimpleAccessManager.controllo(SimpleAccessManager.java:215)
	at it.unict.faq.jackrabbit.SimpleAccessManager.isGranted(SimpleAccessManager.java:183)
	at org.apache.jackrabbit.core.ItemImpl.validateTransientItems(ItemImpl.java:645)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1162)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:821)
	at it.unict.faq.driver.manager.impl.DAO.JcrDAO.CancellaNodo(JcrDAO.java:638)
	at it.unict.faq.driver.manager.impl.DocumentServerManager.ds_del(DocumentServerManager.java:58)
	at elearn.portal.action.ds_del_portal.execute(ds_del_portal.java:28)
	at org.apache.struts.action.RequestProcessor.processActionPerform(RequestProcessor.java:421)
	at org.apache.struts.action.RequestProcessor.process(RequestProcessor.java:226)
	at org.apache.struts.action.ActionServlet.process(ActionServlet.java:1158)
	at org.apache.struts.action.ActionServlet.doPost(ActionServlet.java:415)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:709)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:252)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:173)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:264)
	at org.acegisecurity.intercept.web.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:107)
	at org.acegisecurity.intercept.web.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:72)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.ui.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:110)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.providers.anonymous.AnonymousProcessingFilter.doFilter(AnonymousProcessingFilter.java:125)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.ui.rememberme.RememberMeProcessingFilter.doFilter(RememberMeProcessingFilter.java:142)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.wrapper.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:81)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.ui.AbstractProcessingFilter.doFilter(AbstractProcessingFilter.java:217)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.ui.logout.LogoutFilter.doFilter(LogoutFilter.java:108)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.context.HttpSessionContextIntegrationFilter.doFilter(HttpSessionContextIntegrationFilter.java:193)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.util.FilterChainProxy.doFilter(FilterChainProxy.java:148)
	at org.acegisecurity.util.FilterToBeanProxy.doFilter(FilterToBeanProxy.java:98)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:202)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:173)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:213)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:178)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:126)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:105)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:107)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:148)
	at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:869)
	at org.apache.coyote.http11.Http11BaseProtocol$Http11ConnectionHandler.processConnection(Http11BaseProtocol.java:664)
	at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(PoolTcpEndpoint.java:527)
	at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt(LeaderFollowerWorkerThread.java:80)
	at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadPool.java:684)
	at java.lang.Thread.run(Thread.java:595)
----------------------------------------- END"
1,"DbDatastore: Problems indexing pdf file. As reported by Claus Kll:

When importing a pdf file into a repository configured with a DbDataStore the following exception occurs. This happens only when using the DbDataStore with copyWhenReading=true

java.io.IOException: Stream closed
       at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:156)
       at java.io.BufferedInputStream.read(BufferedInputStream.java:315)
       at org.apache.jackrabbit.core.data.db.TempFileInputStream.read(TempFileInputStream.java:107)
       at java.io.BufferedInputStream.read1(BufferedInputStream.java:265)
       at java.io.BufferedInputStream.read(BufferedInputStream.java:324)
       at java.io.BufferedInputStream.fill(BufferedInputStream.java:229)
       at java.io.BufferedInputStream.read(BufferedInputStream.java:246)
       at java.io.FilterInputStream.read(FilterInputStream.java:89)
       at java.io.PushbackInputStream.read(PushbackInputStream.java:141)
       at org.pdfbox.io.PushBackInputStream.peek(PushBackInputStream.java:71)
       at org.pdfbox.io.PushBackInputStream.isEOF(PushBackInputStream.java:88)
       at org.pdfbox.pdfparser.PDFParser.parseObject(PDFParser.java:370)
       at org.pdfbox.pdfparser.PDFParser.parse(PDFParser.java:176)"
1,"InternalXAResource.rollback() can be called twice and without prepare. during the prepare phase in the transaction context, each resource is 'prepared'. if one of them fails to prepare, the rest is rolledback, and later all of them are rolledback again. this can cause that:
- a resource that is never prepared is rolled back (which is ok, since is may need to cleanup stuff)
- a resource's rollback() may be called twice (which i don't know, if it's ok)

however, some of the resources are buggy and can't handle neither case correctly."
1,"TestIndexWriter failure: AIOOBE. trunk: r1133486 
{code}
    [junit] Testsuite: org.apache.lucene.index.TestIndexWriter
    [junit] Testcase: testEmptyFieldName(org.apache.lucene.index.TestIndexWriter):      Caused an ERROR
    [junit] CheckIndex failed
    [junit] java.lang.RuntimeException: CheckIndex failed
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:158)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:144)
    [junit]     at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:477)
    [junit]     at org.apache.lucene.index.TestIndexWriter.testEmptyFieldName(TestIndexWriter.java:857)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1362)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1280)
    [junit] 
    [junit] 
    [junit] Tests run: 39, Failures: 0, Errors: 1, Time elapsed: 17.634 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] CheckIndex failed
    [junit] Segments file=segments_1 numSegments=1 version=FORMAT_4_0 [Lucene 4.0]
    [junit]   1 of 1: name=_0 docCount=1
    [junit]     codec=SegmentCodecs [codecs=[PreFlex], provider=org.apache.lucene.index.codecs.CoreCodecProvider@3f78807]
    [junit]     compound=false
    [junit]     hasProx=true
    [junit]     numFiles=8
    [junit]     size (MB)=0
    [junit]     diagnostics = {os.version=2.6.39-gentoo, os=Linux, lucene.version=4.0-SNAPSHOT, source=flush, os.arch=amd64, java.version=1.6.0_25, java.vendor=Sun Microsystems Inc.}
    [junit]     no deletions
    [junit]     test: open reader.........OK
    [junit]     test: fields..............OK [1 fields]
    [junit]     test: field norms.........OK [1 fields]
    [junit]     test: terms, freq, prox...ERROR: java.lang.ArrayIndexOutOfBoundsException: -1

    [junit] java.lang.ArrayIndexOutOfBoundsException: -1
    [junit]     at org.apache.lucene.index.codecs.preflex.TermInfosReader.seekEnum(TermInfosReader.java:212)
    [junit]     at org.apache.lucene.index.codecs.preflex.TermInfosReader.seekEnum(TermInfosReader.java:301)
    [junit]     at org.apache.lucene.index.codecs.preflex.TermInfosReader.get(TermInfosReader.java:234)
    [junit]     at org.apache.lucene.index.codecs.preflex.TermInfosReader.terms(TermInfosReader.java:371)
    [junit]     at org.apache.lucene.index.codecs.preflex.PreFlexFields$PreTermsEnum.reset(PreFlexFields.java:719)
    [junit]     at org.apache.lucene.index.codecs.preflex.PreFlexFields$PreTerms.iterator(PreFlexFields.java:249)
    [junit]     at org.apache.lucene.index.PerFieldCodecWrapper$FieldsReader$FieldsIterator.terms(PerFieldCodecWrapper.java:147)
    [junit]     at org.apache.lucene.index.CheckIndex.testTermIndex(CheckIndex.java:610)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:495)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:154)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:144)
    [junit]     at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:477)
    [junit]     at org.apache.lucene.index.TestIndexWriter.testEmptyFieldName(TestIndexWriter.java:857)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)
    [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit]     at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1362)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1280)
    [junit]     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    [junit]     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    [junit]     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:422)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:931)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:758)
    [junit]     test: stored fields.......OK [0 total field count; avg 0 fields per doc]
    [junit]     test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
    [junit] FAILED

    [junit]     WARNING: fixIndex() would remove reference to this segment; full exception:
    [junit] java.lang.RuntimeException: Term Index test failed
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:508)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:154)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:144)
    [junit]     at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:477)
    [junit]     at org.apache.lucene.index.TestIndexWriter.testEmptyFieldName(TestIndexWriter.java:857)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)
    [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit]     at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1362)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1280)
    [junit]     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    [junit]     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    [junit]     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:422)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:931)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:758)
    [junit] 
    [junit] WARNING: 1 broken segments (containing 1 documents) detected
    [junit] 
    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testEmptyFieldName -Dtests.seed=-3770357642070518646:-3121175410586002489 -Dtests.multiplier=3
    [junit] NOTE: test params are: codec=PreFlex, locale=zh, timezone=Indian/Antananarivo
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestDateTools, TestDeletionPolicy, TestDocsAndPositions, TestFlex, TestIndexReaderCloneNorms, TestIndexWriter]
    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=85972280,total=232521728
    [junit] ------------- ---------------- ---------------
    [junit] TEST org.apache.lucene.index.TestIndexWriter FAILED
{code}"
1,"Requests are retried 3 times unconditionaly. Using the 20020811 tarball and jdk1.4.0, a get or post will retry as soon
as it finishes sending the request. I turned on logging and verified that
as soon as the last \r\n hits the wire, it starts on the next retry. For
example:

08-10 09:53:12 [main] httpclient.wire: >> [\r\n]
08-10 09:53:12 [main] httpclient.methods.PostMethod: enter
PostMethod.writeRequestBody(HttpState, HttpConnection)
08-10 09:53:12 [main] commons.httpclient.HttpConnection: enter
HttpConnection.write(byte[], int, int)
08-10 09:53:12 [main] commons.httpclient.HttpMethod: Attempt number 3 to write
request
08-10 09:53:12 [main] commons.httpclient.HttpMethod: enter
HttpMethodBase.writeRequest(HttpState, HttpConnection)
08-10 09:53:12 [main] commons.httpclient.HttpMethod: enter
HttpMethodBase.writeRequestLine(HttpState, HttpConnection)
08-10 09:53:12 [main] commons.httpclient.HttpMethod: enter
HttpMethodBase.generateRequestLine(HttpConnection, String, String, String,
String)
08-10 09:53:12 [main] commons.httpclient.HttpConnection: enter
HttpConnection.print(String)
08-10 09:53:12 [main] commons.httpclient.HttpConnection: enter
HttpConnection.write(byte[])
08-10 09:53:12 [main] httpclient.wire: >> ""POST /lookup.jsp HTTP/1.1"" [\r\n]

The top line is the end of the second post and the last line is the start
of the third post.

To make sure the server really wasn't sending something back, I wrote a
quick server that would listen for a request and send a 404 as soon as it
read a post or get line (but would keep reading and dumping info). In the
httpclient log, it still shoots off 3 requests before it receives the
response and the server got all three requests. (client and server are
running on the same machine)

So why is httpclient sending three requests without waiting for a
response?"
1,"ChunkedInputStream broken (2 bugs + fixes, 1 suggestion). Bug 1.

In the     

read(byte[] b, int off, int len)  method

of ChunkedInputStream, the number of bytes to read from the underlying 
InputStream is calculated wrongly. In the code this is done by

len = Math.min(len, chunkSize);

This could (and will) cause the server (also Apache) to indeed serve that 
number of bytes (let's say chunkSize), but it may be that we already had a 
number of bytes on the first read. The result is that the input is now NOT 
positioned on the end of a chunk and the rest of the reader fails because it 
cannot find CRLF or a valid chunksize.

Proposed fix (works, tested)
len = Math.min(len, chunkSize-pos);

Bug 2.

In the calculation of the chunkSize (method getChunkSizeFromInputStream) the 
conversion to int is done by calling             

result = Integer.parseInt(dataString, 16);

This is not robust and causes the occasional crash. The fix is simple and in 
fact implements what is done when the chunkSize is commented (see lines in code 
above)

result = Integer.parseInt(dataString.trim(), 16);

Tested and works.



Suggestion:
Same routine, input state machine. Perhaps just being pedantic..change while 
loop to:

        while (state != 2) {
            int b = in.read();
            if (b == -1) throw new IOException(""chunked stream ended 
unexpectedly"");
            switch (state) {
                case 0:
                    if (b == '\r')
                      state = 1;
                    else
                      baos.write(b);
                    break;
                case 1:
                    if (b == '\n')
                      state = 2;
                    else{
                     // this was not CRLF, so now write '\r' + this char
                      baos.write('\r');
                      baos.write(b);
                      state = 0;
                    }
                    break;
                default: throw new RuntimeException(""assertion failed"");
            }
        }.

In the     

read(byte[] b, int off, int len)  method

of ChunkedInputStream, the number of bytes to read from the underlying 
InputStream is calculated wrongly. In the code this is done by

len = Math.min(len, chunkSize);

This could (and will) cause the server (also Apache) to indeed serve that 
number of bytes (let's say chunkSize), but it may be that we already had a 
number of bytes on the first read. The result is that the input is now NOT 
positioned on the end of a chunk and the rest of the reader fails because it 
cannot find CRLF or a valid chunksize.

Proposed fix (works, tested)
len = Math.min(len, chunkSize-pos);

Bug 1.

In the     

read(byte[] b, int off, int len)  method

of ChunkedInputStream, the number of bytes to read from the underlying 
InputStream is calculated wrongly. In the code this is done by

len = Math.min(len, chunkSize);

This could (and will) cause the server (also Apache) to indeed serve that 
number of bytes (let's say chunkSize), but it may be that we already had a 
number of bytes on the first read. The result is that the input is now NOT 
positioned on the end of a chunk and the rest of the reader fails because it 
cannot find CRLF or a valid chunksize.

Proposed fix (works, tested)
len = Math.min(len, chunkSize-pos);

Bug 2.

In the calculation of the chunkSize (method getChunkSizeFromInputStream) the 
conversion to int is done by calling             

result = Integer.parseInt(dataString, 16);

This is not robust and causes the occasional crash. The fix is simple and in 
fact implements what is done when the chunkSize is commented (see lines in code 
above)

result = Integer.parseInt(dataString.trim(), 16);

Tested and works.



Suggestion:
Same routine, input state machine. Perhaps just being pedantic..change while 
loop to:

        while (state != 2) {
            int b = in.read();
            if (b == -1) throw new IOException(""chunked stream ended 
unexpectedly"");
            switch (state) {
                case 0:
                    if (b == '\r')
                      state = 1;
                    else
                      baos.write(b);
                    break;
                case 1:
                    if (b == '\n')
                      state = 2;
                    else{
                     // this was not CRLF, so now write '\r' + this char
                      baos.write('\r');
                      baos.write(b);
                      state = 0;
                    }
                    break;
                default: throw new RuntimeException(""assertion failed"");
            }
        }.

In the calculation of the chunkSize (method getChunkSizeFromInputStream) the 
conversion to int is done by calling             

result = Integer.parseInt(dataString, 16);

This is not robust and causes the occasional crash. The fix is simple and in 
fact implements what is done when the chunkSize is commented (see lines in code 
above)

result = Integer.parseInt(dataString.trim(), 16);

Tested and works.



Suggestion:
Same routine, input state machine. Perhaps just being pedantic..change while 
loop to:

        while (state != 2) {
            int b = in.read();
            if (b == -1) throw new IOException(""chunked stream ended 
unexpectedly"");
            switch (state) {
                case 0:
                    if (b == '\r')
                      state = 1;
                    else
                      baos.write(b);
                    break;
                case 1:
                    if (b == '\n')
                      state = 2;
                    else{
                     // this was not CRLF, so now write '\r' + this char
                      baos.write('\r');
                      baos.write(b);
                      state = 0;
                    }
                    break;
                default: throw new RuntimeException(""assertion failed"");
            }
        }"
1,"TestBytesRefHash#testCompact is broken. TestBytesRefHash#testCompact fails when run with ant test -Dtestcase=TestBytesRefHash -Dtestmethod=testCompact -Dtests.seed=-7961072421643387492:5612141247152835360
{noformat}

    [junit] Testsuite: org.apache.lucene.util.TestBytesRefHash
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.454 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestBytesRefHash -Dtestmethod=testCompact -Dtests.seed=-7961072421643387492:5612141247152835360
    [junit] NOTE: test params are: codec=PreFlex, locale=et, timezone=Pacific/Tahiti
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestBytesRefHash]
    [junit] NOTE: Linux 2.6.35-28-generic amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=12,threads=1,free=363421800,total=379322368
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testCompact(org.apache.lucene.util.TestBytesRefHash):	Caused an ERROR
    [junit] bitIndex < 0: -27
    [junit] java.lang.IndexOutOfBoundsException: bitIndex < 0: -27
    [junit] 	at java.util.BitSet.set(BitSet.java:262)
    [junit] 	at org.apache.lucene.util.TestBytesRefHash.testCompact(TestBytesRefHash.java:146)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1260)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1189)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.util.TestBytesRefHash FAILED
{noformat}

the test expects that _TestUtil.randomRealisticUnicodeString(random, 1000); will never return the same string.

I will upload a patch soon."
1,"entity returns the same stream for getContent(). BasicHttpEntity and GzipDecompressingEntity will return the same stream
when getContent() is called multiple times. That is not allowed by the
HttpEntity interface. They should rather throw an IllegalStateException.

Some tests and EntityUtils rely on getContent to return the same stream
for multiple calls.

patch follows,
  Roland"
1,"Importer drops jcr:xmlcharacters fields after a large jcr:xmlcharacters entry. If you have an XML node as follows:

<a>
[lots and lots of data]
</a>

This should translate into

Node: a
 +- Node: jcr:xmltext
   +- Property: jcr:xmlcharacters = [lots and lots of data]

Instead, the following things happen:
- There is no node jcr:xmltext
- If the node a has child nodes, they also lose the jcr:xmltext node
- Any nodes on the same level after node a also lose the jcr:xmltext node
- Nodes that come after a, but are on a higher level, have correct jcr:xmltext 
nodes

(I've used some 100+k of data, namely a base64 encoded picture)"
1,"Ordering of methods in PostMethod changes behaviour. I have just spent the best part of two days trying to work out why
a servlet running in Tomcat was not getting UTF-8 when I had set my
client to send UTF-8. It turns out that if I set my PostMethod request
header after setting the request body the content does not get sent as
UTF-8.

The following gets sent as UTF-8:

      PostMethod post = new PostMethod(destinationUrl.toString());
      post.setStrictMode(false);
      post.setRequestHeader(""Content-Type"",""text/xml; charset=UTF-8"");
      post.setRequestHeader(""user-agent"", ""myAgent"");
      post.setRequestBody(content);
      post.setFollowRedirects(true);

the following doesn't:

      PostMethod post = new PostMethod(destinationUrl.toString());
      post.setStrictMode(false);
      post.setRequestBody(content);
      post.setRequestHeader(""Content-Type"",""text/xml; charset=UTF-8"");
      post.setRequestHeader(""user-agent"", ""myAgent"");
      post.setFollowRedirects(true);

In a live execution I would understand that order makes a big difference, but
when you fill out an object that feels like defining the values of a Java Bean
this likely to be less obvious."
1,"NullPointerException in constructor of JcrDavException. within DavSessionProviderImpl.attachSession() a org.apache.jackrabbit.rmi.client.RemoteRepositoryException is thrown, catched as RepositoryException and passed into the 
ctor of JcrDavException

the lookup for the class in the static HashMap codeMap for the class fails and a NPE is thrown 

suggested fix:
The static lookup by the javax.jcr.*Exception classed must be fixed to include derived exception classes.

"
1,"NPE when copying nodes with Workspace.copy(). I get a NullpointerException when using Workspace.copy():

java.lang.NullPointerException
at org.apache.jackrabbit.core.BatchedItemOperations.copyNodeState(BatchedItemOperations.java:1834)
at org.apache.jackrabbit.core.BatchedItemOperations.copyNodeState(BatchedItemOperations.java:1806)
at org.apache.jackrabbit.core.BatchedItemOperations.copyNodeState(BatchedItemOperations.java:1806)
at org.apache.jackrabbit.core.BatchedItemOperations.copy(BatchedItemOperations.java:423)
at org.apache.jackrabbit.core.WorkspaceImpl.internalCopy(WorkspaceImpl.java:444)
at org.apache.jackrabbit.core.WorkspaceImpl.copy(WorkspaceImpl.java:666)
at xxx.MyClass.myMeth(MyClass.java)

It seems that it happens not all the time. The error occurs since we use Jackrabbit 1.6.0. We do not get the error with previous versions. It seems that we only get the error when trying to copy nodes that were created with Jackrabbit 1.4 and copied with Jackrabbit 1.6."
1,"recoverable exceptions when reading are not retried. If a recoverable exception occurs after a request is written then the method is
not retried."
1,"Repository.login throws IllegalStateException. Calling any login method on Repository instance, which has been shut down throws an IllegalStateException, which is caused by the RepositoryImpl.sanityCheck method.

This exception is unexpected by callers of the login method, which is specified to throw one of LoginException, NoSuchWorkspaceException and RepositoryException. In particular the spec says, that a RepositoryException is thrown ""if another error occurs"".

So I suggest to modify the RepositoryImpl.login(Credentials, String) as follows (patch against trunk):

Index: /usr/src/jackrabbit/trunk/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/RepositoryImpl.java
===================================================================
--- /usr/src/jackrabbit/trunk/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/RepositoryImpl.java	(revision 706543)
+++ /usr/src/jackrabbit/trunk/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/RepositoryImpl.java	(working copy)
@@ -1358,6 +1358,8 @@
         } catch (AccessDeniedException ade) {
             // authenticated subject is not authorized for the specified workspace
             throw new LoginException(""Workspace access denied"", ade);
+        } catch (RuntimeException re) {
+            throw new RepositoryException(re.getMessage(), re);
         } finally {
             shutdownLock.readLock().release();
         }
"
1,"IndexCommit.equals() bug. IndexCommit.equals() checks for equality of Directories and versions, but it doesn't check IMHO the more important generation numbers. It looks like commits are really identified by a combination of directory and segments_XXX, which means the generation number, because that's what the DirectoryReader.open() checks for.

This bug leads to an unexpected behavior when the only change to be committed is in userData - we get two commits then that are declared equal, they have the same version but they have different generation numbers. I have no idea how this situation is treated in a few dozen references to IndexCommit.equals() across Lucene...

On the surface the fix is trivial - either add the gen number to equals(), or use gen number instead of version. However, it's puzzling why these two would ever get out of sync??? and if they are always supposed to be in sync then maybe we don't need both of them at all, maybe just generation or version is sufficient?"
1,"Search with sort fails when a document has a missing value. Testing on version: lucene-1.4-rc2

Call in question: IndexSearcher.search(Query query, Filter filter, int nDocs, 
Sort sort) 

Description: I'm making a call to search with a sort field - in my case I'm 
sorting by date. If any document in the results set (Hits) has a missing value 
in the sort field, the entire call throws an [uncaught] exception during the 
sorting process with no results returned. 

This is an undesireable result, and the prospects for patching this problem 
outside the search classes are ugly, e.g. trying to patch the index itself.

This is actually a critical function in my application. Thank you for 
addressing it.

-Dan"
1,"Single quote in contains function is not parsed correctly. If there is a single quote in the contains statement the parser will throw an exception.

Example:
//element(*, nt:resource)[jcr:contains(., 'it''s fun')]

The LuceneQueryBuilder replaces the single quote with a double quote and hence the lucene fulltext query parser fails because there is a missing closing double quote. Not sure why this is done in the code, maybe this is a left over from an early JSR 170 draft."
1,"spi2jcr: RepositoryServiceImpl.getRootId returns bad NodeId . I think org.apache.jackrabbit.spi2jcr.RepositoryServiceImpl.getRootId() should not return getIdFactory().createNodeId((String) null, Path.ROOT). Rather should it do a round trip to the wrapped repository and return a UUID based NodeId if the root node of the wrapped repository is mix:referenceable.

The javadoc reads: ""If the root node can be identified with a unique ID the returned NodeId simply has a uniqueID part and the path part is null. If the root node cannot be identified with a unique ID the uniqueID part is null and the path part will be set to ""/""."""
1,"JNDI data sources with various PersistenceManager: wrong default values. With JCR-1305 Jackrabbit supports creating a connection throug a JNDI Datasource and without configuring user and password. This works for some but not all provided PersistenceManagers. Some of them - like the Oracle-specific BundleDBPersistenceManager - sets default values for user and password if none are provided in the jackrabbit config. This way its impossible to use such PersistenceManagers with the plain JNDI DS.

This concerns the following BundleDbPersistenceManagers: OraclePersistenceManager, DerbyPersistenceManager, H2PersistenceManager.

There also might be other PMs (perhaps some special SimpleDbPersistenceManagers) with similar behaviour."
1,"Typo in the DeltaVConstants class in constant XML_CHECKOUT_CHECKIN value. Just spotted a typo in the http://svn.apache.org/repos/asf/jackrabbit/trunk/jackrabbit-webdav/src/main/java/org/apache/jackrabbit/webdav/version/DeltaVConstants.java
(same is in released 1.4 version)

There's line
    public static final String XML_CHECKOUT_CHECKIN = ""checkin-checkout"";
Probably should be
    public static final String XML_CHECKOUT_CHECKIN = ""checkout-checkin"";
"
1,"JCR-Server: Workspace.restore not mapped correctly. (issue reported by David Kennedy)

Workspace.restore(Version[], boolean) won't work, since versions are not retrieved correctly. The version history that can be access from the request  resource, cannot be used to retrieve the versions needed for a workspace.restore call.

possible short term fix:
From the version-hrefs present in the request body of the UPDATE request, version resources must be built and
the corresponding version item retrieved.

alternative:
find a proper mapping for Workspace.restore(Version[], boolean). having UPDATE on a resource representing a javax.jcr.Node being mapped to a workspace.restore is odd.

"
1,"empty path not handled correctly. When requesting for a URL which has an empty path, e.g. http://abcnews.go.com ,
the code sends the following line:

GET  HTTP/1.1

which should be

GET / HTTP/1.1

instead"
1,"TestIndexWriterDelete fails randomly. 10 out of 9 runs with that see fail on my trunk:

ant test-core -Dtestcase=TestIndexWriterDelete -Dtestmethod=testErrorAfterApplyDeletes -Dtests.seed=4269712067829708991:1888184886355172227 -Dtests.codec=randomPerField


with this result:

{code}

junit-sequential:
    [junit] Testsuite: org.apache.lucene.index.TestIndexWriterDelete
    [junit] Tests run: 1, Failures: 2, Errors: 0, Time elapsed: 1.725 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterDelete -Dtestmethod=testErrorAfterApplyDeletes -Dtests.seed=4269712067829708991:1888184886355172227 -Dtests.codec=randomPerField
    [junit] NOTE: test params are: codec=RandomCodecProvider: {id=Standard, contents=SimpleText, city=MockSep}, locale=ar_QA, timezone=VST
    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestIndexWriterDelete]
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testErrorAfterApplyDeletes(org.apache.lucene.index.TestIndexWriterDelete):	FAILED
    [junit] 
    [junit] junit.framework.AssertionFailedError: 
    [junit] 	at org.apache.lucene.index.TestIndexWriterDelete.testErrorAfterApplyDeletes(TestIndexWriterDelete.java:736)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1043)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:981)
    [junit] 
    [junit] 
    [junit] Testcase: testErrorAfterApplyDeletes(org.apache.lucene.index.TestIndexWriterDelete):	FAILED
    [junit] ConcurrentMergeScheduler hit unhandled exceptions
    [junit] junit.framework.AssertionFailedError: ConcurrentMergeScheduler hit unhandled exceptions
    [junit] 	at org.apache.lucene.util.LuceneTestCase.tearDown(LuceneTestCase.java:503)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1043)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:981)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.index.TestIndexWriterDelete FAILED
{code}"
1,"maxFieldLength actual limit is 1 greater than expected value.. 
// Prepare document.
Document document = new Document();
document.add(new Field(""name"",
            ""pattern oriented software architecture"", Store.NO,
            Index.TOKENIZED, TermVector.WITH_POSITIONS_OFFSETS));

// Set max field length to 2.
indexWriter.setMaxFieldLength(2);

// Add document into index.
indexWriter.addDocument(document, new StandardAnalyzer());

// Create a query.
QueryParser queryParser = new QueryParser(""name"", new StandardAnalyzer());
Query query = queryParser.parse(""software"");

// Search the 3rd term.
Hits hits = indexSearcher.search(query);

Assert.assertEquals(0, hits.length());
// failed. Actual hits.length() == 1, but expect 0."
1,RMI reference not automatically bound by the standalone server. The RMI servlet in the 1.5.0 standalone server is only initialized (and the remote reference bound to the RMI registry) when the http://.../rmi URL is first accessed. The RMI binding should be made as soon as the standalone server starts.
1,MoreLikeThis ignores custom similarity. MoreLikeThis only allows the use of the DefaultSimilarity.  Patch shortly
1,"Transient states should be persisted in depth-first traversal order. Inside Node.save(), when filling the list of transient (modified) items, the node itself is added first (if transient) and all transient descendant nodes in depth-first order. This can lead to the following problem with shareable nodes and path-based access management: 

1) assume a node N has a shared child S, which is shared with at least one other node N'
2) S.removeShare is invoked: this removes S from the list of child nodes in N
3) N.save is invoked

N is persisted first, then S. If a path-based access manager tries to build the path of S after N has been persisted, S will no longer be returned in the list of removed child node entries, and an exception will be thrown. This can be circumvented by adding N last."
1,"suggest.fst.Sort.BufferSize should not automatically fail just because of freeMemory(). Follow up op dev thread: [FSTCompletionTest failure ""At least 0.5MB RAM buffer is needed"" | http://markmail.org/message/d7ugfo5xof4h5jeh]"
1,"LogSource.setLevel incorrectly uses entrySet. When I call LogSource.setLevel, I get the following exception:

java.lang.ClassCastException: java.util.HashMap$Entry
	at org.apache.commons.httpclient.log.LogSource.setLevel
(LogSource.java:158)

The calling code is :

    LogSource.setLevel (Log.OFF);

The error (I believe) is that you should get the value set from the map, not 
the entry set (in LogSource):

    static public void setLevel(int level) {
        Iterator it = _logs.entrySet().iterator(); <-- should be _logs.values()
        while(it.hasNext()) {
            Log log = (Log)(it.next());
            log.setLevel(level);
        }
    }"
1,"SQL-2 query returns more than the requested column. If I do :

SELECT alias.[jcr:title] FROM [jnt:mainContent] as alias

and then iterate through the returned columns of the rows, I get the same result as for :

SELECT * FROM [jnt:mainContent] 

which is ALL the properties defined for jnt:mainContent.

Only if I use

SELECT alias.[jcr:title] as title FROM [jnt:mainContent] as alias

the result is limited to the title column."
1,"IndexWriter.addIndexes results in java.lang.OutOfMemoryError. I'm re-opening a bug I logged previously. My previous bug report has 
disappeared. 

Issue: IndexWriter.addIndexes results in java.lang.OutOfMemoryError for large 
merges.

Until this writing, I've been merging successfully only through repetition, 
i.e. I keep repeating merges until a success. As my index size has grown, my 
success rate has steadily declined. I've reached the point where merges now 
fail 100% of the time. I can't merge.

My tests indicate the threshold is ~30GB on P4/800MB VM with 6 indexes. I have 
repeated my tests on many different machines (not machine dependent). I have 
repeated my test using local and attached storage devices (not storage 
dependent).

For what its worth, I believe the exception occurs entirely during the optimize 
process which is called implicitly after the merge. I say this because each 
time it appears the correct amount of bytes are written to the new index. Is it 
possible to decouple the merge and optimize processes?


The code snippet follows. I can send you the class file and 120GB data set. Let 
me know how you want it.

>>>>> code sample >>>>>

Directory[] sources = new Directory[paths.length];
...

Directory dest = FSDirectory.getDirectory( path, true);
IndexWriter writer = new IndexWriter( dest, new TermAnalyzer( 
StopWords.SEARCH_MAP), true);

writer.addIndexes( sources);
writer.close();"
1,"IndexWriter.optimize(boolean doWait) ignores doWait parameter. {{IndexWriter.optimize(boolean doWait)}} ignores the doWait parameter and always calls {{optimize(1, true)}}.

That does not seem to be the intended behavior, based on the doc comment."
1,"When trying to reuse version label in transaction, exception is thrown. Following sequence causes failure:
1. first transaction:
  1.1 create versionable node
  1.2 create version 1 of this node
2. second transaction:
  2.1 create version 2 of this node
  2.2 assign a label to version 2
3. third transaction:
  3.1 restore a node to version 1
  3.2 remove version 2
  3.3 make version 3 of  same node
  3.4 assign same label (which was assigned to version 2) to version 3 -> fails

Same sequence which does not use transactions at all works fine.
Going to attach a test case."
1,"webdav's PropertyDefinitionImpl's toXML doesn't seem to attach query operators element to the returned dom. PropertyDefinitionImpl.toXML does

        // JCR 2.0 extension
        Element qopElem = document.createElement(AVAILABLE_QUERY_OPERATORS_ELEMENT);
        String[] qops = getAvailableQueryOperators();
        for (int i = 0; i < qops.length; i++) {
            Element opElem = document.createElement(AVAILABLE_QUERY_OPERATOR_ELEMENT);
            DomUtil.setText(opElem, qops[i]);
            qopElem.appendChild(opElem);
        }

        return elem;

which doesn't attach the qopElem to the returned dom."
1,"FuzzyQuery produces a ""java.lang.NegativeArraySizeException"" in PriorityQueue.initialize if I use Integer.MAX_VALUE as BooleanQuery.MaxClauseCount. PriorityQueue creates an ""java.lang.NegativeArraySizeException"" when initialized with Integer.MAX_VALUE, because Integer overflows. I think this could be a general problem with PriorityQueue. The Error occured when I set BooleanQuery.MaxClauseCount to Integer.MAX_VALUE and user a FuzzyQuery for searching."
1,"QueryParser does not correctly handle escaped characters within quoted strings. The Lucene query parser incorrectly handles escaped characters inside quoted strings; specifically, a quoted string that ends with an (escaped) backslash followed by any additional quoted string will not be properly tokenized. Consider the following example:

bq. {{(name:""///mike\\\\\\"") or (name:""alphonse"")}}

This is not a contrived example -- it derives from an actual bug we've encountered in our system. Running this query will throw an exception, but removing the second clause resolves the problem. After some digging I've found that the problem is with the way quoted strings are processed by the lexer: you'll notice that Mike's name is followed by three escaped backslashes right before the ending quote; looking at the JavaCC code for the query parser highlights the problem:

{code:title=QueryParser.jj|borderStyle=solid}
<DEFAULT> TOKEN : {
  <AND:       (""AND"" | ""&&"") >
| <OR:        (""OR"" | ""||"") >
| <NOT:       (""NOT"" | ""!"") >
| <PLUS:      ""+"" >
| <MINUS:     ""-"" >
| <LPAREN:    ""("" >
| <RPAREN:    "")"" >
| <COLON:     "":"" >
| <STAR:      ""*"" >
| <CARAT:     ""^"" > : Boost
| <QUOTED:     ""\"""" (~[""\""""] | ""\\\"""")* ""\"""">
...
{code}

Take a look at the way the QUOTED token is constructed -- there is no lexical processing of the escaped characters within the quoted string itself. In the above query the lexer matches everything from the first quote through all the backslashes, _treating the end quote as an escaped character_, thus also matching the starting quote of the second term. This causes a lexer error, because the last quote is then considered the start of a new match.

I've come to understand that the Lucene query handler is supposed to be able to handle unsanitized human input; indeed the lexer above would handle a query like {{""blah\""}} without complaining, but that's a ""best-guess"" approach that results in bugs with legal, automatically generated queries. I've attached a patch that fixes the erroneous behavior but does not maintain leniency with malformed queries; I believe this is the correct approach because the two design goals are fundamentally at odds. I'd appreciate any comments."
1,"SnowballFilter loses token position offset. SnowballFilter doesn't set the token position increment (and thus it defaults to 1).
This also affetcs SnowballAnalyzer since it uses SnowballFilter."
1,"If the NRT reader hasn't changed then IndexReader.openIfChanged should return null. I hit a failure in TestSearcherManager (NOTE: doesn't always fail):

{noformat}
  ant test -Dtestcase=TestSearcherManager -Dtestmethod=testSearcherManager -Dtests.seed=459ac99a4256789c:-29b8a7f52497c3b4:145ae632ae9e1ecf
{noformat}

It was tripping the assert inside SearcherLifetimeManager.record,
because two different IndexSearcher instances had different IR
instances sharing the same version.  This was happening because
IW.getReader always returns a new reader even when there are no
changes.  I think we should fix that...

Separately I found a deadlock in
TestSearcherManager.testIntermediateClose, if the test gets
SerialMergeScheduler and needs to merge during the second commit.
"
1,"PropertyValue constraint fails with implicit selectorName using JCR-SQL2. Compiling a JCR-SQL2 query involving a PropertyValue constraint using a qualified property name fails if selectorName is not explicitly defined.

The following query works:

SELECT * FROM [my:thing] AS thing WHERE thing.[my:property] = 'abc'

the following doesn't:

SELECT * FROM [my:thing] AS thing WHERE [my:property] = 'abc'

(the ""AS thing"" is unecessary here, I can leave it out with the same result).

The second query results in an:
javax.jcr.query.InvalidQueryException: Query:
SELECT * FROM [my:thing] AS thing WHERE [(*)my:property] = 'abc';
expected: NOT, (

The spec final draft however states:

PropertyValue ::= [selectorName'.'] propertyName
   /* If only one selector exists in this query,
      explicit specification of the selectorName is
      optional */"
1,"CheckIndex should allow term position = -1. 
Spinoff from this discussion:

    http://mail-archives.apache.org/mod_mbox/lucene-java-user/200803.mbox/%3CPine.LNX.4.62.0803292323350.16762@radix.cryptio.net%3E

Right now CheckIndex claims the index is corrupt if you index a Token with -1 position, which happens if your first token has positionIncrementGap set to 0.

But, as far as I can tell, Lucene doesn't ""mind"" when this happens.

So I plan to fix CheckIndex to allow this case.  I'll backport to 2.3.2 as well.

LUCENE-1253 is one example where Lucene's core analyzers could do this."
1,FastVectorHighlighter adds a multi value separator (space) to the end of the highlighted text. The FVH adds an additional ' ' (the multi value separator) to the end of the highlighted text.
1,"remove IndexInput.copyBuf. this looks really broken/dangerous as an instance variable.

what happens on clone() ?! copyBytes can instead make its own array inside the method.

its protected, so ill list in the 3.x backwards breaks section since its technically a backwards break."
1,"ConcurrentModificationException in WebDAV UPDATE. After fixing JCR-2750, I started seeing the following exception in the jcr2dav integration tests:

java.util.ConcurrentModificationException: null
	at java.util.LinkedHashMap$LinkedHashIterator.nextEntry(LinkedHashMap.java:365) ~[na:1.5.0_22]
	at java.util.LinkedHashMap$ValueIterator.next(LinkedHashMap.java:380) ~[na:1.5.0_22]
	at java.util.AbstractCollection.toArray(AbstractCollection.java:176) ~[na:1.5.0_22]
	at org.apache.jackrabbit.webdav.MultiStatus.getResponses(MultiStatus.java:122) ~[jackrabbit-webdav-2.2-SNAPSHOT.jar:2.2-SNAPSHOT]
	at org.apache.jackrabbit.webdav.MultiStatus.toXml(MultiStatus.java:151) ~[jackrabbit-webdav-2.2-SNAPSHOT.jar:2.2-SNAPSHOT]
	at org.apache.jackrabbit.webdav.WebdavResponseImpl.sendXmlResponse(WebdavResponseImpl.java:145) ~[jackrabbit-webdav-2.2-SNAPSHOT.jar:2.2-SNAPSHOT]
	at org.apache.jackrabbit.webdav.WebdavResponseImpl.sendMultiStatus(WebdavResponseImpl.java:113) ~[jackrabbit-webdav-2.2-SNAPSHOT.jar:2.2-SNAPSHOT]
	at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.doUpdate(AbstractWebdavServlet.java:1117) ~[jackrabbit-webdav-2.2-SNAPSHOT.jar:2.2-SNAPSHOT]
	at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.execute(AbstractWebdavServlet.java:327) ~[jackrabbit-webdav-2.2-SNAPSHOT.jar:2.2-SNAPSHOT]
	at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.service(AbstractWebdavServlet.java:201) ~[jackrabbit-webdav-2.2-SNAPSHOT.jar:2.2-SNAPSHOT]
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820) ~[servlet-api-2.5-20081211.jar:na]
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511) ~[jetty-6.1.22.jar:6.1.22]
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:390) ~[jetty-6.1.22.jar:6.1.22]
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765) [jetty-6.1.22.jar:6.1.22]
	at org.mortbay.jetty.handler.HandlerCollection.handle(HandlerCollection.java:114) [jetty-6.1.22.jar:6.1.22]
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152) [jetty-6.1.22.jar:6.1.22]
	at org.mortbay.jetty.Server.handle(Server.java:326) [jetty-6.1.22.jar:6.1.22]
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542) [jetty-6.1.22.jar:6.1.22]
	at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:938) [jetty-6.1.22.jar:6.1.22]
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:755) [jetty-6.1.22.jar:6.1.22]
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218) [jetty-6.1.22.jar:6.1.22]
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404) [jetty-6.1.22.jar:6.1.22]
	at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228) [jetty-6.1.22.jar:6.1.22]
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582) [jetty-util-6.1.22.jar:6.1.22]

Instead of something caused by JCR-2750, it looks like a deeper problem that the JCR_2750 fix just uncovered. As far as I can tell, the ConcurrentModificationException is coming from the AbstractResource.EListener class that may end up concurrently modifying the MultiStatus response while it's being serialized."
1,"FNFE hit when creating an empty index and infoStream is on. Shai just reported this on the dev list.  Simple test:
{code}
Directory dir = new RAMDirectory();
IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), MaxFieldLength.UNLIMITED);
writer.setInfoStream(System.out);
writer.addDocument(new Document());
writer.commit();
writer.close();
{code}

hits this:

{code}
Exception in thread ""main"" java.io.FileNotFoundException: _0.prx
    at org.apache.lucene.store.RAMDirectory.fileLength(RAMDirectory.java:149)
    at org.apache.lucene.index.DocumentsWriter.segmentSize(DocumentsWriter.java:1150)
    at org.apache.lucene.index.DocumentsWriter.flush(DocumentsWriter.java:587)
    at org.apache.lucene.index.IndexWriter.doFlushInternal(IndexWriter.java:3572)
    at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:3483)
    at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3474)
    at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1940)
    at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1894)
{code}

Turns out it's just silly -- this is actually an issue I've already fixed on the flex (LUCENE-1458) branch.  DocumentsWriter has its own method to enumerate the flushed files and compute their size, but really it shouldn't do that -- it should use SegmentInfo's method, instead."
1,"Superfluous AndQueryNode  in query tree built by SQL parser. Test query (tested with <http://people.apache.org/~mreutegg/jcr-query-translator/translator.html>):

  SELECT * FROM nt:folder WHERE x = 1 

generates the following query tree:

+ Root node
+ Select properties: *
  + PathQueryNode
    + LocationStepQueryNode:  NodeTest=* Descendants=true Index=NONE
      + AndQueryNode
        + RelationQueryNode: Op: =  Prop=@{}x Type=LONG Value=1
      + NodeTypeQueryNode:  Prop={http://www.jcp.org/jcr/1.0}primaryType Value={http://www.jcp.org/jcr/nt/1.0}folder

It seems the AndQueryNode is superfluous.
"
1,"Manually set 'Cookie' & 'Authorization' headers get discarded. HttpClient discards all the 'Cookie' & 'Authorization' headers including those
manually set when populating request header collection with automatically
generated headers."
1,"[PATCH] IndexSearcher.search(query,filter,nDocs) accepts zero nDocs. This caused an npe from the ht.top().score lateron. 
The root cause was a bug in a test case, which took 
more time to track down than would have been necessary 
with the attached patch. 
The patch throws an IllegalArgumentException for non positive nDocs. 
All current tests pass with the patch applied. 
 
Regards, 
Paul"
1,"Moved node disappears. Moving a node and then refreshing it can make it disappear.

deleteDirectory(new File(""repository""));
Repository rep = new TransientRepository();
Session session = rep.login(new SimpleCredentials("""", new char[0]));
Node root = session.getRootNode();
Node a = root.addNode(""a"");
Node b = a.addNode(""b"");
session.save();
session.move(""/a/b"", ""/b"");
b.refresh(false);
// session.save(); // no effect
for (NodeIterator it = root.getNodes(); it.hasNext();) {
    Node n = it.nextNode();
    System.out.println(n.getName());
    for (NodeIterator it2 = n.getNodes(); it2.hasNext();) {
        System.out.println(""  "" + it2.nextNode().getName());
    }
}

In the trunk, the node 'b' is not listed after the refresh (not under the root page, and not under a). The output is:
jcr:system
  jcr:versionStorage
  jcr:nodeTypes
a


Jackrabbit 1.4.x throws an exception:

jcr:system
  jcr:versionStorage
  jcr:nodeTypes
a
Exception in thread ""main"" javax.jcr.RepositoryException: failed to resolve name of acee31c4-c33b-4ed4-b1b5-39db6f17fb09
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getName(HierarchyManagerImpl.java:451)
	at org.apache.jackrabbit.core.CachingHierarchyManager.getName(CachingHierarchyManager.java:287)
	at org.apache.jackrabbit.core.NodeImpl.getName(NodeImpl.java:1931)
	at org.apache.jackrabbit.core.fuzz.TestMoveRemoveRefresh.test(TestMoveRemoveRefresh.java:33)
	at org.apache.jackrabbit.core.fuzz.TestMoveRemoveRefresh.main(TestMoveRemoveRefresh.java:15)


void deleteDirectory(File file) {
    if (file.isDirectory()) {
        File[] list = file.listFiles();
        for(int i=0; i<list.length; i++) {
            deleteDirectory(list[i]);
        }
    }
    file.delete();
}
"
1,"Handle URIs with path component null. HttpClient does not handle URIs with path component null (e.g. http://google.com) the same as path component '/'. This results e.g. in a ProtocolException ""The server failed to respond with a valid HTTP response""."
1,"IllegalStateException thrown when consuming events. i assume, when session is closed, or beeing closed, the observation still tries to deliver some events:

[java] 2005-11-24 22:58:41,764 [ObservationManager] WARN  org.apache.jackrabbit.core.observation.ObservationManagerFactory - EventConsumer threw exception: java.lang.IllegalStateException: not in
itialized
[java] 2005-11-24 22:58:41,764 [ObservationManager] DEBUG org.apache.jackrabbit.core.observation.ObservationManagerFactory - Stacktrace:
[java] java.lang.IllegalStateException: not initialized
[java]     at org.apache.jackrabbit.core.security.SimpleAccessManager.isGranted(SimpleAccessManager.java:119)
[java]     at org.apache.jackrabbit.core.observation.EventConsumer.consumeEvents(EventConsumer.java:231)
[java]     at org.apache.jackrabbit.core.observation.ObservationManagerFactory.run(ObservationManagerFactory.java:161)
[java]     at java.lang.Thread.run(Thread.java:595)

"
1,"FSDirectory doesn't detect double-close nor usage after close. FSDirectory.close implements logic to ensure only a single instance of FSDirectory per canonical directory exists.  This means code that synchronizes on the FSDirectory instance is also synchronized against that canonical directory.  I think only IndexModifier (now deprecated) actually makes use of this, but I'm not certain. 

But, the close() method doesn't detect double close, and doesn't catch usage after being closed, and so one can easily get two instances of FSDirectory for the same canonical directory."
1,"Row.getValue(""rep:excerpt(.)"") may throw ArrayIndexOutOfBoundsException. Happens when the matching node only has properties that are configured not to be used in excerpts."
1,"InputStream.read return value is ignored.. RepositoryImpl.loadRootNodeId reads from an InputStreamReader using read(...) and ignores the return values.

This results in a problem if the input stream doesn't read all bytes."
1,"IllegalNameException when importing document view with two mixins. As reported by Manuel Simoni on the dev mailing list:

----

I have nodes with two mixin types, s1NT:comment and s1NT:authored.

I am exporting the document view:

session.exportDocumentView(session.getRootNode().getPath(),
outputStream, false, false);

When I try to import the document again:

session.getWorkspace().importXML(session.getRootNode().getPath(),
inputStream, ImportUUIDBehavior.IMPORT_UUID_COLLISION_THROW);

...I get this exception:

java.lang.Exception: javax.jcr.InvalidSerializedDataException: failed to
parse XML stream: illegal jcr:mixinTypes value: s1NT:comment
s1NT:authored: illegal jcr:mixinTypes value: s1NT:comment s1NT:authored
       at
at.systemone.wiki.webservice.ImportControllerImpl.importDocumentView(ImportControllerImpl.java:30)
       at
at.systemone.wiki.webservice.ImportControllerTest.testImport(ImportControllerTest.java:12)
       at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
       at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
       at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
       at java.lang.reflect.Method.invoke(Method.java:585)
       at junit.framework.TestCase.runTest(TestCase.java:154)
       at junit.framework.TestCase.runBare(TestCase.java:127)
       at junit.framework.TestResult$1.protect(TestResult.java:106)
       at junit.framework.TestResult.runProtected(TestResult.java:124)
       at junit.framework.TestResult.run(TestResult.java:109)
       at junit.framework.TestCase.run(TestCase.java:118)
       at junit.framework.TestSuite.runTest(TestSuite.java:208)
       at junit.framework.TestSuite.run(TestSuite.java:203)
       at
org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:478)
       at
org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:344)
       at
org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)
Caused by: javax.jcr.InvalidSerializedDataException: failed to parse XML
stream: illegal jcr:mixinTypes value: s1NT:comment s1NT:authored:
illegal jcr:mixinTypes value: s1NT:comment s1NT:authored
       at
org.apache.jackrabbit.core.WorkspaceImpl.importXML(WorkspaceImpl.java:718)
       at
at.systemone.wiki.webservice.ImportControllerImpl.importDocumentView(ImportControllerImpl.java:27)
       ... 16 more
Caused by: org.apache.jackrabbit.name.IllegalNameException:
's1NT:comment s1NT:authored' is not a valid name
       at
org.apache.jackrabbit.core.xml.DocViewImportHandler.startElement(DocViewImportHandler.java:217)
       at
org.apache.jackrabbit.core.xml.ImportHandler.startElement(ImportHandler.java:234)
       at org.apache.xerces.parsers.AbstractSAXParser.startElement(Unknown Source)
       at
org.apache.xerces.impl.XMLNSDocumentScannerImpl.scanStartElement(Unknown
Source)
       at
org.apache.xerces.impl.XMLDocumentFragmentScannerImpl$FragmentContentDispatcher.dispatch(Unknown
Source)
       at
org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanDocument(Unknown
Source)
       at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
       at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
       at org.apache.xerces.parsers.XMLParser.parse(Unknown Source)
       at org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source)
       at
org.apache.jackrabbit.core.WorkspaceImpl.importXML(WorkspaceImpl.java:709)
       ... 17 more

I think the importer chokes on this node (it is the first node with
these two mixin types in the XML file):

<s1:comment jcr:primaryType=""s1NT:page"" jcr:mixinTypes=""s1NT:comment
s1NT:authored"" jcr:uuid=""3ff1022b-3e21-4e44-9a2e-ae3b67e833e5""
jcr:isCheckedOut=""true""
jcr:versionHistory=""17186f20-dab2-42d8-8f66-0895472debea""
jcr:frozenMixinTypes=""s1NT:comment s1NT:authored""
jcr:frozenUuid=""3ff1022b-3e21-4e44-9a2e-ae3b67e833e5""
s1:author=""8db75ec7-eee8-44d8-aeb2-fbd116ea7e01"" s1:title=""""
jcr:predecessors=""fb9eefb2-e2f8-414c-be94-185111a89be9""
s1:creationDate=""2005-09-07T17:59:12.589Z""
s1:editor=""7b778c51-d8d8-474b-a621-f5759fc24cd0"" s1:orphanedPage=""false""
s1:modificationDate=""2006-03-18T17:55:49.838Z"" s1:lowercaseTitle=""""
jcr:baseVersion=""fb9eefb2-e2f8-414c-be94-185111a89be9""
s1:currentEditor=""8db75ec7-eee8-44d8-aeb2-fbd116ea7e01""
jcr:frozenPrimaryType=""s1NT:page"">

----

This issue is related to JCR-325, but there should be a lot easier fix for this special case."
1,ArrayIndexOutOfBoundsException during indexing. http://search.lucidimagination.com/search/document/f29fc52348ab9b63/arrayindexoutofboundsexception_during_indexing
1,"some valid email address characters not correctly recognized. the EMAIL expression in StandardTokenizerImpl.jflex misses some unusual but valid characters in the left-hand-side of the email address. This causes an address to be broken into several tokens, for example:

somename+site@gmail.com gets broken into ""somename"" and ""site@gmail.com""
husband&wife@talktalk.net gets broken into ""husband"" and ""wife@talktalk.net""

These seem to be occurring more often. The first seems to be because of an anti-spam trick you can use with google (see: http://labnol.blogspot.com/2007/08/gmail-plus-smart-trick-to-find-block.html). I see the second in several domains but a disproportionate amount are from talktalk.net, so I expect it's a signup suggestion from the service.

Perhaps a fix would be to change line 102 of StandardTokenizerImpl.jflex from:
EMAIL      =  {ALPHANUM} (("".""|""-""|""_"") {ALPHANUM})* ""@"" {ALPHANUM} (("".""|""-"") {ALPHANUM})+

to 

EMAIL      =  {ALPHANUM} (("".""|""-""|""_""|""+""|""&"") {ALPHANUM})* ""@"" {ALPHANUM} (("".""|""-"") {ALPHANUM})+

I'm aware that the StandardTokenizer is meant to be more of a basic implementation rather than an implementation the full standard, but it is quite useful in places and hopefully this would improve it slightly."
1,"when you clone or reopen an IndexReader with pending changes, the new reader doesn't commit the changes. While working on LUCENE-1647, I came across this issue... we are failing to carry over hasChanges, norms/deletionsDirty, etc, when cloning the new reader."
1,"Problems mapping custom collections. When using a custom list that extends from java.util.AbstractList, ManageableCollectionUtil.getManageableCollection raises a JcrMappingException because it does not consider the custom list to be a java.util.List. This is because it uses ""if (object.getClass().equals(List.class))"" instead of ""if (object instanceof List)"". The same thing will probably happen when using a custom Collection, a custom ArrayList, etc. This is the stack trace:

org.apache.jackrabbit.ocm.exception.JcrMappingException: Unsupported collection 
type : *********** (MyCustomList class) 
        at org.apache.jackrabbit.ocm.manager.collectionconverter.ManageableColle 
ctionUtil.getManageableCollection(ManageableCollectionUtil.java:153) 
        at org.apache.jackrabbit.ocm.manager.objectconverter.impl.ObjectConverte 
rImpl.insertCollectionFields(ObjectConverterImpl.java:780) 
        at org.apache.jackrabbit.ocm.manager.objectconverter.impl.ObjectConverte 
rImpl.insert(ObjectConverterImpl.java:221) 
        at org.apache.jackrabbit.ocm.manager.objectconverter.impl.ObjectConverte 
rImpl.insert(ObjectConverterImpl.java:146) 
        at org.apache.jackrabbit.ocm.manager.impl.ObjectContentManagerImpl.inser 
t(ObjectContentManagerImpl.java:407) 

I have come up to this bug using a MyCustomList<MyClass>, with MyCustomList extending java.util.AbstractList<MyClass>."
1,"SimpleFSLockFactory ignores error on deleting the lock file. Spinoff from here:

    http://www.gossamer-threads.com/lists/lucene/java-user/54438

The Lock.release for SimpleFSLockFactory ignores the return value of lockFile.delete().  I plan to throw a new LockReleaseFailedException, subclassing from IOException, when this returns false.  This is a very minor change to backwards compatibility because all methods in Lucene that release a lock already throw IOException."
1,"Nodes that have properties marked for async extraction should be available for querying. The problems only appears when dealing with nodes that have async extractors. In this case we return a lightweight copy of the node (without the property that will be processed in the background).

The copy algorithm ignores certain field types (that have been probably introduced during the Lucene 3 upgrade, not sure) such as SingletonTokenStream(s).
So the lightweight copy does not include all the existing properties, therefore the node will not appear in queries during the extraction time."
1,"Query for name literal without namespace fails. Query for a name literal without a namespace fails. 

Example:
//*[@foo = 'bla']

should return nodes with foo property that contain the String value 'bla' or the Name value 'bla' (no namespace). Only nodes with String value 'bla' are returned."
1,"method.getURI()  returns escaped URIs but it shouldn't. Hi guys,

Please, consider the following imaginary and simplified code:


URI u = new URI(""http://some.host.com/%41.html"", true);
HttpClient httpClient = new HttpClient();
GetMethod method = new GetMethod();
method.setURI(u);
URI u2 = method.getURI();

System.out.println(""1. "" + u);
System.out.println(""2. "" + new String(u.getRawURI()));
System.out.println(""3. "" + u.getURI());
System.out.println(""4. "" + u2);
System.out.println(""5. "" + new String(u2.getRawURI()));
System.out.println(""6. "" + u2.getURI());


The result that you'll get is:

1. http://some.host.com/%41.html
2. http://some.host.com/%41.html
3. http://some.host.com/A.html
4. http://some.host.com/%2541.html
5. http://some.host.com/%2541.html
6. http://some.host.com/%41.html


You can see that for lines 4, 5, and 6, the URI suddenly gets escaped (the 
percent sign gets converted to %25).

Why is that? Am I doing something wrong? Is this the desired behaviour? I would 
have expected to get the SAME URI back, without any escaping.

Besides, I have another question:

After executing a method -- httpClient.executeMethod(method) -- what will 
method.getURI() return? The URI *after* all redirections or the original URI? 
It seems I get the URI *after* the redirections, which is fine, but the 
documentation doesn't say that. It only explicitly says that the getPath() 
method has that behaviour.

Best regards and thanks,
Bisser"
1,can't add lock token to session after 3 login/logout. I login and lock a file and logout. Perfoms a new login and add the previous lock token to the current session because I want to unlock this file. This works fine. But if I do a new logout/login I can't unlock the file (the file is locked). It is best understanded looking at the test case.
1,"Standard analyzer does not correctly tokenize combining character U+0364 COMBINING LATIN SMALL LETTRE E. Standard analyzer does not correctly tokenize combining character U+0364 COMBINING LATIN SMALL LETTRE E.
The word ""mochte"" is incorrectly tokenized into ""mo"" ""chte"", the combining character is lost.
Expected result is only on token ""mochte""."
1,"removing source parent node after session move throws on save. the following code fragment illustrates the problem:

        /**
         * create the following node tree:
         *     
         *       + A
         *         + B
         *            + C
         *         + D
         */
        Node A;
        if (root.hasNode(""A"")) {
            A = root.getNode(""A"");
        } else {
            A = root.addNode(""A"");
        }
        Node B = A.addNode(""B"");
        Node C = B.addNode(""C"");
        Node D = A.addNode(""D"");
        root.save();

        // move C under D
        session.move(""/A/B/C"", ""/A/D/C"");
        // remove B
        A.getNode(""B"").remove();
        /**
         * the expected resulting node tree:
         *     
         *       + A
         *         + D
         *            + C
         */
        A.save();


==> the last save() will throw 
javax.jcr.RepositoryException: inconsistency: failed to retrieve transient state for ...
 "
1,http client cache: SizeLimitedResponseReader is not setting content type for InputStreamEntity in constructResponse(). the newly created InputStreamEntity should be populated with content-encoding and content-type.
1,"Query for string literal broken when literal can be coerced into other type. When a string literal can be coerced into another type (e.g. integer) a property of type string that matches the literal is not found.

E.g. the following query will match prop if its value is the string '1234'.
//*[@prop = '1234']

The query should match properties with string value '1234' and integer value 1234."
1,"Workspace.copy(src, dest) throws unexpected RepositoryException (""Invalid path""). when using the davex remoting layer (jcr2spi->spi2davex), 
the following code fragment causes an unexpected RepositoryException:

<snip>
    Node testNode1 = session.getRootNode().addNode(""test"", ""nt:folder"");

    Node copyDestination = testNode1.addNode(""CopyDestination"", ""nt:folder"");
    testNode1.addNode(""CopySource"", ""nt:folder"").addNode(""testCopyCommand"", ""nt:folder"").addNode(""abc"", ""nt:folder"");
    session.save();
    copyDestination.addMixin(""mix:referenceable"");
    session.save();

    session.getWorkspace().copy(""/test/CopySource/testCopyCommand"", ""/test/CopyDestination/testCopyCommand"");
</snip>

==> Caused by: javax.jcr.RepositoryException: Invalid path:/test/CopyDestination//testCopyCommand
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.apache.jackrabbit.spi2dav.ExceptionConverter.generate(ExceptionConverter.java:69)
	at org.apache.jackrabbit.spi2dav.ExceptionConverter.generate(ExceptionConverter.java:51)
	at org.apache.jackrabbit.spi2dav.RepositoryServiceImpl.execute(RepositoryServiceImpl.java:482)
	at org.apache.jackrabbit.spi2dav.RepositoryServiceImpl.copy(RepositoryServiceImpl.java:1307)
	at org.apache.jackrabbit.spi2davex.RepositoryServiceImpl.copy(RepositoryServiceImpl.java:326)
	at org.apache.jackrabbit.jcr2spi.WorkspaceManager$OperationVisitorImpl.visit(WorkspaceManager.java:889)
	at org.apache.jackrabbit.jcr2spi.operation.Copy.accept(Copy.java:48)
	at org.apache.jackrabbit.jcr2spi.WorkspaceManager$OperationVisitorImpl.execute(WorkspaceManager.java:848)
	at org.apache.jackrabbit.jcr2spi.WorkspaceManager$OperationVisitorImpl.access$400(WorkspaceManager.java:793)
	at org.apache.jackrabbit.jcr2spi.WorkspaceManager.execute(WorkspaceManager.java:581)
	at org.apache.jackrabbit.jcr2spi.WorkspaceImpl.copy(WorkspaceImpl.java:149)
	[...]  


however, the following slightly altered code fragment works as expected:


<snip>
    Node testNode1 = session.getRootNode().addNode(""test"", ""nt:folder"");
/*
    Node copyDestination = testNode1.addNode(""CopyDestination"", ""nt:folder"");
    testNode1.addNode(""CopySource"", ""nt:folder"").addNode(""testCopyCommand"", ""nt:folder"").addNode(""abc"", ""nt:folder"");
    session.save();
    copyDestination.addMixin(""mix:referenceable"");
    session.save();
*/
    testNode1.addNode(""CopyDestination"", ""nt:folder"").addMixin(NodeType.MIX_REFERENCEABLE);
    Node n = testNode1.addNode(""CopySource"", ""nt:folder"").addNode(""testCopyCommand"", ""nt:folder"").addNode(""abc"", ""nt:folder"");
    session.save();

    session.getWorkspace().copy(""/test/CopySource/testCopyCommand"", ""/test/CopyDestination/testCopyCommand"");
</snip>
"
1,"TestFSTs.testRandomWords failure. Was running some while(1) tests on the docvalues branch (r1103705) and the following test failed:

{code}
    [junit] Testsuite: org.apache.lucene.util.automaton.fst.TestFSTs
    [junit] Testcase: testRandomWords(org.apache.lucene.util.automaton.fst.TestFSTs):	FAILED
    [junit] expected:<771> but was:<TwoLongs:771,771>
    [junit] junit.framework.AssertionFailedError: expected:<771> but was:<TwoLongs:771,771>
    [junit] 	at org.apache.lucene.util.automaton.fst.TestFSTs$FSTTester.verifyUnPruned(TestFSTs.java:540)
    [junit] 	at org.apache.lucene.util.automaton.fst.TestFSTs$FSTTester.doTest(TestFSTs.java:496)
    [junit] 	at org.apache.lucene.util.automaton.fst.TestFSTs$FSTTester.doTest(TestFSTs.java:359)
    [junit] 	at org.apache.lucene.util.automaton.fst.TestFSTs.doTest(TestFSTs.java:319)
    [junit] 	at org.apache.lucene.util.automaton.fst.TestFSTs.testRandomWords(TestFSTs.java:940)
    [junit] 	at org.apache.lucene.util.automaton.fst.TestFSTs.testRandomWords(TestFSTs.java:915)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1282)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1211)
    [junit] 
    [junit] 
    [junit] Tests run: 7, Failures: 1, Errors: 0, Time elapsed: 7.628 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: Ignoring nightly-only test method 'testBigSet'
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestFSTs -Dtestmethod=testRandomWords -Dtests.seed=-269475578956012681:0
    [junit] NOTE: test params are: codec=PreFlex, locale=ar, timezone=America/Blanc-Sablon
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestToken, TestCodecs, TestIndexReaderReopen, TestIndexWriterMerging, TestNoDeletionPolicy, TestParallelReaderEmptyIndex, TestParallelTermEnum, TestPerSegmentDeletes, TestSegmentReader, TestSegmentTermDocs, TestStressAdvance, TestTermVectorsReader, TestSurrogates, TestMultiFieldQueryParser, TestAutomatonQuery, TestBooleanScorer, TestFuzzyQuery, TestMultiTermConstantScore, TestNumericRangeQuery64, TestPositiveScoresOnlyCollector, TestPrefixFilter, TestQueryTermVector, TestScorerPerf, TestSloppyPhraseQuery, TestSpansAdvanced, TestWindowsMMap, TestRamUsageEstimator, TestSmallFloat, TestUnicodeUtil, TestFSTs]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=137329960,total=208207872
    [junit] ------------- ---------------- ---------------
    [junit] TEST org.apache.lucene.util.automaton.fst.TestFSTs FAILED
{code}

I am not able to reproduce"
1,"IndexSearcher fails to pass docBase to Collector when using ExecutorService. This bug is causing the failure in TestSearchAfter.

We are now always passing docBase 0 to Collector when you use ExecutorService with IndexSearcher.

This doesn't affect trunk (AtomicReaderContext carries the right docBase); only 3.x.
"
1,"Lazy Field Loading has edge case bug causing read past EOF. While trying to run some benchmarking of Lazy filed loading, i discovered there seems to be an edge case when accessing the last field of the last doc of an index.

the problem seems to only happen when the doc has been accessed after at least one other doc.

i have not tried to dig into the code to find the root cause, testcase to follow..."
1,"Inconsistencies in BitSetKey comparison. Hi,

I encountered a problem with the BitsetENTCacheImpl and the BitsetKey comparisons. I have 3 bitsets A, B and C , defined as :

A : bits 0,4,17,38,60,63 
B : bits 4,17,38,52,59,60
C : bits 0,17,38,60,61,63

If call BitsetKey.compareTo  method on each pair , i get : 

A < B
B < C
C < A

which is not correct and leads to inconsistencies in the TreeSet.

All 2 bitsets are contained in one single word (max bit is 63). So, the method is comparing first the 32 MSB - which are enough in that case to compare the bits. But the problem is, that the difference between the 32 MSB of B and C is too big to fit in an integer : for B, we have 403701824 - for C , 2952790080 . The difference between both is 2549088256 (positive) , which is bigger than Integer.MAX_VALUE , and makes a  -1745879040 (negative) after casting to an int .

In order to fix that, the shift should either be bigger in order to fit a signed integer ( 33 instead of 32 ), or a simple -1 / 0 / +1 could be returned
"
1,"[patch] fix uppercase/lowercase handling for not equal to. code is missing breaks in switch statements, which causes both uppercase and lowercase terms to the not equal to lucene search. patch fixes."
1,"'OR' in XPath query badly interpreted. executing query: //*[@a=1 and @b=2 or @c=3] leads to creating wrong query tree. The builded tree looks like for query: //*[@a=1 and @b=2 and @c=3](see attachement). using brackets resolves the problem, but without brackets output query is different from input query. When AND and OR are switched(so the OR is in first palce - //*[@a=1 or @b=2 and @c=3]) everything is ok."
1,"QueryManager.createQuery() exception handling. Query q = this.superuser.getWorkspace().getQueryManager()
                .createQuery(""SELECT * FROM nt:base"", Query.XPATH);

produces:
org.apache.jackrabbit.core.query.xpath.TokenMgrError: Lexical error at line 1, column 28.  Encountered: ""b"" (98), after : "":""
	at org.apache.jackrabbit.core.query.xpath.XPathTokenManager.getNextToken(XPathTokenManager.java:14546)
	at org.apache.jackrabbit.core.query.xpath.XPath.jj_ntk(XPath.java:9187)
	at org.apache.jackrabbit.core.query.xpath.XPath.PredicateList(XPath.java:5195)
	at org.apache.jackrabbit.core.query.xpath.XPath.AxisStep(XPath.java:4707)
	at org.apache.jackrabbit.core.query.xpath.XPath.StepExpr(XPath.java:4597)
	at org.apache.jackrabbit.core.query.xpath.XPath.RelativePathExpr(XPath.java:4511)
	at org.apache.jackrabbit.core.query.xpath.XPath.PathExpr(XPath.java:4482)
	at org.apache.jackrabbit.core.query.xpath.XPath.ValueExpr(XPath.java:4125)
	at org.apache.jackrabbit.core.query.xpath.XPath.UnaryExpr(XPath.java:4032)
	at org.apache.jackrabbit.core.query.xpath.XPath.CastExpr(XPath.java:3935)
	at org.apache.jackrabbit.core.query.xpath.XPath.CastableExpr(XPath.java:3898)
	at org.apache.jackrabbit.core.query.xpath.XPath.TreatExpr(XPath.java:3861)
	at org.apache.jackrabbit.core.query.xpath.XPath.InstanceofExpr(XPath.java:3824)
	at org.apache.jackrabbit.core.query.xpath.XPath.IntersectExceptExpr(XPath.java:3748)
	at org.apache.jackrabbit.core.query.xpath.XPath.UnionExpr(XPath.java:3672)
	at org.apache.jackrabbit.core.query.xpath.XPath.MultiplicativeExpr(XPath.java:3622)
	at org.apache.jackrabbit.core.query.xpath.XPath.AdditiveExpr(XPath.java:3510)
	at org.apache.jackrabbit.core.query.xpath.XPath.RangeExpr(XPath.java:3451)
	at org.apache.jackrabbit.core.query.xpath.XPath.ComparisonExpr(XPath.java:3353)
	at org.apache.jackrabbit.core.query.xpath.XPath.AndExpr(XPath.java:3290)
	at org.apache.jackrabbit.core.query.xpath.XPath.OrExpr(XPath.java:3227)
	at org.apache.jackrabbit.core.query.xpath.XPath.ExprSingle(XPath.java:2214)
	at org.apache.jackrabbit.core.query.xpath.XPath.ForClause(XPath.java:2337)
	at org.apache.jackrabbit.core.query.xpath.XPath.FLWORExpr(XPath.java:2233)
	at org.apache.jackrabbit.core.query.xpath.XPath.ExprSingle(XPath.java:2133)
	at org.apache.jackrabbit.core.query.xpath.XPath.Expr(XPath.java:2094)
	at org.apache.jackrabbit.core.query.xpath.XPath.QueryBody(XPath.java:2066)
	at org.apache.jackrabbit.core.query.xpath.XPath.MainModule(XPath.java:512)
	at org.apache.jackrabbit.core.query.xpath.XPath.Module(XPath.java:387)
	at org.apache.jackrabbit.core.query.xpath.XPath.QueryList(XPath.java:151)
	at org.apache.jackrabbit.core.query.xpath.XPath.XPath2(XPath.java:118)
	at org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder.<init>(XPathQueryBuilder.java:224)
	at org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder.createQuery(XPathQueryBuilder.java:255)
	at org.apache.jackrabbit.core.query.QueryParser.parse(QueryParser.java:57)
	at org.apache.jackrabbit.core.query.lucene.QueryImpl.<init>(QueryImpl.java:119)
	at org.apache.jackrabbit.core.query.lucene.SearchIndex.createExecutableQuery(SearchIndex.java:158)
	at org.apache.jackrabbit.core.query.QueryImpl.<init>(QueryImpl.java:90)
	at org.apache.jackrabbit.core.SearchManager.createQuery(SearchManager.java:192)
	at org.apache.jackrabbit.core.query.QueryManagerImpl.createQuery(QueryManagerImpl.java:87)
	at org.apache.jackrabbit.test.api.query.IllegalXPathTest.testIllegalStatement(IllegalXPathTest.java:45)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:324)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at org.apache.jackrabbit.test.AbstractJCRTest.run(AbstractJCRTest.java:401)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:474)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:342)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:194)
"
1,"TestBooleanMinShouldMatch test failure. ant test -Dtestcase=TestBooleanMinShouldMatch -Dtestmethod=testRandomQueries -Dtests.seed=505d62a62e9f90d0:-60daa428161b404b:-406411290a98f416

I think its an absolute/relative epsilon issue"
1,QPropertyDefinitionImpl.equals() is implemented incorrectly . 
1,"Access control evaluation does not properly cope with XA transactions. the following test fails with ItemNotFoundException at the indicated position due to the fact that
the parent n2 is EXISTING but still not visible to the system session responsible for the ac
evaluation.

public void testTransaction() throws Exception {

        // make sure testUser has all privileges
        Privilege[] privileges = privilegesFromName(Privilege.JCR_ALL);
        givePrivileges(path, privileges, getRestrictions(superuser, path));

        // create new node and lock it
        Session s = getTestSession();
        UserTransaction utx = new UserTransactionImpl(s);
        utx.begin();

        // add node and save it
        Node n = s.getNode(childNPath);
        if (n.hasNode(nodeName1)) {
            Node c = n.getNode(nodeName1);
            c.remove();
            s.save();
        }

        Node n2 = n.addNode(nodeName1);
        s.save();
            
        Node n3 = n2.addNode(nodeName2);
        s.save(); // exception

        // commit
        utx.commit();
    }

A possible workaround would be to make sure that ItemSaveOperation.persistTransientItems 
retrieves the parent without having the checkPermission enabled since we can assume that
the new item could not be added if the parent was not readable in the first place.... but careful
evaluation would be required.

NOTE: this is just one example of the AC-evaluation not properly dealing with XA transactions.
I am convinced that other examples could be find....
"
1,"Preemptive Authorization parameter initialization incorrect, causes preemptive auth not to work. Preemptive authorization is defeated by an incorrect initialization. Patch 
follows:
--- DefaultHttpParamsFactory.java       2005-10-10 19:09:10.000000000 -0700
+++ DefaultHttpParamsFactory.java.fixed 2005-10-17 17:00:10.259174920 -0700
@@ -118,9 +118,9 @@
         if (preemptiveDefault != null) {
             preemptiveDefault = preemptiveDefault.trim().toLowerCase();
             if (preemptiveDefault.equals(""true"")) {
-                params.setParameter
(HttpClientParams.PREEMPTIVE_AUTHENTICATION, ""on"");
+                params.setParameter
(HttpClientParams.PREEMPTIVE_AUTHENTICATION, Boolean.TRUE);
             } else if (preemptiveDefault.equals(""false"")) {
-                params.setParameter
(HttpClientParams.PREEMPTIVE_AUTHENTICATION, ""off"");
+                params.setParameter
(HttpClientParams.PREEMPTIVE_AUTHENTICATION, Boolean.FALSE);
             }
         }"
1,"An HTTP ""204 NO CONTENT"" response results in dropped connection. After receiving a ""204 NO CONTENT"" response, HttpClient always closes the 
connection.

This did not happen in earlier versions and appears to have been caused by a 
recent fix to bug# 34262."
1,"NPE w/ AbstractPoolEntry.open. java.lang.NullPointerException
    at org.apache.http.impl.conn.AbstractPoolEntry.open(AbstractPoolEntry.java:171)
    at org.apache.http.impl.conn.AbstractPooledConnAdapter.open(AbstractPooledConnAdapter.java:119)
    at org.apache.http.impl.client.DefaultClientRequestDirector.execute(DefaultClientRequestDirector.java:309)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:501)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:456)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:422)
    at com.limegroup.gnutella.http.DefaultHttpExecutor.performRequest(DefaultHttpExecutor.java:97)
    at com.limegroup.gnutella.http.DefaultHttpExecutor.access$000(DefaultHttpExecutor.java:26)
    at com.limegroup.gnutella.http.DefaultHttpExecutor$MultiRequestor.run(DefaultHttpExecutor.java:135)
    at org.limewire.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1006)
    at org.limewire.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:549)
    at java.lang.Thread.run(Unknown Source)

Seeing a lot of these against Alpha4.  Also seeing still the occassional IllegalStateException of:

java.lang.IllegalStateException: Connection already open.
    at org.apache.http.impl.conn.AbstractPoolEntry.open(AbstractPoolEntry.java:150)
    at org.apache.http.impl.conn.AbstractPooledConnAdapter.open(AbstractPooledConnAdapter.java:119)
    at org.apache.http.impl.client.DefaultClientRequestDirector.execute(DefaultClientRequestDirector.java:309)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:501)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:456)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:422)
    at com.limegroup.gnutella.http.DefaultHttpExecutor.performRequest(DefaultHttpExecutor.java:97)
    at com.limegroup.gnutella.http.DefaultHttpExecutor.access$000(DefaultHttpExecutor.java:26)
    at com.limegroup.gnutella.http.DefaultHttpExecutor$MultiRequestor.run(DefaultHttpExecutor.java:135)
    at org.limewire.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1006)
    at org.limewire.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:549)
    at java.lang.Thread.run(Unknown Source)
"
1,"NumericRangeQuery errors with endpoints near long min and max values. This problem first reported in Solr:

http://lucene.472066.n3.nabble.com/range-query-on-TrieLongField-strange-result-tt970974.html#a970974"
1,Mandatory jcr:activities node missing after upgrade. The mandatory node is only created when the repository is initially empty. The node is missing when an existing repository instance is upgraded. 
1,"NearSpans skipTo bug. NearSpans appears to have a bug in skipTo that causes it to skip over some matching documents completely.  I discovered this bug while investigating problems with SpanWeight.explain, but as far as I can tell the Bug is not specific to Explanations ... it seems like it could potentially result in incorrect matching in some situations where a SpanNearQuery is nested in another query such thatskipTo will be used ... I tried to create a high level test case to exploit the bug when searching, but i could not.  TestCase exploiting the class using NearSpan and SpanScorer will follow..."
1,"RangeQuery & RangeFilter used with collation seek to lowerTerm using compareTo(). The constructor for RangeTermEnum initializes a TermEnum starting with lowerTermText, but when a collator is defined, all terms in the given field need to be checked, since collation can introduce non-Unicode orderings.  Instead, the RangeTermEnum constructor should test for a non-null collator, and if there is one, point the TermEnum at the first term in the given field.

LUCENE-1424 introduced this bug."
1,"cache does not validate multiple cached variants. There is a bug in CachingHttpClient, where when we attempt to collect all the etags for existing cached variants so we can send a conditional request to the origin, we accidentally don't find any, and send an unconditional request instead."
1,"document view: importXML() fails on protected property jcr:primaryType. when trying to import an xml document where elements contain the attribute jcr:primaryType the import fails with:

javax.jcr.nodetype.ConstraintViolationException: cannot set the value of a protected property /testroot/docviewimport/doc/jcr:primaryType
	at org.apache.jackrabbit.core.PropertyImpl.setValue(PropertyImpl.java:907)
	at org.apache.jackrabbit.core.NodeImpl.setProperty(NodeImpl.java:1044)
	at org.apache.jackrabbit.core.xml.DocViewImportHandler.startElement(DocViewImportHandler.java:124)
	at org.apache.jackrabbit.core.xml.ImportHandler.startElement(ImportHandler.java:164)
	at org.apache.xerces.parsers.AbstractSAXParser.startElement(Unknown Source)
	at org.apache.xerces.impl.XMLNSDocumentScannerImpl.scanStartElement(Unknown Source)
	at org.apache.xerces.impl.XMLNSDocumentScannerImpl$NSContentDispatcher.scanRootElementHook(Unknown Source)
	at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl$FragmentContentDispatcher.dispatch(Unknown Source)
	at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanDocument(Unknown Source)
	at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
	at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
	at org.apache.xerces.parsers.XMLParser.parse(Unknown Source)
	at org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source)
	at org.apache.jackrabbit.core.SessionImpl.importXML(SessionImpl.java:836)
	at org.apache.jackrabbit.test.api.DocViewImportTest.setUp(DocViewImportTest.java:92)
	at com.intellij.rt.execution.junit2.JUnitStarter.main(JUnitStarter.java:31)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:78)

if i understand the spec correctly, the import process should take care of this attribute and determine the node type of the new nodes based on it."
1,"PdfTextFilter may leave parsed document open in case of errors. In case of errors in a parsed PDF document jackrabbit may fail to properly close the parsed document. PDFBox will write a stack trace to system out at finalize to warn agains this.

this is the resulting log:

WARN org.apache.jackrabbit.core.query.LazyReader LazyReader.java(read:82) 20.02.2007 15:42:50 exception initializing reader org.apache.jackrabbit.core.query.PdfTextFilter$1: java.io.IOException: Error: Expected hex number, actual=' 2'
java.lang.Throwable: Warning: You did not close the PDF Document
   at org.pdfbox.cos.COSDocument.finalize(COSDocument.java:384)
   at java.lang.ref.Finalizer.invokeFinalizeMethod(Native Method)
   at java.lang.ref.Finalizer.runFinalizer(Finalizer.java:83)
   at java.lang.ref.Finalizer.access$100(Finalizer.java:14)
   at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:160)


this may happens because the parse() method at

parser = new PDFParser(new BufferedInputStream(in));
parser.parse();

immediately creates a document, but it can throw an exception while processing the file.
PdfTextFilter should check if parser still holds a document and close it appropriately.

"
1,"NullPointerException in VirtualNodeTypeStateManager.nodeTypeRegistered. I am working on a custom persistence manager which requires an additional node type being registered. For performance reasons, the existence of this node type is verified during PersistenceManager.init.

Unfortunately this does not seem to work as the VirtualNodeTypeStateManager is not prepared to handle this situation at that point in time - the systemSession field seems to still be null."
1,benchmark alg can't handle negative relative priority. We can now set thread relative priority when we run BG tasks... but if you use a negative number it's parsing it as 0.
1,Deadlock in IndexWriter. If autoCommit == true a merge usually triggers a commit. A commit (prepareCommit) can trigger a merge vi the flush method. There is a synchronization mechanism for commit (commitLock) and a separate synchronization mechanism for merging (ConcurrentMergeScheduler.wait). If one thread holds the commitLock monitor and another one holds the ConcurrentMergeScheduler monitor we have a deadlock.
1,"javax.jcr.NamespaceException: : is not a registered namespace uri. Using the first hops with both versions 1.2.3 and 1.3, the repository is created successfully the first time it is run.  Subsequent attempts to login result in a javax.jcr.NamespaceException.


DEBUG - Initializing transient repository
INFO - Starting repository...
INFO - LocalFileSystem initialized at path repository\repository
Exception in thread ""main"" javax.jcr.NamespaceException: : is not a registered namespace uri.
	at org.apache.jackrabbit.core.NamespaceRegistryImpl.getPrefix(NamespaceRegistryImpl.java:538)
	at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.checkNamespace(NodeTypeRegistry.java:1292)
	at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.validateNodeTypeDef(NodeTypeRegistry.java:1415)
	at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.internalRegister(NodeTypeRegistry.java:1221)
	at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.<init>(NodeTypeRegistry.java:671)
	at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.create(NodeTypeRegistry.java:118)
	at org.apache.jackrabbit.core.RepositoryImpl.createNodeTypeRegistry(RepositoryImpl.java:571)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:262)
	at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:584)
	at org.apache.jackrabbit.core.TransientRepository$2.getRepository(TransientRepository.java:245)
	at org.apache.jackrabbit.core.TransientRepository.startRepository(TransientRepository.java:265)
	at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:333)
	at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:388)
	at testing.FirstHops.main(FirstHops.java:24)"
1,"Exception during IndexWriter.close() prevents release of the write.lock. After encountering a case of index corruption - see http://issues.apache.org/jira/browse/LUCENE-140 - when the close() method encounters an exception in the flushRamSegments() method, the index write.lock is not released (ie. it is not really closed).

The writelock is only released when the IndexWriter is GC'd and finalize() is called."
1,"sysview export/import of multivalue properties seems not to work. the sysview export of multivalue properties does not differ from the export of singlevalue properties.

if a mv-property contains only 1 value, how can the import find the correct property-def?

currently, it throws: 
  javax.jcr.nodetype.ConstraintViolationException: no matching property definition found for <propertyname>"
1,"WorkspaceItemStateFactory#createItemStates throws ClassCastException. When the first item in the ItemInfo iterator returned by the RepositoryService is a PropertyInfo instead a NodeInfo, a ClassCastException is thrown. This should rather be a ItemNotFoundException.
"
1,"fix reverseStringFilter for unicode 4.0. ReverseStringFilter is not aware of supplementary characters: when it reverses it will create unpaired surrogates, which will be replaced by U+FFFD by the indexer (but not at query time).
The wrong words will conflate to each other, and the right words won't match, basically the whole thing falls apart.

This patch implements in-place reverse with the algorithm from apache harmony AbstractStringBuilder.reverse0()
"
1,I/O exceptions can cause loss of buffered deletes. Some I/O exceptions that result in segmentInfos rollback operations can cause buffered deletes that existed before the rollback creation point to be incorrectly lost when the IOException triggers a rollback.
1,"LuceneQueryBuilder assumes readability of root-Node to be granted in any case.. Have a User U. 
Have the User U denied to read ""/"".
Have the User U allowed to read ""/home/u"".

Any query of User U on this workspace fails with an AccessDeniedException.

The exception is caused by a call insided LuceneQueryBuilder on ln212:
NodeId id = ((NodeImpl) session.getRootNode()).getNodeId(); 

I couldn't find a specification that imposes the readability of root-node as a precondtion for query.
Therefore I consider this behavior as a bug."
1,"CLONE -ManageableCollectionUtil doesn't support Maps. ManageableCollectionUtil has two getManageableCollection methods, which do not currently return a ManageableCollection which wraps Maps. 

ManagedHashMap already exists in the codebase which I assume was created for this purpose, so both getManageableCollection methods could be modified so that they do something like:

            if (object instanceof Map){
                return new ManagedHashMap((Map)object);
            }


An alternative solution might be to modify the JCR mapping to support explicitly defining the 'ManagedXXX' class."
1,"Deadlock in DBCP when accessing node. I found a deadlock situation using JR 2.2.10, the problem is with DBCP 1.2.2 and is fixed in DBCP 1.3, JR trunk also uses DBCP 1.2.2 and should also be updated

The ticket in dbcp is #DBCP-270, related tickets are #DBCP-65 #DBCP-281 #DBCP-271

Stack trace of where my call is stalled:
{code}
main@1, prio=5, in group 'main', status: 'MONITOR'
	 blocks Timer-1@2545
	 waiting for Timer-1@2545 to release lock on {1}
	  at org.apache.commons.pool.impl.GenericObjectPool.addObjectToPool(GenericObjectPool.java:1137)
	  at org.apache.commons.pool.impl.GenericObjectPool.returnObject(GenericObjectPool.java:1076)
	  at org.apache.commons.dbcp.PoolableConnection.close(PoolableConnection.java:87)
	  at org.apache.commons.dbcp.PoolingDataSource$PoolGuardConnectionWrapper.close(PoolingDataSource.java:181)
	  at org.apache.jackrabbit.core.util.db.DbUtility.close(DbUtility.java:75)
	  at org.apache.jackrabbit.core.util.db.ResultSetWrapper.invoke(ResultSetWrapper.java:63)
	  at $Proxy12.close(Unknown Source:-1)
	  at org.apache.jackrabbit.core.persistence.pool.BundleDbPersistenceManager.loadBundle(BundleDbPersistenceManager.java:1042)
	  at org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.getBundle(AbstractBundlePersistenceManager.java:669)
	  at org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.load(AbstractBundlePersistenceManager.java:415)
	  at org.apache.jackrabbit.core.state.SharedItemStateManager.loadItemState(SharedItemStateManager.java:1830)
	  at org.apache.jackrabbit.core.state.SharedItemStateManager.getNonVirtualItemState(SharedItemStateManager.java:1750)
	  at org.apache.jackrabbit.core.state.SharedItemStateManager.getItemState(SharedItemStateManager.java:265)
	  at org.apache.jackrabbit.core.state.LocalItemStateManager.getNodeState(LocalItemStateManager.java:109)
	  at org.apache.jackrabbit.core.state.LocalItemStateManager.getItemState(LocalItemStateManager.java:174)
	  at org.apache.jackrabbit.core.state.XAItemStateManager.getItemState(XAItemStateManager.java:260)
	  at org.apache.jackrabbit.core.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:161)
	  at org.apache.jackrabbit.core.ItemManager.getItemData(ItemManager.java:382)
	  at org.apache.jackrabbit.core.ItemManager.getNode(ItemManager.java:669)
	  at org.apache.jackrabbit.core.ItemManager.getNode(ItemManager.java:647)
	  at org.apache.jackrabbit.core.LazyItemIterator.prefetchNext(LazyItemIterator.java:120)
	  at org.apache.jackrabbit.core.LazyItemIterator.next(LazyItemIterator.java:257)
	  at info.magnolia.jcr.iterator.DelegatingNodeIterator.next(DelegatingNodeIterator.java:79)
{code}

This is the offending thread:
{code}
Timer-1@2545 daemon, prio=5, in group 'main', status: 'MONITOR'
	 blocks main@1
	 waiting for main@1 to release lock on {1}
	  at org.apache.commons.dbcp.AbandonedTrace.addTrace(AbandonedTrace.java:176)
	  at org.apache.commons.dbcp.AbandonedTrace.init(AbandonedTrace.java:92)
	  at org.apache.commons.dbcp.AbandonedTrace.<init>(AbandonedTrace.java:82)
	  at org.apache.commons.dbcp.DelegatingStatement.<init>(DelegatingStatement.java:61)
	  at org.apache.commons.dbcp.DelegatingConnection.createStatement(DelegatingConnection.java:224)
	  at org.apache.commons.dbcp.PoolableConnectionFactory.validateConnection(PoolableConnectionFactory.java:331)
	  at org.apache.commons.dbcp.PoolableConnectionFactory.validateObject(PoolableConnectionFactory.java:312)
	  at org.apache.commons.pool.impl.GenericObjectPool.evict(GenericObjectPool.java:1217)
	  at org.apache.commons.pool.impl.GenericObjectPool$Evictor.run(GenericObjectPool.java:1341)
	  at java.util.TimerThread.mainLoop(Timer.java:512)
	  at java.util.TimerThread.run(Timer.java:462)
{code}"
1,"Data Store: garbage collection continues when the session is closed. Currently, the data store garbage collection continues even if the session is closed.

This can cause problems."
1,"XMLTextFilter does not extract text elements. XMLTextFilter only returns the text from attributes, not the content of text elements,"
1,"URI.normalize() error. code:

----------------------------
import org.apache.commons.httpclient.URI;

class Main {
  publi static void main(String[] args) throws Exception {
    URI uri = new URI(""http"", null, ""host"", -1, ""/tmp/../yo"", null, null);
    uri.normalize();
    System.out.println(uri);
  }
} /// end of Main
----------------------------

prints:

http://host/tmp/../yo

instead of

http://host/yo"
1,Fails to remove a previously assigned mixin. Jackrabbit fails to remove a previously assigned mixin. Works fine on version 1.6.1
1,"CVE-2009-0026: Cross site scripting issues in webapp. Some of the jackrabbit-webapp forms don't properly escape user input when displaying it in the resulting HTML page. This leads to potential cross site scripting issues. For example:

    search.jsp?q=%25%22%3Cscript%3Ealert(1)%3C/script%3E
    swr.jsp?q=%25""<script>alert(1)</script>&swrnum=1

The CVE id for this issue is CVE-2009-0026. This issue was reported by the Red Hat Security Response Team."
1,"BLOBFileValue() might be discarded to early. Situation:

if the internal value of a property of type binary is created by the constructor BLOBFileValue(InputStream in) and the content is not stored in an temp-file, then calling the methods 

a) #setProperty(InputStream in) on this node and then
b) #refresh(false) on the node of this property 

on the node of this property leads to an internal value of this property with an erased byte[].

Solution:

Only if the spoolFile is created the field 'temp' should be set to true.
If the InputStream is stored in the byte[] the field 'temp' should be set to false.

Patch:

Index: BLOBFileValue.java
===================================================================
retrieving revision 1.1
diff -u -r1.1 BLOBFileValue.java
--- BLOBFileValue.java	8 May 2006 13:57:49 -0000	1.1
+++ BLOBFileValue.java	8 May 2006 15:19:54 -0000
@@ -142,6 +142,7 @@
                     len += read;
                 }
             }
+            in.close();
         } finally {
             if (out != null) {
                 out.close();
@@ -151,8 +152,15 @@
         // init vars
         file = spoolFile;
         fsResource = null;
-        // this instance is backed by a temporarily allocated resource/buffer
-        temp = true;
+        if (file != null)
+        {
+            // this instance is backed by a temporarily allocated resource
+            temp = true;
+        }
+        else
+        {
+            temp = true;
+        }
     }
 
     /**


"
1,"Finding Newest Segment In Empty Index. While extending the index writer, I discovered that its newestSegment method does not check to see if there are any segments before accessing the segment infos vector. Specifically, if you call the IndexWriter#newestSegment method on a brand-new index which is essentially empty, then it throws an java.lang.ArrayIndexOutOfBoundsException exception.

The proposed fix is to return null if no segments exist, as shown below:

--- lucene/src/java/org/apache/lucene/index/IndexWriter.java	(revision 930788)
+++ lucene/src/java/org/apache/lucene/index/IndexWriter.java	(working copy)
@@ -4587,7 +4587,7 @@
 
   // utility routines for tests
   SegmentInfo newestSegment() {
-    return segmentInfos.info(segmentInfos.size()-1);
+    return segmentInfos.size() > 0 ? segmentInfos.info(segmentInfos.size()-1) : null;
   }
"
1,"Buffered deletes under count RAM. I found this while working on LUCENE-2548: when we freeze the deletes (create FrozenBufferedDeletes), when we set the bytesUsed we are failing to account for RAM required for the term bytes (and now term field)."
1,"Local AuthContext authenticates if LoginModule should be ignored. The LoginModule indicates that it doesn't handle thecurrent login-attemp by returning false:
on login or commit invokation. It should thus be ignored.
The LoginContext calculates the overall login success.
In case of the local context, the login should faile if the only LoginModule is ignored.
Currently this is a success, but the "
1,"ItemStateException on concurrently committing transactions of versioning operations. see tests in JCR-335


org.apache.jackrabbit.core.state.ItemStateException: Unable to resolve path for item: 69d80165-7ef5-4b6b-8aa9-be9c9be1f994
	at org.apache.jackrabbit.core.observation.EventStateCollection.getPath(EventStateCollection.java:525)
	at org.apache.jackrabbit.core.observation.EventStateCollection.createEventStates(EventStateCollection.java:377)
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:547)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:668)
	at org.apache.jackrabbit.core.state.XAItemStateManager.prepare(XAItemStateManager.java:151)
	at org.apache.jackrabbit.core.version.XAVersionManager.prepare(XAVersionManager.java:431)
	at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:129)
	at org.apache.jackrabbit.core.XASessionImpl.prepare(XASessionImpl.java:309)
	at test.JCRUserTransaction.commit(JCRUserTransaction.java:74)
	at org.apache.jackrabbit.JRTestDeadlock.run(JRTestDeadlock.java:110)
Caused by: javax.jcr.ItemNotFoundException: failed to build path of 69d80165-7ef5-4b6b-8aa9-be9c9be1f994: a0ecd4b0-a442-4b1e-a2f6-51441f40d452 has no child entry for 69d80165-7ef5-4b6b-8aa9-be9c9be1f994
	at org.apache.jackrabbit.core.HierarchyManagerImpl.buildPath(HierarchyManagerImpl.java:308)
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getPath(HierarchyManagerImpl.java:357)
	at org.apache.jackrabbit.core.observation.EventStateCollection.getPath(EventStateCollection.java:520)
	... 9 more"
1,"Spell Checker suggestSimilar throws NPE when IndexReader is not null and field is null. The SpellChecker.suggestSimilar(String word, int numSug, IndexReader ir,   String field, boolean morePopular) throws a NullPointerException when the IndexReader is not null, but the Field is.  The Javadocs say that it is fine to have the field be null, but doesn't comment on the fact that the IndexReader also needs to be null in that case.

"
1,"Bundle persistence managers node id key store/load is not symertric on MySql causing NoSuchItemState Exceptions . It looks like the binary values read back from MySql where the UUID contains 0's is not the same as that generated from the UUID getRawBytes() call. As result, you can store a node with the UUID that has 0's but its never found when read back. This therefore causes corruption in random places when certain UUIDs are generated.

Test Case: 

I've attached 2 files. One causes node corruption when imported, the other does not.
The only difference is that I removed any 0 values from the problem UUID in the file that causes corruption.

As Stefan pointed out, I had manipulated the test case to use standard nt types when in fact I should have provided the following info (sorry Stefan) e.g. the test folder types are referencable hence the jcr:uuid allocation

[acme:Folder] > nt:folder, mix:referenceable

If I import causes_corruption.xml and then attempt to ""ls"" AclObjectIdentities then loadBundle() returns null for the UUID 

a55f3f6b-a909-4e8d-b65a-93002ced0920 which in bytes is [-91, 95, 63, 107, -87, 9, 78, -115, -74, 90, -109, 0, 44, -19, 9, 32]

If I import works.xml then ""ls"" works fine for the same node as I've manually changed the UUID to replace 0s with 1s in the last section.

a55f3f6b-a909-4e8d-b65a-93112ced1921 [-91, 95, 63, 107, -87, 9, 78, -115, -74, 90, -109, 17, 44, -19, 25, 33]


Testing shows this issue highlights a problem with the Bundle persistence manager and MySqls method of handling BINARY columns.
The solution looks to be to replace BINARY(16) with VARBINARY(16). Quoting from http://dev.mysql.com/doc/refman/5.0/en/binary-varbinary.html...
""If the value retrieved must be the same as the value specified for storage with no padding, it might be preferable to use VARBINARY or one of the BLOB data types instead.""
A review of our logs shows that all of the corruption we've seen has related to nodes with UUIDs including 0's.

* Shall I log a JIRA ticket for this?
* Anyone see any issues with this fix?


In the following example you can see I'm showing all bundles in the ""test1"" workspace.

mysql> select hex(node_id) from test1_bundle;
+----------------------------------+
| hex(node_id)                     |
+----------------------------------+
| 28126C3E36A0471D9CDC5AC423BAC9C5 |
| A55F3F6BA9094E8DB65A93002CED0920 |
| CAFEBABECAFEBABECAFEBABECAFEBABE |
| D638EACCDEB641FD8868804C8ECEFFFD |
| DEADBEEFCAFEBABECAFEBABECAFEBABE |
+----------------------------------+
5 rows in set (0.00 sec)

...but a select using the same UUID hex value returns no rows.

mysql>  select node_id from test1_bundle where 
mysql> unhex('A55F3F6BA9094E8DB65A93002CED0920') = node_id;
Empty set (0.00 sec)

I've then created a new ""test3"" workspace which I modified to use varbinary instead of binary with:

alter table test3_bundle modify NODE_ID varbinary(16); alter table test3_refs modify NODE_ID varbinary(16);

My import test case now no longer fails and the following query proves that query operations, after a store, return rows as expected.

mysql>  select node_id from test3_bundle where 
mysql> unhex('A55F3F6BA9094E8DB65A93002CED0920') = node_id;
+------------------+
| node_id          |
+--------Z ,--  |
+------------------+
1 row in set (0.00 sec)

mysql> desc test3_bundle;
ERROR 2006 (HY000): MySQL server has gone away No connection. Trying to reconnect...
Connection id:    7116
Current database: mmptest

+-------------+---------------+------+-----+---------+-------+
| Field       | Type          | Null | Key | Default | Extra |
+-------------+---------------+------+-----+---------+-------+
| NODE_ID     | varbinary(16) | YES  | UNI | NULL    |       |
| BUNDLE_DATA | longblob      | NO   |     |         |       |
+-------------+---------------+------+-----+---------+-------+
2 rows in set (0.00 sec)


mysql>  alter table test3_bundle modify NODE_ID varbinary(16);
Query OK, 2 rows affected (0.00 sec)
Records: 2  Duplicates: 0  Warnings: 0

"
1,"don't call SegmentInfo.sizeInBytes for the merging segments. Selckin has been running Lucene's tests on the RT branch, and hit this:
{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestIndexWriter
    [junit] Testcase: testDeleteAllSlowly(org.apache.lucene.index.TestIndexWriter):	FAILED
    [junit] Some threads threw uncaught exceptions!
    [junit] junit.framework.AssertionFailedError: Some threads threw uncaught exceptions!
    [junit] 	at org.apache.lucene.util.LuceneTestCase.tearDown(LuceneTestCase.java:535)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1246)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1175)
    [junit] 
    [junit] 
    [junit] Tests run: 67, Failures: 1, Errors: 0, Time elapsed: 38.357 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testDeleteAllSlowly -Dtests.seed=-4291771462012978364:4550117847390778918
    [junit] The following exceptions were thrown by threads:
    [junit] *** Thread: Lucene Merge Thread #1 ***
    [junit] org.apache.lucene.index.MergePolicy$MergeException: java.io.FileNotFoundException: _4_1.del
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:507)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:472)
    [junit] Caused by: java.io.FileNotFoundException: _4_1.del
    [junit] 	at org.apache.lucene.store.FSDirectory.fileLength(FSDirectory.java:290)
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.fileLength(MockDirectoryWrapper.java:549)
    [junit] 	at org.apache.lucene.index.SegmentInfo.sizeInBytes(SegmentInfo.java:287)
    [junit] 	at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3280)
    [junit] 	at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:2956)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:379)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:447)
    [junit] NOTE: test params are: codec=RandomCodecProvider: {=SimpleText, f6=Pulsing(freqCutoff=15), f7=MockFixedIntBlock(blockSize=1606), f8=SimpleText, f9=MockSep, f1=MockVariableIntBlock(baseBlockSize=99), f0=MockFixedIntBlock(blockSize=1606), f3=Pulsing(freqCutoff=15), f2=MockSep, f5=SimpleText, f4=Standard, f=MockFixedIntBlock(blockSize=1606), c=MockSep, termVector=MockRandom, d9=MockFixedIntBlock(blockSize=1606), d8=Pulsing(freqCutoff=15), d5=SimpleText, d4=Standard, d7=MockRandom, d6=MockVariableIntBlock(baseBlockSize=99), d25=MockRandom, d0=MockRandom, c29=MockFixedIntBlock(blockSize=1606), d24=MockVariableIntBlock(baseBlockSize=99), d1=Standard, c28=Standard, d23=SimpleText, d2=MockFixedIntBlock(blockSize=1606), c27=MockRandom, d22=Standard, d3=MockVariableIntBlock(baseBlockSize=99), d21=Pulsing(freqCutoff=15), d20=MockSep, c22=MockFixedIntBlock(blockSize=1606), c21=Pulsing(freqCutoff=15), c20=MockRandom, d29=MockFixedIntBlock(blockSize=1606), c26=Standard, d28=Pulsing(freqCutoff=15), c25=MockRandom, d27=MockRandom, c24=MockSep, d26=MockVariableIntBlock(baseBlockSize=99), c23=SimpleText, e9=MockRandom, e8=MockSep, e7=SimpleText, e6=MockFixedIntBlock(blockSize=1606), e5=Pulsing(freqCutoff=15), c17=MockFixedIntBlock(blockSize=1606), e3=Standard, d12=MockVariableIntBlock(baseBlockSize=99), c16=Pulsing(freqCutoff=15), e4=SimpleText, d11=MockFixedIntBlock(blockSize=1606), c19=MockSep, e1=MockSep, d14=Pulsing(freqCutoff=15), c18=SimpleText, e2=Pulsing(freqCutoff=15), d13=MockSep, e0=MockVariableIntBlock(baseBlockSize=99), d10=Standard, d19=MockVariableIntBlock(baseBlockSize=99), c11=SimpleText, c10=Standard, d16=Pulsing(freqCutoff=15), c13=MockRandom, c12=MockVariableIntBlock(baseBlockSize=99), d15=MockSep, d18=SimpleText, c15=MockFixedIntBlock(blockSize=1606), d17=Standard, c14=Pulsing(freqCutoff=15), b3=MockSep, b2=SimpleText, b5=Standard, b4=MockRandom, b7=MockVariableIntBlock(baseBlockSize=99), b6=MockFixedIntBlock(blockSize=1606), d50=MockFixedIntBlock(blockSize=1606), b9=Pulsing(freqCutoff=15), b8=MockSep, d43=MockSep, d42=SimpleText, d41=MockFixedIntBlock(blockSize=1606), d40=Pulsing(freqCutoff=15), d47=MockVariableIntBlock(baseBlockSize=99), d46=MockFixedIntBlock(blockSize=1606), b0=MockVariableIntBlock(baseBlockSize=99), d45=Standard, b1=MockRandom, d44=MockRandom, d49=MockVariableIntBlock(baseBlockSize=99), d48=MockFixedIntBlock(blockSize=1606), c6=Pulsing(freqCutoff=15), c5=MockSep, c4=MockVariableIntBlock(baseBlockSize=99), c3=MockFixedIntBlock(blockSize=1606), c9=MockVariableIntBlock(baseBlockSize=99), c8=SimpleText, c7=Standard, d30=SimpleText, d32=MockRandom, d31=MockVariableIntBlock(baseBlockSize=99), c1=SimpleText, d34=MockFixedIntBlock(blockSize=1606), c2=MockSep, d33=Pulsing(freqCutoff=15), d36=MockSep, c0=MockFixedIntBlock(blockSize=1606), d35=SimpleText, d38=MockSep, d37=SimpleText, d39=MockRandom, e92=MockFixedIntBlock(blockSize=1606), e93=MockVariableIntBlock(baseBlockSize=99), e90=MockRandom, e91=Standard, e89=MockVariableIntBlock(baseBlockSize=99), e88=SimpleText, e87=Standard, e86=Pulsing(freqCutoff=15), e85=MockSep, e84=MockVariableIntBlock(baseBlockSize=99), e83=MockFixedIntBlock(blockSize=1606), e80=MockFixedIntBlock(blockSize=1606), e81=SimpleText, e82=MockSep, e77=MockVariableIntBlock(baseBlockSize=99), e76=MockFixedIntBlock(blockSize=1606), e79=Pulsing(freqCutoff=15), e78=MockSep, e73=MockSep, e72=SimpleText, e75=Standard, e74=MockRandom, binary=MockFixedIntBlock(blockSize=1606), f98=Pulsing(freqCutoff=15), f97=MockSep, f99=Standard, f94=Standard, f93=MockRandom, f96=MockVariableIntBlock(baseBlockSize=99), f95=MockFixedIntBlock(blockSize=1606), e95=SimpleText, e94=Standard, e97=MockRandom, e96=MockVariableIntBlock(baseBlockSize=99), e99=MockFixedIntBlock(blockSize=1606), e98=Pulsing(freqCutoff=15), id=MockFixedIntBlock(blockSize=1606), f34=MockSep, f33=SimpleText, f32=MockFixedIntBlock(blockSize=1606), f31=Pulsing(freqCutoff=15), f30=MockRandom, f39=MockFixedIntBlock(blockSize=1606), f38=Standard, f37=MockRandom, f36=MockSep, f35=SimpleText, f43=Standard, f42=MockRandom, f45=MockVariableIntBlock(baseBlockSize=99), f44=MockFixedIntBlock(blockSize=1606), f41=MockSep, f40=SimpleText, f47=MockVariableIntBlock(baseBlockSize=99), f46=MockFixedIntBlock(blockSize=1606), f49=Pulsing(freqCutoff=15), f48=MockSep, content=Pulsing(freqCutoff=15), e19=Standard, e18=MockRandom, e17=MockSep, f12=Pulsing(freqCutoff=15), e16=SimpleText, f11=MockSep, f10=MockVariableIntBlock(baseBlockSize=99), e15=MockFixedIntBlock(blockSize=1606), e14=Pulsing(freqCutoff=15), f16=SimpleText, e13=MockFixedIntBlock(blockSize=1606), f15=Standard, e12=Pulsing(freqCutoff=15), e11=MockRandom, f14=Pulsing(freqCutoff=15), e10=MockVariableIntBlock(baseBlockSize=99), f13=MockSep, f19=Pulsing(freqCutoff=15), f18=MockRandom, f17=MockVariableIntBlock(baseBlockSize=99), e29=MockSep, e26=Standard, f21=SimpleText, e25=MockRandom, f20=Standard, e28=MockVariableIntBlock(baseBlockSize=99), f23=MockRandom, e27=MockFixedIntBlock(blockSize=1606), f22=MockVariableIntBlock(baseBlockSize=99), f25=MockRandom, e22=MockSep, f24=MockVariableIntBlock(baseBlockSize=99), e21=SimpleText, f27=MockFixedIntBlock(blockSize=1606), e24=Standard, f26=Pulsing(freqCutoff=15), e23=MockRandom, f29=MockSep, f28=SimpleText, e20=MockFixedIntBlock(blockSize=1606), field=Pulsing(freqCutoff=15), string=MockVariableIntBlock(baseBlockSize=99), e30=Pulsing(freqCutoff=15), e31=MockFixedIntBlock(blockSize=1606), a98=MockFixedIntBlock(blockSize=1606), e34=MockRandom, a99=MockVariableIntBlock(baseBlockSize=99), e35=Standard, f79=Pulsing(freqCutoff=15), e32=SimpleText, e33=MockSep, b97=Pulsing(freqCutoff=15), f77=Pulsing(freqCutoff=15), e38=MockFixedIntBlock(blockSize=1606), b98=MockFixedIntBlock(blockSize=1606), f78=MockFixedIntBlock(blockSize=1606), e39=MockVariableIntBlock(baseBlockSize=99), b99=SimpleText, f75=MockVariableIntBlock(baseBlockSize=99), e36=MockRandom, f76=MockRandom, e37=Standard, f73=Standard, f74=SimpleText, f71=MockSep, f72=Pulsing(freqCutoff=15), f81=Pulsing(freqCutoff=15), f80=MockSep, e40=MockSep, e41=MockRandom, e42=Standard, e43=MockFixedIntBlock(blockSize=1606), e44=MockVariableIntBlock(baseBlockSize=99), e45=MockSep, e46=Pulsing(freqCutoff=15), f86=SimpleText, e47=MockSep, f87=MockSep, e48=Pulsing(freqCutoff=15), f88=MockRandom, e49=Standard, f89=Standard, f82=MockVariableIntBlock(baseBlockSize=99), f83=MockRandom, f84=Pulsing(freqCutoff=15), f85=MockFixedIntBlock(blockSize=1606), f90=SimpleText, f92=MockRandom, f91=MockVariableIntBlock(baseBlockSize=99), str=MockFixedIntBlock(blockSize=1606), a76=MockVariableIntBlock(baseBlockSize=99), e56=MockVariableIntBlock(baseBlockSize=99), f59=MockSep, a77=MockRandom, e57=MockRandom, a78=Pulsing(freqCutoff=15), e54=Standard, f57=MockFixedIntBlock(blockSize=1606), a79=MockFixedIntBlock(blockSize=1606), e55=SimpleText, f58=MockVariableIntBlock(baseBlockSize=99), e52=MockSep, e53=Pulsing(freqCutoff=15), e50=MockFixedIntBlock(blockSize=1606), e51=MockVariableIntBlock(baseBlockSize=99), f51=SimpleText, f52=MockSep, f50=MockFixedIntBlock(blockSize=1606), f55=MockFixedIntBlock(blockSize=1606), f56=MockVariableIntBlock(baseBlockSize=99), f53=MockRandom, e58=MockVariableIntBlock(baseBlockSize=99), f54=Standard, e59=MockRandom, a80=MockVariableIntBlock(baseBlockSize=99), e60=MockVariableIntBlock(baseBlockSize=99), a82=Pulsing(freqCutoff=15), a81=MockSep, a84=SimpleText, a83=Standard, a86=MockRandom, a85=MockVariableIntBlock(baseBlockSize=99), a89=MockRandom, f68=Standard, e65=Pulsing(freqCutoff=15), f69=SimpleText, e66=MockFixedIntBlock(blockSize=1606), a87=SimpleText, e67=SimpleText, a88=MockSep, e68=MockSep, e61=Standard, e62=SimpleText, e63=MockVariableIntBlock(baseBlockSize=99), e64=MockRandom, f60=MockRandom, f61=Standard, f62=MockFixedIntBlock(blockSize=1606), f63=MockVariableIntBlock(baseBlockSize=99), e69=SimpleText, f64=MockSep, f65=Pulsing(freqCutoff=15), f66=Standard, f67=SimpleText, f70=Standard, a93=MockRandom, a92=MockVariableIntBlock(baseBlockSize=99), a91=SimpleText, e71=SimpleText, a90=Standard, e70=Standard, a97=MockSep, a96=SimpleText, a95=MockFixedIntBlock(blockSize=1606), a94=Pulsing(freqCutoff=15), c58=MockRandom, a63=Pulsing(freqCutoff=15), a64=MockFixedIntBlock(blockSize=1606), c59=Standard, c56=SimpleText, d59=MockVariableIntBlock(baseBlockSize=99), a61=MockVariableIntBlock(baseBlockSize=99), c57=MockSep, a62=MockRandom, c54=Pulsing(freqCutoff=15), c55=MockFixedIntBlock(blockSize=1606), a60=SimpleText, c52=MockVariableIntBlock(baseBlockSize=99), c53=MockRandom, d53=MockSep, d54=Pulsing(freqCutoff=15), d51=MockFixedIntBlock(blockSize=1606), d52=MockVariableIntBlock(baseBlockSize=99), d57=MockVariableIntBlock(baseBlockSize=99), b62=MockSep, d58=MockRandom, b63=Pulsing(freqCutoff=15), d55=Standard, b60=MockFixedIntBlock(blockSize=1606), d56=SimpleText, b61=MockVariableIntBlock(baseBlockSize=99), b56=SimpleText, b55=Standard, b54=Pulsing(freqCutoff=15), b53=MockSep, d61=MockVariableIntBlock(baseBlockSize=99), b59=Pulsing(freqCutoff=15), d60=MockFixedIntBlock(blockSize=1606), b58=MockRandom, b57=MockVariableIntBlock(baseBlockSize=99), c62=MockRandom, c61=MockVariableIntBlock(baseBlockSize=99), a59=Standard, c60=SimpleText, a58=MockRandom, a57=MockSep, a56=SimpleText, a55=MockFixedIntBlock(blockSize=1606), a54=Pulsing(freqCutoff=15), a72=SimpleText, c67=MockFixedIntBlock(blockSize=1606), a73=MockSep, c68=MockVariableIntBlock(baseBlockSize=99), a74=MockRandom, c69=MockSep, a75=Standard, c63=SimpleText, c64=MockSep, a70=Pulsing(freqCutoff=15), c65=MockRandom, a71=MockFixedIntBlock(blockSize=1606), c66=Standard, d62=Standard, d63=SimpleText, d64=MockVariableIntBlock(baseBlockSize=99), b70=Pulsing(freqCutoff=15), d65=MockRandom, b71=Standard, d66=Pulsing(freqCutoff=15), b72=SimpleText, d67=MockFixedIntBlock(blockSize=1606), b73=MockVariableIntBlock(baseBlockSize=99), d68=SimpleText, b74=MockRandom, d69=MockSep, b65=MockRandom, b64=MockVariableIntBlock(baseBlockSize=99), b67=MockFixedIntBlock(blockSize=1606), b66=Pulsing(freqCutoff=15), d70=Pulsing(freqCutoff=15), b69=MockSep, b68=SimpleText, d72=SimpleText, d71=Standard, c71=MockFixedIntBlock(blockSize=1606), c70=Pulsing(freqCutoff=15), a69=MockSep, c73=MockSep, c72=SimpleText, a66=Standard, a65=MockRandom, a68=MockVariableIntBlock(baseBlockSize=99), a67=MockFixedIntBlock(blockSize=1606), c32=MockFixedIntBlock(blockSize=1606), c33=MockVariableIntBlock(baseBlockSize=99), c30=MockRandom, c31=Standard, c36=Standard, a41=MockFixedIntBlock(blockSize=1606), c37=SimpleText, a42=MockVariableIntBlock(baseBlockSize=99), a0=MockSep, c34=MockSep, c35=Pulsing(freqCutoff=15), a40=Standard, b84=SimpleText, d79=MockFixedIntBlock(blockSize=1606), b85=MockSep, b82=Pulsing(freqCutoff=15), d77=MockRandom, c38=Standard, b83=MockFixedIntBlock(blockSize=1606), d78=Standard, c39=SimpleText, b80=MockVariableIntBlock(baseBlockSize=99), d75=SimpleText, b81=MockRandom, d76=MockSep, d73=Pulsing(freqCutoff=15), d74=MockFixedIntBlock(blockSize=1606), d83=MockFixedIntBlock(blockSize=1606), a9=MockVariableIntBlock(baseBlockSize=99), d82=Pulsing(freqCutoff=15), d81=MockRandom, d80=MockVariableIntBlock(baseBlockSize=99), b79=MockFixedIntBlock(blockSize=1606), b78=Standard, b77=MockRandom, b76=MockSep, b75=SimpleText, a1=MockFixedIntBlock(blockSize=1606), a35=Pulsing(freqCutoff=15), a2=MockVariableIntBlock(baseBlockSize=99), a34=MockSep, a3=MockSep, a33=MockVariableIntBlock(baseBlockSize=99), a4=Pulsing(freqCutoff=15), a32=MockFixedIntBlock(blockSize=1606), a5=Standard, a39=MockRandom, c40=Standard, a6=SimpleText, a38=MockVariableIntBlock(baseBlockSize=99), a7=MockVariableIntBlock(baseBlockSize=99), a37=SimpleText, a8=MockRandom, a36=Standard, c41=MockSep, c42=Pulsing(freqCutoff=15), c43=Standard, c44=SimpleText, c45=MockVariableIntBlock(baseBlockSize=99), a50=MockSep, c46=MockRandom, a51=Pulsing(freqCutoff=15), c47=Pulsing(freqCutoff=15), a52=Standard, c48=MockFixedIntBlock(blockSize=1606), a53=SimpleText, b93=MockRandom, d88=MockSep, c49=Pulsing(freqCutoff=15), b94=Standard, d89=Pulsing(freqCutoff=15), b95=MockFixedIntBlock(blockSize=1606), b96=MockVariableIntBlock(baseBlockSize=99), d84=MockRandom, b90=MockFixedIntBlock(blockSize=1606), d85=Standard, b91=SimpleText, d86=MockFixedIntBlock(blockSize=1606), b92=MockSep, d87=MockVariableIntBlock(baseBlockSize=99), d92=MockSep, d91=SimpleText, d94=Standard, d93=MockRandom, b87=MockVariableIntBlock(baseBlockSize=99), b86=MockFixedIntBlock(blockSize=1606), d90=MockFixedIntBlock(blockSize=1606), b89=Pulsing(freqCutoff=15), b88=MockSep, a44=SimpleText, a43=Standard, a46=MockRandom, a45=MockVariableIntBlock(baseBlockSize=99), a48=MockFixedIntBlock(blockSize=1606), a47=Pulsing(freqCutoff=15), c51=Pulsing(freqCutoff=15), a49=SimpleText, c50=MockSep, d98=MockVariableIntBlock(baseBlockSize=99), d97=MockFixedIntBlock(blockSize=1606), d96=Standard, d95=MockRandom, d99=MockSep, a20=MockSep, c99=MockRandom, c98=MockVariableIntBlock(baseBlockSize=99), c97=SimpleText, c96=Standard, b19=MockRandom, a16=MockSep, a17=Pulsing(freqCutoff=15), b17=SimpleText, a14=MockFixedIntBlock(blockSize=1606), b18=MockSep, a15=MockVariableIntBlock(baseBlockSize=99), a12=MockRandom, a13=Standard, a10=SimpleText, a11=MockSep, b11=MockVariableIntBlock(baseBlockSize=99), b12=MockRandom, b10=SimpleText, b15=SimpleText, b16=MockSep, a18=MockSep, b13=Pulsing(freqCutoff=15), a19=Pulsing(freqCutoff=15), b14=MockFixedIntBlock(blockSize=1606), b30=MockFixedIntBlock(blockSize=1606), a31=MockVariableIntBlock(baseBlockSize=99), a30=MockFixedIntBlock(blockSize=1606), b28=MockFixedIntBlock(blockSize=1606), a25=Standard, b29=MockVariableIntBlock(baseBlockSize=99), a26=SimpleText, a27=MockVariableIntBlock(baseBlockSize=99), a28=MockRandom, a21=MockFixedIntBlock(blockSize=1606), a22=MockVariableIntBlock(baseBlockSize=99), a23=MockSep, a24=Pulsing(freqCutoff=15), b20=Pulsing(freqCutoff=15), b21=MockFixedIntBlock(blockSize=1606), b22=SimpleText, b23=MockSep, a29=MockVariableIntBlock(baseBlockSize=99), b24=MockRandom, b25=Standard, b26=MockFixedIntBlock(blockSize=1606), b27=MockVariableIntBlock(baseBlockSize=99), b41=Standard, b40=MockRandom, c77=Standard, c76=MockRandom, c75=MockSep, c74=SimpleText, c79=MockVariableIntBlock(baseBlockSize=99), c78=MockFixedIntBlock(blockSize=1606), c80=MockRandom, c83=SimpleText, c84=MockSep, c81=Pulsing(freqCutoff=15), b39=Standard, c82=MockFixedIntBlock(blockSize=1606), b37=Standard, b38=SimpleText, b35=MockSep, b36=Pulsing(freqCutoff=15), b33=MockFixedIntBlock(blockSize=1606), b34=MockVariableIntBlock(baseBlockSize=99), b31=MockRandom, b32=Standard, str2=MockFixedIntBlock(blockSize=1606), b50=MockVariableIntBlock(baseBlockSize=99), b52=Pulsing(freqCutoff=15), str3=SimpleText, b51=MockSep, c86=MockVariableIntBlock(baseBlockSize=99), tvtest=MockSep, c85=MockFixedIntBlock(blockSize=1606), c88=Pulsing(freqCutoff=15), c87=MockSep, c89=Standard, c90=SimpleText, c91=MockSep, c92=MockRandom, c93=Standard, c94=MockFixedIntBlock(blockSize=1606), c95=MockVariableIntBlock(baseBlockSize=99), content1=Pulsing(freqCutoff=15), b46=MockVariableIntBlock(baseBlockSize=99), b47=MockRandom, content3=MockVariableIntBlock(baseBlockSize=99), b48=Pulsing(freqCutoff=15), content4=MockFixedIntBlock(blockSize=1606), b49=MockFixedIntBlock(blockSize=1606), content5=Standard, b42=MockSep, b43=Pulsing(freqCutoff=15), b44=Standard, b45=SimpleText}, locale=tr, timezone=MET
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestMergeSchedulerExternal, TestCharTokenizers, TestCodecs, TestFieldInfos, TestFlushByRamOrCountsPolicy, TestIndexReaderReopen, TestIndexWriter]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=69508608,total=127336448
{noformat}

Simon dug and it looks like this is a trunk issue, caused by LUCENE-1076 (only committed to trunk so far)."
1,"Node.restore fails with multiple mixin types. Restoring a node that has more than one mixin type causes the exception below. 

java.lang.NullPointerException
	at org.apache.jackrabbit.core.NodeImpl.restoreFrozenState(NodeImpl.java:3286)
	at org.apache.jackrabbit.core.NodeImpl.internalRestore(NodeImpl.java:3243)
	at org.apache.jackrabbit.core.NodeImpl.internalRestore(NodeImpl.java:3210)
	at org.apache.jackrabbit.core.NodeImpl.restore(NodeImpl.java:2821)
	at com.gtnet.jcr.VersionedNodeTest.testNodeVersionAndRestore(VersionedNodeTest.java:311)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at com.intellij.rt.execution.junit2.JUnitStarter.main(JUnitStarter.java:31)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:78)
"
1,"relative URIs with internal double-slashes ('//') misparsed. URI.parseUriReference()'s heuristic for interpreting URI parts is thrown off by relative URIs which include an internal '//'. As a result, portions of the supplied relative URI (path) can be lost. 

For example:

URI rel = new URI(""foo//bar//baz"");
rel.toString();
(java.lang.String) //bar//baz

The culprit seems to be line 1961 of URI improperly concluding that two slashes later than the beginning of 'tmp' are still indicative the URI is a 'net_path'. 

A possible quick fix might be to add a '!isStartedFromPath &&' to the beginning of the line 1961 test, making the line:

            if (!isStartedFromPath && at + 2 < length && tmp.charAt(at + 1) == '/') {

... and thus preventing the misguided authority-parsing from happening when earlier analysis already identified the current string as a strictly path-oriented URI.

(It also appears the setting of the is_net_path boolean at the end of this if's block may be wrong; this code is run for hier_path URIs that are not net_paths in the 2396 syntax. For example:

URI uri = new URI(""http://www.example.com/some/page"");
uri.isNetPath();
 (boolean) true 

)"
1,"Nodes having OPV=Ignore are removed on restore. JCR1.0 Specification mentions:

8.2.11.5 IGNORE
  Child Node
    On checkin of N, no state information about C will be stored in VN.
    On restore of VN, the child node C of the current N will remain and not be removed.
  Property
    On checkin of N, no state information about P will be stored in VN.
    On restore of VN, the property P of the current N will remain and not be removed.

but the current implementation removed the ignore child."
1,"Contrib query org.apache.lucene.search.BoostingQuery sets boost on constructor Query, not cloned copy. BoostingQuery sets the boost value on the passed context Query

    public BoostingQuery(Query match, Query context, float boost) {
      this.match = match;
      this.context = (Query)context.clone();        // clone before boost
      this.boost = boost;

      context.setBoost(0.0f);                      // ignore context-only matches
    }

This should be 
      this.context.setBoost(0.0f);                      // ignore context-only matches

Also, boost value of 0.0 may have wrong effect - see discussion at

http://www.mail-archive.com/java-user@lucene.apache.org/msg12243.html 

"
1,small SentinelIntSet can cause infinite loop on resize. A small initial size of <=4 can cause the set to not rehash soon enough and thus go into an infinite loop searching the table for an open space.
1,"JCA will not compile with J2EE1.3 classes. In JCAManagedConnectionFactory, the constructor invoked to throw ResourceException does not exist under J2EE1.3 / JCA1.1 classes.
 throw new ResourceException(e)  -  line 136 and line 277.

Instead the code needs to do something like:

            ResourceException exception = new ResourceException(""Failed to create session"");
            exception.setLinkedException(e);
            throw exception;

This will allow it to compile/run under J2EE1.3
"
1,"Yet another race in IW#nrtIsCurrent. In IW#nrtIsCurrent looks like this:

{code}
  synchronized boolean nrtIsCurrent(SegmentInfos infos) {
    ensureOpen();
    return infos.version == segmentInfos.version && !docWriter.anyChanges() && !bufferedDeletesStream.any();
  }
{code}

* the version changes once we checkpoint the IW
* docWriter has changes if there are any docs in ram or any deletes in the delQueue
* bufferedDeletes contain all frozen del packages from the delQueue

yet, what happens is 1. we decrement the numDocsInRam in DWPT#doAfterFlush (which is executed during DWPT#flush) but before we checkpoint. 2. if we freeze deletes (empty the delQueue) we put them in the flushQueue to maintain the order.  This means they are not yet in the bufferedDeleteStream.

Bottom line, there is a window where we could see IW#nrtIsCurrent returning true if we check within this particular window. Phew, I am not 100% sure if that is the reason for our latest failure in SOLR-2861 but from what the logs look like this could be what happens. If we randomly hit low values for maxBufferedDocs & maxBufferedDeleteTerms this is absolutely possible."
1,"TestStressNRT failures (reproducible). Build server logs. Reproduces on at least two machines.

{noformat}
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestStressNRT -Dtestmethod=test -Dtests.seed=69468941c1bbf693:19e66d58475da929:69e9d2f81769b6d0 -Dargs=""-Dfile.encoding=UTF-8""
    [junit] NOTE: test params are: codec=Lucene3x, sim=RandomSimilarityProvider(queryNorm=true,coord=false): {}, locale=ro, timezone=Etc/GMT+1
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestStressNRT]
    [junit] NOTE: Linux 3.0.0-16-generic amd64/Sun Microsystems Inc. 1.6.0_27 (64-bit)/cpus=2,threads=1,free=74960064,total=135987200
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: test(org.apache.lucene.index.TestStressNRT):	Caused an ERROR
    [junit] MockDirectoryWrapper: cannot close: there are still open files: {_ng.cfs=8}
    [junit] java.lang.RuntimeException: MockDirectoryWrapper: cannot close: there are still open files: {_ng.cfs=8}
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:555)
    [junit] 	at org.apache.lucene.index.TestStressNRT.test(TestStressNRT.java:385)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$SubclassSetupTeardownRule$1.evaluate(LuceneTestCase.java:743)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$InternalSetupTeardownRule$1.evaluate(LuceneTestCase.java:639)
    [junit] 	at org.apache.lucene.util.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:22)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$TestResultInterceptorRule$1.evaluate(LuceneTestCase.java:538)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$RememberThreadRule$1.evaluate(LuceneTestCase.java:600)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:164)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)
    [junit] 	at org.apache.lucene.util.StoreClassNameRule$1.evaluate(StoreClassNameRule.java:21)
    [junit] 	at org.apache.lucene.util.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:22)
    [junit] Caused by: java.lang.RuntimeException: unclosed IndexInput: _ng.cfs
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.addFileHandle(MockDirectoryWrapper.java:479)
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper$1.openSlice(MockDirectoryWrapper.java:777)
    [junit] 	at org.apache.lucene.store.CompoundFileDirectory.openInput(CompoundFileDirectory.java:221)
    [junit] 	at org.apache.lucene.codecs.lucene3x.TermInfosReader.<init>(TermInfosReader.java:112)
    [junit] 	at org.apache.lucene.codecs.lucene3x.Lucene3xFields.<init>(Lucene3xFields.java:84)
    [junit] 	at org.apache.lucene.codecs.lucene3x.PreFlexRWPostingsFormat$1.<init>(PreFlexRWPostingsFormat.java:51)
    [junit] 	at org.apache.lucene.codecs.lucene3x.PreFlexRWPostingsFormat.fieldsProducer(PreFlexRWPostingsFormat.java:51)
    [junit] 	at org.apache.lucene.index.SegmentCoreReaders.<init>(SegmentCoreReaders.java:108)
    [junit] 	at org.apache.lucene.index.SegmentReader.<init>(SegmentReader.java:51)
    [junit] 	at org.apache.lucene.index.IndexWriter$ReadersAndLiveDocs.getMergeReader(IndexWriter.java:521)
    [junit] 	at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3587)
    [junit] 	at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3257)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:382)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:451)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.index.TestStressNRT FAILED
{noformat}"
1,"jcr-server should respect child node definition of jcr:content. When creating a new file, jcr:content defaults to nt:unstructured. This causes file creation to fail when the underlying persistent store (i.e. SPI implementation) does not support nt:unstructured for jcr:content. 

I suggest to check whether the underlying implementation provides its own node type for jcr:content and use that one. If not, default to nt:unstructured."
1,"Node.orderBefore and JackrabbitNode.rename should check for ability to modify children-collection on parent node. currently the implementation of Node.orderBefore and JackrabbitNode.rename perform the same validation that is executed
for a move operation which includes removal of the original node. however, the methods mentioned above only include
a manipulation on the child-node-collection of the parent (subset of the current check). therefore the permission check should be 
adjusted accordingly."
1,"basetokenstreamtestcase should fail if tokenstream starts with posinc=0. This is meaningless for a tokenstream to start with posinc=0,

Its also caused problems and hairiness in the indexer (LUCENE-1255, LUCENE-1542),
and it makes senseless tokenstreams. We should add a check and fix any that do this.

Furthermore the same bug can exist in removing-filters if they have enablePositionIncrements=false.
I think this option is useful: but it shouldnt mean 'allow broken tokenstream', it just means we
don't add gaps. 

If you remove tokens with enablePositionIncrements=false it should not cause the TS to start with
positionincrement=0, and it shouldnt 'restructure' the tokenstream (e.g. moving synonyms on top of a different word).
It should just not add any 'holes'.
"
1,"Freezes w/ MultiThreadedHttpConnectionManager. My single threaded user of VFS (an HttpClient user, that uses
MultiThreadedHttpConnectionManager) hangs [I suspect indefinitely] on minor
activity.

I've turned on HttpClient debug and I see this, the last line
being the last thing I get...

2003/10/09 09:34:26:482 MDT [DEBUG] wire - -<< ""Content-Type:
text/html[\r][\n]""
2003/10/09 09:34:26:482 MDT [DEBUG] HttpMethodBase - -Resorting to protocol
version default close co
nnection policy
2003/10/09 09:34:26:492 MDT [DEBUG] HttpMethodBase - -Should NOT close
connection, using HTTP/1.1
2003/10/09 09:34:26:502 MDT [DEBUG] HttpMethodDirector - -Execute loop try 1
2003/10/09 09:34:26:512 MDT [DEBUG]
MultiThreadedHttpConnectionManager - -HttpConnectionManager.getC
onnection:  config = HostConfiguration[host=www.ibiblio.org,
protocol=http:80, port=80], timeout = 0

2003/10/09 09:34:26:522 MDT [DEBUG]
MultiThreadedHttpConnectionManager - -Unable to get a connection
, waiting..., hostConfig=HostConfiguration[host=www.ibiblio.org,
protocol=http:80, port=80]

This is pretty reproducible. When I hack VFS not to use the
MultiThreadedHttpConnectionManager I don't get the problem."
1,"[SPI] Node.setProperty with null value throws ItemNotFoundException. Node.setProperty with a null value should not throw a ItemNotFoundException in the case a property with the given name does not exist. Rather should it return a stale property which throws an InvalidItemStateException when its methods are accessed. 

This behavior is also consistent with jackrabbit-core.
"
1,"NullPointerException in GarbageCollector.scan() if no DataStore configured. 
I am running the garbage collector in a separate thread every 5 minutes.

            GarbageCollector gc = ((SessionImpl)mSession).createDataStoreGarbageCollector();
            gc.scan();
            gc.stopScan();
            int du = gc.deleteUnused();

When using Jackrabbit v1.5.2 I get sometimes a null pointer exception;

java.lang.NullPointerException
        at org.apache.jackrabbit.core.data.GarbageCollector.scan(GarbageCollector.java:153)



"
1,"errors in text filters can cause indexing to fail without warning the client. i've opened this issue to track the discussion at <http://thread.gmane.org/gmane.comp.apache.jackrabbit.devel/5086>. briefly, exceptions thrown by text filters are logged and swallowed by jackrabbit; there's no way for a text filter to signal to the jcr client that indexing failed.

some solutions have been proposed, including throwing an unchecked exception, which doesn't allow jackrabbit to maintain transactional integrity, and giving filters veto power over the observed repository operation. depending on the difficulty of the solution that is eventually determined to be correct, it may be sufficient for 1.0 to document the issue and perhaps improve the warning/error logging.
"
1,"If indexwriter hits a non-ioexception from indexExists it leaks a write.lock. the rest of IW's ctor is careful about this.

IndexReader.indexExists catches any IOException and returns false, but the problem
occurs if some other exception (in my test, UnsupportedOperationException, but you
can imagine others are possible), when trying to e.g. read in the segments file.

I think we just need to move the IR.exists stuff inside the try / finally"
1,"lazy fields don't enforce binary vs string value. If you have a binary field, and load it lazy, and then ask that field
for its stringValue, it will incorrectly give you a String back (and
then will refuse to give a binaryValue).  And, vice-versa."
1,"Base64 bug - last buffer not flushed. I found an issue when using the org.apache.jackrabbit.util.Base64.encode(InputStream in, OutputStream out) method. It appears that the issue is that the last buffer is not flushed on the Writer that it creates before returning from the method. I was able to work around this issue by creating a Writer my own program, and call another encode method, and then call the flush() method before using the data. The source in trunk appears the same.
"
1,"Cookies with ',' in the value string is not parsed correctly in some cases. This version extracts the ""Set-Cookie"" statementes of the following
HTTP response headers incorrectly.

The HTTP response is sent when executing GET method on --->
""http://my.taishinbank.com.tw/netbank/nbslogin.asp?
subFunID=https://my.taishinbank.com.tw/netbank/AccountQuery/QAccbyID.asp""

After the HttpClient extracts Set-Cookie from the response, it generates a wrong
cookie statement---->

  [INFO] wire - ->> ""Cookie: $Version=0; _mysite=520163500; 1027657033=null; 
   1027787539=null; 0=null; $Path=/; cata=11; $Path=/;   
   ASPSESSIONIDGGGQQXEU=ADLCDAGAJLKEBJEKBOMMAMOB; 
   $Path=/""

, where it shall 
be ""_mysite=520163500,1027657033,1027787539,1027787539,0;"" ,but 
not ""_mysite=520163500; 1027657033=null; 1027787539=null; 0=null;""
 

Thank you"
1,"Benchmark does not close its Reader when OpenReader/CloseReader are not used. Only the Searcher is closed, but because the reader is passed to the Searcher, the Searcher does not close the Reader, causing a resource leak."
1,"problem with isIPv4address() for relative uri's. the following block of code:

try {
	URI uri = new URI(""http://10.0.1.10:8830"");
	System.out.println(""is IP=""+uri. isIPv4address());
	uri = new URI(uri, ""/04-1.html"");
	System.out.println(""is IP=""+uri. isIPv4address());
} catch (URIException e) { ; }

returns the output:

is IP=true
is IP=false

so by being created from a relative uri URI objects don't have the right setting of  isIPv4address()."
1,"NPE in ObservationManagerImpl.getRegisteredEventListeners() during shutdown after broken startup. See JCR-2378. The variable ""dispatcher"" is passed as null in the constructor."
1,"DefaultLoginModule/SimpleLoginModule don't support custom PrincipalProvider. When configuring a custom PrincipalProvider for the SimpleLoginModule or DefaultLoginModule, inside of a repository.xml file with configuration such as the following:

    <Security appName=""Jackrabbit"">
        <AccessManager
            class=""org.apache.jackrabbit.core.security.DefaultAccessManager"">
        </AccessManager>
        <LoginModule
            class=""org.apache.jackrabbit.core.security.authentication.DefaultLoginModule"">
            <param name=""principalprovider"" value=""com.foo.jcr.BasicPrincipalProvider""/>
        </LoginModule>
      <SecurityManager class=""org.apache.jackrabbit.core.DefaultSecurityManager"">         
      </SecurityManager>    
    </Security>

And that yields the following stacktrace:

javax.jcr.LoginException: org.apache.jackrabbit.core.security.authentication.DefaultLoginModule does not support 'principalprovider: org.apache.jackrabbit.core.security.authentication.DefaultLoginModule does not support 'principalprovider: org.apache.jackrabbit.core.security.authentication.DefaultLoginModule does not support 'principalprovider
	at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1353)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:53)
	at com.foo.jcr.PrincipalProviderTest.testPrincipalProvider(PrincipalProviderTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at org.junit.internal.runners.TestMethod.invoke(TestMethod.java:59)
	at org.junit.internal.runners.MethodRoadie.runTestMethod(MethodRoadie.java:98)
	at org.junit.internal.runners.MethodRoadie$2.run(MethodRoadie.java:79)
	at org.junit.internal.runners.MethodRoadie.runBeforesThenTestThenAfters(MethodRoadie.java:87)
	at org.junit.internal.runners.MethodRoadie.runTest(MethodRoadie.java:77)
	at org.junit.internal.runners.MethodRoadie.run(MethodRoadie.java:42)
	at org.junit.internal.runners.JUnit4ClassRunner.invokeTestMethod(JUnit4ClassRunner.java:88)
	at org.junit.internal.runners.JUnit4ClassRunner.runMethods(JUnit4ClassRunner.java:51)
	at org.junit.internal.runners.JUnit4ClassRunner$1.run(JUnit4ClassRunner.java:44)
	at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:27)
	at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:37)
	at org.junit.internal.runners.JUnit4ClassRunner.run(JUnit4ClassRunner.java:42)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:45)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)
Caused by: javax.security.auth.login.LoginException: org.apache.jackrabbit.core.security.authentication.DefaultLoginModule does not support 'principalprovider
	at org.apache.jackrabbit.core.security.authentication.LocalAuthContext.login(LocalAuthContext.java:68)
	at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1346)
	... 24 more
javax.security.auth.login.LoginException: org.apache.jackrabbit.core.security.authentication.DefaultLoginModule does not support 'principalprovider
	at org.apache.jackrabbit.core.security.authentication.LocalAuthContext.login(LocalAuthContext.java:68)
	at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1346)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:53)
	at com.foo.jcr.PrincipalProviderTest.testPrincipalProvider(PrincipalProviderTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at org.junit.internal.runners.TestMethod.invoke(TestMethod.java:59)
	at org.junit.internal.runners.MethodRoadie.runTestMethod(MethodRoadie.java:98)
	at org.junit.internal.runners.MethodRoadie$2.run(MethodRoadie.java:79)
	at org.junit.internal.runners.MethodRoadie.runBeforesThenTestThenAfters(MethodRoadie.java:87)
	at org.junit.internal.runners.MethodRoadie.runTest(MethodRoadie.java:77)
	at org.junit.internal.runners.MethodRoadie.run(MethodRoadie.java:42)
	at org.junit.internal.runners.JUnit4ClassRunner.invokeTestMethod(JUnit4ClassRunner.java:88)
	at org.junit.internal.runners.JUnit4ClassRunner.runMethods(JUnit4ClassRunner.java:51)
	at org.junit.internal.runners.JUnit4ClassRunner$1.run(JUnit4ClassRunner.java:44)
	at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:27)
	at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:37)
	at org.junit.internal.runners.JUnit4ClassRunner.run(JUnit4ClassRunner.java:42)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:45)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)"
1,"default value with autocreated fields like null. I have a custom nodetype which has two properties, both autocreated (but not mandatory).
If I create the node and want to read the first property (not filling in any value), I get a:

javax.jcr.RepositoryException: Unable to get value of /pages/mjo:page/jcr:content/mjo:title:java.lang.ArrayIndexOutOfBoundsException: 0
 at org.apache.jackrabbit.core.PropertyImpl.getValue(PropertyImpl.java:378)
 at de.freaquac.test.JCRTest.propIterator(JCRTest.java:207)

After setting the property the exception is no longer thrown. Not sure if this really an error but is a little bit anyoing. Are there any default values (null) in jsr170?"
1,"Problem with redirect on HEAD when (bad, naughty) server returns body content. I've been testing/using HttpClient 2.0a3 with Resin 2.1.9. I've found that when
using a HEAD request on a JSP, Resin returns the body content along with the
headers.

In this case, something in the HttpClient breaks. Looking at the httpclient
logs, it looks like:

1) HttpClient does a HEAD against the original URL
2) Resin returns valid status line and headers
3) HttpClient parses the headers and recognizes the redirect header
4) HttpClient does a HEAD against the new URL (from the Location header)
5) HttpMethodBase calls readStatusLine, which (eventually) calles readRawLine in
HttpConnection (which reads from the internal inputStream)
6) readRawLine returns the first line in the body from the original HEAD request
in (1).

It looks like the original body content (in response to the first HEAD) is being
buffered somewhere, but I can't figure out where.

I know that this is invalid behavior on the server's part, but I would like to
be able to recover from it.



---- redir_test.jsp ----
<?xml version=""1.0""?>
<% 
  response.setStatus(response.SC_MOVED_TEMPORARILY);
  response.setHeader(""Location"", ""redirect_pass.xml"");
%>
<some>
  <dummy>
    <data attr=""yea, well""/>
  </dummy>
</some>"
1,"HttpMethodBase(String) incorrectly encoding URI. The HttpMethodBase(String) constructor is handling URIs incorrectly. The
Javadocs indicate that the given URI should already be escaped but the
constructor uses the URI constructor for unescaped URIs."
1,"NullPointerException on removing a node acquired from search result. with a code snipped like the following, i get a NullPointerException in ItemState:

Session s = repo.login(sc,workspace);
QueryManager qm = s.getWorkspace().getQueryManager();
Query q = qm.createQuery(""SELECT * FROM [nt:unstructured]"", Query.JCR_SQL2);
QueryResult r = q.execute();
NodeIterator i = r.getNodes();

Node n = i.nextNode();
n.remove(); // breaks here with NullPointerException

Exception in thread ""main"" java.lang.NullPointerException                                                                                                                                                                                   
        at org.apache.jackrabbit.jcr2spi.state.ItemState.getParent(ItemState.java:210)                                                                                                                                                      
        at org.apache.jackrabbit.jcr2spi.operation.Remove.create(Remove.java:98)                                                                                                                                                            
        at org.apache.jackrabbit.jcr2spi.ItemImpl.remove(ItemImpl.java:306) "
1,"New socket timeout value wont have effect if connection is reused. Reported by Teemu Tingander <Teemu.Tingander at tecnomen.fi> on The Jakarta
Commons HttpClient Developer List:

<snip>
Changing read timeout ()wont affect after successful method execution using
same connection.. 

This seems to be a bug in HttpClient class method
executeMethod(HostConfiguration ...)..

The problematic section seems to be if section checking if connection is
open
	
		method.setStrictMode(strictMode);
        		        
            if (!connection.isOpen()) {                
                connection.setConnectionTimeout(connectionTimeout);
-->		    connection.setSoTimeout(soTimeout);
                connection.open();
                if (connection.isProxied() && connection.isSecure()) {
                    method = new ConnectMethod(method);
                }
            }
 
Problem can be solved by moving the line out of if section

		method.setStrictMode(strictMode);

		connection.setSoTimeout(soTimeout);	
        		        
            if (!connection.isOpen()) {                
                connection.setConnectionTimeout(connectionTimeout);
                connection.open();
                if (connection.isProxied() && connection.isSecure()) {
                    method = new ConnectMethod(method);
                }
            }
</snip>"
1,"JCARepositoryHandle.login(...) methods never throw NoSuchWorkspaceException. Call sequence:
  JCARepositoryHandle.login(Credentials, String)      // (here non-existent workspace is specified for login)
    JCARepositoryHandle.login(JCAConnectionRequestInfo)
      ConnectionManager.allocateConnection(ManagedConnectionFactory, ConnectionRequestInfo)
        ...
          JCAManagedConnection.openSession(JCAConnectionRequestInfo)
            Repository.login(Credentials, String)        // here NoSuchWorkspaceException is thrown, catched by JCAManagedConnection.openSession(JCAConnectionRequestInfo), _set as linkedException_ to ResourceException, which is thrown
        ...
     Here (in JCARepositoryHandle.login(JCAConnectionRequestInfo)) ResourceException is caught, its _cause_ is retreived, and, if cause is NoSuchWorkspaceException, it's thrown, else another exception is thrown.

Note, that when exception occures on lower level, it's wrapped in ResourceException using setLinkedException(), but on upper level it's unwrapped using getCause(). But cause is not set by anyone, it's null, so NoSuchWorkspaceException is never thrown here.

Suggested fix is to use same mechanism on both ends: either change wrapping mechanism to exception chaining (new ResourceException(msg, cause)), or unwrap using ResourceException.getLinkedException()."
1,"KeywordMarkerFilter resets keyword attribute state to false for tokens not in protwords.txt. KeywordMarkerFilter sets true or false for the KeywordAttribute on all tokens. This erases previous state established further up the filter chain, for example in the case where a custom filter wants to prevent a token from being stemmed. 

If a token is already marked as a keyword (KeywordAttribute.isKeyword() == true), perhaps the KeywordMarkerFilterFactory should not re-set the state to false."
1,"setFetchSize() fails in getAllNodeIds(). I get the following exception from the PersistenceManagerIteratorTest on Windows:

org.apache.jackrabbit.core.state.ItemStateException: getAllNodeIds failed.
        at org.apache.jackrabbit.core.persistence.pool.BundleDbPersistenceManager.getAllNodeIds(BundleDbPersistenceManager.java:1043)
        at org.apache.jackrabbit.core.data.PersistenceManagerIteratorTest.testGetAllNodeIds(PersistenceManagerIteratorTest.java:106)
Caused by: java.sql.SQLException: Invalid parameter value '10'000' for Statement.setFetchSize(int rows).
        at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
        at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
        at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
        at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
        at org.apache.derby.impl.jdbc.EmbedConnection.newSQLException(Unknown Source)
        at org.apache.derby.impl.jdbc.ConnectionChild.newSQLException(Unknown Source)
        at org.apache.derby.impl.jdbc.EmbedStatement.setFetchSize(Unknown Source)
        at org.apache.commons.dbcp.DelegatingStatement.setFetchSize(DelegatingStatement.java:279)
        at org.apache.commons.dbcp.DelegatingStatement.setFetchSize(DelegatingStatement.java:279)
        at org.apache.jackrabbit.core.util.db.ConnectionHelper.reallyExec(ConnectionHelper.java:372)
        at org.apache.jackrabbit.core.util.db.ConnectionHelper$3.call(ConnectionHelper.java:353)
        at org.apache.jackrabbit.core.util.db.ConnectionHelper$3.call(ConnectionHelper.java:349)
        at org.apache.jackrabbit.core.util.db.ConnectionHelper$RetryManager.doTry(ConnectionHelper.java:472)
        at org.apache.jackrabbit.core.util.db.ConnectionHelper.exec(ConnectionHelper.java:349)
        at org.apache.jackrabbit.core.persistence.pool.BundleDbPersistenceManager.getAllNodeIds(BundleDbPersistenceManager.java:1020)

It's caused by the following code in ConnectionHelper when 0 < maxRows < 10000:

            stmt.setMaxRows(maxRows);
            stmt.setFetchSize(10000);

A simple fix would be:


            stmt.setMaxRows(maxRows);
            stmt.setFetchSize(Math.min(10000, maxRows));
"
1,"commongrams filter calls incrementToken() after it returns false. In LUCENE-3064, we beefed up MockTokenizer with assertions, and I started cutting over some analysis tests to use MockTokenizer for better coverage.

The commongrams tests fail, because they call incrementToken() after it already returns false. 

In general its my understanding consumers should not do this (and i know of a few tokenizers that will actually throw exceptions if you do this, just like java iterators and such)."
1,"Cluster revision file not closed on repository shutdown.. After having shut down a repository that has configured clustering, the cluster revision file is still open."
1,"More query classes with missing extractTerms(). The query classes DerefQuery, RangeQuery and WildcardQuery do not overwrite the method extractTerms()."
1,"FieldCache keeps hard references to readers, doesn't prevent multiple threads from creating same instance. "
1,"HttpState#matchCredentials is broken. Credentials matching algorithm is flawed, generates unnecessary garbage by
instantiating intermediate object during lookup"
1,"DefaultHighlighter.java does not encode illegal XML characters. When merging excerpts (method protected String mergeFragments(...) in DefaultHighlighter.java), illegal XML characters are not encoded in all places."
1,"AbstractLoginModule#logout() : credentials will not be clared as Subject.getPublicCredentials(Class) isn't backed by the subject internal set. AbstractLoginModule#logout()  clears the credentials list retrieved by calling Subject.getPublicCredentials(Class).
this doesn't have the desired effect as the returned set isn't backed by the subjects internal credentials set. (see API documentation of javax.security.auth.Subject)"
1,"ClassCastExeption when executing union queries. The XPathQueryBuilder throws a ClassCastException on line 322 in release 0.9 when executing syntactically valid union queries. An example query would be ""//element(*, nt:file) or //element(*, mix:lockable)"". It appears that in the invocation of the visit method the SimpleNode id indicates a type of JJTROOTDESCENDANTS at a certain point but the data is actually an OrQueryNode and hence the cast to a PathQueryNode fails."
1,"TrecContentSource should use a fixed encoding, rather than system dependent. TrecContentSource opens InputStreamReader w/o a fixed encoding. On Windows, this means CP1252 (at least on my machine) which is ok. However, when I opened it on a Linux machine w/ a default of UTF-8, it failed to read the files. The patch changes it to use ISO-8859-1, which seems to be the right one (and http://mg4j.dsi.unimi.it/man/manual/ch01s04.html mentions this encoding in its example of a script which reads the data).

Patch to follow shortly."
1,"JCR-Server: IllegalArgumentException when retrieving DateHeader. issue reported by martin perez:

Here is one exception. If I access to any repository through WebDAV using a
web browser (http://localhost:8080/webapp/repository/default the first time
goes well, but if I refresh the page then I get the following exception:

GRAVE: Servlet.service() para servlet Webdav lanz excepcin
java.lang.IllegalArgumentException: mar, 29 nov 2005 22:45:48 CET
    at org.apache.catalina.connector.Request.getDateHeader(Request.java
:1791)
    at org.apache.catalina.connector.RequestFacade.getDateHeader(
RequestFacade.java:630)
    at org.apache.jackrabbit.webdav.WebdavRequestImpl.getDateHeader(
WebdavRequestImpl.java:724)
    at org.apache.jackrabbit.server.AbstractWebdavServlet.spoolResource(
AbstractWebdavServlet.java:363)
    at org.apache.jackrabbit.server.AbstractWebdavServlet.doGet(
AbstractWebdavServlet.java:344)
    at org.apache.jackrabbit.j2ee.SimpleWebdavServlet.execute(
SimpleWebdavServlet.java:191)
    at org.apache.jackrabbit.server.AbstractWebdavServlet.service(
AbstractWebdavServlet.java:170)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(
ApplicationFilterChain.java:252)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(
ApplicationFilterChain.java:173)
    at org.apache.catalina.core.StandardWrapperValve.invoke(
StandardWrapperValve.java:213)
    at org.apache.catalina.core.StandardContextValve.invoke(
StandardContextValve.java:178)
    at org.apache.catalina.core.StandardHostValve.invoke(
StandardHostValve.java:126)
    at org.apache.catalina.valves.ErrorReportValve.invoke(
ErrorReportValve.java:105)
    at org.apache.catalina.core.StandardEngineValve.invoke(
StandardEngineValve.java:107)
    at org.apache.catalina.connector.CoyoteAdapter.service(
CoyoteAdapter.java:148)
    at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java
:868)
    at
org.apache.coyote.http11.Http11BaseProtocol$Http11ConnectionHandler.processConnection
(Http11BaseProtocol.java:663)
    at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(
PoolTcpEndpoint.java:527)
    at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt(
LeaderFollowerWorkerThread.java:80)
    at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(
ThreadPool.java:684)
    at java.lang.Thread.run(Thread.java:595)"
1,custom sort broken if IS uses executorservice. 
1,"Test failures in jcr-rmi and jcr2dav. Integration testing currently fails for jcr-rmi:
  testCloneNodesAccessDenied(org.apache.jackrabbit.test.api.WorkspaceCloneTest)
  testCopyNodesAccessDenied(org.apache.jackrabbit.test.api.WorkspaceCopyBetweenWorkspacesTest)
  testCopyNodesAccessDenied(org.apache.jackrabbit.test.api.WorkspaceCopyTest)
  testMoveNodesAccessDenied(org.apache.jackrabbit.test.api.WorkspaceMoveTest)
  testImpersonate(org.apache.jackrabbit.test.api.ImpersonateTest)
  testCheckPermission(org.apache.jackrabbit.test.api.CheckPermissionTest)
  testRemoveItem4(org.apache.jackrabbit.test.api.SessionRemoveItemTest)
  testReadOnlyPermission(org.apache.jackrabbit.test.api.HasPermissionTest)
  testGetPrivileges(org.apache.jackrabbit.test.api.security.RSessionAccessControlDiscoveryTest)
  testNotHasPrivileges(org.apache.jackrabbit.test.api.security.RSessionAccessControlDiscoveryTest)
  testGetApplicablePolicies(org.apache.jackrabbit.test.api.security.RSessionAccessControlPolicyTest)
  testGetPolicy(org.apache.jackrabbit.test.api.security.RSessionAccessControlPolicyTest)
  testGetEffectivePolicy(org.apache.jackrabbit.test.api.security.RSessionAccessControlPolicyTest)
  testMoveNode(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testSetProperty(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testSetValue(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testDeleteNode(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testDeleteProperty(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testWorkspaceMoveNode(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testCopyNode(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)

and jcr2dav:
  testCloneNodesAccessDenied(org.apache.jackrabbit.test.api.WorkspaceCloneTest)
  testCopyNodesAccessDenied(org.apache.jackrabbit.test.api.WorkspaceCopyBetweenWorkspacesTest)
  testCopyNodesAccessDenied(org.apache.jackrabbit.test.api.WorkspaceCopyTest)
  testMoveNodesAccessDenied(org.apache.jackrabbit.test.api.WorkspaceMoveTest)
  testCheckPermission(org.apache.jackrabbit.test.api.CheckPermissionTest)
  testRemoveItem4(org.apache.jackrabbit.test.api.SessionRemoveItemTest)
  testReadOnlyPermission(org.apache.jackrabbit.test.api.HasPermissionTest)
  testMoveNode(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testSetProperty(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testSetValue(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testDeleteNode(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testDeleteProperty(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testWorkspaceMoveNode(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testCopyNode(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
"
1,jcr-server: NPE in SearchResourceImpl if PathValue path is null. line 368 in SearchResourceImpl doesn't deal with null path thus causing NPE in the valuefactory. 
1,"ItemStateMap warnings during node type changes. As reported already in JCR-1105, the ItemStateMap logs warnings when a cached item state is being overwritten. This shouldn't normally happen, but it turns out that virtual item state providers do this when the root of the virtual tree is modified. Probably the most common such situation is when node types are being modified. This case is luckily not troublesome for the virtual tree functionality, but the logged warnings are annoying.

Here's a relevant part of a stack trace where this problem occurs:

        at org.apache.jackrabbit.core.state.ItemStateMap.put(ItemStateMap.java:72)
        at org.apache.jackrabbit.core.virtual.AbstractVISProvider.cache(AbstractVISProvider.java:324)
        at org.apache.jackrabbit.core.virtual.AbstractVISProvider.createNodeState(AbstractVISProvider.java:284)
        at org.apache.jackrabbit.core.nodetype.virtual.VirtualNodeTypeStateProvider.createNodeTypeState(VirtualNodeTypeStateProvider.java:157)
        at org.apache.jackrabbit.core.nodetype.virtual.VirtualNodeTypeStateProvider.createRootNodeState(VirtualNodeTypeStateProvider.java:80)
        at org.apache.jackrabbit.core.virtual.AbstractVISProvider.stateDiscarded(AbstractVISProvider.java:470)
        at org.apache.jackrabbit.core.state.ItemState.notifyStateDiscarded(ItemState.java:226)
        at org.apache.jackrabbit.core.state.ItemState.discard(ItemState.java:370)
        at org.apache.jackrabbit.core.nodetype.virtual.VirtualNodeTypeStateProvider.onNodeTypesRemoved(VirtualNodeTypeStateProvider.java:139)
        at org.apache.jackrabbit.core.nodetype.virtual.VirtualNodeTypeStateManager.nodeTypesUnregistered(VirtualNodeTypeStateManager.java:199)
        at org.apache.jackrabbit.core.nodetype.virtual.VirtualNodeTypeStateManager.nodeTypeReRegistered(VirtualNodeTypeStateManager.java:174)
        at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.notifyReRegistered(NodeTypeRegistry.java:1821)
        at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.reregisterNodeType(NodeTypeRegistry.java:433)
        at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.reregisterNodeType(NodeTypeRegistry.java:364)
        at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.registerNodeTypes(NodeTypeManagerImpl.java:591)
        at org.apache.jackrabbit.commons.cnd.CndImporter.registerNodeTypes(CndImporter.java:118)"
1,"Offset not working correctly in user/group query when restricting to group members. For user/group queries having a scope *and* a  limit clause offsetting does not work correctly.

    builder.setScope(""contributors"", false);
    builder.setLimit(100, 50);

In the above case, the result is often not offset at 100 but instead at some place >100.
"
1,"Saving concurrent sessions executing random operations causes a corrupt JCR. Run the attached unit test. Several concurrent sessions add, move, and remove nodes. Then the index is removed and the repository is again started. The repository is in an inconsistent state and the index cannot be rebuild. Also a lot of exceptions occur. See (see Output before patch.txt). Note that the unit test also suffers from the deadlock of issue http://issues.apache.org/jira/browse/JCR-2525 about half the time."
1,Exception not caugh in DefaultResponseParser. The method hasProtocolVersion in o.a.h.message.BaseLineParser (httpcore-alpha6) throws an IndexOutOfBoundsException which is not caught by the parseHead method in the o.a.h.impl.conn.DefaultResponseParser.
1,"Indexing configuration not refreshed after node type registration. The indexing configuration has internal caches that speed up node type matches. Those caches are not updated on new node type registration and newly registered node types are not properly resolved when index-rules are checked.

See also test case in attached patch."
1,"NPE in ItemManager when calling Session.save() with nothing to save. I'm getting an NPE on the id.denoteNodes() call below, in ItemManager:

    private ItemData retrieveItem(ItemId id) {
        synchronized (itemCache) {
            ItemData data = itemCache.get(id);
            if (data == null && id.denotesNode()) {
...

because the id is null after taking the second branch of this if in SessionSaveOperation.perform:

        if (context.getSessionImpl().hasPermission(""/"", Session.ACTION_READ)) {
            id = context.getRootNodeId();
        } else {
            id = context.getItemStateManager().getIdOfRootTransientNodeState();
        }

context.toString() says:

session-author-3623:
ItemManager (org.apache.jackrabbit.core.ItemManager@1e911ccc)
Items in cache:

SessionItemStateManager (org.apache.jackrabbit.core.state.SessionItemStateManager@15472b43)
[transient]
{}[attic]
{}

which I assume means there's nothing to save.

The correct behavior is probably to do nothing in perform to avoid the NPE."
1,"FixedIntBlockIndexInput.Reader does not initialise 'pending' int array. The FixedIntBlockIndexInput.Reader.pending int array is not initialised. As a consequence, the FixedIntBlockIndexInput.Reader#next() method returns always 0.

A call to FixedIntBlockIndexInput.Reader#blockReader.readBlock() during the Reader initialisation may solve the issue (to be tested)."
1,"Registering NodeType with defaultvalues fails with IndexOutOfBounds. When trying to register more than one nodetpye with default values I get the following exception:

Caused by: java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.toNodeTypeDef(NodeTypeManagerImpl.java:790)
	at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.registerNodeTypes(NodeTypeManagerImpl.java:560)

I assume there is an index missmatch in the implementation

                Value[] values = pdefs[i].getDefaultValues();
                if (values != null) {
                    InternalValue[] qvalues = new InternalValue[values.length];
                    for (int j = 0; j < values.length; j++) {
                        try {
-->                            qvalues[j] = InternalValue.create(values[i], session);
                        } catch (ValueFormatException e) {
                            throw new InvalidNodeTypeDefinitionException(
                                    ""Invalid default value format"", e);
                        }
                    }
                    qpdef.setDefaultValues(qvalues);
                }
"
1,"WebDAV method invocation trying to create a new resource should fail with 409 (Conflict) if parent resource does not exist. This is Litmus test case copy_nodestcoll. An attempt is made to COPY an existing resource to a new location, where the parent collection of the resource-to-be-created does not exist. RFC2518 asks for status code 409 (Conflict) instead of 403 (Forbidden) in this case.
"
1,"Version history recovery fails in case a version does not have a jcr:frozenNode. With JCR-2551 in place, a version recovery mode has been introduced. Problem now is that in case a version is encountered that misses a mandatory jcr:frozenNode, an InternalError is thrown by o.a.j.c.version.InternalVersionHistoryImpl#createVersionInstance. Since o.a.j.c.RepositoryChecker#checkVersionHistory only catches Exception, it fails to catch it properly which leads to a complete repository shutdown.

Throwing for example a RuntimeException instead fixes the problem."
1,"Range Query works only with lower case terms. I am performing a range query that returns results if the terms are lower 
case, but does not return result when the terms are mixed case.

In my collection, I have terms alpha, beta, delta, gamma.  I am using the 
StandardAnalyzer for both indexing and searching.

The query [alpha TO gamma] returns all four terms.  When I perform the query 
[Alpha TO Gamma], no results are returned.

It appears the lowerCaseFilter(), which is a part of the StandardAnalyzer, 
does not work properly on the search terms.  I've used Luke to peek at my 
collection, and the terms are all lower case in the collection.

I'm fairly new to Lucene, so I hope I'm not making a ""common mistake""."
1,"DecompressingEntity not calling close on InputStream retrieved by getContent. The method DecompressingEntity.writeTo(OutputStream outstream) does not close the InputStream retrieved by getContent().
According to the documentation of HttpEntity.writeTo:
IMPORTANT: Please note all entity implementations must ensure that
all allocated resources are properly deallocated when this method
returns.

-> imho this is not satisfied in DecompressingEntity.writeTo "
1,"CharFilters not being invoked in Solr. 
On Solr trunk, *all* CharFilters have been non-functional since LUCENE-3396 was committed in r1175297 on 25 Sept 2011, until Yonik's fix today in r1235810; Solr 3.x was not affected - CharFilters have been working there all along."
1,"norms reading fails with FileNotFound in exceptional case. If we can't get to the bottom of this, we can always add the fileExists check back...

{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestIndexWriterExceptions
    [junit] Testcase: testRandomExceptionsThreads(org.apache.lucene.index.TestIndexWriterExceptions):	Caused an ERROR
    [junit] No sub-file with id _nrm.cfs found (fileName=_19_nrm.cfs files: [.fdt, .fnm, .per, .fdx])
    [junit] java.io.FileNotFoundException: No sub-file with id _nrm.cfs found (fileName=_19_nrm.cfs files: [.fdt, .fnm, .per, .fdx])
    [junit] 	at org.apache.lucene.store.CompoundFileDirectory.createSlicer(CompoundFileDirectory.java:313)
    [junit] 	at org.apache.lucene.store.CompoundFileDirectory.<init>(CompoundFileDirectory.java:65)
    [junit] 	at org.apache.lucene.codecs.lucene40.Lucene40DocValuesProducer.<init>(Lucene40DocValuesProducer.java:48)
    [junit] 	at org.apache.lucene.codecs.lucene40.Lucene40NormsFormat$Lucene40NormsDocValuesProducer.<init>(Lucene40NormsFormat.java:70)
    [junit] 	at org.apache.lucene.codecs.lucene40.Lucene40NormsFormat.docsProducer(Lucene40NormsFormat.java:49)
    [junit] 	at org.apache.lucene.codecs.lucene40.Lucene40NormsFormat.docsProducer(Lucene40NormsFormat.java:62)
    [junit] 	at org.apache.lucene.index.SegmentCoreReaders.<init>(SegmentCoreReaders.java:122)
    [junit] 	at org.apache.lucene.index.SegmentReader.<init>(SegmentReader.java:54)
    [junit] 	at org.apache.lucene.index.DirectoryReader$1.doBody(DirectoryReader.java:65)
    [junit] 	at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:660)
    [junit] 	at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:55)
    [junit] 	at org.apache.lucene.index.IndexReader.open(IndexReader.java:242)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions.testRandomExceptionsThreads(TestIndexWriterExceptions.java:304)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$3$1.evaluate(LuceneTestCase.java:530)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:165)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)
    [junit] 
    [junit] 
    [junit] Tests run: 22, Failures: 0, Errors: 1, Time elapsed: 3.439 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterExceptions -Dtestmethod=testRandomExceptionsThreads -Dtests.seed=-4ea45cb40d17460b:-459bfb455a2351b9:1abd8f0f3a0611b9 -Dargs=""-Dfile.encoding=UTF-8""
    [junit] NOTE: test params are: codec=Lucene40: {field=MockVariableIntBlock(baseBlockSize=31), id=PostingsFormat(name=NestedPulsing), content=Pulsing40(freqCutoff=2 minBlockSize=58 maxBlockSize=186), contents=MockVariableIntBlock(baseBlockSize=31), content1=MockVariableIntBlock(baseBlockSize=31), content2=PostingsFormat(name=MockSep), content4=Pulsing40(freqCutoff=2 minBlockSize=58 maxBlockSize=186), content5=MockFixedIntBlock(blockSize=964), content6=PostingsFormat(name=Memory), content7=PostingsFormat(name=MockRandom), crash=PostingsFormat(name=NestedPulsing), subid=PostingsFormat(name=NestedPulsing)}, sim=RandomSimilarityProvider(queryNorm=false,coord=true): {other=DFR GB3(800.0), contents=IB SPL-L3(800.0), content=DFR GL3(800.0), id=DFR I(F)L1, field=IB LL-DZ(0.3), content1=DFR I(ne)BZ(0.3), content2=DFR I(n)3(800.0), content3=DFR GZ(0.3), content4=DFR I(ne)B2, content5=IB LL-L3(800.0), content6=IB SPL-D2, crash=DFR I(F)3(800.0), content7=DFR I(F)B3(800.0), subid=IB LL-L1}, locale=de_CH, timezone=Canada/Saskatchewan
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestAssertions, TestNumericTokenStream, TestSimpleAttributeImpl, TestImpersonation, TestPulsingReuse, TestDocument, TestAddIndexes, TestAtomicUpdate, TestByteSlices, TestCheckIndex, TestConcurrentMergeScheduler, TestConsistentFieldNumbers, TestCrashCausesCorruptIndex, TestDocCount, TestDocumentWriter, TestFlex, TestForceMergeForever, TestIndexInput, TestIndexReader, TestIndexWriterConfig, TestIndexWriterExceptions]
    [junit] NOTE: Linux 3.0.0-14-generic amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=8,threads=1,free=186661872,total=245104640
{noformat}
"
1,"TestIndexWriterOnDiskFull.testAddDocumentOnDiskFull seed failure. version: trunk r1155278
reproduce-able: always

{code}
    [junit] Testsuite: org.apache.lucene.index.TestIndexWriterOnDiskFull
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.847 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterOnDiskFull -Dtestmethod=testAddDocumentOnDiskFull -Dtests.seed=-3cc23002ebad518d:70ae722281b31c9f:57406021f8789a22
    [junit] NOTE: test params are: codec=RandomCodecProvider: {content=MockFixedIntBlock(blockSize=1081)}, locale=hr_HR, timezone=Atlantic/Jan_Mayen
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestIndexWriterOnDiskFull]
    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_26 (64-bit)/cpus=8,threads=1,free=85252968,total=125632512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testAddDocumentOnDiskFull(org.apache.lucene.index.TestIndexWriterOnDiskFull):     Caused an ERROR
    [junit] no segments* file found in MockDirWrapper(org.apache.lucene.store.RAMDirectory@65dcc2a3 lockFactory=MockLockFactoryWrapper(org.apache.lucene.store.SingleInstanceLockFactory@6e8f94)): files: [_1.cfs, _1.cfe, _0.cfe, _0.cfs]
    [junit] org.apache.lucene.index.IndexNotFoundException: no segments* file found in MockDirWrapper(org.apache.lucene.store.RAMDirectory@65dcc2a3 lockFactory=MockLockFactoryWrapper(org.apache.lucene.store.SingleInstanceLockFactory@6e8f94)): files: [_1.cfs, _1.cfe, _0.cfe, _0.cfs]
    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:657)
    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:534)
    [junit]     at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:284)
    [junit]     at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:277)
    [junit]     at org.apache.lucene.index.TestIndexWriter.assertNoUnreferencedFiles(TestIndexWriter.java:158)
    [junit]     at org.apache.lucene.index.TestIndexWriterOnDiskFull.testAddDocumentOnDiskFull(TestIndexWriterOnDiskFull.java:114)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1526)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1428)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.index.TestIndexWriterOnDiskFull FAILED
{code}

"
1,"FieldsReader does not regard offset and position flags. When creating a Field the FieldsReader looks at the storeTermVector flag of the FieldInfo. If true Field.TermVector.YES is used as parameter. But it should be checked if storeOffsetWithTermVector and storePositionWithTermVector are set and Field.TermVector.WITH_OFFSETS, ...WITH_POSITIONS, or ...WITH_POSITIONS_OFFSETS should be used as appropriate."
1,Deleted documents are visible across reopened MSRs. 
1,"SQL2 query - supplying column selector fails with NPE on getColumnName(). I am preparing and executing an SQL2 query (JCR 2.0) as follows:

QueryManager qm = jcrSession.getWorkspace().getQueryManager();
String queryString = ""select order.[customerAccountUUID] as cust from [atl:order] as order"";
Query query = qm.createQuery(queryString, Query.JCR_SQL2);
QueryResult queryResult = query.execute();

The following query fails:

select order.[customerAccountUUID] from [atl:order] as order

java.lang.NullPointerException
        org.apache.jackrabbit.commons.query.sql2.QOMFormatter.isSimpleName(QOMFormatter.java:577)
        org.apache.jackrabbit.commons.query.sql2.QOMFormatter.formatName(QOMFormatter.java:567)
        org.apache.jackrabbit.commons.query.sql2.QOMFormatter.format(QOMFormatter.java:452)
        org.apache.jackrabbit.commons.query.sql2.QOMFormatter.format(QOMFormatter.java:123)
        org.apache.jackrabbit.commons.query.sql2.QOMFormatter.format(QOMFormatter.java:117)

line 452: c.getColumnName() returns null.

The following query is fine:

select order.[customerAccountUUID] as cust from [atl:order] as order

I have been using the test case (here: http://svn.apache.org/viewvc/jackrabbit/trunk/jackrabbit-spi-commons/src/test/resources/org/apache/jackrabbit/spi/commons/query/sql2/test.sql2.txt?view=markup) as a guideline.

Cheers,

James "
1,"In XA transaction session.addLockToken() does not have effect. Following sequence does not work as expected:
1. first tx (and first session)
  create node
  make it lockable
2. second tx (and second session)
  lock this node and save lock token
3. third tx (and third session)
  add saved lock token to session
  modify this locked node -> fails as if lock token was not added to session3

The same sequence works as expected without transactions.
I had to separate transactions 1 and 2 because JCR-1633 prevents node from being locked in same tx in which it was created."
1,"FilterIndexReader doesn't work correctly with post-flex SegmentMerger. IndexWriter.addIndexes(IndexReader...) internally uses SegmentMerger to add data from input index readers. However, SegmentMerger uses the new post-flex API to do this, which bypasses the pre-flex TermEnum/TermPositions API that FilterIndexReader implements. As a result, filtering is not applied."
1,"SQL parser chokes on prefixes containing a ""-"" character. SQL parser chokes on prefixes containing a ""-"" character, such as in

  SELECT a-b:c FROM nt:resource

"
1,"Deadlock caused by versioning operations within transaction. Deadlock occurs, while running a very simple test, which is just trying
to checkout/checkin node within transaction concurrently from 2 threads.

Find enclosed thread dump, log and simple Java program.
I'm using UserTransaction implementation from jackrabbit test suite.

Regards
Przemo Pakulski
www.cognifide.com


Full thread dump Java HotSpot(TM) Client VM (1.4.2_08-b03 mixed mode):

""Thread-5"" prio=5 tid=0x03054c48 nid=0x180c in Object.wait() [355f000..355fd8c]
       at java.lang.Object.wait(Native Method)
       - waiting on <0x1148ef20> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock)
       at java.lang.Object.wait(Object.java:429)
       at EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock.acquire(Unknown Source)
       - locked <0x1148ef20> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock)
       at org.apache.jackrabbit.core.state.SharedItemStateManager.acquireWriteLock(SharedItemStateManager.java:1137)
       at org.apache.jackrabbit.core.state.SharedItemStateManager.access$200(SharedItemStateManager.java:110)
       at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:456)
       at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:651)
       at org.apache.jackrabbit.core.state.XAItemStateManager.prepare(XAItemStateManager.java:150)
       at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:128)
       - locked <0x11565ac8> (a org.apache.jackrabbit.core.TransactionContext)
       at org.apache.jackrabbit.core.XASessionImpl.prepare(XASessionImpl.java:300)
       at com.oyster.mom.contentserver.jcr.transaction.JackrabbitUserTransaction.commit(JackrabbitUserTransaction.java:102)
       at com.oyster.mom.contentserver.jcr.transaction.JrTestDeadlock.run(JrTestDeadlock.java:97)

""Thread-4"" prio=5 tid=0x0303b348 nid=0x9d0 in Object.wait() [351f000..351fd8c]
       at java.lang.Object.wait(Native Method)
       - waiting on <0x1148ef20> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock)
       at java.lang.Object.wait(Object.java:429)
       at EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock.acquire(Unknown Source)
       - locked <0x1148ef20> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock)
       at org.apache.jackrabbit.core.state.SharedItemStateManager.acquireWriteLock(SharedItemStateManager.java:1137)
       at org.apache.jackrabbit.core.state.SharedItemStateManager.access$200(SharedItemStateManager.java:110)
       at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:456)
       at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:651)
       at org.apache.jackrabbit.core.state.XAItemStateManager.prepare(XAItemStateManager.java:150)
       at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:128)
       - locked <0x1156f558> (a org.apache.jackrabbit.core.TransactionContext)
       at org.apache.jackrabbit.core.XASessionImpl.prepare(XASessionImpl.java:300)
       at com.oyster.mom.contentserver.jcr.transaction.JackrabbitUserTransaction.commit(JackrabbitUserTransaction.java:102)
       at com.oyster.mom.contentserver.jcr.transaction.JrTestDeadlock.run(JrTestDeadlock.java:97)

""IndexMerger"" daemon prio=5 tid=0x030388b8 nid=0x1858 in Object.wait() [34df000..34dfd8c]
       at java.lang.Object.wait(Native Method)
       - waiting on <0x114fd280> (a org.apache.commons.collections.buffer.BlockingBuffer)
       at java.lang.Object.wait(Object.java:429)
       at org.apache.commons.collections.buffer.BlockingBuffer.remove(BlockingBuffer.java:107)
       - locked <0x114fd280> (a org.apache.commons.collections.buffer.BlockingBuffer)
       at org.apache.jackrabbit.core.query.lucene.IndexMerger.run(IndexMerger.java:235)

""Thread-2"" daemon prio=5 tid=0x0303a230 nid=0xe4c in Object.wait() [349f000..349fd8c]
       at java.lang.Object.wait(Native Method)
       - waiting on <0x114fd2e0> (a java.util.TaskQueue)
       at java.util.TimerThread.mainLoop(Timer.java:429)
       - locked <0x114fd2e0> (a java.util.TaskQueue)
       at java.util.TimerThread.run(Timer.java:382)

""Thread-1"" daemon prio=5 tid=0x0301b7a0 nid=0x1a00 in Object.wait() [345f000..345fd8c]
       at java.lang.Object.wait(Native Method)
       - waiting on <0x114f9058> (a java.util.TaskQueue)
       at java.lang.Object.wait(Object.java:429)
       at java.util.TimerThread.mainLoop(Timer.java:403)
       - locked <0x114f9058> (a java.util.TaskQueue)
       at java.util.TimerThread.run(Timer.java:382)

""ObservationManager"" daemon prio=5 tid=0x02ef6c50 nid=0x10d8 in Object.wait() [341f000..341fd8c]
       at java.lang.Object.wait(Native Method)
       - waiting on <0x114f38e0> (a org.apache.commons.collections.buffer.BlockingBuffer)
       at java.lang.Object.wait(Object.java:429)
       at org.apache.commons.collections.buffer.BlockingBuffer.remove(BlockingBuffer.java:107)
       - locked <0x114f38e0> (a org.apache.commons.collections.buffer.BlockingBuffer)
       at org.apache.jackrabbit.core.observation.ObservationManagerFactory.run(ObservationManagerFactory.java:155)
       at java.lang.Thread.run(Thread.java:534)

""Signal Dispatcher"" daemon prio=10 tid=0x00a05590 nid=0x1914 waiting on condition [0..0]

""Finalizer"" daemon prio=9 tid=0x00a027f8 nid=0x17a4 in Object.wait() [2c9f000..2c9fd8c]
       at java.lang.Object.wait(Native Method)
       - waiting on <0x113db118> (a java.lang.ref.ReferenceQueue$Lock)
       at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:111)
       - locked <0x113db118> (a java.lang.ref.ReferenceQueue$Lock)
       at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:127)
       at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:159)

""Reference Handler"" daemon prio=10 tid=0x00a01478 nid=0x16d4 in Object.wait() [2c5f000..2c5fd8c]
       at java.lang.Object.wait(Native Method)
       - waiting on <0x113db180> (a java.lang.ref.Reference$Lock)
       at java.lang.Object.wait(Object.java:429)
       at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:115)
       - locked <0x113db180> (a java.lang.ref.Reference$Lock)

""main"" prio=5 tid=0x0003e6f0 nid=0x1470 in Object.wait() [7f000..7fc38]
       at java.lang.Object.wait(Native Method)
       - waiting on <0x11524f10> (a com.oyster.mom.contentserver.jcr.transaction.JrTestDeadlock)
       at java.lang.Thread.join(Thread.java:1001)
       - locked <0x11524f10> (a com.oyster.mom.contentserver.jcr.transaction.JrTestDeadlock)
       at java.lang.Thread.join(Thread.java:1054)
       at com.oyster.mom.contentserver.jcr.transaction.JrTestDeadlock.main(JrTestDeadlock.java:33)

""VM Thread"" prio=5 tid=0x00a42730 nid=0x17d0 runnable

""VM Periodic Task Thread"" prio=10 tid=0x00a45540 nid=0x1928 waiting on condition
""Suspend Checker Thread"" prio=10 tid=0x00a04af8 nid=0x17ac runnable

import javax.jcr.Node;
import javax.jcr.RepositoryException;
import javax.jcr.Session;
import javax.jcr.SimpleCredentials;

import org.apache.jackrabbit.core.RepositoryImpl;
import org.apache.jackrabbit.core.config.RepositoryConfig;

public class JrTestDeadlock extends Thread {

   private static final org.apache.commons.logging.Log log = org.apache.commons.logging.LogFactory.getLog(JrTestDeadlock.class);

   public static String REPOSITORY_HOME = ""d:/repo/jackrabbit/"";

   public static String REPOSITORY_CONFIG = REPOSITORY_HOME + ""repository.xml"";

   public static void main(String[] args) throws Exception {

       JrTestDeadlock test = new JrTestDeadlock(-1);
       test.startup();

       JrTestDeadlock tests[] = new JrTestDeadlock[2];

       for (int i = 0; i < tests.length; i++) {
           JrTestDeadlock x = new JrTestDeadlock(i);
           x.start();
           tests[i] = x;
       }

       for (int i = 0; i < tests.length; i++) {
           tests[i].join();
       }

       test.shutdown();
   }

   private static RepositoryImpl repository;

   private int id;

   public JrTestDeadlock(int i) {
       this.id = i;
   }

   public void startup() throws Exception {
       System.setProperty(""java.security.auth.login.config"", ""c:/jaas.config"");

       RepositoryConfig config = RepositoryConfig.create(REPOSITORY_CONFIG, REPOSITORY_HOME);
       repository = RepositoryImpl.create(config);

       Session session = repository.login(new SimpleCredentials(""admin"", ""admin"".toCharArray()));
       Node rootNode = session.getRootNode();
       if (!rootNode.hasNode(""folder"")) {
           Node folder = rootNode.addNode(""folder"");
           folder.addMixin(""mix:versionable"");
           folder.addMixin(""mix:lockable"");
           rootNode.save();
       }
       session.logout();
   }

   public void shutdown() throws RepositoryException {
       repository.shutdown();
   }

   public Node getFolder(Session session) throws RepositoryException {
       return session.getRootNode().getNode(""folder"");
   }

   public void run() {
       try {
           Session session = repository.login(new SimpleCredentials(""admin"", ""admin"".toCharArray()));
           for (int i = 0; i < 100; i++) {
               log.error(""START id:"" + id + "", i="" + i);

               boolean success = false;

               JackrabbitUserTransaction ut = new JackrabbitUserTransaction(session);
               try {
                   ut.begin();

                   Node folder = getFolder(session);
                   folder.checkout();
                   folder.checkin();

                   success = true;
                   log.info(""SUCCESS id:"" + id + "", i="" + i);
               }
               catch (Exception e) {
                   log.warn(""FAIL:"" + id + "", i="" + i, e);
               }
               finally {
                   try {
                       if (success) {
                           ut.commit();
                       }
                       else {
                           ut.rollback();
                       }
                   }
                   catch (Exception e) {
                       log.fatal(e);
                   }
               }
           }
           session.logout();
       }
       catch (RepositoryException e) {
           e.printStackTrace();
       }
   }
}


13:46 ERROR JrTestDeadlock.run(JrTestDeadlock.java:76) - START id:0, i=0
13:46 ERROR JrTestDeadlock.run(JrTestDeadlock.java:76) - START id:1, i=0
13:46 INFO  JrTestDeadlock.run(JrTestDeadlock.java:89) - SUCCESS id:0, i=0
13:46 INFO  JrTestDeadlock.run(JrTestDeadlock.java:89) - SUCCESS id:1, i=0
13:46 ERROR org.apache.jackrabbit.core.state.XAItemStateManager.prepare(XAItemStateManager.java:156) - org.apache.jackrabbit.core.state.StaleItemStateException: 233e656f-79f8-414d-9e37-3fce865b492d/{http://www.jcp.org/jcr/1.0}isCheckedOut has been modified externally
13:46 FATAL JrTestDeadlock.run(JrTestDeadlock.java:104) - javax.transaction.RollbackException: Transaction rolled back: XA_ERR=104
13:46 ERROR JrTestDeadlock.run(JrTestDeadlock.java:76) - START id:1, i=1
13:46 WARN  JrTestDeadlock.run(JrTestDeadlock.java:92) - FAIL:1, i=1
ax.jcr.InvalidItemStateException: f83a830b-abbf-4ab2-8625-b9e2c4802316: the item does not exist anymore
    at org.apache.jackrabbit.core.version.XAVersion.sanityCheck(XAVersion.java:81)
    at org.apache.jackrabbit.core.version.XAVersion.getInternalVersion(XAVersion.java:70)
    at org.apache.jackrabbit.core.version.AbstractVersion.getUUID(AbstractVersion.java:107)
    at org.apache.jackrabbit.core.NodeImpl.checkout(NodeImpl.java:2759)
    at JrTestDeadlock.run(JrTestDeadlock.java:85)
13:46 ERROR JrTestDeadlock.run(JrTestDeadlock.java:76) - START id:1, i=2
13:46 INFO  JrTestDeadlock.run(JrTestDeadlock.java:89) - SUCCESS id:1, i=2
13:51 WARN  org.apache.jackrabbit.core.TransactionContext.run(TransactionContext.java:239) - Transaction rolled back because timeout expired.

"
1,"Bundle Persistence Manager error - failing to read bundle the first time. Code:
NodeIterator entiter = null;
Node root = null, contNode = null, entsNode = null;

try
{
    root = session.getRootNode();
    contNode = root.getNode(""sr:cont"");
    entsNode = contNode.getNode(""sr:ents"");
    entiter = entsNode.getNodes();
}
catch (Exception e)
{
    logger.error(""Getting ents nodes"", e);
}

Output:
12359 [http-8080-Processor24] ERROR org.apache.jackrabbit.core.persistence.bundle.BundleDbPersistenceManager - failed to read bundle: c3a09c19-cc6b-45bd-a42e-c4c925b67d02: java.io.IOException: ERROR 40XD0: Container has been closed
12375 [http-8080-Processor24] ERROR com.taxila.editor.sm.RepoOperations - Getting ents nodes
javax.jcr.PathNotFoundException: sr:ents
    at org.apache.jackrabbit.core.NodeImpl.getNode(NodeImpl.java:2435)
    at com.taxila.editor.sm.RepoOperations.getEntityNodes (RepoOperations.java:4583)
    at com.taxila.editor.sm.RepoOperations.displayEntities(RepoOperations.java:4159)
    at com.taxila.editor.sm.RepoOperations.displayEntities(RepoOperations.java:4114)
    at com.taxila.editor.em.um.MainEntityForm.reset (MainEntityForm.java:215)
    at org.apache.struts.taglib.html.FormTag.doStartTag(FormTag.java:640)
    at org.apache.jsp.pages.jsp.entity.MainEntity_jsp._jspService(MainEntity_jsp.java:414)
    at org.apache.jasper.runtime.HttpJspBase.service (HttpJspBase.java:97)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
    at org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:332)
    at org.apache.jasper.servlet.JspServlet.serviceJspFile (JspServlet.java:314)
    at org.apache.jasper.servlet.JspServlet.service(JspServlet.java:264)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter (ApplicationFilterChain.java:252)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:173)
    at org.apache.catalina.core.ApplicationDispatcher.invoke(ApplicationDispatcher.java :672)
    at org.apache.catalina.core.ApplicationDispatcher.processRequest(ApplicationDispatcher.java:463)
    at org.apache.catalina.core.ApplicationDispatcher.doForward(ApplicationDispatcher.java:398)
    at org.apache.catalina.core.ApplicationDispatcher.forward (ApplicationDispatcher.java:301)
    at org.apache.struts.action.RequestProcessor.doForward(RequestProcessor.java:1014)
    at org.apache.struts.action.RequestProcessor.processForwardConfig(RequestProcessor.java:417)
    at org.apache.struts.action.RequestProcessor.processActionForward(RequestProcessor.java:390)
    at org.apache.struts.action.RequestProcessor.process(RequestProcessor.java:271)
    at org.apache.struts.action.ActionServlet.process (ActionServlet.java:1292)
    at org.apache.struts.action.ActionServlet.doPost(ActionServlet.java:510)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:709)
    at javax.servlet.http.HttpServlet.service (HttpServlet.java:802)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:252)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java :173)
    at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:213)
    at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:178)
    at org.apache.catalina.core.StandardHostValve.invoke (StandardHostValve.java:126)
    at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:105)
    at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:107)
    at org.apache.catalina.connector.CoyoteAdapter.service (CoyoteAdapter.java:148)
    at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:869)
    at org.apache.coyote.http11.Http11BaseProtocol$Http11ConnectionHandler.processConnection(Http11BaseProtocol.java :664)
    at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(PoolTcpEndpoint.java:527)
    at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt(LeaderFollowerWorkerThread.java:80)
    at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run (ThreadPool.java:684)
    at java.lang.Thread.run(Unknown Source)

On the other hand if I do this:
Code:
try
{
    root = session.getRootNode ();
    contNode = root.getNode(""sr:cont"");
    entsNode = contNode.getNode(""sr:ents"");
    entiter = entsNode.getNodes();
}
catch (Exception e)
{
    logger.error(""Getting ents nodes"", e);
    try
    {
        entsNode = contNode.getNode(""sr:ents"");
        entiter = entsNode.getNodes();
    }
    catch (Exception e1)
    {
        e1.printStackTrace();
    }
}

Output:
The first error as in the previous case comes, but the second execution of the entsNode = contNode.getNode(""sr:ents""); statement returns the right node, and hence the iterator."
1,"Trouble undeploying jackrabbit-webapp from Tomcat. When testing jackrabbit-webapp for the 1.4 release, I again came across this issue that I've occasionally seen also before, but never qualified enough for a bug report.

The Jackrabbit webapp would deploy without problems, but when I undeploy the webapp Tomcat fails to remove the Derby jar in WEB-INF/lib (I have unpackWARs enabled). This causes problems especially when I have autoDeploy enabled, as Tomcat then deploys the skeleton webapp right after undeployment, and the only way to really get rid of the webapp is to shutdown Tomcat and to manually remove the webapp on the file system.

I suspect that this problem is related to Derby jar being somehow referenced even after the webapp is undeployed, causing Windows to prevent the jar file from being removed.

Unless someone has some bright idea on how to resolve this, I'll consider this a known issue in Jackrabbit 1.4."
1,"ConcurrentModificationException in SessionItemStateManager.getIdOfRootTransientNodeState(). SessionItemStateManager.getIdOfRootTransientNodeState() is throwing a ConcurrentModificationException on line 607:

Here's a snippet of the code:
{code}
                    for (NodeId id : candidateIds) {
                        if (nodeId.equals(id) || hierMgr.isAncestor(id, nodeId)) {
                            // already a candidate or a descendant thereof
                            // => skip
                            skip = true;
                            break;
                        }
                        if (hierMgr.isAncestor(nodeId, id)) {
                            // candidate is a descendant => remove
                            candidateIds.remove(id);
                        }
                    }
{code}

Can't use Collection.remove(Object) in the middle of iterating. It should probably be changed to use Iterator.remove():
{code}
                    Iterator<NodeId> nodeIdItor = candidateIds.iterator();
                    while (nodeIdItor.hasNext()) {
                        NodeId id = nodeIdItor.next();
                        if (nodeId.equals(id) || hierMgr.isAncestor(id, nodeId)) {
                            // already a candidate or a descendant thereof
                            // => skip
                            skip = true;
                            break;
                        }
                        if (hierMgr.isAncestor(nodeId, id)) {
                            // candidate is a descendant => remove
                            nodeIdItor.remove();
                        }
                    }
{code}

Any idea what I could do differently to workaround the issue?"
1,"importXML prepending line feeds to tag values. Importing using Session.importXML(...) results in new line characters being inserted at the beginning of tag
values:

<?xml version=""1.0"" encoding=""UTF-8""?>
<Policy xmlns=""urn:oasis:names:tc:xacml:1.0:policy""  xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" PolicyId=""test:policy-one"" RuleCombiningAlgId=""urn:oasis:names:tc:xacml:1.0:rule-combining-algorithm:ordered-permit-overrides"">
  <Description>policy-description</Description>
  <Target>
...

Becomes

/test/policies/Policy/jcr:primaryType=nt:unstructured
/test/policies/Policy/PolicyId=test:policy-one
/test/policies/Policy/RuleCombiningAlgId=urn:oasis:names:tc:xacml:1.0:rule-combining-algorithm:ordered-permit-overrides
/test/policies/Policy/Description/jcr:primaryType=nt:unstructured
/test/policies/Policy/Description/jcr:xmltext/jcr:primaryType=nt:unstructured
/test/policies/Policy/Description/jcr:xmltext/jcr:xmlcharacters=
policy-description
/test/policies/Policy/Target/jcr:primaryType=nt:unstructured

(in other cases, many LFs are inserted)

FULL EXAMPLE XML FILE:

<?xml version=""1.0"" encoding=""UTF-8""?>
<Policy xmlns=""urn:oasis:names:tc:xacml:1.0:policy"" 
  xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" 
  PolicyId=""test:policy-one"" 
  RuleCombiningAlgId=""urn:oasis:names:tc:xacml:1.0:rule-combining-algorithm:ordered-permit-overrides"">
  <Description>policy-description</Description>
  <Target>
    <Resources>
      <Resource>
        <ResourceMatch 
          MatchId=""urn:oasis:names:tc:xacml:1.0:function:string-equal"">
          <AttributeValue DataType=""http://www.w3.org/2001/XMLSchema#string"">test/12345-resource-67890</AttributeValue>
          <ResourceAttributeDesignator 
            DataType=""http://www.w3.org/2001/XMLSchema#string"" 
            AttributeId=""urn:oasis:names:tc:xacml:1.0:resource:resource-id""/>
        </ResourceMatch>
      </Resource>
    </Resources>
    <Actions>
      <AnyAction/>
    </Actions>
  </Target>
  <Rule RuleId=""PermitRule"" Effect=""Permit"">
    <Target>
      <Subjects>
        <Subject>
          <SubjectMatch 
            MatchId=""urn:oasis:names:tc:xacml:1.0:function:string-equal"">
            <AttributeValue DataType=""http://www.w3.org/2001/XMLSchema#string"">alice</AttributeValue>
            <SubjectAttributeDesignator 
              DataType=""http://www.w3.org/2001/XMLSchema#string"" 
              AttributeId=""urn:oasis:names:tc:xacml:1.0:subject:subject-id""/>
          </SubjectMatch>
        </Subject>
      </Subjects>
      <Actions>
        <Action>
          <ActionMatch 
            MatchId=""urn:oasis:names:tc:xacml:1.0:function:string-equal"">
            <AttributeValue DataType=""http://www.w3.org/2001/XMLSchema#string"">read</AttributeValue>
            <ActionAttributeDesignator 
              DataType=""http://www.w3.org/2001/XMLSchema#string"" 
              AttributeId=""urn:oasis:names:tc:xacml:1.0:action:action-id""/>
          </ActionMatch>
        </Action>
        <Action>
          <ActionMatch 
            MatchId=""urn:oasis:names:tc:xacml:1.0:function:string-equal"">
            <AttributeValue DataType=""http://www.w3.org/2001/XMLSchema#string"">write</AttributeValue>
            <ActionAttributeDesignator 
              DataType=""http://www.w3.org/2001/XMLSchema#string"" 
              AttributeId=""urn:oasis:names:tc:xacml:1.0:action:action-id""/>
          </ActionMatch>
        </Action>
        <Action>
          <ActionMatch 
            MatchId=""urn:oasis:names:tc:xacml:1.0:function:string-equal"">
            <AttributeValue DataType=""http://www.w3.org/2001/XMLSchema#string"">delete</AttributeValue>
            <ActionAttributeDesignator 
              DataType=""http://www.w3.org/2001/XMLSchema#string"" AttributeId=""urn:oasis:names:tc:xacml:1.0:action:action-id""/>
          </ActionMatch>
        </Action>
      </Actions>
    </Target>
  </Rule>
</Policy>"
1,"Deadlock between SingleClientConnManager.releaseConnection() and SingleClientConnManager.shutdown(). It's possible to create a deadlock within SingleClientConnectionManager.

When JMeter interrupts a test, it calls HttpUriRequest.abort(), and as part of thread end processing it calls SingleClientConnManager.shutdown().

See deadlock details below.

I don't yet know why the shutdown is called before the abort finishes; that is probably a bug.

However, there may be a issue with the locking strategy within SCCM, hence this report.

""Thread-18"":
        at org.apache.http.impl.conn.SingleClientConnManager.releaseConnection(SingleClientConnManager.java:258)
        - waiting to lock <0x19e00118> (a org.apache.http.impl.conn.SingleClientConnManager)
        at org.apache.http.impl.conn.AbstractClientConnAdapter.abortConnection(AbstractClientConnAdapter.java:323)
        - locked <0x19e00148> (a org.apache.http.impl.conn.SingleClientConnManager$ConnAdapter)
        at org.apache.http.client.methods.HttpRequestBase.abort(HttpRequestBase.java:161)
        at org.apache.jmeter.protocol.http.sampler.HTTPHC4Impl.interrupt(HTTPHC4Impl.java:1090)
        at org.apache.jmeter.protocol.http.sampler.HTTPSamplerProxy.interrupt(HTTPSamplerProxy.java:77)
        at org.apache.jmeter.threads.JMeterThread.interrupt(JMeterThread.java:580)
        at org.apache.jmeter.engine.StandardJMeterEngine.tellThreadsToStop(StandardJMeterEngine.java:552)
        at org.apache.jmeter.engine.StandardJMeterEngine.access$2(StandardJMeterEngine.java:547)
        at org.apache.jmeter.engine.StandardJMeterEngine$StopTest.run(StandardJMeterEngine.java:284)
        at java.lang.Thread.run(Thread.java:662)
""Thread Group 1-1"":
        at org.apache.http.impl.conn.AbstractPooledConnAdapter.detach(AbstractPooledConnAdapter.java:106)
        - waiting to lock <0x19e00148> (a org.apache.http.impl.conn.SingleClientConnManager$ConnAdapter)
        at org.apache.http.impl.conn.SingleClientConnManager.shutdown(SingleClientConnManager.java:342)
        - locked <0x19e00118> (a org.apache.http.impl.conn.SingleClientConnManager)
        at org.apache.jmeter.protocol.http.sampler.HTTPHC4Impl.closeThreadLocalConnections(HTTPHC4Impl.java:1076)
        at org.apache.jmeter.protocol.http.sampler.HTTPHC4Impl.threadFinished(HTTPHC4Impl.java:1065)
        at org.apache.jmeter.protocol.http.sampler.HTTPSamplerProxy.threadFinished(HTTPSamplerProxy.java:71)
        at org.apache.jmeter.threads.JMeterThread$ThreadListenerTraverser.addNode(JMeterThread.java:553)
        at org.apache.jorphan.collections.HashTree.traverseInto(HashTree.java:986)
        at org.apache.jorphan.collections.HashTree.traverse(HashTree.java:969)
        at org.apache.jmeter.threads.JMeterThread.threadFinished(JMeterThread.java:528)
        at org.apache.jmeter.threads.JMeterThread.run(JMeterThread.java:308)
        at java.lang.Thread.run(Thread.java:662)
"
1,"Jcr2Spi: ExportSysViewTest#testExportSysView_handler_session_saveBinary_* occasionally failing. from time to time i saw ExportSysViewTest#testExportSysView_handler_session_saveBinary_* test failing. this doesn't occur consistently and i never managed to reproduce it when running the tests in the idea.
"
1,"URI Absolutization does not follow browser behavior. This was encountered using Heritrix to crawl a prominent website.

The URI resulting from the HttpClient URI constructor (base, relative) does not follow browser behavior:
URI newUrl = new URI(new URI(""http://www.theirwebsite.com/browse/results?type=browse&att=1""), ""?sort=0&offset=11&pageSize=10"")

Results in newUrl:
http://www.theirwebsite.com/browse/?sort=0&offset=11&pageSize=10

The desired behavior based on Firefox and IE should be:
http://www.theirwebsite.com/browse/results?sort=0&offset=11&pageSize=10

These browsers treat the question mark similar to a directory separator and do not require a file to be specified before the query.

HttpClient's current behavior does not correspond to current browser behavior and leads to an inability to crawl certain websites if HttpClient's URI class is used.

"
1,"new QueryParser fails to set AUTO REWRITE for multi-term queries. The old QueryParser defaults to constant score rewrite for Prefix,Fuzzy,Wildcard,TermRangeQuery, but the new one seems not to."
1,"FieldInfo omitTerms bug. Around line 95 you have:

    if (this.omitTf != omitTf) {
      this.omitTf = true;                // if one require omitTf at least once, it remains off for life
    }

Both references of the omitTf booleans in the if statement refer to the same field. I am guessing its meant to be other.omitTf like the norms code above it."
1,"Combination of BooleanQuery and PhrasePrefixQuery can provoke UnsupportedOperationException. A BooleanQuery including a PhrasePrefixQuery can cause an exception to be thrown
from BooleanScorer#skipTo when the search is executed:  

java.lang.UnsupportedOperationException
	at org.apache.lucene.search.BooleanScorer.skipTo(BooleanScorer.java:189)
	at org.apache.lucene.search.ConjunctionScorer.doNext(ConjunctionScorer.java:53)
	at org.apache.lucene.search.ConjunctionScorer.next(ConjunctionScorer.java:48)
	at org.apache.lucene.search.Scorer.score(Scorer.java:37)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:92)
	at org.apache.lucene.search.Hits.getMoreDocs(Hits.java:64)
	at org.apache.lucene.search.Hits.<init>(Hits.java:43)
	at org.apache.lucene.search.Searcher.search(Searcher.java:33)
	at org.apache.lucene.search.Searcher.search(Searcher.java:27)
        ... (non-lucene code)

The problem appears to be that PhrasePrefixQuery optimizes itself into a
BooleanQuery when it contains only one term.  However, it does this in the
createWeight() method of its scorer instead of in the rewrite method of the
query itself.  Thus it bypasses the boolean typecheck when BooleanQuery is
deciding whether to use ConjunctionScorer or BooleanScorer, eventually resulting
in the UOE."
1,"Node.getLock() on a lock non-lockable node throws javax.jcr.UnsupportedRepositoryOperationException. Consider Node ""/test"" with mixin type mix:lockable and Node ""/test/child"" without mixin type mix:lockable.

       Node root = Session.getRootNode();
       Node test = root.getNode(""test"");
       Node child = test.getNode(""child"");

       // create lock on /test
       test.lock();

       // succeeds
       Lock lock = test.getLock();

       // sets flag true
       boolean locked = child.isLocked();

       // throws UnsupportedRepositoryOperationException
       lock = child.getLock();

According to the spec, the getLock() method must return the applicable lock if the node is locked regardless of whether the node has mixin type ""mix:lockable"" or not."
1,"DefaultSimilarity.queryNorm() should never return Infinity. Currently DefaultSimilarity.queryNorm() returns Infinity if sumOfSquaredWeights=0.
This can result in a score of NaN (e. g. in TermScorer) if boost=0.0f.

A simple fix would be to return 1.0f in case zero is passed in.

See LUCENE-698 for discussions about this."
1,"Docview import fails, if attribute and childelem have same name. docimport fails, if element has same name as one of the attributes of its parent element.

example:

<?xml version=""1.0"" encoding=""UTF-8""?>
<feature plugin=""foobar"">
    <plugin>test</plugin>
</feature>

importing this results in a ItemExistsException: 'plugin'"
1,"testIWondiskfull unreferenced files failure. NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterOnDiskFull -Dtestmethod=testAddDocumentOnDiskFull -Dtests.seed=aff9b14dd518cfb:4d2f112726e2947f:-2b03094a43a947ee -Dtests.multiplier=3 -Dargs=""-Dfile.encoding=ISO8859-1""

Reproduces some of the time..."
1,"Create or Append mode determined before obtaining write lock. If an IndexWriter(""writer1"") is opened in CREATE_OR_APPEND mode, it determines whether to CREATE or APPEND before obtaining the write lock.  When another IndexWriter(""writer2"") is in the process of creating the index, this can result in writer1 entering create mode and then waiting to obtain the lock.  When writer2 commits and releases the lock, writer1 is already in create mode and overwrites the index created by write2.

This bug was probably effected by LUCENE-2386 as prior to that Lucene generated an empty commit when a new index was created.  I think the issue could still have occurred prior to that but the two IndexWriters would have needed to be opened nearly simultaneously and the first IndexWriter would need to release the lock before the second timed out."
1,"PROPPATCH on collection gets 403 Forbidden. DefaultHandler.canImport(PropertyImportContext, boolean) prevents setting properties (PROPPATCH) on collections through WebDAV ... returns 403 Forbidden. It checks to see whether the contextItem is not a collection, or has a jcr:content node. This test fails for a collection and should probably allow collections or nodes that have a jcr:content subnode. Here is a patch for the change

Index: jackrabbit-jcr-server/src/main/java/org/apache/jackrabbit/server/io/DefaultHandler.java
===================================================================
--- jackrabbit-jcr-server/src/main/java/org/apache/jackrabbit/server/io/DefaultHandler.java	(revision 567695)
+++ jackrabbit-jcr-server/src/main/java/org/apache/jackrabbit/server/io/DefaultHandler.java	(working copy)
@@ -570,7 +570,7 @@
         }
         Item contextItem = context.getImportRoot();
         try {
-            return contextItem != null && contextItem.isNode() && (!isCollection || ((Node)contextItem).hasNode(JcrConstants.JCR_CONTENT));
+            return contextItem != null && contextItem.isNode() && (isCollection || ((Node)contextItem).hasNode(JcrConstants.JCR_CONTENT));
         } catch (RepositoryException e) {
             log.error(""Unexpected error: "" + e.getMessage());
             return false;
"
1,"SpanRegexQuery and SpanNearQuery is not working with MultiSearcher. MultiSearcher is using:
queries[i] = searchables[i].rewrite(original);
to rewrite query and then use combine to combine them.

But SpanRegexQuery's rewrite is different from others.
After you call it on the same query, it always return the same rewritten queries.

As a result, only search on the first IndexSearcher work. All others are using the first IndexSearcher's rewrite queries.
So many terms are missing and return unexpected result.

Billow"
1,"Bundle cache is not rolled back when the storage of a ChangeLog fails. The bundle cache in the bundle persistence managers is not restored to its old state when the AbstractBundlePersistenceManager.store(ChangeLog changeLog) method throws an exception. If, for instance, the storage of references fails then the AbstractBundlePersistenceManager.putBundle(NodePropBundle bundle) method has already been called for all modified bundles. Because of the connection rollback, the bundle cache will be out-of-sync with the persistent state. As a result, the SharedItemStateManager will have an incorrect view of the persistent state.
Furthermore, if the blockOnConnectionLoss property is set to true, then the BundleDbPersistenceManager can be caught in an infinite loop because of invalid SQL inserts because of an incorrect bundle cache; see attached stacktrace."
1,Some code still compares string equality instead using equals. I found a couple of places where we still use string == otherstring which don't look correct. I will attache a patch soon.
1,"exception during writeRequest leaves the connection un-released. The execute method has the following (simplified) flow:
1) get connection
2) write request
3) read result
4) release connection.
The release in step 4 happens when the input is completely read, which works fine.
If an exception occurs between steps 1 and 2, the connection is also released
properly.
However, if an exception occurs during step 2, the connection is never released
back and the connection manager eventually runs out of connections.

The easiest way to test this is to make a simple subclass of PostMethod that
overrides the writeRequest method:

public class TestConnectionReleaseMethod extends PostMethod
{
    protected void writeRequest(HttpState state, HttpConnection conn) throws
IOException, HttpException
    {
         throw new IOException(""for testing"");
    }
}"
1,"CircularRedirectException encountered when using a proxy, but not when reaching the target directly. A CircularRedirectException is encountered when using a proxy (tinyproxy on a remote machine), whereas everything is fine when using no proxy. The target is a URL such as http://www.seoconsultants.com/w3c/status-codes/301.asp which has a 301 redirection.

The issue can be fixed by using ALLOW_CIRCULAR_REDIRECTS set to true (client params), but I can't consider this a ""real"" fix.

Here is a snippet of code that exemplifies the problem (use your own proxy):

---
String proxyHost = ""xyz.webfactional.com"";
int proxyPort = 7295;

DefaultHttpClient httpclient = new DefaultHttpClient();
// without a proxy it's OK!
httpclient.getParams().setParameter(ConnRoutePNames.DEFAULT_PROXY,
        new HttpHost(proxyHost, proxyPort, ""http""));

HttpParams params = httpclient.getParams();
HttpClientParams.setRedirecting(params, true);
HttpProtocolParams.setUserAgent(params,
        ""Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.5; en-US; rv:1.9.0.10) Gecko/2009042315 Firefox/3.0.10"");

// OK, this fixes the problem, but at what cost / other problems ?
//httpclient.getParams().setParameter(ClientPNames.ALLOW_CIRCULAR_REDIRECTS, true);

String url = ""http://www.seoconsultants.com/w3c/status-codes/301.asp"";

HttpUriRequest request;
HttpResponse response;

request = new HttpGet(url);
System.out.println(""request = "" + request.getRequestLine());
response = httpclient.execute(request);
System.out.println(""status = "" + response.getStatusLine());
System.out.println(""headers = "" + Arrays.asList(response.getAllHeaders()));
---"
1,"contrib/benchmark assumes Locale.US for parsing dates in Reuters collection. SimpleDateFormat used for parsing dates in Reuters documents is instantiated without specifying a locale. So it is using the default locale. If that happens to be US, it will work. But for another locale a parse exception is likely.

Affects both StandardBenchmarker and ReutersDocMaker.

Fix is trivial - specify Locale.US for SimpleDateFormat's constructor.
"
1,"cache does not honor must-revalidate or proxy-revalidate Cache-Control directives. http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.4

There are a couple of missed requirements here regarding must-revalidate and proxy-revalidate (which applies only to shared caches).
1. When a cache entry with this directive is revalidated, it must be an end-to-end revalidation (meaning it must include 'max-age=0' on the request).
2. If the revalidation with the origin fails, the cache MUST NOT return a stale entry and MUST return a 504 response.
"
1,"VersionIteratorImpl problem?. I meet with problem in VersionIterator:
Classic nextVersion()/hasNext() loop  for VersionIterator become endless. 

I think problem with peek/pop misprint:

    public Version nextVersion() {
.......
        InternalVersion ret = (InternalVersion) successors.<b>peek</b>();
......
    }

I change to
InternalVersion ret = (InternalVersion) successors.<b>pop</b>();


"
1,"Possible thread hazard in IndexWriter.close(false). Spinoff from this thread:

  http://www.gossamer-threads.com/lists/lucene/java-dev/55391

On reviewing the code I found one case where an aborted merge (from
calling close(false)) could write to files that a newly opened
IndexWriter would also try to write to.

I strengthened an existing test case in TestConcurrentMergeScheduler
to tickle this case, and also modified MockRAMDirectory to throw an
IOException if ever a file besides segments.gen is overwritten.

However, strangely, I can't get an unhandled exception to occur during
the test and I'm not sure why.  Still I think this is a good defensive
check so we should commit it.
"
1,"Unable to save session after saving a renamed node. 		TransientRepository repo = new TransientRepository(
				""applications/test/repository.xml"", ""applications/test"");
		Session s = repo.login(new SimpleCredentials(""test"", """".toCharArray()));

		if (s.getRootNode().hasNode(""parent"")) {
			s.getRootNode().getNode(""parent"").remove();
			s.save();
		}

		// create parent node
		Node parent = s.getRootNode().addNode(""parent"");
		
		// create node to rename
		parent.addNode(""newnode"");
		s.save();

		// rename node
		s.move(""/parent/newnode"", ""/parent/renamedNewNode"");

		// save renamed node
		s.getRootNode().getNode(""parent/renamedNewNode"").save();

		// try to save session --> FAILS
		s.save();

		s.logout();"
1,"Inconsistency when version with a label is removed. While executing random operations on the version storage I came across a situation where a version is removed that has a version label.

The current behaviour in jackrabbit is IMO not correct because the version node gets removed, but the version label is still present in the version history. This means there is a referenceable property that points to nowhere.

So I guess either:

- the version label property must be removed as well when the version is removed
or
- the remove version operation should fail because the version is still referenced

I wasn't able to find a relevant section in the spec, though I must admit that I don't know the versioning section that well."
1,"LockMethod.getResponseAsLockDiscovery() fails when status==201 . RFC 4918 Section 9.10.6 specifies that 201 is a valid response code for LOCK: ""201 (Created) - The LOCK request was to an unmapped URL, the request succeeded and resulted in the creation of a new resource, and the value of the DAV:lockdiscovery property is included in the response body.""

However, LockMethod.getResponseAsLockDiscovery() would fail in that scenario. 
    org.apache.jackrabbit.webdav.DavException: Created
	 at org.apache.jackrabbit.webdav.client.methods.DavMethodBase.getResponseException(DavMethodBase.java:164)
	 at org.apache.jackrabbit.webdav.client.methods.LockMethod.getResponseAsLockDiscovery(LockMethod.java:119)


The reason is in LockMethod:175 
      return statusCode == DavServletResponse.SC_OK;

Should be: 
      return statusCode == DavServletResponse.SC_OK
             || statusCode ==DavServletResponse.SC_CREATED;"
1,"Deadlock case in IndexWriter on exception just before flush. If a document hits a non-aborting exception, eg something goes wrong
in tokenStream.next(), and, that document had triggered a flush
(due to RAM or doc count) then DocumentsWriter will deadlock because
that thread marks the flush as pending but fails to clear it on
exception.

I have a simple test case showing this, and a fix fixing it."
1,"NullPointerException in ClassDescriptor. Index: /Users/cziegeler/Developer/workspaces/default/jackrabbit/contrib/jackrabbit-jcr-mapping/jcr-mapping/src/main/java/org/apache/jackrabbit/ocm/mapper/model/ClassDescriptor.java
===================================================================
--- /Users/cziegeler/Developer/workspaces/default/jackrabbit/contrib/jackrabbit-jcr-mapping/jcr-mapping/src/main/java/org/apache/jackrabbit/ocm/mapper/model/ClassDescriptor.java	(revision 579109)
If a class descriptor (for whatever reason) does not have a jcr type, a npe is thrown in ClassDescriptor.
The following patch solves this issue:

+++ contrib/jackrabbit-jcr-mapping/jcr-mapping/src/main/java/org/apache/jackrabbit/ocm/mapper/model/ClassDescriptor.java	(working copy)
@@ -468,7 +468,7 @@
         while (iterator.hasNext()) {
             ClassDescriptor descendantClassDescriptor = (ClassDescriptor) iterator.next();
   
-            if (descendantClassDescriptor.getJcrType().equals(nodeType)) {
+            if (nodeType.equals(descendantClassDescriptor.getJcrType())) {
                 return descendantClassDescriptor;
             }
   
"
1,"""fdx size mismatch"" exception in StoredFieldsWriter.closeDocStore() when closing index with 500M documents. When closing index that contains 500,000,000 randomly generated documents, an exception is thrown:

java.lang.RuntimeException: after flush: fdx size mismatch: 500000000 docs vs 4000000004 length in bytes of _0.fdx
	at org.apache.lucene.index.StoredFieldsWriter.closeDocStore(StoredFieldsWriter.java:94)
	at org.apache.lucene.index.DocFieldConsumers.closeDocStore(DocFieldConsumers.java:83)
	at org.apache.lucene.index.DocFieldProcessor.closeDocStore(DocFieldProcessor.java:47)
	at org.apache.lucene.index.DocumentsWriter.closeDocStore(DocumentsWriter.java:367)
	at org.apache.lucene.index.IndexWriter.flushDocStores(IndexWriter.java:1688)
	at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:3518)
	at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3442)
	at org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:1623)
	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1588)
	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1562)
        ...

This appears to be a bug at StoredFieldsWriter.java:93:

      if (4+state.numDocsInStore*8 != state.directory.fileLength(state.docStoreSegmentName + ""."" + IndexFileNames.FIELDS_INDEX_EXTENSION))

where the multiplication by 8 is causing integer overflow. The fix would be to cast state.numDocsInStore to long before multiplying.

It appears that this is another instance of the mistake that caused bug LUCENE-1519. I did a cursory seach for \*8 against the code to see if there might be yet more instances of the same mistake, but found none. 

"
1,".toString on empty MultiPhraseQuery hits NPE. Ross Woolf hit this on java-user thread ""MultiPhraseQuery.toString() throws null pointer exception"".  It's still present on trunk..."
1,"Default proxy set at the client level has no effect. Default proxy set at the client level has no effect, as client parameters are not correctly propagated to the HttpRoutePlanner"
1,"LOWER operand with nested LOCALNAME operand not work with SQL2. Below query was running successful using Query.SQL languange:
SELECT * FROM nt:file WHERE (CONTAINS(*, 'Jon') OR  LOWER(fn:name()) LIKE '%jon%') AND jcr:path LIKE '/Resources/%' ORDER BY jcr:score()

But equivalent next query in Query.JCR_SQL2 will fail with exception UnsupportedRepositoryOperationException():
SELECT * FROM [nt:file] WHERE (CONTAINS([nt:file].*, 'Jon') OR  LOWER(LOCALNAME()) LIKE '%jon%') AND ISDESCENDANTNODE('/Resources') ORDER BY SCORE()

From my investigation seems LOWER function will not work with nested function LOCALNAME. According to section ""6.7.32 LowerCase"" JCR 2.0 Specs, LOWER operand able to work on DynamicOperand argument."
1,PrivilegeDefinition should implement equals and hashcode. 
1,"DirectoryTaxonomyWriter can lose the INDEX_CREATE_TIME property, causing DirTaxoReader.refresh() to falsely succeed (or fail). DirTaxoWriter sets createTime to null after it put it in the commit data once. But that's wrong because if one calls commit(Map<>) twice, the second time doesn't record the creation time. Also, in the ctor, if an index exists and OpenMode is not CREATE, the creation time property is not read.

I wrote a couple of unit tests that assert this, and modified DirTaxoWriter to always record the creation time (in every commit) -- that's the only safe way.

Will upload a patch shortly."
1,"DateValue.getDate not a copy. I noticed that getDate() in org.apache.jackrabbit.value.DateValue is returned 
by reference. According to the specification it should be a copy. (see.  JSR 170 section 6.2.7)

 
 private Calendar date;
 
 public Calendar getDate()
             throws ValueFormatException, IllegalStateException,
             RepositoryException {
         setValueConsumed();
 
         if (date != null) {
             return date; // <-- HERE
         } else {
             throw new ValueFormatException(""empty value"");
         }
     }

short test:

ValueFactory factory = session.getValueFactory();
 Value v = factory.createValue(GregorianCalendar.getInstance());
 Calendar c0 = v.getDate();   
 Calendar c1 = v.getDate();
               
 if(c0 == c1){
                   out.println(""error - references are equal"");
                    out.println(c0);
 }"
1,"Clustering is broken due to duplicated CachingPersistenceManager interface. There are now two interfaces CachingPersistenceManager in the packages org.apache.jackrabbit.core.persistence.bundle and org.apache.jackrabbit.core.persistence.pool. A persistence manager that implements the ..bundle... interface doesn't receive the onExternalUpdate events that are required for clustering to work.

I will move this interface to the package org.apache.jackrabbit.core.persistence and remove the second implementation.

This change has no affect to backward compatibility, because anyway there were many breaking changes in the past (NodeId / UUID for example).
"
1,"DateTools UTC/GMT mismatch. Post from Antony Bowesman on java-user:

-----

I just noticed that although the Javadocs for Lucene 2.2 state that the dates 
for DateTools use UTC as a timezone, they are actually using GMT.

Should either the Javadocs be corrected or the code corrected to use UTC instead.

-----

I'm attaching a patch that changes the javadoc and will commit it, unless someone knows a reason the javadoc is correct and the code should be changed to UTC. To my understanding, there's no significant difference between UTC and GMT.
"
1,"NullPointerException in CompoundFileReader. Hello,

we have got a NullPointerException in the Lucene-class CompoundFileReader:

java.lang.NullPointerException
        at
org.apache.lucene.index.CompoundFileReader.<init>(CompoundFileReader.java:94)
        at org.apache.lucene.index.SegmentReader.<init>(SegmentReader.java:97)
        at org.apache.lucene.index.IndexWriter.mergeSegments(IndexWriter.java:466)
        at
org.apache.lucene.index.IndexWriter.flushRamSegments(IndexWriter.java:426)
        at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:236)

Lucene has been working fine for some days, until this NullPointerException
has occured which has corrupted the complete index.

The reason for this NullPointerException is the following Code 
in Lucenes source file CompoundFileReader.java:

    public CompoundFileReader(Directory dir, String name)
    throws IOException
    {
        boolean success = false;
        ...

        try {
            stream = dir.openFile(name);

            // read the directory and init files
            ...

            success = true;

        } finally {
            if (! success) {
                try {
                    stream.close();
                } catch (IOException e) { }
            }
        }
    }

If the IO-method-call ""dir.openFile()"" throws an IOExeption,
then the variable ""stream"" remains its null value.
The statement ""stream.close()"" in the finally clause will then cause a
NullPointerException.

I would suggest that you change the code from:
    stream.close();
to:
    if ( stream != null ) {
        stream.close();
    }

There are a lot of reasons why an IO-operation like ""dir.openFile()""
could throw an IOException.
I cannot guarantee that such an IO exception will never occur again.
Therefore it is better to handle such an IO exception correctly.

This issue is similar to bug# 29774, except that I recommand an easy way
to solve this problem."
1,"Formatting error in ReportTask in contrib/benchmark. I am building a new Task, AnalyzerTask, that lets you change the Analyzer in the loop, thus allowing for the comparison of the same Analyzers over the set of documents.

My algorithm declaration looks like:
NewAnalyzer(WhitespaceAnalyzer, SimpleAnalyzer, StopAnalyzer, standard.StandardAnalyzer)

And it could be longer.

The exception is:
Error: cannot execute the algorithm! String index out of range: 85
java.lang.StringIndexOutOfBoundsException: String index out of range: 85
	at java.lang.String.substring(String.java:1765)
	at org.apache.lucene.benchmark.byTask.utils.Format.format(Format.java:85)
	at org.apache.lucene.benchmark.byTask.tasks.ReportTask.tableTitle(ReportTask.java:85)
	at org.apache.lucene.benchmark.byTask.tasks.ReportTask.genPartialReport(ReportTask.java:140)
	at org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask.reportSumByName(RepSumByNameTask.java:77)
	at org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask.doLogic(RepSumByNameTask.java:39)
	at org.apache.lucene.benchmark.byTask.tasks.PerfTask.runAndMaybeStats(PerfTask.java:83)
	at org.apache.lucene.benchmark.byTask.tasks.TaskSequence.doSerialTasks(TaskSequence.java:112)
	at org.apache.lucene.benchmark.byTask.tasks.TaskSequence.doLogic(TaskSequence.java:93)
	at org.apache.lucene.benchmark.byTask.utils.Algorithm.execute(Algorithm.java:228)
	at org.apache.lucene.benchmark.byTask.Benchmark.execute(Benchmark.java:73)
	at org.apache.lucene.benchmark.byTask.Benchmark.main(Benchmark.java:109)

The error seems to be caused by the fact that ReportTask uses the OP (operation) column for the String, but then uses the length of the algorithm declaration to index into the String, resulting in the index out of bounds exception.

The line in question is:
return (s + padd).substring(0, col.length());

And probably should be changed to something like:
    String s1 = (s + padd);
    return s1.substring(0, Math.min(col.length(), s1.length()));

Either that or the column should be trimmed.  The workaround is to explicitly name the task.

If no objections, I will make the change, tomorrow.  "
1,"AssertionError on creating doc containing field with empty string as field name. Spinoff from here:

  http://www.gossamer-threads.com/lists/lucene/java-user/58496

Pre-2.3 you were allowed to add Fields to a Document where the field name is the empty string.  In 2.3.0 it broke: you hit this during flush:

{code}
java.lang.AssertionError
    at org.apache.lucene.index.TermInfosWriter.add(TermInfosWriter.java:143)
    at org.apache.lucene.index.DocumentsWriter.appendPostings(DocumentsWriter.java:2290)
    at org.apache.lucene.index.DocumentsWriter.writeSegment(DocumentsWriter.java:1985)
    at org.apache.lucene.index.DocumentsWriter.flush(DocumentsWriter.java:539)
    at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:2497)
    at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:2397)
    at org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:1204)
    at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1178)
    at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1153) 
{code}

The bug is just an over-aggressive assert statement.  I'll commit a fix shortly & port to 2.3 branch for 2.3.1 release."
1,"Changes of JCR-313 introduced db-transaction problem. the fix of JCR-313 changed the autocommit from 'true' to 'false', resulting the DatabaseFileSystems not to write back correctly anymore."
1,"FileInputStream never closed in HTMLParser. HTMLParser.java contains this code: 
 
  public HTMLParser(File file) throws FileNotFoundException { 
    this(new FileInputStream(file)); 
  } 
 
This FileInputStream should be closed with the close() method, as there's no 
guarantee that the garbage collection will run and do this for you. I don't 
know how to fix this without changing the API to take a FileInputStream 
instead of a File, as the call to this() must be the first thing in the 
constructor, i.e. you cannot create the stream, call this(...), and then close 
the stream."
1,"TokenSources.getTokenStream() does not assign positionIncrement. TokenSources.StoredTokenStream does not assign positionIncrement information. This means that all tokens in the stream are considered adjacent. This has implications for the phrase highlighting in QueryScorer when using non-contiguous tokens.

For example:
Consider  a token stream that creates tokens for both the stemmed and unstemmed version of each word - the fox (jump|jumped)
When retrieved from the index using TokenSources.getTokenStream(tpv,false), the token stream will be - the fox jump jumped

Now try a search and highlight for the phrase query ""fox jumped"". The search will correctly find the document; the highlighter will fail to highlight the phrase because it thinks that there is an additional word between ""fox"" and ""jumped"". If we use the original (from the analyzer) token stream then the highlighter works.

Also, consider the converse - the fox did not jump
""not"" is a stop word and there is an option to increment the position to account for stop words - (the,0) (fox,1) (did,2) (jump,4)
When retrieved from the index using TokenSources.getTokenStream(tpv,false), the token stream will be - (the,0) (fox,1) (did,2) (jump,3).

So the phrase query ""did jump"" will cause the ""did"" and ""jump"" terms in the text ""did not jump"" to be highlighted. If we use the original (from the analyzer) token stream then the highlighter works correctly."
1,SearchIndex parameter cacheSize is ignored. The cacheSize is always set to 1024 no matter what is specified in the configuration.
1,"typo in the mimetypes.properties file. The Powerpoint mime-type (ppt) is wrong in mimetypes.properties file. It was written with a equals(=) caracter in place of a dot (.).
"
1,jackrabbit-jcr-client tests fail (and are disabled in pom). I suggest to enable the tests and fix the issues causing them to fail. 
1,"When BG merge hits an exception, optimize sometimes throws an IOException missing the root cause. 
When IndexWriter.optimize() is called, ConcurrentMergeScheduler will
run the requested merges with background threads and optimize() will
wait for these merges to complete.

If a merge hits an exception, it records the root cause exception such
that optimize can then retrieve this root cause and throw its own
exception, with the root cause.

But there is a bug: sometimes, the fact that an exception occurred on
a merge is recorded, but the root cause is missing.  In this cause,
optimize() still throws an exception (correctly indicating that the
optimize() has not finished successfully), but it's not helpful
because it's missing the root cause.  You must then go find the root
cause in the JRE's stderr logs.

This has hit a few users on this lists, most recently:

  http://www.nabble.com/Background-merge-hit-exception-td19540409.html#a19540409

I found the isssue, and finally got a unit test to intermittently show
it.  It's a simple thread safety issue: in a finally clause in
IndexWriter.merge we record the fact that the merge hit an exception
before actually setting the root cause, and then only in
ConcurrentMergeScheduler's exception handler do we set the root
cause.  If the optimize thread is scheduled in between these two, it
can throw an exception missing its root cause.

The fix is straightforward.  I plan to commit to 2.4 & 2.9.
"
1,"leak in MultiThreadedHttpConnectionManager.ConnectionPool.mapHosts. Once entries are added to MultiThreadedHttpConnectionManager.ConnectionPool.mapHosts, they are never cleaned up unless MultiThreadedHttpConnectionManager is shutdown."
1,"InternalVersionManagerBase.calculateCheckinVersionName will fail with NPE upon empty predecessors property. (Note: this can only happen on inconsistent version storage)

We should add a check here, and throw a more descriptive exception."
1,"MultipartEntity incorrectly computes unknown length. If any Part of a MultipartEntity reports an unknown length (-1), MultipartEntity
reports an erroneous length value. It should report an unknown length (-1) if
any of the parts is of unknown length, that would cause the POST to be chunked.

See
http://mail-archives.apache.org/mod_mbox/jakarta-httpclient-user/200510.mbox/ajax/%3ceb3d689c0510250851t2eb78462tbf701135bbf718c9@mail.gmail.com%3e"
1,"Remove FieldMaskingSpanQuery (or fix its scoring). In Lucene 4.0 we added new scoring mechanisms, but FieldMaskingSpanQuery is a serious problem:

Because it lies about the fields of its terms, this sometimes results in totally bogus
statistics, cases where a single terms totalTermFreq exceeds sumTotalTermFreq for the entire field (since its lying about it).

Such lying could result in NaN/Inf/Negative scores, exceptions, divide by zero, and other problems,
because the statistics are impossibly bogus."
1,"More Locale problems in Lucene. This is a followup to LUCENE-1836: I found some more Locale problems in Lucene with Date Formats. Even for simple date formats only consisting of numbers (like ISO dates), you should always give the US locale. Because the dates in DateTools should sort according to String.compare(), it is important, that the decimal digits are western ones. In some strange locales, this may be different. Whenever you want to format dates for internal formats you exspect to behave somehow, you should at least set the locale to US, which uses ASCII. Dates entered by users and displayed to users, should be formatted according to the default or a custom specified locale.
I also looked for DecimalFormat (especially used for padding numbers), but found no problems."
1,"Jcr-Server: BasicCredentialsProviderTest throws NPE if defaultAuthHeader init param misses the password. issue reported by dominique jaeggi:

a missing-auth-header init param that has the form ""uid"" instead of ""uid:pw"" or ""uid:"" results in NPE upon SimpleCredentials creation.



"
1,"ArrayIndexOutofBoundException while setting a reference property. I have a node whith a multivalued reference property.
I try to add a reference as follows (the spec is outdated at this point, so I'm not sure if I use the right approach to add a reference):

  ReferenceValue rv = new ReferenceValue(rn.getNode(""pages/mjo:page""));
  Value[] values = {rv};
  tstN.setProperty(""mjo:testCon"",values);
  session.save();

This results in a 

2004-12-09 16:11:43,614 WARN org.apache.jackrabbit.core.ItemManager - node at /pages/mjo:page has invalid definitionId (1512950840)
java.lang.ArrayIndexOutOfBoundsException: 0
	at org.apache.jackrabbit.core.ItemImpl.validateTransientItems(ItemImpl.java:626)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1148)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:633)
	at de.freaquac.test.JCRTest.main(JCRTest.java:174)

ArrayIndexOutOfBoundsException is never a good sign, so I assume it's a bug. I should say that crx throws the same exception if I try it there.
"
1,"Binding repository to a nameserver with RegistryHelper causes failure on lookup.. Binding a repository to a nameserver using RegistryHelper causes the next subsequent lookup to fail.  This is what I observerd:

1. RegistryHelper.registerRepository creates a new BindableRepository and initializes it.  This, in turn, initializes the ""real"" repository (i.e. delagtee).  It then binds this reference with the nameserver.

2. On the next lookup, BindableRepositoryFactory.getObjectInstance is invoked.  Thie method checks it's cache for a repository.  Since one does not exist yet, it creates a new BindableRepository and tries to initialize it.  This fails since the call to RegistryHelper.registerRepository already initialized the repository.

The error message basically says the repository is already in use by another process because the .lock file is present.  To fix this, I modified RegistryHelper.registerRepository to NOT initialize the repository and simply bind the ""Reference""."
1,jcr:encoding not respected in NodeIndexer. The value of the jcr:encoding property is not passed to the TextFilter instances.
1,"Select * does not return declared properties of node type in FROM clause. The query only returns the default columns: jcr:primaryType, jcr:score and jcr:path"
1,"renaming a node with Session.move() or Workspace.move() is not visible from other concurrent sessions. this is a CachingHierarchyManager issue.

to reproduce:

Node n1 = session1.getRootNode().getNode(""foo"");
String path = n1.getPath();   // returns ""/foo""

Node n2 = session2.getRootNode().getNode(""foo"");
path = n2.getPath();   // returns ""/foo""

session1.move(""/foo"", ""/bar"");
session1.save();

path = n1.getPath();   // returns ""/bar""
path = n2.getPath();   // should return ""/bar"" but still returns ""/foo""
"
1,"Duplicate request headers when connect through proxies. When negotiating proxy servers or during write-failure retries, the httpClient 
adds duplicate request headers to each retry.  The result is that each header 
is duplicated multiple times (number of retries).  This only impact the headers 
that allow multiple values (the others were prevented byt the code).  In  
Particular, it affects ""cookie"" header.  It happens more often when going 
through proxy server with tunnelling connections (https), but also happens on 
http on NTLM proxy server (need multiple round trips to get authenticated).

Steps to Reproduce:
Setup a client application to go to a website that requires going through a 
proxy server that supports tunnelling for https connections and authenticate 
users (Basic and/or NTLM).  The website also need to support keep-alive and 
support https. Initilize the HttpState with a cookie. Turn httpClient 
logging ""wire, debug and trace"" logging on.  Set the proxy credential with 
valid user id and password.

Then run the application against any url on the website.

Test Results and fixes:
1. When connect to https through a (Netscape/Basic auth) proxy that does 
the ""tunnelling"", the initial ""CONNECT"" adds the cookie header once.  The proxy 
returns the 407.  Then the code will use the proxy credential to do 
the ""CONNECT"" again.  After successful connection (200), the code will do the 
proper ""POST"".  The httpClient code adds the same cookie one more time in here.

This case was caused by the wrapper class ConnectMethod using the wrapped 
method ""addRequestHeaders"" to build headers for the ""CONENCT"".  It can be fixed 
by having the ConnectMethod only adds headers it needs (""addHostRequestHeader"" 
and ""addProxyAuthorizationRequestHeader"").

2. When connect to http through proxy.  The client first sends a ""POST"" without 
proxy credentials, and gets a 407 back.  The cookie header is added before that 
happens.  Then (loop in the HttpMethodBase.execute) the client will retry 
the ""POST"" with the credentials (the logic require to have a response header to 
submit credentials).  In the retry, the cookie is added again 
(addRequestHeaders is called inside the writeRequest).

3. When using kee-alive with no-proxy on http, if the connection times out, the 
next call of the method will get a socket error on write.  The retry loop in 
the processRequest method will retry the request again.  It will add the cookie 
again.  To deal with both case 2 and 3, the addRequestHeadres call need to be 
moved up to the beginning of the HttpMethodBase.execute.  We tested this 
approach and it worked for us.

4. When negotiating NTLM proxy for https, multiple round trips are needed to 
get user authenticated.  The same ConnectMethod instance is used.  So the 
adding of the proxy authenticate headers need to be done with the ConnectMethod 
instance (vs. the wrapped method for Host header).  Otherwise, the 
Authenticator class would not be able to find the information for 
authenticating user.


I will send in our suggested fixes later.

Build: This is based on 0307 nightly build.  We run into some other problems 
when trying the 0410 nightly build."
1,"nt:versionedChild problem. Problem occurs when both parent and child beans are versionable.  Jackrabbit creates an nt:versionedChild node that is referenced by the parent node, referencing the childs versionedHistory node of the child.  The current OCM code does not handle this correctly and produces an error:  ""Node type  'nt:versionedChild' does not match descriptor node type 'nt:unstructured'""

Below is a example code of the problem and a patch that appears to correctly resolve the problem.


 Within ObjectConverterImpl created the below method.

        public Node getActualNode(Session session,Node node) throws
 RepositoryException
        {
                NodeType type = node.getPrimaryNodeType();
                if (type.getName().equals(""nt:versionedChild""))
                {

                        String uuid =
 node.getProperty(""jcr:childVersionHistory"").getValue().getString();
                        Node actualNode = session.getNodeByUUID(uuid);
                        String name = actualNode.getName();
                        actualNode = session.getNodeByUUID(name);

                        return actualNode;
                }

                return node;
        }

 AND modified the following to call the above method


        public Object getObject(Session session, Class clazz, String path)
        {
                try {
                        if (!session.itemExists(path)) {
                                return null;
                        }

                        if (requestObjectCache.isCached(path))
                    {
                        return requestObjectCache.getObject(path);
                    }

                        ClassDescriptor classDescriptor =
 getClassDescriptor(clazz);

                        checkNodeType(session, classDescriptor);

                        Node node = (Node) session.getItem(path);
                        if (!classDescriptor.isInterface()) {
                                {
                                node = getActualNode(session,node);
                                checkCompatiblePrimaryNodeTypes(session,
 node, classDescriptor, true);
                                }
                        }

                        ClassDescriptor alternativeDescriptor = null;
                        if
 (classDescriptor.usesNodeTypePerHierarchyStrategy())
 {
                                if
 (node.hasProperty(ManagerConstant.DISCRIMINATOR_PROPERTY_NAME))
 {
                        String className =
 node.getProperty(ManagerConstant.DISCRIMINATOR_PROPERTY_NAME
 ).getValue().getString();
                        alternativeDescriptor =
 getClassDescriptor(ReflectionUtils.forName(className));
                                }
                        } else {
                                if
 (classDescriptor.usesNodeTypePerConcreteClassStrategy())
 {
                                        String nodeType =
 node.getPrimaryNodeType().getName();
                                        if
 (!nodeType.equals(classDescriptor.getJcrType()))
 {
                                            alternativeDescriptor =
 classDescriptor.getDescendantClassDescriptor(nodeType);

                                            // in case we an alternative
 could not be found by walking
                                            // the class descriptor
 hierarchy, check whether we
 would
                                            // have a descriptor for the
 node type directly (which
                                            // may the case if the class
 descriptor hierarchy is
                                            // incomplete due to missing
 configuration. See JCR-1145
                                            // for details.
                                            if (alternativeDescriptor ==
 null) {
                                                alternativeDescriptor =
 mapper.getClassDescriptorByNodeType(nodeType);
                                            }
                                        }
                                }
                        }

                        // if we have an alternative class descriptor,
 check whether its
                        // extends (or is the same) as the requested class.
                        if (alternativeDescriptor != null) {
                            Class alternativeClazz =
 ReflectionUtils.forName(alternativeDescriptor.getClassName());
                            if (clazz.isAssignableFrom(alternativeClazz)) {
                                clazz = alternativeClazz;
                                classDescriptor = alternativeDescriptor;
                            }
                        }

                        // ensure class is concrete (neither interface nor
 abstract)
                        if (clazz.isInterface() ||
 Modifier.isAbstract(clazz.getModifiers())) {
                            throw new JcrMappingException( ""Cannot
 instantiate non-concrete
 class "" + clazz.getName()
                        + "" for node "" + path + "" of type "" +
 node.getPrimaryNodeType().getName());
                        }

            Object object =
 ReflectionUtils.newInstance(classDescriptor.getClassName());

            if (! requestObjectCache.isCached(path))
            {
                          requestObjectCache.cache(path, object);
            }

            simpleFieldsHelp.retrieveSimpleFields(session,
 classDescriptor, node, object);
                        retrieveBeanFields(session, classDescriptor, node,
 path, object, false);
                        retrieveCollectionFields(session, classDescriptor,
 node, object, false);

                        return object;
                } catch (PathNotFoundException pnfe) {
                        // HINT should never get here
                        throw new
 ObjectContentManagerException(""Impossible to get
 the object
 at "" + path, pnfe);
                } catch (RepositoryException re) {
                        throw new
 org.apache.jackrabbit.ocm.exception.RepositoryException(""Impossible to
 get the object at "" + path, re);
                }
        }




>
>
>
> > I am building a test application against OCM.  I have the following
> > classes that are annotated for OCM.  The problem is that when I update
> and
> > version the root object PressRelease the Bean Author is versioned to
> > nt:versionedChild.  While the OCM is checking for node type
> compatibility
> > it is throwing the following exception.  It looks like the
> versionedChild
> > is not handled correctly.  Any suggestions?
> >
> > I also attempted to retrieve the version based on the version name for
> the
> > rootVersion but also trapped. From a Version object how should I access
> > each of the versioned entries?
> >
> > Thanks
> > Wes
> >
> > @Node (jcrMixinTypes=""mix:versionable"")
> > public class PressRelease
> > {
> >       @Field(path=true) String path;
> >       @Field String title;
> >       @Field Date pubDate;
> >       @Field String content;
> >       @Bean Author author;
> >       @Collection (elementClassName=Comment.class) List<Comment>
> comments = new
> > ArrayList<Comment>();
> >
> >       public String getPath() {
> >               return path;
> >       }
> >       public void setPath(String path) {
> >               this.path = path;
> >       }
> >       public String getContent() {
> >               return content;
> >       }
> >       public void setContent(String content) {
> >               this.content = content;
> >       }
> >       public Date getPubDate() {
> >               return pubDate;
> >       }
> >       public void setPubDate(Date pubDate) {
> >               this.pubDate = pubDate;
> >       }
> >       public String getTitle() {
> >               return title;
> >       }
> >       public void setTitle(String title) {
> >               this.title = title;
> >       }
> >       public Author getAuthor() {
> >               return author;
> >       }
> >       public void setAuthor(Author author) {
> >               this.author = author;
> >       }
> >       public List<Comment> getComments() {
> >               return comments;
> >       }
> >       public void setComments(List<Comment> comments) {
> >               this.comments = comments;
> >       }
> >
> >
> > }
> >
> > @Node (jcrMixinTypes=""mix:versionable"")
> > public class Author {
> >
> >       @Field(path=true) String path;
> >       @Field String name;
> >
> >
> >       public String getName() {
> >               return name;
> >       }
> >       public void setName(String name) {
> >               this.name = name;
> >       }
> >       public String getPath() {
> >               return path;
> >       }
> >       public void setPath(String path) {
> >               this.path = path;
> >       }
> >
> > }
> >
> > MAIN
> >
> >       while (versionIterator.hasNext())
> >       {
> >           Version version = (Version) versionIterator.next();
> >           System.out.println(""version found : ""+ version.getName() + "" -
> "" +
> >                                 version.getPath() + "" - "" +
> > version.getCreated().getTime());
> >
> >
> >           if (!version.getName().equals(""jcr:rootVersion""))
> >           {
> >
> > //      Get the object matching to the first version
> >           pressRelease = (PressRelease)
> > ocm.getObject(""/newtutorial"",version.getName());
> >
> >
> >               System.out.println(""PressRelease title : "" +
> pressRelease.getTitle());
> >               System.out.println(""             author: "" +
> > pressRelease.getAuthor().getName());
> >               System.out.println(""            content: "" +
> pressRelease.getContent());
> >               List comments = pressRelease.getComments();
> >               Iterator iterator = comments.iterator();
> >               while (iterator.hasNext())
> >               {
> >                       comment = (Comment) iterator.next();
> >                       System.out.println(""Comment : <"" + comment.getData()
> + "">"" +
> > comment.getText());
> >               }
> >           }
> >       }
> >
> >
> > CONSOLE
> > version found : jcr:rootVersion -
> >
> /jcr:system/jcr:versionStorage/fc/0b/fd/fc0bfd89-c487-4fbe-930f-d837e5dfed79/jcr:rootVersion
> > - Thu Feb 28 15:54:42 EST 2008
> > version found : 1.0 -
> >
> /jcr:system/jcr:versionStorage/fc/0b/fd/fc0bfd89-c487-4fbe-930f-d837e5dfed79/1.0
> > - Thu Feb 28 15:54:59 EST 2008
> > Exception in thread ""main""
> > org.apache.jackrabbit.ocm.exception.ObjectContentManagerException:
> Cannot
> > map object of type 'com..pc.repository.Author'. Node type
> > 'nt:versionedChild' does not match descriptor node type
> 'nt:unstructured'
> >       at
> >
> org.apache.jackrabbit.ocm.manager.objectconverter.impl.ObjectConverterImpl.checkCompatiblePrimaryNodeTypes
> (ObjectConverterImpl.java:552)
> >       at
> >
> org.apache.jackrabbit.ocm.manager.objectconverter.impl.ObjectConverterImpl.getObject
> (ObjectConverterImpl.java:361)
> >       at
> >
> org.apache.jackrabbit.ocm.manager.beanconverter.impl.DefaultBeanConverterImpl.getObject
> (DefaultBeanConverterImpl.java:80)
> >       at
> >
> org.apache.jackrabbit.ocm.manager.objectconverter.impl.ObjectConverterImpl.retrieveBeanField
> (ObjectConverterImpl.java:666)
> >       at
> >
> org.apache.jackrabbit.ocm.manager.objectconverter.impl.ObjectConverterImpl.retrieveBeanFields
> (ObjectConverterImpl.java:621)
> >       at
> >
> org.apache.jackrabbit.ocm.manager.objectconverter.impl.ObjectConverterImpl.getObject
> (ObjectConverterImpl.java:309)
> >       at
> >
> org.apache.jackrabbit.ocm.manager.impl.ObjectContentManagerImpl.getObject(
> ObjectContentManagerImpl.java:313)
> >       at com.pc.repository.Main.main(Main.java:345)






"
1,"NullPointerException when deleting a property of type REFERENCE. In method org.apache.jackrabbit.rmi.value.SerialValueFactory#createValue a NPE is thrown when parameter value is null.

Solution:

Change:

   public final Value createValue(Node value) throws RepositoryException {
        return createValue(value.getUUID(), PropertyType.REFERENCE);
    }

to

   public final Value createValue(Node value) throws RepositoryException {
        if (value == null) {
           return null;
        }
        
        return createValue(value.getUUID(), PropertyType.REFERENCE);
    }"
1,"spi2dav: Overwrite header T specified for MOVE and COPY causes failure if some API tests. failing tests are:

org.apache.jackrabbit.test.api.WorkspaceCopySameNameSibsTest#testCopyNodesNodeExistsAtDestPath
org.apache.jackrabbit.test.api.WorkspaceMoveSameNameSibsTest#testMoveNodesNodeExistsAtDestPath

those would be fixed by setting the overwrite header to F(alse)... however, this doesn't fit those cases where same-same
siblings would be allowed and the copy/move to a destination with existing item would succeed in JCR."
1,Scorer.skipTo() does not initialize hits. Some of the custom Scorer implementations in Jackrabbit do not initialize the internal hits BitSet if skipTo() is called before next().
1,"TCK: OrderByMultiTypeTest doesn't respect nodetype configuration property. OrderByMultiTypeTest creates test data by calling addNode(String).  This fails if there is no default primary type.

Proposal: call addNode(String, String)

--- OrderByMultiTypeTest.java   (revision 428760)
+++ OrderByMultiTypeTest.java   (working copy)
@@ -43,9 +43,9 @@
      * Tests order by queries with a String property and a long property.
      */
     public void testMultipleOrder() throws Exception {
-        Node n1 = testRootNode.addNode(nodeName1);
-        Node n2 = testRootNode.addNode(nodeName2);
-        Node n3 = testRootNode.addNode(nodeName3);
+        Node n1 = testRootNode.addNode(nodeName1, testNodeType);
+        Node n2 = testRootNode.addNode(nodeName2, testNodeType);
+        Node n3 = testRootNode.addNode(nodeName3, testNodeType);
  
         n1.setProperty(propertyName1, ""aaa"");
         n1.setProperty(propertyName2, 3);
"
1,"Concurrent Repository.login() throws IllegalStateException. See test case: org.apache.jackrabbit.test.core.ConcurrentLoginTest

java.lang.IllegalStateException: workspace 'default' not initialized
	at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.getItemStateProvider(RepositoryImpl.java:1448)
	at org.apache.jackrabbit.core.RepositoryImpl.getWorkspaceStateManager(RepositoryImpl.java:712)
	at org.apache.jackrabbit.core.SessionImpl.<init>(SessionImpl.java:247)
	at org.apache.jackrabbit.core.SessionImpl.<init>(SessionImpl.java:214)
	at org.apache.jackrabbit.core.XASessionImpl.<init>(XASessionImpl.java:98)
	at org.apache.jackrabbit.core.RepositoryImpl.createSessionInstance(RepositoryImpl.java:1233)
	at org.apache.jackrabbit.core.RepositoryImpl.createSession(RepositoryImpl.java:800)
	at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1119)
	at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1146)
	at org.apache.jackrabbit.core.jndi.BindableRepository.login(BindableRepository.java:181)
	at org.apache.jackrabbit.core.ConcurrentLoginTest$1.run(ConcurrentLoginTest.java:56)
	at java.lang.Thread.run(Thread.java:534)"
1,"Bad check for sv:name attribute presence in system view import. sax content handler checks the wrong variable to see if it's null in SysViewImportHandler

name vs. svName

patch fixes this"
1,"Duplicate key in DatabasePersistenceManager. Hi,

I ran into the exception pasted below. We had 2 threads that both were saving. Maybe it is a race condition?  

Regards,

Martijn Hendriks
<GX> creative online development B.V.
 
t: 024 - 3888 261
f: 024 - 3888 621
e: martijnh@gx.nl
 
Wijchenseweg 111
6538 SW Nijmegen
http://www.gx.nl/ 


Jan 26, 2007 2:23:36 PM org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager store
SEVERE: failed to write property state: e3847bad-f1ee-4adb-a109-e134900935b7/{http://gx.nl}edit_language
ERROR 23505: The statement was aborted because it would have caused a duplicate key value in a unique or primary key constraint or unique in dex identified by 'DEFAULT_PROP_IDX' defined on 'DEFAULT_PROP'.
        at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
        at org.apache.derby.impl.sql.execute.IndexChanger.insertAndCheckDups(Unknown Source)
        at org.apache.derby.impl.sql.execute.IndexChanger.doInsert(Unknown Source)
        at org.apache.derby.impl.sql.execute.IndexChanger.insert(Unknown Source)
        at org.apache.derby.impl.sql.execute.IndexSetChanger.insert(Unknown Source)
        at org.apache.derby.impl.sql.execute.RowChangerImpl.insertRow(Unknown Source)
        at org.apache.derby.impl.sql.execute.InsertResultSet.normalInsertCore(Unknown Source)
        at org.apache.derby.impl.sql.execute.InsertResultSet.open(Unknown Source)
        at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
        at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
        at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeStatement(Unknown Source)
        at org.apache.derby.impl.jdbc.EmbedPreparedStatement.execute(Unknown Source)
        at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.executeStmt(DatabasePersistenceManager.java:835)
        at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.store(DatabasePersistenceManager.java:466)
        at org.apache.jackrabbit.core.persistence.AbstractPersistenceManager.store(AbstractPersistenceManager.java:75)
        at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.store(DatabasePersistenceManager.java:274)
        at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:675)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:808)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:326)
        at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:313)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:302)
        at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:295)
        at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1210)
"
1,"UserInfo disapears after creating URI. I tested this using firefox (Where I have configured our proxy server)
I run the following URI: ftp://username:password@ftp.mytest.test/testdir/

I use a sniffer to look at the GET commond send to the proxy server. It looks as
follows:

GET ftp://username:password@ftp.mytest.test/testdir/ HTTP/1.1
Host: ftp.mytest.test
User-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.0; en-US; rv:1.7.12)
Gecko/20050915 Firefox/1.0.7
Accept:
text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain;q=0.8,image/png,*/*;q=0.5
Accept-Language: en-us,en;q=0.5
Accept-Encoding: gzip,deflate
Accept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7
Keep-Alive: 300
Proxy-Connection: keep-alive

Using this request we get access to the directory and see the contents displayed.
However, when I try the same in Java (using HttpClient) I get the following GET
request (Java code included below):

GET ftp://ftp.mytest.test/testdir/ HTTP/1.1
User-Agent: Jakarta Commons-HttpClient/3.0-rc3
Host: ftp.mytest.test
Proxy-Connection: Keep-Alive

Finally I get a ACCESS DENIED error. 
This seems to be because the GET request does not contain the USER / PASSWORD
info in the URL.

/// JAVA CODE:

package nl.essent.test.ftp.httptest;

import java.io.IOException;

import org.apache.commons.httpclient.Credentials;
import org.apache.commons.httpclient.DefaultHttpMethodRetryHandler;
import org.apache.commons.httpclient.HostConfiguration;
import org.apache.commons.httpclient.HttpClient;
import org.apache.commons.httpclient.HttpException;
import org.apache.commons.httpclient.HttpMethod;
import org.apache.commons.httpclient.HttpStatus;
import org.apache.commons.httpclient.NTCredentials;
import org.apache.commons.httpclient.UsernamePasswordCredentials;
import org.apache.commons.httpclient.auth.AuthScope;
import org.apache.commons.httpclient.methods.GetMethod;
import org.apache.commons.httpclient.params.HttpMethodParams;
import org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory;
import org.apache.commons.httpclient.protocol.Protocol;

public class TestClient {

    public static void main(String[] args) {
        new TestClient().testFtpViaHttp();
    }
    
    public void testFtpViaHttp() {
        
        HttpClient client = new HttpClient();
        
        HostConfiguration hostConfig = client.getHostConfiguration();
        hostConfig.setProxy(""proxy"", 8080);
        client.setHostConfiguration(hostConfig);
        
        Protocol protol = new Protocol(""ftp"", new
DefaultProtocolSocketFactory(), 21);
        Protocol.registerProtocol(""ftp"", protol);
        
        Credentials proxyCreds = new NTCredentials(""xxxx"", ""xxxxx"","""", ""xxxx"" );
        client.getState().setProxyCredentials(AuthScope.ANY, proxyCreds);
        
        GetMethod gmethod = new
GetMethod(""ftp://username:password@ftp.mytest.test/testdir/"");
        
        gmethod.getParams().setParameter(HttpMethodParams.RETRY_HANDLER, 
                new DefaultHttpMethodRetryHandler(3, false));
       
        try {
            // Execute the method.
            int statusCode = client.executeMethod(gmethod);

            if (statusCode != HttpStatus.SC_OK) {
              System.err.println(""Method failed: "" + gmethod.getStatusLine());
            }

            // Read the response body.
            byte[] responseBody = gmethod.getResponseBody();

            // Deal with the response.
            // Use caution: ensure correct character encoding and is not binary data
            System.out.println(new String(responseBody));

          } catch (HttpException e) {
            System.err.println(""Fatal protocol violation: "" + e.getMessage());
            e.printStackTrace();
          } catch (IOException e) {
            System.err.println(""Fatal transport error: "" + e.getMessage());
            e.printStackTrace();
          } finally {
            // Release the connection.
            gmethod.releaseConnection();
          } 

    }

}

//// END JAVA CODE"
1,"Dead code in SpellChecker.java (branch never executes). SpellChecker contains the following lines of code:

    final int goalFreq = (morePopular && ir != null) ? ir.docFreq(new Term(field, word)) : 0;
    // if the word exists in the real index and we don't care for word frequency, return the word itself
    if (!morePopular && goalFreq > 0) {
      return new String[] { word };
    }

The branch will never execute: the only way for goalFreq to be greater than zero is if morePopular is true, but if morePopular is true, the expression in the if statement evaluates to false.

"
1,"jcr:frozenUuid does not contain jcr:content. When I store versionable files, I get problems retrieving the jcr:data from a custom node type.

I am storing a node type:

xrc:learningContent
        pd: xrc:Keywords
        pd: xrc:MimeType
        pd: jcr:mixinTypes
        pd: xrc:Description
        pd: xrc:Language
        pd: xrc:Creator
        pd: jcr:created
        pd: xrc:Title
        pd: jcr:primaryType
Extends: nt:resource
        pd: jcr:uuid
        pd: jcr:mixinTypes
        pd: jcr:data
        pd: jcr:encoding
        pd: jcr:mimeType
        pd: jcr:lastModified
        pd: jcr:primaryType

So I commit the changes, then later pull up the version and get it's frozenNode.

Node frozenNode = v.getNode(JcrConstants.JCR_FROZENNODE);

And then I return all of the properties contained within:

PropertyIterator pi = frozenNode.getProperties();
                while (pi.hasNext()) {
                    System.out.println(pi.nextProperty().getName());
}


All that are returned are:

jcr:frozenUuid
jcr:uuid
jcr:frozenPrimaryType
jcr:frozenMixinTypes
jcr:primaryType

Here is the frozen node type:

nt:frozenNode
        pd: *
        pd: *
        pd: jcr:frozenUuid
        pd: jcr:uuid
        pd: jcr:mixinTypes
        pd: jcr:frozenPrimaryType
        pd: jcr:frozenMixinTypes
        pd: jcr:primaryType



So basically it would seem that the recursive copy inside the InternalFrozenNodeImpl is not working. But it seems that is not the case from the code trace I did. Add this to line 368 of InternalFrozenNodeImpl.java

System.out.println(""New node created. Props: "");
        try {
            PropertyState [] ps = node.getProperties();
            for (PropertyState p : ps) {
                System.out.println(p.getName());
                System.out.println(p.toString());
            }
            NodeStateEx [] ns = node.getChildNodes();
            for (NodeStateEx n : ns) {
                System.out.println(n.getName());
                System.out.println(n.toString());
            }
        } catch (ItemStateException e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        }


And you will get the result:


New node created. Props:
{http://www.jcp.org/jcr/1.0}uuid
org.apache.jackrabbit.core.state.PropertyState@10dd791
{http://www.jcp.org/jcr/1.0}frozenPrimaryType
org.apache.jackrabbit.core.state.PropertyState@1c38291
{http://www.jcp.org/jcr/1.0}frozenMixinTypes
org.apache.jackrabbit.core.state.PropertyState@b12fbb
{http://www.jcp.org/jcr/1.0}baseVersion
org.apache.jackrabbit.core.state.PropertyState@b4d4b6
{http://www.jcp.org/jcr/1.0}primaryType
org.apache.jackrabbit.core.state.PropertyState@1f9045f
{http://www.jcp.org/jcr/1.0}isCheckedOut
org.apache.jackrabbit.core.state.PropertyState@18e16b5
{http://www.jcp.org/jcr/1.0}frozenUuid
org.apache.jackrabbit.core.state.PropertyState@174cb00
{http://www.jcp.org/jcr/1.0}predecessors
org.apache.jackrabbit.core.state.PropertyState@bb7c1b
{http://www.jcp.org/jcr/1.0}data
org.apache.jackrabbit.core.state.PropertyState@d10133
{http://www.jcp.org/jcr/1.0}versionHistory
org.apache.jackrabbit.core.state.PropertyState@1a5f001
{http://www.jcp.org/jcr/1.0}encoding
org.apache.jackrabbit.core.state.PropertyState@12fe3ef
{http://www.jcp.org/jcr/1.0}mimeType
org.apache.jackrabbit.core.state.PropertyState@11d92c8
{http://www.jcp.org/jcr/1.0}lastModified
org.apache.jackrabbit.core.state.PropertyState@8fb83a
New node created. Props:
{http://www.xerceo.com/learn/jcr-1.0}Keywords
org.apache.jackrabbit.core.state.PropertyState@18808f3
{http://www.jcp.org/jcr/1.0}uuid
org.apache.jackrabbit.core.state.PropertyState@397a4
{http://www.jcp.org/jcr/1.0}frozenPrimaryType
org.apache.jackrabbit.core.state.PropertyState@1d88ffd
{http://www.xerceo.com/learn/jcr-1.0}Creator
org.apache.jackrabbit.core.state.PropertyState@d5625b
{http://www.xerceo.com/learn/jcr-1.0}Language
org.apache.jackrabbit.core.state.PropertyState@12c70e6
{http://www.xerceo.com/learn/jcr-1.0}Title
org.apache.jackrabbit.core.state.PropertyState@a836b3
{http://www.jcp.org/jcr/1.0}frozenMixinTypes
org.apache.jackrabbit.core.state.PropertyState@19f273c
{http://www.jcp.org/jcr/1.0}primaryType
org.apache.jackrabbit.core.state.PropertyState@1c8e97d
{http://www.jcp.org/jcr/1.0}frozenUuid
org.apache.jackrabbit.core.state.PropertyState@15915a3
{http://www.jcp.org/jcr/1.0}predecessors
org.apache.jackrabbit.core.state.PropertyState@19ba907
{http://www.xerceo.com/learn/jcr-1.0}MimeType
org.apache.jackrabbit.core.state.PropertyState@763ca1
{http://www.xerceo.com/learn/jcr-1.0}Description
org.apache.jackrabbit.core.state.PropertyState@8687e8
{http://www.jcp.org/jcr/1.0}versionHistory
org.apache.jackrabbit.core.state.PropertyState@44ca0f
{http://www.jcp.org/jcr/1.0}content
org.apache.jackrabbit.core.version.NodeStateEx@2da721

So the new Node definately has these new properties.

Do I have to somehow extend my frozenNode to work with this? Can anyone help me?"
1,"Null Pointer Exception while looking for a DavProperty that hasn't been set. Null pointer exception.
Exception occurs because the DavPropertySet.map does not contain an expected entry: ItemResourceConstants.JCR_NAME

Suggested fix: add the constant to the nameSet in RepositoryServiceImpl.java:760
 nameSet.add(ItemResourceConstants.JCR_NAME);

I tried that and it works. See stack trace at below.

Exception in thread ""main"" java.lang.NullPointerException
  at org.apache.jackrabbit.spi2dav.URIResolverImpl.buildPropertyId(URIResolverImpl.java:201)
  at org.apache.jackrabbit.spi2dav.RepositoryServiceImpl.getNodeInfo(RepositoryServiceImpl.java:808)
  at org.apache.jackrabbit.spi2dav.RepositoryServiceImpl.getItemInfos(RepositoryServiceImpl.java:834)
  at org.apache.jackrabbit.jcr2spi.state.WorkspaceItemStateFactory.createNodeState(WorkspaceItemStateFactory.java:88)
  at org.apache.jackrabbit.jcr2spi.state.TransientISFactory.createNodeState(TransientISFactory.java:99)
  at org.apache.jackrabbit.jcr2spi.hierarchy.NodeEntryImpl.doResolve(NodeEntryImpl.java:959)
  at org.apache.jackrabbit.jcr2spi.hierarchy.HierarchyEntryImpl.resolve(HierarchyEntryImpl.java:95)
  at org.apache.jackrabbit.jcr2spi.hierarchy.HierarchyEntryImpl.getItemState(HierarchyEntryImpl.java:212)
  at org.apache.jackrabbit.jcr2spi.ItemManagerImpl.getItem(ItemManagerImpl.java:157)
  at org.apache.jackrabbit.jcr2spi.SessionImpl.getRootNode(SessionImpl.java:225)
"
1,"BooleanQuery.rewrite does not work properly for minNumberShouldMatch. BooleanQuery.rewrite does not respect minNumberShouldMatch if the number of clauses is 1. This causes inconsistencies for the queries ""+def"" and ""+abc +def"", while setting the minNumShouldMatch to '1' for both.
For the first query, results are returned although there are no SHOULD clauses in the query.
For the second query no results are returned.
The reason lies in the optimization BooleanQuery.rewrite has for one clauses queries.
Patch included - optimize the query for a single clause only if the minNumShouldMatch <= 0."
1,"DataStore: garbage collection can fail when using workspace maxIdleTime. The GarbageCollectorTest fails because some workspaces have an idle timeout. The data store garbage collector should prevent workspace close-on-idle.

Proposed solution: instead of using the 'regular' system sessions in the garbage collector, use special 'registered system sessions'. The sessions get garbage collected when no longer used, that means this patch requires that JCR-1216 ""Unreferenced sessions should get garbage collected"" is applied. So for each workspace, the code is:

// this will initialize the workspace if required
wspInfo.getSystemSession();

SessionImpl session = SystemSession.create(rep, wspInfo.getConfig());
// mark this session as 'active' for so the workspace does
// not get disposed by workspace-janitor until the garbage collector is done
rep.onSessionCreated(session);            
"
1,"RepositoryException when using BindVariables in JCR-SQL2 CONTAINS. When using a BindVariable in a JCR-SQL2 CONTAINS constraint, the query fails with a RepositoryException.

For example:

String sql = ""SELECT * FROM [nt:unstructured] WHERE ISCHILDNODE([/testroot]) AND CONTAINS(mytext, $searchExpression)"";
Query q = superuser.getWorkspace().getQueryManager().createQuery(sql, Query.JCR_SQL2);
q.bindValue(""searchExpression"", superuser.getValueFactory().createValue(""fox""));
q.execute();

Results in:

javax.jcr.RepositoryException: Unknown static operand type: org.apache.jackrabbit.spi.commons.query.qom.BindVariableValueImpl@591a4d
        at org.apache.jackrabbit.core.query.lucene.LuceneQueryFactoryImpl.create(LuceneQueryFactoryImpl.java:215)
        at org.apache.jackrabbit.core.query.lucene.constraint.FullTextConstraint.<init>(FullTextConstraint.java:42)
        at org.apache.jackrabbit.core.query.lucene.constraint.ConstraintBuilder$Visitor.visit(ConstraintBuilder.java:175)
        at org.apache.jackrabbit.spi.commons.query.qom.FullTextSearchImpl.accept(FullTextSearchImpl.java:117)
        at org.apache.jackrabbit.core.query.lucene.constraint.ConstraintBuilder$Visitor.visit(ConstraintBuilder.java:137)
        at org.apache.jackrabbit.spi.commons.query.qom.AndImpl.accept(AndImpl.java:72)
        at org.apache.jackrabbit.core.query.lucene.constraint.ConstraintBuilder.create(ConstraintBuilder.java:82)
        at org.apache.jackrabbit.core.query.lucene.QueryObjectModelImpl.execute(QueryObjectModelImpl.java:109)
        at org.apache.jackrabbit.core.query.QueryImpl.execute(QueryImpl.java:127)

I tried to fix this issue but there is no way to access the BindVariables from the ConstraintBuilder from the LuceneQueryFactoryImpl and the ConstraintBuilder just passes the FullTextSearchImpl QOM subtree to the factory (via FullTextConstraint constructor) without any further visiting. If the signature would be ""LuceneQueryFactoryImpl#create(FullTextSearchImpl fts, Value searchExpression)"" we could visit the StaticOperand in the ConstraintBuilder and then modify the FullTextSearchImpl constructor accordingly, but this would imply that LuceneQueryFactory interface would need to be change accordingly and I don't know what that would mean."
1,"IndexOutOfBoundsException from FieldsReader after problem reading the index. There is a situation where there is an IOException reading from Hits, and then the next time you get a NullPointerException instead of an IOException.

Example stack traces:

java.io.IOException: The specified network name is no longer available
	at java.io.RandomAccessFile.readBytes(Native Method)
	at java.io.RandomAccessFile.read(RandomAccessFile.java:322)
	at org.apache.lucene.store.FSIndexInput.readInternal(FSDirectory.java:536)
	at org.apache.lucene.store.BufferedIndexInput.readBytes(BufferedIndexInput.java:74)
	at org.apache.lucene.index.CompoundFileReader$CSIndexInput.readInternal(CompoundFileReader.java:220)
	at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:93)
	at org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:34)
	at org.apache.lucene.store.IndexInput.readVInt(IndexInput.java:57)
	at org.apache.lucene.index.FieldsReader.doc(FieldsReader.java:88)
	at org.apache.lucene.index.SegmentReader.document(SegmentReader.java:344)
	at org.apache.lucene.index.IndexReader.document(IndexReader.java:368)
	at org.apache.lucene.search.IndexSearcher.doc(IndexSearcher.java:84)
	at org.apache.lucene.search.Hits.doc(Hits.java:104)

That error is fine.  The problem is the next call to doc generates:

java.lang.NullPointerException
	at org.apache.lucene.index.FieldsReader.getIndexType(FieldsReader.java:280)
	at org.apache.lucene.index.FieldsReader.addField(FieldsReader.java:216)
	at org.apache.lucene.index.FieldsReader.doc(FieldsReader.java:101)
	at org.apache.lucene.index.SegmentReader.document(SegmentReader.java:344)
	at org.apache.lucene.index.IndexReader.document(IndexReader.java:368)
	at org.apache.lucene.search.IndexSearcher.doc(IndexSearcher.java:84)
	at org.apache.lucene.search.Hits.doc(Hits.java:104)

Presumably FieldsReader is caching partially-initialised data somewhere.  I would normally expect the exact same IOException to be thrown for subsequent calls to the method.
"
1,"org.apache.commons.httpclient.HeaderElement fail to parse cookie header. if Set-Cookie header has value such ""expires=Mon, .."".
org.apache.commons.httpclient.HeaderElement will fail to parse this header.

Cause:
In the source cord:
-------------
            try {
                /*
                 * Following to RFC 2109 and 2965, in order not to conflict
                 * with the next header element, make it sure to parse tokens.
                 * the expires date format is ""Wdy, DD-Mon-YY HH:MM:SS GMT"".
                 * Notice that there is always comma(',') sign.
                 * For the general cases, rfc1123-date, rfc850-date.
                 */
                if (tokenizer.hasMoreTokens()) {
                    String s = nextToken.toLowerCase();
                    if (nextToken.endsWith(""mon"") 
                        || s.endsWith(""tue"")
                        || s.endsWith(""wed"") 
                        || s.endsWith(""thu"")
                        || s.endsWith(""fri"")
                        || s.endsWith(""sat"")
                        || s.endsWith(""sun"")
                        || s.endsWith(""monday"") 
                        || s.endsWith(""tuesday"") 
---- snip ---
 
""if (nextToken.endsWith(""mon"") "" is wrong.
this must be ""if (s.endsWith(""mon"") "".

Source cord version:
 * $Header:
/home/cvspublic/jakarta-commons/httpclient/src/java/org/apache/commons/httpclient/HeaderElement.java,v
1.17 2003/03/08 21:30:02 olegk Exp $
 * $Revision: 1.17 $
 * $Date: 2003/03/08 21:30:02 $"
1,"org.apache.http.impl.client.cache.memcached.MemcachedHttpCacheStorage should verify class of returned object before casting. org.apache.http.impl.client.cache.memcached.MemcachedHttpCacheStorage

Original (in getEntry function): 
  byte[] data = (byte[]) client.get(url);

Should be:
  Object obj= client.get(url);
  if (null == obj || !(objinstanceof byte[])) {
    return null;
  }
  byte[] data = (byte[])obj;


Original (in updateEntry function):
  byte[] oldBytes = (v != null) ? (byte[]) v.getValue() : null;

Should be:
  byte[] oldBytes = (v != null && (v.getValue() instanceof byte[])) ? (byte[]) v.getValue() : null;



  
"
1,Node.removeMixin fails if the mixin defines a protected child node. Node.removeMixin fails if the mixin to removed defines a protected child node.... the problem is caused by line 253 of RemoveMixinOperation.
1,"Exception handling in HttpClient requires redesign. When I use httpclient2.0-alpha3 and setTimeout(60000), after the specified 
time, I would like to see InterruptedIOException thrown, but I got 
HttpRecoverableException instead, which is pretty general. I would like to see 
the original exception. Thanks"
1,"DatabaseJournal improperly finds tables in external schemas when used on Oracle. The DatabaseJournal currently calls database metadata to determine if the journal table has already been created.  It uses the following code to do so:

ResultSet rs = metaData.getTables(null, null, tableName, null);

The Oracle driver sometimes will return the table if it is in another schema on the same database.  Other DBMS code within JackRabbit has a specific Oracle version that handles this case.  In order for the journal table to be properly created, Oracle databases will need the schema name included in the getTables() call."
1,"ParallelMultiSearcher should shut down thread pool on close. ParallelMultiSearcher does not shut down its internal thread pool on close. As a result, programs that create multiple instances of this class over their lifetime end up ""leaking"" threads."
1,"NodeTypeRegistry.registerNodeType(NodeTypeDef) does not verify that the referenced namespaces are registered. currently it's possible to register a node type using a defintion that contains references to unregistered namespaces.

using such a  node type in content would lead to unpredictable results."
1,"WebDAV/DaveX Servlets susceptible to CSRF Attacks. Both the WebDAV and the remoting (DaveX) servlets are susceptible to CSRF attacks.

"
1,"Add the org.apache.jackrabbit.rmi.jackrabbit package to the rmic generation . From the UnicastRemoteObject's (ServerJackrabbitNodeTypeManager, ServerJackrabbitWorkspace) should be stubs generated.
"
1,"URLEncodedUtils fails to parse form-url-encoded entities that specify a charset. If a form-url-encoded HTTP entity specifies a charset in its Content-Type header, then URLEncodedUtils.parse(HttpEntity) fails to parse it.

An entity with content type ""application/x-www-form-urlencoded; charset=UTF-8"" should be detected as form-url-encoded and parsed as such, honoring the specified character set. Currently the code requires an exact, case-insensitive match with ""application/x-www-form-urlencoded"" for an entity to be detected as form-url-encoded.

It appears that the author of URLEncodedUtils.parse(HttpEntity) tried to take character sets into account, but expected to find them in the Content-Encoding header instead of as a parameter in the Content-Length header. The HTTP 1.1 spec makes it clear that the Content-Encoding header is for specifying transformations like gzip compression or the identity transformation -- not for specifying the entity's character set.

Here are some helpful links.
http://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.4
http://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.5
http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.11

This is related to: https://issues.apache.org/jira/browse/HTTPCLIENT-884"
1,"Syns2Index fails. Running Syns2Index fails with a
java.lang.IllegalArgumentException: maxBufferedDocs must at least be 2 when enabled exception.
at org.apache.lucene.index.IndexWriter.setMaxBufferedDocs(IndexWriter.java:883)
at org.apache.lucene.wordnet.Syns2Index.index(Syns2Index.java:249)
at org.apache.lucene.wordnet.Syns2Index.main(Syns2Index.java:208)

The code is here
		// blindly up these parameters for speed
		writer.setMergeFactor( writer.getMergeFactor() * 2);
		writer.setMaxBufferedDocs( writer.getMaxBufferedDocs() * 2);

It looks like getMaxBufferedDocs used to return 10, and now it returns -1, not sure when that started happening.

My suggestion would be to just remove these three lines.  Since speed has already improved vastly, there isn't a need to speed things up.

To run this, Syns2Index requires two args.  The first is the location of the wn_s.pl file, and the second is the directory to create the index in."
1,"Memory leak when sorting. This is the same post I sended two days before to the Lucene user's list. This 
bug seems to have something in common with bug no. 30628 but that bug is closed 
as invalid.

I'm sending test code that everyone can try. The code is singular, don't say 
there is no sense in reopening the same index. I can only show, that reopening 
leaks memory. The index is filled by pseudo-real data, they aren't significant 
and the process of index creation as well. 

The problem must be in field caching code used by sort.

Affected versions of Lucene:
1.4.1
CVS 1.5-rc1-dev

This code survives only few first iterations if you run java with -Xmx5m. With 
Lucene 1.4-final ends regulary.

import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.document.Document;
import org.apache.lucene.document.Field;
import org.apache.lucene.index.IndexReader;
import org.apache.lucene.index.IndexWriter;
import org.apache.lucene.index.Term;
import org.apache.lucene.search.Hits;
import org.apache.lucene.search.IndexSearcher;
import org.apache.lucene.search.Searcher;
import org.apache.lucene.search.Sort;
import org.apache.lucene.search.SortField;
import org.apache.lucene.search.TermQuery;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.RAMDirectory;

import java.io.IOException;
import java.text.SimpleDateFormat;
import java.util.Calendar;
import java.util.Date;

/**
 * Run this test with Lucene 1.4.1 and -Xmx5m
 */
public class ReopenTest
{
    private static long mem_last = 0;

    public static void main(String[] args) throws IOException
    {
        Directory directory = create_index();

        for (int i = 1; i < 100; i++) {
            System.err.println(""loop "" + i + "", index version: "" + IndexReader.
getCurrentVersion(directory));
            search_index(directory);
            add_to_index(directory, i);
        }
    }

    private static void add_to_index(Directory directory, int i) throws 
IOException
    {
        IndexWriter writer = new IndexWriter(directory, new StandardAnalyzer(), 
false);

        SimpleDateFormat df = new SimpleDateFormat(""yyyy-MM-dd"");
        Document doc = new Document();

        doc.add(Field.Keyword(""date"", 
          df.format(new Date(System.currentTimeMillis()))));
        doc.add(Field.Keyword(""id"", ""CD"" + String.valueOf(i)));
        doc.add(Field.Text(""text"", ""Tohle neni text "" + i));
        writer.addDocument(doc);

        System.err.println(""index size: "" + writer.docCount());
        writer.close();
    }

    private static void search_index(Directory directory) throws IOException
    {
        IndexReader reader = IndexReader.open(directory);
        Searcher searcher = new IndexSearcher(reader);

        print_mem(""search 1"");
        SortField[] fields = new SortField[2];
        fields[0] = new SortField(""date"", SortField.STRING, true);
        fields[1] = new SortField(""id"", SortField.STRING, false);
        Sort sort = new Sort(fields);
        TermQuery query = new TermQuery(new Term(""text"", ""\""text 5\""""));

        print_mem(""search 2"");
        Hits hits = searcher.search(query, sort);
        print_mem(""search 3"");

        for (int i = 0; i < hits.length(); i++) {
            Document doc = hits.doc(i);
            System.out.println(""doc "" + i + "": "" + doc.toString());
        }
        print_mem(""search 4"");
        searcher.close();
        reader.close();
    }

    private static void print_mem(String log)
    {
        long mem_free = Runtime.getRuntime().freeMemory();
        long mem_total = Runtime.getRuntime().totalMemory();
        long mem_max = Runtime.getRuntime().maxMemory();

        long delta = (mem_last - mem_free) * -1;

        System.out.println(log + ""= delta: "" + delta + "", free: "" + mem_free + 
"", used: "" + (mem_total-mem_free) + "", total: "" + mem_total + "", max: "" + 
mem_max);

        mem_last = mem_free;
    }

    private static Directory create_index() throws IOException
    {
        print_mem(""create 1"");
        Directory directory = new RAMDirectory();

        Calendar c = Calendar.getInstance();
        SimpleDateFormat df = new SimpleDateFormat(""yyyy-MM-dd"");
        IndexWriter writer = new IndexWriter(directory, new StandardAnalyzer(), 
true);
        for (int i = 0; i < 365 * 15; i++) {
            Document doc = new Document();

            doc.add(Field.Keyword(""date"", 
               df.format(new Date(c.getTimeInMillis()))));
            doc.add(Field.Keyword(""id"", ""AB"" + String.valueOf(i)));
            doc.add(Field.Text(""text"", ""Tohle je text "" + i));
            writer.addDocument(doc);

            doc = new Document();

            doc.add(Field.Keyword(""date"", 
               df.format(new Date(c.getTimeInMillis()))));
            doc.add(Field.Keyword(""id"", ""ef"" + String.valueOf(i)));
            doc.add(Field.Text(""text"", ""Je tohle text "" + i));
            writer.addDocument(doc);

            c.add(Calendar.DAY_OF_YEAR, 1);
        }
        writer.optimize();
        System.err.println(""index size: "" + writer.docCount());
        writer.close();

        print_mem(""create 2"");
        return directory;
    }
}"
1,"DataStore.close() is never called. I've searched through the jackrabbit-core code and never found a call to DataStore.close(), although the method exists on the DataStore interface"
1,"FilteredDocIdSet does not handle a case where the inner set iterator is null. DocIdSet#iterator is allowed to return null, when used in FilteredDocIdSet, if null is returned from the inner set, the FilteredDocIdSetIterator fails since it does not allow for nulls to be passed to it.

The fix is simple, return null in FilteredDocIdSet in the iterator method is the iterator is null."
1,"AttributeSource can have an invalid computed state. If you work a tokenstream, consume it, then reuse it and add an attribute to it, the computed state is wrong.
thus for example, clearAttributes() will not actually clear the attribute added.

So in some situations, addAttribute is not actually clearing the computed state when it should.
"
1,"Namespace comparison in Namespace.java doesn't work, if a node return null, but it expects an empty string. If a node returns null for Node.getNamespaceUri(), but Namespace.EMPTY_NAMESPACE expects an empty string, the comparison fails. But to my knowledge a null and """" namespaceUri should be treated the same.

"
1,"Highlight fragment does not extend to maxDocCharsToAnalyze. The current highlighter code checks whether the total length of the text to highlight is strictly smaller than maxDocCharsToAnalyze before adding any text remaining after the last token to the fragment. This means that if maxDocCharsToAnalyse is set to exactly the length of the text and the last token of the text is the term to highlight and is followed by non-token text, this non-token text will not be highlighted.

For example, consider the phrase ""this is a text with searchterm in it"". ""In"" and ""it"" are not tokenized because they're stopwords. Setting maxDocCharsToAnalyze to 36 (the length of the sentence) and searching for ""searchterm"" gives a fragment ending in ""searchterm"". The expected behaviour is to have ""in it"" at the end of the fragment, since maxDocCharsToAnalyse explicitely states that the whole phrase should be considered."
1,"EnwikiContentSource does not properly identify the name/id of the Wikipedia article. The EnwikiContentSource does not properly identify the id (name in benchmark parlance) of the documents.  It currently produces assigns the id on the last <id> tag it sees in the document, as opposed to the id of the document.  Most documents have multiple <id> tags in them.  This prevents the ContentSource from being used effectively in producing documents for updating.

Example doc:
{quote}
<page>
    <title>AlgeriA</title>
    <id>5</id>
    <revision>
      <id>133452200</id>
      <timestamp>2007-05-25T17:11:48Z</timestamp>
      <contributor>
        <username>Gurch</username>
        <id>241822</id>
      </contributor>
      <minor />
      <comment>[[WP:AES|<86><90>]]Redirected page to [[Algeria]]</comment>
      <text xml:space=""preserve"">#REDIRECT [[Algeria]] {{R from CamelCase}}</text>
    </revision>
  </page>
{quote}

In this case, the getName() return 241822 instead of 5.  page/id is unique according to the schema at  http://www.mediawiki.org/xml/export-0.3.xsd, so we should just get that one."
1,"need a test that uses termsenum.seekExact() (which returns true), then calls next(). i tried to do some seekExact (where the result must exist) then next()ing in the faceting module,
and it seems like there could be a bug here.

I think we should add a test that mixes seekExact/seekCeil/next like this, to ensure that
if seekExact returns true, that the enum is properly positioned."
1,"JCR2SPI; setProperty(name, date-string) fails when property is added and property type is PropertyType.DATE.. Example code:

        Node l_parent = (Node)session.getItem(this.m_path);
        
        Node l_test = l_parent.addNode(""createcontenttest"", ""nt:file"");
        Node l_content = l_test.addNode(""jcr:content"", ""nt:resource"");
        
        l_content.setProperty(""jcr:encoding"", ""UTF-8"");
        l_content.setProperty(""jcr:mimeType"", ""text/plain"");
        l_content.setProperty(""jcr:data"", new ByteArrayInputStream(""foobar"".getBytes()));
        l_content.setProperty(""jcr:lastModified"", ""2007-07-25T17:04:00.000Z""); // TODO: this should work as well, bug in JCR2SPI?
        session.save();

This will fail when the property is defined as DATE, what should happen is that a value comparison is attempted (note that it works when the property already exists and just is overwritten).

The exception is:

javax.jcr.nodetype.ConstraintViolationException: no matching property definition found for {http://www.jcp.org/jcr/1.0}lastModified
        at org.apache.jackrabbit.jcr2spi.nodetype.ItemDefinitionProviderImpl.getQPropertyDefinition(ItemDefinitionProviderImpl.java:269)
        at org.apache.jackrabbit.jcr2spi.nodetype.ItemDefinitionProviderImpl.getQPropertyDefinition(ItemDefinitionProviderImpl.java:159)
        at org.apache.jackrabbit.jcr2spi.NodeImpl.getApplicablePropertyDefinition(NodeImpl.java:1672)
        at org.apache.jackrabbit.jcr2spi.NodeImpl.createProperty(NodeImpl.java:1369)
        at org.apache.jackrabbit.jcr2spi.NodeImpl.setProperty(NodeImpl.java:264)
        at org.apache.jackrabbit.jcr2spi.NodeImpl.setProperty(NodeImpl.java:345)
        at org.apache.jackrabbit.jcr2spi.NodeImpl.setProperty(NodeImpl.java:336)
"
1,"CartesianPolyFilterBuilder doesn't handle edge case around the 180 meridian. Test case:  
Points all around the globe, plus two points at 0, 179.9 and 0,-179.9 (on each side of the meridian).  Then, do a Cartesian Tier filter on a point right near those two.  It will return all the points when it should just return those two.

The flawed logic is in the else clause below:
{code}
if (longX2 != 0.0) {
		//We are around the prime meridian
		if (longX == 0.0) {
			longX = longX2;
			longY = 0.0;
        	shape = getShapeLoop(shape,ctp,latX,longX,latY,longY);
		} else {//we are around the 180th longitude
			longX = longX2;
			longY = -180.0;
			shape = getShapeLoop(shape,ctp,latY,longY,latX,longX);
	}
{code}

Basically, the Y and X values are transposed.  This currently says go from longY (-180) all the way around  to longX which is the lower left longitude of the box formed.  Instead, it should go from the lower left long to -180."
1,Stale connections are never detected when wireLog.isDebugEnabled() == true. When wireLog.isDebugEnabled() == true SessionInputBuffer is wrapped with LoggingSessionInputBuffer which doesn't implement EofSensor that AbstractHttpClientConnection.isStale() is relying on. This causes stale connections to be never detected and attempted to use.
1,Jcr-Server: ItemDefinitionImpl.toXml throws NPE for the root node.. ItemDefinitionImpl.toXml throws NPE for the root node due to a missing assertion regarding the declaring nodetype.
1,"CheckIndex incorrectly sees deletes as index corruption. There is a silly bug in CheckIndex whereby any segment with deletes is
considered corrupt.

Thanks to Bogdan Ghidireac for reporting this."
1,"IndexOutOfBoundsException at ShingleMatrixFilter's Iterator#hasNext method. I tried to use the ShingleMatrixFilter within Solr. To test the functionality etc., I first used the built-in field analysis view.The filter was configured to be used only at query time analysis with ""_"" as spacer character and a min. and max. shingle size of 2. The generation of the shingles for query strings with this filter seems to work at this view, but by turn on the highlighting of indexed terms that will match the query terms, the exception was thrown. Also, each time I tried to query the index the exception was immediately thrown.

Stacktrace:
{code}
java.lang.IndexOutOfBoundsException: Index: 1, Size: 1
	at java.util.ArrayList.RangeCheck(Unknown Source)
	at java.util.ArrayList.get(Unknown Source)
	at org.apache.lucene.analysis.shingle.ShingleMatrixFilter$Matrix$1.hasNext(ShingleMatrixFilter.java:729)
	at org.apache.lucene.analysis.shingle.ShingleMatrixFilter.next(ShingleMatrixFilter.java:380)
	at org.apache.lucene.analysis.StopFilter.next(StopFilter.java:120)
	at org.apache.lucene.analysis.TokenStream.next(TokenStream.java:47)
	...
{code}

Within the hasNext method, there is the {{s-1}}-th Column from the ArrayList {{columns}} requested, but there isn't this entry within columns.

I created a patch that checks, if {{columns}} contains enough entries."
1,"RegexQuery matches terms the input regex doesn't actually match. I was writing some unit tests for our own wrapper around the Lucene regex classes, and got tripped up by something interesting.

The regex ""cat."" will match ""cats"" but also anything with ""cat"" and 1+ following letters (e.g. ""cathy"", ""catcher"", ...)  It is as if there is an implicit .* always added to the end of the regex.

Here's a unit test for the behaviour I would expect myself:

    @Test
    public void testNecessity() throws Exception {
        File dir = new File(new File(System.getProperty(""java.io.tmpdir"")), ""index"");
        IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(), true);
        try {
            Document doc = new Document();
            doc.add(new Field(""field"", ""cat cats cathy"", Field.Store.YES, Field.Index.TOKENIZED));
            writer.addDocument(doc);
        } finally {
            writer.close();
        }

        IndexReader reader = IndexReader.open(dir);
        try {
            TermEnum terms = new RegexQuery(new Term(""field"", ""cat."")).getEnum(reader);
            assertEquals(""Wrong term"", ""cats"", terms.term());
            assertFalse(""Should have only been one term"", terms.next());
        } finally {
            reader.close();
        }
    }

This test fails on the term check with terms.term() equal to ""cathy"".

Our workaround is to mangle the query like this:

    String fixed = String.format(""(?:%s)$"", original);
"
1,"IndexReader.termDocs() retrieves no documents. TermDocs object returned by indexReader.termDocs() retrieves no documents, howerver, the documents are retrieved correctly when using indexReader.termDocs(Term), indexReader.termDocs(null) and indexSearcher.search(Query)."
1,"Incorrect parsing by QueryParser.parse() when it encounters backslashes (always eats one backslash.). Test code and output follow. Tested  Lucene 1.9 version only. Affects hose who would index/search for Lucene's reserved characters.

Description: When an input search string has a sequence of N (java-escaped) backslashes, where N >= 2, the QueryParser will produce a query in which that sequence has N-1 backslashes.

TEST CODE:
    Analyzer analyzer = new WhitespaceAnalyzer();
    String[] queryStrs = {""item:\\\\"",
                          ""item:\\\\*"",
                          ""(item:\\\\ item:ABCD\\\\))"",
                          ""(item:\\\\ item:ABCD\\\\)""};
    for (String queryStr : queryStrs) {
      System.out.println(""--------------------------------------"");
      System.out.println(""String queryStr = "" + queryStr);
      Query luceneQuery = null;
      try {
        luceneQuery = new QueryParser(""_default_"", analyzer).parse(queryStr);
        System.out.println(""luceneQuery.toString() = "" + luceneQuery.toString());
      } catch (Exception e) {
        System.out.println(e.getClass().toString());
      }
    }

OUTPUT (with remarks in comment notation:) 
--------------------------------------
String queryStr = item:\\
luceneQuery.toString() = item:\             //One backslash has disappeared. Searcher will fail on this query.
--------------------------------------
String queryStr = item:\\*
luceneQuery.toString() = item:\*           //One backslash has disappeared. This query will search for something unintended.
--------------------------------------
String queryStr = (item:\\ item:ABCD\\))
luceneQuery.toString() = item:\ item:ABCD\)     //This should have thrown a ParseException because of an unescaped ')'. It did not.
--------------------------------------
String queryStr = (item:\\ item:ABCD\\)
class org.apache.lucene.queryParser.ParseException        //...and this one should not have, but it did.

"
1,"REMOVE access is not checked when moving a node. When a node cannot be removed because AccessManager does not allow this, it still can be moved (using Session.move())."
1,"Inconsistent behaviour sorting against field with no related documents. In StringSortedHitQueue - generateSortIndex seems to mistake 
the TermEnum having values as indicating that the sort field 
has entries in the index.

In the case where the search has matching results an ArrayIndexOutOfBounds
exception is thrown in sortValue (line 177 StringSortedHitQueue)
as generateSortIndex creates a terms array of zero length and fieldOrder
contains 0 for all documents.

It would seem more helpful if:
a) generateSortIndex catches the lack of any documents with the sort field.

or

b) reserve terms[0] as a special value for documents that do not have
matching sort field values. ie Change the current implementation to add 1
to the index and change terms[0] to ensure it sorts ""untagged"" documents to
first or last.

For my application Id much prefer solution (b) as it allows much smaller 
indexes and make searching using sort values less brittle.

Thats the best my communication skills can muster just now. Could change
current code to something like:

private final int[] generateSortIndex()
throws IOException {

	final int[] retArray = new int[reader.maxDoc()];
	final String[] mterms = new String[reader.maxDoc() + 1];  // guess length
	if (retArray.length > 0) {
		TermDocs termDocs = reader.termDocs();
		// change this value to control if documents without sort field come first or last
		mterms[0] = """";  // XXXXXXXXX change
		int t = 1;  // current term number  XXXXXXXXXXXXX change
		try {
	

			do {
				Term term = enumerator.term();
				if (term.field() != field) break;

				// store term text
				// we expect that there is at most one term per document
				if (t >= mterms.length) throw new RuntimeException (""there are more terms
than documents in field \""""+field+""\"""");
				mterms[t] = term.text();

				// store which documents use this term
				termDocs.seek (enumerator);
				while (termDocs.next()) {
					retArray[termDocs.doc()] = t;
				}

				t++;
			} while (enumerator.next());
		} finally {
			termDocs.close();
		}

		// if there are less terms than documents,
		// trim off the dead array space
		if (t < mterms.length) {
			terms = new String[t];
			System.arraycopy (mterms, 0, terms, 0, t);
		} else {
			terms = mterms;
		}
	}
	return retArray;
}

Having very quick look at IntegerSortedHitQueue would seem possible
to do same thing. Maybe creating Integer wrapper objects once.

Hope that made some sort of sense. Im not very familiar with the code
or Lucene terminology.
If the above seems like a useful approach Id be glad to generate patches
for a cleaned up version.

Thanks

Sam"
1,"TestIndexWriterException fails with NPE on realtime. {noformat}
   [junit] Testsuite: org.apache.lucene.index.TestIndexWriterExceptions
    [junit] Testcase: testRandomExceptionsThreads(org.apache.lucene.index.TestIndexWriterExceptions):	Caused an ERROR
    [junit] (null)
    [junit] java.lang.NullPointerException
    [junit] 	at org.apache.lucene.index.DocumentsWriterPerThread.prepareFlush(DocumentsWriterPerThread.java:329)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:378)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:512)
    [junit] 	at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:2619)
    [junit] 	at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:2594)
    [junit] 	at org.apache.lucene.index.IndexWriter.prepareCommit(IndexWriter.java:2464)
    [junit] 	at org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:2537)
    [junit] 	at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2519)
    [junit] 	at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2503)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions.testRandomExceptionsThreads(TestIndexWriterExceptions.java:230)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1226)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1154)
    [junit] 
    [junit] 
    [junit] Tests run: 18, Failures: 0, Errors: 1, Time elapsed: 22.548 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterExceptions -Dtestmethod=testRandomExceptionsThreads -Dtests.seed=-5079747362001734044:1572064802119081373
    [junit] WARNING: test method: 'testRandomExceptionsThreads' left thread running: merge thread: _25(4.0):cv2/1 _29(4.0):cv2/1 _20(4.0):cv3/1 into _2m
    [junit] RESOURCE LEAK: test method: 'testRandomExceptionsThreads' left 1 thread(s) running
    [junit] NOTE: test params are: codec=RandomCodecProvider: {content=Pulsing(freqCutoff=2), field=MockSep, id=Pulsing(freqCutoff=2), other=MockSep, contents=SimpleText, content1=MockSep, content2=SimpleText, content4=MockRandom, content5=MockRandom, content6=MockVariableIntBlock(baseBlockSize=41), crash=Standard, content7=MockFixedIntBlock(blockSize=1633)}, locale=en_GB, timezone=Europe/Vaduz
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestToken, TestDateTools, Test2BTerms, TestAddIndexes, TestFilterIndexReader, TestIndexWriterExceptions]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=8,threads=1,free=155417240,total=292945920
    [junit] ------------- ---------------- ---------------
{noformat}"
1,ChainedFilter does not work well in the event of filters in ANDNOT. First ANDNOT operation takes place against a completely false bitset and will always return zero results.  
1,JCA build failure with J2EE 1.3. The fix to JCR-736 introduced a similar problem as was previously reported in JCR-413. The fix in JCR-413 should apply also to this case.
1,"new QueryParser over-increment position for MultiPhraseQuery. If the new QP is parsing a phrase, and when the analyzer runs on the text within the phrase it produces some tokens with posIncr=0, a MultiPhraseQuery is produced.  But, the positions of the added terms are over-incremented, and don't match what the current QueryParser does."
1,TermsFilter.getDocIdSet(context) NPE on missing field. If the context does not contain the field for a term when calling TermsFilter.getDocIdSet(AtomicReaderContext context) then a NullPointerException is thrown due to not checking for null Terms before getting iterator.
1,"VolatileIndex not closed properly. The MultiIndex.resetVolatileIndex() method doesn't properly close the existing VolatileIndex instance before creating a new one. This can confuse the DynamicPooledExecutor reference count added in JCR-2836, leading to a background thread leak."
1,"Change value of 'Expect' header in org.apache.http.client.methods.HttpPost. see original report at http://code.google.com/p/android/issues/detail?id=7208.

i'm going to apply the obvious patch to Android:

diff --git a/src/org/apache/http/params/CoreProtocolPNames.java b/src/org/apache/http/params/CoreProtocolPNames.java
index a42c5de..a0a726d 100644
--- a/src/org/apache/http/params/CoreProtocolPNames.java
+++ b/src/org/apache/http/params/CoreProtocolPNames.java
@@ -94,8 +94,8 @@ public interface CoreProtocolPNames {
 
     /**
      * <p>
-     * Activates 'Expect: 100-Continue' handshake for the 
-     * entity enclosing methods. The purpose of the 'Expect: 100-Continue'
+     * Activates 'Expect: 100-continue' handshake for the
+     * entity enclosing methods. The purpose of the 'Expect: 100-continue'
      * handshake to allow a client that is sending a request message with 
      * a request body to determine if the origin server is willing to 
      * accept the request (based on the request headers) before the client
diff --git a/src/org/apache/http/protocol/HTTP.java b/src/org/apache/http/protocol/HTTP.java
index de76ca6..9223955 100644
--- a/src/org/apache/http/protocol/HTTP.java
+++ b/src/org/apache/http/protocol/HTTP.java
@@ -60,7 +60,7 @@ public final class HTTP {
     public static final String SERVER_HEADER = ""Server"";
     
     /** HTTP expectations */
-    public static final String EXPECT_CONTINUE = ""100-Continue"";
+    public static final String EXPECT_CONTINUE = ""100-continue"";
 
     /** HTTP connection control */
     public static final String CONN_CLOSE = ""Close"";
"
1,"Constants causing NullPointerException when fetching metadata ""Implementation Version"" in MANIFEST. If the MANIFEST.MF file does not contain the metadata IMPLEMENTATION_VERSION, a null value is returned, causing NullPointerException during commit:

Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.lucene.store.IndexOutput.writeString(IndexOutput.java:109)
	at org.apache.lucene.store.IndexOutput.writeStringStringMap(IndexOutput.java:229)
	at org.apache.lucene.index.SegmentInfo.write(SegmentInfo.java:558)
	at org.apache.lucene.index.SegmentInfos.write(SegmentInfos.java:337)
	at org.apache.lucene.index.SegmentInfos.prepareCommit(SegmentInfos.java:808)
	at org.apache.lucene.index.IndexWriter.startCommit(IndexWriter.java:5319)
	at org.apache.lucene.index.IndexWriter.prepareCommit(IndexWriter.java:3895)
	at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:3956)

This happened after having build a jar assembly using Maven, the original MANIFEST.MF of lucene jar has been overwritten, and didn't contain anynore the implementation version metadata.

Path attached."
1,"TestMinimize.testAgainstBrzozowski reproducible seed OOM. {code}
    [junit] Testsuite: org.apache.lucene.util.automaton.TestMinimize
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 3.792 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestMinimize -Dtestmethod=testAgainstBrzozowski -Dtests.seed=-7429820995201119781:1013305000165135537
    [junit] NOTE: test params are: codec=PreFlex, locale=ru, timezone=America/Pangnirtung
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestMinimize]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=294745976,total=310378496
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testAgainstBrzozowski(org.apache.lucene.util.automaton.TestMinimize):     Caused an ERROR
    [junit] Java heap space
    [junit] java.lang.OutOfMemoryError: Java heap space
    [junit]     at java.util.BitSet.initWords(BitSet.java:144)
    [junit]     at java.util.BitSet.<init>(BitSet.java:139)
    [junit]     at org.apache.lucene.util.automaton.MinimizationOperations.minimizeHopcroft(MinimizationOperations.java:85)
    [junit]     at org.apache.lucene.util.automaton.MinimizationOperations.minimize(MinimizationOperations.java:52)
    [junit]     at org.apache.lucene.util.automaton.RegExp.toAutomaton(RegExp.java:502)
    [junit]     at org.apache.lucene.util.automaton.RegExp.toAutomatonAllowMutate(RegExp.java:478)
    [junit]     at org.apache.lucene.util.automaton.RegExp.toAutomaton(RegExp.java:428)
    [junit]     at org.apache.lucene.util.automaton.AutomatonTestUtil.randomAutomaton(AutomatonTestUtil.java:256)
    [junit]     at org.apache.lucene.util.automaton.TestMinimize.testAgainstBrzozowski(TestMinimize.java:43)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1282)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1211)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.util.automaton.TestMinimize FAILED


{code}"
1,"Bug in UtilDateTypeConverterImpl. In this converter following line is used:
return this.getValueFactory().createValue(((java.util.Date) propValue).getTime());
but propValue must be converted to java.util.Calendar, not into long! ValueFactory than converts to LongValue not DateValue as expected.

Following code works OK:
final long timeInMilis = ((java.util.Date) propValue).getTime();
final Calendar calendar = Calendar.getInstance();
calendar.setTimeInMillis(timeInMilis);
return this.getValueFactory().createValue( calendar );

but I dont know better Date-> Calendar conversion.
"
1,"User-Agent string violates RFC. Our User-Agent says ""Jakarta Commons-HttpClient/3.1-rc1"". But space is a reserved character to separate individual *products* and comments according to RFC 2616, section 14.43. Jakarta is not a product. At the same time we may want to drop the Jakarta name altogether.

We should change this to something more standard like: 

""Apache-HttpClient/3.1-rc1 (""+ System.getProperty(""os.name"") +"";""+ System.getProperty(""os.arch"") +"") ""+
""Java/""+ System.getProperty(""java.vm.version"") +"" (""+ System.getProperty(""java.vm.vendor"") +"")""

which renders:

""Apache-HttpClient/3.1-rc1 (Windows XP 5.1;x86) Java/1.5.0_08 (Sun Microsystems Inc.)""

Sun's internal Http client uses something like ""Java/1.5.0_08"".

I am completely ignoring the fact that real-world user agents use almost arbitrary strings.
Some fine examples of misbehaviour from my private logs:

""Jakmpqes dihurxf wfyiupsc"" -- apparently somebody has to hide something...
""Missigua Locator 1.9""
""Poodle predictor 1.0""
""shelob v1.0""
""ISC Systems iRc Search 2.1""
""ping.blogug.ch aggregator 1.0""
""http://www.uni-koblenz.de/~flocke/robot-info.txt""  -- ...sigh

I am very tempted to write a User-Agent string validator that prevents misuse of this field in HttpClient."
1,"UserManagement: IndexNodeResolver.findNodes(..... , nonExact) fails to find values containing backslash. "
1,"NTCollectionConverterImpl throws a null pointer exception on update. When calling update on a node which has no child nodes stored (but which can have child nodes) the code can generate a null pointer exception. In the case where one goes to remove JCR nodes which are not present in the current objects collection of child objects the code is calling getCollectionNodes().iterator(). However, since is not checking for the case where getCollectionNodes() returns null if there are no child nodes present a null pointer exception will be generated. "
1,Incorrect results from joins on multivalued properties. It looks like join conditions on multivalued properties only use one of the multiple values for the comparison.
1,"MultipartPost closes input stream. This is something of a collection of issues that are all interrelated.

1. MultipartPost calls close on the outputstream it retrieved from 
HttpConnection which causes an exception to be thrown later on.  This call 
should be replaced with a call to flush().

2. The MultipartPost classes do not have any logging in them.  We should add 
trace statements at a minimum.

3. new FilePart(String, File) throws a null pointer exception.

4. The tests in TestPartsNoHost are broken.

I'll attach patches for these fixes in a moment, broken down as much as 
possible."
1,"builtin_nodetypes.cnd contains non-ascii chars and is not correctly decoded using websphere. jackrabbit can't be started within websphere with the following error:

Caused by: 
org.apache.jackrabbit.spi.commons.nodetype.compact.ParseException: IOException
while attempting to read input stream
(org/apache/jackrabbit/core/nodetype/builtin_nodetypes.cnd, line 272)
.
.
Caused by: 
sun.io.MalformedInputException
	at sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:262)
	at sun.nio.cs.StreamDecoder$ConverterSD.convertInto(StreamDecoder.java:314)
	at sun.nio.cs.StreamDecoder$ConverterSD.implRead(StreamDecoder.java:345)


I quickly checked the file and it contains some non ascii-7 characters. actually it should be stored, packaged and read as utf8. somthing was list in translation.
the simplest fix would be to remove all non-ascii7 characters from the file."
1,"RTF text extractor fails on Java 1.4 in some environments. I've seen the RTF text extractor fail with the following errors with Java 1.4 on Unix platforms. Both are platform issues, but Jackrabbit should be prepared for such cases and for example just log a warning and return an empty text stream when encountering these errors.

java.lang.UnsatisfiedLinkError: /home/jukka/bin/java/j2sdk1.4.2_18/jre/lib/i386/libawt.so: libXp.so.6: cannot open shared object file: No such file or directory
        at java.lang.ClassLoader$NativeLibrary.load(Native Method)
        at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1586)
        at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1503)
        at java.lang.Runtime.loadLibrary0(Runtime.java:788)
        at java.lang.System.loadLibrary(System.java:834)
        at sun.security.action.LoadLibraryAction.run(LoadLibraryAction.java:50)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.awt.NativeLibLoader.loadLibraries(NativeLibLoader.java:38)
        at sun.awt.DebugHelper.<clinit>(DebugHelper.java:29)
        at java.awt.EventQueue.<clinit>(EventQueue.java:83)
        at javax.swing.SwingUtilities.isEventDispatchThread(SwingUtilities.java:1238)
        at javax.swing.text.StyleContext.reclaim(StyleContext.java:419)
        at javax.swing.text.StyleContext.addAttribute(StyleContext.java:276)
        at javax.swing.text.StyleContext$NamedStyle.addAttribute(StyleContext.java:1468)
        at javax.swing.text.StyleContext$NamedStyle.setName(StyleContext.java:1278)
        at javax.swing.text.StyleContext$NamedStyle.<init>(StyleContext.java:1226)
        at javax.swing.text.StyleContext.addStyle(StyleContext.java:88)
        at javax.swing.text.StyleContext.<init>(StyleContext.java:68)
        at javax.swing.text.DefaultStyledDocument.<init>(DefaultStyledDocument.java:88)
        at org.apache.jackrabbit.extractor.RTFTextExtractor.extractText(RTFTextExtractor.java:60)
        at org.apache.jackrabbit.extractor.RTFTextExtractorTest.testExtractor(RTFTextExtractorTest.java:35)

java.lang.InternalError: Can't connect to X11 window server using ':0.0' as the value of the DISPLAY variable.
	at sun.awt.X11GraphicsEnvironment.initDisplay(Native Method)
	at sun.awt.X11GraphicsEnvironment.<clinit>(X11GraphicsEnvironment.java:134)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:141)
	at java.awt.GraphicsEnvironment.getLocalGraphicsEnvironment(GraphicsEnvironment.java:62)
	at sun.awt.motif.MToolkit.<clinit>(MToolkit.java:81)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:141)
	at java.awt.Toolkit$2.run(Toolkit.java:748)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.awt.Toolkit.getDefaultToolkit(Toolkit.java:739)
	at java.awt.Toolkit.getEventQueue(Toolkit.java:1519)
	at java.awt.EventQueue.isDispatchThread(EventQueue.java:676)
	at javax.swing.SwingUtilities.isEventDispatchThread(SwingUtilities.java:1238)
	at javax.swing.text.StyleContext.reclaim(StyleContext.java:419)
	at javax.swing.text.StyleContext.addAttribute(StyleContext.java:276)
	at javax.swing.text.StyleContext$NamedStyle.addAttribute(StyleContext.java:1468)
	at javax.swing.text.StyleContext$NamedStyle.setName(StyleContext.java:1278)
	at javax.swing.text.StyleContext$NamedStyle.<init>(StyleContext.java:1226)
	at javax.swing.text.StyleContext.addStyle(StyleContext.java:88)
	at javax.swing.text.StyleContext.<init>(StyleContext.java:68)
	at javax.swing.text.DefaultStyledDocument.<init>(DefaultStyledDocument.java:88)
	at org.apache.jackrabbit.extractor.RTFTextExtractor.extractText(RTFTextExtractor.java:60)
	at org.apache.jackrabbit.extractor.RTFTextExtractorTest.testExtractor(RTFTextExtractorTest.java:35)

"
1,"Exception in DocumentsWriter.addDocument can corrupt stored fields file (fdt). DocumentsWriter writes the number of stored fields, up front, into the
fdtLocal buffer.  Then, as each field is processed, it writes each
stored field into this buffer.  When the document is done, in a
finally clause, it flushes the buffer to the real fdt file in the
Directory.

The problem is, if an exception is hit, that number of stored fields
can be too high, which corrupts the fdt file.

The solution is to not write it up front, and instead write only the
number of fields we actually saw."
1,"Possible hidden exception on SegmentInfos commit. I am not sure if this is that big of a deal, but I just ran into it and thought I might mention it.

SegmentInfos.commit removes the Segments File if it hits an exception. If it cannot remove the Segments file (because its not there or on Windows something has a hold of it), another Exception is thrown about not being able to delete the Segments file. Because of this, you lose the first exception, which might have useful info, including why the segments file might not be there to delete.

- Mark"
1,"Inconsistency between Session.getProperty and Node.getProperty for binary values. there an inconsistency in the binary handling between the batch-reading facility and those cases where a property is directly
accessed without having accessed the parent node before.

this issue came up with timothee maret running into performance issues when retrieving the length of a binary property:

if the property-entry has been created in the run of a batch-read operation the corresponding property-data object
contains internal values that contain the length of the binary (such as transported with the json response) and only
read the data from the server if the value stream is explicitly requested.
however, if the property is accessed directly (e.g. Session.getProperty or Node.getProperty with a relative path) 
a GET request is made to the corresponding dav resource and the stream is read immediately.

possible solution:

if RepositoryService#getItemInfos(SessionInfo, ItemId) is called with a PropertyId the implementation
should not result in a GET request to the corresponding resource by calling super.getPropertyInfo(sessionInfo, (PropertyId) itemId).
instead it should be consistent with the batch-read and only make a PROPFIND request for the property
length. the returned PropertyInfo object would in that case be identical to the one generated by the batch-read functionality.
"
1,"EdgeNGramTokenFilter stops on tokens smaller then minimum gram size.. If a token is encountered in the stream that is shorter in length than the min gram size, the filter will stop processing the token stream.

Working up a unit test now, but may be a few days before I can provide it. Wanted to get it in the system."
1,"StandardBenchmarker#makeDocument does not explicitly close opened files. StandardBenchmarker#makeDocument(File in, String[] tags, boolean stored, boolean tokenized, boolean tfv)

        BufferedReader reader = new BufferedReader(new FileReader(in));

Above reader is not closed until GC hits it. Can cause problems in cases where ulimit is set too low.

I did this:

        while ((line = reader.readLine()) != null)
        {
            body.append(line).append(' ');
        }
+        reader.close();"
1,"JCR2SPI: updating events swallowed (CacheBehavior.OBSERVATION). with CacheBehavior.OBSERVATION the hierarchy held within jcr2spi is updated based on events.

if Session-A persistently adds a mix:referenceable to a Node that is already loaded in Session-B, the latter will not be informed about this change.

Reason: upon processing the SPI Event (-> HierarchyEventListener#onEvent) the parent is retrieved by the Event ItemId, which in the former case contains a uniqueID part, which is not known yet to the listening Session-B.
Consequently the NodeEntry affected by the event is not updated.

Possible fix: If looking up the parent entry of the event doesn't succeed, a 2nd lookup using the Event path should be performed."
1,"Session.getUserID returns first principal in the set obtained from Subject.getPrincipals(). this may lead to a wrong value for the UserID (e.g. the name of a Group principal).

jsr 170 defines the getUserID() to return "" the user ID associated with this Session."" and implies (javadoc) that the method has a relation to the login.

This issues has already been partially addressed while working on jsr 283 access control (trunk)."
1,Incorrect iterator position in JCR-RMI when skipping large number of entries. The positionOfBuffer variable in ClientIterator gets out of sync more than ServerAdapter.bufferLength items.
1,"Restore to base version throws NullPointerException. This only happens when the operations are enclosed in an XA transaction.
See test: org.apache.jackrabbit.core.version.RestoreTest"
1,TestNRTManager test failure. reproduces for me
1,"PostingsConsumer#merge does not call finishDoc. We discovered that the current merge function in PostingsConsumer is not calling the #finishDoc method. This does not have consequences for the standard codec (since the lastPosition is set to 0 in #startDoc, and its #finishDoc method is empty), but for the SepCodec, this results in position file corruption (the lastPosition is set to 0 in #finishDoc for the SepCodec)."
1,"TestIndexFileDeleter checkIndex fail. found on 3.x

{noformat}
ant test-tag -Dtestcase=TestIndexFileDeleter -Dtestmethod=testDeleteLeftoverFiles -Dtests.seed=7631088157098800527:4270221915205524915
{noformat}"
1,DWPT doesn't see changes to DW#infoStream. DW does not push infostream changes to DWPT since DWPT#infoStream is final and initialized on DWPTPool initialization (at least for initial DWPT) we should push changes to infostream to DWPT too
1,"WorkspaceConfig.init() throws NullPointerException if Search configuration is missing. When the search configuration is missing from the repository.xml configuration, the WorkspaceConfig.sc field is null and consequently the WorkspaceConfig.init() method throws a NullPointerException.

This is by itself a bug, especially since missing search configuration is perfectly ok resulting in Jackrabbit not building the search index (which is - believe it or - what really want).

On that matter, since the configuration file structure seems to be implied by the configuration framework but the DTD is inlined into the configuration file, the configuration framework should act very gracefully to missing or wrong or unexpected configuration elements. Thus, for example, if the file system configuration (WorkspaceConfig.fsc) would be missing, the WorkspaceConfig should probably hint at this point and not throw a NullPointerException without further explanations (of course throwing anything at all is still better than going wild)."
1,"SQLException with OracleBundle PM in name index. The oracle bundle pm shows errors like:

java.lang.IllegalStateException: Unable to insert index: java.sql.SQLException:  
  ORA-01400: cannot insert NULL into  (""MARTIJNH"".""WM9_VERSIONING_PM_NAMES"".""ID"")

this is due to the fact that oracle treats empty strings as NULL values which does the schema not allow."
1,spi2davex: uri-lookup not cleared after reordering referenceable same-name-siblings. 
1,Like expression does not match line terminator in String. If a string property contains a line terminator a like pattern with a % or _ does not match the line terminator. This is because the implementation uses the java.util.regexp.Pattern class without the DOTALL option.
1,"InternalVersionManager deadlock. The changes in JCR-2753 exposed the InternalVersionManager classes to the following deadlock scenario:

   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x0000000085edb5a0> (a org.apache.jackrabbit.core.state.DefaultISMLocking)
	at java.lang.Object.wait(Object.java:485)
	at org.apache.jackrabbit.core.state.DefaultISMLocking.acquireReadLock(DefaultISMLocking.java:92)
	- locked <0x0000000085edb5a0> (a org.apache.jackrabbit.core.state.DefaultISMLocking)
	at org.apache.jackrabbit.core.version.InternalVersionManagerBase.acquireReadLock(InternalVersionManagerBase.java:192)
	at org.apache.jackrabbit.core.version.InternalVersionManagerImpl.getItem(InternalVersionManagerImpl.java:324)
	at org.apache.jackrabbit.core.version.InternalVersionManagerBase.createInternalVersionItem(InternalVersionManagerBase.java:761)
	at org.apache.jackrabbit.core.version.InternalVersionManagerImpl.getItem(InternalVersionManagerImpl.java:329)
	- locked <0x0000000085edb770> (a org.apache.commons.collections.map.ReferenceMap)
	at org.apache.jackrabbit.core.version.InternalVersionManagerBase.getVersionHistory(InternalVersionManagerBase.java:130)
	at org.apache.jackrabbit.core.version.InternalVersionManagerImpl.getVersionHistory(InternalVersionManagerImpl.java:70)
	at org.apache.jackrabbit.core.version.InternalVersionManagerImpl$4.run(InternalVersionManagerImpl.java:415)
	at org.apache.jackrabbit.core.version.InternalVersionManagerImpl$DynamicESCFactory.doSourced(InternalVersionManagerImpl.java:720)
	at org.apache.jackrabbit.core.version.InternalVersionManagerImpl.checkin(InternalVersionManagerImpl.java:407)
	at org.apache.jackrabbit.core.version.InternalXAVersionManager.checkin(InternalXAVersionManager.java:251)
	at org.apache.jackrabbit.core.version.VersionManagerImplBase.checkoutCheckin(VersionManagerImplBase.java:190)
	at org.apache.jackrabbit.core.VersionManagerImpl.access$100(VersionManagerImpl.java:72)
	at org.apache.jackrabbit.core.VersionManagerImpl$1.perform(VersionManagerImpl.java:121)
	at org.apache.jackrabbit.core.VersionManagerImpl$1.perform(VersionManagerImpl.java:114)
	at org.apache.jackrabbit.core.session.SessionState.perform(SessionState.java:200)
	at org.apache.jackrabbit.core.VersionManagerImpl.perform(VersionManagerImpl.java:95)
	at org.apache.jackrabbit.core.VersionManagerImpl.checkin(VersionManagerImpl.java:114)
	at org.apache.jackrabbit.core.VersionManagerImpl.checkin(VersionManagerImpl.java:100)

   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.jackrabbit.core.version.InternalVersionManagerImpl.getItem(InternalVersionManagerImpl.java:327)
	- waiting to lock <0x0000000085edb770> (a org.apache.commons.collections.map.ReferenceMap)
	at org.apache.jackrabbit.core.version.InternalXAVersionManager.getItem(InternalXAVersionManager.java:442)
	at org.apache.jackrabbit.core.version.InternalVersionManagerBase.getVersionHistory(InternalVersionManagerBase.java:130)
	at org.apache.jackrabbit.core.version.InternalXAVersionManager.getVersionHistory(InternalXAVersionManager.java:58)
	at org.apache.jackrabbit.core.version.VersionHistoryImpl.getInternalVersionHistory(VersionHistoryImpl.java:78)
	at org.apache.jackrabbit.core.version.VersionHistoryImpl.isSame(VersionHistoryImpl.java:278)
	at org.apache.jackrabbit.core.version.VersionHistoryImpl.checkOwnVersion(VersionHistoryImpl.java:326)
	at org.apache.jackrabbit.core.version.VersionHistoryImpl.getVersionLabels(VersionHistoryImpl.java:218)

The problem is the ReferenceMap synchronization (object 0x0000000085edb770) that now interferes with the more general read/write locking mechanism."
1,"MultiTermsEnum over-shares between different Docs/AndPositionsEnum. Robert found this in working on LUCENE-2352.

MultiTermsEnum incorrectly shared sub-enums on two different invocation of .docs/AndPositionsEnum."
1,Session logout doesn't release locks acquired using addLockToken. Session.addLockToken doesn't register locks with the session so when logout is called they are not released. Locks acquired this way maintain a reference to the Session after logout and new sessions attempting to acquire the locks will fail.
1,"Adding empty ParallelReader indexes to an IndexWriter may cause ArrayIndexOutOfBoundsException or NoSuchElementException. Hi,
I recently stumbled upon this:

It is possible (and perfectly legal) to add empty indexes (IndexReaders) to an IndexWriter. However, when using ParallelReaders in this context, in two situations RuntimeExceptions may occur for no good reason.

Condition 1:
The indexes within the ParallelReader are just empty.

When adding them to the IndexWriter, we get a java.util.NoSuchElementException triggered by ParallelTermEnum's constructor. The reason for that is the TreeMap#firstKey() method which was assumed to return null if there is no entry (which is not true, apparently -- it only returns null if the first key in the Map is null).


Condition 2 (Assuming the aforementioned bug is fixed):
The indexes within the ParallelReader originally contained one or more fields with TermVectors, but all documents have been marked as deleted.

When adding the indexes to the IndexWriter, we get a java.lang.ArrayIndexOutOfBoundsException triggered by TermVectorsWriter#addAllDocVectors. The reason here is that TermVectorsWriter assumes that if the index is marked to have TermVectors, at least one field actually exists for that. This unfortunately is not true, either.

Patches and a testcase demonstrating the two bugs are provided.

Cheers,
Christian"
1,"[httpclient] Incorrect credentials loop infinitely. If incorrect credentials are assigned to the request, HttpClient will loop 
forever.  It should only try once, and fail with an HttpException if a request 
with credentials set fails.

In org.apache.commons.httpclient.HttpMethodBase.execute(), a check is needed to 
track if credentials have been sent before."
1,"TestSort testParallelMultiSort reproducible seed failure. trunk r1202157
{code}
    [junit] Testsuite: org.apache.lucene.search.TestSort
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.978 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestSort -Dtestmethod=testParallelMultiSort -Dtests.seed=-2996f3e0f5d118c2:32c8e62dd9611f63:7a90f44586ae8263 -Dargs=""-Dfile.encoding=UTF-8""
    [junit] WARNING: test method: 'testParallelMultiSort' left thread running: Thread[pool-1-thread-1,5,main]
    [junit] WARNING: test method: 'testParallelMultiSort' left thread running: Thread[pool-1-thread-2,5,main]
    [junit] WARNING: test method: 'testParallelMultiSort' left thread running: Thread[pool-1-thread-3,5,main]
    [junit] NOTE: test params are: codec=Lucene40: {short=Lucene40(minBlockSize=98 maxBlockSize=214), contents=PostingsFormat(name=MockSep), byte=PostingsFormat(name=SimpleText), int=Pulsing40(freqCutoff=4 minBlockSize=58 maxBlockSize=186), string=PostingsFormat(name=NestedPulsing), i18n=Lucene40(minBlockSize=98 maxBlockSize=214), long=PostingsFormat(name=Memory), double=Pulsing40(freqCutoff=4 minBlockSize=58 maxBlockSize=186), parser=MockVariableIntBlock(baseBlockSize=88), float=Lucene40(minBlockSize=98 maxBlockSize=214), custom=PostingsFormat(name=MockRandom)}, sim=RandomSimilarityProvider(queryNorm=false,coord=false): {short=BM25(k1=1.2,b=0.75), tracer=DFR I(ne)B2, byte=DFR I(ne)B3(800.0), contents=IB LL-LZ(0.3), int=DFR I(n)BZ(0.3), string=IB LL-D3(800.0), i18n=DFR GB2, double=DFR I(ne)B2, long=DFR GB1, parser=DFR GL2, float=BM25(k1=1.2,b=0.75), custom=DFR I(ne)Z(0.3)}, locale=ga_IE, timezone=America/Louisville
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestSort]
    [junit] NOTE: Linux 3.0.6-gentoo amd64/Sun Microsystems Inc. 1.6.0_29 (64-bit)/cpus=8,threads=4,free=78022136,total=125632512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testParallelMultiSort(org.apache.lucene.search.TestSort): FAILED
    [junit] expected:<[ZJ]I> but was:<[JZ]I>
    [junit] junit.framework.AssertionFailedError: expected:<[ZJ]I> but was:<[JZ]I>
    [junit]     at org.apache.lucene.search.TestSort.assertMatches(TestSort.java:1245)
    [junit]     at org.apache.lucene.search.TestSort.assertMatches(TestSort.java:1216)
    [junit]     at org.apache.lucene.search.TestSort.runMultiSorts(TestSort.java:1202)
    [junit]     at org.apache.lucene.search.TestSort.testParallelMultiSort(TestSort.java:855)
    [junit]     at org.apache.lucene.util.LuceneTestCase$3$1.evaluate(LuceneTestCase.java:523)
    [junit]     at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:149)
    [junit]     at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:51)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.search.TestSort FAILED
{code}"
1,"Constructor of IndexWriter let's runtime exceptions pop up, while keeping the writeLock obtained. Constructor of IndexWriter let's runtime exceptions pop up, while keeping the writeLock obtained.

The init method in IndexWriter catches IOException only (I got NegativeArraySize by reading up a _corrupt_ index), and now, there is no way to recover, since the writeLock will be kept obtained. Moreover, I don't have IndexWriter instance either, to ""grab"" the lock somehow, since the init() method is called from IndexWriter constructor.

Either broaden the catch to all exceptions, or at least provide some circumvention to clear up. In my case, I'd like to ""fallback"", just delete the corrupted index from disk and recreate it, but it is impossible, since the LOCK_HELD NativeFSLockFactory's entry about obtained WriteLock is _never_ cleaned out and is no (at least apparent) way to clean it out forcibly. I can't create new IndexWriter, since it will always fail with LockObtainFailedException."
1,"RMI problems prevent proper startup of the Jackrabbit webapp. A trouble in binding the repository to a RMI registry will prevent the entire Jackrabbit webapp from starting properly. Since RMI is seldom the primary function of the webapp, it's more appropriate to simply log a warning in such cases."
1,"wrong eval order of access control entries within a single node (node-based ac). it seems to me that with the node-based access control the ac entries within a given node are currently collected in the wrong order.
if i remember correctly this worked before and i removed at some point (for reasons i don't recall exactly but have the vague idea that it
was related to the allow-only for groups).

anyway:
while playing around with the permission in our CRX recently i found, that the evaluation of the following setup didn't work as I would
have expected:

- user A is member of group B and C
- for both groups an ACE exists on a given node /a/b/c
- the acl looks like  { deny for B, allow for C }

I would have expected that the allow for C would have reverted the previous deny for B since - in the GUI - I read the ace eval order from first entry to last entry... in the order I added them."
1,"Missing sync in IndexWriter.addIndexes(IndexReader[]). The 3.x build just hit this:

{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestAddIndexes
    [junit] Testcase: testAddIndexesWithThreads(org.apache.lucene.index.TestAddIndexes):	FAILED
    [junit] expected:<3160> but was:<2701>
    [junit] junit.framework.AssertionFailedError: expected:<3160> but was:<2701>
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:779)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:745)
    [junit] 	at org.apache.lucene.index.TestAddIndexes.testAddIndexesWithThreads(TestAddIndexes.java:708)
    [junit] 
    [junit] 
    [junit] Tests run: 15, Failures: 1, Errors: 0, Time elapsed: 9.28 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] java.lang.AssertionError: RefCount is 0 pre-decrement for file ""_8a.tvf""
    [junit] 	at org.apache.lucene.index.IndexFileDeleter$RefCount.DecRef(IndexFileDeleter.java:608)
    [junit] 	at org.apache.lucene.index.IndexFileDeleter.decRef(IndexFileDeleter.java:505)
    [junit] 	at org.apache.lucene.index.IndexFileDeleter.decRef(IndexFileDeleter.java:496)
    [junit] 	at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:2972)
    [junit] 	at org.apache.lucene.index.TestAddIndexes$CommitAndAddIndexes.doBody(TestAddIndexes.java:681)
    [junit] 	at org.apache.lucene.index.TestAddIndexes$RunAddIndexesThreads$1.run(TestAddIndexes.java:624)
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestAddIndexes -Dtestmethod=testAddIndexesWithThreads -Dtests.seed=-6912763261803132408:-5575674032550262483 -Dtests.multiplier=3
    [junit] NOTE: test params are: locale=en_AU, timezone=America/Atka
{noformat}

It looks like it's caused by a long-standing missing sync (since at least 2.9.x).  I think likely we hit it just now thanks to adding random Thread.yield()'s in MockDirWrapper!"
1,"HttpClient does not correctly handle escaped characters in HTTP header elements. An excerpt from Microsoft's ""How Digest Authentication Works"":
http://www.microsoft.com/technet/prodtechnol/windowsserver2003/library/TechRef/717b450c-f4a0-4cc9-86f4-cc0633aae5f9.mspx

<quote>
* RFC 2617-compliant Digest Authentication challenges and responses must also
comply with RFC 2616: Hypertext Transfer Protocol -- HTTP/1.1 quoted string
requirements. This requirement particularly affects the use of backslash (\) and
embedded double quotes. Both must be preceded (escaped) with a backslash.

* For example, domain\username according to RFC 2616 is read as domainusername.
This reading is important because if an application sends information in this
format rather than as domain\\username, authentication fails.

* However, because this is a known issue with domain\username , if
authenticating with backslash encoding fails, Digest SSP attempts to
authenticate the response and assumes that the backslash is part of the string.
This behavior can be turned off by setting the ServerCompat registry key.
</quote>

Review and fix the ParameterParser class and classes implemeting CookieSpec or
AuthScheme interfaces

See also PR #34909"
1,"The PostMethod did not bring back response headers from proxy servers. Description:

When doing tunnelling through proxy servers, in case of 407 response, the 
wrapper class ConnectMethod failed to pass the response header back to the 
wrapped method (PostMethod in our case).  As result, the response headers are 
not passed back to the application.

Proposed Fix:
Change the ConnectMethod to use the wrapped method instance to get response 
headers.  It will be reinitialized again if ""CONNECT"" is successful.

Also have to modify the addProxyAuthorizationRequestHeader code to use the 
wrapped method for the authenticator to work."
1,"IndexingQueue not checked on initial index creation. With a default value of 100 for extractorBackLogSize and lots of text extractions that time out, the temp folder gets filled with extractor*.tmp files. This is because the IndexingQueue.getFinishedDocuments() is not called during the initial index creation. This is not an issue during regular operation because the method is called periodically from a timer thread."
1,"Wrong argument check in BTreeManager constructor. In the constructor of BTreeManager the argument check on maxChildren and minChildren is wrong. Instead of

        if (2 * minChildren < maxChildren) {
            throw new IllegalArgumentException(""maxChildren must be at least twice minChildren"");
        }

it should be

        if (2 * minChildren > maxChildren) {
            throw new IllegalArgumentException(""maxChildren must be at least twice minChildren"");
        }
"
1,"move method of the MemoryFileSystem may accept invalid destination path resulting in invalid entries in FS. It seems there can be a problem with the move method of the MemoryFileSystem class: when looking at its code it looks that it can accept destinations, specifying folders that do not exist in the file system.
For example, if there is a ""somefolder/somefile"" file and I call the method passing, say ""somefolder/someotherfolder/somefile"" as the destination. The destination will be accepted by the code even if ""somefolder/someotherfolder"" is not an existing folder and the function execution will result in a file having the ""somefolder/someotherfolder/somefile"" path within a file system having no ""somefolder/someotherfolder"" folder - the code should probably check whether the destination path is really a valid one. Such  validation could be performed , for example, in the following way: take all path elements from the destination path except the last one and insure that the resulting path points to an existing folder, throw an exception otherwise.
Currently I have no JackRabbit build/test environment set up and could not verify practically whether the issue described can really take place, the supposition is made after looking at the move method implementation."
1,"[PATCH] MultiSearcher problems with Similarity.docFreq(). When MultiSearcher invokes its subsearchers, it is the subsearchers' docFreq()
that is accessed by Similarity.docFreq().  This causes idf's to be computed
local to each index rather than globally, which causes ranking across multiple
indices to not be equivalent to ranking across the entire global collection.

The attached files (if I can figure out how to attach them) provide a potential
partial solution for this.  They properly fix a simple test case, RankingTest,
that was provided by Daniel Naber.

The changes are:
  1.  Searcher:  Add topmostSearcher() field with getter and setter to record
the outermost Searcher.  Default to this.
  2.  MultiSearcher:  Pass down the topmostSearcher when creating the subsearchers.
  3.  IndexSearcher:  Call Query.weight() everywhere with the topmostSearcher
instead of this.
  4.  Query:  Provide a default implementation of Query.combine() so that
MultiSearcher works with all queries.

Problems or possible problems I see:
  1.  This does not address the same issue with RemoteSearchable. 
RemoteSearchable is not a Searcher, nor can it be due to lack of multiple
inheritance in Java, but Query.weight() requires a Searcher.  Perhaps
Query.weight() should be changed to take a Searchable, but this requires
changing many places and I suspect would break apps.
  2.  There may be other places that topmostSearcher should be used instead of this.
  3.  The default implementation for Query.combine() is a guess on my part - it
works for TermQuery.  It's fragile in that the default implementation will hide
bugs caused by queries that inadvertently omit a more precise Query.combine()
method.
  4.  The prior comment on Query.combine() indicates that whoever wrote it was
fully aware of this problem and so probably had another usage in mind, so the
whole issue may just be Daniel's usage in the test case.  It's not apparent to
me, so I probably don't understand something."
1,"When creating multiple repository instances pointing to the same home, opening a second session will remove the .lock file. The following test case can be used to reproduce the bug:
Repository repo1 = new TransientRepository(repoConfig);
Session session1_1 = repo1.login(...);
Session session2_2 = repo1.login(...);
Repository repo2 = new TransientRepository(repoConfig); // Will not fail (expected)
Session session2_1 = repo2.login(...); // Will fail with javax.jcr.RepositoryException: The repository home /tmp/_repository appears to be already locked by the current process (expected)
Session session2_2 = repo2.login(...); // Will work!
Repository repo3 = new TransientRepository(repoConfig); // Will not fail either (expected)
Session session3_1 = repo3.login(...); // Will fail with javax.jcr.RepositoryException: The repository home /tmp/_repository appears to be already locked by the current process (expected)
Session session3_2 = repo3.login(...); // Will fail with javax.jcr.RepositoryException: Directory was previously created with a different LockFactory instance

Open the first session in repo2 will fails but will also remove the .lock file, thus the second
session will succeed and may corrupt the repository because there are multiple session
opened from multiple repository.
The same behaviour occurs for repo3, the .lock file is removed but it is a slightly different case
as a new exception will be thrown while creating the Lucene index.

This is a clearly a twisted case as repositories pointing to the same home must not be created
simultaneously but i think that it must be more robust to prevent data corruption.

I reproduce the bug on JR 1.4.7 and 1.5.3 but i think it affects at least all versions of JR < 1.5.3."
1,"TestNRTManager hang. didn't check 3.x yet, just encountered this one running the tests"
1,"RAMDirectory reports incorrect EOF on seek. If you create a file whose length is a multiple of 1024 (BUFFER_SIZE),
and then try to seek to the very end of the file, you hit
EOFException.

But this is actually ""legal"" as long as you don't try to read any
bytes at that point.

I'm hitting this (rarely) with the bulk-merging logic for term vectors
(LUCENE-1120), which can seek to the very end of the file but not read
any bytes if conditions are right.

"
1,"DirectoryReader ignores NRT SegmentInfos in #isOptimized(). DirectoryReader  only takes shared (with IW) SegmentInfos into account in DirectoryReader#isOptimized(). This can return true even if the actual realtime reader sees more than one segments. 

{code}
public boolean isOptimized() {
    ensureOpen();
   // if segmentsInfos changes in IW this can return false positive
    return segmentInfos.size() == 1 && !hasDeletions();
  }
{code}

DirectoryReader should check if this reader has a non-nul segmentInfosStart and use that instead"
1,repository-2.0.dtd missing. We introduced new configuration elements that need to be reflected in a new DTD version.
1,"JCAResourceAdapter must implement Serializable. We are running Weblogic 10.0 servers in cluster environment.   When deploying the rar, we always get this warning from weblogic stdout.log: 

<Jan 15, 2009 2:42:10 AM PST> <Warning> <Connector> <BEA-190155> <Compliance checking/validation of the resource adapter /home/user/jackrabbit_rar/jackrabbit-jca-1.5.0.rar resulted in the following warnings:  The ra.xml <resourceadapter-class> class 'org.apache.jackrabbit.jca.JCAResourceAdapter' should implement java.io.Serializable but does not.> 

When trying to do the JNDI lookup the repository, we got the error ""No Object found: jackrabbit|null"".   The jackrabbit entry in the jndi tree is visible only as a javax.naming.reference and not as the JCARepositoryHandle due to the above warning.  Due to that, we can't deploy jackrabbit-jca in Test/Production environment.  

I'm no expert in JCA, but feel it is fairly easy to implement Serializable for  JCAResourceAdapter.  Please help us out.
"
1,"Transient Repository cannot be used more than once when configured with DataSources. The TransientRepository cannot be used more than once when the repository is configured with the DataSources construct. This has been verified with both Oracle and Derby configurations. Once the TransientRepository closes for the first time, the ConnectionFactory class sets a boolean value named closed to 'true'.  Thereafter, any use of the ConnectionFactory throws a runtime exception.

The following stacktrace is thrown on the second attempt to utilize the repository:

2011-01-25 08:12:14 DatabaseFileSystem [ERROR] failed to initialize file system
java.lang.IllegalStateException: this factory has already been closed
	at org.apache.jackrabbit.core.util.db.ConnectionFactory.sanityCheck(ConnectionFactory.java:213)
	at org.apache.jackrabbit.core.util.db.ConnectionFactory.getDataBaseType(ConnectionFactory.java:134)
	at org.apache.jackrabbit.core.fs.db.DbFileSystem.getDataSource(DbFileSystem.java:228)
	at org.apache.jackrabbit.core.fs.db.DatabaseFileSystem.init(DatabaseFileSystem.java:190)
	at org.apache.jackrabbit.core.config.RepositoryConfigurationParser$6.getFileSystem(RepositoryConfigurationParser.java:1057)
	at org.apache.jackrabbit.core.config.RepositoryConfig.getFileSystem(RepositoryConfig.java:892)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:284)
	at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:602)
	at org.apache.jackrabbit.core.TransientRepository$1.getRepository(TransientRepository.java:179)
	at org.apache.jackrabbit.core.TransientRepository.startRepository(TransientRepository.java:279)
	at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:375)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:123)
	...
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:115)
2011-01-25 08:12:14 RepositoryImpl [ERROR] failed to start Repository: File system initialization failure.
javax.jcr.RepositoryException: File system initialization failure.
	at org.apache.jackrabbit.core.config.RepositoryConfigurationParser$6.getFileSystem(RepositoryConfigurationParser.java:1060)
	at org.apache.jackrabbit.core.config.RepositoryConfig.getFileSystem(RepositoryConfig.java:892)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:284)
	at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:602)
	at org.apache.jackrabbit.core.TransientRepository$1.getRepository(TransientRepository.java:179)
	at org.apache.jackrabbit.core.TransientRepository.startRepository(TransientRepository.java:279)
	at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:375)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:123)
	at TransientRepositoryTest.addNodeToRepository(TransientRepositoryTest.java:32)
	at TransientRepositoryTest.shouldNotFailWhenUsingTransientRepositoryTwice(TransientRepositoryTest.java:26)
	...
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:115)
Caused by: org.apache.jackrabbit.core.fs.FileSystemException: failed to initialize file system
	at org.apache.jackrabbit.core.fs.db.DatabaseFileSystem.init(DatabaseFileSystem.java:210)
	at org.apache.jackrabbit.core.config.RepositoryConfigurationParser$6.getFileSystem(RepositoryConfigurationParser.java:1057)
	... 42 more
Caused by: java.lang.IllegalStateException: this factory has already been closed
	at org.apache.jackrabbit.core.util.db.ConnectionFactory.sanityCheck(ConnectionFactory.java:213)
	at org.apache.jackrabbit.core.util.db.ConnectionFactory.getDataBaseType(ConnectionFactory.java:134)
	at org.apache.jackrabbit.core.fs.db.DbFileSystem.getDataSource(DbFileSystem.java:228)
	at org.apache.jackrabbit.core.fs.db.DatabaseFileSystem.init(DatabaseFileSystem.java:190)
	... 43 more
2011-01-25 08:12:14 RepositoryImpl [ERROR] Error while closing Version Manager.
java.lang.NullPointerException
	at org.apache.jackrabbit.core.RepositoryImpl.doShutdown(RepositoryImpl.java:1117)
	at org.apache.jackrabbit.core.RepositoryImpl.shutdown(RepositoryImpl.java:1063)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:388)
	at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:602)
	at org.apache.jackrabbit.core.TransientRepository$1.getRepository(TransientRepository.java:179)
	at org.apache.jackrabbit.core.TransientRepository.startRepository(TransientRepository.java:279)
	at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:375)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:123)
	at TransientRepositoryTest.addNodeToRepository(TransientRepositoryTest.java:32)
	at TransientRepositoryTest.shouldNotFailWhenUsingTransientRepositoryTwice(TransientRepositoryTest.java:26)
	...
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:115)
2011-01-25 08:12:14 RepositoryImpl [ERROR] In addition to startup fail, another unexpected problem occurred while shutting down the repository again.
java.lang.NullPointerException
	at org.apache.jackrabbit.core.RepositoryImpl.doShutdown(RepositoryImpl.java:1136)
	at org.apache.jackrabbit.core.RepositoryImpl.shutdown(RepositoryImpl.java:1063)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:388)
	at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:602)
	at org.apache.jackrabbit.core.TransientRepository$1.getRepository(TransientRepository.java:179)
	at org.apache.jackrabbit.core.TransientRepository.startRepository(TransientRepository.java:279)
	at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:375)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:123)
	at TransientRepositoryTest.addNodeToRepository(TransientRepositoryTest.java:32)
	at TransientRepositoryTest.shouldNotFailWhenUsingTransientRepositoryTwice(TransientRepositoryTest.java:26)
	...
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:115)

javax.jcr.RepositoryException: File system initialization failure.
	at org.apache.jackrabbit.core.config.RepositoryConfigurationParser$6.getFileSystem(RepositoryConfigurationParser.java:1060)
	at org.apache.jackrabbit.core.config.RepositoryConfig.getFileSystem(RepositoryConfig.java:892)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:284)
	at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:602)
	at org.apache.jackrabbit.core.TransientRepository$1.getRepository(TransientRepository.java:179)
	at org.apache.jackrabbit.core.TransientRepository.startRepository(TransientRepository.java:279)
	at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:375)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:123)
	at TransientRepositoryTest.addNodeToRepository(TransientRepositoryTest.java:32)
	at TransientRepositoryTest.shouldNotFailWhenUsingTransientRepositoryTwice(TransientRepositoryTest.java:26)
	...
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:115)
Caused by: org.apache.jackrabbit.core.fs.FileSystemException: failed to initialize file system
	at org.apache.jackrabbit.core.fs.db.DatabaseFileSystem.init(DatabaseFileSystem.java:210)
	at org.apache.jackrabbit.core.config.RepositoryConfigurationParser$6.getFileSystem(RepositoryConfigurationParser.java:1057)
	... 42 more
Caused by: java.lang.IllegalStateException: this factory has already been closed
	at org.apache.jackrabbit.core.util.db.ConnectionFactory.sanityCheck(ConnectionFactory.java:213)
	at org.apache.jackrabbit.core.util.db.ConnectionFactory.getDataBaseType(ConnectionFactory.java:134)
	at org.apache.jackrabbit.core.fs.db.DbFileSystem.getDataSource(DbFileSystem.java:228)
	at org.apache.jackrabbit.core.fs.db.DatabaseFileSystem.init(DatabaseFileSystem.java:190)
	... 43 more"
1,CharsRef#append broken on trunk & 3.x. Current impl. for append on CharsRef is broken - it overrides the actual content rather than append. its used in many places especially in solr so we might have some broken 
1,"Cluster Journal directory should be created automatically. If the cluster journal directory does not exist when starting the cluster, an exception is thrown: ERROR org.apache.jackrabbit.core.RepositoryImpl - failed to start Repository: Directory specified does either not exist or is not a directory: ...

As far as I know, this is not consistent with how all other components of Jackrabbit work. I think the directory should be created automatically if it does not exist:

new File(...).mkdirs();

I know you could argue this is not a bug, but in my view it is an important usability issue."
1,"Query index not in sync with workspace. After some time the search index is not in sync anymore with the data in the workspace and returns uuids which have no corresponding Node in the workspace. This results in a NodeIterator which throws an ItemNotFoundException on nextNode().

Instructions how to reproduce this error are not yet available.

Possible areas for further investigation are:
- NodeType registry which maps the node types into the workspace with the use of virtual item states
- versioning?
- atomicity of indexing?"
1,"Cookie.java blowing up on cookies from ""country code"" domains. The following exception is thrown from Cookie.java when receiving a cookie from
a ""country code"" domain such as amazon.ca.

     [java] INFO: Cookie.parse(): Rejecting set cookie header
""session-id=702-1613649-9326458; path=/; domain=
.amazon.ca; expires=Tuesday, 29-Oct-2002 08:00:00 GMT,
session-id-time=1035878400; path=/; domain=.amazon.ca;
expires=Tuesday, 29-Oct-2002 08:00:00 GMT"" because ""session-id"" has an illegal
domain attribute ("".amazon.ca"")
 for the given domain ""www.amazon.ca"".  It violoates the Netscape cookie
specification for non-special TLDs.
     [java] Oct 22, 2002 9:32:37 AM org.apache.commons.httpclient.HttpMethodBase
processResponseHeaders
     [java] SEVERE: Exception processing response headers
     [java] org.apache.commons.httpclient.HttpException: Bad Set-Cookie header:
session-id=702-1613649-9326458
; path=/; domain=.amazon.ca; expires=Tuesday, 29-Oct-2002 08:00:00 GMT,
session-id-time=1035878400; path=/; do
main=.amazon.ca; expires=Tuesday, 29-Oct-2002 08:00:00 GMT Illegal domain
attribute .amazon.ca
     [java]     at org.apache.commons.httpclient.Cookie.parse(Cookie.java:944)
     [java]     at
org.apache.commons.httpclient.HttpMethodBase.processResponseHeaders(HttpMethodBase.java:141
9)
     [java]     at
org.apache.commons.httpclient.HttpMethodBase.readResponse(HttpMethodBase.java:1504)
     [java]     at
org.apache.commons.httpclient.HttpMethodBase.processRequest(HttpMethodBase.java:2128)
     [java]     at
org.apache.commons.httpclient.HttpMethodBase.execute(HttpMethodBase.java:790)
     [java]     at
org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:442)


The http response that caused this exception is below.

HTTP/1.1 302 Found
Date: Tue, 22 Oct 2002 13:30:11 GMT
Server: Stronghold/2.4.2 Apache/1.3.6 C2NetEU/2412 (Unix)
Set-Cookie: session-id=702-8591055-5561622; path=/; domain=.amazon.ca;
expires=Tuesday, 29-Oct-2002 08:00:00 GMT
Set-Cookie: session-id-time=1035878400; path=/; domain=.amazon.ca;
expires=Tuesday, 29-Oct-2002 08:00:00 GMT
Location: http://www.amazon.ca/exec/obidos/tg/browse/-/915398/702-8591055-5561622
Connection: close
Transfer-Encoding: chunked
Content-Type: text/html

I've seen this problem with other .ca domains so this isn't a problem unique to
amazon.ca.

My guess would be that the problem is on line 929 of Cookie.java:

int domainParts = new StringTokenizer(cookie.getDomain(), ""."").countTokens();

Where domainParts would be 2 for a domain like "".amazon.ca"" instead of the 3
that the code is expecting.  I'm not that familiar with the cookie spec so I
could be completely wrong ;-)

The results above were done with the Oct 20/2002 gump build."
1,"[patch] QValueFactoryImpl.equals doesn't do compare correctly. equals compares it's uri to it's own uri, as poosed to the other one.

                 // for both the value has not been loaded yet
                 if (!initialized) {
                     if (other.uri != null) {
-                        return uri.equals(uri);
+                        return other.uri.equals(uri);
                     } else {
                         // need to load the binary value in order to be able
                         // to compare the 2 values."
1,"OverlappingFileLockException with JRE 1.6. Per email discussion:
On Mon, 2007-02-26 at 10:26 +0100, Marcel Reutegger wrote:
> > just my 2c, I didn't really investigated this issue in more detail...
> >
> > according to the javadoc of FileChannel.tryLock() the
> > OverlappingFileLockException is thrown if the JVM already holds a lock on the
> > channel.
> >
> > in contrast, the current check in the repository startup method primarily
> > focuses on the situation where *two* JVMs start a repository on the same home
> > directory.
> >
> > I'd say the OverlappingFileLockException is thrown because two repository
> > instances are startup within the *same* JVM using the same repository home
> > directory.
> >
> > I suggest we add a catch clause, which also covers OverlappingFileLockException
> > in addition to IOException.
> >
> > regards
> >   marcel
> >
> > Stefan Guggisberg wrote:
> > > btw, afaik OverlappingFileLockException is only thrown on linux,
> > > FileChannel#getLock on windows e.g. returns null in the same situation.
> > >
> > > you might want to test on a different platform to further isolate the
> > > issue.
> > > you could also place a breakpoint at the top of the
> > > RepositoryImpl#acquireRepositoryLock
> > > method, step through the code, verify the contents of your fs etc.
> >
>


=== Original email

On 2/19/07, Patrick Haggood <codezilla@> wrote:
I'm using Linux, Sun Java 6 and Jackrabbit 1.3 with Derby persistance.
I have a putNode(object) function that's giving the above error in unit
tests.  It always fails after the second update, even when I swap tests
(i.e. save user doc then save user).  Prior to each test, I delete the
repository directory.

Do I need to set explicit locks before/after each session.save()?

*********** Unit Test
DBConn dbc;

    public SessionUtilTest(String testName) {
        super(testName);
        dbc = new DBConn();
    }

// Note - putUser and putDocument both use putNode after determining
which rootnode will be used

   /**
     * Test of putUnityUser method, of class unityjsr170.jr.SessionUtil.
     */
    public void testPutUnityUser() {
        System.out.println(""putUnityUser"");
        UnityUser usr = usr1;
        SessionUtil instance = dbc.getSutil();
        String result = instance.putUnityUser(usr1);
        assertNotNull(result);
        usr = (UnityUser) instance.getUnityUserByID(result);
        assertEquals(usr1.getName(),usr.getName());
    }
       
    /**
     * Test of putUnityDocument method, of class
unityjsr170.jr.SessionUtil.
     */
    public void testPutUnityDocument() {
        System.out.println(""putUnityDocument"");
        UnityDocument udoc = adr1;
        SessionUtil instance = dbc.getSutil();
        String result = instance.putUnityDocument(udoc);   <---- File
Lock Error
        assertNotNull(result);
        udoc = (UnityDocument) instance.getUnityDocumentByID(result);
        assertEquals(adr1.getName(),udoc.getName());
    }


********* Here's where I setup my repository connection

    public DBConn(){
        sutil = null;
        try {
            rp = new TransientRepository();
            sutil= new SessionUtil(rp);
        } catch (IOException ex) {
            ex.printStackTrace();
        }
    }
    
    public void shutdown(){
        sutil.closeAll();
    }
    
    public SessionUtil getSutil(){
        return sutil;
    }

****************  SessionUtil

    public SessionUtil(Repository rp){
        try {
            session = rp.login(new
SimpleCredentials(""username"",""password"".toCharArray()));
            
        } catch (LoginException ex) {
            ex.printStackTrace();
        } catch (RepositoryException ex) {
            ex.printStackTrace();
        } 
        
    }
    
    public void closeAll(){
        try {
            session.logout();
        } catch (Exception ex) {
            ex.printStackTrace();
            System.out.println(""Error closing repository"");
        }
    }
    
 // Put a node on the tree under the root node, return the uuid of the
new or updated node
    private String putNode(String nodetype, UnityBaseObject ubo){
        String resultuuid =null;
        String uname = ubo.getName();
        String utype = ubo.getType();
        String objectuid = ubo.getId();
        Node pnode; //  node to add or update
        Session ses = null;
        try {
            ses = getSession();
            // Does updateable node already have node Id?
            if (objectuid==null) {
                Node rn = ses.getRootNode();
                pnode = rn.addNode(utype);
                pnode.addMixin(""mix:referenceable"");
            } else{
                // grab existing node by uuid
                pnode = ses.getNodeByUUID(objectuid);
            }
            // Did we get an updateable node?
            if (pnode!=null){
                ubo.setId(pnode.getUUID());
                String unityXML =
utrans.getXMLStringFromUnityBaseObject(ubo);
                // update all the properties
                pnode.setProperty(""name"",ubo.getName());
                pnode.setProperty(""type"",ubo.getType());
                pnode.setProperty(""xmldata"",unityXML);
                ses.save();
                resultuuid = ubo.getId();
            }
        } catch(Exception e) {
            e.printStackTrace();
        } 
        return resultuuid;
    }

    private Session getSession(){
        return session;
    }
    

************  repository.xml

 <Workspace name=""${wsp.name}"">
        <FileSystem
class=""org.apache.jackrabbit.core.fs.local.LocalFileSystem"">
            <param name=""path"" value=""${wsp.home}""/>
        </FileSystem>
        <PersistenceManager
class=""org.apache.jackrabbit.core.state.db.DerbyPersistenceManager"">
            <param name=""url"" value=""jdbc:derby:
${wsp.home}/db;create=true""/>
            <param name=""schemaObjectPrefix"" value=""${wsp.name}_""/>
        </PersistenceManager>
        <SearchIndex
class=""org.apache.jackrabbit.core.query.lucene.SearchIndex"">
            <param name=""path"" value=""${wsp.home}/index""/>
            <param name=""useCompoundFile"" value=""true""/>
            <param name=""minMergeDocs"" value=""100""/>
            <param name=""volatileIdleTime"" value=""3""/>
            <param name=""maxMergeDocs"" value=""100000""/>
            <param name=""mergeFactor"" value=""10""/>
            <param name=""bufferSize"" value=""10""/>
            <param name=""cacheSize"" value=""1000""/>
            <param name=""forceConsistencyCheck"" value=""false""/>
            <param name=""autoRepair"" value=""true""/>
            <param name=""analyzer""
value=""org.apache.lucene.analysis.standard.StandardAnalyzer""/>
        </SearchIndex>
    </Workspace>

"
1,"remove NativeFSLockFactory's attempt to acquire a test lock. NativeFSLockFactory tries to acquire a test lock the first time a lock is created.  It's the only LF to do this, and, it's caused us hassle (LUCENE-2421,  LUCENE-2688).

I think we should just remove it.  The caller of .makeLock will presumably immediately thereafter acquire the lock and at the point hit any exception that acquireTestLock would've hit."
1,"SegmentInfo.sizeInBytes ignore includeDocStore when caching. I noticed that SegmentInfo's sizeInBytes cache is potentially buggy -- it doesn't take into account 'includeDocStores'. I.e., if you call it once w/ 'false' (sizeInBytes won't include the store files) and then with 'true' (or vice versa), you won't get the right sizeInBytes (it won't re-compute, with the store files).

I'll fix and add a test case demonstrating the bug."
1,"AbstractClientConnAdapter#abortConnection() does not release the connection if called from the main execution thread while there is no blocking I/O operation . #abortConnection() is usually expected to be  called from a helper thread in order to unblock the main execution thread blocked in an I/O operation. It may be unsafe to call #releaseConnection() from the helper thread, so we have to rely on an IOException thrown by the closed socket on the main thread to trigger the release of the connection back to the connection manager. However, if this method is called from the main execution thread it should be safe to release the connection immediately. Besides, this also helps ensure the connection gets released back to the manager if #abortConnection() is called from the main execution thread while there is no blocking I/O operation."
1,"Database Data Store: close result sets. The database data store doesn't close one result sets. This is not a problem for most databases, but anyway should be fixed."
1,"NPE in OpenOfficeTextExtractor. I try to load some Open Office Writer document (see attachment) and receive such exception. 

2008-06-10 17:19:59 <WARN > [btpool0-1] CompositeTextExtractor: Failed to extract text content(92)
java.lang.NullPointerException
    at org.apache.jackrabbit.extractor.OpenOfficeTextExtractor.extractText(OpenOfficeTextExtractor.java:7
8)
    at org.apache.jackrabbit.extractor.CompositeTextExtractor.extractText(CompositeTextExtractor.java:90)
    at org.apache.jackrabbit.core.query.lucene.JackrabbitTextExtractor.extractText(JackrabbitTextExtracto
r.java:195)
    at org.apache.jackrabbit.core.query.lucene.NodeIndexer.addBinaryValue(NodeIndexer.java:393)
    at org.apache.jackrabbit.core.query.lucene.NodeIndexer.addValue(NodeIndexer.java:282)
    at org.apache.jackrabbit.core.query.lucene.NodeIndexer.createDoc(NodeIndexer.java:221)
    at org.apache.jackrabbit.core.query.lucene.SearchIndex.createDocument(SearchIndex.java:892)
    at org.apache.jackrabbit.core.query.lucene.SearchIndex$2.next(SearchIndex.java:543)
    at org.apache.jackrabbit.core.query.lucene.MultiIndex.update(MultiIndex.java:428)
    at org.apache.jackrabbit.core.query.lucene.SearchIndex.updateNodes(SearchIndex.java:527)
    at org.apache.jackrabbit.core.SearchManager.onEvent(SearchManager.java:504)
    at org.apache.jackrabbit.core.observation.EventConsumer.consumeEvents(EventConsumer.java:231)
    at org.apache.jackrabbit.core.observation.ObservationDispatcher.dispatchEvents(ObservationDispatcher.
java:201)
    at org.apache.jackrabbit.core.observation.EventStateCollection.dispatch(EventStateCollection.java:425
)
    at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:737
)
    at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:873)
    at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:334)
    at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:337)
    at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:310)
    at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:317)
    at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1247)
    at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:897)
    at org.apache.jackrabbit.jca.JCASessionHandle.save(JCASessionHandle.java:178)"
1,"Don't leak deleted open file handles with pooled readers. If you have CFS enabled today, and pooling is enabled (either directly
or because you've pulled an NRT reader), IndexWriter will hold open
SegmentReaders against the non-CFS format of each merged segment.

So even if you close all NRT readers you've pulled from the writer,
you'll still see file handles open against files that have been
deleted.

This count will not grow unbounded, since it's limited by the number
of segments in the index, but it's still a serious problem since the
app had turned off CFS in the first place presumably to avoid risk of
too-many-open-files.  It's also bad because it ties up disk space
since these files would otherwise be deleted.
"
1,"Escaped quotes inside a phrase cause a ParseException. QueryParser cannot handle escaped quotes when inside a phrase. Escaped quotes not in a phrase are not a problem. This can be added to TestQueryParser.testEscaped() to demonstrate the issue - the second assert throws an exception:

assertQueryEquals(""a \\\""b c\\\"" d"", a, ""a \""b c\"" d"");
assertQueryEquals(""\""a \\\""b c\\\"" d\"""", a, ""\""a \""b c\"" d\"""");

See also this thread:
http://www.nabble.com/ParseException-with-escaped-quotes-in-a-phrase-t1647115.html
"
1,"Incorrect handling of InputStreams when connecting to a server that requires authentication. I'm trying to upload a file to a WebDav server (mod_dav on Apache Web Server 2.2.14) that has basic (or digest, the result is the same) authentication enabled.
I'm using the following code:
        String url = ""http://myserver/dir/test2.gif"";
        File file = new File(""d:/test2.gif"");
        DefaultHttpClient httpClient = new DefaultHttpClient();
        HttpPut put = new HttpPut(url);
        put.setEntity(new InputStreamEntity(new FileInputStream(file), file.length()));
        
        URI uri = put.getURI();
        httpClient.getCredentialsProvider().setCredentials(new AuthScope(uri.getHost(), uri.getPort()),
                getCredentials());
        put.getParams().setBooleanParameter(CoreProtocolPNames.USE_EXPECT_CONTINUE, true);
        HttpResponse response = httpClient.execute(put);
        System.out.println(response.getStatusLine());

When running the above code, I'm getting a org.apache.http.client.NonRepeatableRequestException: Cannot retry request with a non-repeatable request entity. I tested both the latest alpha & the svn head. Doing the same thing in HttpClient 3.1 worked as expected. 

This could be normal, as I'm using an InputStream that is indeed not repeatable, but as I'm also using Expect: 100-Continue, the stream shouldn't have been consumed with the first connection (the one that gets a code 401 from the WebDav server), and only in the second one, when the credentials are provided.

The problem is that DefaultRequestDirector.execute doesn't take this into account and assumes that if a request has been tried once, its associated entity (if any) has been consumed.
Here's the fix that I came up with:
Change DefaultRequestDirector.execute so that if the wrapper is an EntityEnclosingRequestWrapper, it checks if the entity has actually been consumed before throwing a NonRepeatableRequestException. I'm using the method isStreaming() from HttpEntity, as it's the closest thing to what I was looking for. Reading the JavaDoc, it could lead to the situation where an entity has started streaming but has not yet finished, and so is not in a state where it can be used. However I don't think that's a problem as the javadoc for HttpEntity.getContent() states that it can't be called two times on a non-repeatable entity, so it's just a matter of when the request will fail.
This lead me to also modify InputStreamEntity (from the httpCore project) as it didn't comply with the javadoc. With these two modifications, The file upload completes successfully.

I also modified:
 * TestInputStreamEntity.testBasics() (from the httpCore project) test so that it complies with getContent()'s Javadoc.
 * TestDefaultClientRequestDirector.FaultyHttpRequestExecutor because it didn't consume the entity's content.
All the tests from both httpCore and httpClient pass.
I tested both InputStreamEntity and BasicHttpEntity.
 
Please keep in mind that I am by no means an httpClient (or http, for that matter) expert, and these modifications may have some unexpected side-effects that I did not foresee, contain plain dumb code, or whatever, so it would be great if someone could review my changes and give their opinion.
"
1,NodeTypeDefinitionFactory does not set PropertyDefinition#isQueryOrderable. 
1,"Deadlock in acl.EntryCollector / ItemManager. Here's another three-way deadlock that we've encountered:

* Thread A holds a downgraded SISM write lock and is about to start delivering observation events to synchronous listeners
* Thread B wants to write something and blocks waiting for the SISM write lock (since A holds the lock)
* Thread C wants to read something and blocks waiting for the SISM read lock (since B waits for the lock)

Normally such a scenario is handled without any problems, but there's a problem if the session used by thread C has a synchronous observation listener that attempts to read something from the repository during event delivery. In this case the following can happen:

* Thread C holds the ItemManager synchronization lock higher up in the call chain
* Observation listener code called by thread A attempts to read something from the repository, and blocks trying to acquire the ItemManager synchronization lock (since C holds it)

In principle such a scenario should never happen as an observation listener (much less a synchronous one) should never try to use the session that might already be in use by another thread.

Unfortunately the EntryCollector class in o.a.j.core.security.authorization.acl does not follow this guideline, which leads to the deadlock as shown below:

Thread A:
""127.0.0.1 [1297191119365] POST /bin/wcmcommand HTTP/1.0"" nid=1179 state=BLOCKED
    - waiting on <0x11c329fd> (a org.apache.jackrabbit.core.ItemManager)
    - locked <0x11c329fd> (a org.apache.jackrabbit.core.ItemManager)
     owned by 127.0.0.1 [1297191138443] POST /bin/wcmcommand HTTP/1.0 id=67
    at org.apache.jackrabbit.core.ItemManager.getNode(ItemManager.java:653)
    at org.apache.jackrabbit.core.ItemManager.getNode(ItemManager.java:605)
    at org.apache.jackrabbit.core.SessionImpl.getNode(SessionImpl.java:1406)
    at org.apache.jackrabbit.core.security.authorization.acl.EntryCollector.onEvent(EntryCollector.java:253)
    at org.apache.jackrabbit.core.observation.EventConsumer.consumeEvents(EventConsumer.java:246)
    at org.apache.jackrabbit.core.observation.ObservationDispatcher.dispatchEvents(ObservationDispatcher.java:214)
    at org.apache.jackrabbit.core.observation.EventStateCollection.dispatch(EventStateCollection.java:475)
    at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:786)
    at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:1488)
    at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:349)
    at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:354)
    at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:324)
    at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:328)
    at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1141)
    at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:920)

Thread B:
""Thread-2438"" nid=2582 state=WAITING
    - waiting on <0x166e790e> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock)
    - locked <0x166e790e> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock)
    at java.lang.Object.wait(Native Method)
    at java.lang.Object.wait(Object.java:485)
    at EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock.acquire(Unknown Source)
    at org.apache.jackrabbit.core.state.DefaultISMLocking$WriteLockImpl.<init>(DefaultISMLocking.java:76)
    at org.apache.jackrabbit.core.state.DefaultISMLocking$WriteLockImpl.<init>(DefaultISMLocking.java:70)
    at org.apache.jackrabbit.core.state.DefaultISMLocking.acquireWriteLock(DefaultISMLocking.java:64)
    at org.apache.jackrabbit.core.state.SharedItemStateManager.acquireWriteLock(SharedItemStateManager.java:1808)
    at org.apache.jackrabbit.core.state.SharedItemStateManager.access$200(SharedItemStateManager.java:112)
    at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:565)
    at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:1458)
    at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:1488)
    at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:349)
    at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:354)
    at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:324)
    at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:328)
    at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1141)
    at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:920)

Thread C:
""127.0.0.1 [1297191138443] POST /bin/wcmcommand HTTP/1.0"" nid=67 state=WAITING
    - waiting on <0xf820edb> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$ReaderLock)
    - locked <0xf820edb> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$ReaderLock)
    at java.lang.Object.wait(Native Method)
    at java.lang.Object.wait(Object.java:485)
    at EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$ReaderLock.acquire(Unknown Source)
    at org.apache.jackrabbit.core.state.DefaultISMLocking$ReadLockImpl.<init>(DefaultISMLocking.java:102)
    at org.apache.jackrabbit.core.state.DefaultISMLocking$ReadLockImpl.<init>(DefaultISMLocking.java:96)
    at org.apache.jackrabbit.core.state.DefaultISMLocking.acquireReadLock(DefaultISMLocking.java:53)
    at org.apache.jackrabbit.core.state.SharedItemStateManager.acquireReadLock(SharedItemStateManager.java:1794)
    at org.apache.jackrabbit.core.state.SharedItemStateManager.getItemState(SharedItemStateManager.java:257)
    at org.apache.jackrabbit.core.state.LocalItemStateManager.getNodeState(LocalItemStateManager.java:107)
    at org.apache.jackrabbit.core.state.LocalItemStateManager.getItemState(LocalItemStateManager.java:171)
    at org.apache.jackrabbit.core.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:200)
    at org.apache.jackrabbit.core.ItemManager.getItemData(ItemManager.java:391)
    at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:337)
    at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:638)
    at org.apache.jackrabbit.core.security.authorization.acl.ACLProvider$AclPermissions.canRead(ACLProvider.java:507)
      - locked java.lang.Object@6ad9b475
    at org.apache.jackrabbit.core.security.DefaultAccessManager.canRead(DefaultAccessManager.java:251)
    at org.apache.jackrabbit.core.query.lucene.QueryResultImpl.isAccessGranted(QueryResultImpl.java:374)
    at org.apache.jackrabbit.core.query.lucene.QueryResultImpl.collectScoreNodes(QueryResultImpl.java:353)
    at org.apache.jackrabbit.core.query.lucene.QueryResultImpl.getResults(QueryResultImpl.java:310)
    at org.apache.jackrabbit.core.query.lucene.SingleColumnQueryResult.<init>(SingleColumnQueryResult.java:70)
    at org.apache.jackrabbit.core.query.lucene.QueryImpl.execute(QueryImpl.java:133)
    at org.apache.jackrabbit.core.query.QueryImpl.execute(QueryImpl.java:127)
"
1,"NamespaceRegistry.registerNamespace(pre, uri)  might accidentally remove namespace in certain situations. assume the following mappings exist in the global NamespaceRegistry:

pre1 <-> uri1
pre2 <-> uri2

the following stmt correclty throws a NamespaceException, complaining that an existing prefix can not be remapped:

nsReg.registerNamespace(""pre2"", ""uri1"")

but, as a sideeffect, it has also removed the mapping pre1 <-> uri1."
1,"AlreadyClosedException on initial index creation. Happens when the indexing queue is checked while creating an initial index. This is probably a regression caused by JCR-2035.

Caused by: org.apache.lucene.store.AlreadyClosedException: this Directory is closed
        at org.apache.lucene.store.Directory.ensureOpen(Directory.java:220)
        at org.apache.lucene.store.FSDirectory.getFile(FSDirectory.java:533)
        at org.apache.jackrabbit.core.query.lucene.directory.FSDirectoryManager$FSDir.list(FSDirectoryManager.java:149)
        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:533)
        at org.apache.lucene.index.DirectoryIndexReader.open(DirectoryIndexReader.java:115)
        at org.apache.lucene.index.IndexReader.open(IndexReader.java:316)
        at org.apache.lucene.index.IndexReader.open(IndexReader.java:263)
        at org.apache.jackrabbit.core.query.lucene.AbstractIndex.getIndexReader(AbstractIndex.java:245)
        at org.apache.jackrabbit.core.query.lucene.AbstractIndex.removeDocument(AbstractIndex.java:225)
        at org.apache.jackrabbit.core.query.lucene.PersistentIndex.removeDocument(PersistentIndex.java:90)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex$DeleteNode.execute(MultiIndex.java:1952)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.executeAndLog(MultiIndex.java:1085)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.checkIndexingQueue(MultiIndex.java:1308)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.createIndex(MultiIndex.java:1177)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.createIndex(MultiIndex.java:1191)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.createIndex(MultiIndex.java:1191)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.createIndex(MultiIndex.java:1191)
[...]

I assume there is an index merge happening at the same time that closes index segments."
1,"SnapshotDeletionPolicy.snapshot() throws NPE if no commits happened. SDP throws NPE if no commits occurred and snapshot() was called. I will replace it w/ throwing IllegalStateException. I'll also move TestSDP from o.a.l to o.a.l,index. I'll post a patch soon"
1,"DatabaseJournal.checkSchema generates ""Cannot call commit when autocommit=true"" Exception. When I tried to activate clustering with adding the following XML to repository.xml, 

    <Cluster id=""node_1"" syncDelay=""5"">
		<Journal class=""org.apache.jackrabbit.core.journal.DatabaseJournal"">
			<param name=""revision"" value=""${rep.home}/revision""/>
			<param name=""driver"" value=""com.mysql.jdbc.Driver""/>
			<param name=""url"" value=""jdbc:mysql://localhost/jcr""/>
			<param name=""user"" value=""userX""/>
			<param name=""password"" value=""passWordC""/>
			<param name=""schema"" value=""mysql""/>
			<param name=""schemaObjectPrefix"" value=""J_C_""/>
		</Journal>
    </Cluster>

Databse Journal threw the following exception:

....
Caused by: javax.jcr.RepositoryException: Unable to initialize connection.: Unable to initialize connection.
        at org.apache.jackrabbit.core.RepositoryImpl.createClusterNode(RepositoryImpl.java:677)
        at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:276)
        at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:584)
        at org.apache.jackrabbit.core.TransientRepository$2.getRepository(TransientRepository.java:245)
        at org.apache.jackrabbit.core.TransientRepository.startRepository(TransientRepository.java:265)
        at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:333)
        at com.liferay.portal.jcr.jackrabbit.JCRFactoryImpl.createSession(JCRFactoryImpl.java:71)
        at com.liferay.portal.jcr.JCRFactoryUtil.createSession(JCRFactoryUtil.java:53)
        at com.liferay.portal.jcr.JCRFactoryUtil.createSession(JCRFactoryUtil.java:57)
        at com.liferay.documentlibrary.util.IndexerImpl.reIndex(IndexerImpl.java:258)
        ... 17 more
Caused by: org.apache.jackrabbit.core.cluster.ClusterException: Unable to initialize connection.
        at org.apache.jackrabbit.core.cluster.ClusterNode.init(ClusterNode.java:218)
        at org.apache.jackrabbit.core.cluster.ClusterNode.init(ClusterNode.java:189)
        at org.apache.jackrabbit.core.RepositoryImpl.createClusterNode(RepositoryImpl.java:674)
        ... 26 more
Caused by: java.sql.SQLException: Can't call commit when autocommit=true
        at com.mysql.jdbc.Connection.commit(Connection.java:2161)
        at org.apache.jackrabbit.core.journal.DatabaseJournal.checkSchema(DatabaseJournal.java:437)
        at org.apache.jackrabbit.core.journal.DatabaseJournal.init(DatabaseJournal.java:168)
        at org.apache.jackrabbit.core.cluster.ClusterNode.init(ClusterNode.java:213)
        ... 28 more

When I examined the source code of the release jackrabbit 1.3, I saw that the init() method for DatabaseJournal class is:

			...
			Class.forName(driver);
			con = DriverManager.getConnection(url, user, password);
			con.setAutoCommit(true);

			checkSchema();
			prepareStatements();
			...

and just before checkSchema() method's finally block:

			...
			// commit the changes
			con.commit();
			...

So, it seemed normal to see the mentioned exception. I just commented out the commit expression and continued my development. Am I missing something?"
1,"Token div exceeds length of provided text sized 4114. I have a doc which contains html codes. I want to strip html tags and make the test clear after then apply highlighter on the clear text . But highlighter throws an exceptions if I strip out the html characters  , if i don't strip out , it works fine. It just confuses me at the moment 

I copy paste 3 thing here from the console as it may contain special characters which might cause the problem.


1 -) Here is the html text 

          <h2>Starter</h2>
          <div id=""tab1-content"" class=""tabContent selected"">
            <div class=""head""></div>
            <div class=""body"">
             <div class=""subject-header"">Learning path: History</div>
              <h3>Key question</h3>
              <p>Did transport fuel the industrial revolution?</p>
              <h3>Learning Objective</h3>
	      <ul>
              <li>To categorise points as for or against an argument</li>
              </ul>
	      <p>
              <h3>What to do?</h3>
              <ul>
                <li>Watch the clip: <em>Transport fuelled the industrial revolution.</em></li>
              </ul>
              <p>The clips claims that transport fuelled the industrial revolution. Some historians argue that the industrial revolution only happened because of developments in transport.</p>
			  <ul>
			  	<li>Read the statements below and decide which points are <em>for</em> and which points are <em>against</em> the argument that industry expanded in the 18th and 19th centuries because of developments in transport.</li>
			</ul>
			
			<ol type=""a"">
				<li>Industry expanded because of inventions and the discovery of steam power.</li>
				<li>Improvements in transport allowed goods to be sold all over the country and all over the world so there were more customers to develop industry for.</li>
				<li>Developments in transport allowed resources, such as coal from mines and cotton from America to come together to manufacture products.</li>
				<li>Transport only developed because industry needed it. It was slow to develop as money was spent on improving roads, then building canals and the replacing them with railways in order to keep up with industry.</li>
			</ol>
			
			<p>Now try to think of 2 more statements of your own.</p>
			
            </div>
            <div class=""foot""></div>
          </div>
          <h2>Main activity</h2>
          <div id=""tab2-content"" class=""tabContent"">
            <div class=""head""></div>
            <div class=""body""><div class=""subject-header"">Learning path: History</div>
              <h3>Learning Objective</h3>
              <ul>
                <li>To select evidence to support points</li>
              </ul>
              <h3>What to do?</h3>
              <!--<ul>
                <li>Watch the clip: <em>Windmill and water mill</em></li>
              </ul>-->
              <ul><li>Choose the 4 points that you think are most important - try to be balanced by having two <strong>for</strong> and two <strong>against</strong>.</li>
			  <li>Write one in each of the point boxes of the paragraphs on the sheet <a href=""lp_history_industry_transport_ws1.html"" class=""link-internal"">Constructing a balanced argument</a>.</li></ul> <p>You might like to re write the points in your own words and use connectives to link the paragraphs.</p>
              
			  <p>In history and in any argument, you need evidence to support your points.</p>
			  <ul><li>Find evidence from these sources and from your own knowledge to support each of your points:</li></ul>
			  <ol>
                <li><a href=""../servlet/link?template=vid&macro=setResource&resourceID=2044"" class=""link-internal"">At a toll gate</a></li>
                <li><a href=""../servlet/link?macro=setResource&template=vid&resourceID=2046"" class=""link-internal"">Canals</a></li>
                <li><a href=""../servlet/link?macro=setResource&template=vid&resourceID=2043"" class=""link-internal"">Growing cities: traffic</a></li>
				<li><a href=""../servlet/link?macro=setResource&template=vid&resourceID=2047"" class=""link-internal"">Impact of the railway</a> </li>
				<li><a href=""../servlet/link?macro=setResource&template=vid&resourceID=2048"" class=""link-internal"">Sailing ships</a> </li>
				<li><a href=""../servlet/link?macro=setResource&template=vid&resourceID=2050"" class=""link-internal"">Liverpool: Capital of Culture</a> </li>
              </ol>
			  <p>Try to be specific in your evidence - use named examples of places or people. Use dates if you can.</p>
            </div>
            <div class=""foot""></div>
          </div>
          <h2>Plenary</h2>
          <div id=""tab3-content"" class=""tabContent"">
            <div class=""head""></div>
            <div class=""body""><div class=""subject-header"">Learning path: History</div>
              <h3>Learning Objective</h3>
              <ul>
                <li>To judge which of the arguments is most valid</li>
              </ul>
              <h3>What to do?</h3>
<!--              <ul>
                <li>Watch the clip: <em>Food of the rich</em></li>
              </ul>-->
              <p>In order to be a good historian, and get good marks in exams, you need to show your evaluation skills and make a judgement. Having been through the evidence which point do you think is most important? Why? Is there more evidence? Is the evidence more convincing?</p>
			  <ul><li>In the final box on your worksheet write a conclusion explaining whether on balance the evidence is enough to convince you that transport fuelled the industrial revolution.</li></ul>
            </div>
            <div class=""foot""></div>
          </div>
          <h2>Extension</h2>
          <div id=""tab4-content"" class=""tabContent"">
            <div class=""head""></div>
            <div class=""body""><div class=""subject-header"">Learning path: History</div>
              <h3>What to do?</h3>
              <p>Watch the clip <em>Stress in a ski resort</em></p>
			  <p>New industries, such as tourism, can now be said to be fuelled by transport improvements.</p>
              <ul><li>Search Clipbank, using the Related clip lists as well as the search function, to find examples from around the world of how transport has helped industry.</li></ul>              
            </div>
            <div class=""foot""></div>
          </div>
          
          
2-) here is the text after stripped html tags  out 

           Starter 
           
              
             
              Learning path: History 
               Key question 
               Did transport fuel the industrial revolution? 
               Learning Objective 
	       
               To categorise points as for or against an argument 
               
	       
               What to do? 
               
                 Watch the clip:  Transport fuelled the industrial revolution.  
               
               The clips claims that transport fuelled the industrial revolution. Some historians argue that the industrial revolution only happened because of developments in transport. 
			   
			  	 Read the statements below and decide which points are  for  and which points are  against  the argument that industry expanded in the 18th and 19th centuries because of developments in transport. 
			 
			
			 
				 Industry expanded because of inventions and the discovery of steam power. 
				 Improvements in transport allowed goods to be sold all over the country and all over the world so there were more customers to develop industry for. 
				 Developments in transport allowed resources, such as coal from mines and cotton from America to come together to manufacture products. 
				 Transport only developed because industry needed it. It was slow to develop as money was spent on improving roads, then building canals and the replacing them with railways in order to keep up with industry. 
			 
			
			 Now try to think of 2 more statements of your own. 
			
             
              
           
           Main activity 
           
              
              Learning path: History 
               Learning Objective 
               
                 To select evidence to support points 
               
               What to do? 
               
                Choose the 4 points that you think are most important - try to be balanced by having two  for  and two  against . 
			   Write one in each of the point boxes of the paragraphs on the sheet  Constructing a balanced argument .    You might like to re write the points in your own words and use connectives to link the paragraphs. 
              
			   In history and in any argument, you need evidence to support your points. 
			    Find evidence from these sources and from your own knowledge to support each of your points:  
			   
                  At a toll gate  
                  Canals  
                  Growing cities: traffic  
				  Impact of the railway   
				  Sailing ships   
				  Liverpool: Capital of Culture   
               
			   Try to be specific in your evidence - use named examples of places or people. Use dates if you can. 
             
              
           
           Plenary 
           
              
              Learning path: History 
               Learning Objective 
               
                 To judge which of the arguments is most valid 
               
               What to do? 
 
               In order to be a good historian, and get good marks in exams, you need to show your evaluation skills and make a judgement. Having been through the evidence which point do you think is most important? Why? Is there more evidence? Is the evidence more convincing? 
			    In the final box on your worksheet write a conclusion explaining whether on balance the evidence is enough to convince you that transport fuelled the industrial revolution.  
             
              
           
           Extension 
           
              
              Learning path: History 
               What to do? 
               Watch the clip  Stress in a ski resort  
			   New industries, such as tourism, can now be said to be fuelled by transport improvements. 
                Search Clipbank, using the Related clip lists as well as the search function, to find examples from around the world of how transport has helped industry.                
             
              
           
          
         3-) here is the exception I get

org.apache.lucene.search.highlight.InvalidTokenOffsetsException: Token div exceeds length of provided text sized 4114
	at org.apache.lucene.search.highlight.Highlighter.getBestTextFragments(Highlighter.java:228)
	at org.apache.lucene.search.highlight.Highlighter.getBestFragments(Highlighter.java:158)
	at org.apache.lucene.search.highlight.Highlighter.getBestFragments(Highlighter.java:462)

"
1,"Deleting binary property does not remove 'blob file' in filesystem. when deleting a binary property or its containing node, the 'blob-file' sometime does not get removed.

the reason for this, is an open FileInputStream, that gets referenced in the property value."
1,"""overwriting cached entry"" warnings. when using multiple concurrent sessions you'll find *lots* of log entries like:

    03.11.2010 21:17:03 *WARN * ItemStateReferenceCache: overwriting cached entry ad79ca57-5eb1-4b7d-a439-a9fd73cc8c5a (ItemStateReferenceCache.java, line 176)

those are actually legitimate warnings since there's a siginificant risk of data loss/inconsistency involved.

this is apparently a regression of changes introduced by JCR-2699, specifically svn r1004223"
1,"Unencoded redirect URI causes exception when following redirects. When HttpClient is set to follow redirects, the DefaultRedirectHandler gets the redirect location from the appropriate request header and attempts to create a new java.net.URI from it. If the location contains an invalid URI character, creating the URI fails. For example, if the redirect location were ""/foo?bar=<baz/>"", it would fail because the '<' and '>' are not legal in a URI.

I'm not sure if this should actually be considered a bug in HttpClient, since the website in question should probably be responsible for encoding the URI appropriately; however, browsers handle the situation gracefully, and it would be nice if this excellent library would do so as well."
1,cookies > 20 years invalidated.  
1,"Saving a node deletion that has been modified externally throws a ConstraintViolationException. Deleting a node ""a"" and saving its parent might result in a ConstraintViolationException if node ""a"" has been modified externally, where an InvalidItemStateException with message ""item x has been modified externally"" would be more intuitive.
"
1,"Uncaught AbstractMethodError exception in in DomUtil.createFactory(). DomUtil.createFactory() throws an uncaught AbstractMethodError exception when xerces is on the classpath and the jackrabbit webdav module is used. 

This can render the class unusable when used in conjunction with the xerces library. 
"
1,"Rollback doesn't preserve integrity of original index. After several ""updateDocuments"" calls a rollback call does not return the index to the prior state.
This seems to occur if the number of updates exceeds the RAM buffer size i.e. when some flushing of updates occurs.

Test fails in Lucene 2.4, 2.9, 3.0.1 and 3.0.2

JUnit to follow.
"
1,"BooleanScorer2 does not compile with ecj. BooleanScorer2, derived from scorer, has two inner classes both derived, ultimately, from Scorer.
As such they all define doc() or inherit it.
ecj produces an error when doc() is called from score in the inner classes in the methods
        countingDisjunctionSumScorer
    and
        countingConjunctionSumScorer

The error message is:
    The method doc is defined in an inherited type and in an enclosing scope.

The affected lines are: 160, 161, 178, and 179


I have run the junit test TestBoolean2 (as well as all the others) with
        doc()
    changed to
        BooleanScorer2.this.doc()
    and also to:
        this.doc();
The result was that the tests passed for both.

I added debug statements to all the doc methods and the score methods in the affected classes, but I could not determine what it should be.
"
1,"IW#nrtIsCurrent retruns true if changes are in del queue but not in bufferedDeleteStream yet. spinnoff from SOLR-2861 - since the delete queue is not necessarily applied entirely on each request there is a chance that there are changes in the delete queue but not yet in buffered deletes. this can prevent NRT readers from reopen when they should... this shows the problematic code:

{code}
Index: java/org/apache/lucene/index/IndexWriter.java
===================================================================
--- java/org/apache/lucene/index/IndexWriter.java	(revision 1195214)
+++ java/org/apache/lucene/index/IndexWriter.java	(working copy)
@@ -4074,7 +4074,7 @@
   synchronized boolean nrtIsCurrent(SegmentInfos infos) {
     //System.out.println(""IW.nrtIsCurrent "" + (infos.version == segmentInfos.version && !docWriter.anyChanges() && !bufferedDeletesStream.any()));
     ensureOpen();
-    return infos.version == segmentInfos.version && !docWriter.anyChanges() && !bufferedDeletesStream.any();
+    return infos.version == segmentInfos.version && !docWriter.anyChanges() && !docWriter.deleteQueue.anyChanges();
   }
{code}"
1,"Chunked Stream Encoding Problems Fails to throw Exceptions. Using the HttpClient 2.0.1 with Sun's JDK 1.4.1_01 and connecting to a site 
that appareantly has problems generating proper chunked output causes the http 
client to catch and log an exception then return null data. Ideally the http 
client should throw the IOException to the calling class so that it can be 
handled by the programmer. It's not a problem that an exception is being 
generated it is a bug that the exception is being trapped in the somewhere in 
the httpclient code.

2004-08-27 21:19:01,013 main HttpMethodBase [ERROR]: I/O failure reading 
response body
java.io.IOException: chunked stream ended unexpectedly
        at 
org.apache.commons.httpclient.ChunkedInputStream.getChunkSizeFromInputStream
(ChunkedInputStream.java:234)
        at org.apache.commons.httpclient.ChunkedInputStream.nextChunk
(ChunkedInputStream.java:205)
        at org.apache.commons.httpclient.ChunkedInputStream.read
(ChunkedInputStream.java:160)
        at java.io.FilterInputStream.read(FilterInputStream.java:111)
        at org.apache.commons.httpclient.AutoCloseInputStream.read
(AutoCloseInputStream.java:110)
        at java.io.FilterInputStream.read(FilterInputStream.java:90)
        at org.apache.commons.httpclient.AutoCloseInputStream.read
(AutoCloseInputStream.java:129)
        at org.apache.commons.httpclient.HttpMethodBase.getResponseBody
(HttpMethodBase.java:685)
        at com.algorim.ei.cets.EmailPreProcessor.processMessage
(EmailPreProcessor.java:565)
        at com.algorim.ei.cets.EmailUpdate.run(EmailUpdate.java:332)
        at com.algorim.ei.cets.EmailUpdate.main(EmailUpdate.java:89)

Request and response that are causing the error:
GET /aeq.aspx?k=32226&k=sb1313@xcorp5.com HTTP/1.1
User-Agent: Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.0; .NET CLR 
1.1.4322)
Host: 38.117.227.56

HTTP/1.1 200 OK
Date: Fri, 27 Aug 2004 20:55:27 GMT
Server: Microsoft-IIS/6.0
X-Powered-By: ASP.NET
X-AspNet-Version: 1.1.4322
Transfer-Encoding: chunked
Cache-Control: private
Content-Type: text/html; charset=utf-8

179
<html><head><META HTTP-EQUIV=Refresh CONTENT=""1; 
URL=http://www.datingresults.com/default.asp?
p=7090&PRM=38664""</head><body><script>win2=win
dow.open('http://m.qmct.com/images/d.html?
a=1', 'newwin','toolbar=0,width=730,height=500');if (win2 != null) win2.blur
();window.focus();wind
ow.location = 'http://www.datingresults.com/default.asp?
p=7090&PRM=38664';</script></body></html>"
1,"after enabling access manager, I can't createNode and setProperty without a node.save in the middle. I added my own access manager. after that I can't get the following code working 

Node n = createNewNode(parentNode);
n.setProperty();
parentNode.save();

It seems that setProperty will invoke access control check, but since the new node is not in the repository yet, my access manager implementation won't be able to grant permission. I also tried to use hierachyManager to get the path of the new node, it also returned null. 


"
1,"Data Store: Oracle fails to create the table. When using an Oracle database, the following exception occurs when trying to create the table: ORA.00902: invalid datatype
The problem is that Oracle doesn't support the data type BIGINT. Instead, LONG should be used."
1,"ResponseCachingPolicy uses integers for sizes. ResponseCachingPolicy currently uses integers for interpreting the size of Content-Length, as well internally.

This causes issues in attempting to use the module for caching entities that are over 2GB in size, the module does not fail gracefully, but throws a NumberFormatException

I have a patch that fixes this, by promoting the int -> long, which should allow for larger entities to be cached, it also updates the public facing API where possible, I don't think that the promotion should break compatibility massively

The changes can also be seen here:
https://github.com/GregBowyer/httpclient/commit/1197d3f94bd2eedcec32646cd6146748ca2e6fa1"
1,"SimpleSpanFragmenter can create very short fragments. Line 74 of SimpleSpanFragmenter returns true when the current token is the start of a hit on a span or phrase, thus starting a new fragment. Two problems occur:

- The previous fragment may be very short, but if it contains a hit it will be combined with the new fragment later so this disappears.
- If the token is close to a natural fragment boundary the new fragment will end up very short; possibly even as short as just the span or phrase itself. This is the result of creating a new fragment without incrementing currentNumFrags.

To fix, remove or comment out line 74. The result is that fragments average to the fragment size unless a span or phrase hit is towards the end of the fragment - that fragment is made larger and the following fragment shorter to accommodate the hit."
1,"HttpClient:- Connections not released when SSL Tunneling fails.. Trying to use HTTPS, and SSL tunneling fails as expected because the host is not accepted by the squid proxy, so squid proxy return 403. 

The problem I am seeing is that, when ever this happens the connections are not released to the pool. I traced the code and it appears that in 
HttpMethidDirector.java:  executeWithRetry()
when executeConnect() return false and there is no retry, the connections are not released.

Is this expected? Or am I doing something wrong."
1,"offset gap should be added regardless of existence of tokens in DocInverterPerField. Problem: If a multiValued field which contains a stop word (e.g. ""will"" in the following sample) only value is analyzed by StopAnalyzer when indexing, the offsets of the subsequent tokens are not correct.

{code:title=indexing a multiValued field}
doc.add( new Field( F, ""Mike"", Store.YES, Index.ANALYZED, TermVector.WITH_OFFSETS ) );
doc.add( new Field( F, ""will"", Store.YES, Index.ANALYZED, TermVector.WITH_OFFSETS ) );
doc.add( new Field( F, ""use"", Store.YES, Index.ANALYZED, TermVector.WITH_OFFSETS ) );
doc.add( new Field( F, ""Lucene"", Store.YES, Index.ANALYZED, TermVector.WITH_OFFSETS ) );
{code}

In this program (soon to be attached), if you use WhitespaceAnalyzer, you'll get the offset(start,end) for ""use"" and ""Lucene"" will be use(10,13) and Lucene(14,20). But if you use StopAnalyzer, the offsets will be use(9,12) and lucene(13,19). When searching, since searcher cannot know what analyzer was used at indexing time, this problem causes out of alignment of FVH.

Cause of the problem: StopAnalyzer filters out ""will"", anyToken flag set to false then offset gap is not added in DocInverterPerField:

{code:title=DocInverterPerField.java}
if (anyToken)
  fieldState.offset += docState.analyzer.getOffsetGap(field);
{code}

I don't understand why the condition is there... If always the gap is added, I think things are simple."
1,"New versions added after a restore have bad version name. I add several versions to a node (1.0, 1.1, 1.2, 1.3, 1.4). Perform a restore to version 1.2 and add more versions. After that VersionHistory is like this:

- 1.0
- 1.1
- 1.2
- 1.3
- 1.4
- 1.3.1
- 1.3.2
- 1.3.3
- 1.3.4
- 1.3.5

New versions should be 1.2.x no 1.3.x, isn't it?"
1,"NullPointerException in DatabasePersistenceManager and DatabaseFileSystem after a failed reconnection attempt. As reported on the dev mailing-list, this is what happens:

The reconnection/retry mechanism in DatabasePersistenceManager/DatabaseFileSystem seems to behave fine when the connection times out or is killed for some reason, and the DB server is in fact still running.
However there is a problem if the connection cannot be re-established directly, for example if a transient network outage lasts longer than the few reconnection attempts.
Inside DatabasePersistenceManager.reestablishConnection(), initConnection() will fail, and the preparedStatements map will stay empty.
This in turn will trigger a nasty NullPointerException (never caught) next time executeStmt() is called, because the map is still empty, and there is no check for that.


The following proposed fix from Stefan Guggisberg has been tested to work when applied on 1.2-rc2:

> the simplest fix would be to remove line 783 in
> DatabasePersistenceManager.java and line 1010 in
> DatabaseFileSystem.java,
> i.e. the following stmt:
> 
 >      preparedStatements.clear();
"
1,"VersionHistory.removeVersion() does not throw ReferentialIntegrityException. Inside an XATransaction immediately removing a version that was created by a checkin succeeds, even though it should fail because referential integrity is violated. The reason seems to be that the created version does not return any references.

In the end the transaction fails because referential integrity is checked again in the SharedItemStateManager, which is correct. But IMO removeVersion() should fail first.

Added test case: org.apache.jackrabbit.core.version.CheckinRemoveVersionTest"
1,"IllegalStateException: Authentication state already initialized. Hi,

I am running HttpClient 3.0 RC2 in my application and a user send me a logfile
telling ""IllegalStateException: Authentication state already initialized"". 

He wanted to access a site on SUN.com and is behind a proxy. The site seems to
redirect to a different domain.

I have attached a Debug+Trace HttpClient log.

Ben"
1,"Deadlock in DefaultISMLocking. There seems to be a bug in DefaultISMLocking which was detected as part of JCR-2746.

1) The main thread gets a read lock.

2) The ObservationManager thread tries to lock for writing, which is blocked because there is still a read lock.

3) Then the main thread tries to get a second read lock, which is blocked because there is a waiting write lock.

The bug was introduced as part of JCR-2089 (Use java.util.concurrent), revisions 995411 and 995412. I think the safe solution is to revert those to commits, and add a test case. If the DefaultISMLocking is changed later on, more test cases are required. An efficient solution is relatively complicated.
"
1,"trunk tests hang/deadlock TestIndexWriterWithThreads. trunk tests have been hanging often lately in hudson, this time i was careful to kill and get a good stacktrace:"
1,TestCompoundFile fails on windows. ant test-core -Dtestcase=TestCompoundFile -Dtestmethod=testReadNestedCFP -Dtests.seed=-61cb66ec0d71d1ac:-46685c36ec38fd32:568c63299214892c
1,"Do not increment revison while target workspace is not initialized. It can happen that a workspace is not initialized at all during a syncronize should be performed.
This can happen when a cluster member performs a re-index of a workspace. The changelog should not be ignored it
should be processed after the workspace is initailized."
1,"BytesRef copy short missed the length setting. when storing a short type integer to BytesRef, BytesRef missed the length setting. then it will cause the storage size is ZERO if no continuous options on this BytesRef"
1,"charset in Content-Type header shouldn't be in quotes. The charset value in the Content-Type header returned from IOUtil.buildContentType is enclosed in quotes. This value should be a token which does not include double quotes.

Index: C:/jprojects/eclipse/jackrabbit/jcr-server/server/src/java/org/apache/jackrabbit/server/io/IOUtil.java
===================================================================
--- C:/jprojects/eclipse/jackrabbit/jcr-server/server/src/java/org/apache/jackrabbit/server/io/IOUtil.java	(revision 397215)
+++ C:/jprojects/eclipse/jackrabbit/jcr-server/server/src/java/org/apache/jackrabbit/server/io/IOUtil.java	(working copy)
@@ -112,7 +112,7 @@
     public static String buildContentType(String mimeType, String encoding) {
         String contentType = mimeType;
         if (contentType != null && encoding != null) {
-            contentType += ""; charset=\"""" + encoding + ""\"""";
+            contentType += ""; charset="" + encoding;
         }
         return contentType;
     }
"
1,"inconsistent session state after Item/Session.save() throwing ReferentialIntegrityException. issue reported by Tomasz.Dabrowski@cognifide.com on jackrabbit dev list.

code fragment to reproduce issue:

// setup test case

Node parent = root.addNode(""a"", ""nt:unstructured"");
Node child1 = parent.addNode(""b"", ""nt:unstructured"");
child1.addMixin(""mix:referenceable"");
Node child2 = parent.addNode(""c"", ""nt:unstructured"");
child2.setProperty(""ref"", child1);
root.save();

// perform test

try {
    child1.remove();
    parent.save();
} catch (ReferentialIntegrityException rie) {
    // expected since child1 is still being referenced by property ""ref"" of child2
}

parent.remove();     // ==> should succeed but throws ItemNotFoundException 

"
1,"Response Folded Headers throws HttpException. As of 4/4/02 CVS repository the HttpMethodBase class
doesn't handle folded headers in the 
readResponseHeaders method

HTTP/1.1 and HTTP/1.0 descriptions of folded headers (see 
section 2.2 Basic Rules)
http://www.ietf.org/rfc/rfc2616.txt
http://www-
old.ics.uci.edu/pub/ietf/http/rfc1945.html#Basic-Rules

I've prepared a patch and was 
emailed to jakarta-commons@jakarta.apache.org"
1,"Session.import() failes to resolve propert property definition in some cases. Some Properties get assigned the wrong definiton when imported via SysView XML.

The selecteion of the definition failes under the following condition:
The nodetype contains a multi-valued property and a single-valued
residual property.
If the data to be imported than contains only one value for the multivalued property, it will be created with the residual definition.
A later access to this propertie's values will fail with an ValueFormatException.

Example:
Node-Type
 - Property
  - name: myapp:name
  - mulitple: true
 - Property
  - name: *
  - multible: false

Sysview:
<sv:node sv:name=""somenode"">
  <sv:property sv:name=""jcr:primaryType"" sv:type=""Name"">
   <sv:value>myapp:sampleNt</sv:value> 
  </sv:property>
 
  <sv:property sv:name=""myapp:name"" sv:type=""String"">
   <sv:value>At least I could have multi values</sv:value> 
  </sv:property>
</sv:node>

=> The ""mayapp:name"" will be imported into the residule property."
1,"NPE in EventStateCollection. When removing a Version with a versionlabel and restoring an other Version from the same containing history within 1 transaction, a NPE occured. When debugging I noticed the method createEventStates was entered with an UUID from a versionLabel. The ChangeLog.get(id) returned null.

Caused by: java.lang.NullPointerException
	at org.apache.jackrabbit.core.observation.EventStateCollection.getNodeType(EventStateCollection.java:614)
	at org.apache.jackrabbit.core.observation.EventStateCollection.createEventStates(EventStateCollection.java:381)
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:697)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:1085)
	at org.apache.jackrabbit.core.state.XAItemStateManager.prepare(XAItemStateManager.java:163)
	at org.apache.jackrabbit.core.version.XAVersionManager.prepare(XAVersionManager.java:509)
	at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:154)
	at org.apache.jackrabbit.core.XASessionImpl.prepare(XASessionImpl.java:331)
	at org.springmodules.jcr.jackrabbit.support.JackRabbitUserTransaction.commit(JackRabbitUserTransaction.java:100)
	at org.springmodules.jcr.jackrabbit.LocalTransactionManager.doCommit(LocalTransactionManager.java:192)"
1,"JcrParser: use of bitwise instead of logical AND operator. JcrParser, line 134:

            if ((!insideSingleQuote & !insideDoubleQuote & Character
"
1,"NullpointerException in SessionItemStateManager. I got the following exception which is not reproducible and occured during a large batch of write operations. Unfortunately I got no idea how this happened. May be someone has an idea?

[2006-11-27 21:43:53,065, WARN ] {} support.RemoteInvocationTraceInterceptor:80: Processing of RmiServiceExporter remote call resulted in fatal exception: com.subshell.sophora.content.server.IContentManager.importDocument
org.springframework.transaction.TransactionSystemException: Could not commit JCR transaction; nested exception is java.lang.NullPointerException Caused by: 
java.lang.NullPointerException
        at org.apache.jackrabbit.core.state.SessionItemStateManager.nodeModified(SessionItemStateManager.java:878)
        at org.apache.jackrabbit.core.state.StateChangeDispatcher.notifyNodeModified(StateChangeDispatcher.java:143)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.stateModified(LocalItemStateManager.java:426)
        at org.apache.jackrabbit.core.state.StateChangeDispatcher.notifyStateModified(StateChangeDispatcher.java:85)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.stateModified(SharedItemStateManager.java:388)
        at org.apache.jackrabbit.core.state.StateChangeDispatcher.notifyStateModified(StateChangeDispatcher.java:85)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.stateModified(SharedItemStateManager.java:388)
        at org.apache.jackrabbit.core.state.ItemState.notifyStateUpdated(ItemState.java:241)
        at org.apache.jackrabbit.core.state.ChangeLog.persisted(ChangeLog.java:271)
        at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:691)
        at org.apache.jackrabbit.core.state.XAItemStateManager.commit(XAItemStateManager.java:169)
        at org.apache.jackrabbit.core.version.XAVersionManager.commit(XAVersionManager.java:478)
        at org.apache.jackrabbit.core.TransactionContext.commit(TransactionContext.java:172)
        at org.apache.jackrabbit.core.XASessionImpl.commit(XASessionImpl.java:315)
        at org.springmodules.jcr.jackrabbit.support.JackRabbitUserTransaction.commit(JackRabbitUserTransaction.java:104)
        at org.springmodules.jcr.jackrabbit.LocalTransactionManager.doCommit(LocalTransactionManager.java:192)
        at org.springframework.transaction.support.AbstractPlatformTransactionManager.processCommit(AbstractPlatformTransactionManager.java:540)
        at org.springframework.transaction.support.AbstractPlatformTransactionManager.commit(AbstractPlatformTransactionManager.java:510)
        at org.springframework.transaction.interceptor.TransactionAspectSupport.commitTransactionAfterReturning(TransactionAspectSupport.java:310)
        at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:117)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:185)
        at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:89)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:185)
        at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:209)
        at $Proxy14.importDocument(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:318)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:203)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:162)
        at org.springframework.remoting.support.RemoteInvocationTraceInterceptor.invoke(RemoteInvocationTraceInterceptor.java:70)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:185)
        at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:209)
        at $Proxy15.importDocument(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor21.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.springframework.remoting.support.RemoteInvocation.invoke(RemoteInvocation.java:181)
        at org.springframework.remoting.support.DefaultRemoteInvocationExecutor.invoke(DefaultRemoteInvocationExecutor.java:38)
        at org.springframework.remoting.support.RemoteInvocationBasedExporter.invoke(RemoteInvocationBasedExporter.java:76)
        at org.springframework.remoting.rmi.RmiBasedExporter.invoke(RmiBasedExporter.java:72)
        at org.springframework.remoting.rmi.RmiInvocationWrapper.invoke(RmiInvocationWrapper.java:62)
        at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:294)
        at sun.rmi.transport.Transport$1.run(Transport.java:153)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:149)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:460)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:701)
        at java.lang.Thread.run(Thread.java:595)
"
1,"setFlushPending fails if we concurrently hit a aborting exception. If we select a DWPT for flushing but that DWPT is currently in flight and hits an exception after we selected them for flushing the num of docs is reset to 0 and we trip that exception. So we rather check if it is > 0 than assert on it here.
{noformat}
[junit] Testsuite: org.apache.lucene.index.TestIndexWriterExceptions
    [junit] Testcase: testRandomExceptionsThreads(org.apache.lucene.index.TestIndexWriterExceptions):	FAILED
    [junit] thread Indexer 3: hit unexpected failure
    [junit] junit.framework.AssertionFailedError: thread Indexer 3: hit unexpected failure
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions.testRandomExceptionsThreads(TestIndexWriterExceptions.java:227)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1226)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1154)
    [junit] 
    [junit] 
    [junit] Tests run: 18, Failures: 1, Errors: 0, Time elapsed: 30.287 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] Indexer 3: unexpected exception2
    [junit] java.lang.AssertionError
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushControl.setFlushPending(DocumentsWriterFlushControl.java:170)
    [junit] 	at org.apache.lucene.index.FlushPolicy.markLargestWriterPending(FlushPolicy.java:108)
    [junit] 	at org.apache.lucene.index.FlushByRamOrCountsPolicy.onInsert(FlushByRamOrCountsPolicy.java:61)
    [junit] 	at org.apache.lucene.index.FlushPolicy.onUpdate(FlushPolicy.java:77)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushControl.doAfterDocument(DocumentsWriterFlushControl.java:115)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:341)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1367)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1339)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread.run(TestIndexWriterExceptions.java:92)
    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterExceptions -Dtestmethod=testRandomExceptionsThreads -Dtests.seed=3493970007652348212:2010109588873167237
    [junit] WARNING: test method: 'testRandomExceptionsThreads' left thread running: merge thread: _1v(4.0):Cv2 _27(4.0):cv1 into _2h
    [junit] WARNING: test method: 'testRandomExceptionsThreads' left thread running: merge thread: _2c(4.0):cv1 into _2m
    [junit] RESOURCE LEAK: test method: 'testRandomExceptionsThreads' left 2 thread(s) running
    [junit] NOTE: test params are: codec=RandomCodecProvider: {content=MockFixedIntBlock(blockSize=421), field=MockSep, id=SimpleText, other=MockSep, contents=MockRandom, content1=Pulsing(freqCutoff=11), content2=MockSep, content4=SimpleText, content5=SimpleText, content6=MockRandom, crash=MockRandom, content7=MockVariableIntBlock(baseBlockSize=109)}, locale=mk_MK, timezone=Europe/Malta
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestToken, TestDateTools, Test2BTerms, TestAddIndexes, TestFilterIndexReader, TestIndexWriterExceptions]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=8,threads=1,free=78897400,total=195821568
{noformat}"
1,"Request with two forward slashes for path fails. The following code demonstrates the problem:
        DefaultHttpClient client = new DefaultHttpClient();
        client.execute(new HttpGet(""http://www.google.com//""));

When a request is made, the DefaultRequestDirector invokes rewriteRequestURI(). I don't fully understand why this method does what it does. For a non-proxied request, it attempts to render the URI to a relative URI. In doing so, it tries to create a relative URI whose content is ""//"". Per RFC 2396 section 5 (Relative URI References), a relative URI that begins with ""//"" is a network-path reference, and the ""//"" must be immediately followed by an authority. Therefore, while ""http://www.google.com//"" is a valid absolute URI, ""//"" is not a valid relative one. The resulting exception:

[...]
Caused by: org.apache.http.ProtocolException: Invalid URI: http://www.google.com//
	at org.apache.http.impl.client.DefaultRequestDirector.rewriteRequestURI(DefaultRequestDirector.java:339)
	at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:434)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:641)
	... 31 more
Caused by: java.net.URISyntaxException: Expected authority at index 2: //
	at java.net.URI$Parser.fail(URI.java:2809)
	at java.net.URI$Parser.failExpecting(URI.java:2815)
	at java.net.URI$Parser.parseHierarchical(URI.java:3063)
	at java.net.URI$Parser.parse(URI.java:3024)
	at java.net.URI.<init>(URI.java:578)
	at org.apache.http.client.utils.URIUtils.createURI(URIUtils.java:106)
	at org.apache.http.client.utils.URIUtils.rewriteURI(URIUtils.java:141)
	at org.apache.http.client.utils.URIUtils.rewriteURI(URIUtils.java:159)
	at org.apache.http.impl.client.DefaultRequestDirector.rewriteRequestURI(DefaultRequestDirector.java:333)
	... 33 more
"
1,Background text extraction not possible when supportHighlighting is set true. There is an IndexingQueue that holds nodes that are indexed with a background thread when text extraction takes more time than a configurable limit. When supportHighlighting is set to true the IndexingQueue is never used because the text extract is immediately requested in NodeIndexer. Instead the text extract should be retrieved only when the node is added to the index. 
1,"DocumentsWriter blocks flushes when applyDeletes takes forever - memory not released. In DocumentsWriter we have a safety check that applies all deletes if the deletes consume too much RAM to prevent too-frequent flushing of a long tail of tiny segments. If we enter applyAllDeletes we essentially lock on IW -> BufferedDeletes which is fine since this usually doesn't take long and doesn't keep DWPTs from indexing. Yet, if that takes long and at the same time a semgent is flushed and subsequently published to the IW we take the lock on the ticket queue and the IW. Now this prevents all other threads to append to the ticketQueue which is done BEFORE we actually flush the segment concurrently and free up the RAM.

Essentially its ok to block on the IW lock but we should not keep concurrent flushed from execution just because we apply deletes. The threads will block once they try to execute maybeMerge after the segment is flushed so we don't pile up subsequent memory but we should actually allow the DWPT to be flushed since we actually try to get rid of memory.

I ran into this by accident due to a coding bug using delete queries instead of terms for each document. This thread dump show the problem:

{noformat}
""Application Worker Thread"" prio=10 tid=0x00007fdda0238000 nid=0x3256 waiting for
monitor entry [0x00007fddad3c2000]
  java.lang.Thread.State: BLOCKED (on object monitor)
       at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:424)
       - waiting to lock <0x00007fddb74ff990> (a
org.apache.lucene.index.DocumentsWriter$TicketQueue)
       at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:320)
       at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:393)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1484)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)

""Application Worker Thread"" prio=10 tid=0x00007fdda0236000 nid=0x3255 waiting for
monitor entry [0x00007fddad4c3000]
  java.lang.Thread.State: BLOCKED (on object monitor)
       at org.apache.lucene.index.IndexWriter.updatePendingMerges(IndexWriter.java:1854)
       - waiting to lock <0x00007fddb74fe350> (a
org.apache.solr.update.SolrIndexWriter)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1848)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1843)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1493)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)

""Application Worker Thread"" prio=10 tid=0x00007fdda0234000 nid=0x3254 waiting for
monitor entry [0x00007fddad5c4000]
  java.lang.Thread.State: BLOCKED (on object monitor)
       at org.apache.lucene.index.IndexWriter.updatePendingMerges(IndexWriter.java:1854)
       - waiting to lock <0x00007fddb74fe350> (a
org.apache.solr.update.SolrIndexWriter)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1848)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1843)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1493)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)

""Application Worker Thread"" prio=10 tid=0x00007fdda0232000 nid=0x3253 waiting for
monitor entry [0x00007fddad6c5000]
  java.lang.Thread.State: BLOCKED (on object monitor)
       at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:424)
       - waiting to lock <0x00007fddb74ff990> (a
org.apache.lucene.index.DocumentsWriter$TicketQueue)
       at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:320)
       at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:393)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1484)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)

""Application Worker Thread"" prio=10 tid=0x00007fdda0230800 nid=0x3252 waiting for
monitor entry [0x00007fddad7c6000]
  java.lang.Thread.State: BLOCKED (on object monitor)
       at org.apache.lucene.index.IndexWriter.updatePendingMerges(IndexWriter.java:1854)
       - waiting to lock <0x00007fddb74fe350> (a
org.apache.solr.update.SolrIndexWriter)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1848)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1843)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1493)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)

""Application Worker Thread"" prio=10 tid=0x00007fdda022e800 nid=0x3251 runnable
[0x00007fddad8c6000]
  java.lang.Thread.State: RUNNABLE
       at java.nio.Bits.copyToArray(Bits.java:715)
       at java.nio.DirectByteBuffer.get(DirectByteBuffer.java:233)
       at org.apache.lucene.store.MMapDirectory$MMapIndexInput.readBytes(MMapDirectory.java:319)
       at org.apache.lucene.index.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum$Frame.loadBlock(BlockTreeTermsReader.java:2283)
       at org.apache.lucene.index.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum.seekExact(BlockTreeTermsReader.java:1600)
       at org.apache.lucene.util.TermContext.build(TermContext.java:97)
       at org.apache.lucene.search.TermQuery.createWeight(TermQuery.java:180)
       at org.apache.lucene.search.BooleanQuery$BooleanWeight.<init>(BooleanQuery.java:186)
       at org.apache.lucene.search.BooleanQuery.createWeight(BooleanQuery.java:423)
       at org.apache.lucene.search.IndexSearcher.createNormalizedWeight(IndexSearcher.java:583)
       at org.apache.lucene.search.QueryWrapperFilter.getDocIdSet(QueryWrapperFilter.java:55)
       at org.apache.lucene.index.BufferedDeletesStream.applyQueryDeletes(BufferedDeletesStream.java:431)
       at org.apache.lucene.index.BufferedDeletesStream.applyDeletes(BufferedDeletesStream.java:268)
       - locked <0x00007fddb751e1e8> (a
org.apache.lucene.index.BufferedDeletesStream)
       at org.apache.lucene.index.IndexWriter.applyAllDeletes(IndexWriter.java:2852)
       - locked <0x00007fddb74fe350> (a org.apache.solr.update.SolrIndexWriter)
       at org.apache.lucene.index.DocumentsWriter.applyAllDeletes(DocumentsWriter.java:188)
       at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:470)
       at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:320)
       at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:393)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1484)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)
       

""Application Worker Thread"" prio=10 tid=0x00007fdda022d800 nid=0x3250 waiting for
monitor entry [0x00007fddad9c8000]
  java.lang.Thread.State: BLOCKED (on object monitor)
       at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:424)
       - waiting to lock <0x00007fddb74ff990> (a
org.apache.lucene.index.DocumentsWriter$TicketQueue)
       at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:320)
       at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:393)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1484)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)
   
""Application Worker Thread"" prio=10 tid=0x00007fdda022d000 nid=0x324f waiting for
monitor entry [0x00007fddadac9000]
  java.lang.Thread.State: BLOCKED (on object monitor)
       at org.apache.lucene.index.IndexWriter.useCompoundFile(IndexWriter.java:2274)
       - waiting to lock <0x00007fddb74fe350> (a
org.apache.solr.update.SolrIndexWriter)
       at org.apache.lucene.index.IndexWriter.prepareFlushedSegment(IndexWriter.java:2156)
       at org.apache.lucene.index.DocumentsWriter.publishFlushedSegment(DocumentsWriter.java:526)
       at org.apache.lucene.index.DocumentsWriter.finishFlush(DocumentsWriter.java:506)
       at org.apache.lucene.index.DocumentsWriter.applyFlushTickets(DocumentsWriter.java:483)
       - locked <0x00007fddb74ff990> (a
org.apache.lucene.index.DocumentsWriter$TicketQueue)
       at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:449)
       at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:320)
       at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:393)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1484)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)

{noformat}"
1,"CharReader should delegate reset/mark/markSupported. The final class CharReader should delegate reset/mark/markSupported to its wrapped reader. Otherwise clients will get ""reset() not supported"" exception."
1,"NullPointerException when accessing the SimpleWebdavServlet at the prefix path. When accessing the SimpleWebdavServlet with the ""root"" path, that is the same path as set with the resource-path-prefix, a NullPointerException is thrown:

java.lang.NullPointerException
	at org.apache.jackrabbit.name.ParsingPathResolver.getQPath(ParsingPathResolver.java:91)
	at org.apache.jackrabbit.name.CachingPathResolver.getQPath(CachingPathResolver.java:74)
	at org.apache.jackrabbit.core.SessionImpl.getQPath(SessionImpl.java:601)
	at org.apache.jackrabbit.core.SessionImpl.getItem(SessionImpl.java:804)
	at org.apache.sling.jcr.api.internal.PooledSession.getItem(PooledSession.java:157)
	at org.apache.jackrabbit.webdav.simple.ResourceFactoryImpl.getNode(ResourceFactoryImpl.java:140)
	at org.apache.jackrabbit.webdav.simple.ResourceFactoryImpl.createResource(ResourceFactoryImpl.java:89)
	at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.service(AbstractWebdavServlet.java:187)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:487)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:362)
	at org.ops4j.pax.web.service.internal.HttpServiceServletHandler.handle(HttpServiceServletHandler.java:51)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:722)
	at org.ops4j.pax.web.service.internal.HttpServiceContext.handle(HttpServiceContext.java:87)
	at org.ops4j.pax.web.service.internal.JettyServerHandlerCollection.handle(JettyServerHandlerCollection.java:63)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:139)
	at org.mortbay.jetty.Server.handle(Server.java:324)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:505)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:828)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:514)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:211)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:380)
	at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)
	at org.mortbay.thread.BoundedThreadPool$PoolThread.run(BoundedThreadPool.java:450)


The problem seems to be that the ResourceFactoryImpl.createResource method (or rather the getNode method) is not prepared to a DavResourceLocator instance whose resourcePath is null.

I could imagine, that the ResourceFactoryImpl.getNode() method might want to return the root node in this case ?"
1,"Errors during concurrent session import of nodes with same UUIDs. 21.08.2009 16:22:14 *ERROR* [Executor 0] ConnectionRecoveryManager: could not execute statement, reason: The statement was aborted because it would have caused a duplicate key value in a unique or primary key constraint or unique index identified by 'SQL090821042140130' defined on 'DEFAULT_BUNDLE'., state/code: 23505/20000 (ConnectionRecoveryManager.java, line 453)
21.08.2009 16:22:14 *ERROR* [Executor 0] BundleDbPersistenceManager: failed to write bundle: 6c292772-349e-42b3-8255-7729615c67de (BundleDbPersistenceManager.java, line 1212)
ERROR 23505: The statement was aborted because it would have caused a duplicate key value in a unique or primary key constraint or unique index identified by 'SQL090821042140130' defined on 'DEFAULT_BUNDLE'.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.IndexChanger.insertAndCheckDups(Unknown Source)
	at org.apache.derby.impl.sql.execute.IndexChanger.doInsert(Unknown Source)
	at org.apache.derby.impl.sql.execute.IndexChanger.insert(Unknown Source)
	at org.apache.derby.impl.sql.execute.IndexSetChanger.insert(Unknown Source)
	at org.apache.derby.impl.sql.execute.RowChangerImpl.insertRow(Unknown Source)
	at org.apache.derby.impl.sql.execute.InsertResultSet.normalInsertCore(Unknown Source)
	at org.apache.derby.impl.sql.execute.InsertResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.execute(Unknown Source)
	at org.apache.jackrabbit.core.persistence.bundle.util.ConnectionRecoveryManager.executeStmtInternal(ConnectionRecoveryManager.java:371)
	at org.apache.jackrabbit.core.persistence.bundle.util.ConnectionRecoveryManager.executeStmtInternal(ConnectionRecoveryManager.java:298)
	at org.apache.jackrabbit.core.persistence.bundle.util.ConnectionRecoveryManager.executeStmt(ConnectionRecoveryManager.java:261)
	at org.apache.jackrabbit.core.persistence.bundle.util.ConnectionRecoveryManager.executeStmt(ConnectionRecoveryManager.java:239)
	at org.apache.jackrabbit.core.persistence.bundle.BundleDbPersistenceManager.storeBundle(BundleDbPersistenceManager.java:1209)
	at org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.putBundle(AbstractBundlePersistenceManager.java:709)
	at org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.storeInternal(AbstractBundlePersistenceManager.java:651)
	at org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.store(AbstractBundlePersistenceManager.java:527)
	at org.apache.jackrabbit.core.persistence.bundle.BundleDbPersistenceManager.store(BundleDbPersistenceManager.java:563)
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:724)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:1101)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:351)
	at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:354)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:326)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:326)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1098)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:925)
	at org.apache.jackrabbit.core.ConcurrentImportTest$1.execute(ConcurrentImportTest.java:73)
	at org.apache.jackrabbit.core.AbstractConcurrencyTest$Executor.run(AbstractConcurrencyTest.java:209)
	at java.lang.Thread.run(Thread.java:637)
"
1,"return value of PostMethod#removeParameter. <HttpClient2.0-rc1>

About 
public boolean removeParameter(String paramName)
                 throws IllegalArgumentException
method.

-------------------------------------------------
PostMethod method = new PostMethod(uri);
method.addParameter(""name"", ""Matsui Hideki"");
boolean b;
b = method.removeParameter(""name""); // returns ""true"".
b = method.removeParameter(""XXXX""); // returns ""true"". why??? 
---------------------------------------------------

sorry for my poor english."
1,"MultiSearcher.explain returns incorrect score/explanation relating to docFreq. Creating 2 different indexes, searching  each individually and print score details and compare to searching both indexes with MulitSearcher and printing score details.  
 
The ""docFreq"" value printed isn't correct - the values it prints are as if each index was searched individually.

Code is like:
{code}
MultiSearcher multi = new MultiSearcher(searchables);
Hits hits = multi.search(query);
for(int i=0; i<hits.length(); i++)
{
  Explanation expl = multi.explain(query, hits.id(i));
  System.out.println(expl.toString());
}
{code}

I raised this in the Lucene user mailing list and was advised to log a bug, email thread given below.

{noformat} 
-----Original Message-----
From: Chris Hostetter  
Sent: Friday, December 07, 2007 10:30 PM
To: java-user
Subject: Re: does the MultiSearcher class calculate IDF properly?


a quick glance at the code seems to indicate that MultiSearcher has code 
for calcuating the docFreq accross all of the Searchables when searching 
(or when the docFreq method is explicitly called) but that explain method 
just delegates to Searchable that the specific docid came from.

if you compare that Explanation score you got with the score returned by 
a HitCollector (or TopDocs) they probably won't match.

So i would say ""yes MultiSearcher calculates IDF properly, but 
MultiSeracher.explain is broken.  Please file a bug about this, i can't 
think of an easy way to fix it, but it certianly seems broken to me.


: Subject: does the MultiSearcher class calculate IDF properly?
: 
: I tried the following.  Creating 2 different indexes, search each
: individually and print score details and compare to searching both
: indexes with MulitSearcher and printing score details.  
: 
: The ""docFreq"" value printed don't seem right - is this just a problem
: with using Explain together with the MultiSearcher?
: 
: 
: Code is like:
: MultiSearcher multi = new MultiSearcher(searchables);
: Hits hits = multi.search(query);
: for(int i=0; i<hits.length(); i++)
: {
:   Explanation expl = multi.explain(query, hits.id(i));
:   System.out.println(expl.toString());
: }
: 
: 
: Output:
: id = 14 score = 0.071
: 0.07073946 = (MATCH) fieldWeight(contents:climate in 2), product of:
:   1.0 = tf(termFreq(contents:climate)=1)
:   1.8109303 = idf(docFreq=1)
:   0.0390625 = fieldNorm(field=contents, doc=2)
{noformat} "
1,"Unable to login with two different Credentials to same workspace in one Transaction. I'm using the Jackrabbit 1.2.1 JCA adapter and trying to access in a SessionBean-Method with Container Transaction a Workspace with 2 different Credentials. 
The Method takes about 400ms to finish but no commit on TransactionContextr occurs (Debugging ..) only the prepare was called 2 times .
The Container hangs on the PostInvoke Method about 5 seconds and then i get a ""javax.transaction.xa.XAException"" 
with the Warn Message: Transaction rolled back because timeout expired

The code ..
Context ctx = new InitialContext(); 
Repository repository = (Repository) ctx.lookup(""java:comp/env/jackrabbit""); 
Credentials credentials = new SimpleCredentials(""user1"", ""password1"".toCharArray()); 
Credentials credentials2 = new SimpleCredentials(""user2"", ""password2"".toCharArray()); 
Session session1 = repository.login(credentials, ""default""); 
Session session2 = repository.login(credentials2, ""default""); 

Session1 adds a node to the workspace .. and with the session2 i do nothing except the login !
If i make no second login the Method works fine."
1,"FilteredQuery explanation inaccuracy with boost. The value of explanation is different than the product of its part if boost > 1.
This is exposed after tightening the explanation check (part of LUCENE-446).

"
1,"ParallelReader crashes when trying to merge into a new index. ParallelReader causes a NullPointerException in
org.apache.lucene.index.ParallelReader$ParallelTermPositions.seek(ParallelReader.java:318)
when trying to merge into a new index.

See test case and sample output:

$ svn diff
Index: src/test/org/apache/lucene/index/TestParallelReader.java
===================================================================
--- src/test/org/apache/lucene/index/TestParallelReader.java    (revision 179785)
+++ src/test/org/apache/lucene/index/TestParallelReader.java    (working copy)
@@ -57,6 +57,13 @@
 
   }
  
+  public void testMerge() throws Exception {
+    Directory dir = new RAMDirectory();
+    IndexWriter w = new IndexWriter(dir, new StandardAnalyzer(), true);
+    w.addIndexes(new IndexReader[] { ((IndexSearcher)
parallel).getIndexReader() });
+    w.close();
+  }
+
   private void queryTest(Query query) throws IOException {
     Hits parallelHits = parallel.search(query);
     Hits singleHits = single.search(query);
$ ant -Dtestcase=TestParallelReader test
Buildfile: build.xml
[...]
test:
    [mkdir] Created dir:
/Users/skirsch/text/lectures/da/thirdparty/lucene-trunk/build/test
    [junit] Testsuite: org.apache.lucene.index.TestParallelReader
    [junit] Tests run: 2, Failures: 0, Errors: 1, Time elapsed: 1.993 sec

    [junit] Testcase: testMerge(org.apache.lucene.index.TestParallelReader):  
Caused an ERROR
    [junit] null
    [junit] java.lang.NullPointerException
    [junit]     at
org.apache.lucene.index.ParallelReader$ParallelTermPositions.seek(ParallelReader.java:318)
    [junit]     at
org.apache.lucene.index.ParallelReader$ParallelTermDocs.seek(ParallelReader.java:294)
    [junit]     at
org.apache.lucene.index.SegmentMerger.appendPostings(SegmentMerger.java:325)
    [junit]     at
org.apache.lucene.index.SegmentMerger.mergeTermInfo(SegmentMerger.java:296)
    [junit]     at
org.apache.lucene.index.SegmentMerger.mergeTermInfos(SegmentMerger.java:270)
    [junit]     at
org.apache.lucene.index.SegmentMerger.mergeTerms(SegmentMerger.java:234)
    [junit]     at
org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:96)
    [junit]     at
org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:596)
    [junit]     at
org.apache.lucene.index.TestParallelReader.testMerge(TestParallelReader.java:63)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit]     at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit]     at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)


    [junit] Test org.apache.lucene.index.TestParallelReader FAILED

BUILD FAILED
/Users/skirsch/text/lectures/da/thirdparty/lucene-trunk/common-build.xml:188:
Tests failed!

Total time: 16 seconds
$"
1,"SloppyPhraseScorer sometimes computes Infinite freq. reported on user list:
http://www.lucidimagination.com/search/document/400cbc528ed63db9/score_of_infinity_on_dismax_query
"
1,"Impossible comparison in NodeTypeImpl. org.apache.jackrabbit.jcr2spi.nodetype.NodeTypeImpl does

    public boolean isNodeType(Name nodeTypeName) {
        return getName().equals(nodeTypeName) ||  ent.includesNodeType(nodeTypeName);
    }


as getName() is a string and nodeTypeName is a Name this will always be false. Perhaps you meant

    public boolean isNodeType(Name nodeTypeName) {
        return getName().equals(nodeTypeName.getLocalName()) ||  ent.includesNodeType(nodeTypeName);
    }

"
1,"Problem importing node with binary property in a repository configured with datastore. Using the importXML method of workspace to import some node containing binary properties the nodes are imported correctly and the value of the binary data property is imported
However the binary data goes to the db (persistenceManager) an not to the datastore.

Creating a new node of the same type using the api, the binary data go to the datastore."
1,"""Directory was previously created with a different LockFactory"" when open, close, delete a repository in a loop. Opening a TransientRepository in a loop throws the exception ""Directory was previously created with a different LockFactory instance"".

Test case:

for (int i = 0; i < 3; i++) {
	FileUtils.deleteDirectory(new File(""repository""));
	Repository rep = new TransientRepository();
	Session session = rep.login(new SimpleCredentials("""", new char[0]));
	session.logout();
}

The problem seems to be that org.apache.lucene.store.FSDirectory.DIRECTORIES is not cleared (FSDirectory.close() is not called?).

Stack trace:

Exception in thread ""main"" javax.jcr.RepositoryException: Directory was previously created with a different LockFactory instance; please pass null as the lockFactory instance and use setLockFactory to change it: Directory was previously created with a different LockFactory instance; please pass null as the lockFactory instance and use setLockFactory to change it: Directory was previously created with a different LockFactory instance; please pass null as the lockFactory instance and use setLockFactory to change it
	at org.apache.jackrabbit.core.SearchManager.initializeQueryHandler(SearchManager.java:555)
	at org.apache.jackrabbit.core.SearchManager.<init>(SearchManager.java:239)
	at org.apache.jackrabbit.core.RepositoryImpl.getSystemSearchManager(RepositoryImpl.java:688)
	at org.apache.jackrabbit.core.RepositoryImpl.access$3(RepositoryImpl.java:681)
	at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.getSearchManager(RepositoryImpl.java:1780)
	at org.apache.jackrabbit.core.RepositoryImpl.initWorkspace(RepositoryImpl.java:667)
	at org.apache.jackrabbit.core.RepositoryImpl.initStartupWorkspaces(RepositoryImpl.java:480)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:321)
	at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:618)
	at org.apache.jackrabbit.core.TransientRepository$2.getRepository(TransientRepository.java:241)
	at org.apache.jackrabbit.core.TransientRepository.startRepository(TransientRepository.java:261)
Caused by: java.io.IOException: Directory was previously created with a different LockFactory instance; please pass null as the lockFactory instance and use setLockFactory to change it
	at org.apache.lucene.store.FSDirectory.getDirectory(FSDirectory.java:192)
	at org.apache.jackrabbit.core.query.lucene.directory.FSDirectoryManager.getDirectory(FSDirectoryManager.java:64)
	at org.apache.jackrabbit.core.query.lucene.MultiIndex.<init>(MultiIndex.java:227)
	at org.apache.jackrabbit.core.query.lucene.SearchIndex.doInit(SearchIndex.java:477)
	at org.apache.jackrabbit.core.query.AbstractQueryHandler.init(AbstractQueryHandler.java:59)
"
1,CompactNodeTypeDefWriter does not escaped names properly. CompactNodeTypeDefWriter does not escaped names properly. If the name includes a '-' or a '+' the names must be surrounded by single quotes.
1,"jackrabbit-jcr-rmi: Supplied javax.transaction.xa.Xid is assumed serializable, but is not on some environments.. Websphere provides a non-serializable javax.transaction.xa.Xid implementation, causing ClientXAResource to fail with NotSerializableException when passing Xid over RMI.
I have worked around this by converting the supplied Xid to a local serializable Xid implementation that takes the supplied Xid parameters, and implements hashCode() and equals() correctly:

    private static class SerializableXID implements javax.transaction.xa.Xid, Serializable {
    	/**
		 * Serial version ID
		 */
		private static final long serialVersionUID = -1390620315181450507L;
		
		private final byte[] branchQualifier;
    	private final byte[] globalTransactionId;
    	private final int formatId;
    	private final int hashCode;
   	
    	public SerializableXID(Xid xid) {
    		branchQualifier = xid.getBranchQualifier();
    		globalTransactionId = xid.getGlobalTransactionId();
    		formatId = xid.getFormatId();
    		hashCode = xid.hashCode();
    	}

		public byte[] getBranchQualifier() {
			return branchQualifier;
		}

		public int getFormatId() {
			return formatId;
		}

		public byte[] getGlobalTransactionId() {
			return globalTransactionId;
		}

        public final int hashCode() {
        	return hashCode;
        }
        
        public final boolean equals(Object obj) {
	        if(obj == this) {
	            return true;
	        }
	        
        	if(!(obj instanceof Xid)) {
        		return false;        		
        	}
        	
        	Xid xidimpl = (Xid)obj;
        	if(formatId != xidimpl.getFormatId()) {
        		return false;
        	}
	        else {
	            return Arrays.equals(branchQualifier, xidimpl.getBranchQualifier())
	            	&& Arrays.equals(globalTransactionId, xidimpl.getGlobalTransactionId());
	        }
    	}
    }
"
1,"HttpClient 4.1 ignores request retry handler and stops retrying when a read timeout is followed by a connection refusal. I encountered an issue while writing unit tests for the RestBackup(tm) API Client Library, https://github.com/mleonhard/restbackup-java .  HttpClient 4.1 is failing to retry when it encounters a read timeout followed by a connection refusal.  This problem occurs on Windows but not on Linux.  Below is a short program that reproduces the problem.  It performs the expected 5 request attempts on Linux but only 2 on Windows.

My Windows environment is a laptop with Windows 7 Ultimate 64-bit and Oracle Java SE Development Kit Update 21 32-bit.  My Linux environment is Amazon EC2 with Ubuntu 10.04 LTS 32-bit and Oracle Java SE Development Kit Update 21 32-bit.

This is my first bug report to an Apache project.  I'd like to add that I'm a big fan of the Commons libraries and Http Components.

Sincerely,
-Michael

=== RetryBug.java ===

import java.io.IOException;
import java.net.ServerSocket;
import java.util.logging.Logger;

import org.apache.http.client.HttpRequestRetryHandler;
import org.apache.http.client.methods.HttpGet;
import org.apache.http.impl.client.DefaultHttpClient;
import org.apache.http.params.CoreConnectionPNames;
import org.apache.http.protocol.HttpContext;

public class RetryBug {
    private static final Logger _log = Logger.getLogger(RetryBug.class.getName());

    public static void main(String[] args) throws IOException {
        ServerSocket serverSocket = new ServerSocket(0, 1);
        DefaultHttpClient httpClient = new DefaultHttpClient();
        HttpRequestRetryHandler retryHandler = new HttpRequestRetryHandler() {
                public boolean retryRequest(IOException e, int count, HttpContext context) {
                    _log.info(""count="" + count + "" "" + e.toString());
                    return count < 5;
                }
            };
        httpClient.setHttpRequestRetryHandler(retryHandler);
        httpClient.getParams().setIntParameter(CoreConnectionPNames.SO_TIMEOUT, 100);
        try {
            String url = ""http://127.0.0.1:"" + serverSocket.getLocalPort() + ""/"";
            httpClient.execute(new HttpGet(url));
        } finally {
            serverSocket.close();
        }
    }
}


=== Windows 7 ===

C:\RetryBug>md5sum httpcomponents-client-4.1-bin.zip
008ad15560249bcde42cfe34fdb4e858 *httpcomponents-client-4.1-bin.zip

C:\RetryBug>""c:\Program Files (x86)\Java\jdk1.6.0_21\bin\java.exe"" -version
java version ""1.6.0_21""
Java(TM) SE Runtime Environment (build 1.6.0_21-b06)
Java HotSpot(TM) Client VM (build 17.0-b16, mixed mode)

C:\RetryBug>""c:\Program Files (x86)\Java\jdk1.6.0_21\bin\javac.exe"" -cp httpcomponents-client-4.1\lib\commons-codec-1.4.jar;httpcomponents-client-4.1\lib\commons-logging-1.1.1.jar;httpcomponents-client-4.1\lib\httpclient-4.1.jar;httpcomponents-client-4.1\lib\httpcore-4.1.jar RetryBug.java

C:\RetryBug>""c:\Program Files (x86)\Java\jdk1.6.0_21\bin\java.exe"" -cp httpcomponents-client-4.1\lib\commons-codec-1.4.jar;httpcomponents-client-4.1\lib\commons-logging-1.1.1.jar;httpcomponents-client-4.1\lib\httpclient-4.1.jar;httpcomponents-client-4.1\lib\httpcore-4.1.jar;. RetryBug
Mar 9, 2011 9:14:36 PM RetryBug$1 retryRequest
INFO: count=1 java.net.SocketTimeoutException: Read timed out
Mar 9, 2011 9:14:36 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: I/O exception (java.net.SocketTimeoutException) caught when processing request: Read timed out
Mar 9, 2011 9:14:36 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: Retrying request
Exception in thread ""main"" org.apache.http.conn.HttpHostConnectException: Connection to http://127.0.0.1:56361 refused
        at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:158)
        at org.apache.http.impl.conn.AbstractPoolEntry.open(AbstractPoolEntry.java:149)
        at org.apache.http.impl.conn.AbstractPooledConnAdapter.open(AbstractPooledConnAdapter.java:121)
        at org.apache.http.impl.client.DefaultRequestDirector.tryExecute(DefaultRequestDirector.java:650)
        at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:454)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:820)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:754)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:732)
        at RetryBug.main(RetryBug.java:27)
Caused by: java.net.ConnectException: Connection refused: connect
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333)
        at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195)
        at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
        at java.net.Socket.connect(Socket.java:529)
        at org.apache.http.conn.scheme.PlainSocketFactory.connectSocket(PlainSocketFactory.java:120)
        at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:148)
        ... 8 more

C:\RetryBug>


=== Ubuntu 10 ===

$ md5sum httpcomponents-client-4.1-bin.tar.gz
f043c1cc016cb3b720be9fb020bfa755  httpcomponents-client-4.1-bin.tar.gz
$ ~/jdk1.6.0_21/bin/java -version
java version ""1.6.0_21""
Java(TM) SE Runtime Environment (build 1.6.0_21-b06)
Java HotSpot(TM) Client VM (build 17.0-b16, mixed mode, sharing)
$ ~/jdk1.6.0_21/bin/javac -cp httpcomponents-client-4.1/lib/httpclient-cache-4.1.jar:httpcomponents-client-4.1/lib/commons-logging-1.1.1.jar:httpcomponents-client-4.1/lib/httpcore-4.1.jar:httpcomponents-client-4.1/lib/httpclient-4.1.jar:httpcomponents-client-4.1/lib/httpmime-4.1.jar:httpcomponents-client-4.1/lib/commons-codec-1.4.jar RetryBug.java
$ ~/jdk1.6.0_21/bin/java -cp httpcomponents-client-4.1/lib/httpclient-cache-4.1.jar:httpcomponents-client-4.1/lib/commons-logging-1.1.1.jar:httpcomponents-client-4.1/lib/httpcore-4.1.jar:httpcomponents-client-4.1/lib/httpclient-4.1.jar:httpcomponents-client-4.1/lib/httpmime-4.1.jar:httpcomponents-client-4.1/lib/commons-codec-1.4.jar:. RetryBug
Mar 9, 2011 1:09:42 PM RetryBug$1 retryRequest
INFO: count=1 java.net.SocketTimeoutException: Read timed out
Mar 9, 2011 1:09:42 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: I/O exception (java.net.SocketTimeoutException) caught when processing request: Read timed out
Mar 9, 2011 1:09:42 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: Retrying request
Mar 9, 2011 1:09:42 PM RetryBug$1 retryRequest
INFO: count=2 java.net.SocketTimeoutException: Read timed out
Mar 9, 2011 1:09:42 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: I/O exception (java.net.SocketTimeoutException) caught when processing request: Read timed out
Mar 9, 2011 1:09:42 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: Retrying request
Mar 9, 2011 1:09:42 PM RetryBug$1 retryRequest
INFO: count=3 java.net.SocketTimeoutException: Read timed out
Mar 9, 2011 1:09:42 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: I/O exception (java.net.SocketTimeoutException) caught when processing request: Read timed out
Mar 9, 2011 1:09:42 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: Retrying request
Mar 9, 2011 1:09:42 PM RetryBug$1 retryRequest
INFO: count=4 java.net.SocketTimeoutException: Read timed out
Mar 9, 2011 1:09:42 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: I/O exception (java.net.SocketTimeoutException) caught when processing request: Read timed out
Mar 9, 2011 1:09:42 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: Retrying request
Mar 9, 2011 1:09:51 PM RetryBug$1 retryRequest
INFO: count=5 java.net.SocketTimeoutException: Read timed out
Exception in thread ""main"" java.net.SocketTimeoutException: Read timed out
        at java.net.SocketInputStream.socketRead0(Native Method)
        at java.net.SocketInputStream.read(SocketInputStream.java:129)
        at org.apache.http.impl.io.AbstractSessionInputBuffer.fillBuffer(AbstractSessionInputBuffer.java:149)
        at org.apache.http.impl.io.SocketInputBuffer.fillBuffer(SocketInputBuffer.java:110)
        at org.apache.http.impl.io.AbstractSessionInputBuffer.readLine(AbstractSessionInputBuffer.java:260)
        at org.apache.http.impl.conn.DefaultResponseParser.parseHead(DefaultResponseParser.java:98)
        at org.apache.http.impl.io.AbstractMessageParser.parse(AbstractMessageParser.java:252)
        at org.apache.http.impl.AbstractHttpClientConnection.receiveResponseHeader(AbstractHttpClientConnection.java:281)
        at org.apache.http.impl.conn.DefaultClientConnection.receiveResponseHeader(DefaultClientConnection.java:247)
        at org.apache.http.impl.conn.AbstractClientConnAdapter.receiveResponseHeader(AbstractClientConnAdapter.java:219)
        at org.apache.http.protocol.HttpRequestExecutor.doReceiveResponse(HttpRequestExecutor.java:298)
        at org.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:125)
        at org.apache.http.impl.client.DefaultRequestDirector.tryExecute(DefaultRequestDirector.java:622)
        at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:454)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:820)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:754)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:732)
        at RetryBug.main(RetryBug.java:27)
$"
1,"ObjectContentManagerImpl.getObject(Query) throws NoSuchElementException when query does not match an object. When a query returns no objects, ObjectContentManagerImpl.getObject(Query) throws the following exception:

java.util.NoSuchElementException
        at java.util.AbstractList$Itr.next(AbstractList.java:427)
        at org.apache.jackrabbit.ocm.manager.impl.ObjectContentManagerImpl.getObject(ObjectContentManagerImpl.java:538)

Javadocs for ObjectContentManager interface suggest that a ObjectContentManagerException should be thrown in this case."
1,"shareable nodes: wrong path returned, causes remove() to delete wrong node. It seems that for shareable nodes it can happen that getPath() returns the wrong path (one of another node in the shared set):

/**
* Verify that shared nodes return correct paths.
*/
public void testPath() throws Exception {
   Node a1 = testRootNode.addNode(""a1"");
   Node a2 = a1.addNode(""a2"");
   Node b1 = a1.addNode(""b1"");
   b1.addMixin(""mix:shareable"");
   testRootNode.save();

   //now we have a shareable node N with path a1/b1

   Session session = testRootNode.getSession();
   Workspace workspace = session.getWorkspace();
   String path = a2.getPath() + ""/b2"";
   workspace.clone(workspace.getName(), b1.getPath(), path, false);

   //now we have another shareable node N' in the same shared set as N with path a1/a2/b2

   //using the path a1/a2/b2, we should get the node N' here
   Item item = session.getItem(path);
   String p = item.getPath();
   assertFalse(""unexpectedly got the path from another node from the same shared set "", p.equals(b1.getPath()));
} 

Note that when this happens, a subsequent remove() deletes the wrong node.

(Thanks Manfred for spotting this one)."
1,"Deadlock on concurrent save/checkin operations possible. Save and checkin operations are trying to acquire 2 locks in different order, what leads to deadlock.

->save
1.SharedItemStateManager.acquireWriteLock
2.AbstractVersionManager.acquireWriteLock	->	locked

->checkin
1.AbstractVersionManager.acquireWriteLock
2.SharedItemStateManager.acquireReadLock	->	locked

""Thread-4"" prio=6 tid=0x0312d840 nid=0x824 in Object.wait() [0x03cef000..0x03cefa68]
	at java.lang.Object.wait(Native Method)
	- waiting on <0x23210968> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock)
	at java.lang.Object.wait(Unknown Source)
	at EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock.acquire(Unknown Source)
	- locked <0x23210968> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock)
	at org.apache.jackrabbit.core.version.AbstractVersionManager.acquireWriteLock(AbstractVersionManager.java:124)
	at org.apache.jackrabbit.core.version.VersionManagerImpl.setNodeReferences(VersionManagerImpl.java:413)
	at org.apache.jackrabbit.core.version.VersionItemStateProvider.setNodeReferences(VersionItemStateProvider.java:125)
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:699)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:810)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:326)
	at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:313)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:302)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:295)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1204)
	- locked <0x2332eaa0> (a org.apache.jackrabbit.core.XASessionImpl)
	at JrTestDeadlock.run(JrTestDeadlock.java:87)

""Thread-3"" prio=6 tid=0x0312db18 nid=0xa04 in Object.wait() [0x03caf000..0x03cafae8]
	at java.lang.Object.wait(Native Method)
	- waiting on <0x232d1360> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$ReaderLock)
	at java.lang.Object.wait(Unknown Source)
	at EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$ReaderLock.acquire(Unknown Source)
	- locked <0x232d1360> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$ReaderLock)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.acquireReadLock(SharedItemStateManager.java:1361)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.hasItemState(SharedItemStateManager.java:270)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.hasItemState(LocalItemStateManager.java:180)
	at org.apache.jackrabbit.core.state.XAItemStateManager.hasItemState(XAItemStateManager.java:252)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.hasItemState(SessionItemStateManager.java:188)
	at org.apache.jackrabbit.core.ItemManager.itemExists(ItemManager.java:256)
	at org.apache.jackrabbit.core.NodeImpl.hasProperty(NodeImpl.java:1509)
	at org.apache.jackrabbit.core.version.InternalFrozenNodeImpl.checkin(InternalFrozenNodeImpl.java:276)
	at org.apache.jackrabbit.core.version.InternalFrozenNodeImpl.checkin(InternalFrozenNodeImpl.java:248)
	at org.apache.jackrabbit.core.version.InternalVersionHistoryImpl.checkin(InternalVersionHistoryImpl.java:440)
	at org.apache.jackrabbit.core.version.AbstractVersionManager.checkin(AbstractVersionManager.java:397)
	at org.apache.jackrabbit.core.version.VersionManagerImpl$2.run(VersionManagerImpl.java:289)
	at org.apache.jackrabbit.core.version.VersionManagerImpl$DynamicESCFactory.doSourced(VersionManagerImpl.java:611)
	- locked <0x2320c5d8> (a org.apache.jackrabbit.core.version.VersionManagerImpl$DynamicESCFactory)
	at org.apache.jackrabbit.core.version.VersionManagerImpl.checkin(VersionManagerImpl.java:285)
	at org.apache.jackrabbit.core.version.XAVersionManager.checkin(XAVersionManager.java:161)
	at org.apache.jackrabbit.core.NodeImpl.checkin(NodeImpl.java:2944)
	at JrTestDeadlock.run(JrTestDeadlock.java:103)
"
1,"processing a synonym in a token stream will remove the following token from the stream. If you do a phrase search on a field derived from a fieldtype with the synonym filter which includes a synonym, the term following the synonym vanishes after synonym expansion.

e.g. http://host:port/solr/corename/select/?q=desc:%22xyzzy%20%20bbb%20pot%20of%20gold%22&version=2.2&start=0&rows=10&indent=on&debugQuery=true   (bbb is in the default synonyms file, desc is a ""text"" fieldtype)

outputs
....
<str name=""rawquerystring"">desc:""xyzzy  bbb pot of gold""</str>
<str name=""querystring"">desc:""xyzzy  bbb pot of gold""</str>
<str name=""parsedquery"">PhraseQuery(desc:""xyzzy bbbb 1 bbbb 2 of gold"")</str>
<str name=""parsedquery_toString"">desc:""xyzzy bbbb 1 bbbb 2 of gold""</str>
....

You can also see this behavior using the admin console analysis.jsp

Solr 3.3 behaves properly.
"
1,LuceneTestCase#newFSDirectoryImpl misses to set LockFactory if ctor call throws exception. selckin reported on IRC that if you run ant test -Dtestcase=TestLockFactory -Dtestmethod=testNativeFSLockFactoryPrefix -Dtests.directory=FSDirectory the test fails. Since FSDirectory is an abstract class it can not be instantiated so our code falls back to FSDirector.open. yet we miss to set the given lockFactory though.
1,"occasional index out of bounds exception while running UserManagerImplTest.testFindAuthorizableByRelativePath. Stack trace:

java.lang.ArrayIndexOutOfBoundsException: 8
	at org.apache.jackrabbit.core.query.lucene.MultiScorer.score(MultiScorer.java:89)
	at org.apache.lucene.search.ConjunctionScorer.score(ConjunctionScorer.java:133)
	at org.apache.lucene.search.BooleanScorer2$2.score(BooleanScorer2.java:182)
	at org.apache.lucene.search.BooleanScorer2.score(BooleanScorer2.java:303)
	at org.apache.jackrabbit.core.query.lucene.LuceneQueryHits.nextScoreNode(LuceneQueryHits.java:68)
	at org.apache.jackrabbit.core.query.lucene.QueryHitsAdapter.nextScoreNodes(QueryHitsAdapter.java:54)
	at org.apache.jackrabbit.core.query.lucene.FilterMultiColumnQueryHits.nextScoreNodes(FilterMultiColumnQueryHits.java:63)
	at org.apache.jackrabbit.core.query.lucene.QueryResultImpl.collectScoreNodes(QueryResultImpl.java:328)
	at org.apache.jackrabbit.core.query.lucene.QueryResultImpl.getResults(QueryResultImpl.java:291)
	at org.apache.jackrabbit.core.query.lucene.SingleColumnQueryResult.<init>(SingleColumnQueryResult.java:66)
	at org.apache.jackrabbit.core.query.lucene.QueryImpl.execute(QueryImpl.java:134)
	at org.apache.jackrabbit.core.query.QueryImpl$1.perform(QueryImpl.java:130)
	at org.apache.jackrabbit.core.query.QueryImpl$1.perform(QueryImpl.java:1)
	at org.apache.jackrabbit.core.session.SessionState.perform(SessionState.java:200)
	at org.apache.jackrabbit.core.query.QueryImpl.execute(QueryImpl.java:126)
	at org.apache.jackrabbit.core.security.user.IndexNodeResolver.findNodes(IndexNodeResolver.java:109)
	at org.apache.jackrabbit.core.security.user.UserManagerImpl.findAuthorizables(UserManagerImpl.java:498)
	at org.apache.jackrabbit.core.security.user.UserManagerImpl.findAuthorizables(UserManagerImpl.java:462)
	at org.apache.jackrabbit.core.security.user.UserManagerImplTest.testFindAuthorizableByRelativePath(UserManagerImplTest.java:560)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at org.apache.jackrabbit.test.AbstractJCRTest.run(AbstractJCRTest.java:456)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:130)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)

"
1,"Cookies with null path attribute are rejected in the compatibility mode. Weblogic sends cookies with path empty, httpclient emits a warning
and doesn't send back the cookie to server.

Maybe httpclient works in the RFC's ways but this doesn't reproduce
common web browsers behaviours. Our application works well with IE,
Opera and Netscape, httpunit also sends back the cookie to the server.

When receving the response, httpclient emits the followin warning :

[WARN] HttpMethod - -Invalid cookie header: ""JTD=O%
2FdF13CDb1W7H2GNfUTS2YQ3Zt6bCW6ZKZRvVJ9FwaadQLxXVI7rgii%2FwbxeCsqym7dcWKDxSj%
2Bg1ubJRSVRhYGb7wRLjp5c0v2R3QrCIXVhMKDjuwuXDXnjbH3LHSWG7bfzJSmS7nXk9R%
2FqMIRHb5najLQkU7WkuPGgXUnUln%2BF51TajkVmXkrLMYN7MHDT48BEHvFQFNXBlmSRejWqrd%
2Fiiao0flObOrT3HcaWI09B1vekpAcPmgvMD2oZzXQWJwjDZIX6QoVVD6U8CXPSvVQjITyaxf6AqaS%
2BAFJgRsqbZBc0%2BV5G%2FnzE87ggOVIozfPFn99ny0kxiPGBEisJIy%3D%3D; Version=1; 
Path=; Max-Age=604800"". Missing value for path attribute

That's right, maybe the http header is not correct, but I think httpclient
should handle this case without error in order to have the same behaviour
as common browsers. We have no way to give a better value to this path."
1,"Compatibility issue if admin impersonates admin session. in revision 1076596 in made some improvements in ImpersonationImpl removing the shortcut for ""AdminPrincipal"" which from my point of view is problematic.

however, this introduced the following compatibility issue (detected by tom):
while - according to my tests - a user is allowed to impersonate itself (jcr isn't totally clear about this but states that Session.impersonate is used to ""[...] impersonate"" another [...]"" this was possible for the admin-user due to the shortcut mentioned above.

in order not to break existing code relying on that special case, i would suggest to change the code accordingly.


"
1,"persistent locks persist even after removing the locked node. I read my post to the mailing list and I realized it wasn't clear at all :( . So I decided to upload the code to reproduce the error.

        RepositoryConfig conf = RepositoryConfig.create(""/temp/repository/repository.xml"", ""/temp/repository/repository"");
        Repository repo = RepositoryImpl.create(conf);
        
        // Session 1 creates a locked node
        Session s1 = repo.login(new SimpleCredentials(""user1"", ""pwd1"".toCharArray()));
        Node root1 = s1.getRootNode() ;
        Node test1 = root1.addNode(""test"") ;
        test1.addMixin(""mix:lockable"");
        s1.save() ;
        test1.lock(false, false);
        s1.logout() ;
        
        // Session 2 deletes the node and create a new one
        // with the same path
        Session s2 = repo.login(new SimpleCredentials(""user2"", ""pwd2"".toCharArray()));
        s2.getRootNode().getNode(""test"").remove();
        Node test2 = s2.getRootNode().addNode(""test"");
        s2.save() ;
        // the next line returns true :(
        System.out.println(test2.isLocked()) ;
        // the next line will throw an exception 
        test2.addMixin(""mix:lockable"") ;

javax.jcr.lock.LockException: Node locked.
	at org.apache.jackrabbit.core.lock.LockManagerImpl.checkLock(LockManagerImpl.java:449)
	at org.apache.jackrabbit.core.lock.LockManagerImpl.checkLock(LockManagerImpl.java:435)
	at org.apache.jackrabbit.core.NodeImpl.checkLock(NodeImpl.java:3847)
	at org.apache.jackrabbit.core.NodeImpl.addMixin(NodeImpl.java:964)
	at org.apache.jackrabbit.core.NodeImpl.addMixin(NodeImpl.java:2420)
	at org.apache.jackrabbit.core.LockTest.main(LockTest.java:55)"
1,"Executing query throws UnsupportedRepositoryOperationException(LEVEL_2_SUPPORTED) for a level 1 only implementation . Executing a query  throws UnsupportedRepositoryOperationException(LEVEL_2_SUPPORTED) it the spi implementation is not level 2. This is because org.apache.jackrabbit.jcr2spi.query.execute() calls session.getValueFactory() which - by contract - throws if level 2 is not supported. A quick fix would be to call getJcrValueFactory() (available from the ManagerProvider interface implemented by SessionImpl) instead of getValueFactory(). However, I think a better fix might be to pass the ManagerProvider to the QueryImpl constructor instead of the session, all the managers and providers separately."
1,"PropertyState binary type desirialsation only returns half of content. Create a PropertyState for a binary Property (e.g jcr:data) set a value larger than the BLOBFileValues#MAX_BUFFER_SIZE  (e.g. 300Kbyte) serialse it.
On deserialisation the resulting PropertyState's InternalValue's size is only half as the origianl (e.g. 150Kbyte)

Most probably this is due to the States InputStream implementation marking bytes twice to be read.
Following fix solves the issue for call to #read(byte[], in, int),
but other Stream methods may fail as well.

Index: jackrabbit/java/org/apache/jackrabbit/core/state/PropertyState.java
===================================================================
--- jackrabbit/java/org/apache/jackrabbit/core/state/PropertyState.java (revision 399293)
+++ jackrabbit/java/org/apache/jackrabbit/core/state/PropertyState.java (working  copy)
@@ -305,7 +305,6 @@
                                 len = (int) (length - consumed);
                             }
                             int read = super.read(b, off, len);
-                            consumed += read;
                             return read;
                         }
"
1,"Impossible to import a string containing _x0020_  with Session.importXml. The importXml uses the ValueHelper.serialize methods. The option ""decodeBlanks"" does a simple string replace which replaces _x0020_ in spaces (line 695 and 793). This option is always set to true unless the imported data is binary. See: BufferedStringValue and StringValue getValue methods.

The result is that it is now impossible to import a string with _x0020_ in it, because it gets translated in a space. The simple solution would be to just turn off the declodeBlanks option, but I'm not sure why it was added in the first place. Another option would be to use real encoding instead of a replace like the o.a.j.util.ISO9075.

"
1,"Repository holds onto Session instance after logout. 
After a call to Session.logout the Repository instance's activeSession map still holds a reference to the session. This causes a problem when trying to unlock nodes locked by another session, the addLockToken method rejects the lock token.

Looking at the code in Session.logout, it tries to notify SessionListeners about the logout but Repository, which implements the SessionListener interface and will remove a session on logout, doesn't register with the Session to receive the logout notification.
"
1,"BundleDbPersistenceManager does not work with MySQL. It seems that the bundle persistence manager base does not work with MySQL. A SQLException is thrown on the line ""con.commit();"" in BundleDbPersistenceManager.checkSchema() because autoCommit is set to true in the init method. For some reason, this is ignored by the Oracle and MSSQL drivers. Anyway, commenting out the line fixes the issue, I think."
1,Bundle consistency check does not work. The consistencyCheck feature in the database bundle persistence manager does not work because the correct table prefix is not used when loading all bundles.
1,"DataStore: changing the modified date fails if the file is open for reading (Windows only). If the file is open for reading, Windows doesn't allow to change the last modified time using File.setLastModified():

org.apache.jackrabbit.core.data.DataStoreException: Failed to update record
modified date: 2ac72495fd1e270777821b8a872903c79c84a8d9
        at org.apache.jackrabbit.core.data.FileDataStore.addRecord(FileDataStore.java:250)
        at org.apache.jackrabbit.core.value.BLOBInDataStore.getInstance(BLOBInDataStore.java:119)
        at org.apache.jackrabbit.core.value.InternalValue.getBLOBFileValue(InternalValue.java:619)
        at org.apache.jackrabbit.core.value.InternalValue.create(InternalValue.java:369)
        at org.apache.jackrabbit.core.value.InternalValueFactory.create(InternalValueFactory.java:94)
        at org.apache.jackrabbit.core.value.ValueFactoryImpl.createBinary(ValueFactoryImpl.java:74)

Test case and possible workaround:

import java.io.File;
import java.io.FileInputStream;
import java.io.RandomAccessFile;
public class Test {
    public static void main(String... args) throws Exception {
        String name = ""test.txt"";
        File test = new File(name);
        RandomAccessFile r = new RandomAccessFile(name, ""rw"");
        r.write(0);
        r.close();
        long mod = test.lastModified();
        Thread.sleep(3000);
        FileInputStream in = new FileInputStream(name);
        if (!test.setLastModified(test.lastModified()+1)) {
        	if (!test.canWrite()) {
        		System.out.println(""Can't write to "" + name);
        	} else {
            	System.out.println(""canWrite ok"");
            	r = new RandomAccessFile(name, ""rw"");
            	int old = r.read();
            	r.seek(0);
            	r.write(old);
            	r.close();
        	}
        } else {
        	System.out.println(""setLastModified ok"");
        }
        System.out.println(""Modified old: "" + mod);
        System.out.println(""Modified now: "" + test.lastModified());
        in.close();
        System.out.println(""input closed"");
        if (!test.setLastModified(test.lastModified()+1)) {
        	if (!test.canWrite()) {
        		System.out.println(""Can't write to "" + name);
        	} else {
            	System.out.println(""canWrite ok"");
        	}
        } else {
        	System.out.println(""setLastModified ok"");
        }
        new File(name).delete();
    }
}
"
1,"FVH: slow performance on very large queries. The change from HashSet to ArrayList for flatQueries in LUCENE-3019 resulted in very significant slowdown in some of our e-discovery queries after upgrade from 3.4.0 to 3.5.0. Our queries sometime contain tens of thousands of terms. As a result, major portion of execution time for such queries is now spent in the flatQueries.contains( sourceQuery ) method calls."
1,"NullPointerException in BooleanFilter . BooleanFilter getDISI() method used with QueryWrapperFilter occur NullPointerException,
if any QueryWrapperFilter not match terms in IndexReader.

---------------------------------------------------
java.lang.NullPointerException
	at org.apache.lucene.util.OpenBitSetDISI.inPlaceAnd(OpenBitSetDISI.java:66)
	at org.apache.lucene.search.BooleanFilter.getDocIdSet(BooleanFilter.java:102)
	at org.apache.lucene.search.IndexSearcher.searchWithFilter(IndexSearcher.java:551)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:532)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:463)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:433)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:356)
	at test.BooleanFilterTest.main(BooleanFilterTest.java:50)
---------------------------------------------------

null-check below lines.
---------------------------------------------------
res = new OpenBitSetDISI(getDISI(shouldFilters, i, reader), reader.maxDoc());
res.inPlaceOr(getDISI(shouldFilters, i, reader));
res = new OpenBitSetDISI(getDISI(notFilters, i, reader), reader.maxDoc());
res.inPlaceNot(getDISI(notFilters, i, reader));
res = new OpenBitSetDISI(getDISI(mustFilters, i, reader), reader.maxDoc());
res.inPlaceAnd(getDISI(mustFilters, i, reader));
---------------------------------------------------"
1,"Fix small perf issues with String/TermOrdValComparator. Uncovered some silliness when working on LUCENE-2504, eg we are doing unnecessary binarySearch on a single-segment reader."
1,FilteredQuery ignores boost. Filtered query ignores it's own boost.
1,"Cluster revision entries should be retrieved in order. The selectRevisionStmtSQL (DatabaseJournal#buildSqlStatements) returns a result set which may not be ordered by REVISION_ID. This has the effect that cluster instances that want to synchronize to the latest revision do not update their local revision appropriately since they assume that the revision result set is ordered (see code in AbstractJournal#doSync). This might cause a lot of unnecessary CPU cycles on these machines with degraded performance as a result. Furthermore, it causes functional issues as well as events may be fired multiple times and in the wrong order."
1,"replace invalid U+FFFF character during indexing. If the invalid U+FFFF character is embedded in a token, it actually causes indexing to silently corrupt the index by writing duplicate terms into the terms dict.  CheckIndex will catch the error, and merging will hit exceptions (I think).

We already replace invalid surrogate pairs with the replacement character U+FFFD, so I'll just do the same with U+FFFF."
1,"DOMException: NAMESPACE_ERR thrown when exporting document view. When I try to export some nodes with ExportDocumentView I get a DOMException with Jackrabbit 1.5.2. Version 1.4.6 works fine. Xerces version was 2.8.1.

Code:

Document document = documentBuilder.newDocument();
Element exportElement = (Element) document.appendChild(document.createElement(""Export""));
Result result = new DOMResult(exportElement);
TransformerHandler transformerHandler = saxTransformerFactory.newTransformerHandler();
transformerHandler.setResult(result);
session.exportDocumentView(workflowNode.getPath(), transformerHandler, true, false);

Exception:

org.w3c.dom.DOMException: NAMESPACE_ERR: An attempt is made to create or change an object in a way which is incorrect with regard to namespaces.
	at org.apache.xerces.dom.CoreDocumentImpl.checkDOMNSErr(Unknown Source)
	at org.apache.xerces.dom.AttrNSImpl.setName(Unknown Source)
	at org.apache.xerces.dom.AttrNSImpl.<init>(Unknown Source)
	at org.apache.xerces.dom.CoreDocumentImpl.createAttributeNS(Unknown Source)
	at org.apache.xerces.dom.ElementImpl.setAttributeNS(Unknown Source)
	at com.sun.org.apache.xalan.internal.xsltc.trax.SAX2DOM.startElement(SAX2DOM.java:194)
	at com.sun.org.apache.xml.internal.serializer.ToXMLSAXHandler.closeStartTag(ToXMLSAXHandler.java:204)
	at com.sun.org.apache.xml.internal.serializer.ToSAXHandler.flushPending(ToSAXHandler.java:277)
	at com.sun.org.apache.xml.internal.serializer.ToXMLSAXHandler.startElement(ToXMLSAXHandler.java:646)
	at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerHandlerImpl.startElement(TransformerHandlerImpl.java:263)
	at org.apache.jackrabbit.commons.xml.Exporter.startElement(Exporter.java:438)
	at org.apache.jackrabbit.commons.xml.DocumentViewExporter.exportNode(DocumentViewExporter.java:76)
	at org.apache.jackrabbit.commons.xml.Exporter.exportNode(Exporter.java:298)
	at org.apache.jackrabbit.commons.xml.Exporter.exportNodes(Exporter.java:214)
	at org.apache.jackrabbit.commons.xml.DocumentViewExporter.exportNode(DocumentViewExporter.java:77)
	at org.apache.jackrabbit.commons.xml.Exporter.exportNode(Exporter.java:295)
	at org.apache.jackrabbit.commons.xml.Exporter.export(Exporter.java:144)
	at org.apache.jackrabbit.commons.AbstractSession.export(AbstractSession.java:461)
	at org.apache.jackrabbit.commons.AbstractSession.exportDocumentView(AbstractSession.java:241)"
1,"QOM to SQL2 doesn't cast numeric literals. SQL2 statements generated by the QueryObjectModel.getStatement() don't contain CAST(... AS ...) for numeric literals of types DECIMAL, DOUBLE, and LONG. The type information is lost, which can result in incorrect query results (depending on the query engine) if the generated SQL2 statement is executed."
1,"Missing 'node removed' event when removing a version. When a version is removed only one 'node removed' event is triggered for the version node. Even though the frozen node under that version also gets removed there is no event for the frozen node.

See failing test cases:
org.apache.jackrabbit.core.observation.VersionEventsTest#testRemoveVersion()
org.apache.jackrabbit.core.observation.VersionEventsTest#testXARemoveVersion()
"
1,"SQL2 Left Outer Join. Create this nodes.
def n1 = root.addNode(""node1"", ""sling:SamplePage"");
n1.setProperty(""n1prop1"", ""page1"");
def n2 = n1.addNode(""node2"", ""sling:SampleContent"");
n2.setProperty(""n2prop1"", ""content1"");

Execute this Query:
Select * from [sling:SamplePage] as page left outer join [sling:SampleContent] as content on ISDESCENDANTNODE(content,page) where page.n1prop1 = 'page1' and content.n2prop1 = 'content1';
The resultset have 1 row with 2 Nodes. This OK.

Then execute this:
Select * from [sling:SamplePage] as page left outer join [sling:SampleContent] as content on ISDESCENDANTNODE(content,page) where page.n1prop1 = 'page1' and content.n2prop1 = 'XXXXX';

The resultset has 1 row with 1 node.
This wrong. The result should be 0 rows.

Old Versions, prior 2.2.2 have also 0 rows as result.

Also, if nodes ""n2"" not exists, jackrabbit reports 1 row as result.

"
1,"Deadlock on version operations in a clustered environment. Version operations in a cluster may end up in a deadlock: a write operation in the version store will acquire the version manager's write lock (N1.VW) and subsequently the cluster journal's write lock (N1.JW). Another cluster node's write operation in some workspace will acquire the journal's write lock (N2.JW) and first process the journal record log: if some of these changes concern the version store, the version manager's read lock (N2.VR) has to be acquired in order to deliver them. If the first cluster node reaches N1.VW, and the second reaches N2.JW, we have a deadlock. The same scenario takes place when the second cluster node synchronizes to the latest journal changes and reaches N2.JR, when the first cluster node is in N1.VW."
1,"TestFSTs.testRandomWords throws AIOBE when ""verbose""=true. Seems like invalid utf-8 sometimes gets passed to Bytesref.utf8ToString() in the verbose ""println""s."
1,"IOExeception can cause loss of data due to premature segment deletion. If you hit an IOException, e.g., disk full, while making a cfs from its constituent parts, you may not be able to rollback to the before-merge process. This happens via addIndexes.

I don't have a nice easy test for this; generating IOEs ain't so easy. But it does happen in the patch for the factored merge policy with the existing tests because the pseudo-randomly generated IOEs fall in a different place."
1,"Can not instantiate lucene Analyzer in SearchIndex. In the Lucene 3, the there is no default constructor anymore in Analyzer classes


11:46:45.946 [main] WARN  o.a.j.core.query.lucene.SearchIndex - Invalid Analyzer class: org.apache.lucene.analysis.standard.StandardAnalyzer
java.lang.InstantiationException: org.apache.lucene.analysis.standard.StandardAnalyzer
        at java.lang.Class.newInstance0(Class.java:340) ~[na:1.6.0_26]
        at java.lang.Class.newInstance(Class.java:308) ~[na:1.6.0_26]
        at org.apache.jackrabbit.core.query.lucene.SearchIndex.setAnalyzer(SearchIndex.java:1892) ~[jackrabbit-core-2.4.0.jar:2.4.0]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.6.0_26]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) ~[na:1.6.0_26]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) ~[na:1.6.0_26]
        at java.lang.reflect.Method.invoke(Method.java:597) ~[na:1.6.0_26]
        at org.apache.jackrabbit.core.config.BeanConfig.setProperty(BeanConfig.java:255) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.config.BeanConfig.newInstance(BeanConfig.java:203) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.config.RepositoryConfigurationParser$1.getQueryHandler(RepositoryConfigurationParser.java:652) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.config.WorkspaceConfig.getQueryHandler(WorkspaceConfig.java:251) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.SearchManager.<init>(SearchManager.java:171) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.getSearchManager(RepositoryImpl.java:1855) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.doPostInitialize(RepositoryImpl.java:2092) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.initialize(RepositoryImpl.java:1997) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.RepositoryImpl.initStartupWorkspaces(RepositoryImpl.java:510) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:318) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:582) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.jndi.BindableRepository.createRepository(BindableRepository.java:141) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.jndi.BindableRepository.init(BindableRepository.java:117) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.jndi.BindableRepository.<init>(BindableRepository.java:106) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.jndi.BindableRepositoryFactory.getObjectInstance(BindableRepositoryFactory.java:52) [jackrabbit-core-2.4.0.jar:2.4.0]
"
1,"CachingWrapperFilter crashes if you call both bits() and getDocIdSet(). CachingWrapperFilter uses only a single cache, so calling bits() after calling getDocIdSet() will result in a type error. Additionally, more code than is necessary is wrapped in the @synchronized blocks."
1,"Text.unescape() should should preserve 'unicode' characters. When an input to Text.unescape() contains characters > \u00ff, the most significant byte is lost resulting in garbled output. The unescape() function should preserve such characters in order to be useful to decode Internationalized Resource Identifiers (RFC 3987). "
1,"XMLTextExtractor returns an empty reader when encoding is unsupported. XMLTextExtractor is failing to index xml files.  Searching for content in xml files is not coming back with results.

On the extractText(InputStream stream, String type, String encoding) method, the encoding is coming in as an empty string, and it throws an exception at line 62 (reader.parse(source)).

modifying the following statement fixes the problem:
before:  if (encoding != null) {
after:  if (encoding != null && !encoding.equals("""")) {"
1,"PrecedenceQueryParser misinterprets queries starting with NOT. ""NOT foo AND baz"" is parsed as ""-(+foo +baz)"" instead of ""-foo +bar"".

(I'm setting parser.setDefaultOperator(PrecedenceQueryParser.AND_OPERATOR) but the issue applies otherwise too.)
"
1,"HttpMethodBase: Port mismatch in URL for redirect to absolute location. The CVS version of latka was failing a test ( $ ant test, not maven ), when
using the CVS version of httpclient. The message was

 [java] WARN  [main] org.apache.commons.httpclient.HttpMethod
      - Redirect from port 80 to -1 is not supported: 21 Sep 2002 00:54:32,098

The request preceding this was:

 [java] DEBUG [main] httpclient.wire - >> ""GET /commons HTTP/1.1
     [java] "" [\r\n]: 21 Sep 2002 00:54:31,262
     [java] DEBUG [main] org.apache.commons.httpclient.HttpMethod - enter
[java] DEBUG [main] httpclient.wire - >> ""Host: jakarta.apache.org
     [java] "" [\r\n]: 21 Sep 2002 00:54:31,441
     [java] DEBUG [main] httpclient.wire - >> ""User-Agent: Jakarta
Commons-HttpClient/2.0M1
 [java] "" [\r\n]: 21 Sep 2002 00:54:31,442

And the response was:

 [java] DEBUG [main] org.apache.commons.httpclient.HttpMethod - enter
HttpMethodBase.readStatusLine(HttpState, HttpConnection): 21 Sep 2002 00:54:31,444
     [java] DEBUG [main] org.apache.commons.httpclient.HttpConnection - enter
HttpConnection.readLine(): 21 Sep 2002 00:54:31,444
[java] DEBUG [main] httpclient.wire - << ""HTTP/1.1 301 Moved Permanently""
[\r\n]: 21 Sep 2002 00:54:32,080
     [java] DEBUG [main] org.apache.commons.httpclient.HttpMethod - enter
HttpMethodBase.readResponseHeaders(HttpState,HttpConnection): 21 Sep 2002
00:54:32,086
     [java] DEBUG [main] org.apache.commons.httpclient.HttpConnection - enter
HttpConnection.readLine(): 21 Sep 2002 00:54:32,087
[java] DEBUG [main] httpclient.wire - << ""Date: Fri, 20 Sep 2002 23:54:30 GMT""
[\r\n]: 21 Sep 2002 00:54:32,088
     [java] DEBUG [main] org.apache.commons.httpclient.HttpConnection - enter
HttpConnection.readLine(): 21 Sep 2002 00:54:32,089
[java] DEBUG [main] httpclient.wire - << ""Server: Apache/2.0.42 (Unix)"" [\r\n]:
21 Sep 2002 00:54:32,090
     [java] DEBUG [main] org.apache.commons.httpclient.HttpConnection - enter
HttpConnection.readLine(): 21 Sep 2002 00:54:32,090
[java] DEBUG [main] httpclient.wire - << ""Location:
http://jakarta.apache.org/commons/"" [\r\n]: 21 Sep 2002 00:54:32,091
     [java] DEBUG [main] org.apache.commons.httpclient.HttpConnection - enter
HttpConnection.readLine(): 21 Sep 2002 00:54:32,091
[java] DEBUG [main] httpclient.wire - << ""Content-Length: 319"" [\r\n]: 21 Sep
2002 00:54:32,091
     [java] DEBUG [main] org.apache.commons.httpclient.HttpConnection - enter
HttpConnection.readLine(): 21 Sep 2002 00:54:32,092
[java] DEBUG [main] httpclient.wire - << ""Content-Type: text/html;
charset=iso-8859-1"" [\r\n]: 21 Sep 2002 00:54:32,092
     [java] DEBUG [main] org.apache.commons.httpclient.HttpConnection - enter
HttpConnection.readLine(): 21 Sep 2002 00:54:32,092
[java] DEBUG [main] org.apache.commons.httpclient.HttpMethod - enter
HttpMethodBase.processResponseHeaders(HttpState, HttpConnection): 21 Sep 2002
00:54:32,093
     [java] DEBUG [main] org.apache.commons.httpclient.methods.GetMethod - enter
GetMethod.readResponseBody(HttpState, HttpConnection): 21 Sep 2002 00:54:32,093
     [java] DEBUG [main] org.apache.commons.httpclient.HttpMethod - enter
HttpMethodBase.readResponseBody(HttpState, HttpConnection): 21 Sep 2002 00:54:32,093
     [java] DEBUG [main] org.apache.commons.httpclient.HttpMethod - enter
HttpMethodBase.readResponseBody(HttpState, HttpConnection): 21 Sep 2002 00:54:32,094
     [java] DEBUG [main] org.apache.commons.httpclient.HttpConnection - enter
HttpConnection.getRequestOutputStream(HttpMethod): 21 Sep 2002 00:54:32,094
     [java] DEBUG [main] org.apache.commons.httpclient.HttpMethod - enter
writeRemainingRequestBody(HttpState, HttpConnection): 21 Sep 2002 00:54:32,096
     [java] DEBUG [main] org.apache.commons.httpclient.HttpMethod - Redirect
required: 21 Sep 2002 00:54:32,097
     [java] DEBUG [main] org.apache.commons.httpclient.HttpMethod - Redirect
requested to location 'http://jakarta.apache.org/commons/': 21 Sep 2002 00:54:32,097
     [java] WARN  [main] org.apache.commons.httpclient.HttpMethod - Redirect
from port 80 to -1 is not supported: 21 Sep 2002 00:54:32,098


The problem appears to be in this section of code from HttpMethodBase

if (url == null) {
    //try to construct the new url based on the current url
    try {
        URL currentUrl = new URL(conn.getProtocol(),
                                 conn.getHost(),
                                 conn.getPort(), getPath());
        url = new URL(currentUrl, location);   <--- is this inheriting the port?
    } catch (Exception ex) {
        log.error(""Redirected location '""
                  + locationHeader.getValue()
                  + ""' is malformed"");
        return statusCode;
    }
}"
1,"ERROR 40XD0: Container has been closed exception with Derby DB. This seems very similar to JCR-1039, only I am getting it on 1.4 using the regular DatabasePersistenceManager.
Was the fix for JCR-1039 in 1.3.3 merged to 1.4.x?

Here is the relevant part of the exception:

INFO | jvm 1 | 2008/04/10 14:00:37 | Caused by: javax.jcr.RepositoryException: failed to retrieve item state of item fb648866-a236-42aa-8039-df68f26dd2ad/{http://www.jcp.org/jcr/1.0}data: failed to read property state: fb648866-a236-42aa-8039-df68f26dd2ad/{http://www.jcp.org/jcr/1.0}data: failed to read property state: fb648866-a236-42aa-8039-df68f26dd2ad/{http://www.jcp.org/jcr/1.0}data
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.ItemManager.createItemInstance(ItemManager.java:570)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:395)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.NodeImpl.getProperty(NodeImpl.java:2553)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.artifactory.jcr.JcrFile.getStream(JcrFile.java:133)
INFO | jvm 1 | 2008/04/10 14:00:37 | ... 55 more
INFO | jvm 1 | 2008/04/10 14:00:37 | Caused by: org.apache.jackrabbit.core.state.ItemStateException: failed to read property state: fb648866-a236-42aa-8039-df68f26dd2ad/{http://www.jcp.org/jcr/1.0}data
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.load(DatabasePersistenceManager.java:406)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.state.SharedItemStateManager.loadItemState(SharedItemStateManager.java:1161)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.state.SharedItemStateManager.getNonVirtualItemState(SharedItemStateManager.java:1086)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.state.SharedItemStateManager.getItemState(SharedItemStateManager.java:248)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.state.LocalItemStateManager.getPropertyState(LocalItemStateManager.java:118)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.state.LocalItemStateManager.getItemState(LocalItemStateManager.java:150)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.state.XAItemStateManager.getItemState(XAItemStateManager.java:226)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:175)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.ItemManager.createItemInstance(ItemManager.java:564)
INFO | jvm 1 | 2008/04/10 14:00:37 | ... 58 more
INFO | jvm 1 | 2008/04/10 14:00:37 | Caused by: javax.jcr.RepositoryException: Error creating temporary file: ERROR 40XD0: Container has been closed.: ERROR 40XD0: Container has been closed.
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.value.BLOBInTempFile.<init>(BLOBInTempFile.java:69)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.value.BLOBInTempFile.getInstance(BLOBInTempFile.java:103)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.value.InternalValue.getBLOBFileValue(InternalValue.java:630)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.value.InternalValue.create(InternalValue.java:265)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.persistence.util.Serializer.deserialize(Serializer.java:296)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.load(DatabasePersistenceManager.java:397)
INFO | jvm 1 | 2008/04/10 14:00:37 | ... 66 more
INFO | jvm 1 | 2008/04/10 14:00:37 | Caused by: java.io.IOException: ERROR 40XD0: Container has been closed.
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.derby.impl.store.raw.data.OverflowInputStream.fillByteHolder(Unknown Source)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.derby.impl.store.raw.data.BufferedByteHolderInputStream.read(Unknown Source)
INFO | jvm 1 | 2008/04/10 14:00:37 | at java.io.DataInputStream.read(DataInputStream.java:132)
INFO | jvm 1 | 2008/04/10 14:00:37 | at java.io.FilterInputStream.read(FilterInputStream.java:116)
INFO | jvm 1 | 2008/04/10 14:00:37 | at java.io.FilterInputStream.read(FilterInputStream.java:116)
INFO | jvm 1 | 2008/04/10 14:00:37 | at java.io.SequenceInputStream.read(SequenceInputStream.java:191)
INFO | jvm 1 | 2008/04/10 14:00:37 | at java.io.InputStream.read(InputStream.java:85)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.value.BLOBInTempFile.<init>(BLOBInTempFile.java:61)
INFO | jvm 1 | 2008/04/10 14:00:37 | ... 71 more"
1,"Unclosed files when aggregated property states are indexed. This is a regression caused by JCR-1990.

The lucene document for the node that contains the aggregated property may contain an extractor job that has an open file handle to the jcr:data binary property. The document must be disposed after the properties are transferred."
1,"Escaped wildcard character in wildcard term not handled correctly. If an escaped wildcard character is specified in a wildcard query, it is treated as a wildcard instead of a literal.
e.g., t\??t is converted by the QueryParser to t??t - the escape character is discarded."
1,"SpellChecker.clearIndex calls unlock inappropriately. As noted in LUCENE-1050, fixing a bug in SimpleLockFactory related to not reporting success/filure of lock file deletion has surfaced bad behavior in SpellChecker.clearIndex...

Grant...
{quote}
It seems the SpellChecker is telling the IndexReader to delete the lockFile, but the lockFile doesn't exist.
  ...
I don't know much about the locking mechanism, but it seems like this should check to see if the lockFile exists before trying to delete it.
{quote}

Hoss...
{quote}
Grant: my take on this is that SpellChecker.clearIndex is in the wrong. it shouldn't be calling unlock unless it has reason to think there is a ""stale lock"" that needs to be closed - ie: this is a bug in SpellChecker that you have only discovered because this bug LUCENE-1050 was fixed.

I would suggest a new issue for tracking, and a patch in which SpellChecker.clearIndex doesn't call unlock unless isLocked returns true. Even then, it might make sense to catch and ignore LockReleaseFailedException and let whatever resulting exception may originate from ""new IndexWriter"" be returned.
{quote}

marking for 2.3 since it seems like a fairly trivial fix, and if we don't deal with it now it will be a bug introduced in 2.3.

"
1,"Oracle bundle PM fails checking schema if 2 users use the same database. When using the OracleBundlePersistenceManager there is an issue when two users use the same database for persistence. In  that case, the checkSchema() method of the BundleDbPersistenceManager  does not work like it should. More precisely, the call ""metaData.getTables(null,  null, tableName, null);"" will also includes table names of other  schemas/users. Effectively, only the first user of a database is able to create  the schema.

probably same issue as here: JCR-582"
1,"DocumentsWriter.abort fails to clear docStoreOffset. I hit this in working on LUCENE-1044.

If you disk full event during flush, then DocumentsWriter will abort
(clear all buffered docs).  Then, if you then add another doc or two,
and then close your writer, and this time succeed in flushing (say
because it's only a couple buffered docs so the resulting segment is
smaller), you can flush a corrupt segment (that incorrectly has a
non-zero docStoreOffset).

I modified the TestConcurrentMergeScheduler test to show this bug.
I'll attach a patch shortly.
"
1,"PlainTextExtractor returns an empty reader when encoding is unsupported. PlainTextExtractor is failing to index text files.  Searching for content in text files is not coming back with results.

On the extractText(InputStream stream, String type, String encoding) method, the encoding is coming in as an empty string, and it throws the java.io.UnsupportedEncodingException at line 40 ( return new InputStreamReader(stream, encoding); ).

modifying the following statement fixes the problem:
before:  if (encoding != null) {
after:  if (encoding != null && !encoding.equals("""")) {"
1,"preflex codec returns wrong terms if you use an empty field name. spinoff from LUCENE-3473.

I have a standalone test for this... the termsenum is returning a bogus extra empty-term (I assume it has no postings, i didnt try).

This causes the checkindex test in LUCENE-3473 to fail, because there are 4 terms instead of 3. 

"
1,"DatabaseFileSystem: mysql.ddl works for mysql5 but not mysql 4.1.20. Perhaps a new column ( primary key ) could get added to the table called uid, which is actually an md5checksum of FSENTRY_PATH and FSENTRY_NAME."
1,"ConcurrentScheduleManager.addMyself() has wrong inted. This method has the wrong index for the 'size' variable, I think it should b allInstances.size.

{code:java}
private void addMyself() {
    synchronized(allInstances) {
      final int size=0;
      int upto = 0;
      for(int i=0;i<size;i++) {
        final ConcurrentMergeScheduler other = (ConcurrentMergeScheduler) allInstances.get(i);
        if (!(other.closed && 0 == other.mergeThreadCount()))
          // Keep this one for now: it still has threads or
          // may spawn new threads
          allInstances.set(upto++, other);
      }
      allInstances.subList(upto, allInstances.size()).clear();
      allInstances.add(this);
    }
  }
{code}"
1,"304 response status handling. I have an IBM WebSphere server that returns 304 responses with a Content-
Length header set to something other than 0 and the server is not closing the 
connection.  According to the HTTP RFC 
(http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html#sec10.3.5):

""The 304 response MUST NOT contain a message-body, and thus is always 
terminated by the first empty line after the header fields.""

Obviously, the web server is returning a bad response but the HTTPClient 
blocks waiting on data in the response even though there shouldn't be any.  
Other HTTP clients (browsers) do not have this issue and seem to ignore the 
fact that the server set an invalid Content-Length in the response."
1,"IndexWriterConfig does not allow readerTermsIndexDivisor to be -1, while the latest indicates the terms index should not be loaded. While you can pass -1 to IR.open(), and it's documented, you cannot do the same for IndexWriter's readers (b/c IWC blocks it). Need to allow this setting as well as add support for it in our tests, e.g. we should randomly set it to -1. Robert also suggested RandomIW use -1 randomly when it opens readers.

I'll work on a patch"
1,"null domains break Cookie.java. the domain is assumed to be non-null in a few places in Cookie.java, see matches
method, for example"
1,"Lower-Case Search-Function works with Upper-Case Searchstring. if you perform a query like this
testroot/*[jcr:like(fn:lower-case(@prop1), 'FO%')]
you get valid results even though the value in the property has the ""foo"" value
The search works with lower and upper-case search strings."
1,"IndexWriter.addIndexesNoOptimize ignores the compound file setting of the destination index. IndexWriter.addIndexesNoOptimize(Directory[]) ignores the compound file setting of the destination index. It is using the compound file flags of segments in the source indexes.
This sometimes causes undesired increase of the number of files in the destination index when non-compound file indexes are added until merge kicks in."
1,"Versioning fixup leaves persistence in a state where the node can't be made versionable again. Jackrabbit's version recovery mode (org.apache.jackrabbit.version.recovery system property) disconnects all version histories that expose problems that manifest in unexpected exceptions being thrown. ""disconnects"" means removing the properties defined for mix:versionable and removing the mixin type. The actual versioning related nodes remain in place.

The problem: when re-adding mix:versionable, ItemSaveOperation.initVersionHistories tries to create the new version history in the same location (the path being derived from the versionable node's identifier), and consequently fails because of the broken underlying storage.

(attaching a work-in-progress test case that illustrates the problem)"
1,"ClassCastException when registering new node type. java.lang.ClassCastException: org.apache.jackrabbit.core.nodetype.NodeTypeImpl
	at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.registerNodeTypes(NodeTypeManagerImpl.java:708)
	at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.registerNodeType(NodeTypeManagerImpl.java:637)

"
1,"Fix and simplify CryptedSimpleCredentials. the credentials retrieved from UserImpl and used to validate the simplecredentials passed to the repository login is overly complex
and buggy as it tries to match all kind credentials variants with and without hashed password.
in particular it contains the following problems:
- simplecredentials containing the hashed pw are considered valid
- passwords startign with {something} cause inconsistencies and may even prevent the user from login

it should be improved as follows:
- simplecredentials are always expected to contain the plain text password both for creation and
  comparison with the cryptedsimplecredentials.
- creating cryptedsimplecredentials from uid/pw however is left unchanged: the specified pw is
  hashed with the default algorithm if it turns out not to be in the hashed format.
- in addition the pw should also be hashed if it has the form {something}whatever but something
  is an invalid algorithm.
"
1,"IllegalStateException on session#save(). The following code throws an IllegalStateException:

Node node = ...
Session session = node.getSession();
node.setProperty( ""tags"", ""test1"");
node.setProperty( ""tags"", ""test2"");
node.remove();
session.save();


"
1,"Phrase query with term repeated 3 times requires more slop than expected. Consider a document with the text ""A A A"".
The phrase query ""A A A"" (exact match) succeeds.
The query ""A A A""~1 (same document and query, just increasing the slop value by one) fails.
""A A A""~2 succeeds again.

If the exact match succeeds, I wouldn't expect the same query but with more slop to fail.  The fault seems to require some term to be repeated at least three times in the query, but the three occurrences do not need to be adjacent.  I will attach a file that contains a set of JUnit tests that demonstrate what I mean."
1,"NodeType.canSetProperty() does not include type conversion. for example, NodeType.canSetProperty(String propertyName, Value value) must return true if the property requires a StringValue and value is a DateValue (but does not)"
1,"UUIDDocId.getDocumentNumbers() may return illegal value. Happens when the node with the given UUID is not present in the index. The method then returns -1, which is illegal. Document numbers must be >= 0. The method must returns an empty array when the id is invalid, as documented in DocId.getDocumentNumbers()."
1,"JCR2SPI: remove node operation missing in submitted SPI batch. In JCR2SPI, the following sequence of operations seems to lead to an incorrect SPI batch being submitted:

1) remove ""/a""
2) add ""/a""
3) add ""/a/b""
4) session.save()

This seems to create an SPI batch where the first remove operation is missing.

Note that the problem only seems to occur when step 3 is part of the sequence.

Full Java source for test:

    try {
      if (session.getRepository().getDescriptor(Repository.LEVEL_2_SUPPORTED).equals(""true"")) {
        Node testnode;
        String name = ""delete-test"";
          
        Node root = session.getRootNode();
        
        // make sure it's there
        if (! root.hasNode(name)) {
          root.addNode(name, ""nt:folder"");
          session.save();
        }
        
        // now test remove/add in one batch
        if (root.hasNode(name)) {
          testnode = root.getNode(name);
          testnode.remove();
          // session.save(); // un-commenting this makes the test pass
        }
        
        testnode = root.addNode(name, ""nt:folder"");
        // add one child
        testnode.addNode(name, ""nt:folder""); // commenting this out makes the test pass
        
        session.save();
      }
    } finally {
      session.logout();
    }
    
    "
1,"XPath relative path support missing for ""is null"" and ""is not null"". I believe the change for issue JCR-247 is incomplete, for instance

  //*[@x]

and

  //*[foo/@x]

are parsed into the same query tree."
1,"Missing XPath escape in query.jsp. As reported by Canberk Bolat of ADEO Security in a private communication, there search.jsp script in jackrabbit-webapp is missing an escape when it injects the path of a ""related:"" query into the constructed XPath statement. Further analysis showed that this issue has no security implications, so we can treat this as a normal bug report.

search.jsp
...
String q = request.getParameter(""q"");
...
      if (q != null && q.length() > 0) {
           String stmt;
           if (q.startsWith(""related:"")) {
               String path = q.substring(""related:"".length());
               stmt = ""//element(*, nt:file)[rep:similar(jcr:content,
'"" + path + ""/jcr:content')]/rep:excerpt(.) order by @jcr:score
descending"";
               queryTerms = ""similar to <b>"" +
Text.encodeIllegalXMLCharacters(path) + ""</b>"";
           }
...

"
1,JEDirectory delete issue. JEDirectory is not deleting files properly.  Blocks are left behind due to an error in cursor operations.
1,"Document not guaranteed to be found after write and commit. after same email on developer list:
""I developed a stress test to assert that a new document containing a
specific term ""X"" is always found after a commit on the IndexWriter.
This works most of the time, but it fails under load in rare occasions.

I'm testing with 40 Threads, both with a SerialMergeScheduler and a
ConcurrentMergeScheduler, all sharing a common IndexWriter.
Attached testcase is using a RAMDirectory only, but I verified a
FSDirectory behaves in the same way so I don't believe it's the
Directory implementation or the MergeScheduler.
This test is slow, so I don't consider it a functional or unit test.
It might give false positives: it doesn't always fail, sorry I
couldn't find out how to make it more likely to happen, besides
scheduling it to run for a longer time.""

I tested this to affect versions 2.4.1 and 2.9.1;
"
1,"Problems with jackrabbit-standalone. I'm having problems with the jackrabbit-standalone component not starting up properly because of two issues:

* The bundle packaging doesn't include the WEB-INF/config.xml file. I assume this is because of the more recent bundle plugin version treating whitespace differently in the inlining settings.

* The RMI binding fails if a local RMI registry is already running with another repository reference. I'm thinking of simply disabling the RMI registry bindings and using the http://localhost:8080/rmi address as the recommended RMI endpoint."
1,"SearchManager might throw when handling cluster event. When handling events that are generated from another node in the cluster, the SearchManager might try to index an index that does no longer exist. This results in an error message or even a NPE. The scenario looks as follows (A and B are nodes in a repository cluster)

1: A adds node N
2: A saves changes
3: A removes node N
4: A saves changes

Upon receiving the event of a newly created node, B starts indexing node N. If this process hasn't been concluded before step 3 above, it will throw.
"
1,"LockableFileRevision not thread-safe. LockableFileRevision works well across process boundaries, but does not within the same JVM. The methods lock() and unlock() must be synchronized similar to DatabaseRevision."
1,"Paths not correct after reordering children. Reordered, unsaved children of a node do not have the correct path. In the test case attached, the following operation is attempted with three SNS children named b[1], b[2], b[3]: the last element is ordered before the first three times, which should result in the initial children order.
"
1,"Problems with BundleDbPersistenceManager getAllNodeIds. When using MySQL:
The problem arises when the method parameter maxcount is less than the total amount of records in the bundle table.

First of all I found out that mysql orders the nodeid objects different than jackrabbit does. The following test describes this idea:

    public void testMySQLOrderByNodeId() throws Exception {
        NodeId nodeId1 = new NodeId(""7ff9e87c-f87f-4d35-9d61-2e298e56ac37"");
        NodeId nodeId2 = new NodeId(""9fd0d452-b5d0-426b-8a0f-bef830ba0495"");

        PreparedStatement stmt = connection.prepareStatement(""SELECT NODE_ID FROM DEFAULT_BUNDLE WHERE NODE_ID = ? OR NODE_ID = ? ORDER BY NODE_ID"");

        Object[] params = new Object[] { nodeId1.getRawBytes(), nodeId2.getRawBytes() };
        stmt.setObject(1, params[0]);
        stmt.setObject(2, params[1]);

        ArrayList<NodeId> nodeIds = new ArrayList<NodeId>();
        ResultSet resultSet = stmt.executeQuery();
        while(resultSet.next()) {
            NodeId nodeId = new NodeId(resultSet.getBytes(1));
            System.out.println(nodeId);
            nodeIds.add(nodeId);
        }
        Collections.sort(nodeIds);
        for (NodeId nodeId : nodeIds) {
            System.out.println(nodeId);
        }
    }

Which results in the following output:

7ff9e87c-f87f-4d35-9d61-2e298e56ac37
9fd0d452-b5d0-426b-8a0f-bef830ba0495
9fd0d452-b5d0-426b-8a0f-bef830ba0495
7ff9e87c-f87f-4d35-9d61-2e298e56ac37


Now the problem with the getAllNodeIds method is that it fetches an extra 10 records on top of maxcount (to avoid a problem where the first key is not the one you that is wanted). Afterwards it skips a number of records again, this time using nodeid.compareto. This compareto statement returns true unexpectedly for mysql because the code doesn't expect the mysql ordering.

I had the situation where I had about 17000 records in the bundle table but consecutively getting the ids a thousand records at a time returned only about 8000 records in all.
"
1,"workspace.copy does not copy binary properties properly. Workspace copy works fine for everything else but if you copy a hierarchy which contains binary properties
and remove the source after copying it removes the binary from ""copied"" newly created hierarchy as well.

How to reproduce:

1. create hierarchy with any type of nodes  - /site / en / image [Type Binary]
2. create another hierarchy at different level - /site2
3. copy /site/en under /site2
-- till now everything is fine, its a proper copy and if you export xml out of these 2 hierarchies you will see that ""image"" is actually copied
4. now delete /site/en
5. you will see all other properties as copied before /site2/en... except binary.

are binary types are always referenced? even if you copy via workspace copy
"
1,"Restore of empty multivalue property always changes property type to String. When you do a restore of  empty multivalue property (OPV=COPY), restored property always has the String type (no matter of property type in frozen state). The solution is to set the property type from frozen state instead of retriving it from 'first' value. If mulitvalue does not have any values the type is set to UNDEFINED and finally changed to STRING in restore method.

Attached patch with test case."
1,"FilterIndexReader in trunk does not implement getSequentialSubReaders() correctly. Since LUCENE-2459, getSequentialSubReaders() in FilterIndexReader returns null, so it returns an atomic reader. But If you call then any of the enum methods, it throws Exception because the underlying reader is not atomic.

We should move the null-returning method to SlowMultiReaderWrapper and fix FilterIndexReader's default to return in.getSequentialSubReaders(). Ideally an implementation must of course also wrap the sub-readers.

If we change this we have to look into other Impls like the MultiPassIndexSplitter if we need to add atomicity."
1,"XPath query with child axis predicates. Executing a query using a long child path in a child axis predicate (like //*[a/b/c/d/e/@prop='something']) may return too many or not enough nodes.

I'll attach a zip file containing 2 tests cases showing this issue (I apologize but the test data are in French).
"
1,"InitiatedIndex: CCE on casting NumericField to Field. An unchecked cast to List<Field> throws a ClassCastException when applied to, for example, a NumericField.
Appearently, this has been fixed trunk, but for a 2.9.1 release, this could be helpful.
The patch can be applied against the 2.9.0 tag."
1,"creating empty field + empty term leads to invalid index. Spinoff from LUCENE-3526.

* if you create new Field("""", """"), you get IllegalArgumentException from Field's ctor: ""name and value cannot both be empty""
* But there are tons of other ways to index an empty term for the empty field (for example initially make it ""garbage"" then .setValue(""""), or via tokenstream).
* If you do this, and you have assertions enabled, you will trip an assert (the assert is fixed in trunk, in LUCENE-3526)
* But If you don't have assertions enabled, you will create a corrupt index: test: terms, freq, prox...ERROR [term : docFreq=1 != num docs seen 0 + num docs deleted 0]
"
1,"ProxyCredentials disclosed to remote host. I'm using httpclient (svn-trunk of today) to connect to a remote SSL-Host 
via a proxy. The proxy requires authorization (basic) and I want to use 
preemptive authorization. 
 
Since HTTPCLIENT-514 is fixed the preemptive authorization works, but my traces 
show that the proxy credentials are also transmitted to the remote host 
through the CONNECT-tunnel, thus disclosing sensitive information to the 
remote host. 
 
My code looks like this: 
 
HttpClient client = new HttpClient(); 
HttpMethod method = new GetMethod(""https://test""); 
 
client.getHostConfiguration().setProxy(""127.0.0.1"",3128); 
client.getState().setProxyCredentials( 
                new AuthScope(""127.0.0.1"", 3128), 
                new UsernamePasswordCredentials(""proxy"", ""test"")); 
client.getState().setAuthenticationPreemptive(true); 
client.executeMethod(method); 
 
The trace: 
 
2005/11/03 13:53:13:244 CET [DEBUG] HttpMethodDirector - Preemptively 
sending default basic credentials 
2005/11/03 13:53:13:261 CET [DEBUG] HttpMethodDirector - Authenticating 
with BASIC <any realm>@127.0.0.1:3128 
2005/11/03 13:53:13:262 CET [DEBUG] HttpMethodParams - Credential charset 
not configured, using HTTP element charset 
2005/11/03 13:53:13:266 CET [DEBUG] HttpMethodDirector - Authenticating 
with BASIC <any realm>@test:443 
2005/11/03 13:53:13:267 CET [WARN] HttpMethodDirector - Required 
credentials not available for BASIC <any realm>@test:443 
2005/11/03 13:53:13:267 CET [WARN] HttpMethodDirector - Preemptive 
authentication requested but no default credentials available 
2005/11/03 13:53:13:268 CET [DEBUG] HttpConnection - Open connection to 
127.0.0.1:3128 
2005/11/03 13:53:13:279 CET [DEBUG] HttpMethodDirector - Preemptively 
sending default basic credentials 
2005/11/03 13:53:13:280 CET [DEBUG] HttpMethodDirector - Authenticating 
with BASIC <any realm>@127.0.0.1:3128 
2005/11/03 13:53:13:280 CET [DEBUG] HttpMethodParams - Credential charset 
not configured, using HTTP element charset 
2005/11/03 13:53:13:283 CET [DEBUG] header - >> ""CONNECT test:443 HTTP/1.1"" 
2005/11/03 13:53:13:284 CET [DEBUG] HttpMethodBase - Adding Host request 
header 
2005/11/03 13:53:13:284 CET [DEBUG] header - >> ""Proxy-Authorization: 
Basic cHJveHk6dGVzdA==[\r][\n]"" 
2005/11/03 13:53:13:285 CET [DEBUG] header - >> ""User-Agent: Jakarta 
Commons-HttpClient/3.0-rc4[\r][\n]"" 
2005/11/03 13:53:13:285 CET [DEBUG] header - >> ""Host: test[\r][\n]""       
                                                                           
2005/11/03 13:53:13:286 CET [DEBUG] header - >> ""Proxy-Connection: 
Keep-Alive[\r][\n]"" 
2005/11/03 13:53:13:286 CET [DEBUG] header - >> ""[\r][\n]""                 
                                                                         
2005/11/03 13:53:13:311 CET [DEBUG] header - << ""HTTP/1.0 200 
Connection established[\r][\n]""                                            
2005/11/03 13:53:13:326 CET [DEBUG] ConnectMethod - CONNECT status code 200 
2005/11/03 13:53:13:327 CET [DEBUG] HttpConnection - Secure tunnel to 
test:443 
2005/11/03 13:53:13:418 CET [DEBUG] header - >> ""GET / HTTP/1.1[\r][\n]"" 
2005/11/03 13:53:13:420 CET [DEBUG] HttpMethodBase - Adding Host request 
header 
2005/11/03 13:53:13:423 CET [DEBUG] header - >> ""Proxy-Authorization: 
Basic cHJveHk6dGVzdA==[\r][\n]"" 
2005/11/03 13:53:13:424 CET [DEBUG] header - >> ""User-Agent: Jakarta 
Commons-HttpClient/3.0-rc4[\r][\n]"" 
2005/11/03 13:53:13:425 CET [DEBUG] header - >> ""Host: test[\r][\n]"" 
2005/11/03 13:53:13:425 CET [DEBUG] header - >> ""[\r][\n]"" 
2005/11/03 13:53:14:391 CET [DEBUG] header - << ""HTTP/1.1 200 OK[\r][\n]"" 
 
As you can see the proxy credentials are also transmitted through the 
SSL-tunnel to the remote host which is a security risk."
1,"NPE doing local sensitive sorting when sort field is missing. If you do a local sensitive sort against a field that is missing from some documents in the index an NPE will get thrown.

Attached is a patch which resolved the issue and updates the sort test case to give coverage to this issue."
1,"Redirect 302 to the same URL causes max redirects exception. I noticed that if the server returns a 302 without a URL in the link, the 
HttpClient follows the empty URL up to the maximum times (100 by default).  
Instead it should check and if the URL is an empty string it shouldn't try to 
follow the redirect.

12:18:17,430 [U:          ] [main                ] ERROR 
HttpMethodBase               - Narrowly avoided an infinite loop in execute
12:18:17,430 [U:          ] [main                ] DEBUG 
URLMonitor                   - Method.execute attempt 1 failed 
http://www.stagecoach.co.uk: 
org.apache.commons.httpclient.HttpRecoverableException: Maximum redirects (100) 
exceeded
12:18:17,430 [U:          ] [main                ] DEBUG 
URLMonitor                   - HttpRecoverableException 
(http://www.stagecoach.co.uk) : 
org.apache.commons.httpclient.HttpRecoverableException: Maximum redirects (100) 
exceeded
	at org.apache.commons.httpclient.HttpMethodBase.execute
(HttpMethodBase.java:1065)
	at com.verideon.veriguard.domain.URLMonitor.monitor(URLMonitor.java:189)
	at com.verideon.veriguard.domain.URLMonitor.monitor(URLMonitor.java:101)
	at com.verideon.veriguard.domain.TestURLMonitor.getPage
(TestURLMonitor.java:58)
	at com.verideon.veriguard.domain.TestURLMonitor.monitorURL
(TestURLMonitor.java:47)
	at com.verideon.veriguard.domain.TestURLMonitor.testMonitorURLStageCoach
(TestURLMonitor.java:138)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke
(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke
(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:324)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests
(RemoteTestRunner.java:392)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run
(RemoteTestRunner.java:276)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main
(RemoteTestRunner.java:167)

Result with telnet:

GET /
HTTP/1.1 302 Object moved
Server: Microsoft-IIS/5.0
Date: Tue, 01 Jul 2003 10:05:58 GMT
X-Powered-By: ASP.NET
Location: http://www.stagecoach.co.uk
Connection: Keep-Alive
Content-Length: 121
Content-Type: text/html
Set-Cookie: ASPSESSIONIDCQCSRAAB=IFJJLEADOPDDNNGHLPFBIIIE; path=/
Cache-control: private

<head><title>Object moved</title></head>
<body><h1>Object Moved</h1>This object may be found <a HREF="""">here</a>.</body>
Connection closed by foreign host."
1,"NPE in SimpleHttpConnectionManager.shutdown(). SimpleHttpConnectionManager.shutdown() causes NPE if no connection has been created, whereas MultiThreadedHttpConnectionManager.shutdown() does not.

Simple test case:

	MultiThreadedHttpConnectionManager cm = new MultiThreadedHttpConnectionManager();
	cm.shutdown(); // OK
		
	SimpleHttpConnectionManager sm = new SimpleHttpConnectionManager();
	sm.shutdown(); // NPE


I came across this in JMeter - a sample was using Post with AutoRedirect, which (correctly) caused an IllegalArgumentException, and so the connection was not created. 

The JMeter code could try to keep track of this, but it would be tedious, and it seems to me that SimpleHttpConnectionManager should ignore the shutdown() if the connection is null.

The problem does not arise when using closeIdleConnections(timeout) - unless one uses the special value:

      closeIdleConnections(System.currentTimeMillis() - Long.MAX_VALUE)

but it would probably be sensible to protect against this as well."
1,"Intermittent failure in TestIndexWriter.testCommitThreadSafety. Mark's while(1) hudson box found this failure (and I can repro it too):

{noformat}
Error Message

MockRAMDirectory: cannot close: there are still open files: {_1m.cfs=1,
_1k.cfs=1, _14.cfs=1, _1g.cfs=1, _1h.cfs=1, _1f.cfs=1, _1n.cfs=1,
_1i.cfs=1, _1j.cfs=1, _1l.cfs=1}

Stacktrace

java.lang.RuntimeException: MockRAMDirectory: cannot close: there are
still open files: {_1m.cfs=1, _1k.cfs=1, _14.cfs=1, _1g.cfs=1,
_1h.cfs=1, _1f.cfs=1, _1n.cfs=1, _1i.cfs=1, _1j.cfs=1, _1l.cfs=1}
       at
org.apache.lucene.store.MockRAMDirectory.close(MockRAMDirectory.java:282)
       at
org.apache.lucene.index.TestIndexWriter.testCommitThreadSafety(TestIndexWriter.java:4616)
       at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:328)

Standard Output

NOTE: random codec of testcase 'testCommitThreadSafety' was: Sep

Standard Error

The following exceptions were thrown by threads:
*** Thread: Thread-1784 ***
java.lang.RuntimeException: junit.framework.AssertionFailedError: null
       at org.apache.lucene.index.TestIndexWriter$9.run(TestIndexWriter.java:4606)
Caused by: junit.framework.AssertionFailedError: null
       at junit.framework.Assert.fail(Assert.java:47)
       at junit.framework.Assert.assertTrue(Assert.java:20)
       at junit.framework.Assert.assertTrue(Assert.java:27)
       at org.apache.lucene.index.TestIndexWriter$9.run(TestIndexWriter.java:4597)
{noformat}"
1,jcr2spi: wrong status change upon conflicting removal (CacheBehaviour.OBSERVATION). with CacheBehaviour.OBSERVATION the external removal of a transiently removed item results in wrong status change that brings it back to life.
1,"IndexWriter.rollback can hang if a previous call hit an exception. IW.rollback has logic to make sure only one thread actually gets to do
the rollback whenever multiple threads are calling it at the same
time, by setting the ""boolean closing"" to true in the thread that got
there first.

Other threads wait for that variable to become false again before
returning from abort.

But, we are not restoring closing to false in a try/finally in
rollback(), which means on hitting an exception in rollback, a
subsequent call to rollback() will hang forever.

close() has the same logic, but there is already a try/finally there
to restore closing to false on exception.

The fix is straightforward.
"
1,"Node.hasProperty() with relative path can throw ClassCastException. Calling Node.hasProperty() with a relative path that traverses higher than the root node will throw a ClassCastException because the ItemId returned by HierarchyManagerImpl.resolvePath() will be the root node id.  The blind cast in the HierarchyManagerImpl.resolvePropertyPath() will then throw the ClassCastException.  This issue is not just with hasProperty/resolvePropertyPath, but any call to resolvePath that goes higher than the root node, will wrongfully get the root node id returned as result.
"
1,ItemManager registers itself as listener too early. This is similar to JCR-2168 but for ItemManager and SessionItemStateManager.
1,"Property.getLength() returns -1 for BOOLEAN, REFERENCE and DATE values. It seems those three are simply missing in PropertyImpl.getLength()."
1,StringHelper#stringDifference is wrong about supplementary chars . StringHelper#stringDifference does not take supplementary characters into account. Since this is not used internally at all we should think about removing it but I guess since it is not too complex we should just or fix it for bwcompat reasons. For released versions we should really fix it since folks might use it though. For trunk we could just drop it.
1,"spi2dav: EventFilters not respected. i have the impression that the event filter passed to the event subscription in spi2dav is not (or not properly) respected.

marcel, is there a specific reason that you always pass the static SubscriptionInfo constant (no node type filter, noLocal false) to the SubscribeMethod
in spi2dav/RepositoryServiceImpl#createSubscription ?

i guess this is the reason for the failure of
  testNodeType(org.apache.jackrabbit.test.api.observation.AddEventListenerTest)
  testNoLocalTrue(org.apache.jackrabbit.test.api.observation.AddEventListenerTest)
"
1,"Request is retried if preemptive authentication fails. Hello,

I'm using premptive authentification from an Axis client using BASIC Http
authentification. When the user isn't authenticated/authorized by server (in my
case, credentials are expired), httpclient runs a ""Chalenge"" that produces a
second request to server with same credentials.

when using preemptive mode, chalenge should be skipped if authentication scheme
hasn't changed !"
1,"SharedFieldCache can cause a memory leak. The SharedFieldCache has some problems with the way it builds the cache:
 - as key is has the IndexReader
 - as value it has a inner cache (another map) that has as a key a static inner class called 'Key'.

This 'Key' holds a reference to the comparator used for in the queries ran.
Assuming this comparator is of any type that extends from AbstractFieldComparator (I think all of the custom JR comparators), then it keeps a reference to all the InderReader instances in order to be able to load the values as Comparable(s).

So the circle is complete and the SharedFieldCache entries never get GC'ed.

One option would have been to implement a 'purge' method on the cache, similar to the lucene mechanism, and when an InderReader gets closed is could call 'purge'. But that is both ugly AND is doesn't seem to work that well :)

A more radical option is to remove the cache completely. Each instance of SimpleFieldComparator (the only client of this cache) already builds an array of the available values, so the cache would only help other instances of the same type. We'll not analyze this further.

The proposed solution (patch will follow shortly) is to remove the Comparator reference from the Key class. 
It looks like it has no real purpose there, just to impact the 'equals' of the key, which makes no sense in the first place as the lucene query does not use the Comparator info at all.
If anything, using the same field and 2 different Comparators we'll get 2 different cache entries based on the same values from the lucene index.

Feedback is appreciated!








"
1,"Thai token type() bug. While adding tests for offsets & type to ThaiAnalyzer, i discovered it does not type Thai numeric digits correctly.
ThaiAnalyzer uses StandardTokenizer, and this is really an issue with the grammar, which adds the entire [:Thai:] block to ALPHANUM.

i propose that alphanum be described a little bit differently in the grammar.
Instead, [:letter:] should be allowed to have diacritics/signs/combining marks attached to it.

this would allow the [:thai:] hack to be completely removed, would allow StandardTokenizer to parse complex writing systems such as Indian languages, and would fix LUCENE-1545.
"
1,"SO_TIMEOUT not set early enough for SOCKS proxies in PlainSocketFactory. I've created my own delegating SchemeSocketFactory implementation which supports setting SOCKS proxies on socket creation. In the connectSocket implementation, I previously just delegated to PlainSocketFactory.

The problem there was, that the SO_TIMEOUT was not set on the socket before the connection was established through the SOCKS proxy. This lead to a stop on the native read0 method because the socket is endlessly waiting for a read to occur from the proxy, so it can continue with the the connect to the actual socket destination through the proxy. I made sure I set the SO_TIMEOUT parameter in HttpParams, but it did not get honored by PlainSocketFactory.

To fix this and make HttpClient honor SO_TIMEOUT for SOCKS proxies, the following line has to be added:
  sock.setSoTimeout(HttpConnectionParams.getSoTimeout(params));
in PlainSocketFactory.connectSocket(...).

Heres the complete fixed method:

PlainSocketFactory:            
    public Socket connectSocket(
            final Socket socket,
            final InetSocketAddress remoteAddress,
            final InetSocketAddress localAddress,
            final HttpParams params) throws IOException, ConnectTimeoutException {
        if (remoteAddress == null) {
            throw new IllegalArgumentException(""Remote address may not be null"");
        }
        if (params == null) {
            throw new IllegalArgumentException(""HTTP parameters may not be null"");
        }
        Socket sock = socket;
        if (sock == null) {
            sock = createSocket();
        }
        if (localAddress != null) {
            sock.setReuseAddress(HttpConnectionParams.getSoReuseaddr(params));
            sock.bind(localAddress);
        }
        
        //FIX for SOCKS proxies which get stalled if they don't answer
        sock.setSoTimeout(HttpConnectionParams.getSoTimeout(params));
        
        int timeout = HttpConnectionParams.getConnectionTimeout(params);
        try {
            sock.connect(remoteAddress, timeout);
        } catch (SocketTimeoutException ex) {
            throw new ConnectTimeoutException(""Connect to "" + remoteAddress.getHostName() + ""/""
                    + remoteAddress.getAddress() + "" timed out"");
        }
        return sock;
    }

Currently I've implemented this in my delegating SchemeSocketFactory, because PlainSocketFactory misses this setting.

Dunno if there are other implementations of SocketFactory in HttpClient, which might need this fix. Anyway I hope this helps other people who get  headaches about halting threads because they use SOCKS proxies. :)"
1,"MultiReader does not propagate readerFinishedListeners to clones/reopened readers. While working on refactoring MultiReader/DirectoryReader in trunk, I found out that MultiReader does not correctly pass readerFinishedListeners to its clones and reopened readers."
1,"NPE in NearSpansUnordered.isPayloadAvailable() . Using RC1 of lucene 2.4 resulted in null pointer exception with some constructed SpanNearQueries

Implementation of isPayloadAvailable() (results in exception)
{code}
 public boolean isPayloadAvailable() {
   SpansCell pointer = min();
   do {
     if(pointer.isPayloadAvailable()) {
       return true;
     }
     pointer = pointer.next;
   } while(pointer.next != null);

   return false;
  }
{code}

""Fixed"" isPayloadAvailable()
{code}
 public boolean isPayloadAvailable() {
   SpansCell pointer = min();
   while (pointer != null) {
     if(pointer.isPayloadAvailable()) {
       return true;
     }
     pointer = pointer.next;
   }

   return false;
  }
{code}

Exception produced:
{code}
  [junit] java.lang.NullPointerException
    [junit]     at org.apache.lucene.search.spans.NearSpansUnordered$SpansCell.access$300(NearSpansUnordered.java:65)
    [junit]     at org.apache.lucene.search.spans.NearSpansUnordered.isPayloadAvailable(NearSpansUnordered.java:235)
    [junit]     at org.apache.lucene.search.spans.NearSpansOrdered.shrinkToAfterShortestMatch(NearSpansOrdered.java:246)
    [junit]     at org.apache.lucene.search.spans.NearSpansOrdered.advanceAfterOrdered(NearSpansOrdered.java:154)
    [junit]     at org.apache.lucene.search.spans.NearSpansOrdered.next(NearSpansOrdered.java:122)
    [junit]     at org.apache.lucene.search.spans.SpanScorer.next(SpanScorer.java:54)
    [junit]     at org.apache.lucene.search.Scorer.score(Scorer.java:57)
    [junit]     at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:137)
    [junit]     at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:113)
    [junit]     at org.apache.lucene.search.Hits.getMoreDocs(Hits.java:113)
    [junit]     at org.apache.lucene.search.Hits.<init>(Hits.java:80)
    [junit]     at org.apache.lucene.search.Searcher.search(Searcher.java:50)
    [junit]     at org.apache.lucene.search.Searcher.search(Searcher.java:40)
    [junit]     at com.attivio.lucene.SpanQueryTest.search(SpanQueryTest.java:79)
    [junit]     at com.attivio.lucene.SpanQueryTest.assertHitCount(SpanQueryTest.java:75)
    [junit]     at com.attivio.lucene.SpanQueryTest.test(SpanQueryTest.java:67)
{code}

will attach unit test that causes exception (and passes with updated isPayloadAvailable())
"
1,SPI: Helper does not properly retrieve org.apache.jackrabbit.spi.workspacename param.. consequently the Helper always obtains SessionInfo with null workspace name.
1,"Prefix fulltext queries with Japanese or Chinese characters fail to match. Prefix fulltext queries with Japanese or Chinese characters do not match because the prefix part is not tokenized. This means, when the prefix length is >1 the sequence of characters is taken as one term to do the index lookup. This will not match anything because on indexing time such characters are always broken into individual tokens."
1,"FSDirectory.list() is inconsistent. LUCENE-638 added a check to the FSDirectory.list() method to only return files that are Lucene related. I think this change made the FSDirectory implementation inconsistent with all other methods in Directory. E.g. you can create a file with an arbitrary name using FSDirectory, fileExists() will report that it is there, deleteFile() will remove it, but the array returned by list() will not contain the file.

The actual issue that was reported in LUCENE-638 was about sub directories. Those should clearly not be listed, but IMO it is not the responsibility of a Directory implementation to decide what kind of files can be created or listed. The Directory class is an abstraction of a directory and it should't to more than that.
"
1,"getAllLinearVersions does not return the base version. It appears that for a given linear version history, getAllLinearVersions returns less versions than getAllVersions -- the root version seems to be missing."
1,"BooleanFilter changed behavior in 3.5, no longer acts as if ""minimum should match"" set to 1. The change LUCENE-3446 causes a change in behavior in BooleanFilter. It used to work as if minimum should match clauses is 1 (compared to BQ lingo), but now, if no should clauses match, then the should clauses are ignored, and for example, if there is a must clause, only that one will be used and returned.

For example, a single must clause and should clause, with the should clause not matching anything, should not match anything, but, it will match whatever the must clause matches.

The fix is simple, after iterating over the should clauses, if the aggregated bitset is null, return null."
1,"BooleanQuery assumes everything else implements skipTo. skipTo seems to be optional functionality on the Scorer class (BooleanScorer
doesn't implement it).  BooleanQuery.scorer() tests all subclauses using
""instanceof BooleanQuery"" to determine if it can use a ConjunctionScorer that
requires skipTo functionality.

This means that any other new Query/Scorer that don't implement skipTo will get
into trouble when included in a BooleanQuery.

If skipTo is really optional, then there should be some way of telling by the
Scorer or the Query in a more generic manner.

Some options:
1) have a ""boolean Scorer.hasSkipTo()"" method
2) have a ""boolean Query.hasSkipTo()"" method
3) remove Scorer.skipTo and have a ""public interface ScorerSkipTo{boolean
skipTo(int doc)}"" that scorers may implement"
1,"AccessControlManager#getEffectivePolicies(String) may expose AC content without proper permissions. The implementation of AccessControlManager#getEffectivePolicies(String) in the DefaultAccessManager only checks if the session is allowed
to read AC content at the specified path. However the result may also include policies effective at absPath that should not be visible to the editing
session (read_AC permissions denied e.g. at an ancestor node) and could not be read by the editing session be means of #getPolicies().
"
1,RepositoryConfig created by Jcr2spiRepositoryFactory should always return same RepositoryService instance. The Jcr2spiRepositoryFactory uses a default implementation of RepositoryConfig if none is passed to it by the user. Currently this default implementation returns a new RepositoryService instance on each call to getRepositoryService(). This is not correct since the consumer of the RepositoryConfig instance expects the same RepositoryService instance on every call. 
1,"ObjectConverterImpl.getObject(Session, Class, String) may not resolve mapping correctly for incompletely described mappings. When a node is mapped by calling the ObjectConverter.getObject(Session, Class, String) method and no discriminator property is configured the ObjectConverterImpl class tries to find a ""best"" mapping for the effective node. This is done by walking the class descriptor hierarchy starting at the descriptor for the selected class until a mapping for the node type is found.

In case the class descriptor hierarchy is incomplete because an improperly defined class descriptor would actually perfectly map the node but is not declared to extend (or implement) its parent classes/interfaces, the hierarchy walk down will not find the mapping and thus in the end, the originally requested class will be instantiated. If the class is abstract or an interface this of course fails.

If an exact class descriptor for the node type would be looked up directly, the mapping might be found immediately and the class of the descriptor can be verified it actually is assignement compatible with the requested class. If this would fail, we could still walk the hierarchy to see, whether we find another classdescriptor.

To clarify the issue consider the following example of an abstract base class and a concrete extension class with their node types

   AbstractBaseClass maps abstractly to AbstractBaseType
   BaseClass (extends AbstractBaseClass) maps to BaseType ( with supertype AbstractBaseType )

Note, that the BaseClass mapping does not declare to extend the AbstractBaseClass.

When calling ObjectConverterImpl.getObject(session, AbstractBaseClass.class, aBaseTypeNode), the descriptor fore the AbstractBaseClass is inspected agains the node and then it is decided to check the class descriptor hierarchy. Node mapping can be found by walking the hierarchy and hence the AbstractBaseClass is instantiated, which of course fails.

If the BaseClass mapping would be declared as extending the AbstractBaseClass mapping, everything would be fine."
1,"ConcurrentModificationException during logout. We regularly get the following exception:

java.util.ConcurrentModificationException
        at org.apache.commons.collections.map.AbstractReferenceMap$ReferenceEntrySetIterator.checkMod(AbstractReferenceMap.java:761)
        at org.apache.commons.collections.map.AbstractReferenceMap$ReferenceEntrySetIterator.hasNext(AbstractReferenceMap.java:735)
        at java.util.Collections$UnmodifiableCollection$1.hasNext(Collections.java:1009)
        at java.util.Collections$UnmodifiableCollection$1.hasNext(Collections.java:1009)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.dispose(LocalItemStateManager.java:341)
        at org.apache.jackrabbit.core.WorkspaceImpl.dispose(WorkspaceImpl.java:170)
        at org.apache.jackrabbit.core.SessionImpl.logout(SessionImpl.java:1225)
        at org.apache.jackrabbit.core.XASessionImpl.logout(XASessionImpl.java:379)

Two causes for this exception have been identified:

 (Taken from an email to the dev-list from Marcel Reutegger):
> - session A reads some items I
> - session B transiently removes items in I
> - session A logs out and starts to iterate over I in  LocalItemStateManager (LISM)
> - session B saves changes and removed items are evicted from A's LISM
> - session A gets concurrent modification exception

Another scenario is the following:
- Session A gets the iterator of the values of (the primary cache of) an ItemStateReferenceCache in LocalItemStateManager.dispose.
- Session B then does something that triggers the CacheManager.
- The CacheManager then calls resizeAll, and evicts some items from the secondary cache of the ItemStateReferenceCache of which the LocalItemStateManager has a values iterator.
- The garbage collector then runs and evicts the removed items also from the primary cache, which effectively modifies the set over which is iterated.

Regards,

Martijn Hendriks"
1,"JCR-SQL2 : no count when WHERE clause is provided. whenever you provide a where-clause to a sql2 select, jcr/jackrabbit does not provide the hit count.

E.g.:
   select * from [nt:unstructured]
   order by [jcr:score]
returns the hit count (query.execute().getRows().getSize()), 
whereas
  select * from [nt:unstructured]
  where entity = ""customer""
  order by [jcr:score]
doesn't.
"
1,"Node#addNode fails with AccessDeniedException if session lacks read-permission to an ancestor. Consider a Session that has following permissions:
/home  -> no permission
/home/myself -> read|remove|set_property|add_node

if this session tries to add a Node to /home/myself.
An AccessDeniedException is thrown indicateing that it can not read /home.

The Exception is caused by the Node's check, if it is checked-out.
This check asumes that the session has read-access to all its ancestors.
Which breaks in this case:

see NodeImpl internalIsCheckedOut()   (ln 3875)
"
1,"Restoring a node fails (partially) if done within a XA transaction. A problem occurs with the following sequence of steps: 

1) Create a versionable node that has a child and a grandchild.
2) Perform a check-in of the versionable node and give a version-label.
3) Perform a restore by using the version-label.
4) Access the grandchild.

Step 4 fails, if step 3 is executed within a transaction. If no transaction is used, then step 4 succeeds. 
The test-case attached below can be executed within XATest.java (http://svn.apache.org/repos/asf/jackrabbit/trunk/jackrabbit-core/src/test/java/org/apache/jackrabbit/core/XATest.java).


public void testRestore() throws Exception {
        Session session = null;
        try {
            session = getHelper().getSuperuserSession();

            // make sure that 'testNode' does not exist at the beginning of the test
            for (NodeIterator ni = session.getRootNode().getNodes(); ni.hasNext();) {
                Node aNode = ni.nextNode();
                if (aNode.getName().equals(""testNode"")) {
                    aNode.remove();
                }
            }

            // 1) create 'testNode' that has a child and a grandchild
            session.getRootNode().addNode(""testNode"").addMixin(NodeType.MIX_VERSIONABLE);
            session.getRootNode().getNode(""testNode"").addNode(""child"").addNode(""grandchild"");
            session.save();

            // 2) check in 'testNode' and give a version-label
            Version version = session.getWorkspace().getVersionManager().checkin(
                    session.getRootNode().getNode(""testNode"").getPath());
            session.getWorkspace().getVersionManager().getVersionHistory(
                    session.getRootNode().getNode(""testNode"").getPath()).addVersionLabel(version.getName(),
                    ""testLabel"", false);

            // 3) do restore by label
            UserTransaction utx = new UserTransactionImpl(session);
            utx.begin();
            session.getWorkspace().getVersionManager().restoreByLabel(
                    session.getRootNode().getNode(""testNode"").getPath(), ""testLabel"", true);
            utx.commit();

            // 4) try to get the grandchild (fails if the restoring has been done within a transaction)
            session.getRootNode().getNode(""testNode"").getNode(""child"").getNode(""grandchild"");
        } finally {
            if (session != null) {
                session.logout();
            }
        }
    } "
1,"Clustering: re-registration of nodetypes is not  synchronized. The re-registration of nodetypes is not yet synchronized between clusternodes, although re-registration is already (partially) implemented in the NodeTypeRegistry."
1,"NPE when versioning operations are concurrent. InternalVersionManagerBase.getParentNode occasionally throws an NPE:

    protected static NodeStateEx getParentNode(NodeStateEx parent, String uuid, Name interNT)
            throws RepositoryException {
        NodeStateEx n = parent;
        for (int i = 0; i < 3; i++) {
            Name name = getName(uuid.substring(i * 2, i * 2 + 2));
            if (n.hasNode(name)) {
                n = n.getNode(name, 1);
                assert n != null;
            } else if (interNT != null) {
                n.addNode(name, interNT, null, false);
                n.store();
                n = n.getNode(name, 1);
                assert n != null;
            } else {
                return null;
            }
        }
        return n;
    }

Apparently getNode occasionally returns null due to race conditions.

Changing the code to what's below appears to fix it:



    protected static NodeStateEx getParentNode(NodeStateEx parent, String uuid, Name interNT)
            throws RepositoryException {
        NodeStateEx n = parent;
        for (int i = 0; i < 3; i++) {
            Name name = getName(uuid.substring(i * 2, i * 2 + 2));
            NodeStateEx n2 = n.getNode(name, 1);
            if (n2 != null) {
                n = n2;
            } else if (interNT != null) {
                n2 = n.addNode(name, interNT, null, false);
                n.store();
                n = n2;
            } else {
                return null;
            }
        }
        return n;
    }

(but likely moves the race condition somewhere else)"
1,"System search manager uses a SessionItemStateManager. As noted in JCR-2000, the system search manager (responsible for indexing the /jcr:system subtree) uses the SessionItemStateManager instance of the system session instead of the SharedItemStateManager of the underlying default workspace.

This can cause a deadlock (see the thread dumps in JCR-2000) when one thread is accessing the LockManager (that also uses the system session) while another thread is persisting versioning changes.

See the search-on-sism.patch attachment in JCR-2000 for a fix to this issue."
1,"DavMethodBase#getResponseException fails if the body is not (valid) XML. I have a set up that uses the JCR Webdav Server from a custom remote client.
I've noticed one thing, anytime I request a node that doesn't exist the error that comes back from the server is as follows:

[Fatal Error] :1:941: The element type ""HR"" must be terminated by the matching end-tag ""</HR>"".
javax.jcr.RepositoryException: The element type ""HR"" must be terminated by the matching end-tag ""</HR>"".: The element type ""HR"" must be terminated by the matching end-tag ""</HR>"".

Doesn't really make sense, but that is OK, I can handle that.

My problem:
I have a partially populated repository that at the root has a few nodes like
/edu/....
/com/ibm/..

So, I want to create a few nodes of type nt:folder under

com/myCompany/folder1

I have no problem creating them, but since ""com"" already exists I end up with
com[2]/myCompany/folder1.

So, I went ahead and used the parentNode.hasNode(""folderName"") method.

This method returns true for the ""com"" portion, but when I test for the ""myCompany"" folder which should return false I get the error response shown above from the server.

The webdav request looks as follows:
PROPFIND /jackrabbit/server/default/jcr%3aroot/com/myCompany

The snippet of code looks as follows:
  private Node createFolders (Session session, Node parentNode, List <String> folders)
	  throws RepositoryException {
    Node folderNode = null;
    for (String folder : folders) {
	if (parentNode.hasNode(folder))
	    folderNode = parentNode.getNode(folder);
	else
	    folderNode = parentNode.addNode(folder, ""nt:folder"");
	parentNode = folderNode;
    }
    session.save();
    return (folderNode);
  }"
1,"The repeats mechanism in SloppyPhraseScorer is broken when doc has tokens at same position. In LUCENE-736 we made fixes to SloppyPhraseScorer, because it was
matching docs that it shouldn't; but I think those changes caused it
to fail to match docs that it should, specifically when the doc itself
has tokens at the same position.
"
1,"TestParser.testSpanTermXML fails with some sims. here is why this test sometimes fails (my explanation in the test i wrote):

{noformat}
  /** make sure all sims work with spanOR(termX, termY) where termY does not exist */
  public void testCrazySpans() throws Exception {
    // The problem: ""normal"" lucene queries create scorers, returning null if terms dont exist
    // This means they never score a term that does not exist.
    // however with spans, there is only one scorer for the whole hierarchy:
    // inner queries are not real queries, their boosts are ignored, etc.
{noformat}

Basically, SpanQueries aren't really queries, you just get one scorer. it calls extractTerms on the whole hierarchy and computes weights (e.g. IDF) on
the whole bag of terms, even if they don't exist.

This is fine, we already have tests that sim's won't bug-out in computeStats() here: however they don't expect to actually score documents based on
these terms that don't exist... however this is exactly what happens in Spans because it doesn't use sub-scorers.

Lucene's sim avoids this with the (docFreq + 1)
"
1,"DocumentViewExportVisitor class incorrectly handles XML escaping for element names. The method private static String escapeName(String name) should have the following test:

 if ((i == 0) ? XMLChar.isNCNameStart(ch) : XMLChar.isNCName(ch)) {

changed into

 if ((i == 0) ? !XMLChar.isNCNameStart(ch) :! XMLChar.isNCName(ch)) {

in order to properly escape the text (those two methods in XMLChar return true when the character is valid, not the other way around."
1,"NPE if RepositoryService#getItemInfos throws ItemNotFoundException. When RepositoryService#getItemInfos throws an ItemNotFoundException, HierarchyEntryImpl#internalRemove in some cases throws an NPE. This is caused by a missing null check of the parent node."
1,"Cookie.parse exception when parsing expiry date in single quotes. A Netscape-Enterprise/3.6 SP3 server sends a cookie where the parameter expires='Thu, 05-Dec-
2002 12:07:45 GMT'. 
Cookie.parse throws an exception because none of the four built-in formats 
matches - I have tested that the parse code works OK if the single quotes are omitted from the value 
being parsed.

Resolution: If the value of the 'expires' parameter starts and ends with a 
single quote then strip the first and last character before parsing."
1,"MatchAllDocsQuery, MultiSearcher and a custom HitCollector throwing exception. I have encountered an issue with lucene1.9.1. It involves MatchAllDocsQuery, MultiSearcher and a custom HitCollector. The following code throws  java.lang.UnsupportedOperationException.

If I remove the MatchAllDocsQuery  condition (comment whole //1 block), or if I dont use the custom hitcollector (ms.search(mbq); instead of ms.search(mbq, allcoll);) the exception goes away. By stepping into the source I can see it seems due to MatchAllDocsQuery no implementing extractTerms()....


           Searcher searcher = new
IndexSearcher(""c:\\projects\\mig\\runtime\\index\\01Aug16\\"");
           Searchable[] indexes = new IndexSearcher[1];
           indexes[0] = searcher;
           MultiSearcher ms = new MultiSearcher(indexes);

           AllCollector allcoll = new AllCollector(ms);

           BooleanQuery mbq = new BooleanQuery();
           mbq.add(new TermQuery(new Term(""body"", ""value1"")),
BooleanClause.Occur.MUST_NOT);
// 1
           MatchAllDocsQuery alld = new MatchAllDocsQuery();
           mbq.add(alld, BooleanClause.Occur.MUST);
//

           System.out.println(""Query: "" + mbq.toString());

           // 2
           ms.search(mbq, allcoll);
           //ms.search(mbq);"
1,"javax.jcr.RepositoryException when a JOIN SQL2 query is send via Davex and has results. see the following thread for details:
http://www.mail-archive.com/users@jackrabbit.apache.org/msg17975.html

assuming a data structure as follows:
/foo [nt:unstructured]
/foo/bar [nt:unstructured]
/foo/bar@lala = huii (lala is string property of bar)
/ding [nt:unstructured]
/ding@dong = ##barUUID### (dong is a property of type ""Reference"")

then the following code will throw an exception:

DavexClient Client = new DavexClient(url);
Repository repo = Client.getRepository();
Credentials sc = new SimpleCredentials(""admin"",""admin"".toCharArray());
Session s = repo.login(sc,workspace);

QueryManager qm = s.getWorkspace().getQueryManager();

String sql = ""SELECT data.* FROM [nt:unstructured] AS data WHERE data.lala= 'huii'"";
sql = ""SELECT * FROM [nt:unstructured] AS data INNER JOIN [nt:unstructured] AS referring ON referring.[dong] = data.[jcr:uuid] WHERE data.lala = 'huii'"";
sql = ""SELECT * FROM [nt:unstructured] AS data INNER JOIN [nt:unstructured] AS referring ON ISDESCENDANTNODE(data, referring) WHERE data.lala = 'huii'"";
Query query = qm.createQuery(sql, Query.JCR_SQL2);
QueryResult qr = query.execute();

The first query works just fine and I can iterate over the result. Neither the second nor the third query works.
In both cases I end up with a javax.jcr.RepositoryException. Note the exception only happens if the query returns results. Aka a join will work just fine if it matches no rows."
1,"SloppyPhraseScorer returns non-deterministic results for queries with many repeats. Proximity queries with many repeats (four or more, based on my testing) return non-deterministic results. I run the same query multiple times with the same data set and get different results.

So far I've reproduced this with Solr 1.4.1, 3.1, 3.2, 3.3, and latest 4.0 trunk.

Steps to reproduce (using the Solr example):
1) In solrconfig.xml, set queryResultCache size to 0.
2) Add some documents with text ""dog dog dog"" and ""dog dog dog dog"". http://localhost:8983/solr/update?stream.body=%3Cadd%3E%3Cdoc%3E%3Cfield%20name=%22id%22%3E1%3C/field%3E%3Cfield%20name=%22text%22%3Edog%20dog%20dog%3C/field%3E%3C/doc%3E%3Cdoc%3E%3Cfield%20name=%22id%22%3E2%3C/field%3E%3Cfield%20name=%22text%22%3Edog%20dog%20dog%20dog%3C/field%3E%3C/doc%3E%3C/add%3E&commit=true
3) Do a ""dog dog dog dog""~1 query. http://localhost:8983/solr/select?q=%22dog%20dog%20dog%20dog%22~1
4) Repeat step 3 many times.

Expected results: The document with id 2 should be returned.

Actual results: The document with id 2 is always returned. The document with id 1 is sometimes returned.

Different proximity values show the same bug - ""dog dog dog dog""~5, ""dog dog dog dog""~100, etc show the same behavior.

So far I've traced it down to the ""repeats"" array in SloppyPhraseScorer.initPhrasePositions() - depending on the order of the elements in this array, the document may or may not match. I think the HashSet may be to blame, but I'm not sure - that at least seems to be where the non-determinism is coming from."
1,Unable to add/lock and unlock/remove Node with shared Session in 2 Transactions. If you try to unlock and remove a node the NodeState can be run out of sync between the two operations.
1,"UUID check in BundleFsPersistenceManager.getListRecursive() leads to endless loop. The UUID comparison in getListRecursive() is wrong and leads to an endless loop when the test PersistenceManagerIteratorTest.getAllNodeIds() is run on a workspace using BundleFsPersistenceManager.

I'm not sure this always happens, but for sure in a workspace with no content (just root and jcr:system nodes).

There's also an problem with the test case. In batch mode the after NodeId is set to the last id returned by the previous get all nodes fetch. This means batch retrieval is never actually tested, because there is no NodeId after the last one."
1,"ConcurrentModificationException in FineGrainedISMLocking. We have a report where the FineGrainedISMLocking throws a ConcurrentModificationException (stacktrace
from a Jackrabbit 2.2.x):

java.util.ConcurrentModificationException
	at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
	at java.util.HashMap$KeyIterator.next(HashMap.java:828)
	at org.apache.jackrabbit.core.state.FineGrainedISMLocking$LockMap.hasDependency(FineGrainedISMLocking.java:388)
	at org.apache.jackrabbit.core.state.FineGrainedISMLocking.acquireWriteLock(FineGrainedISMLocking.java:138)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.acquireWriteLock(SharedItemStateManager.java:1848)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.access$200(SharedItemStateManager.java:113)
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:563)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:1457)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:1487)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:351)
	at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:354)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:326)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:289)
	at org.apache.jackrabbit.core.ItemSaveOperation.perform(ItemSaveOperation.java:258)
	at org.apache.jackrabbit.core.session.SessionState.perform(SessionState.java:200)
	at org.apache.jackrabbit.core.ItemImpl.perform(ItemImpl.java:91)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:329)
	at org.apache.jackrabbit.core.session.SessionSaveOperation.perform(SessionSaveOperation.java:42)
	at org.apache.jackrabbit.core.session.SessionState.perform(SessionState.java:200)
	at org.apache.jackrabbit.core.SessionImpl.perform(SessionImpl.java:355)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:758)"
1,"Parsing expires. Seeing this very often:

 Invalid cookie header: ""Set-Cookie: _asid=011e7014f5e7718e02d893335aa5a16e; path=/; expires=Wed, 16 May 2018 17:13:32 GMT"". Unable to parse expires attribute: Wed, 16 May 2018 17:13:32 GMT"
1,GC resources in TermInfosReader when exception occurs in its constructor. I replaced IndexModifier with IndexWriter in test case TestStressIndexing and noticed the test failed from time to time because some .tis file is still open when MockRAMDirectory.close() is called. It turns out it is because .tis file is not closed if an exception occurs in TermInfosReader's constructor.
1,Incomplete JCR-1664 fix in BindableRepository. The changes in JCR-1664 were not fully merged from trunk to the 1.4 branch due to a typo in the commit message of revision 683268. As a result a BindableRepository subclass becomes a bit cumbersome to implement. I hope to fix this in 1.4.7 so that subclasses written for both the partial and fully merged JCR-1664 fix should work.
1,"CLONE -Merge error during add to index (IndexOutOfBoundsException). I've been batch-building indexes, and I've build a couple hundred indexes with 
a total of around 150 million records.  This only happened once, so it's 
probably impossible to reproduce, but anyway... I was building an index with 
around 9.6 million records, and towards the end I got this:

java.lang.IndexOutOfBoundsException: Index: 54, Size: 24
        at java.util.ArrayList.RangeCheck(ArrayList.java:547)
        at java.util.ArrayList.get(ArrayList.java:322)
        at org.apache.lucene.index.FieldInfos.fieldInfo(FieldInfos.java:155)
        at org.apache.lucene.index.FieldInfos.fieldName(FieldInfos.java:151)
        at org.apache.lucene.index.SegmentTermEnum.readTerm(SegmentTermEnum.java
:149)
        at org.apache.lucene.index.SegmentTermEnum.next
(SegmentTermEnum.java:115)
        at org.apache.lucene.index.SegmentMergeInfo.next
(SegmentMergeInfo.java:52)
        at org.apache.lucene.index.SegmentMerger.mergeTermInfos
(SegmentMerger.java:294)
        at org.apache.lucene.index.SegmentMerger.mergeTerms
(SegmentMerger.java:254)
        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:93)
        at org.apache.lucene.index.IndexWriter.mergeSegments
(IndexWriter.java:487)
        at org.apache.lucene.index.IndexWriter.maybeMergeSegments
(IndexWriter.java:458)
        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:310)
        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:294)"
1,"IndexReader.open(String|File) may incorrectly throw AlreadyClosedException. Spinoff from here:

    http://www.nabble.com/Runtime-exception-when-creating-IndexSearcher-to20226279.html

If you open an IndexSearcher/Reader, passing in String or File, then
closeDirectory is set to true in the reader.

If the index has a single segment, then SegmentReader.get is used to
open the index.  If an IOException is hit in there, the SegmentReader
closes itself and then closes the directory since closeDirectory is
true.

The problem is, the retry logic in SegmentInfos (to look for another
segments_N to try) kicks in and hits an AlreadyClosedException,
masking the original root cause.

Workaround is to separately get the Directory using
FSDirectory.getDirectory, and then instantiate IndexSearcher/Reader
from that.

This manifests as masking the root cause of a corrupted single-segment
index with a confusing AlreadyClosedException.  You could also hit
the false exception if the writer was in the process of committing
(ie, a retry was really needed) or if there is some transient IO
problem opening the index (eg too many open files).
"
1,"An IOException or RuntimeException leaves the underlying socket in an undetermined state. If an application level IOException or RuntimeException occurs, the underlying
socket will be in an undetermined state. In many cases, this will lead to zombie
connections in the pool that do not respond properly.

Simple example: uploading a file via POST. If we promise the server 1MB of data.
Shortly after starting the transfer an IOException occurs (e.g. the NFS server
the file was residing on stops responding). The connection is returned to the
pool (see HTTPCLIENT-302) but the the server is still expecting close to 1MB of data
on that socket. The next request on that socket (e.g. a GET) will send the HTTP
header but  the server thinks the header is part of the old stream and doesn't
respond."
1,"JCR2SPI: NPE when parentId returned by NodeInfo.getParentId does not show up in parent's child node list. In this custom SPI implementation, version history nodes appear as children of jcr:versionStorage, but jcr:versionStorage does not return them as children (which would be impractical for performance reasons - I expect similar approaches used by others...).

getParentId of a NodeInfo of a VersionHistory return the NodeId for jcr:versionStorage. In this case, I get the NPE below:

java.lang.NullPointerException
	at org.apache.jackrabbit.jcr2spi.state.WorkspaceItemStateFactory.createNodeState(WorkspaceItemStateFactory.java:99)
	at org.apache.jackrabbit.jcr2spi.state.CachingItemStateManager.resolve(CachingItemStateManager.java:168)
	at org.apache.jackrabbit.jcr2spi.state.CachingItemStateManager.getItemState(CachingItemStateManager.java:94)
	at org.apache.jackrabbit.jcr2spi.WorkspaceManager.getItemState(WorkspaceManager.java:328)
	at org.apache.jackrabbit.jcr2spi.state.TransientISFactory.createNodeState(TransientISFactory.java:120)
	at org.apache.jackrabbit.jcr2spi.state.CachingItemStateManager.resolve(CachingItemStateManager.java:168)
	at org.apache.jackrabbit.jcr2spi.state.CachingItemStateManager.getItemState(CachingItemStateManager.java:94)
	at org.apache.jackrabbit.jcr2spi.state.TransientItemStateManager.getItemState(TransientItemStateManager.java:209)
	at org.apache.jackrabbit.jcr2spi.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:155)
	at org.apache.jackrabbit.jcr2spi.SessionImpl.getNodeById(SessionImpl.java:271)
	at org.apache.jackrabbit.jcr2spi.SessionImpl.getNodeByUUID(SessionImpl.java:239)

Returning null in this special case fixes the problem over here, but seems to create new problems elsewhere.

Need to clarify the SPI itself, and potentially fix JCR2CPI.
"
1,"Long values not properly stored. When a long value assigned to a property is too big, when restarting the server the value become 0 !! 

The test pass with versions 1.6.4 and 2.0"
1,"SO_TIMEOUT is not set on a request level. The scenario is as follows: I'm doing two consecutive requests to the same host, using a multi-threaded (or thread safe) connection pool manager. The first invocation has a timeout of 10s and the second has a timeout of 30s. 

In version 3.1 of HttpClient all works well, but in 4.0 I get a timeout exception in the second request, after ~10 seconds, which means the first timeout is used.

Looking at the code, I see that in version 3.1, the HttpMethodDirector.executeWithRetry() method invokes a method named applyConnectionParams() that took care of setting the timeout taken from the request on the socket. 

But in version 4.0, the only place I see the timeout is set on the socket is when DefaultRequestDirector.execute(HttpHost, HttpRequest, HttpContext) opens a connection using the managedConn.open() method. Since the connection is reused between the requests, the second request uses a socket with a timeout of the first request.
"
1,"NPE in StopFilter caused by StandardAnalyzer(boolean replaceInvalidAcronym) constructor. I think that I found a problem with the new code (https://issues.apache.org/jira/browse/LUCENE-1068).
Usage of the new constructor StandardAnalyzer(boolean replaceInvalidAcronym) causes NPE in
StopFilter:

java.lang.NullPointerException
        at org.apache.lucene.analysis.StopFilter.<init>(StopFilter.java:74)
        at org.apache.lucene.analysis.StopFilter.<init>(StopFilter.java:86)
        at
org.apache.lucene.analysis.standard.StandardAnalyzer.tokenStream(StandardAnalyzer.java:151)
        at
org.apache.lucene.queryParser.QueryParser.getFieldQuery(QueryParser.java:452)
        at
org.apache.lucene.queryParser.QueryParser.Term(QueryParser.java:1133)
        at
org.apache.lucene.queryParser.QueryParser.Clause(QueryParser.java:1020)
        at
org.apache.lucene.queryParser.QueryParser.Query(QueryParser.java:948)
        at
org.apache.lucene.queryParser.QueryParser.Clause(QueryParser.java:1024)
        at
org.apache.lucene.queryParser.QueryParser.Query(QueryParser.java:948)
        at
org.apache.lucene.queryParser.QueryParser.TopLevelQuery(QueryParser.java:937)
        at
org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:147)

The reason is that new constructor forgets to initialize the stopSet field:
  public StandardAnalyzer(boolean replaceInvalidAcronym) {
    this.replaceInvalidAcronym = replaceInvalidAcronym;
  }

I guess this should be changed to something like this:
  public StandardAnalyzer(boolean replaceInvalidAcronym) {
    this(STOP_WORDS);
    this.replaceInvalidAcronym = replaceInvalidAcronym;
  }

The bug is present in RC3. Fix is one line, it'll be great to have it in 2.3
release.
"
1,"Check for correct content-type in URLEncodedUtils not working for encoding-suffixes. Dear DEV-Team,

i am developing an application with the httpclient. Today i found a small problem, related to URLEncodedUtils.

Our Tomcat-Server deliveres for Server-Requests, the HTTP-Header: ""Content-Type=application/x-www-form-urlencoded;charset=UTF-8"", but the httpclient only checks for: ""Content-Type=application/x-www-form-urlencoded"". This failing check results in an empty result of call to the method: URLEncodedUtils.parse(entity);

Following source-code causes the prob: 

public class URLEncodedUtils {

    /**
     * Returns true if the entity's Content-Type header is
     * <code>application/x-www-form-urlencoded</code>.
     */
    public static boolean isEncoded (final HttpEntity entity) {
        final Header contentType = entity.getContentType();
        return (contentType != null && contentType.getValue().equalsIgnoreCase(CONTENT_TYPE));
    }
}

IMO the method should be changed to:


public class URLEncodedUtils {

    /**
     * Returns true if the entity's Content-Type header is
     * <code>application/x-www-form-urlencoded</code>.
     */
    public static boolean isEncoded (final HttpEntity entity) {
        final Header contentType = entity.getContentType();
        return (contentType != null && contentType.getValue().startsWith(CONTENT_TYPE + "";""));
    }
}

Best Regards,"
1,"sysview import does not resolve references. when importing a sysview with references, those are not resolved against the new uuids of the mix:referenceable nodes"
1,"ISOLatin1AccentFilter discards position increments of filtered terms. Not sure if this is a bug, but looks like one to me..."
1,"FileNotFoundException thrown by Directory.copy(). java.io.FileNotFoundException: segments_bu
        at org.apache.lucene.store.RAMDirectory.openInput(RAMDirectory.java:234)
        at org.apache.lucene.store.Directory.copy(Directory.java:190)

"
1,"DigestScheme.authenticate returns invalid authorization string when algorithm is null. DigestScheme.authenticate returns invalid authorization string when algorithm 
is null. 
I traced the bug and found the following from the method call:
authenticate(Credentials credentials, String method, String uri) calls
authenticate(UsernamePasswordCredentials credentials,
            Map params) calls
createDigest(String uname, String pwd,
            Map params)
  and properly defaults algorithm to MD5 if null
but the final call to createDigestHeader(String uname, Map params,
            String digest) does not default algorithm to MD5 if null"
1,"Filters need hashCode() and equals(). Filters need to implement hashCode() and equals(), esp since certain query types can contain a filter (FilteredQuery, ConstantScoreQuery)"
1,"OOM erros with CheckIndex with indexes containg a lot of fields with norms. All index readers have a cache of the last used norms (SegmentReader, MultiReader, MultiSegmentReader,...). This cache is never cleaned up, so if you access norms of a field, the norm's byte[maxdoc()] array is not freed until you close/reopen the index.

You can see this problem, if you create an index with many fields with norms (I tested with about 4,000 fields) and many documents (half a million). If you then call CheckIndex, that calls norms() for each (!) field in the Segment and each of this calls creates a new cache entry, you get OutOfMemoryExceptions after short time (I tested with the above index: I was not able to do a CheckIndex even with ""-Xmx 16GB"" on 64bit Java).

CheckIndex opens and then tests each segment of a index with a separate SegmentReader. The big index with the OutOfMemory problem was optimized, so consisting of one segment with about half a million docs and about 4,000 fields. Each byte[] array takes about a half MiB for this index. The CheckIndex funtion created the norm for 4000 fields and the SegmentReader cached them, which is about 2 GiB RAM. So OOMs are not unusal.

In my opinion, the best would be to use a Weak- or better a SoftReference so norms.bytes gets java.lang.ref.SoftReference<byte[]> and used for caching. With proper synchronization (which is done on the norms cache in SegmentReader) you can do the best with SoftReference, as this reference is garbage collected only when an OOM may happen. If the byte[] array is freed (but it is only freed if no other references exist), a lter call to getNorms() creates a new array. When code is hard referencing the norms array, it will not be freed, so no problem. The same could be done for the other IndexReaders.

Fields without norm() do not have this problem, as all these fields share a one-time allocated dummy norm array. So the same index without norms enabled for most of the fields checked perfectly.

I will prepare a patch tomorrow.

Mike proposed another quick fix for CheckIndex:
bq. we could do something first specifically for CheckIndex (eg it could simply use the 3-arg non-caching bytes method instead) to prevent OOM errors when using it.
"
1,ItemInfoBuilder fails to set correct path on properties. This only happens if the parent node's nodeId is id based (in contrast to path based). The build() method should not rely on the nodeId providing the full path. Instead it should us the parent node's getPath() method to construct the full path. 
1,"NPE in MultiReader.isCurrent() and getVersion(). I'm attaching a fix for the NPE in MultiReader.isCurrent() plus a testcase. For getVersion(), we should throw a better exception that NPE. I will commit unless someone objects or has a better idea."
1,"Repository lock keeps file open. The RepositoryLock opens a RandomAccessFile, but does not close it. The problematic line is:

lock = new RandomAccessFile(file, ""rw"").getChannel().tryLock();

This is usually not a problem as the file will be closed when the RandomAccessFile object is garbage collected. However, if called a lot in a short time frame, this results in 'too many open files' in some environments (for example Linux). "
1,"TermVectors index files can become corrupt when autoCommit=false. Spinoff from this thread:

  http://www.gossamer-threads.com/lists/lucene/java-dev/55951

There are actually 2 separate cases here, both only happening when
autoCommit=false:

  * First issue was caused by LUCENE-843 (sigh): if you add a bunch of
    docs with no term vectors, such that 1 or more flushes happen;
    then you add docs that do have term vectors, the tvx file will not
    have enough entries (= corruption).

  * Second issue was caused by bulk merging of term vectors
    (LUCENE-1120 -- only in trunk) and bulk merging of stored fields
    (LUCENE-1043, in 2.3), and only shows when autoCommit=false, and,
    the bulk merging optimization runs.  In this case, the code that
    reads the rawDocs tries to read too far in the tvx/fdx files (it's
    not really index corruption but rather a bug in the rawDocs
    reading).

"
1,"When fetching node ids in checks for the checker all queries should use the same ordering. The ""bundleSelectAllIdsSQL"" query and the ""bundleSelectAllIdsFromSQL"" should use the same ordering."
1,"Connection with the proxy is not reopened if an proxy auth failure occurs while SSL tunnel is being established. Connection with the proxy is not reopened if an proxy auth failure occurs while
SSL tunnel is being established.

This problem has been reported by on the httpclient-user by Gebhard Gaukler
<gebhard.gaukler at db.com>.

My bad.

Oleg"
1,"Highlighter has problems when you use StandardAnalyzer with LUCENE_29 or simplier StopFilter with stopWordsPosIncr mode switched on. This is a followup on LUCENE-1987:

If you set in HighligterTest the constant static final Version TEST_VERSION = Version.LUCENE_24 to LUCENE_29 or LUCENE_CURRENT, the test testSimpleQueryScorerPhraseHighlighting fails. Please note, that currently (before LUCENE-2002 is fixed), you must also set the QueryParser to respect posIncr."
1,"HttpClient does not retry authentication when multiple challenges are present if the primary one fails. I'm trying to request a page from IIS (6 and 7.5).  If the IIS is configured with providers for ""negotiate"" and ""ntlm"" then the Negotiate authentication is tried and fails, but it does not then try to use the NTLM authentication which is what I require.  If I removed ""negotiate"" as a provider from IIS and just use NTLM then all works well - but this is not a solution as I don't have control of the web servers. 

Output below...


[DEBUG] SingleClientConnManager - Get connection for route HttpRoute[{}->http://WIN-HNB91NNAB2G]
[DEBUG] DefaultClientConnectionOperator - Connecting to WIN-HNB91NNAB2G/147.183.80.134:80
[DEBUG] RequestAddCookies - CookieSpec selected: best-match
[DEBUG] DefaultHttpClient - Attempt 1 to execute request
[DEBUG] DefaultClientConnection - Sending request: GET / HTTP/1.1
[DEBUG] wire - >> ""GET / HTTP/1.1[\r][\n]""
[DEBUG] wire - >> ""Host: WIN-HNB91NNAB2G[\r][\n]""
[DEBUG] wire - >> ""Connection: Keep-Alive[\r][\n]""
[DEBUG] wire - >> ""User-Agent: Apache-HttpClient/4.1 (java 1.5)[\r][\n]""
[DEBUG] wire - >> ""[\r][\n]""
[DEBUG] headers - >> GET / HTTP/1.1
[DEBUG] headers - >> Host: WIN-HNB91NNAB2G
[DEBUG] headers - >> Connection: Keep-Alive
[DEBUG] headers - >> User-Agent: Apache-HttpClient/4.1 (java 1.5)
[DEBUG] wire - << ""HTTP/1.1 401 Unauthorized[\r][\n]""
[DEBUG] wire - << ""Content-Type: text/html[\r][\n]""
[DEBUG] wire - << ""Server: Microsoft-IIS/7.5[\r][\n]""
[DEBUG] wire - << ""WWW-Authenticate: Negotiate[\r][\n]""
[DEBUG] wire - << ""WWW-Authenticate: NTLM[\r][\n]""
[DEBUG] wire - << ""Date: Fri, 15 Jul 2011 12:15:11 GMT[\r][\n]""
[DEBUG] wire - << ""Content-Length: 58[\r][\n]""
[DEBUG] wire - << ""[\r][\n]""
[DEBUG] DefaultClientConnection - Receiving response: HTTP/1.1 401 Unauthorized
[DEBUG] headers - << HTTP/1.1 401 Unauthorized
[DEBUG] headers - << Content-Type: text/html
[DEBUG] headers - << Server: Microsoft-IIS/7.5
[DEBUG] headers - << WWW-Authenticate: Negotiate
[DEBUG] headers - << WWW-Authenticate: NTLM
[DEBUG] headers - << Date: Fri, 15 Jul 2011 12:15:11 GMT
[DEBUG] headers - << Content-Length: 58
[DEBUG] DefaultHttpClient - Connection can be kept alive indefinitely
[DEBUG] DefaultHttpClient - Target requested authentication
[DEBUG] DefaultTargetAuthenticationHandler - Authentication schemes in the order of preference: [negotiate, NTLM, Digest, Basic]
[DEBUG] DefaultTargetAuthenticationHandler - negotiate authentication scheme selected
[DEBUG] NegotiateScheme - Received challenge '' from the auth server
[DEBUG] DefaultHttpClient - Authorization challenge processed
[DEBUG] DefaultHttpClient - Authentication scope: NEGOTIATE <any realm>@win-hnb91nnab2g:80
[DEBUG] DefaultHttpClient - Found credentials
[DEBUG] wire - << ""You do not have permission to view this directory or page.""
[DEBUG] RequestAddCookies - CookieSpec selected: best-match
[DEBUG] NegotiateScheme - init WIN-HNB91NNAB2G
[ERROR] RequestTargetAuthentication - Authentication error: Invalid name provided (Mechanism level: Could not load configuration file C:\WINDOWS\krb5.ini (The system cannot find the file specified))
[DEBUG] DefaultHttpClient - Attempt 2 to execute request
[DEBUG] DefaultClientConnection - Sending request: GET / HTTP/1.1
[DEBUG] wire - >> ""GET / HTTP/1.1[\r][\n]""
[DEBUG] wire - >> ""Host: WIN-HNB91NNAB2G[\r][\n]""
[DEBUG] wire - >> ""Connection: Keep-Alive[\r][\n]""
[DEBUG] wire - >> ""User-Agent: Apache-HttpClient/4.1 (java 1.5)[\r][\n]""
[DEBUG] wire - >> ""[\r][\n]""
[DEBUG] headers - >> GET / HTTP/1.1
[DEBUG] headers - >> Host: WIN-HNB91NNAB2G
[DEBUG] headers - >> Connection: Keep-Alive
[DEBUG] headers - >> User-Agent: Apache-HttpClient/4.1 (java 1.5)
[DEBUG] wire - << ""HTTP/1.1 401 Unauthorized[\r][\n]""
[DEBUG] wire - << ""Content-Type: text/html[\r][\n]""
[DEBUG] wire - << ""Server: Microsoft-IIS/7.5[\r][\n]""
[DEBUG] wire - << ""WWW-Authenticate: Negotiate[\r][\n]""
[DEBUG] wire - << ""WWW-Authenticate: NTLM[\r][\n]""
[DEBUG] wire - << ""Date: Fri, 15 Jul 2011 12:15:11 GMT[\r][\n]""
[DEBUG] wire - << ""Content-Length: 58[\r][\n]""
[DEBUG] wire - << ""[\r][\n]""
[DEBUG] DefaultClientConnection - Receiving response: HTTP/1.1 401 Unauthorized
[DEBUG] headers - << HTTP/1.1 401 Unauthorized
[DEBUG] headers - << Content-Type: text/html
[DEBUG] headers - << Server: Microsoft-IIS/7.5
[DEBUG] headers - << WWW-Authenticate: Negotiate
[DEBUG] headers - << WWW-Authenticate: NTLM
[DEBUG] headers - << Date: Fri, 15 Jul 2011 12:15:11 GMT
[DEBUG] headers - << Content-Length: 58
[DEBUG] DefaultHttpClient - Connection can be kept alive indefinitely
[DEBUG] DefaultHttpClient - Target requested authentication
[DEBUG] NegotiateScheme - Received challenge '' from the auth server
[DEBUG] NegotiateScheme - Authentication already attempted
[DEBUG] DefaultHttpClient - Authorization challenge processed
[DEBUG] DefaultHttpClient - Authentication scope: NEGOTIATE <any realm>@win-hnb91nnab2g:80
[DEBUG] DefaultHttpClient - Authentication failed
[DEBUG] wire - << ""You do not have permission to view this directory or page.""
content:You do not have permission to view this directory or page.
[DEBUG] SingleClientConnManager - Releasing connection org.apache.http.impl.conn.SingleClientConnManager$ConnAdapter@17fa65e
"
1,"SegmentMerger should assert .del and .s* files are not passed to createCompoundFile. Spinoff from LUCENE-3126. SegmentMerger.createCompoundFile does not document that it should not receive files that are not included in the .cfs, such as .del and .s* (separate norms). Today, that method is called from code which ensures that, but we should:
# Add some documentation to clarify that.
# Add some asserts so that if a test (or other code, running w/ -ea) does that, we catch it.

Will post a patch soon"
1,"Sleep in possibly endless loop in ObservationDispatcher. The rate-limitation code we added in JCR-2402 to prevent the observation queue from growing too large was a good idea, but the current implementation is a bit troublesome since it blocks the thread while it still holds the journal lock, the SISM reader lock, and the SessionState lock. This can cause a deadlock under heavy workloads if any of the observation listeners attempts to reuse the session (not recommended/supported, but can still happen) or write to the repository (quite likely).

To solve this problem we should move the rate-limiter code to outside the scope of any internal locks."
1,"Large distances in Spatial go beyond Prime MEridian. http://amidev.kaango.com/solr/core0/select?fl=*&json.nl=map&wt=json&radius=5000&rows=20&lat=39.5500507&q=honda&qt=geo&long=-105.7820674

Get an error when using Solr when distance is calculated for the boundary box past 90 degrees.


Aug 4, 2009 1:54:00 PM org.apache.solr.common.SolrException log
SEVERE: java.lang.IllegalArgumentException: Illegal lattitude value 93.1558669413734
        at org.apache.lucene.spatial.geometry.FloatLatLng.<init>(FloatLatLng.java:26)
        at org.apache.lucene.spatial.geometry.shape.LLRect.createBox(LLRect.java:93)
        at org.apache.lucene.spatial.tier.DistanceUtils.getBoundary(DistanceUtils.java:50)
        at org.apache.lucene.spatial.tier.CartesianPolyFilterBuilder.getBoxShape(CartesianPolyFilterBuilder.java:47)
        at org.apache.lucene.spatial.tier.CartesianPolyFilterBuilder.getBoundingArea(CartesianPolyFilterBuilder.java:109)
        at org.apache.lucene.spatial.tier.DistanceQueryBuilder.<init>(DistanceQueryBuilder.java:61)
        at com.pjaol.search.solr.component.LocalSolrQueryComponent.prepare(LocalSolrQueryComponent.java:151)
        at org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:174)
        at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:131)
        at org.apache.solr.core.SolrCore.execute(SolrCore.java:1328)
        at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:341)
        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:244)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:128)
        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102)
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)
        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:286)
        at org.apache.coyote.http11.Http11AprProcessor.process(Http11AprProcessor.java:857)
        at org.apache.coyote.http11.Http11AprProtocol$Http11ConnectionHandler.process(Http11AprProtocol.java:565)
        at org.apache.tomcat.util.net.AprEndpoint$Worker.run(AprEndpoint.java:1509)
        at java.lang.Thread.run(Thread.java:619)


"
1,"NullPointerException thrown when invalid header encountered. If a server returns a header with no name but with a value (ie: an invalid line in the headers), HttpClient throws a NullPointerException instead of just skipping that header line or perhaps treating it as a continuation of the previous header (need to consult the RFC to confirm this).

Problem reported by Eduardo Francos on the commons-user list.

A good test URL for this problem is:

http://www.pc.ibm.com/us/accessories/monitors/p_allmodelos.html

which should return a 404 error but throws the NullPointerException instead."
1,JCARepositoryManager does not close InputStream. JCR-3129 opened a already closed issue [JCR-1667]
1,"Lock.obtain(timeout) behaves incorrectly for large timeouts. Because timeout is a long, but internal values derived from timeout
are ints, its possible to overflow those internal values into negative
numbers and cause incorrect behavior.

Spinoff from this thread:

  http://www.gossamer-threads.com/lists/lucene/java-user/54376

"
1,"SessionItemStateManager.getIdOfRootTransientNodeState() may cause NPE. regression of JCR-2425

in certain scenarios, calling SessionItemStateManager.getIdOfRootTransientNodeState() may cause a NPE.

Test case: 

        Repository repository = new TransientRepository(); 
        Session session = repository.login( 
                new SimpleCredentials(""admin"", ""admin"".toCharArray())); 
        Session session2 = repository.login( 
                new SimpleCredentials(""admin"", ""admin"".toCharArray())); 

        try { 
            while (session.getRootNode().hasNode(""test"")) { 
                session.getRootNode().getNode(""test"").remove(); 
            } 
            Node test = session.getRootNode().addNode(""test""); 
            session.save(); 
            Node x = test.addNode(""x""); 
            session.save(); 

            Node x2 = session2.getRootNode().getNode(""test"").getNode(""x""); 
            x2.remove(); 
            x.addNode(""b""); 
            session2.save(); 
            session.save(); // throws NPE 
        } finally { 
            session.logout(); 
            session2.logout(); 
        }"
1,"Node.restore() fails for existing non-versioned OPV=Version child nodes. I have a node whose definition has properties and child nodes.  The
definitions of the nodetypes for the node and the child include
mix:versionable.  The properties definitions have onParentVersion=COPY and
the child nodes have onParentVersion=VERSION.  When I create a node with
child nodes and checkin and then restore the node, I get a
""....VersionException: Restore of root node not allowed""  This is
occurring on the restore of the child node.

According to the spec:

Child Node
On checkin of N, the node VN will get a subnode of type nt:versionedChild
with the same name as C. The single property of this node,
jcr:childVersionHistory is a REFERENCE to the version history of C (not to
C or any actual version of C). This also requires that C itself be
versionable (otherwise it would not have a version history).
.
.
.
On restore of VN, if the workspace currently has an already existing node
corresponding to C?s version history and the removeExisting flag of the
restore is set to true, then that instance of C becomes the child of the
restored N. If the workspace currently has an already existing node
corresponding to C?s version history and the removeExisting flag of the
restore is set to false then an ItemExistsException is thrown.


I'm restoring the node using

   node.restore(version, true);

Is this expected behavior?"
1,"FilterImpl.getStringValue() does not use custom converter class specified in @Field annotation. I have a POJO with the following field:

    @Field(converter = LocaleConverter.class)
    private Locale                locale;

When I attempt to query for objects based on this field, I get a NullPointerException:

java.lang.NullPointerException
        at org.apache.jackrabbit.ocm.query.impl.FilterImpl.getStringValue(FilterImpl.java:281)
        at org.apache.jackrabbit.ocm.query.impl.FilterImpl.addEqualTo(FilterImpl.java:129)

FilterImpl should preferentially use the atomic type converter defined in the @Field annotation to convert the value, then fallback to the global converters.  Converting the Locale to a string for use in the query is a workaround, but the logic for string conversion should only reside in my LocaleConverter class.
"
1,"OracleFileSystem can't handle empty files. the following exception is thrown when trying to access a 0-length file
in an OracleFileSystem:
java.sql.SQLException: ORA-22275: invalid LOB locator specified

issue reported on the users list, 
see http://www.nabble.com/problems-with-Oracle-tf2483987.html#a6926522

"
1,"Text.isDescendant returns false if parent is '/'. the method isDescendant(String, String) of the Text utility class returns false if the 
passed potential parent-path represents the root node (""/"").

"
1,"SPI2DAVex: HttpClient StringPart uses charset US-ASCII by default. if the diff is sent as multipart instead of a urlencoded post string properties may be garbeled.
reason: instances of httpclient StringPart are created without specifying the charset in which case US-ASCII is used by default."
1,"ParameterParser parse method for authentication headers does not appear to deal with empty value strings. Hi, I have found an issue with HTTPClient due to the way it parses parameter 
strings.

In particular, consider the following WWW-Authenticate header:

WWW-Authenticate: Digest realm="""", algorithm=MD5, qop=""auth"", 
domain=""/content"", nonce=""0e11dcf146563c3a89e5327f0c5f5bad""
 
The realm is definitely specified, but is equal to the empty string.  It is not 
a null value.

However, the extractParams method of AuthChallengeParser which in turn calls 
ParameterParser will actually parse the value as Null  instead of an empty 
string.

This is due to parseQuotedToken getToken(true) call which essentially returns a 
null String result  as the condition i2>i1 fails :-

        String result = null;
        if (i2 > i1) {
            result = new String(chars, i1, i2 - i1);
        }
        return result;

As the processChallenge method of DigestScheme throws an exception when 
getParameter(""realm"") == null, HTTPClient is not able to process the digest 
request when an empty string realm value is present."
1,"Setting a property which has been transiently removed fails with a PathNotFoundException. The following tests currently all fail with a PathNotFoundException

org.apache.jackrabbit.jcr2spi.AddPropertyTest#testReplacingProperty
org.apache.jackrabbit.jcr2spi.AddPropertyTest#testReplacingProperty2
org.apache.jackrabbit.jcr2spi.AddPropertyTest#testAddingProperty
org.apache.jackrabbit.jcr2spi.AddPropertyTest#testAddingProperty2

"
1,"StackOverflowError in HttpConnection. When the HttpConnection#WrappedOutputStream.flush () encounters IOException 
druign write, it is calling HttpConnection.close which calls 
HttpConnection.closeSocketAndStreams and which eventually calls 
HttpConnection#WrappedOutputStream.flush again.  The circular calls will cause 
StackOverflowError.

I run into this accidentally when I was trying to extend HttpConnection.  But 
looking through the code, I believe any IOException may cause the same 
problem.  The circular calls should be either removed or controlled.  Below is 
part of teh stack trace

java.lang.StackOverflowError
        at java.lang.Exception.<init>(Unknown Source)
        at java.io.IOException.<init>(Unknown Source)
        at java.net.SocketException.<init>(Unknown Source)
        at java.net.SocketOutputStream.socketWrite(Native Method)
        at java.net.SocketOutputStream.write(Unknown Source)
        at org.apache.commons.httpclient.HttpConnection$WrappedOutputStream.writ
e(HttpConnection.java:1273)
        at java.io.BufferedOutputStream.flushBuffer(Unknown Source)
        at java.io.BufferedOutputStream.flush(Unknown Source)
        at java.io.FilterOutputStream.close(Unknown Source)
        at org.apache.commons.httpclient.HttpConnection.closeSocketAndStreams(Ht
tpConnection.java:1083)
        at org.apache.commons.httpclient.HttpConnection.close(HttpConnection.jav
a:1024)
        at org.apache.commons.httpclient.HttpConnection$WrappedOutputStream.hand
leException(HttpConnection.java:1235)
        at org.apache.commons.httpclient.HttpConnection$WrappedOutputStream.writ
e(HttpConnection.java:1275)
        at java.io.BufferedOutputStream.flushBuffer(Unknown Source)
        at java.io.BufferedOutputStream.flush(Unknown Source)
        at java.io.FilterOutputStream.close(Unknown Source)
        at org.apache.commons.httpclient.HttpConnection.closeSocketAndStreams(Ht
tpConnection.java:1083)
        at org.apache.commons.httpclient.HttpConnection.close(HttpConnection.jav
a:1024)
        at org.apache.commons.httpclient.HttpConnection$WrappedOutputStream.hand
leException(HttpConnection.java:1235)
        at org.apache.commons.httpclient.HttpConnection$WrappedOutputStream.writ
e(HttpConnection.java:1275)
        at java.io.BufferedOutputStream.flushBuffer(Unknown Source)
        at java.io.BufferedOutputStream.flush(Unknown Source)
        at java.io.FilterOutputStream.close(Unknown Source)
        at org.apache.commons.httpclient.HttpConnection.closeSocketAndStreams(Ht
tpConnection.java:1083)
        at org.apache.commons.httpclient.HttpConnection.close(HttpConnection.jav
a:1024)
        at org.apache.commons.httpclient.HttpConnection$WrappedOutputStream.hand
leException(HttpConnection.java:1235)
        at org.apache.commons.httpclient.HttpConnection$WrappedOutputStream.writ
e(HttpConnection.java:1275)
        at java.io.BufferedOutputStream.flushBuffer(Unknown Source)
        at java.io.BufferedOutputStream.flush(Unknown Source)
        at java.io.FilterOutputStream.close(Unknown Source)
        at org.apache.commons.httpclient.HttpConnection.closeSocketAndStreams(Ht
tpConnection.java:1083)
        at org.apache.commons.httpclient.HttpConnection.close(HttpConnection.jav
a:1024)
        at org.apache.commons.httpclient.HttpConnection$WrappedOutputStream.hand
leException(HttpConnection.java:1235)
        at org.apache.commons.httpclient.HttpConnection$WrappedOutputStream.writ
e(HttpConnection.java:1275)"
1,"Contrib RMI: NotSerializableException. org.apache.jackrabbit.rmi.client.RemoteRepositoryException:

error unmarshalling return; nested exception is:.java.io.WriteAbortedException: writing aborted; java.io.NotSerializableException: javax.jcr.NameValue

"
1,"Prevent data inconsistencies due to incorrect or missed merges in the ItemStateManagers. There are two places where transient state changes are merged with persisted state: (i) in the SessionISM.stateModified call back method (typically called by a saving thread after it's changes have been saved) and (ii) in the SharedISM.Update.begin method. Sometimes the merge strategy fails in the sense that corrupt data is written to database. 

How to reproduce:
The attached test program contains steps to reproduce the issue. The testInconsistency1 method concurrently adds a node D to a node A and moves child node B of A to another place. This sometimes results in a situation in which node A still has a child reference to node B whereas B has another parent. In this situation, Jackrabbit cannot be started anymore if the search index is missing: 

Jun 8, 2009 2:16:23 PM org.apache.jackrabbit.core.query.OnWorkspaceInconsistency$1 handleMissingChildNode
SEVERE: Node /A (a7a29e8c-8d13-4fbd-b0ca-4f93f9c0ef42) has missing child 'B' (80aa13c5-1db6-4f62-b576-5e7f626d90c1)
Jun 8, 2009 2:16:23 PM org.apache.jackrabbit.core.RepositoryImpl initStartupWorkspaces
SEVERE: Failed to initialize workspace 'wm9'

The testInconsistency2 method concurrently adds a reference property to a node B (the threads do exactly the same). This sometimes results in a situation in which the referenced node can never be removed anymore because there is a ""ghost"" reference to it which cannot be removed. (It gives a ReferentialIntegrityException).

Jun 8, 2009 2:19:51 PM org.apache.jackrabbit.core.state.MLRUItemStateCache cache
WARNING: overwriting cached entry bc28ff87-216d-4ccd-bd73-03e7499ab54e/{}ref to B
Exception in thread ""main"" javax.jcr.ReferentialIntegrityException: 9f025634-d3e1-448e-904c-1c285f6b1bf6: the node cannot be removed because it is still being referenced.

It seems the first problem (the parent-child relation) is caused by an incorrect merge in the NodeStateMerger class. The second problem might also be caused by an incorrect or missed merge, but I am not sure whether that's the real problem.

"
1,"NodeReferencesId.equals() is not symetric. NodeReferencesId.equals() is not symetric when equality is tested against a NodeId.

Code example:
UUID uuid = UUID.randomUUID();
NodeId id = new NodeId(uuid);
NodeReferencesId refId = new NodeReferencesId(uuid);
id.equals(refId); // will return true
refId.equals(id); // will return false

NodeReferencesId should be decouled from the ItemId hierarchy. The class NodeReferences already does not extend from NodeState which makes perfectly sense. So, the same should apply to the identifier of NodeReferences.

The attached patch to NodeReferencesId also requires minor changes to some of the persistence managers."
1,"IndexReader overwrites future commits when you open it on a past commit. Hit this on trying to build up a test index for perf testing...

IndexReader (and Writer) accept an IndexCommit on open.

This is quite powerful, because, if you use a deletion policy that keeps multiple commits around, you can open a not-current commit, make some changes, write a new commit, all without altering the ""future"" commits.

I use this to first build up a big wikipedia index, including one commit w/ multiple segments, then another commit after optimize(), and then I open an writable IR to perform deletions off of both those commits.  This gives me a single test index that has all four combinations (single vs multi segment; deletions vs no deletions).

But IndexReader has a bug whereby it overwrites the segments_N file.  (IndexWriter works correctly)."
1,"intermittent deadlock in TestAtomicUpdate,TestIndexWriterExceptions. While backporting issues for 2.9.x/3.0.x release I hit deadlocks in these two tests, under both test-core and test-tag."
1,"ConcurrentModificationException thrown in MultiThreaded code. Now seeing this error.  This is with default cookie settings.  Happening rarely, however the web sites we're talking to do not use cookies very much.


java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
        at java.util.AbstractList$Itr.next(AbstractList.java:343)
        at java.util.Collections$UnmodifiableCollection$1.next(Collections.java:1010)
        at org.apache.http.client.protocol.RequestAddCookies.process(RequestAddCookies.java:152)
        at org.apache.http.protocol.BasicHttpProcessor.process(BasicHttpProcessor.java:290)
        at org.apache.http.protocol.HttpRequestExecutor.preProcess(HttpRequestExecutor.java:160)
        at org.apache.http.impl.client.DefaultClientRequestDirector.execute(DefaultClientRequestDirector.java:355)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:501)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:456)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:422)
        at com.hi5.os.Hi5RemoteContentFetcher.fetch(Hi5RemoteContentFetcher.java:279)"
1,"ArrayStoreException while reregistering existing node types. 
class: NodeTypeManagerImpl
method: public NodeType[] registerNodeTypes(InputStream in, String contentType, boolean reregisterExisting)

                ...
                return (NodeType[]) nodeTypes.toArray(new NodeTypeDef[nodeTypes.size()]);
                ...

=> should be (I suppose !)

                return (NodeType[]) nodeTypes.toArray(new NodeType[nodeTypes.size()]);
"
1,"ConnectionTimeoutException doesn't releaseConnection(). When a ConnectionTimeoutException is thrown, HttpConnection doesn't seem to
release the connection. Instead, the connection is properly released if an
InterruptedIOException is thrown.

This is the pattern I use:

Try {
     method.execute(...);
     method.getResponseBodyAsString();
 } catch (ConnectionTimeoutException cte) {
     ...
 } catch (InterruptedIOException ioe) {
     ...
 } finally {
     method.releaseConnection();
     LOG.info(""RELEASED"");   
 }

The following log shows that no actual release is performed, while the message
""RELEASED"" is logged.

10544  DEBUG [MainCheck2] httpclient.HttpConnection - enter
HttpConnection.isResponseAvailable(int)
10930  WARN  [MainCheck1] httpclient.HttpConnection - The host
www.pccomputing.com:80 (or proxy null:-1) did not accept the connection within
timeout of 3000 milliseconds
10931  WARN  [MainCheck1] CheckPerformer - Connection Timeout occurred..
org.apache.commons.httpclient.HttpConnection$ConnectionTimeoutException
at org.apache.commons.httpclient.HttpConnection.open(HttpConnection.java:659) 
...
at PersistenceCheck$MainCheck.run(PersistenceCheck.java:306)
10932  INFO  [MainCheck1] CheckPerformer - RELEASED

->Here no call to HttpConnection.releaseConnection() is performed. 

Thanks"
1,"Creating a node of type nt:hierarchyNode (or derived) on a JCR 1.0 compliant repository fails. When creating a node of type nt:hierarchyNode (or derived) on a JCR 1.0 compliant repository, the auto-created property named jcr:created does not get a default value and an exception:

javax.jcr.RepositoryException: createFromDefinition not implemented for: {http://www.jcp.org/jcr/1.0}created

The code in SessionItemStateManager#computeSystemGeneratedPropertyValues handles jcr:created only when found in node type mix:created. In JCR 1.0, however, this property was declared in nt:hierarchyNode. Adding this extra case would allow interoperation with such a repository.



"
1,"Custom LoginModule configurations broken in 1.5.0. Upgrading Jackrabbit from 1.4.5 to 1.5 has created an LDAP exception.  The configuration file which has not changed (except for the adding the new SimpleSecurityManager as required) is the default with the following substituted for the LoginModule:

        <LoginModule class=""com.sun.security.auth.module.LdapLoginModule"">
            <param name=""userProvider"" value=""ldap://localhost/ou=people,dc=example,dc=com"" />
            <param name=""userFilter"" value=""(&amp;(uid={USERNAME})(objectClass=inetOrgPerson))"" />
            <param name=""authzIdentity"" value=""{USERNAME}"" />
            <param name=""debug"" value=""true"" />
        </LoginModule>

This configuration worked correctly and I was able to authenticate properly with Jackrabbit 1.4.5
The same configuration with 1.5 throws the following exception:

javax.jcr.LoginException: com.sun.security.auth.module.LdapLoginModule does not support 'userProvider: com.sun.security.auth.module.LdapLoginModule does not support 'userProvider: com.sun.security.auth.module.LdapLoginModule does not support 'userProvider
        at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1414)
        at org.apache.jackrabbit.jca.JCAManagedConnectionFactory.openSession(JCAManagedConnectionFactory.java:140)
        at org.apache.jackrabbit.jca.JCAManagedConnectionFactory.createManagedConnection(JCAManagedConnectionFactory.java:176)
        at org.apache.jackrabbit.jca.JCAManagedConnectionFactory.createManagedConnection(JCAManagedConnectionFactory.java:168)
        at com.sun.enterprise.resource.ConnectorAllocator.createResource(ConnectorAllocator.java:136)
        at com.sun.enterprise.resource.AbstractResourcePool.createSingleResource(AbstractResourcePool.java:891)
        at com.sun.enterprise.resource.AbstractResourcePool.createResourceAndAddToPool(AbstractResourcePool.java:1752)
        at com.sun.enterprise.resource.AbstractResourcePool.createResources(AbstractResourcePool.java:917)
        at com.sun.enterprise.resource.AbstractResourcePool.initPool(AbstractResourcePool.java:225)
        at com.sun.enterprise.resource.AbstractResourcePool.internalGetResource(AbstractResourcePool.java:516)
        at com.sun.enterprise.resource.AbstractResourcePool.getResource(AbstractResourcePool.java:443)
        at com.sun.enterprise.resource.PoolManagerImpl.getResourceFromPool(PoolManagerImpl.java:248)
        at com.sun.enterprise.resource.PoolManagerImpl.getResource(PoolManagerImpl.java:176)
        at com.sun.enterprise.connectors.ConnectionManagerImpl.internalGetConnection(ConnectionManagerImpl.java:337)
        at com.sun.enterprise.connectors.ConnectionManagerImpl.allocateConnection(ConnectionManagerImpl.java:189)
        at com.sun.enterprise.connectors.ConnectionManagerImpl.allocateConnection(ConnectionManagerImpl.java:165)
        at com.sun.enterprise.connectors.ConnectionManagerImpl.allocateConnection(ConnectionManagerImpl.java:158)
        at org.apache.jackrabbit.jca.JCARepositoryHandle.login(JCARepositoryHandle.java:98)
        at org.apache.jackrabbit.jca.JCARepositoryHandle.login(JCARepositoryHandle.java:89)
        at org.apache.jackrabbit.jca.JCARepositoryHandle.login(JCARepositoryHandle.java:73)
        at com.threesl.Sapphire.CradleJCR.login(CradleJCR.java:44)   

 try {
            InitialContext ctx = new InitialContext();
            repository = (Repository) ctx.lookup(""jcr/repository"");
            session = repository.login(credentials);
        } catch (Exception e) {

        at com.threesl.Sapphire.CradleWS.doLogin(CradleWS.java:68)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jersey.impl.model.method.dispatch.EntityParamDispatchProvider$TypeOutInvoker._dispatch(EntityParamDispatchProvider.java:136)
        at com.sun.jersey.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:85)
        at com.sun.jersey.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:123)
        at com.sun.jersey.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:111)
        at com.sun.jersey.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:71)
        at com.sun.jersey.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:111)
        at com.sun.jersey.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:63)
        at com.sun.jersey.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:722)
        at com.sun.jersey.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:692)
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:344)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:831)
        at org.apache.catalina.core.ApplicationFilterChain.servletService(ApplicationFilterChain.java:411)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:290)
        at org.apache.catalina.core.StandardContextValve.invokeInternal(StandardContextValve.java:271)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:202)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:632)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:577)
        at com.sun.enterprise.web.WebPipeline.invoke(WebPipeline.java:94)
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:206)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:632)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:577)
        at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:571)
        at org.apache.catalina.core.ContainerBase.invoke(ContainerBase.java:1080)
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:150)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:632)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:577)
        at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:571)
        at org.apache.catalina.core.ContainerBase.invoke(ContainerBase.java:1080)
        at org.apache.coyote.tomcat5.CoyoteAdapter.service(CoyoteAdapter.java:272)
        at com.sun.enterprise.web.connector.grizzly.DefaultProcessorTask.invokeAdapter(DefaultProcessorTask.java:637)
        at com.sun.enterprise.web.connector.grizzly.DefaultProcessorTask.doProcess(DefaultProcessorTask.java:568)
        at com.sun.enterprise.web.connector.grizzly.DefaultProcessorTask.process(DefaultProcessorTask.java:813)
        at com.sun.enterprise.web.connector.grizzly.DefaultReadTask.executeProcessorTask(DefaultReadTask.java:341)
        at com.sun.enterprise.web.connector.grizzly.DefaultReadTask.doTask(DefaultReadTask.java:263)
        at com.sun.enterprise.web.connector.grizzly.DefaultReadTask.doTask(DefaultReadTask.java:214)
        at com.sun.enterprise.web.connector.grizzly.TaskBase.run(TaskBase.java:265)
        at com.sun.enterprise.web.connector.grizzly.ssl.SSLWorkerThread.run(SSLWorkerThread.java:106)
Caused by: javax.security.auth.login.LoginException: com.sun.security.auth.module.LdapLoginModule does not support 'userProvider
        at org.apache.jackrabbit.core.security.authentication.LocalAuthContext.login(LocalAuthContext.java:68)
        at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1407)
        ... 62 more
javax.security.auth.login.LoginException: com.sun.security.auth.module.LdapLoginModule does not support 'userProvider
        at org.apache.jackrabbit.core.security.authentication.LocalAuthContext.login(LocalAuthContext.java:68)
        at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1407)
        at org.apache.jackrabbit.jca.JCAManagedConnectionFactory.openSession(JCAManagedConnectionFactory.java:140)
        at org.apache.jackrabbit.jca.JCAManagedConnectionFactory.createManagedConnection(JCAManagedConnectionFactory.java:176)
        at org.apache.jackrabbit.jca.JCAManagedConnectionFactory.createManagedConnection(JCAManagedConnectionFactory.java:168)
        at com.sun.enterprise.resource.ConnectorAllocator.createResource(ConnectorAllocator.java:136)
        at com.sun.enterprise.resource.AbstractResourcePool.createSingleResource(AbstractResourcePool.java:891)
        at com.sun.enterprise.resource.AbstractResourcePool.createResourceAndAddToPool(AbstractResourcePool.java:1752)
        at com.sun.enterprise.resource.AbstractResourcePool.createResources(AbstractResourcePool.java:917)
        at com.sun.enterprise.resource.AbstractResourcePool.initPool(AbstractResourcePool.java:225)
        at com.sun.enterprise.resource.AbstractResourcePool.internalGetResource(AbstractResourcePool.java:516)
        at com.sun.enterprise.resource.AbstractResourcePool.getResource(AbstractResourcePool.java:443)
        at com.sun.enterprise.resource.PoolManagerImpl.getResourceFromPool(PoolManagerImpl.java:248)
        at com.sun.enterprise.resource.PoolManagerImpl.getResource(PoolManagerImpl.java:176)
        at com.sun.enterprise.connectors.ConnectionManagerImpl.internalGetConnection(ConnectionManagerImpl.java:337)
        at com.sun.enterprise.connectors.ConnectionManagerImpl.allocateConnection(ConnectionManagerImpl.java:189)
        at com.sun.enterprise.connectors.ConnectionManagerImpl.allocateConnection(ConnectionManagerImpl.java:165)
        at com.sun.enterprise.connectors.ConnectionManagerImpl.allocateConnection(ConnectionManagerImpl.java:158)
        at org.apache.jackrabbit.jca.JCARepositoryHandle.login(JCARepositoryHandle.java:98)
        at org.apache.jackrabbit.jca.JCARepositoryHandle.login(JCARepositoryHandle.java:89)
        at org.apache.jackrabbit.jca.JCARepositoryHandle.login(JCARepositoryHandle.java:73)
        at com.threesl.Sapphire.CradleJCR.login(CradleJCR.java:44)
        at com.threesl.Sapphire.CradleWS.doLogin(CradleWS.java:68)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jersey.impl.model.method.dispatch.EntityParamDispatchProvider$TypeOutInvoker._dispatch(EntityParamDispatchProvider.java:136)
        at com.sun.jersey.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:85)
        at com.sun.jersey.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:123)
        at com.sun.jersey.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:111)
        at com.sun.jersey.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:71)
        at com.sun.jersey.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:111)
        at com.sun.jersey.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:63)
        at com.sun.jersey.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:722)
        at com.sun.jersey.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:692)
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:344)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:831)
        at org.apache.catalina.core.ApplicationFilterChain.servletService(ApplicationFilterChain.java:411)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:290)
        at org.apache.catalina.core.StandardContextValve.invokeInternal(StandardContextValve.java:271)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:202)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:632)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:577)
        at com.sun.enterprise.web.WebPipeline.invoke(WebPipeline.java:94)
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:206)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:632)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:577)
        at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:571)
        at org.apache.catalina.core.ContainerBase.invoke(ContainerBase.java:1080)
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:150)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:632)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:577)
        at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:571)
        at org.apache.catalina.core.ContainerBase.invoke(ContainerBase.java:1080)
        at org.apache.coyote.tomcat5.CoyoteAdapter.service(CoyoteAdapter.java:272)
        at com.sun.enterprise.web.connector.grizzly.DefaultProcessorTask.invokeAdapter(DefaultProcessorTask.java:637)
        at com.sun.enterprise.web.connector.grizzly.DefaultProcessorTask.doProcess(DefaultProcessorTask.java:568)
        at com.sun.enterprise.web.connector.grizzly.DefaultProcessorTask.process(DefaultProcessorTask.java:813)
        at com.sun.enterprise.web.connector.grizzly.DefaultReadTask.executeProcessorTask(DefaultReadTask.java:341)
        at com.sun.enterprise.web.connector.grizzly.DefaultReadTask.doTask(DefaultReadTask.java:263)
        at com.sun.enterprise.web.connector.grizzly.DefaultReadTask.doTask(DefaultReadTask.java:214)
        at com.sun.enterprise.web.connector.grizzly.TaskBase.run(TaskBase.java:265)
        at com.sun.enterprise.web.connector.grizzly.ssl.SSLWorkerThread.run(SSLWorkerThread.java:106)
RAR5117 : Failed to obtain/create connection from connection pool [ jackrabbit-connection-pool ]. Reason : Failed to create session: com.sun.security.auth.module.LdapLoginModule does not support 'userProvider: com.sun.security.auth.module.LdapLoginModule does not support 'userProvider

"
1,"Passing a null fieldname to MemoryFields#terms in MemoryIndex throws a NPE. I found this when querying a MemoryIndex using a RegexpQuery wrapped by a SpanMultiTermQueryWrapper.  If the regexp doesn't match anything in the index, it gets rewritten to an empty SpanOrQuery with a null field value, which then triggers the NPE."
1,"Item.isNew() does not work correctly within a transaction. javadoc on Item.isNew() states:

     * Returns <code>true</code> if this is a new item, meaning that it exists only in transient
     * storage on the <code>Session</code> and has not yet been saved. Within a transaction,
     * <code>isNew</code> on an <code>Item</code> may return <code>false</code> (because the item
     * has been saved) even if that <code>Item</code> is not in persistent storage (because the
     * transaction has not yet been committed).

but currently, Item.isNew() returns ""true"" after beeing saved in a transaction."
1,"Intermittent failure in TestFieldCacheTermsFilter.testMissingTerms. Running tests in while(1) I hit this:

{noformat}

NOTE: reproduce with: ant test -Dtestcase=TestFieldCacheTermsFilter -Dtestmethod=testMissingTerms -Dtests.seed=-1046382732738729184:5855929314778232889

1) testMissingTerms(org.apache.lucene.search.TestFieldCacheTermsFilter)
java.lang.AssertionError: Must match 1 expected:<1> but was:<0>
	at org.junit.Assert.fail(Assert.java:91)
	at org.junit.Assert.failNotEquals(Assert.java:645)
	at org.junit.Assert.assertEquals(Assert.java:126)
	at org.junit.Assert.assertEquals(Assert.java:470)
	at org.apache.lucene.search.TestFieldCacheTermsFilter.testMissingTerms(TestFieldCacheTermsFilter.java:63)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1214)
	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1146)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:24)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:157)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:136)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at org.junit.runner.JUnitCore.runMain(JUnitCore.java:98)
	at org.junit.runner.JUnitCore.runMainAndExit(JUnitCore.java:53)
	at org.junit.runner.JUnitCore.main(JUnitCore.java:45)
{noformat}

Unfortunately the seed doesn't [consistently] repro for me..."
1,"MultiPhraseQuery sums its own idf instead of Similarity.. MultiPhraseQuery is a generalized version of PhraseQuery, and computes IDF the same way by default (by summing across the terms).

The problem is it doesn't let the Similarity do this: PhraseQuery calls Similarity.idfExplain(Collection<Term> terms, IndexSearcher searcher),
but MultiPhraseQuery just sums itself, calling Similarity.idf(int, int) for each term.

"
1,"Parsing built-in CND and XML nodetypes does not result in equal nt-definitions. i created a test in order to make sure builtin-nodetypes.xml and builtin-nodetypes.cnd provide the same definitions (actually i only wanted to test my own changes).

it reveals that the existing built-in NodeTypeDefinitions are not equal due to the following reason:

- in the xml-format nt:base is always specified if no other super type extends from nt:base
- in the cnd notation the nt:base is omitted (see below for quote from appendix of jsr 283) even if other super type(s) are
  defined and none of them extends from nt:base.

this affects the following nodetypes (all extending from mix:referenceable only):

nt:versionHistory
nt:version
nt:frozenNode
nt:resource


quote from public-review of jsr 283:

""7.2.2.4 Supertypes [...]
After the node type name comes the optional list of supertypes. If this element is not present and the node type is not a mixin (see 7.2.2.5 Options), then a supertype of nt:base is assumed.""


I'm not totally sure, if according to the quote above the built-in cnd-definitions are valid at all. since it states, that the nt:base is assumed if no other super type is defined. In the case of the node types above, mix:referenceable is defined to be the only super type, which is not totally true... the non-mixin types are always sub types of nt:base.

In either case: From my understanding the node types resulting from parsing the xml and the cnd file should be equal.
If the definitions are valid, we may need to adjust the CompactNodeTypeDefReader.




"
1,"SegmentInfo should explicitly track whether that segment wrote term vectors. Today SegmentInfo doesn't know if it has vectors, which means its files() method must check if the files exist.

This leads to subtle bugs, because Si.files() caches the files but then we fail to invalidate that later when the term vectors files are created.

It also leads to sloppy code, eg TermVectorsReader ""gracefully"" handles being opened when the files do not exist.  I don't like that; it should only be opened if they exist.

This also fixes these intermittent failures we've been seeing:

{noformat}
junit.framework.AssertionFailedError: IndexFileDeleter doesn't know about file _1e.tvx
       at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:979)
       at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:917)
       at org.apache.lucene.index.IndexWriter.filesExist(IndexWriter.java:3633)
       at org.apache.lucene.index.IndexWriter.startCommit(IndexWriter.java:3699)
       at org.apache.lucene.index.IndexWriter.prepareCommit(IndexWriter.java:2407)
       at org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:2478)
       at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2460)
       at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2444)
       at org.apache.lucene.index.TestIndexWriterExceptions.testRandomExceptionsThreads(TestIndexWriterExceptions.java:213)
{noformat}"
1,"A failure to connect to a MySQL database when JackRabbit starts a session leaves a .lock file in the repository. Subsequent sessions cannot be created by the same thread.. I investigating the robustness of JackRabbit in the face of unexpected database errors, such as the database being unavailable. In my particular case, I am attempting to start a JackRabbit session using a TransientRepository while the database is not yet running. This correctly fails. However, if I attempt to create another session within the same thread after a short while, an exception occurs saying that the repository has already been locked. I would expect the repository folder not to be locked. Maybe the code meant to remove the .lock file was not triggered because of an uncaught exception.

Please see the attached files:
-a test class to reproduce the problem
-my repository.xml config
-the log file quantel.txt with details about the stack trace.
"
1,"coord should still apply to missing terms/clauses. Missing terms in a boolean query ""disappear"" (i.e. they don't even affect the coord factor)."
1,"Config incorrectly handles Windows absolute pathnames. I have no idea how no one ran into this so far, but I tried to execute an .alg file which used ReutersContentSource and referenced both docs.dir and work.dir as Windows absolute pathnames (e.g. d:\something). Surprisingly, the run reported an error of missing content under benchmark\work\something.

I've traced the problem back to Config, where get(String, String) includes the following code:
{code}
    if (sval.indexOf("":"") < 0) {
      return sval;
    }
    // first time this prop is extracted by round
    int k = sval.indexOf("":"");
    String colName = sval.substring(0, k);
    sval = sval.substring(k + 1);
    ...
{code}

It detects "":"" in the value and so it thinks it's a per-round property, thus stripping ""d:"" from the value ... fix is very simple:
{code}
    if (sval.indexOf("":"") < 0) {
      return sval;
    } else if (sval.indexOf("":\\"") >= 0) {
      // this previously messed up absolute path names on Windows. Assuming
      // there is no real value that starts with \\
      return sval;
    }
    // first time this prop is extracted by round
    int k = sval.indexOf("":"");
    String colName = sval.substring(0, k);
    sval = sval.substring(k + 1);
{code}

I'll post a patch w/ the above fix + test shortly."
1,"support stores where binary properties are mandatory (such as in nt:resource). SetValueBinaryTest tries to remove binary properties by setting them to null. However, some stores only support binary properties in the case of jcr:content/jcr:data, in which case the property can not be removed directly.

Suggestion: check for mandatory/protected, and throw NotExecutableException in that case.
"
1,"Invalid journal records during XATransactions. During the prepare phase of a XATransaction, XAItemStateManager.prepare calls ShareItemStageManager.beginUpdate that, in case of a ClusterNode, calls ClusterNode.updatePrepared that does a ChangeLogRecord.write().

This last method is located in ClusterRecord and systematically write the begin and the end of the journal record.

As a consequence, useless corrupted records are written in the journal everytime a transaction ends without jackrabbit update! This causes polution of the journal, as other cluster nodes try to sync with the corrupted updates and fail doing so as ClusterRecordDeserializer can't deserialize it (the record identifier is empty).

ChangeLogRecord (and even other ClusterRecord implementations too) should only write if there's effective updates.

I propose the following solution:
*) add the following method in Changelog so clients can know if there's effective updates:
    public boolean hasUpdates() {
    	return !(addedStates.isEmpty() && modifiedStates.isEmpty() && deletedStates.isEmpty() && modifiedRefs.isEmpty());
    }

*) change ClusterRecord with:
    public final void write() throws JournalException {
    	
    	if (hasUpdates()) {
	        record.writeString(workspace);
	        doWrite();
	        record.writeChar(END_MARKER);
    	}
    }
    
    protected abstract boolean hasUpdates();

*) implement hasUpdates for every ClusterRecord implementation:
 ----> ChangeLogRecord:
    protected boolean hasUpdates() {
    	return changes.hasUpdates() || !events.isEmpty();
    }
 ----> LockRecord and NamespaceRecord:
    protected boolean hasUpdates() {
    	return true;
    }

 ----> NodeTypeRecord:
    protected boolean hasUpdates() {
    	return !collection.isEmpty();
    }

Best regards,

Stephane Landelle"
1,"RAMInputStream hits false EOF if you seek to EOF then seek back then readBytes. TestLazyLoadThreadSafety fails in hudson, possibly an issue with RAMDirectory.
If you hack lucene testcase to return another directory, the same seed will pass."
1,"TestStressIndexing has intermittent failures. See http://www.gossamer-threads.com/lists/lucene/java-dev/55092 copied below:

 OK, I have seen this twice in the last two days:
Testsuite: org.apache.lucene.index.TestStressIndexing
[junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 18.58
sec
[junit]
[junit] ------------- Standard Output ---------------
[junit] java.lang.NullPointerException
[junit] at
org.apache.lucene.store.RAMInputStream.readByte(RAMInputStream.java:67)
[junit] at
org.apache.lucene.store.IndexInput.readInt(IndexInput.java:66)
[junit] at org.apache.lucene.index.SegmentInfos
$FindSegmentsFile.run(SegmentInfos.java:544)
[junit] at
org
.apache
.lucene.index.DirectoryIndexReader.open(DirectoryIndexReader.java:63)
[junit] at
org.apache.lucene.index.IndexReader.open(IndexReader.java:209)
[junit] at
org.apache.lucene.index.IndexReader.open(IndexReader.java:192)
[junit] at
org.apache.lucene.search.IndexSearcher.<init>(IndexSearcher.java:56)
[junit] at org.apache.lucene.index.TestStressIndexing
$SearcherThread.doWork(TestStressIndexing.java:111)
[junit] at org.apache.lucene.index.TestStressIndexing
$TimedThread.run(TestStressIndexing.java:55)
[junit] ------------- ---------------- ---------------
[junit] Testcase:
testStressIndexAndSearching
(org.apache.lucene.index.TestStressIndexing): FAILED
[junit] hit unexpected exception in search1
[junit] junit.framework.AssertionFailedError: hit unexpected
exception in search1
[junit] at
org
.apache
.lucene.index.TestStressIndexing.runStressTest(TestStressIndexing.java:
159)
[junit] at
org
.apache
.lucene
.index
.TestStressIndexing
.testStressIndexAndSearching(TestStressIndexing.java:187)
[junit]
[junit]
[junit] Test org.apache.lucene.index.TestStressIndexing FAILED

Subsequent runs have, however passed. Has anyone else hit this on
trunk?

I am running using ""ant clean test""

I'm on a Mac Pro 4 core, 4GB machine, if that helps at all. Not sure
how to reproduce at this point, but strikes me as a threading issue.
Oh joy!

I'll try to investigate more tomorrow to see if I can dream up a test
case.

-Grant 

"
1,"NodeTypeDef depends on supertype ordering. Currently the NodeTypeDef.setSupertypes() method simply sets the given QName array as the supertype QName array of the defined node type, thus preserving whatever ordering a node type parser or ultimately a node type definition file uses. This causes problems for example in the equals() method that uses the order-sensitive Arrays.equals() method to check for equality of the supertype QName arrays. The current implementation does therefore not consider the node type definitions ""A > B, C"" and ""A > C, B"" as equal even though they really should be so considered.

The same problem affects also child node and property definitions. The proper fix for this issue would probably be to use Sets to store and handle this information."
1,"CorruptIndexException on indexing after a failure occurs after segments file creation but before any bytes are written. FSDirectory.createOutput(..) uses a RandomAccessFile to do its work.  On my system the default FSDirectory.open(..) creates an NIOFSDirectory.  If createOutput is called on a segments_* file and a crash occurs between RandomAccessFile creation (file system shows a segments_* file exists but has zero bytes) but before any bytes are written to the file, subsequent IndexWriters cannot proceed.  The difficulty is that it does not know how to clear the empty segments_* file.  None of the file deletions will happen on such a segment file because the opening bytes cannot not be read to determine format and version.

An initial proposed patch file is attached below.

"
1,"session.exportDocumentView() does not work with jaxb 2.1.x  UnmarshallerHandler. I tried to update my project from Jackrabbit 1.4 to 1.5 and found following error, that is critical for my app.
Project uses Import/Export features of JCR and JAXB to map XML from JCR to java objects.

exportDocumentView() works with streams when I call it like this:

              Unmarshaller umr = getUnmarshaller();
        ...
                fo = new FileOutputStream(""/tmp/export-node.xml"");
                jcrs.exportDocumentView(path,fo , false, false);
                fi = new FileInputStream(""/tmp/export-node.xml"");
                umr.unmarshal(new InputSource(fi));    

But it does not work when I call it using SAX event handler:

            UnmarshallerHandler ctxh = umr.getUnmarshallerHandler();
             jcrs.exportDocumentView(path, ctxh, false, false);

giving following exception:

java.lang.NullPointerException
        at org.xml.sax.helpers.AttributesImpl.getIndex(AttributesImpl.java:203)
        at com.sun.xml.bind.v2.runtime.unmarshaller.InterningXmlVisitor$AttributesImpl.getIndex(InterningXmlVisitor.java:112)
        at com.sun.xml.bind.v2.runtime.unmarshaller.XsiNilLoader.selectLoader(XsiNilLoader.java:62)
        at com.sun.xml.bind.v2.runtime.unmarshaller.ProxyLoader.startElement(ProxyLoader.java:53)
        at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallingContext._startElement(UnmarshallingContext.java:449)
        at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallingContext.startElement(UnmarshallingContext.java:427)
        at com.sun.xml.bind.v2.runtime.unmarshaller.InterningXmlVisitor.startElement(InterningXmlVisitor.java:71)
        at com.sun.xml.bind.v2.runtime.unmarshaller.SAXConnector.startElement(SAXConnector.java:137)
        at org.apache.jackrabbit.commons.xml.Exporter.startElement(Exporter.java:438)
        at org.apache.jackrabbit.commons.xml.DocumentViewExporter.exportNode(DocumentViewExporter.java:76)
        at org.apache.jackrabbit.commons.xml.Exporter.exportNode(Exporter.java:298)
        at org.apache.jackrabbit.commons.xml.Exporter.exportNodes(Exporter.java:214)
        at org.apache.jackrabbit.commons.xml.DocumentViewExporter.exportNode(DocumentViewExporter.java:77)
        at org.apache.jackrabbit.commons.xml.Exporter.exportNode(Exporter.java:298)
        at org.apache.jackrabbit.commons.xml.Exporter.exportNodes(Exporter.java:214)
        at org.apache.jackrabbit.commons.xml.DocumentViewExporter.exportNode(DocumentViewExporter.java:77)
        at org.apache.jackrabbit.commons.xml.Exporter.exportNode(Exporter.java:298)
        at org.apache.jackrabbit.commons.xml.Exporter.export(Exporter.java:144)
        at org.apache.jackrabbit.commons.AbstractSession.export(AbstractSession.java:461)
        at org.apache.jackrabbit.commons.AbstractSession.exportDocumentView(AbstractSession.java:241)
        at ua.org.dg.semaril.helpers.AbstractTypeResolver.getContent(AbstractTypeResolver.java:31

Version 1.4. works fine.

Jukka, please check your changes to  org.apache.jackrabbit.commons.xml.Exporter.
"
1,"NPE Exception Thrown By AbstractJournal During Commit Operation. This seems related to JCR-712 (which was apparently fixed in 1.2.2), but I see the following error now-and-then on JR 1.2.2 (I'm using the DB based journal implementation with Oracle 10g):

java.lang.NullPointerException
        at org.apache.jackrabbit.core.cluster.AbstractJournal.commit(AbstractJournal.java:525)
        at org.apache.jackrabbit.core.cluster.ClusterNode.updateCommitted(ClusterNode.java:424)
        at org.apache.jackrabbit.core.cluster.ClusterNode$WorkspaceUpdateChannel.updateCommitted(ClusterNode.java:565)
        at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:712)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:808)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:326)
        at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:313)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:302)
        at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:308)
        at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1204)
        at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:823)
"
1,"EventFilter misses Events for same Nodetype. If an ObservationListener registers with a NodeType-filter, 
it only gets informed about events on Sub-NodeTypes of the ones specified in the filter but not on the NodeType itself.

Example:
========
ObservationManager om = wsp.getObservationManager();
om.addEventListener(listener, Event.PROPERTY_ADDED, ""/"", true, null, new String[]{""nt:unstructured""}, true);

would receive notifications on nodes of type ""rep:root"", which is based on ""nt:unstructured"" but not of ""nt:unstructured""


"
1,"TransientRepository with LocalFileSystem eventually causes Repository data to be stored at path '/'. I'm using a TransitoryRepository for my unit testing, with the repository's file system specified as:

    <FileSystem class=""org.apache.jackrabbit.core.fs.local.LocalFileSystem"">
        <param name=""path"" value=""${rep.home}/repository""/>
    </FileSystem>

I noticed today that when I run my unit tests Jackrabbit is creating four directories at the root of my hard drive: ""meta"", ""namespaces"", ""nodetypes"", and ""data"". I tracked the problem the fact that when a LocalFileSystem is closed, it sets the ""root"" to null - an invalid state. But when using a TransitoryRepository, the invalid state is never discovered because the LocalFileSystem object itself is not released, or re-initialized. It is simply used to create BasedFileSystem objects in RepositoryImpl. Calls to BasedFileSystem defer to the LocalFileSystem object that now has a null root. Inside the LocalFileSystem, all the calls to Java's io.File constructor have a ""null"" parent parameter, causing File to fall back to its single argument constructor which sees the path ""/meta"" and happily creates files at the root of the disk.

I'm not sure what the best solution is, but some thoughts I've had are:
- don't set the ""root"" property to null when closing a LocalFileSystem
- make RepositoryConfig re-init the FileSystem variable when it is accessed.
- don't cache the RepositoryConfig in TransitoryRepository (this might also require a new constructor that takes a class-path resource for the repository configuration file)"
1,"SQL2 query: QOMFormatter create incorrect NOT conditions. Then the following query is parsed:
SELECT test.* FROM test WHERE (NOT test.name = 'Hello') AND test.id = 3
then the SQL statement generated, it becomes:
SELECT test.* FROM test WHERE NOT test.name = 'Hello' AND test.id = 3
which is parsed differently and becomes:
SELECT test.* FROM test WHERE NOT (test.name = 'Hello' AND test.id = 3)"
1,"TestAddIndexes reproducible test failure on turnk. trunk: r1133385

{code}
    [junit] Testsuite: org.apache.lucene.index.TestAddIndexes
    [junit] Tests run: 2843, Failures: 1, Errors: 0, Time elapsed: 137.121 sec
    [junit]
    [junit] ------------- Standard Output ---------------
    [junit] java.io.FileNotFoundException: _cy.fdx
    [junit]     at org.apache.lucene.store.RAMDirectory.fileLength(RAMDirectory.java:121)
    [junit]     at org.apache.lucene.store.MockDirectoryWrapper.fileLength(MockDirectoryWrapper.java:606)
    [junit]     at org.apache.lucene.index.SegmentInfo.sizeInBytes(SegmentInfo.java:294)
    [junit]     at org.apache.lucene.index.TieredMergePolicy.size(TieredMergePolicy.java:633)
    [junit]     at org.apache.lucene.index.TieredMergePolicy.useCompoundFile(TieredMergePolicy.java:611)
    [junit]     at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:2459)
    [junit]     at org.apache.lucene.index.TestAddIndexes$CommitAndAddIndexes3.doBody(TestAddIndexes.java:847)
    [junit]     at org.apache.lucene.index.TestAddIndexes$RunAddIndexesThreads$1.run(TestAddIndexes.java:675)
    [junit] java.io.FileNotFoundException: _cx.fdx
    [junit]     at org.apache.lucene.store.RAMDirectory.fileLength(RAMDirectory.java:121)
    [junit]     at org.apache.lucene.store.MockDirectoryWrapper.fileLength(MockDirectoryWrapper.java:606)
    [junit]     at org.apache.lucene.index.SegmentInfo.sizeInBytes(SegmentInfo.java:294)
    [junit]     at org.apache.lucene.index.TieredMergePolicy.size(TieredMergePolicy.java:633)
    [junit]     at org.apache.lucene.index.TieredMergePolicy.useCompoundFile(TieredMergePolicy.java:611)
    [junit]     at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:2459)
    [junit]     at org.apache.lucene.index.TestAddIndexes$CommitAndAddIndexes3.doBody(TestAddIndexes.java:847)
    [junit]     at org.apache.lucene.index.TestAddIndexes$RunAddIndexesThreads$1.run(TestAddIndexes.java:675)
    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestAddIndexes -Dtestmethod=testAddIndexesWithRollback -Dtests.seed=9026722750295014952:2645762923088581043 -Dtests.multiplier=3
    [junit] NOTE: test params are: codec=RandomCodecProvider: {id=SimpleText, content=SimpleText, d=MockRandom, c=SimpleText}, locale=fr, timezone=Africa/Kigali
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestAddIndexes]
    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=68050392,total=446234624
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testAddIndexesWithRollback(org.apache.lucene.index.TestAddIndexes):       FAILED
    [junit]
    [junit] junit.framework.AssertionFailedError:
    [junit]     at org.apache.lucene.index.TestAddIndexes.testAddIndexesWithRollback(TestAddIndexes.java:932)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1362)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1280)
    [junit]
    [junit]
    [junit] Test org.apache.lucene.index.TestAddIndexes FAILED
{code}


Fails randomly in my while(1) test run, and Fails after a few min of running: 
{code}
ant test -Dtestcase=TestAddIndexes -Dtests.seed=9026722750295014952:2645762923088581043 -Dtests.multiplier=3 -Dtests.iter=200 -Dtests.iter.min=1
{code}"
1,"jcr2spi spi2dav getProperties returns only cached properties. I'm using JCR through webdav (contrib jcr2spi and spi2dav libraries).
Server is default jcr server (jackrabbit-webapp-1.3.1 on tomcat),
client is 2007/09/28 svn snapshot

I've noticed that Node.getProperty returns only cached properties. 

Sample test case can be found at http://kplab.tuke.sk/svn/kplab/ctm/trunk/test/org/kplab/tsf/CTMTest.java

Note that sometimes properties do get printed, so it may not be so straightforwarding to reproduce this bug.


Simple Example:

// I have nt:file node at kplab/jojo in my repository
...
Session session = repository.login();
Node root  = session.getRootNode();
Node node = root.getNode(""kplab/jojo"");
// node.getProperty(""jcr:content/jcr:data""); // force to load property
from server
dump(node); // simple dump method from (
http://jackrabbit.apache.org/doc/firststeps.html )


dump prints:
/kplab/jojo
/kplab/jojo/jcr:content

But when I uncomment the getProperty line above, it prints:
/kplab/jojo
/kplab/jojo/jcr:content
/kplab/jojo/jcr:content/jcr:lastModified = 2007-09-27T15:52:27.312+02:00
/kplab/jojo/jcr:content/jcr:uuid = 4421ed5a-6200-4918-864e-c58643bc8d4e
/kplab/jojo/jcr:content/jcr:mimeType = text/plain
/kplab/jojo/jcr:content/jcr:data = hura hura
/kplab/jojo/jcr:content/jcr:primaryType = nt:resource

--
Jozef Wagner"
1,"AccessManager asks for property (jcr:created) permissions before the actual creation of the object. 
When implementing a custom AccessManager for Jackrabbit v1.5+ there a bug when creating a new object.

I perform an addNode() and my own accessmanager the isGranted() method is override'd and performs the following code;

            ......
            String perm = null;
            NodeId     nodeId = mHierMgr.resolveNodePath( pPath );
            PropertyId propId = null;
            if (nodeId==null) {
              propId = mHierMgr.resolvePropertyPath( pPath );
     System.out.println(""path = "" + pPath.toString() );
              // **** TODO is this ok?... it happens when a new object is created and the accessmanager ask for read access on a property.
//              if (propId==null) return true;
              nodeId = propId.getParentId();
            }

this is the System.out.println result

path = {}        {}JeCARS        {}default        {}Groups        {}testGroup        {http://www.jcp.org/jcr/1.0}created


and the stacktrace


	at org.jecars.CARS_AccessManager.isGranted(CARS_AccessManager.java:844)
	at org.jecars.CARS_AccessManager.isGranted(CARS_AccessManager.java:806)
	at org.apache.jackrabbit.core.ItemManager.canRead(ItemManager.java:339)
	at org.apache.jackrabbit.core.ItemManager.canRead(ItemManager.java:326)
	at org.apache.jackrabbit.core.ItemManager.createItemData(ItemManager.java:696)
	at org.apache.jackrabbit.core.ItemManager.getItemData(ItemManager.java:291)
	at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:228)
	at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:493)
	at org.apache.jackrabbit.core.NodeImpl.createChildProperty(NodeImpl.java:479)
	at org.apache.jackrabbit.core.NodeImpl.createChildNode(NodeImpl.java:535)
	at org.apache.jackrabbit.core.NodeImpl.internalAddChildNode(NodeImpl.java:795)
	at org.apache.jackrabbit.core.NodeImpl.internalAddNode(NodeImpl.java:729)
	at org.apache.jackrabbit.core.NodeImpl.internalAddNode(NodeImpl.java:677)
	at org.apache.jackrabbit.core.NodeImpl.addNode(NodeImpl.java:2110)


It seems that READ permission for the jcr:created property is requested before the object is actually created


"
1,"'100-continue' response times out unexpectedly. Entity encosing methods time out (3 seconds) rather than getting the
100-continue response. Then, after it has send the body, the 100-continue
response is received and returned.

Adding

  method.setUseExpectHeader(false);

seems to fix it.

Platform 1: Jetty server on Windows XP, Sun JDK 1.4.1_01, 
Platform 2: Tomcat-4.1.18 + Turbine on Windows 2000 Pro, Sun JDK 1.3.1
Platform 3: Tomcat-4.1.18 on Linux if the connection is running over stunnel-4.00

Reported by: 
 Simon Roberts <simon.roberts@fifthweb.net>
 Aurelien Pernoud <apernoud@sopragroup.com>
 Ingo Brunberg <ib@fiz-chemie.de>"
1,"Persistence data of versioning not cleaned up correctly. when deleting a version or removing its label, the respective persistence data is not always properly removed."
1,Fix for NPE's in Spatial Lucene for searching bounding box only. NPE occurs when using DistanceQueryBuilder for minimal bounding box search without the distance filter.
1,"After RepositoryImpl instance has been created and shut down, some classes cannot be unloaded. I've built a simple web-application, which contains one servlet loaded at start-up. In its init() method an instance of RepositoryImpl() is created, in its destroy() method this instance is stopped (using shutdown()).
From the servlet code, only classes in jackrabbit-core, JCR API and Servlet API are referenced.
jackrabbit-core version is 1.4.5, and jackrabbit-jcr-commons version is 1.4.2. Other jackrabbit libs are all of 1.4 version.

Even if servlet's doGet() method never gets called, when the web-application is redeployed, all its classes still hang in memory, which produces a memory leak.

init() method is 

    public void init() throws ServletException {
        super.init();
        try {
            RepositoryConfig repoConfig = RepositoryConfig.create(getClass().getResourceAsStream(""repository.xml""), ""."");
            repo = RepositoryImpl.create(repoConfig);
        } catch (Exception e) {
            throw new ServletException(e);
        }
    }

while destroy() method is

    public void destroy() {
        repo.shutdown();
        super.destroy();
    }

Even when I applied patches from JCR-1636 and added TransientFileFactory.shutdown() call to destroy() method, nothing has changed.
Tested this in Jetty 6.1.9 and Tomcat 6.0.14."
1,"Redirection of a POST method. I execute a PostMethod to an URL which redirects me to a HTML page. If I set 
follow redirects to true the HttpClient wants to execute once more a POST. Of 
course a POST is not allowed to HTML pages. I think the HttpClient should 
exectue a GET method instead. That's also what is in the RFC2616:

10.3 Redirection 3xx

   This class of status code indicates that further action needs to be
   taken by the user agent in order to fulfill the request.  The action
   required MAY be carried out by the user agent without interaction
   with the user if and only if the method used in the second request is
   GET or HEAD. A client SHOULD detect infinite redirection loops, since
   such loops generate network traffic for each redirection.

      Note: previous versions of this specification recommended a
      maximum of five redirections. Content developers should be aware
      that there might be clients that implement such a fixed
      limitation."
1,SPI: Description of Path.isDescendantOf(Path) . Description of Path.isDescendantOf lists different reasons for IllegalArgumentException and RepositoryException than isAncestorOf... this is obviously a mistake.
1,"StringIndexOutOfBound exception in RFC2109 cookie validate when host name contains no domain information and is short in length than the cookie domain.. If the target server is identified by hostname only (no domain) and the domain
of the cookie is greater in length than the target hostname, a
StringIndexOutOfBoundsException occurs.

Offending line(s) of code: 174-176 in o.a.c.h.cookie.RFC2109Spec.java"
1,"incorrect HTML excerpt generation for queries on japanese text content . The generated excerpt highlights single characters instead of full words. Test case (to be added to FullTextQueryTest):

     public void testJapaneseAndHighlight() throws RepositoryException {
        // http://translate.google.com/#auto|en|%E3%82%B3%E3%83%B3%E3%83%86%E3%83%B3%E3%83%88
        String jContent = ""\u30b3\u30fe\u30c6\u30f3\u30c8"";
        // http://translate.google.com/#auto|en|%E3%83%86%E3%82%B9%E3%83%88
        String jTest = ""\u30c6\u30b9\u30c8"";
        
        String content = ""some text with japanese: "" + jContent
                + "" ('content')"" + "" and "" + jTest + "" ('test')."";

        // expected excerpt; note this may change if excerpt providers change
        String expectedExcerpt = ""<div><span>some text with japanese: "" + jContent
                + "" ('content') and <strong>"" + jTest
                + ""</strong> ('test').</span></div>"";
        
        Node n = testRootNode.addNode(""node1"");
        n.setProperty(""title"", content);
        testRootNode.getSession().save();
        
        String xpath = ""/jcr:root"" + testRoot + ""/element(*, nt:unstructured)""
                + ""[jcr:contains(., '"" + jTest + ""')]/rep:excerpt(.)"";
        Query q = superuser.getWorkspace().getQueryManager()
                .createQuery(xpath, Query.XPATH);
        
        QueryResult qr = q.execute();
        RowIterator it = qr.getRows();
        int cnt = 0;
        while (it.hasNext()) {
            cnt++;
            Row found = it.nextRow();
            assertEquals(n.getPath(), found.getPath());
            String excerpt = found.getValue(""rep:excerpt(.)"").getString();
            assertEquals(expectedExcerpt, excerpt);
        }
        
        assertEquals(1, cnt);
    }
"
1,"Deadlock in SharedItemStateManager on session.move and node.save. I have multiple threads in a test case for performing action on different workflow instances stored inside a Jackrabbit repository.
Most of the time the test case hangs because of the following deadlock:

""ActionTrigger38"" daemon prio=10 tid=0x9060ec00 nid=0x2f7e in Object.wait() [0x8f505000..0x8f505f20]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0xb3ef0208> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$ReaderLock)
        at java.lang.Object.wait(Object.java:485)
        at EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$ReaderLock.acquire(Unknown Source)
        - locked <0xb3ef0208> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$ReaderLock)
        at org.apache.jackrabbit.core.state.DefaultISMLocking$ReadLockImpl.<init>(DefaultISMLocking.java:84)
        at org.apache.jackrabbit.core.state.DefaultISMLocking$ReadLockImpl.<init>(DefaultISMLocking.java:78)
        at org.apache.jackrabbit.core.state.DefaultISMLocking.acquireReadLock(DefaultISMLocking.java:44)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.acquireReadLock(SharedItemStateManager.java:1409)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.hasItemState(SharedItemStateManager.java:286)
        at org.apache.jackrabbit.core.state.XAItemStateManager.hasItemState(XAItemStateManager.java:295)
        at org.apache.jackrabbit.core.state.SessionItemStateManager.hasItemState(SessionItemStateManager.java:206)
        at org.apache.jackrabbit.core.HierarchyManagerImpl.hasItemState(HierarchyManagerImpl.java:164)
        at org.apache.jackrabbit.core.CachingHierarchyManager.nodeAdded(CachingHierarchyManager.java:674)
        at org.apache.jackrabbit.core.CachingHierarchyManager.nodeAdded(CachingHierarchyManager.java:366)
        - locked <0xb4020630> (a java.lang.Object)
        at org.apache.jackrabbit.core.state.StateChangeDispatcher.notifyNodeAdded(StateChangeDispatcher.java:159)
        at org.apache.jackrabbit.core.state.SessionItemStateManager.nodeAdded(SessionItemStateManager.java:905)
        at org.apache.jackrabbit.core.state.NodeState.notifyNodeAdded(NodeState.java:852)
        at org.apache.jackrabbit.core.state.NodeState.renameChildNodeEntry(NodeState.java:370)
        - locked <0xb26df780> (a org.apache.jackrabbit.core.state.NodeState)
        at org.apache.jackrabbit.core.NodeImpl.renameChildNode(NodeImpl.java:559)
        at org.apache.jackrabbit.core.SessionImpl.move(SessionImpl.java:1034)
        at org.ametys.plugins.repository.DefaultSessionFactory$SessionWrapper.move(DefaultSessionFactory.java:398)
        at org.ametys.plugins.workflow.store.JackrabbitWorkflowStore.moveToHistory(JackrabbitWorkflowStore.java:797)
        at com.opensymphony.workflow.AbstractWorkflow.createNewCurrentStep(AbstractWorkflow.java:1474)
        at com.opensymphony.workflow.AbstractWorkflow.transitionWorkflow(AbstractWorkflow.java:1256)
        at com.opensymphony.workflow.AbstractWorkflow.doAction(AbstractWorkflow.java:567)
        at org.ametys.plugins.workflow.Workflow.doAction(Workflow.java:227)
        at org.ametys.plugins.workflow.WorkflowTestCase$ActionTrigger.run(WorkflowTestCase.java:195)

""ActionTrigger12"" daemon prio=10 tid=0x904dd000 nid=0x2f64 waiting for monitor entry [0x8fd3f000..0x8fd40020]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at org.apache.jackrabbit.core.CachingHierarchyManager.nodeModified(CachingHierarchyManager.java:306)
        - waiting to lock <0xb4020630> (a java.lang.Object)
        at org.apache.jackrabbit.core.state.StateChangeDispatcher.notifyNodeModified(StateChangeDispatcher.java:189)
        at org.apache.jackrabbit.core.state.SessionItemStateManager.nodeModified(SessionItemStateManager.java:929)
        at org.apache.jackrabbit.core.state.StateChangeDispatcher.notifyNodeModified(StateChangeDispatcher.java:189)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.stateModified(LocalItemStateManager.java:446)
        at org.apache.jackrabbit.core.state.XAItemStateManager.stateModified(XAItemStateManager.java:595)
        at org.apache.jackrabbit.core.state.StateChangeDispatcher.notifyStateModified(StateChangeDispatcher.java:111)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.stateModified(SharedItemStateManager.java:400)
        at org.apache.jackrabbit.core.state.ItemState.notifyStateUpdated(ItemState.java:244)
        at org.apache.jackrabbit.core.state.ChangeLog.persisted(ChangeLog.java:285)
        at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:735)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:1092)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:337)
        at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:347)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:312)
        at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:313)
        at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1103)
        - locked <0xb4004650> (a org.apache.jackrabbit.core.XASessionImpl)
        at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:858)
        at org.ametys.plugins.repository.DefaultSessionFactory$SessionWrapper.save(DefaultSessionFactory.java:414)
        at org.ametys.plugins.workflow.store.JackrabbitWorkflowStore.markFinished(JackrabbitWorkflowStore.java:757)
        at com.opensymphony.workflow.AbstractWorkflow.createNewCurrentStep(AbstractWorkflow.java:1473)
        at com.opensymphony.workflow.AbstractWorkflow.transitionWorkflow(AbstractWorkflow.java:1256)
        at com.opensymphony.workflow.AbstractWorkflow.doAction(AbstractWorkflow.java:567)
        at org.ametys.plugins.workflow.Workflow.doAction(Workflow.java:227)
        at org.ametys.plugins.workflow.WorkflowTestCase$ActionTrigger.run(WorkflowTestCase.java:203)

Different session instances (not XA) are used in both threads.
The nodes modified are different:
- ActionTrigger38 is moving a node and the session has not been saved yet.
- ActionTrigger12 has updated another node and is saving it.

I try to reproduce this behavior in a Jackrabbit test case without success.
If you need more information or a test case, let me know i will give it a second try."
1,"ContentLengthInputStream does not implement available() properly. ContentLengthInputStream should either extend FilterInputStream or should delegate available() to wrappedStream.

Otherwise, available() on the response stream (an instance of AutoCloseInputStream, which is properly extending FilterInputStream, and, therefore, delegating to the ContentLengthInputStream) always returns 0.

This issue is important for the clients that try to improve performance by processing all data that can be read in a non-blocking way before blocking on the network."
1,"PerFieldCodecWrapper.loadTermsIndex concurrency problem. Selckin's while(1) testing on RT branch hit another error:
{noformat}
    [junit] Testsuite: org.apache.lucene.TestExternalCodecs
    [junit] Testcase: testPerFieldCodec(org.apache.lucene.TestExternalCodecs):	Caused an ERROR
    [junit] (null)
    [junit] java.lang.NullPointerException
    [junit] 	at org.apache.lucene.index.PerFieldCodecWrapper$FieldsReader.loadTermsIndex(PerFieldCodecWrapper.java:202)
    [junit] 	at org.apache.lucene.index.SegmentReader.loadTermsIndex(SegmentReader.java:1005)
    [junit] 	at org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:652)
    [junit] 	at org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:609)
    [junit] 	at org.apache.lucene.index.BufferedDeletesStream.applyDeletes(BufferedDeletesStream.java:276)
    [junit] 	at org.apache.lucene.index.IndexWriter.applyAllDeletes(IndexWriter.java:2660)
    [junit] 	at org.apache.lucene.index.IndexWriter.maybeApplyDeletes(IndexWriter.java:2651)
    [junit] 	at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:381)
    [junit] 	at org.apache.lucene.index.IndexReader.open(IndexReader.java:316)
    [junit] 	at org.apache.lucene.TestExternalCodecs.testPerFieldCodec(TestExternalCodecs.java:541)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1246)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1175)
    [junit] 
    [junit] 
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.909 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestExternalCodecs -Dtestmethod=testPerFieldCodec -Dtests.seed=-7296204858082494534:5010909751437000758
    [junit] WARNING: test method: 'testPerFieldCodec' left thread running: merge thread: _i(4.0):Cv130 _m(4.0):Cv30 _n(4.0):cv10 into _o
    [junit] RESOURCE LEAK: test method: 'testPerFieldCodec' left 1 thread(s) running
    [junit] NOTE: test params are: codec=PreFlex, locale=zh_TW, timezone=America/Santo_Domingo
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestDemo, TestExternalCodecs]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=2,free=104153512,total=125632512
    [junit] ------------- ---------------- ---------------
    [junit] TEST org.apache.lucene.TestExternalCodecs FAILED
    [junit] Exception in thread ""Lucene Merge Thread #5"" org.apache.lucene.util.ThreadInterruptedException: java.lang.InterruptedException: sleep interrupted
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:505)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:472)
    [junit] Caused by: java.lang.InterruptedException: sleep interrupted
    [junit] 	at java.lang.Thread.sleep(Native Method)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:503)
    [junit] 	... 1 more
{noformat}

I suspect this is also a trunk issue, but I can't reproduce it yet.

I think this is happening because the codecs HashMap is changing (via another thread), while .loadTermsIndex is called."
1,"TestNRTThreads test failure. hit a fail in TestNRTThreads running tests over and over:
"
1,"BeanConfig may incorrectly throw ConfigurationException. With the changes from JCR-1462 the BeanConfig.newInstance() may throw a ConfigurationException if the bean does not support a configuration parameter that is configured.

There may be cases where the check in newInstance() yields an unsupported property even though there is a bean property present with the given key. Because the implementation uses 'map.get(key) == null'  as a check for a property name the method will throw if the key exists but the value is null.

The implementation should rather use 'map.containsKey(key)'."
1,SimpleDateFormat used in a non thread safe manner. As Mike pointed out in http://www.mail-archive.com/java-dev@lucene.apache.org/msg10831.html SimpleDateFormat is not thread safe and hence DocMakers need to maintain it in a ThreadLocal.
1,"Instantiated/IndexWriter discrepanies.  * RAMDirectory seems to do a reset on tokenStreams the first time, this permits to initialise some objects before starting streaming, InstantiatedIndex does not.
 * I can Serialize a RAMDirectory but I cannot on a InstantiatedIndex because of : java.io.NotSerializableException: org.apache.lucene.index.TermVectorOffsetInfo

http://www.nabble.com/InstatiatedIndex-questions-to20576722.html

"
1,"DefaultHttpParamsFactory violates applet sandbox. The DefaultHttpParamsFactory in nightly build 20031009 makes two calls to 
System.getProperties().  This is by default verboten in an applet.  I have 
patched the source to catch the security exceptions and set the properties to a 
default value.  My modified code block follows:

        // TODO: To be removed. Provided for backward compatibility
        try {
          String agent = System.getProperties().getProperty
(""httpclient.useragent"");
          if (agent != null) {
            params.setParameter(HttpMethodParams.USER_AGENT, agent);
          }
        }
        catch (SecurityException dontCare) { }

        // TODO: To be removed. Provided for backward compatibility
        try {
          String preemptiveDefault = System.getProperties()
              .getProperty(""httpclient.authentication.preemptive"");
          if (preemptiveDefault != null) {
            preemptiveDefault = preemptiveDefault.trim().toLowerCase();
            if (preemptiveDefault.equals(""true"")) {
              params.setParameter
(HttpClientParams.PREEMPTIVE_AUTHENTICATION, ""on"");
            }
            else if (preemptiveDefault.equals(""false"")) {
              params.setParameter
(HttpClientParams.PREEMPTIVE_AUTHENTICATION, ""off"");
            }
          }
        }
        catch(SecurityException dontCare) { }"
1,"NullPointerException in ItemManager. We have a lot of these occurring:
java.lang.NullPointerException
	at org.apache.jackrabbit.core.ItemManager.getDefinition(ItemManager.java:206)
	at org.apache.jackrabbit.core.ItemData.getDefinition(ItemData.java:99)
	at org.apache.jackrabbit.core.AbstractNodeData.getNodeDefinition(AbstractNodeData.java:73)
	at org.apache.jackrabbit.core.NodeImpl.getDefinition(NodeImpl.java:2430)
	at org.apache.jackrabbit.core.ItemValidator.isProtected(ItemValidator.java:373)
	at org.apache.jackrabbit.core.ItemValidator.checkCondition(ItemValidator.java:273)
	at org.apache.jackrabbit.core.ItemValidator.checkRemove(ItemValidator.java:254)
	at org.apache.jackrabbit.core.ItemRemoveOperation.perform(ItemRemoveOperation.java:63)
	at org.apache.jackrabbit.core.session.SessionState.perform(SessionState.java:200)
	at org.apache.jackrabbit.core.ItemImpl.perform(ItemImpl.java:91)
	at org.apache.jackrabbit.core.ItemImpl.remove(ItemImpl.java:322)
	at org.apache.jackrabbit.core.NPEandCMETest$TestTask.run(NPEandCMETest.java:87)
	at java.lang.Thread.run(Thread.java:679)

I'll attach a junit test to reproduce this exception."
1,"QValueFactoryImpl$BinaryQValue must not return 'this' on getBinary. This is basically the same as JCR-2238, but for the spi-commons module. BinaryQValue returns 'this' on getBinary(), which will lead to a file not found exception because Binary.dispose() will delete the the underlying temp file.

The issue is partially hidden by the presence of a bug in BinaryQValue.read(): the RandomAccessFile is not closed after reading, which might prevent deleting of the temp file."
1,"""Socket Closed"" IOException thrown by HttpConnection. HttpClient.java was modified in version 2.0 Final in method executeMethod().
The call to connection.setSoTimeout() used to be in RC3 after the call to
connection.isOpen(), but in the final version the call happens before the call 
to isOpen(). The result of the change is that the setSoTimeout() call could 
throw IOException because of closed socket.

I would fix the problem by adding to HttpConnection.setSoTimeout() (and to 
other similar methods in HttpConnection) an explicit check (a call to isOpen
()) whether the socket is closed as the existence of socket object does not 
guarantee it. I.e the following code:

    public void setSoTimeout(int timeout)
        throws SocketException, IllegalStateException {
        LOG.debug(""HttpConnection.setSoTimeout("" + timeout + "")"");
        soTimeout = timeout;
        if (socket != null) {
            socket.setSoTimeout(timeout);
        }
    }

would be changed to

    public void setSoTimeout(int timeout)
        throws SocketException, IllegalStateException {
        LOG.debug(""HttpConnection.setSoTimeout("" + timeout + "")"");
        soTimeout = timeout;
        if (isOpen()) {
            socket.setSoTimeout(timeout);
        }
    }"
1,"if you open an NRT reader while addIndexes* is running it may miss segments. Earwin spotted this in pending ongoing refactoring of Dir/MultiReader, but I wanted to open this separately just to make sure we fix it for 3.1...

This is the fix:
{code}
Index: src/java/org/apache/lucene/index/DirectoryReader.java
===================================================================
--- src/java/org/apache/lucene/index/DirectoryReader.java	(revision 919119)
+++ src/java/org/apache/lucene/index/DirectoryReader.java	(working copy)
@@ -145,7 +145,7 @@
     for (int i=0;i<numSegments;i++) {
       boolean success = false;
       try {
-        final SegmentInfo info = infos.info(upto);
+        final SegmentInfo info = infos.info(i);
         if (info.dir == dir) {
           readers[upto++] = writer.readerPool.getReadOnlyClone(info, true, termInfosIndexDivisor);
         }
{code}

"
1,"adding docs with large (binary) fields of 5mb causes OOM regardless of heap size. as reported by George Washington in a message to java-user@lucene.apache.org with subect ""Storing large text or binary source documents in the index and memory usage"" arround 2006-01-21 there seems to be a problem with adding docs containing really large fields.

I'll attach a test case in a moment, note that (for me) regardless of how big i make my heap size, and regardless of what value I set  MIN_MB to, once it starts trying to make documents of containing 5mb of data, it can only add 9 before it rolls over and dies.

here's the output from the code as i will attach in a moment...

    [junit] Testsuite: org.apache.lucene.document.TestBigBinary
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 78.656 sec

    [junit] ------------- Standard Output ---------------
    [junit] NOTE: directory will not be cleaned up automatically...
    [junit] Dir: /tmp/org.apache.lucene.document.TestBigBinary.97856146.100iters.4mb
    [junit] iters completed: 100
    [junit] totalBytes Allocated: 419430400
    [junit] NOTE: directory will not be cleaned up automatically...
    [junit] Dir: /tmp/org.apache.lucene.document.TestBigBinary.97856146.100iters.5mb
    [junit] iters completed: 9
    [junit] totalBytes Allocated: 52428800
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testBigBinaryFields(org.apache.lucene.document.TestBigBinary):    Caused an ERROR
    [junit] Java heap space
    [junit] java.lang.OutOfMemoryError: Java heap space


    [junit] Test org.apache.lucene.document.TestBigBinary FAILED
"
1,WeightedHighlighter does not encode XML markup characters. See JCR-2611; the same problem applies to WeightedHighlighter.
1,"ALLOW_CIRCULAR_REDIRECTS has no effect if references include query string. ALLOW_CIRCULAR_REDIRECTS parameter in HttpClientParameters has no effect if
circular reference contains in URL parameters."
1,"Connection.setAutoCommit(...) fails if connection is managed for JNDIDatabasePersistenceManager. Invoking setAutoCommit() on a db connection fails if the connection is managed.

I propose as a workaround to check if the auto commit must be set previous to setting it (a trivial patch will be provided).

This can happen eg. if you use JNDI (eg JNDIDatabasePersistenceManager) to fetch the connection on JBoss, and the persistent manager tries to reconnect (see stack trace below).

05 Jul 09:54:24 ERROR sePersistenceManager| failed to re-establish connection
java.sql.SQLException: You cannot set autocommit during a managed transaction!
        at org.jboss.resource.adapter.jdbc.BaseWrapperManagedConnection.setJdbcAutoCommit(BaseWrapperManagedConnection.java:482)
        at org.jboss.resource.adapter.jdbc.WrappedConnection.setAutoCommit(WrappedConnection.java:322)
        at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.initConnection(DatabasePersistenceManager.java:731)
        at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.reestablishConnection(DatabasePersistenceManager.java:806)
        at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.executeStmt(DatabasePersistenceManager.java:852)
        at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.exists(DatabasePersistenceManager.java:647)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.hasNonVirtualItemState(SharedItemStateManager.java:1102)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.hasItemState(SharedItemStateManager.java:289)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.hasItemState(LocalItemStateManager.java:180)
        at org.apache.jackrabbit.core.state.XAItemStateManager.hasItemState(XAItemStateManager.java:252)
        at org.apache.jackrabbit.core.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:174)"
1,CompactNodeTypeDefWriter uses bad format for residual properties . CompactNodeTypeDefWriter writes {}* instead of * for residual properties. 
1,"FileDataStore garbage collection can throw a NullPointerException if there is I/O problem. The FileDataStore can throw a NPE when doing garbage collection, if there is file I/O problem (for example an access rights problem). The reason is that it doesn't check if File.list / listFiles returns null. Stack trace:

Exception in thread ""Thread-461"" java.lang.NullPointerException
	at org.apache.jackrabbit.core.data.FileDataStore.deleteOlderRecursive(FileDataStore.java:334)
	at org.apache.jackrabbit.core.data.FileDataStore.deleteOlderRecursive(FileDataStore.java:328)
	at org.apache.jackrabbit.core.data.FileDataStore.deleteOlderRecursive(FileDataStore.java:328)
	at org.apache.jackrabbit.core.data.FileDataStore.deleteOlderRecursive(FileDataStore.java:328)

"
1,"GzipDecompressingEntity (and therefore ContentEncodingHttpClient) not consistent with EntityUtils.consumeEntity. Invoking EntityUtils.consume( entity ) after a previous call to entity.getContent (and subsequent processing of the content) throws a java.io.EOFException when gzip decompression support is enabled via ContentEncodingHttpClient or some similar mechanism.  I invoke EntityUtils.consume in a 'finally' block - maybe I'm not using the API correctly ... ?  

java.io.EOFException
	at java.util.zip.GZIPInputStream.readUByte(GZIPInputStream.java:207)
	at java.util.zip.GZIPInputStream.readUShort(GZIPInputStream.java:197)
	at java.util.zip.GZIPInputStream.readHeader(GZIPInputStream.java:136)
	at java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:58)
	at java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:68)
	at org.apache.http.client.entity.GzipDecompressingEntity.getContent(GzipDecompressingEntity.java:63)
	at org.apache.http.conn.BasicManagedEntity.getContent(BasicManagedEntity.java:88)
	at org.apache.http.util.EntityUtils.consume(EntityUtils.java:65)

I believe the problem is that the underlying DecompressingEntity allocates a new GzipInputStream for each call to getContent, rather than caching the stream created by the first getContent call.  
       http://svn.apache.org/repos/asf/httpcomponents/httpclient/trunk/httpclient/src/main/java/org/apache/http/client/entity/DecompressingEntity.java
The ""CustomProtocolInterceptors"" example has the same bug:  http://hc.apache.org/httpcomponents-client-ga/examples.html

I worked around the problem implementing the example with my own GzipDecompressingEntity (scala code - lazy value not evaluated till accessed):

  class GzipDecompressingEntity( entity:http.HttpEntity) extends http.entity.HttpEntityWrapper(entity) {
    private lazy val gzipStream = new GZIPInputStream( entity.getContent() )
    
    /** 
     * Wrap entity stream in GZIPInputStream
     */
    override def getContent():java.io.InputStream = gzipStream

    /**
     * Return -1 - don't know unzipped content size
     */
    override def getContentLength():Long = -1L
  }

"
1,"XML import always throws ItemExistsException when trying to overwrite existing nodes. According to the JCR-API, it should be possible to govern the import of XML serialized referenceable nodes in case of UUID collision. Unfortunately, the UUID conflict is handled too late during import, an ItemExistsException is always thrown beforehand due to not allowed same-name-siblings.

Simply try to import a previously exported referenceable node twice, providing either

- ImportUUIDBehavior.IMPORT_UUID_COLLISION_REMOVE_EXISTING or
- ImportUUIDBehavior.IMPORT_UUID_COLLISION_REPLACE_EXISTING.

This will fail and result in an ItemExistsException."
1,"QueryWrapperFilter gets null DocIdSetIterator when wrapping TermQuery. If you try to get the iterator for the DocIdSet returned by a QueryWrapperFilter which wraps a TermQuery you get null instead of an iterator that returns the same documents as the search on the TermQuery.

Code demonstrating the issue:

{code:java}
import java.io.IOException;
import org.apache.lucene.analysis.WhitespaceAnalyzer;
import org.apache.lucene.document.Document;
import org.apache.lucene.document.Field;
import org.apache.lucene.document.Field.Index;
import org.apache.lucene.document.Field.Store;
import org.apache.lucene.index.IndexReader;
import org.apache.lucene.index.IndexWriter;
import org.apache.lucene.index.IndexWriterConfig;
import org.apache.lucene.index.Term;
import org.apache.lucene.store.RAMDirectory;
import org.apache.lucene.util.Version;
import org.apache.lucene.search.DocIdSet;
import org.apache.lucene.search.DocIdSetIterator;
import org.apache.lucene.search.Filter;
import org.apache.lucene.search.IndexSearcher;
import org.apache.lucene.search.QueryWrapperFilter;
import org.apache.lucene.search.TermQuery;
import org.apache.lucene.search.TopDocs;

public class TestQueryWrapperFilterIterator {
   public static void main(String[] args) {
		try {
			IndexWriterConfig iwconfig = new IndexWriterConfig(Version.LUCENE_34, new WhitespaceAnalyzer(Version.LUCENE_34));
			iwconfig.setOpenMode(IndexWriterConfig.OpenMode.CREATE);
			RAMDirectory dir = new RAMDirectory();
		
			IndexWriter writer = new IndexWriter(dir, iwconfig);
			Document d = new Document();
			d.add(new Field(""id"", ""1001"", Store.YES, Index.NOT_ANALYZED));
			d.add(new Field(""text"", ""headline one group one"", Store.YES, Index.ANALYZED));
			d.add(new Field(""group"", ""grp1"", Store.YES, Index.NOT_ANALYZED));
		    writer.addDocument(d);
			writer.commit();
			writer.close();
			
			IndexReader rdr = IndexReader.open(dir);
			IndexSearcher searcher = new IndexSearcher(rdr);
			
			TermQuery tq = new TermQuery(new Term(""text"", ""headline""));
			
			TopDocs results = searcher.search(tq, 5);
			System.out.println(""Number of search results: "" + results.totalHits);
			
			Filter f = new QueryWrapperFilter(tq);
			
			DocIdSet dis = f.getDocIdSet(rdr);
			
			DocIdSetIterator it = dis.iterator();
			if (it != null) {
				int docId = it.nextDoc();
				while (docId != DocIdSetIterator.NO_MORE_DOCS) {
					Document doc = rdr.document(docId);
					System.out.println(""Iterator doc: "" + doc.get(""id""));
					docId = it.nextDoc();
				}
			} else {
				System.out.println(""Iterator was null: "");
			}
			
			searcher.close();
			rdr.close();
		} catch (IOException ioe) {
			ioe.printStackTrace();
		}

	}
}
{code}"
1,"workspace.copy causes 2 nodes in the same workspace to have the same version history. workspace.copy creates a copy of a versionable node with a new uuid which share the same version. ""In a given workspace, there is at most one versionable node per version history"" (4.11 spec)"
1,"JNDI data sources with BundleDbPersistenceManager: UnsupportedOperationException. When using the org.apache.tomcat.dbcp.dbcp.BasicDataSourceFactory, the BundleDbPersistenceManager can not open a database connection via JNDI because the method DataSource.getConnection(user, password) is not supported. Instead, DataSource.getConnection() must be used for this to work.

ConnectionFactory.getConnection should be changed to call this method if user name and password are empty."
1,"AccessControlManager#setPolicy may fail for new applicable policy despite jcr:modifyAccessControl privilege being granted. the sequence AccessControlManager.getApplicablePolicies -> modify -> AccessControlManager#setPolicy fails
due to bug in AC evaluation if target is an AC-item but not yet existing. in this case the
wrong parent node is used for AC-evaluation."
1,"Unsynchronized access to MultiIndex#indexes. This may result in a concurrent modification exception:

java.util.ConcurrentModificationException
	at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
	at java.util.AbstractList$Itr.next(AbstractList.java:343)
	at org.apache.jackrabbit.core.query.lucene.MultiIndex.getIndexReader(MultiIndex.java:744)
	at org.apache.jackrabbit.core.query.lucene.MultiIndex.getIndexReader(MultiIndex.java:712)
	at org.apache.jackrabbit.core.query.lucene.SearchIndex.getIndexReader(SearchIndex.java:1024)
	at org.apache.jackrabbit.core.query.lucene.SearchIndex.executeQuery(SearchIndex.java:820)
	at org.apache.jackrabbit.core.query.lucene.SingleColumnQueryResult.executeQuery(SingleColumnQueryResult.java:78)
	at org.apache.jackrabbit.core.query.lucene.QueryResultImpl.getResults(QueryResultImpl.java:293)
	at org.apache.jackrabbit.core.query.lucene.SingleColumnQueryResult.<init>(SingleColumnQueryResult.java:70)
	at org.apache.jackrabbit.core.query.lucene.QueryImpl.execute(QueryImpl.java:132)
	at org.apache.jackrabbit.core.query.QueryImpl.execute(QueryImpl.java:127)

This is usually very unlikely but with the recent changes to our tests, index flushes are very frequent and may cause the above exception."
1,"PersistentNode.store() ignores status when storing. While looking for a performance bottle neck I came across this issue: When a PersistentNodeState is asked to store itself in the PersistentNodeState.store() method, it calls its PersistenceManager to store it.

This is not a problem in itself. The problem is, that if the PersistentNodeState has not been modified, the object does not need to be stored. Doiing it anyway just consumes cycles ! In the case of a deep, unmodified hierarchy, this just results in nodes being written to persistence for nothing.

Comes to it, that this method sends an event, which in the case of an unmodified node state will be notifyStateUpdated(), which is complete nonsense, because nothing has actually been updated.

I suggest to modify the PersistentNodeState.store() method to only do work if modified.

Note: I encountered this issue, whily tracking down performance problems when creating versionable nodes, which turned out to be located somewhere within the PersistentVersionManager.createVersionHistory(NodeImpl) method. And there, predominantly the store() methods consume time."
1,"a dead lock in DefaultISMLocking. The jackrabbit 2.2 's org.apache.jackrabbit.core.state.DefaultISMLocking has a defect which will cause a dead lock in concurrent use cases.
The use case is as follows:
1.	Thread A apply a read lock, now there is an active reader hold the read lock.

2.	Thread B apply a write lock, and then thread B will wait for thread A's reading end. You could see below code snippet from the Jackrabbit source. readerCount is the current active reader.
writersWaiting++;
while (writerId != null? !isSameThreadId(writerId, currentId) : readerCount > 0) {
                                wait();
}

3.	Thread A apply another read lock, then it will wait too, since there is a writer is waiting.  Then a dead lock happens.
while (writerId != null? (writerCount > 0 && !isSameThreadId(writerId, currentId)): writersWaiting > 0) {
                                wait();
}

Since the lock in DefaultISMLocking is global lock, so I think if a thread has already hold a reader lock, it could get the reader lock again. I create a fix with this idea.
"
1,"GData TestDateFormater (sic) fails when the Date returned is: Sun, 23 Sep 2007 01:29:06 GMT+00:00. TestDateFormater.testFormatDate fails when the Date returned is Sun, 23 Sep 2007 01:29:06 GMT+00:00

The issue lies with the +00:00 at the end of the string.  

The question is, though, is that a valid date for GData?

This is marked as major b/c it is causing nightly builds to fail."
1,"AccessControlProvider#getEffectivePolicies for a set of principals does not include repo-level ac. as of JCR-2774 the resource based ac implementation allows to edit permissions for repository level operations.
however, ACLProvider#getEffectivePolicies(Set<Principal>, CompiledPermissions) does not include the repo level AC
in the result set due to a missing test for regular acl OR repo-level acl."
1,"HostConfiguration.setHost(String) causes NullPointerException. Calling setHost(String) on a HostConfiguration object causes a null pointer
exception.

As far as I can tell, this is due to it incorrectly calling the deprecated
setHost(String, String, int, Protocol) method, rather than setHost(String, int,
Protocol)

So:

public synchronized void setHost(final String host) {
  Protocol defaultProtocol = Protocol.getProtocol(""http""); 
  setHost(host, null, defaultProtocol.getDefaultPort(), defaultProtocol);
}

should become :

public synchronized void setHost(final String host) {
    Protocol defaultProtocol = Protocol.getProtocol(""http""); 
    setHost(host, defaultProtocol.getDefaultPort(), defaultProtocol);
}"
1,AbstractSession should not synchronize on the session instance. The local namespace mapping methods in AbstractSession are synchronized to protect against concurrent access. That's troublesome since our observation delivery needs to be able to get those mappings even when the session is synchronized to do something else.
1,"MoreLikeThis reuses a reader after it has already closed it. MoreLikeThis has a fatal bug whereby it tries to reuse a reader for multiple fields:

{code}
    Map<String,Int> words = new HashMap<String,Int>();
    for (int i = 0; i < fieldNames.length; i++) {
        String fieldName = fieldNames[i];
        addTermFrequencies(r, words, fieldName);
    }
{code}

However, addTermFrequencies() is creating a TokenStream for this reader:

{code}
    TokenStream ts = analyzer.reusableTokenStream(fieldName, r);
    int tokenCount=0;
    // for every token
    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);
    ts.reset();
    while (ts.incrementToken()) {
        /* body omitted */
    }
    ts.end();
    ts.close();
{code}

When it closes this analyser, it closes the underlying reader.  Then the second time around the loop, you get:

{noformat}
Caused by: java.io.IOException: Stream closed
	at sun.nio.cs.StreamDecoder.ensureOpen(StreamDecoder.java:27)
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:128)
	at java.io.InputStreamReader.read(InputStreamReader.java:167)
	at com.acme.util.CompositeReader.read(CompositeReader.java:101)
	at org.apache.lucene.analysis.standard.StandardTokenizerImpl.zzRefill(StandardTokenizerImpl.java:803)
	at org.apache.lucene.analysis.standard.StandardTokenizerImpl.getNextToken(StandardTokenizerImpl.java:1010)
	at org.apache.lucene.analysis.standard.StandardTokenizer.incrementToken(StandardTokenizer.java:178)
	at org.apache.lucene.analysis.standard.StandardFilter.incrementTokenClassic(StandardFilter.java:61)
	at org.apache.lucene.analysis.standard.StandardFilter.incrementToken(StandardFilter.java:57)
	at com.acme.storage.index.analyser.NormaliseFilter.incrementToken(NormaliseFilter.java:51)
	at org.apache.lucene.analysis.LowerCaseFilter.incrementToken(LowerCaseFilter.java:60)
	at org.apache.lucene.search.similar.MoreLikeThis.addTermFrequencies(MoreLikeThis.java:931)
	at org.apache.lucene.search.similar.MoreLikeThis.retrieveTerms(MoreLikeThis.java:1003)
	at org.apache.lucene.search.similar.MoreLikeThis.retrieveInterestingTerms(MoreLikeThis.java:1036)
{noformat}

My first thought was that it seems like a ""ReaderFactory"" of sorts should be passed in so that a new Reader can be created for the second field (maybe the factory could be passed the field name, so that if someone wanted to pass a different reader to each, they could.)

Interestingly, the methods taking File and URL exhibit the same issue.  I'm not sure what to do about those (and we're not using them.)  The method taking File could open the file twice, but the method taking a URL probably shouldn't fetch the same URL twice.
"
1,"StandardCodec sometimes supplies skip pointers past EOF. Pretty sure this is 4.0-only:
I added an assertion, the test to reproduce is:

ant test-core -Dtestcase=TestPayloadNearQuery -Dtestmethod=testMinFunction -Dtests.seed=4841190615781133892:3888521539169738727 -Dtests.multiplier=3

{noformat}
    [junit] Testcase: testMinFunction(org.apache.lucene.search.payloads.TestPayloadNearQuery):  FAILED
    [junit] invalid skip pointer: 404, length=337
    [junit] junit.framework.AssertionFailedError: invalid skip pointer: 404, length=337
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1127)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1059)
    [junit]     at org.apache.lucene.index.codecs.MultiLevelSkipListReader.init(MultiLevelSkipListReader.java:176)
    [junit]     at org.apache.lucene.index.codecs.standard.DefaultSkipListReader.init(DefaultSkipListReader.java:50)
    [junit]     at org.apache.lucene.index.codecs.standard.StandardPostingsReader$SegmentDocsAndPositionsAndPayloadsEnum.advance(StandardPostingsReader.java:742)
    [junit]     at org.apache.lucene.search.spans.TermSpans.skipTo(TermSpans.java:72)
{noformat}"
1,"Removal of a node with shared subnodes fails. A simple testcase:

Set up (first transaction):
Node a1 = testRootNode.addNode(""a1"");
Node a2 = testRootNode.addNode(""a2"");
a2.addMixin(""mix:shareable"");
session.save();
// now we have a shareable node N with path a2

Workspace workspace = session.getWorkspace();
String path = a1.getPath() + ""/b1"";
workspace.clone(workspace.getName(), a2.getPath(), path, false);
session.save();
// now we have another shareable node N' in the same shared set as N with path a1/b1

Test(second transaction):
testRootNode.remove(""a1"");
session.save();

At least in a transactional repository the node will not be removed, an error will be thrown instead."
1,"Range queries fail on large repositories. As discussed on the user mailing list, queries on large repositories with date constraints like ""field > constant"" treat the constraint as always true, returning results that should not be returned."
1,"NullPointerException in HttpMethodBase.getResponseBodyAsString. The following code in a cocoon component, causes the NPE.
A delay seems to help sometimes.

-------------
      int htcode = httpClient.executeMethod( method );
       
      // @todo: fix-me
      // This sleep() is a temporary workaround 
      // to avoid NullPointerException in the next line.
      Thread.currentThread().sleep( 100 ); 

      String ret = method.getResponseBodyAsString();
---------------------

java.lang.NullPointerException 
at java.lang.String.<init>(String.java:399) 
at org.apache.commons.httpclient.HttpMethodBase.getResponseBodyAsString
(HttpMethodBase.java:579) 
at org.apache.cocoon.generation.WebServiceProxyGenerator.fetch
(WebServiceProxyGenerator.java:264)"
1,"MultiReader.numDocs incorrect after undeleteAll. Calling MultiReader.undeleteAll does not clear cached numDocs value. So the subsequent numDocs() call returns a wrong value if there were deleted documents in the index. Following patch fixes the bug and adds a test showing the issue.


Index: src/test/org/apache/lucene/index/TestMultiReader.java
===================================================================
--- src/test/org/apache/lucene/index/TestMultiReader.java       (revision 354923)
+++ src/test/org/apache/lucene/index/TestMultiReader.java       (working copy)
@@ -69,6 +69,18 @@
     assertTrue(vector != null);
     TestSegmentReader.checkNorms(reader);
   }
+
+  public void testUndeleteAll() throws IOException {
+    sis.read(dir);
+    MultiReader reader = new MultiReader(dir, sis, false, readers);
+    assertTrue(reader != null);
+    assertEquals( 2, reader.numDocs() );
+    reader.delete(0);
+    assertEquals( 1, reader.numDocs() );
+    reader.undeleteAll();
+    assertEquals( 2, reader.numDocs() );
+  }
+

   public void testTermVectors() {
     MultiReader reader = new MultiReader(dir, sis, false, readers);
Index: src/java/org/apache/lucene/index/MultiReader.java
===================================================================
--- src/java/org/apache/lucene/index/MultiReader.java   (revision 354923)
+++ src/java/org/apache/lucene/index/MultiReader.java   (working copy)
@@ -122,6 +122,7 @@
     for (int i = 0; i < subReaders.length; i++)
       subReaders[i].undeleteAll();
     hasDeletions = false;
+    numDocs = -1;      // invalidate cache
   }

   private int readerIndex(int n) {    // find reader for doc n:"
1,"CJKTokenizer generates tokens with incorrect offsets. If I index a Japanese *multi-valued* document with CJKTokenizer and highlight a term with FastVectorHighlighter, the output snippets have incorrect highlighted string. I'll attach a program that reproduces the problem soon."
1,"[PATCH] Ordered spanquery with slop can fail. In CVS of 7 April 2004. 
An ordered SpanQuery with slop 1 querying: w1 w2 w3 
in document: w1 w3 w2 w3 
fails. It should match as: w1 . w2 w3"
1,"when many query clases are specified in boolean or dismax query, highlighted terms are always ""yellow"" if multi-colored feature is used. The problem is the following snippet:

{code}
protected String getPreTag( int num ){
  return preTags.length > num ? preTags[num] : preTags[0];
}
{code}

it should be:

{code}
protected String getPreTag( int num ){
  int n = num % preTags.length;
  return  preTags[n];
}
{code}
"
1,"FieldCache should not pay attention to deleted docs when creating entries. The FieldCache uses a key that ignores deleted docs, so it's actually a bug to use deleted docs when creating an entry.  It can lead to incorrect values when the same entry is used with a different reader."
1,"OutOfMemoryError when re-indexing the repository. [ERROR] 20060825 17:06:40
(org.apache.jackrabbit.core.observation.ObservationManagerFactory) -
Synchronous EventConsumer threw exception. java.lang.OutOfMemoryError

when we try to re-index a repository, the repository is quite big (more then 4 Gb of disk usage) and sometimes it stores 40Mb size documents.

As attach I put all the last logs we registered, with the full stack traces.

Related to this whe have also errors with Lucene:

[DEBUG] 20060803 08:24:01 (org.apache.jackrabbit.core.query.LazyReader)
- Dump: 
java.io.IOException: Invalid header signature; read 8656037701166316554,
expected -2226271756974174256
        at org.apache.jackrabbit.core.query.MsWordTextFilter

and then this ones:

[DEBUG] 20060803 08:37:17 (org.apache.jackrabbit.core.ItemManager) -
removing item 8637bf5f-4689-4e75-888f-b7b89bef40c8 from cache
[ WARN] 20060803 08:40:13 (org.apache.jackrabbit.core.RepositoryImpl) -
Existing lock file at C:\Wave\Repository\.lock deteteced. Repository was
not shut down properly.
[ERROR] 20060803 09:33:14
(org.apache.jackrabbit.core.observation.ObservationManagerFactory) -
Synchronous EventConsumer threw exception.
java.lang.NullPointerException: null values not allowed

this is our repository.xml configuration for indexing

<SearchIndex
class=""org.apache.jackrabbit.core.query.lucene.SearchIndex"">
        <param name=""path"" value=""${wsp.home}/index""/>
        <param name=""textFilterClasses""
value=""org.apache.jackrabbit.core.query.lucene.TextPlainTextFilter,
org.apache.jackrabbit.core.query.MsExcelTextFilter,
org.apache.jackrabbit.core.query.MsPowerPointTextFilter, 
org.apache.jackrabbit.core.query.MsWordTextFilter,
org.apache.jackrabbit.core.query.PdfTextFilter,
org.apache.jackrabbit.core.query.HTMLTextFilter,
org.apache.jackrabbit.core.query.XMLTextFilter,
org.apache.jackrabbit.core.query.RTFTextFilter,
                        org.apache.jackrabbit.core.query.OpenOfficeTextFilter""/>
        <param name=""useCompoundFile"" value=""true""/>
        <param name=""minMergeDocs"" value=""100""/>
        <param name=""volatileIdleTime"" value=""3""/>
        <param name=""maxMergeDocs"" value=""100000""/>
        <param name=""mergeFactor"" value=""10""/>
        <param name=""bufferSize"" value=""10""/>
        <param name=""cacheSize"" value=""1000""/>
        <param name=""forceConsistencyCheck"" value=""false""/>
        <param name=""autoRepair"" value=""true""/>
                <param name=""respectDocumentOrder"" value=""false""/>
        <param name=""analyzer""
value=""org.apache.lucene.analysis.standard.StandardAnalyzer""/>
</SearchIndex>"
1,"InternalVersionManagerBase; missing null check after getNode(). There are at least two instances where we check for a node with hasNode(), and then call getNode() without checking for null."
1,"DatabaseJournal assigns same revision id to different revisions. Running a transaction that updates multiple workspaces (e.g. a versioning operation) will fail in DatabaseJournal, because every individual update will ultimately be assigned the same revision id. An indication of this failure when e.g. using Oracle as backend for journaling will look as follows::

java.sql.SQLException: ORA-00001: unique constraint (JOURNAL_IDX) violated
 at oracle.jdbc.dbaccess.DBError.throwSqlException(DBError.java:134)
 at oracle.jdbc.ttc7.TTIoer.processError(TTIoer.java:289)
 at oracle.jdbc.ttc7.Oall7.receive(Oall7.java:590)
 at oracle.jdbc.ttc7.TTC7Protocol.doOall7(TTC7Protocol.java:1973)
 at oracle.jdbc.ttc7.TTC7Protocol.executeFetch(TTC7Protocol.java:977)
 at oracle.jdbc.driver.OracleStatement.executeNonQuery(OracleStatement.java:2205)
 at oracle.jdbc.driver.OracleStatement.doExecuteOther(OracleStatement.java:2064)
 at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:2989)
 at oracle.jdbc.driver.OraclePreparedStatement.executeUpdate(OraclePreparedStatement.java:658)
 at oracle.jdbc.driver.OraclePreparedStatement.execute(OraclePreparedStatement.java:736)
 at org.apache.jackrabbit.core.journal.DatabaseJournal.append(DatabaseJournal.java:293)
 ... 24 more

This bug has been reported by Rafa Kwiecie."
1,NPE in event polling thread. This exception occurs when running the jcr2dav integration tests. This surfaces as a  side effect of JCR-3046. The root cause is refresh(Event) not guarding against null values returned from Event.getItemId().
1,"Mixin type loss. When using a bundle persistence manager, the mixin type information may be corrupted in the lucene index, causing queries like '//element(*, my:mixin)' to fail.



The problem is that the 'jcr:mixinTypes' may be stored in the bundle. Here is how this could happen :


First step: Create a node and add a mixin 'A'.

Everything's fine. The query '//element(*, 'A')' works.


Second step : Select the node and add a second mixin 'B'.

When the second mixin is added, the AbstractBundlePersistenceManager#load(PropertyId) is called to get the current mixins for the node. This method will store the PropertyState for 'jcr:mixinTypes' in the bundle (containing only the mixin 'A'). Then the NodeImpl#setMixinTypesProperty() will set the PropertyState for 'jcr:mixinTypes' in the node state (containing the mixins 'A' and 'B').
When the session is saved, the ChangeLog in AbstractBundlePersistenceManager#store() contains a modification for the 'jcr:mixinTypes' but it's being ignored, leaving the bundle with only mixin 'A'. The NodeIndexer looks into the node state to get the mixin types and indexes the node correctly. The queries '//element(*, 'A')' and '//element(*, 'B')' work.


Thrid step : Select the node and update a property. 

When the session is saved, the NodeIndexer asks again for the 'jcr:mixinTypes' property, by calling the AbstractBundlePersistenceManager#load(PropertyId) to load it. The bundle contains this property and returns only mixin 'A' (as it was stored in the second step), causing the index to use only mixin 'A'. The query '//element(*, 'A')' still works but '//element(*, 'B') doesn't work anymore.



A simple solution to this would be to not store the PropertyState for the 'jcr:mixinTypes' (and 'jcr:uuid' and 'jcr:primaryType', as the class description states) in the bundle when the PropertyState is loaded. It would fix the issue but not the contents on existing repositories. One way to allow the repositories to fix themselves is to not read or write these 3 properties in the BundleBinding#readBundle and BundleBinding#writeBundle methods, but I'm not sure wether or not it would have a performance impact.


"
1," inconsistent session and persistent state after ReferentialIntegrityException. When a ReferentialIntegrityException occurs in a session it seems that subsequent actions on that session may result in a inconsistent session state AND even inconsistent persistent state. The latter will even make jackrabbit fail to bootstrap an index from that persistent state.

Typical rootcause:

Caused by: org.apache.jackrabbit.core.state.NoSuchItemStateException: ddb9d3ea-59c1-4eb4-a83e-332f646d4f40
        at org.apache.jackrabbit.core.state.SharedItemStateManager.getItemState(SharedItemStateManager.java:270)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.createIndex(MultiIndex.java:1082)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.createIndex(MultiIndex.java:1088)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.createInitialIndex(MultiIndex.java:395)

Bootstrap failure:

java.io.IOException: Error indexing workspace
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.createInitialIndex(MultiIndex.java:402)
        at org.apache.jackrabbit.core.query.lucene.SearchIndex.doInit(SearchIndex.java:465)
        at org.apache.jackrabbit.core.query.AbstractQueryHandler.init(AbstractQueryHandler.java:59)
        at org.apache.jackrabbit.core.SearchManager.initializeQueryHandler(SearchManager.java:553)

"
1,"Workspace operations (copy/clone) do not handle references correctly. REFERENCE properties created through Workspace.copy() or Workspace.clone() are not reflected by 
Node.getReferences() and are as a consequence not enforced."
1,"BindableRepositoryFactory doesn't handle repository shutdown. The BindableRepositoryFactory class keeps a cached reference to a repository even after the repository has been shut down.

This causes the following code snippet to fail with an IllegalStateException:

        Hashtable environment = new Hashtable();
        environment.put(
                Context.INITIAL_CONTEXT_FACTORY,
                DummyInitialContextFactory.class.getName());
        environment.put(Context.PROVIDER_URL, ""http://jackrabbit.apache.org/"");
        Context context = new InitialContext(environment);

        JackrabbitRepository repository;
        String xml = ""src/test/repository/repository.xml"";
        String dir = ""target/repository"";
        String key = ""repository"";

        // Create first repository
        RegistryHelper.registerRepository(context, key, xml, dir, true);
        repository = (JackrabbitRepository) context.lookup(key);
        repository.login().logout();
        repository.shutdown();

        // Create second repository with the same configuration
        RegistryHelper.registerRepository(context, key, xml, dir, true);
        repository = (JackrabbitRepository) context.lookup(key);
        repository.login().logout(); // throws an IllegalStateException!
        repository.shutdown();
"
1,"JCR2SPI: test regression for WorkspaceMoveReferenceableTest.testMoveNodesReferenceableNodesNewUUID. The latest changes (up to 581637) seems to have broken TCK tests:


-------------------------------------------------------------------------------
Test set: org.apache.jackrabbit.test.TestAll
-------------------------------------------------------------------------------
Tests run: 1037, Failures: 6, Errors: 2, Skipped: 0, Time elapsed: 102.644 sec <<< FAILURE!
testMoveNodesReferenceableNodesNewUUID(org.apache.jackrabbit.test.api.WorkspaceMoveReferenceableTest)  Time elapsed: 0.03 sec  <<< ERROR!
javax.jcr.InvalidItemStateException: Item 'org.apache.jackrabbit.jcr2spi.NodeImpl@13ef9df' doesn't exist anymore
    at org.apache.jackrabbit.jcr2spi.ItemImpl.checkStatus(ItemImpl.java:428)
    at org.apache.jackrabbit.jcr2spi.NodeImpl.getName(NodeImpl.java:120)
    at org.apache.jackrabbit.test.api.WorkspaceMoveReferenceableTest.testMoveNodesReferenceableNodesNewUUID(WorkspaceMoveReferenceableTest.java:57) "
1,"Spatial checks for a string in an int,double map. {code}
  private Map<Integer,Double> distances;
{code}

{code}
    if (precise != null) {
      double xLat = getPrecision(lat, precise);
      double xLng = getPrecision(lng, precise);
      
      String k = new Double(xLat).toString() +"",""+ new Double(xLng).toString();
    
      Double d = (distances.get(k));
      if (d != null){
        return d.doubleValue();
      }
    }
{code}

Something is off here eh?"
1,"Random Test Failure org.apache.lucene.TestExternalCodecs.testPerFieldCodec (from TestExternalCodecs). Error Message

state.ord=54 startOrd=0 ir.isIndexTerm=true state.docFreq=1
Stacktrace

junit.framework.AssertionFailedError: state.ord=54 startOrd=0 ir.isIndexTerm=true state.docFreq=1
	at org.apache.lucene.index.codecs.standard.StandardTermsDictReader$FieldReader$SegmentTermsEnum.seek(StandardTermsDictReader.java:395)
	at org.apache.lucene.index.DocumentsWriter.applyDeletes(DocumentsWriter.java:1099)
	at org.apache.lucene.index.DocumentsWriter.applyDeletes(DocumentsWriter.java:1028)
	at org.apache.lucene.index.IndexWriter.applyDeletes(IndexWriter.java:4213)
	at org.apache.lucene.index.IndexWriter.doFlushInternal(IndexWriter.java:3381)
	at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:3221)
	at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3211)
	at org.apache.lucene.index.IndexWriter.optimize(IndexWriter.java:2345)
	at org.apache.lucene.index.IndexWriter.optimize(IndexWriter.java:2323)
	at org.apache.lucene.index.IndexWriter.optimize(IndexWriter.java:2293)
	at org.apache.lucene.TestExternalCodecs.testPerFieldCodec(TestExternalCodecs.java:645)
	at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:381)
	at org.apache.lucene.util.LuceneTestCase.run(LuceneTestCase.java:373)
Standard Output

NOTE: random codec of testcase 'testPerFieldCodec' was: MockFixedIntBlock(blockSize=1327)
NOTE: random locale of testcase 'testPerFieldCodec' was: lt_LT
NOTE: random timezone of testcase 'testPerFieldCodec' was: Africa/Lusaka
NOTE: random seed of testcase 'testPerFieldCodec' was: 812019387131615618"
1,"TestPerFieldCodecSupport intermittent fail. {noformat}

    [junit] Testsuite: org.apache.lucene.index.TestPerFieldCodecSupport
    [junit] Testcase: testChangeCodecAndMerge(org.apache.lucene.index.TestPerFieldCodecSupport):	FAILED
    [junit] expected:<4> but was:<3>
    [junit] junit.framework.AssertionFailedError: expected:<4> but was:<3>
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:881)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:847)
    [junit] 	at org.apache.lucene.index.TestPerFieldCodecSupport.assertHybridCodecPerField(TestPerFieldCodecSupport.java:189)
    [junit] 	at org.apache.lucene.index.TestPerFieldCodecSupport.testChangeCodecAndMerge(TestPerFieldCodecSupport.java:145)
    [junit] 
    [junit] 
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.416 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestPerFieldCodecSupport -Dtestmethod=testChangeCodecAndMerge -Dtests.seed=1508266713336297966:-102145263724760840
    [junit] NOTE: test params are: codec=SimpleText, locale=ms, timezone=America/Winnipeg
    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestPerFieldCodecSupport]
    [junit] ------------- ---------------- ---------------
    [junit] Test org.apache.lucene.index.TestPerFieldCodecSupport FAILED
{noformat}

I haven't tried to figure it out yet..."
1,"Exception executing SQL2/JQOM with non-admin session. Constraints are correctly handled when session does not have access to a node:

Caused by: javax.jcr.ItemNotFoundException: 21232f29-7a57-35a7-8389-4a0e4a801fc3
        at org.apache.jackrabbit.core.SessionImpl.getNodeById(SessionImpl.java:545)
        at org.apache.jackrabbit.core.query.lucene.constraint.NodeLocalNameOperand.getValues(NodeLocalNameOperand.java:44)
        at org.apache.jackrabbit.core.query.lucene.constraint.ComparisonConstraint.evaluate(ComparisonConstraint.java:80)"
1,"IW.commit() writes but fails to fsync the N.fnx file. In making a unit test for NRTCachingDir (LUCENE-3092) I hit this surprising bug!

Because the new N.fnx file is written at the ""last minute"" along with the segments file, it's not included in the sis.files() that IW uses to figure out which files to sync.

This bug means one could call IW.commit(), successfully, return, and then the machine could crash and when it comes back up your index could be corrupted.

We should hopefully first fix TestCrash so that it hits this bug (maybe it needs more/better randomization?), then fix the bug...."
1,"Infinite loop on NTLM authentication. I got an infinite loop on NTLM authentication if the authentication failed (bad credentials).

The state FAILED of the NTLM sheme is never catched in the method authenticate of the class HttpAuthenticator (line 123).
I fix temporatily this bug by adding a case for the protocol state HANDSHAKE."
1,WorkspaceInfo.dispose() does not deregister from obs dispatcher. JCR-305 introduces an automatic disposing of idle workspaces. this can lead to memory leaks because the observation factory is not deregistered from the delegating one.
1,"ClassCastException MultiReader. (See original message below)
Sure.  'Bugzilla it', please.

Otis
P.S.
That line 274 should be line 273 in the CVS HEAD as of now.

--- Rasik Pandey <rasik.pandey@ajlsm.com> wrote:
> Howdy,
> 
> This exception was thrown with 1.4rc3. Do you need a test case for 
> this one?
> 
> java.lang.ClassCastException
>         at
> org.apache.lucene.index.MultiTermEnum.<init>(MultiReader.java:274)
>         at
> org.apache.lucene.index.MultiReader.terms(MultiReader.java:187)
> 
> 
> Regards,
> RBP
> 
> 
> 
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: lucene-dev-unsubscribe@jakarta.apache.org
> For additional commands, e-mail: lucene-dev-help@jakarta.apache.org
> 


---------------------------------------------------------------------
To unsubscribe, e-mail: lucene-dev-unsubscribe@jakarta.apache.org
For additional commands, e-mail: lucene-dev-help@jakarta.apache.org"
1,"CloseableThreadLocal does not work well with Tomcat thread pooling. We tracked down a large memory leak (effectively a leak anyway) caused
by how Analyzer users CloseableThreadLocal.
CloseableThreadLocal.hardRefs holds references to Thread objects as
keys.  The problem is that it only frees these references in the set()
method, and SnowballAnalyzer will only call set() when it is used by a
NEW thread.

The problem scenario is as follows:

The server experiences a spike in usage (say by robots or whatever)
and many threads are created and referenced by
CloseableThreadLocal.hardRefs.  The server quiesces and lets many of
these threads expire normally.  Now we have a smaller, but adequate
thread pool.  So CloseableThreadLocal.set() may not be called by
SnowBallAnalyzer (via Analyzer) for a _long_ time.  The purge code is
never called, and these threads along with their thread local storage
(lucene related or not) is never cleaned up.

I think calling the purge code in both get() and set() would have
avoided this problem, but is potentially expensive.  Perhaps using 
WeakHashMap instead of HashMap may also have helped.  WeakHashMap 
purges on get() and set().  So this might be an efficient way to
clean up threads in get(), while set() might do the more expensive
Map.keySet() iteration.

Our current work around is to not share SnowBallAnalyzer instances
among HTTP searcher threads.  We open and close one on every request.

Thanks,
Matt"
1,"NullPointerException on DelegatingObservationDispatcher cause by parameter null on call : createEventStateCollection(null). There is a NullPointerException when jackrabbit try to synchronise its indexes :
22 janv. 2009 09:53:56 INFO  [ClusterNode] - Processing revision: 4485
22 janv. 2009 09:53:56 ERROR [ClusterNode] - Unexpected error while syncing of journal: null
java.lang.NullPointerException
        at org.apache.jackrabbit.core.observation.DelegatingObservationDispatcher.createEventStateCollection(DelegatingObservationDispatcher.java:80)
        at org.apache.jackrabbit.core.version.VersionManagerImpl$DynamicESCFactory.createEventStateCollection(VersionManagerImpl.java:556)
        at org.apache.jackrabbit.core.version.VersionManagerImpl.externalUpdate(VersionManagerImpl.java:500)
        at org.apache.jackrabbit.core.cluster.ClusterNode.process(ClusterNode.java:853)
        at org.apache.jackrabbit.core.cluster.ChangeLogRecord.process(ChangeLogRecord.java:457)
        at org.apache.jackrabbit.core.cluster.ClusterNode.consume(ClusterNode.java:799)
        at org.apache.jackrabbit.core.journal.AbstractJournal.doSync(AbstractJournal.java:213)
        at org.apache.jackrabbit.core.journal.AbstractJournal.sync(AbstractJournal.java:188)
        at org.apache.jackrabbit.core.cluster.ClusterNode.sync(ClusterNode.java:315)
        at org.apache.jackrabbit.core.cluster.ClusterNode.run(ClusterNode.java:286)
        at java.lang.Thread.run(Thread.java:595)

In fact the method createEventStateCollection() of DelegatingObservationDispatcher is called by the VersionManagerImpl with session parameter as null...

DelegatingObservationDispatcher:

 public EventStateCollection createEventStateCollection(SessionImpl session,
                                                           Path pathPrefix) {
        String userData = null;
        try {
            userData = ((ObservationManagerImpl) session.getWorkspace().getObservationManager()).getUserData();
        } catch (RepositoryException e) {
            // should never happen because this
            // implementation supports observation
        }
        return new EventStateCollection(this, session, pathPrefix, userData);
    }

VersionManagerImpl$DynamicESCFactory :
 public EventStateCollection createEventStateCollection(SessionImpl source) {
            return obsMgr.createEventStateCollection(source, VERSION_STORAGE_PATH);
        }

VersionManagerImpl :
public void externalUpdate(ChangeLog changes, List events,
                               long timestamp, String userData)
            throws RepositoryException {
        EventStateCollection esc = getEscFactory().createEventStateCollection(null);
        esc.addAll(events);
        esc.setTimestamp(timestamp);
        esc.setUserData(userData);

        sharedStateMgr.externalUpdate(changes, esc);
    }"
1,"Internal error in WorkspaceItemStateFactory#createDeepNodeState . When WorkspaceItemStateFactory#createDeepNodeState receives the current entry as argument for anyParent, it throws RepositoryException with the message ""Internal error while getting deep itemState"". This is incorrect (probably a leftover from JCR-1797) since any entry is valid as argument for anyParent. "
1,"DocMakers setup for the ""docs.dir"" property fails when passing an absolute path.. setConfig in TrecDocMaker assumes docs.dir is a relative path. Therefore it create new File(workDir, docs.dir). However, if docs.dir is an absolute path, this works incorrectly and results in No txt files in dataDir exception."
1,"PATCH MultiLevelSkipListReader NullPointerException.  When Reconstructing Document Using Luke Tool, received NullPointerException.

java.lang.NullPointerException
        at org.apache.lucene.index.MultiLevelSkipListReader.loadSkipLevels(MultiLevelSkipListReader.java:188)
        at org.apache.lucene.index.MultiLevelSkipListReader.skipTo(MultiLevelSkipListReader.java:97)
        at org.apache.lucene.index.SegmentTermDocs.skipTo(SegmentTermDocs.java:164)
        at org.getopt.luke.Luke$2.run(Unknown Source)

Luke version 0.7.1

I emailed with Luke author Andrzej Bialecki and he suggested the attached patch file which fixed the problem.
"
1,"MergeThread throws unchecked exceptions from toString(). This causes nearly all thread-dumping routines to fail and in the effect obscure the original problem. I think this
should return a string (always), possibly indicating the underlying writer has been closed or something."
1,NodeDefinitionTemplateImpl.setDefaultPrimaryTypeName(null) throws exception. expect to clear the name.
1,"corrupted paths after moving nodes. we just found a bug which corrupts the results of Node.getPath() - it seems to be related to older Jackrabbit bugs (e.g. JCR-768) but still happens in jackrabbit 1.3 and jackrabbit-1.4-SNAPSHOT

Basically we have a node with 3 subnodes (a, b, c), we move all of them to index 1 - this works fine, unless we call getPath() of the third Node before moving it.

The expected paths after moving would be:
a: /pages[37]/page/element[3]
b: /pages[37]/page/element[2]
c: /pages[37]/page/element

But we get these paths:

a: /pages[37]/page/element[3]
b: /pages[37]/page/element
c: /pages[37]/page/element"
1,"DEFAULT_ATTRIBUTE_FACTORY faills to load implementation class when iterface comes from different classloader. This is a followup for [http://www.lucidimagination.com/search/document/1724fcb3712bafba/using_the_new_tokenizer_api_from_a_jar_file]:

The DEFAULT_ATTRIBUTE_FACTORY should load the implementation class for a given attribute interface from the same classloader like the attribute interface. The current code loads it from the classloader of the lucene-core.jar file. In solr this fails when the interface is in a JAR file coming from the plugins folder. 

The interface is loaded correctly, because the addAttribute(FooAttribute.class) loads the FooAttribute.class from the plugin code and this with success. But as addAttribute tries to load the class from its local lucene-core.jar classloader it will not find the attribute.

The fix is to tell Class.forName to use the classloader of the corresponding interface, which is the correct way to handle it, as the impl and the attribute should always be in the same classloader and file.

I hope I can somehow add a test for that."
1,"ConstraintSplitter.getSelectorNames doesn't support FullTextSearch constraints. The constraint type FullTextSearch is missing in the tested types in org.apache.jackrabbit.core.query.lucene.join.ConstraintSplitter.getSelectorNames(Constraint) method. Submitting a QOM query with a full-text constraint throws a javax.jcr.UnsupportedRepositoryOperationException, while the repository reports supporting such queries : session.getRepository().getDescriptorValue(Repository.QUERY_FULL_TEXT_SEARCH_SUPPORTED).getBoolean() returns TRUE.

Typical stack trace :

javax.jcr.UnsupportedRepositoryOperationException: Unknown constraint type: CONTAINS(r.[jcr:title], 'REGA -APA')
	org.apache.jackrabbit.core.query.lucene.join.ConstraintSplitter.getSelectorNames(ConstraintSplitter.java:177)
	org.apache.jackrabbit.core.query.lucene.join.ConstraintSplitter.getSelectorNames(ConstraintSplitter.java:195)
	org.apache.jackrabbit.core.query.lucene.join.ConstraintSplitter.getSelectorNames(ConstraintSplitter.java:157)
	org.apache.jackrabbit.core.query.lucene.join.ConstraintSplitter.split(ConstraintSplitter.java:106)
	org.apache.jackrabbit.core.query.lucene.join.ConstraintSplitter.split(ConstraintSplitter.java:104)
	org.apache.jackrabbit.core.query.lucene.join.ConstraintSplitter.<init>(ConstraintSplitter.java:80)
	org.apache.jackrabbit.core.query.lucene.join.QueryEngine.execute(QueryEngine.java:162)
	org.apache.jackrabbit.core.query.lucene.join.QueryEngine.execute(QueryEngine.java:147)
	org.apache.jackrabbit.core.query.QueryObjectModelImpl.execute(QueryObjectModelImpl.java:114)"
1,"Unable to set LockFactory implementation via ${org.apache.lucene.store.FSDirectoryLockFactoryClass}. While trying to move from Lucene 2.0 to Lucene 2.1 I noticed a problem with the LockFactory instantiation code.
During previous tests we successfully specified the LockFactory implementation by setting the property
${org.apache.lucene.store.FSDirectoryLockFactoryClass} to ""org.apache.lucene.store.NativeFSLockFactory"".
This does no longer work due to a bug in the FSDirectory class. The problem is caused from the fact that this
class tries to invoke the default constructor of the specified LockFactory class. However neither NativeFSLockFactory
nor SimpleFSLockFactory do have a default constructor.

FSDirectory, Line 285:
          try {
            lockFactory = (LockFactory) c.newInstance();          
          } catch (IllegalAccessException e) {
            throw new IOException(""IllegalAccessException when instantiating LockClass "" + lockClassName);
          } catch (InstantiationException e) {
            throw new IOException(""InstantiationException when instantiating LockClass "" + lockClassName);
          } catch (ClassCastException e) {
            throw new IOException(""unable to cast LockClass "" + lockClassName + "" instance to a LockFactory"");
          }

A possible workaround is to not set the property at all and call FSDirectory.setLockFactory(...) instead. "
1,"Setting Query.setOffset() passed the results total returns negative getSize() instead of zero. 1. Have a query that returns 3 results
2. Now set Query.setOffset(10) (passed the total of 3)
3. Row/NodeIterator.getSize() returns -7 (incorrect)

Expected: getSize() should return 0"
1,"Session.getAttributes( ) call always returns an empty array. Repository repository = new RMIRemoteRepository(""//localhost:1099/jackrabbit.repository"");
SimpleCredentials c = new SimpleCredentials(""alex"",""ok"".toCharArray());
c.setAttribute(""anAttribute"", ""aValue"");
Session s = repository.login(c, ""aWorkspace"");
String[]attr=s.getAttributeNames();

array attr is empty.
according to docs it should contains attributes from the SimpleCredentials object."
1,"Highlighting overlapping tokens outputs doubled words. If for the text ""the fox did not jump"" we generate following tokens :
(the, 0, 0-3),({fox},0,0-7),(fox,1,4-7),(did,2,8-11),(not,3,12,15),(jump,4,16,18)

If TermVector for field is stored WITH_OFFSETS and not WITH_POSITIONS_OFFSETS, highlighing would output
""the<em>the fox</em> did not jump""

I join a patch with 2 additive JUnit tests and a fix of TokenSources class where token ordering by offset did'nt manage well overlapping tokens.
"
1,"DateUtil.formatDate() uses default timezone instead of GMT. DateUtil.formatDate() uses default timezone instead of GMT.  In section 3.3.1,
RFC 2616 states:  ""All HTTP date/time stamps MUST be represented in Greenwich
Mean Time (GMT), without exception.""

To reproduce, run the following snippet:

   public static void main(String[] args) {
      TimeZone tz = TimeZone.getTimeZone(""GMT"");
      GregorianCalendar gc = new GregorianCalendar(tz);
      gc.set(1900 + 104, GregorianCalendar.JANUARY, 1, 0, 0, 0);
      System.out.println(DateUtil.formatDate(gc.getTime()));
      
   }

Expected result:
Thu, 01 Jan 2004 00:00:00 GMT

Actual result (if your default timezone is PST):
Wed, 31 Dec 2003 16:00:00 PST"
1,ItemManager.toString() causes StackOverflowError. 
1,"Field.setValue(...) doesn't properly handle switching between byte[] and other types. This came up in PyLucene testing, based on Lucene 2.4.1.  Thread here:

  http://pylucene.markmail.org/message/75jzxzqi3smp2s4z

The problem is that Field.setValue does not fix up the isBinary
boolean, so if you create a String field, and then do
setValue(byte[]), you'll get an exception when adding a document
containing that field to the index."
1,MemoryFileSystem.deleteFolder deletes all folders starting with this name . MemoryFileSystem.deleteFolder deletes not only the folder requested but all folders that start with the given name.
1,"PROPPATCH in simple webdav server failing with 403 Forbidden error. 
i've configured the import-collection chain (called via MKCOL) to add nodes with the node type ""dav:collection"", which is defined as such:

NodeTypeName
  dav:collection
SuperTypes
  nt:folder
IsMixin
  false
HasOrderableChildNodes
  false
PrimaryItemName
  null
PropertyDef
  Name *
  RequiredType UNDEFINED
  DefaultValues null
  AutoCreate false
  Mandatory false
  OnParentVersion COPY
  Protected false
  Multiple false
PropertyDef
  Name *
  RequiredType UNDEFINED
  DefaultValues null
  AutoCreate false
  Mandatory false
  OnParentVersion COPY
  Protected false
  Multiple true

the idea is that i should be able to set arbitrary webdav properties (single- or multi-value) on a webdav collection.

when i use cadaver to mkcol a collection (creating a jcr node of type dav:collection) and then try to propset an arbitrary property on that collection (which as i understand it would set a jcr property of the same name on the dav:collection node), i get a 403 Forbidden error:

dav:/home/> mkcol hi
Creating `hi': succeeded.

dav:/home/> propset hi hi hi
Setting property on `hi': failed:
403 Forbidden

it's not clear to me if i'm using cadaver incorrectly, misunderstanding the PROPPATCH implementation, or both :) "
1,"addIndexes unexpectedly closes index. It seems that in 1.4rc2, a call to IndexWriter.addIndexes (IndexReader[]) will
close the provided IndexReader; in 1.3-final this does not happen.  So my code
which uses addIndexes to merge new information into an index and then calls
close() on the IndexReader now crashes with an ""already closed"" exception.  I
can attach test code which works in 1.3 but not in 1.4rc2 if that would be helpful.

If this is an intentional change in behavior, it needs to be documented.  Thanks!"
1,NullPointerException when using ancestor axis in indexing configuration. Happens when there is an index-rule that matches the root node type and has a condition that uses the ancestor axis.
1,"missing sync in InternalVersionManagerImpl.externalUpdate can cause ConcurrentModificationException. In

        for (Map.Entry<ItemId, InternalVersionItem> entry : versionItems.entrySet()) {
            if (changes.has(entry.getKey())) {
                items.add(entry.getValue());
            }
        }

we need to sync on versionItems, I believe."
1,"TrecDocMaker skips over documents when ""Date"" is missing from documents. TrecDocMaker skips over Trec documents if they do not have a ""Date"" line. When such a document is encountered, the code may skip over several documents until the next tag that is searched for is found.
The result is, instead of reading ~25M documents from the GOV2 collection, the code reads only ~23M (don't remember the actual numbers).

The fix adds a terminatingTag to read() such that the code looks for prefix, but only until terminatingTag is found. Appropriate changes were made in getNextDocData().

Patch to follow"
1,"double encoding of URLs. In HttpMethodBase.generateRequestLine(HttpConnection connection, String name, 
String reqPath, String qString, String protocol)

the path is always encoded using URIUtil.encode(reqPath,URIUtil.pathSafe()). 
However, if the path already contains an encoding space, i.e. %20, the % will 
be encoded again, so we get %2520. This behavior is not correct. We shouldn't 
encode any % signs."
1,"Cookies with names containing blanks or starting with $ should be rejected by RFC2109 spec only. Reported by John Patterson:

> The Cookie class does not like names with spaces in them.  It throws an
> IllegalArgumentException.  Unfortunately the server that my app interacts
> with uses a space in the cookie name.  Both IE and Mozilla don't mind."
1,"VersionManager lock not released in some circumstances. In some circumstances it is possible that lock is not released in VersionManager.checkin method.

There is following block of code in checkin nethod :

        aquireReadLock();
        try {
              .....
        } catch (IllegalStateException e) {
            releaseReadLock();
            throw new RepositoryException(""Unable to start edit operation."");
        }

Lock release should be in finally block to make sure that lock is released when unexpected exception is thrown.
In our environment we are getting NPE in mentioned block of code, it results in persisten lock. 
No versioning operation is possible and our application server is running ot of threads (all threads are locked).
"
1,WebDAV: LocatorFactoryImplEx doesn't properly evaluate resource path.... ... if the item path happens to start with the workspace name.
1,"Digest auth uses wrong uri in proxy authentication. I'm having a problem getting httpclient-rc1 to authenticate using
digest to our IAS server.  I've tried upgrading to rc3 without any
effect.  I also got our IT guys to upgrade IAS without luck.  I was
also able to have the GET method work under IAS and CONNECT to work
with a couple other proxy servers.  After examining ethereal logs for
my (commons) code and firefox to the same URLs I noticed that the
value for the ""uri"" setting in the ""Proxy-Authorization"" header was
the only significant difference.  After looking at RFC 2617 I noticed
that in section 3.2.2 (The Authorization Request Header) it states:

digest-uri
The URI from Request-URI of the Request-Line; duplicated here because
proxies are allowed to change the Request-Line in transit.

A re-examination of the headers showed that firefox was matching the Request-URI
with the digest-uri but that httpclient was not.  I reproduced partial headers
below.  I tried modifying the RC3 source to produce a hard-coded value for ""uri""
and demonstrated that it would successfully authenticate to that URI.  I also
checked that authentication would fail to any other URI and it did.

partial httpclient header (fails with 407):
CONNECT gmail.google.com:443 HTTP/1.1
Proxy-Authorization: Digest username=""proxytest"", realm=""Digest"",
nonce=""503902c343c8c501057a85cea6bad2734378fb44b4cbd1970bf320637871dae85373082cf70ac254"",
uri=""/"", response=""7717d0738332a3d8e83e9102b5ead6b9"", qop=""auth"", nc=00000001,
cnonce=""583aa0469b31290dc2acd7ec6cfc98f1"", algorithm=""MD5-sess"",
opaque=""bb319760fce84856e5648d3536502d81""

partial firefox header (succeeds with 200):
CONNECT mail1.combrio.local:443 HTTP/1.1
Proxy-Authorization: Digest username=""proxytest"", realm=""Digest"",
nonce=""0e61fe645ec8c5015aa3afe8cfe5219488ed473e277a8cddf8225ad66e74fd214f97d9d96ac99991"",
uri=""mail1.combrio.local:443"", algorithm=MD5-sess,
response=""bfac109287273e867531170475172ccf"",
opaque=""70cb2a1533b85882d0f1aa1e2ad1fbae"", qop=auth, nc=00000001,
cnonce=""b41aecd6e527e774"""
1,PdfTextExtractor does not close temp file in case of an error. If PDF parsing fails in PDFParser.parse() a temp file is not closed and results in an open file handle.
1,"StandardTokenizer splits host names with hyphens into multiple tokens. 
StandardTokenizer does not recognize host names with hyphens as a single HOST token. Specifically ""www.m-w.com"" is tokenized as ""www.m"" and ""w.com"", both of ""<HOST>"" type.

StandardTokenizer should instead output a single HOST token for ""www.m-w.com"", since hyphens are a legitimate character in DNS host names.

We've a local fix to the grammar file which also required us to significantly simplify the NUM type to get the behavior we needed for host names.

here's a junit test for the desired behavior;

	public void testWithHyphens() throws Exception {
		final String host = ""www.m-w.com"";
		final StandardTokenizer tokenizer = new StandardTokenizer(
				new StringReader(host));
		final Token token = new Token();
		tokenizer.next(token);
		assertEquals(""<HOST>"", token.type());
		assertEquals(""www.m-w.com"", token.term());
	}

"
1,"HttpClient does not compile 'out of the box' in IBM's VisualAge IDE. This was observed with IBM VisualAge 3.5, which runs JDK1.2.2:

Importing the HTTPClient source code into the IDE brings up a
compilation error in 

org.apache.commons.httpclient.HttpMethodBase.

The initialization of ""private ResponseConsumedWatcher m_responseWatcher""
using an anyonymous inner class seems to cause some trouble. Implicated code:

private ResponseConsumedWatcher m_responseWatcher = new ResponseConsumedWatcher
() {
	public void responseConsumed() {
		responseBodyConsumed();
	}
};

The error message is: ""Field initialization: The constructor invoked to create
org.apache.commons.httpclient.HttpMethodBase$1 with arguments () is not defined""

...but only in the context of HttpMethodBase(String uri) constructor, i.e.
the HttpMethodBase() constructor *can* be compiled, HttpMethodBase(String uri)
*cannot* be compiled with error ""Cannot create constructor due to incorrect
field initialization"".

I interpret this to mean that the compiler is looking for a parameterless
constructor for the anonymous class in the context of 
HttpMethodBase(String uri). The message did not really make sense to me. 
Checked the syntax, checked in the Language Definition whether setting up an
anonymous class like that is permitted; found nothing obviously wrong.

Fix:

The code above is equivalent to constructing the instance at the beginning
of each constructor of the enclosing class. A copy and paste of the
construction code into each of the two constructors fixes things...until
the next update."
1,"ItemInfoCacheImpl.getNodeInfo() and .getPropertyInfo() might not clear all relevant entries. ItemInfoCacheImpl.getNodeInfo() and .getPropertyInfo() remove the retrieved entry from the cache.

since entries might be cached by id AND path, entires identified by path are not removed from the cache if they're retrieved by id."
1,FastVectorHighlighter: latter terms cannot be highlighted if two or more terms are concatenated. My customer found a bug in FastVectorHighlighter. I'm working for the fix. I'll post it as soon as possible. We hope the fix in 2.9.
1,"Protocol interceptors not called when executing CONNECT methods. When the DefaultRequestDirector tries to establish a route via a proxy to a https target, registered protocol interceptors aren't being called in the createTunnelToTarget method. "
1,"consistency check fails with derbypm if bundle size exceeds 32k. due to a 'problem' in derby DERBY-1486 interleaved reads on a bundle that is larger than about 32k results in an error:
  ERROR XJ073: The data in this BLOB or CLOB is no longer available.  
               The BLOB or CLOBs transaction may be committed, or its 
               connection is closed.

this issue was already addressed in JCR-1039 but not fixed for the consistency check."
1,"ParallelReader fails on deletes and on seeks of previously unused fields. In using ParallelReader I've hit two bugs:

1.  ParallelReader.doDelete() and doUndeleteAll() call doDelete() and doUndeleteAll() on the subreaders, but these methods do not set hasChanges.  Thus the changes are lost when the readers are closed.  The fix is to call deleteDocument() and undeleteAll() on the subreaders instead.

2.  ParallelReader discovers the fields in each subindex by using IndexReader.getFieldNames() which only finds fields that have occurred on at least one document.  In general a parallel index is designed with assignments of fields to sub-indexes and term seeks (including searches) may be done on any of those fields, even if no documents in a particular state of the index have yet had an assigned field.  Seeks/searches on fields that have not yet been indexed generated an NPE in ParallelReader's various inner class seek() and next() methods because fieldToReader.get() returns null on the unseen field.  The fix is to extend the add() methods to supply the correct list of fields for each subindex.

Patch that corrects both of these issues attached.
"
1,"Several bugs in last SVN commit. Just a few bugs in the last SVN commit, but since I work with it, i thought useful to mention them :
1) org.apache.jackrabbit.ocm.reflection.ReflectionUtils should handle Set --> just add it in defaultImplementation
2) in org.apache.jackrabbit.ocm.manager.collectionconverter.ManageableObjectsUtil.getManageableObjects, correct defaultImplementation test :
        		if (defaultImplementation == null)
        		{
        			throw new JcrMappingException(""No default implementation for the interface "" + manageableObjectsClass);

Thank you and keep up the good work!

Sincerely yours,

Stephane"
1,"QNodeTypeDefinitionImpl.getSerializablePropertyDefs() might return non serializable property definitions . QNodeTypeDefinitionImpl.getSerializablePropertyDefs() returns a set-version of the passed in parameter, irrespective of whether the property defs are serializable or not.

See http://markmail.org/thread/65ngqvyxnu4nn3su"
1,"Possible HttpClient codepage issue (ascii/ebcdic) on WebSphere z/OS. I am working with Cactus 1.4.1 on WebSphere NT and also WebSphere z/OS
(mainframe). The problem seems to be with HTTPClient. I have tried with the
nuild on the 7th December.

I am trying to get basic cactus servlet tests working on WebSphere z/OS.
Everything works fine through WebSphere NT, however, when the same application
is deployed to WebSphere z/OS then we get the following error:

<?xml version=""1.0"" encoding=""UTF-8"" ?><?xml-stylesheet type=""text/xsl""
href=""junit-noframes.xsl""?><testsuites><testsuite name=""TestCactusServlet""
tests=""1"" failures=""0"" errors=""1"" time=""10.184""><testcase name=""testNeal""
time=""10.182""><error message=""Error in parsing the status  line from the
response: unable to find line starting with &quot;HTTP/&quot;""
type=""org.apache.commons.httpclient.HttpRecoverableException"">org.apache.commons.httpclient.HttpRecoverableException:
Error in parsing the status  line from the response: unable to find line
starting with &quot;HTTP/&quot;
	at
org.apache.commons.httpclient.HttpMethodBase.readStatusLine(HttpMethodBase.java:1791)
	at
org.apache.commons.httpclient.HttpMethodBase.readResponse(HttpMethodBase.java:1559)
	at
org.apache.commons.httpclient.HttpMethodBase.processRequest(HttpMethodBase.java:2219)
	... etc ...

I have verified that the Application Server on WebSPhere z/OS is working fine. I
set the logging on the cactus to DEBUG and noticed that the data that the
HTTPClient is retrieving from the connection is scrambled in some way. For example:

16:06:20,213 [WebSphere t=009d7920] DEBUG ent.HttpClientConnectionHelper  -
>getCookieString = [null] 
16:06:20,317 [WebSphere t=009d7920] DEBUG httpclient.wire                 - >> ""@a??? etc...

On WebSphere NT the data at this point looks OK.

What springs to mind is maybe ascii/ebcdic conversion problem. z/OS uses unicode
 for java, as it should. However, the HTTPClient creates it own socket
connection to the app server and therefore it is connecting to non java code. In
such a situation codepage conversion is necessary.

Could anybody adsvise on how to get this to work?

Regards,

Neal Johnston-Ward"
1,"FSDirectory.copyBytes isn't safe for SimpleFSDirectory. the copyBytes optimization from LUCENE-2574 is not safe for SimpleFSDirectory, but works fine for NIOFSDirectory.

With SimpleFSDirectory, the copyBytes optimization causes index corruption.

see http://www.lucidimagination.com/search/document/36d2dbfc691909d5/bug_triggered_by_testindexwriter_testrandomstoredfields for background

here are my steps to reproduce (most of the time, at least on windows):
{noformat}
1. edit line 87 of TestIndexWriter to plugin the seed:
    random = newRandom(3312389322103990899L);
2. edit line 5138 of TestIndexWriter to force SimpleFSDirectory:
    Directory dir = new SimpleFSDirectory(index);
3. run this command:
    ant clean test-core -Dtestcase=TestIndexWriter
-Dtestmethod=testRandomStoredFields -Dtests.iter=10
-Dtests.codec=""MockVariableIntBlock(29)""
{noformat}
"
1,"SSL + proxy + Host auth + Keep Alive off causes an infinite loop in HttpMethodDirector. The combination of SSL tunnelling, host authentication, and disabled persistent
connection support (HTTPD KeepAlive off) causes an infinite loop in
HttpMethodDirector. 

The problem has been reported on the httpclient-dev list by Rindress MacDonald
<RMacDona at enterasys.com>"
1,"BooleanQuery explain with boost==0. BooleanWeight.explain() uses the returned score of subweights to determine if a clause matched.
If any required clause has boost==0, the returned score will be zero and the explain for the entire BooleanWeight will be simply  Explanation(0.0f, ""match required"").

I'm not sure what the correct fix is here.  I don't think it can be done based on score alone, since that isn't how scorers work.   Perhaps we need a new method ""boolean Explain.matched()"" that returns true on a match, regardless of what the score may be? 

Related to the problem above, even if no boosts are zero, it it sometimes nice to know *why* a particular query failed to match.  It would mean a longer explanation, but maybe we should include non matching explains too?"
1,"WriteLineDocTask incorrectly normalizes fields. WriteLineDocTask normalizes the body, title and date fields by replacing any ""\t"" with a space. However, if any one of them contains newlines, LineDocMaker will fail, since the first line read will include some of the text, however the second line, which it now expects to be a new document, will include other parts of the text.

I don't know how we didn't hit it so far. Maybe the wikipedia text doesn't have such lines, however when I ran over the TREC collection I hit a lot of those.

I will attach a patch shortly."
1,"TimeLimitingCollector's TimeExceededException contains useless relative docid. We found another bug with the RandomIndexWriter: When TimeLimitingCollector breaks collection after timeout, it records the last/next collected docid. It does this without rebasing, so the docid is useless. TestTimeLimitingCollector checks the docid, but correctly rebases it (as only this makes sense). Because the RandomIndexWriter uses different merge settings, the index is now sometimes not optimized and so the test fails (which is correct, as the docid is useless for non-optimized index).

Attached is a patch that fixes this. Please tell me if I should backport to 2.9 and 3.0!"
1,"Webdav: creating resource in case of RepositoryException. if accessing item fails for any other reason than PathNotFoundException, creating
the resource should rather fail (throwing 403).

(reported by brian)"
1,"DistanceFilter problem with deleted documents. I know this is the locallucene lib, but wanted to make sure we don't get this bug when it gets into lucene contrib.

I suspect that the issue is that deleted documents are trying to be evaluated by the filter.  I did some debugging and I confirmed that it is bombing on a document that is marked as deleted (using Luke).


Thanks!

Using the locallucene library 1.51, I get a NullPointerException at line 123 of DistanceFilter
The method is 	public BitSet bits(IndexReader reader) 
The line is double x = NumberUtils.SortableStr2double(sx);

The stack trace is:
java.lang.NullPointerException
	at org.apache.solr.util.NumberUtils.SortableStr2long(NumberUtils.java:149)
	at org.apache.solr.util.NumberUtils.SortableStr2double(NumberUtils.java:104)
	at com.pjaol.search.geo.utils.DistanceFilter.bits(DistanceFilter.java:123)
	at org.apache.lucene.search.Filter.getDocIdSet(Filter.java:49)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:140)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:112)
	at org.apache.lucene.search.Hits.getMoreDocs(Hits.java:113)
	at org.apache.lucene.search.Hits.<init>(Hits.java:90)
	at org.apache.lucene.search.Searcher.search(Searcher.java:72)"
1,"Under heavy load, database journal may contain empty update records.. In a clustering scenario with a database journal, empty records may be produced. A passive node will then throw the following exception during its synchronization: 

28.02.2007 08:34:11 *ERROR* ClusterNode: Unexpected error while syncing of journal: null (ClusterNode.java, line 260)
java.lang.NullPointerException
	at java.io.FilterInputStream.close(Unknown Source)
	at org.apache.jackrabbit.core.journal.ReadRecord.close(ReadRecord.java:197)
	at org.apache.jackrabbit.core.journal.DatabaseRecordIterator.close(DatabaseRecordIterator.java:148)
	at org.apache.jackrabbit.core.journal.DatabaseRecordIterator.close(DatabaseRecordIterator.java:114)
	at org.apache.jackrabbit.core.journal.AbstractJournal.doSync(AbstractJournal.java:196)
	at org.apache.jackrabbit.core.journal.AbstractJournal.sync(AbstractJournal.java:165)
	at org.apache.jackrabbit.core.journal.ClusterNode.sync(ClusterNode.java:283)
	at org.apache.jackrabbit.core.journal.ClusterNode.run(ClusterNode.java:254)
	at java.lang.Thread.run(Unknown Source)"
1,"WriteLineDocTask should keep docs w/ just title and no body. WriteLineDocTask throws away a document if it does not have a body element. However, if the document has a title, then it should be kept. Some documents, such as emails, may not have a body which is legitimate. I'll post a patch + a test case."
1,"Merging between workspaces fails. I have setup 2 workspaces in Jackrabbit.  I have a preview and a production
workspace.  These workspaces keep a tree of menu nodes that can have content
associated to those menus.  Each node is of type nt:unstructured and has
mixin types of versionable, lockable, and referenceable.

In our system you are only allowed to edit nodes in the preview workspace.
So what I do is when you go to edit a node we check it out, allow for edits,
then check it in.  This creates a new version on the node.  Then we merge
the node up to the production workspace.  All nodes in the production
workspace are always checked in and not locked.

When I go to do a merge I run into problems when I try to merge a node that
has children.  Lets say I have node A with children B and C.  These all have
the same node types as stated above.  I make a change to a property in Node
A in the preview workspace and now want to merge it into the production
workspace (where it exists already).  Here is the code that is run:

Node destNode = destSession.getNodeByUUID(getUUID());
NodeIterator ni = destNode.merge(""preview"", true);

Now this fails in the ItemImpl.internalRemove() method with a
VersionException of cannot remove a child of a checked-in node.  Here is the
trace for the error:
at org.apache.jackrabbit.core.ItemImpl.internalRemove(ItemImpl.java:848)
at org.apache.jackrabbit.core.NodeImpl.internalMerge(NodeImpl.java:3693)
at org.apache.jackrabbit.core.NodeImpl.internalMerge(NodeImpl.java:3587)
at org.apache.jackrabbit.core.NodeImpl.merge(NodeImpl.java:3003)

Now if I understand correctly when doing a merge the node that you are
trying to merge to needs to be older then the source node and the
destination node cannot be checked out (NodeImpl.doMergeTest() is where I
figured that out).  But then when I step through further in the merge in
NodeImpl it gets all the nodes of the src node and retrieves the same
children in the destination workspace and then tries to remove those
destination children but it can't remove those children b/c the parent node
(which is node A in the production workspace) is not checked out, but
according to the mergeTest it can't be checked out or the merge won't even
begin."
1,"StaleItemStateException during distributed transaction. We use the Jackrabbit JCA Component within a Weblogic 10.3 Application Server with distributed transactions between an Oracle Database an the Jackrabbit JCA.

Updating a node property multiple times in a transaction results in a XAException. Root cause seems to be a StaleItemStateException (see Stack-Trace).
Googling revealed, that a similar bug was fixed for Jackrabbit 1.5.3. Looking through the code showed, that the proposed fix in JCR-1554 seems not to be applied on Jackrabbit 2.0 (tag and trunk).

I tried to apply the proposed fix on the trunk code base, but this seemed not to help.

Stack-Trace:
javax.ejb.TransactionRolledbackLocalException: Error committing transaction:; nested exception is: javax.transaction.xa.XAException                                                                                             
        at weblogic.ejb.container.internal.EJBRuntimeUtils.throwTransactionRolledbackLocal(EJBRuntimeUtils.java:238)                                                                                                            
        at weblogic.ejb.container.internal.EJBRuntimeUtils.throwEJBException(EJBRuntimeUtils.java:133)                                                                                                                          
        at weblogic.ejb.container.internal.BaseLocalObject.postInvoke1(BaseLocalObject.java:623)                                                                                                                                
        at weblogic.ejb.container.internal.BaseLocalObject.postInvokeTxRetry(BaseLocalObject.java:424)                                                                                                                          
        at ch.ejpd.sireneit.facade.ejb.ablage.DokumentFacadeBean_7xdnsq_DokumentFacadeImpl.updateStructuredDokument(DokumentFacadeBean_7xdnsq_DokumentFacadeImpl.java:340)                                                      
        at ch.ejpd.sireneit.access.rest.ablage.DokumentResource.update(DokumentResource.java:453)                                                                                                                               
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)                                                                                                                                                          
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)                                                                                                                                        
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)                                                                                                                                
        at java.lang.reflect.Method.invoke(Method.java:597)                                                                                                                                                                     
        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:175)                                                
        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:67)                                                                                         
        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:208)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:109)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:109)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:109)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:75)                                                                                                                             
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:67)                                                                                                                 
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:724)                                                                                                                
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:689)                                                                                                                 
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:680)                                                                                                                 
        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:324)                                                                                                                                     
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:425)                                                                                                                             
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:604)                                                                                                                             
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)                                                                                                                                                         
        at weblogic.servlet.internal.StubSecurityHelper$ServletServiceAction.run(StubSecurityHelper.java:227)                                                                                                                   
        at weblogic.servlet.internal.StubSecurityHelper.invokeServlet(StubSecurityHelper.java:125)                                                                                                                              
        at weblogic.servlet.internal.ServletStubImpl.execute(ServletStubImpl.java:292)                                                                                                                                          
        at weblogic.servlet.internal.TailFilter.doFilter(TailFilter.java:26)                                                                                                                                                    
        at weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:42)                                                                                                                                          
        at ch.ejpd.lib.webclient.jfa.JfaTokenServletFilter.doFilter(JfaTokenServletFilter.java:108)                                                                                                                             
        at weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:42)                                                                                                                                          
        at weblogic.servlet.internal.WebAppServletContext$ServletInvocationAction.run(WebAppServletContext.java:3496)                                                                                                           
        at weblogic.security.acl.internal.AuthenticatedSubject.doAs(AuthenticatedSubject.java:321)                                                                                                                              
        at weblogic.security.service.SecurityManager.runAs(Unknown Source)                                                                                                                                                      
        at weblogic.servlet.internal.WebAppServletContext.securedExecute(WebAppServletContext.java:2180)                                                                                                                        
        at weblogic.servlet.internal.WebAppServletContext.execute(WebAppServletContext.java:2086)                                                                                                                               
        at weblogic.servlet.internal.ServletRequestImpl.run(ServletRequestImpl.java:1406)                                                                                                                                       
        at weblogic.work.ExecuteThread.execute(ExecuteThread.java:201)                                                                                                                                                          
        at weblogic.work.ExecuteThread.run(ExecuteThread.java:173)                                                                                                                                                              
javax.transaction.xa.XAException                                                                                                                                                                                                
        at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:171)                                                                                                                                   
        at org.apache.jackrabbit.core.XASessionImpl.commit(XASessionImpl.java:346)                                                                                                                                              
        at org.apache.jackrabbit.jca.TransactionBoundXAResource.commit(TransactionBoundXAResource.java:39)                                                                                                                      
        at weblogic.connector.security.layer.AdapterLayer.commit(AdapterLayer.java:252)                                                                                                                                         
        at weblogic.connector.transaction.outbound.XAWrapper.commit(XAWrapper.java:113)                                                                                                                                         
        at weblogic.transaction.internal.XAServerResourceInfo.commit(XAServerResourceInfo.java:1334)                                                                                                                            
        at weblogic.transaction.internal.XAServerResourceInfo.commit(XAServerResourceInfo.java:578)                                                                                                                             
        at weblogic.transaction.internal.ServerSCInfo.startCommit(ServerSCInfo.java:547)                                                                                                                                        
        at weblogic.transaction.internal.ServerTransactionImpl.localCommit(ServerTransactionImpl.java:2006)                                                                                                                     
        at weblogic.transaction.internal.ServerTransactionImpl.globalRetryCommit(ServerTransactionImpl.java:2723)                                                                                                               
        at weblogic.transaction.internal.ServerTransactionImpl.globalCommit(ServerTransactionImpl.java:2645)                                                                                                                    
        at weblogic.transaction.internal.ServerTransactionImpl.internalCommit(ServerTransactionImpl.java:282)                                                                                                                   
        at weblogic.transaction.internal.ServerTransactionImpl.commit(ServerTransactionImpl.java:230)                                                                                                                           
        at weblogic.ejb.container.internal.BaseLocalObject.postInvoke1(BaseLocalObject.java:591)                                                                                                                                
        at weblogic.ejb.container.internal.BaseLocalObject.postInvokeTxRetry(BaseLocalObject.java:424)                                                                                                                          
        at ch.ejpd.sireneit.facade.ejb.ablage.DokumentFacadeBean_7xdnsq_DokumentFacadeImpl.updateStructuredDokument(DokumentFacadeBean_7xdnsq_DokumentFacadeImpl.java:340)                                                      
        at ch.ejpd.sireneit.access.rest.ablage.DokumentResource.update(DokumentResource.java:453)                                                                                                                               
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)                                                                                                                                                          
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)                                                                                                                                        
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)                                                                                                                                
        at java.lang.reflect.Method.invoke(Method.java:597)                                                                                                                                                                     
        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:175)                                                
        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:67)                                                                                         
        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:208)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:109)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:109)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:109)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:75)                                                                                                                             
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:67)                                                                                                                 
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:724)                                                                                                                
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:689)                                                                                                                 
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:680)                                                                                                                 
        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:324)                                                                                                                                     
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:425)                                                                                                                             
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:604)                                                                                                                             
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)                                                                                                                                                         
        at weblogic.servlet.internal.StubSecurityHelper$ServletServiceAction.run(StubSecurityHelper.java:227)                                                                                                                   
        at weblogic.servlet.internal.StubSecurityHelper.invokeServlet(StubSecurityHelper.java:125)                                                                                                                              
        at weblogic.servlet.internal.ServletStubImpl.execute(ServletStubImpl.java:292)                                                                                                                                          
        at weblogic.servlet.internal.TailFilter.doFilter(TailFilter.java:26)                                                                                                                                                    
        at weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:42)                                                                                                                                          
        at ch.ejpd.lib.webclient.jfa.JfaTokenServletFilter.doFilter(JfaTokenServletFilter.java:108)                                                                                                                             
        at weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:42)                                                                                                                                          
        at weblogic.servlet.internal.WebAppServletContext$ServletInvocationAction.run(WebAppServletContext.java:3496)                                                                                                           
        at weblogic.security.acl.internal.AuthenticatedSubject.doAs(AuthenticatedSubject.java:321)                                                                                                                              
        at weblogic.security.service.SecurityManager.runAs(Unknown Source)                                                                                                                                                      
        at weblogic.servlet.internal.WebAppServletContext.securedExecute(WebAppServletContext.java:2180)                                                                                                                        
        at weblogic.servlet.internal.WebAppServletContext.execute(WebAppServletContext.java:2086)                                                                                                                               
        at weblogic.servlet.internal.ServletRequestImpl.run(ServletRequestImpl.java:1406)                                                                                                                                       
        at weblogic.work.ExecuteThread.execute(ExecuteThread.java:201)                                                                                                                                                          
        at weblogic.work.ExecuteThread.run(ExecuteThread.java:173)                                                                                                                                                              
org.apache.jackrabbit.core.TransactionException: Unable to prepare transaction.                                                                                                                                                 
        at org.apache.jackrabbit.core.state.XAItemStateManager.prepare(XAItemStateManager.java:169)                                                                                                                             
        at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:154)                                                                                                                                   
        at org.apache.jackrabbit.core.XASessionImpl.commit(XASessionImpl.java:346)                                                                                                                                              
        at org.apache.jackrabbit.jca.TransactionBoundXAResource.commit(TransactionBoundXAResource.java:39)                                                                                                                      
        at weblogic.connector.security.layer.AdapterLayer.commit(AdapterLayer.java:252)                                                                                                                                         
        at weblogic.connector.transaction.outbound.XAWrapper.commit(XAWrapper.java:113)                                                                                                                                         
        at weblogic.transaction.internal.XAServerResourceInfo.commit(XAServerResourceInfo.java:1334)                                                                                                                            
        at weblogic.transaction.internal.XAServerResourceInfo.commit(XAServerResourceInfo.java:578)                                                                                                                             
        at weblogic.transaction.internal.ServerSCInfo.startCommit(ServerSCInfo.java:547)                                                                                                                                        
        at weblogic.transaction.internal.ServerTransactionImpl.localCommit(ServerTransactionImpl.java:2006)                                                                                                                     
        at weblogic.transaction.internal.ServerTransactionImpl.globalRetryCommit(ServerTransactionImpl.java:2723)                                                                                                               
        at weblogic.transaction.internal.ServerTransactionImpl.globalCommit(ServerTransactionImpl.java:2645)                                                                                                                    
        at weblogic.transaction.internal.ServerTransactionImpl.internalCommit(ServerTransactionImpl.java:282)                                                                                                                   
        at weblogic.transaction.internal.ServerTransactionImpl.commit(ServerTransactionImpl.java:230)                                                                                                                           
        at weblogic.ejb.container.internal.BaseLocalObject.postInvoke1(BaseLocalObject.java:591)                                                                                                                                
        at weblogic.ejb.container.internal.BaseLocalObject.postInvokeTxRetry(BaseLocalObject.java:424)                                                                                                                          
        at ch.ejpd.sireneit.facade.ejb.ablage.DokumentFacadeBean_7xdnsq_DokumentFacadeImpl.updateStructuredDokument(DokumentFacadeBean_7xdnsq_DokumentFacadeImpl.java:340)                                                      
        at ch.ejpd.sireneit.access.rest.ablage.DokumentResource.update(DokumentResource.java:453)                                                                                                                               
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)                                                                                                                                                          
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)                                                                                                                                        
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)                                                                                                                                
        at java.lang.reflect.Method.invoke(Method.java:597)                                                                                                                                                                     
        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:175)                                                
        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:67)                                                                                         
        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:208)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:109)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:109)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:109)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:75)                                                                                                                             
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:67)                                                                                                                 
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:724)                                                                                                                
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:689)                                                                                                                 
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:680)                                                                                                                 
        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:324)                                                                                                                                     
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:425)                                                                                                                             
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:604)                                                                                                                             
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)                                                                                                                                                         
        at weblogic.servlet.internal.StubSecurityHelper$ServletServiceAction.run(StubSecurityHelper.java:227)                                                                                                                   
        at weblogic.servlet.internal.StubSecurityHelper.invokeServlet(StubSecurityHelper.java:125)                                                                                                                              
        at weblogic.servlet.internal.ServletStubImpl.execute(ServletStubImpl.java:292)                                                                                                                                          
        at weblogic.servlet.internal.TailFilter.doFilter(TailFilter.java:26)                                                                                                                                                    
        at weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:42)                                                                                                                                          
        at ch.ejpd.lib.webclient.jfa.JfaTokenServletFilter.doFilter(JfaTokenServletFilter.java:108)                                                                                                                             
        at weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:42)                                                                                                                                          
        at weblogic.servlet.internal.WebAppServletContext$ServletInvocationAction.run(WebAppServletContext.java:3496)                                                                                                           
        at weblogic.security.acl.internal.AuthenticatedSubject.doAs(AuthenticatedSubject.java:321)                                                                                                                              
        at weblogic.security.service.SecurityManager.runAs(Unknown Source)                                                                                                                                                      
        at weblogic.servlet.internal.WebAppServletContext.securedExecute(WebAppServletContext.java:2180)                                                                                                                        
        at weblogic.servlet.internal.WebAppServletContext.execute(WebAppServletContext.java:2086)                                                                                                                               
        at weblogic.servlet.internal.ServletRequestImpl.run(ServletRequestImpl.java:1406)                                                                                                                                       
        at weblogic.work.ExecuteThread.execute(ExecuteThread.java:201)                                                                                                                                                          
        at weblogic.work.ExecuteThread.run(ExecuteThread.java:173)                                                                                                                                                              
org.apache.jackrabbit.core.state.StaleItemStateException: e1863ec3-4eb7-483b-b1db-7586c089bc64/{}To has been modified externally                                                                                                
        at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:653)                                                                                                                
        at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:1110)                                                                                                                
        at org.apache.jackrabbit.core.state.XAItemStateManager.prepare(XAItemStateManager.java:163)                                                                                                                             
        at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:154)                                                                                                                                   
        at org.apache.jackrabbit.core.XASessionImpl.commit(XASessionImpl.java:346)                                                                                                                                              
        at org.apache.jackrabbit.jca.TransactionBoundXAResource.commit(TransactionBoundXAResource.java:39)                                                                                                                      
        at weblogic.connector.security.layer.AdapterLayer.commit(AdapterLayer.java:252)                                                                                                                                         
        at weblogic.connector.transaction.outbound.XAWrapper.commit(XAWrapper.java:113)                                                                                                                                         
        at weblogic.transaction.internal.XAServerResourceInfo.commit(XAServerResourceInfo.java:1334)                                                                                                                            
        at weblogic.transaction.internal.XAServerResourceInfo.commit(XAServerResourceInfo.java:578)                                                                                                                             
        at weblogic.transaction.internal.ServerSCInfo.startCommit(ServerSCInfo.java:547)                                                                                                                                        
        at weblogic.transaction.internal.ServerTransactionImpl.localCommit(ServerTransactionImpl.java:2006)                                                                                                                     
        at weblogic.transaction.internal.ServerTransactionImpl.globalRetryCommit(ServerTransactionImpl.java:2723)                                                                                                               
        at weblogic.transaction.internal.ServerTransactionImpl.globalCommit(ServerTransactionImpl.java:2645)                                                                                                                    
        at weblogic.transaction.internal.ServerTransactionImpl.internalCommit(ServerTransactionImpl.java:282)                                                                                                                   
        at weblogic.transaction.internal.ServerTransactionImpl.commit(ServerTransactionImpl.java:230)                                                                                                                           
        at weblogic.ejb.container.internal.BaseLocalObject.postInvoke1(BaseLocalObject.java:591)                                                                                                                                
        at weblogic.ejb.container.internal.BaseLocalObject.postInvokeTxRetry(BaseLocalObject.java:424)                                                                                                                          
        at ch.ejpd.sireneit.facade.ejb.ablage.DokumentFacadeBean_7xdnsq_DokumentFacadeImpl.updateStructuredDokument(DokumentFacadeBean_7xdnsq_DokumentFacadeImpl.java:340)                                                      
        at ch.ejpd.sireneit.access.rest.ablage.DokumentResource.update(DokumentResource.java:453)                                                                                                                               
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)                                                                                                                                                          
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)                                                                                                                                        
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)                                                                                                                                
        at java.lang.reflect.Method.invoke(Method.java:597)                                                                                                                                                                     
        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:175)                                                
        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:67)                                                                                         
        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:208)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:109)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:109)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:109)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:75)                                                                                                                             
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:67)                                                                                                                 
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:724)                                                                                                                
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:689)                                                                                                                 
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:680)                                                                                                                 
        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:324)                                                                                                                                     
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:425)                                                                                                                             
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:604)                                                                                                                             
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)                                                                                                                                                         
        at weblogic.servlet.internal.StubSecurityHelper$ServletServiceAction.run(StubSecurityHelper.java:227)                                                                                                                   
        at weblogic.servlet.internal.StubSecurityHelper.invokeServlet(StubSecurityHelper.java:125)                                                                                                                              
        at weblogic.servlet.internal.ServletStubImpl.execute(ServletStubImpl.java:292)                                                                                                                                          
        at weblogic.servlet.internal.TailFilter.doFilter(TailFilter.java:26)                                                                                                                                                    
        at weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:42)                                                                                                                                          
        at ch.ejpd.lib.webclient.jfa.JfaTokenServletFilter.doFilter(JfaTokenServletFilter.java:108)                                                                                                                             
        at weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:42)                                                                                                                                          
        at weblogic.servlet.internal.WebAppServletContext$ServletInvocationAction.run(WebAppServletContext.java:3496)                                                                                                           
        at weblogic.security.acl.internal.AuthenticatedSubject.doAs(AuthenticatedSubject.java:321)                                                                                                                              
        at weblogic.security.service.SecurityManager.runAs(Unknown Source)                                                                                                                                                      
        at weblogic.servlet.internal.WebAppServletContext.securedExecute(WebAppServletContext.java:2180)                                                                                                                        
        at weblogic.servlet.internal.WebAppServletContext.execute(WebAppServletContext.java:2086)                                                                                                                               
        at weblogic.servlet.internal.ServletRequestImpl.run(ServletRequestImpl.java:1406)                                                                                                                                       
        at weblogic.work.ExecuteThread.execute(ExecuteThread.java:201)                                                                                                                                                          
        at weblogic.work.ExecuteThread.run(ExecuteThread.java:173)                                                                                                                                                              
>                                                                                                                                                                                                                               "
1,"BasicOperations.concatenate creates invariants. I started writing a test for LUCENE-2716, and i found a problem with BasicOperations.concatenate(Automaton, Automaton):
it creates automata with invariant representation (which should never happen, unless you manipulate states/transitions manually).

strangely enough the BasicOperations.concatenate(List<Automaton>) does not have this problem.
"
1,"MultiFields.getUniqueFieldCount is broken. this returns terms.size(), but terms is lazy-initted. So it wrongly returns 0.

Simplest fix would be to return -1."
1,"KeywordTokenizer does not set start/end offset of the Token it produces. I think just adding these two lines in the next(Token) method is the right fix:

           reusableToken.setStartOffset(0);
           reusableToken.setEndOffset(upto);

I don't think this is a back compat issue because the start/end offset are now meaningless since they will inherit whatever the reusable token had previously been used for."
1,Port fix for HTTPCLIENT-633 to 4.0. The fix for MultiThreadedHttpConnectionManager from HTTPCLIENT-633 should be ported to ThreadSafeClientConnManager in 4.0.
1,"HttpMethodBase#aborted variable mistakenly declared transient instead of volatile. HttpMethodBase#aborted variable mistakenly declared transient instead of volatile. This is quite nasty. 

Do we want to cut an emergency release (3.0.2) because of that or can this wait until 3.1-beta1?

Fix attached.

Oleg"
1,"JCR2SPI NamespaceRegistryImpl.unregisterNamespace passes prefix to storage when uri is expected. When trying to unregister a namespace through SPI, Jackrabbit throws a NamespaceException : <prefix>: is not a registered namespace uri.

javax.jcr.NamespaceRegistry.unregisterNamespace(String prefix) expects the namespace prefix. Though, org.apache.jackrabbit.jcr2spi.NamespaceRegistryImpl.unregisterNamespace(String prefix) calls directly org.apache.jackrabbit.jcr2spi.NamespaceStorage.unregisterNamespace(String uri), which expects the namespace uri.

The namespace registry should first retrieve the uri for the provided prefix."
1,"XPath QueryFormat may produce malformed XPath statement. When the query tree contains select properties *and* an order by clause, then the XPath QueryFormat will produce a malformed XPath statement.

E.g.:

//element(*, foo)/(@a|@b) order by @bar

round trips to:

//element(*, foo) order by @bar/(@a|@b)

"
1,"NTLM authentication failed due to closing of connection. Description:

When dealing with a NTLM proxy server that sends response back with lines:

14:51:27:750 << HTTP/1.0 407 Proxy Authentication Required
14:51:27:796 << Date: Mon, 14 Apr 2003 19:52:43GMT[\r][\n]
14:51:27:796 << Content-Length: 257[\r][\n]
14:51:27:796 << Content-Type: text/html[\r][\n]
14:51:27:796 << Server: NetCache appliance (NetApp/5.3.1R1)[\r][\n]
14:51:27:796 << Connection: keep-alive[\r][\n]
14:51:27:796 << Proxy-Authenticate: NTLM 
TlRMTVNTUAACAAAABgAGACgAAAAGggEAtOoNy4M0g0EAAAAAAAAAAEdMT0JBTA==[\r][\n]

The httpClient code is using the ""HTTP/1.0"" as clue for closing the connection 
and ignored the ""Connection: keep-alive"".  That caused the NTLM authentication 
to fail as the NTLM requires the response to the challenge to be sent back on 
the same connection.

Proposed Fix:

Our fix is to add a flag inProxyAuthenticationRetry (in HttpMethodBase) to 
indicate that the method is doing proxy authentication retry.  When the flag is 
true, in ""HttpMethodBase.shouldCloseConnection"", check the ""Connection: keep-
alive"" before determining to close the connection."
1,"[PATCH] npe if java.io.tmpdir does not exist. In org.apache.lucene.store.FSDirectory from Lucene-1.3-final, on line 170-171:

File tmpdir = new File(System.getProperty(""java.io.tmpdir""));
files = tmpdir.list();

if the directory specified by the property ""java.io.tmpdir"" does not exist, a
null pointer exception is thrown.  Perhaps a check to see if the directory
exists is in order, and if it doesn't, use a directory you know exists (e.g. a
/temp directory in the directory created earlier in the create() method)."
1,"SSLSocketFactory.connectSocket() possible NPE - or use of wrong variable?. SSLSocketFactory.connectSocket() has a possible NPE at line 324:

            sock.connect(remoteAddress, connTimeout);

Or perhaps this should really be:

            sslsock.connect(remoteAddress, connTimeout);"
1,"Thread safety issue can cause index corruption when autoCommit=true and multiple threads are committing. This is only present in 2.9 trunk, but has been there since
LUCENE-1516 was committed I believe.

It's rare to hit: it only happens if multiple calls to commit() are in
flight (from different threads) and where at least one of those calls
is due to a merge calling commit (because autoCommit is true).

When it strikes, it leaves the index corrupt because it incorrectly
removes an active segment.  It causes exceptions like this:
{code}
java.io.FileNotFoundException: _1e.fnm
	at org.apache.lucene.store.MockRAMDirectory.openInput(MockRAMDirectory.java:246)
	at org.apache.lucene.index.FieldInfos.<init>(FieldInfos.java:67)
	at org.apache.lucene.index.SegmentReader.initialize(SegmentReader.java:536)
	at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:468)
	at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:414)
	at org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:641)
	at org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:627)
	at org.apache.lucene.index.DocumentsWriter.applyDeletes(DocumentsWriter.java:923)
	at org.apache.lucene.index.IndexWriter.applyDeletes(IndexWriter.java:4987)
	at org.apache.lucene.index.IndexWriter.doFlushInternal(IndexWriter.java:4165)
	at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:4025)
	at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:4016)
	at org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:2077)
	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:2040)
	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:2004)
	at org.apache.lucene.index.TestStressIndexing2.indexRandom(TestStressIndexing2.java:210)
	at org.apache.lucene.index.TestStressIndexing2.testMultiConfig(TestStressIndexing2.java:104)
{code}

It's caused by failing to increment changeCount inside the same
synchronized block where segmentInfos was changed, in commitMerge.
The fix is simple -- I plan to commit shortly.
"
1,"Explicit VirtualHosts Can Cause Issues On Redirects. If you set an explicit virtual host then a getmethod may get stuck in a redirect
loop (up to maxRedirects).

e.g. execute a get on www.google.com (with a www.google.com virtualhost).  That
redirects to www.google.co.nz (at least if you come from an NZ IP).  The current
httpclient behavior is to then connect to www.google.co.nz but pass through,
with the request, ""Host: www.google.com"".  Google will then reply with another
www.google.co.nz redirect and the loop continues.

There are probably a few ways to work around this.  It seems reasonable to drop
an explicity set virtual host in the event a redirect redirects to a different
uri authority.  The following patch works for me:


diff -Naur
../../t2/commons-httpclient/src/java/org/apache/commons/httpclient/HttpMethodDirector.java
src/java/org/apache/commons/httpclient/HttpMethodDirector.java
---
../../t2/commons-httpclient/src/java/org/apache/commons/httpclient/HttpMethodDirector.java
2005-12-22 01:06:55.000000000 +1300
+++ src/java/org/apache/commons/httpclient/HttpMethodDirector.java	2005-12-22
19:09:51.000000000 +1300
@@ -605,6 +605,27 @@
 					redirectUri = new URI(currentUri, redirectUri);
 				}
 			}
+            do {
+                // scenario we're trying to avoid:
+                // virtual host is set (e.g. google.com); a request to that
server responds
+                // with a redirect to google.co.nz; we issue a request to
google.co.nz with 
+                // a virtual host request of google.com
+                // 
+                // This code will remove any set virtual host if the redirect
is to a different 
+                // domain
+
+                if(redirectUri.isRelativeURI()) {
+                    break;
+                }
+
+                String vhost = hostConfiguration.getParams().getVirtualHost();
+                if(vhost==null)
+                    break;
+                if(redirectUri.getAuthority()!=currentUri.getAuthority()) {
+                    hostConfiguration.getParams().setVirtualHost(null);
+                }
+            } while(false);
+"
1,"Retry on ConnectionException does not work. I noticed that the Retry handler mechanism does not work when the client cannot
initiate a connection (which throws java.net.ConnectionException). This happens
for me for instance when there is proxy and a tunneling in the picture and
sometimes there are connectivity problems.

I had my own RetryHandler, however, the Connection Timeout exception never falls
in it. I took a look at the source code and noticed that the open() method and
any thrown exception at this level occurs outside the control of the Retry
Handler (which seems to be involved only after open() succeeds).

In fact, if the open() throws ConnectionException (as is my case), since the
try/catch wrapping the open() is not inside the while() but on top of it, it
stops the loop and the retry handler does not get a chance to be invoked .

riad"
1,"disk full can cause index corruption in certain cases. Robert uncovered this nasty bug, in adding more randomness to
oal.index tests...

I got a standalone test to show the issue; the corruption path is
as follows:

  * The merge hits an initial exception (eg disk full when merging the
    postings).

  * In handling this exception, IW closes all the sub-readers,
    suppressing any further exceptions.

  * If one of these sub-readers has pending deletions, which happens
    if readers are pooled in IW, it will flush them.  If that flush
    hits a 2nd exception (eg disk full), then SegmentReader
    [incorrectly] leaves the SegmentInfo's delGen advanced by 1,
    referencing a corrupt file, yet the SegmentReader is still
    forcefully closed.

  * If enough disk frees up such that a later IW.commit/close
    succeeds, the resulting segments file will reference an invalid
    deletions file.
"
1,LevenshteinDistance code normalization is incorrect. The normalization of the edit distance should use the maximum of the two string being compared instead of the minimum.  Otherwise negative distances are possible.  The spell checker filters out edits below a certain threshold so this hasn't been a problem in practice.
1,"Using ConstantScoreQuery on a RemoteSearchable throws java.io.NotSerializableException. Using a ConstantScoreQuery through a MultiSearcher on a Searchable obtained through RMI (RemoteSearchable) will throw a java.io.NotSerializableException

The problem seems to be the fact that the ConstantScoreQuery.ConstantWeight has a Searcher member variable which is not serializable. Keeping a reference to the Searcher does not seem to be required: the fix seems trivial.

I've created the TestCase to reproduce the issue and the patch to fix it."
1,"CJKTokenizer convert   HALFWIDTH_AND_FULLWIDTH_FORMS wrong. CJKTokenizer have these lines..
                if (ub == Character.UnicodeBlock.HALFWIDTH_AND_FULLWIDTH_FORMS) {
                    /** convert  HALFWIDTH_AND_FULLWIDTH_FORMS to BASIC_LATIN */
                    int i = (int) c;
                    i = i - 65248;
                    c = (char) i;
                }

This is wrong. Some character in the block (e.g. U+ff68) have no BASIC_LATIN counterparts.
Only 65281-65374 can be converted this way.

The fix is

             if (ub == Character.UnicodeBlock.HALFWIDTH_AND_FULLWIDTH_FORMS && i <= 65474 && i> 65281) {
                    /** convert  HALFWIDTH_AND_FULLWIDTH_FORMS to BASIC_LATIN */
                    int i = (int) c;
                    i = i - 65248;
                    c = (char) i;
                }"
1,"KuromojiTokenizer fails with large docs. just shoving largeish random docs triggers asserts like:

{noformat}
    [junit] Caused by: java.lang.AssertionError: backPos=4100 vs lastBackTracePos=5120
    [junit] 	at org.apache.lucene.analysis.kuromoji.KuromojiTokenizer.backtrace(KuromojiTokenizer.java:907)
    [junit] 	at org.apache.lucene.analysis.kuromoji.KuromojiTokenizer.parse(KuromojiTokenizer.java:756)
    [junit] 	at org.apache.lucene.analysis.kuromoji.KuromojiTokenizer.incrementToken(KuromojiTokenizer.java:403)
    [junit] 	at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:404)
{noformat}

But, you get no seed...

I'll commit the test case and @Ignore it."
1,"Node.orderBefore does not check permissions. It seems that Node.orderBefore(String, String) does not check if the editing session is allowed to modify the parent, neither immediately nor upon saving the transient changes.

This issue was found by Alexandre Capt. Thanks!"
1,"Error reading data. Hi,

I have some problems with HttpClient HEAD. It works fine with a build of 
20020720 of HttpClient though.

It seems HttpClient is not reading correctly the returned HTTP response.

I'm attaching the logs.

Here is the output from Cactus build:



     [java]     [junit] Testcase: testLongProcess took 3.645 sec
     [java]     [junit]         Caused an ERROR
     [java]     [junit] Failed to get the test results. This is probably due 
to an error that happen
ed on the server side when trying to execute the tests. Here is what was 
returned by the server : [<
html><head><Long Process></head><body>Some data</body></html>
     [java]     [junit] ]
     [java]     [junit] org.apache.cactus.util.ChainedRuntimeException: Failed 
to get the test resul
ts. This is probably due to an error that happened on the server side when 
trying to execute the tes
ts. Here is what was returned by the server : [<html><head><Long 
Process></head><body>Some data</bod
y></html>
     [java]     [junit] ]
     [java]     [junit]         at 
org.apache.cactus.client.AbstractHttpClient.doTest(Unknown Source
)
     [java]     [junit]         at 
org.apache.cactus.AbstractWebTestCase.runWebTest(Unknown Source)
     [java]     [junit]         at 
org.apache.cactus.AbstractWebTestCase.runGenericTest(Unknown Sour
ce)
     [java]     [junit]         at org.apache.cactus.ServletTestCase.runTest
(Unknown Source)
     [java]     [junit]         at org.apache.cactus.AbstractTestCase.runBare
(Unknown Source)
     [java]     [junit] org.apache.cactus.client.ParsingException: Not a valid 
response. First 100 c
haracters of the reponse: [</webresult>HTTP/1.1 200 OK
     [java]     [junit] Server: Resin/2.1.2
     [java]     [junit] Content-Length: 23
     [java]     [junit] Date: Tue, 13 Aug 2002 08:45:2]
     [java]     [junit]         at 
org.apache.cactus.client.WebTestResultParser.readExceptionClassna
me(Unknown Source)
     [java]     [junit]         at 
org.apache.cactus.client.WebTestResultParser.parse(Unknown Source

Thanks
-Vincent"
1,Extra </div> in populate.jsp. The populate.jsp page in jackrabbit-webapp has an extra </div> that causes minor breakage to the page layout.
1,"dead lock while locking or unlocking nodes. JackRabbit is still hanging on the Node.lock() or Node.unlock() function.

... everything fine until here...
s13: 4
s13: 5
s13: 6
s13: 7   -> unlock()
s14: started.
s14: 1   -> session.getRootNode()
s15: started.
s15: 1
s16: started.

I just find this failure during the first run (emtpy repository home directory). 2nd and 3th run are fine after killing the vm from first run, but with already initialized repository directory these time.

1. rm -rf repository.home
2. run -> hang
3. kill
4. run -> ok
5. run -> ok
"
1,"ManagedConnection#cleanup doesn't refresh the session. the ManagedConnection is not cleaned up correctly. I think that the underlying jcr Session should be refreshed by calling
Session#refresh(false) at JCAManagedConnection#cleanup. In the current implementation a new Session see the changes stored in the transient level of a closed session"
1,"Jcr-server: DeltaVResource lists MKWORKSPACE in the method constant.. RFC 3253 requires REPORT to be supported by all DeltaV compliant resources.
The method constant therefore should list REPORT only."
1,"JCR2SPI: lockmgr isn't aware about external unlock (CacheBehavior.OBSERVATION). issue occurring with CacheBehavior.OBSERVATION only:

the lock manager expects the jcr:lockIsDeep property to be created upon successful lock.
this however isn't the case since the time, we changed the Operation.persisted method to invalidate the affected states. consequently the mgr never started to listen on changes made to the jcr:lockIsDeep property and consequently wasn't aware of an external removal.

suggested fix:
force re-loading of the lock holding node."
1,"o.a.j.spi.commons.query.sql2.ParserTest uses platform encoding with non-ASCII characters. The ParserTest class loads a series of test SQL statements from test.sql2.txt, which contains a few non-ASCII characters (good to test those!). Unfortunately the file is read using the default platform encoding, which breaks the Linux-based test builds.

I'll recode the file to UTF-8 and explicitly specify the encoding when the file is read."
1,"Token of  """" returns in CJKTokenizer + new TestCJKTokenizer. The """" string returns as Token in the boundary of two byte character and one byte character. 

There is no problem in CJKAnalyzer. 
When CJKTokenizer is used with the unit, it becomes a problem. (Use it with 
Solr etc.)"
1,spi2dav : move/reorder not properly handled by observation. all TCK tests including move or reorder fail in the setup jcr2spi - spi2dav(ex) - jcr-server.
1,"HttpClient enter 100% for endless time. I was working masively using HttpClient (I was testing it for usage within a 
server) and it got to 100% CPU for an endless time.

I was querying urls of the type 
http://search.barnesandnoble.com/booksearch/results.asp?WRD=<text>&sort=R&SAT=1

To reproduce it, run 100-200 urls with random words instead of <text> and 
you'll probably reproduce the problem."
1,"impl.conn.Wire uses String.getBytes() which depends on the default charset. impl.conn.Wire uses String.getBytes() which depends on the default charset

The methods 
public void output(final String s)
and
public void input(final String s)

could probably be recoded to avoid this problem, as the output routine uses a StringBuilder."
1,"cluster synchronization NPE. we have a 4 machines setup and encountered the following NPE in one of the nodes. After restarting tomcat, the problem seems to go away. But it would be nice to find out why.


java.lang.NullPointerException
        at org.apache.jackrabbit.core.query.lucene.NodeIndexer.createDoc(NodeInd
exer.java:146)
        at org.apache.jackrabbit.core.query.lucene.SearchIndex.createDocument(Se
archIndex.java:566)
        at org.apache.jackrabbit.core.query.lucene.SearchIndex$2.next(SearchInde
x.java:368)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.update(MultiIndex.
java:354)
        at org.apache.jackrabbit.core.query.lucene.SearchIndex.updateNodes(Searc
hIndex.java:356)
        at org.apache.jackrabbit.core.SearchManager.onEvent(SearchManager.java:4
23)
        at org.apache.jackrabbit.core.observation.EventConsumer.consumeEvents(Ev
entConsumer.java:231)
        at org.apache.jackrabbit.core.observation.ObservationDispatcher.dispatch
Events(ObservationDispatcher.java:201)
        at org.apache.jackrabbit.core.observation.EventStateCollection.dispatch(
EventStateCollection.java:424)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.externalUpdat
e(SharedItemStateManager.java:882)
        at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.externalUpdat
e(RepositoryImpl.java:1957)
        at org.apache.jackrabbit.core.cluster.ClusterNode.end(ClusterNode.java:8
34)
        at org.apache.jackrabbit.core.cluster.ClusterNode.consume(ClusterNode.ja
va:929)
        at org.apache.jackrabbit.core.journal.AbstractJournal.doSync(AbstractJou
rnal.java:191)
        at org.apache.jackrabbit.core.journal.AbstractJournal.sync(AbstractJourn
al.java:166)
        at org.apache.jackrabbit.core.cluster.ClusterNode.sync(ClusterNode.java:
283)
        at org.apache.jackrabbit.core.cluster.ClusterNode.start(ClusterNode.java
:229)
        at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:
308)
        at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:
584)
        at org.apache.jackrabbit.core.jndi.BindableRepository.createRepository(B
indableRepository.java:174)"
1,"position increment bug: smartcn. If i use LUCENE_VERSION >= 2.9 with smart chinese analyzer, it will crash indexwriter with any reasonable amount of chinese text.

its especially annoying because it happens in 2.9.1 RC as well.

this is because the position increments for tokens after stopwords are bogus:

Here's an example (from test case), where the position increment should be 2, but is instead 91975314!

{code}
  public void testChineseStopWords2() throws Exception {
    Analyzer ca = new SmartChineseAnalyzer(Version.LUCENE_CURRENT); /* will load stopwords */
    String sentence = ""Title:San""; // : is a stopword
    String result[] = { ""titl"", ""san""};
    int startOffsets[] = { 0, 6 };
    int endOffsets[] = { 5, 9 };
    int posIncr[] = { 1, 2 };
    assertAnalyzesTo(ca, sentence, result, startOffsets, endOffsets, posIncr);
  }
{code}

junit.framework.AssertionFailedError: posIncrement 1 expected:<2> but was:<91975314>
	at junit.framework.Assert.fail(Assert.java:47)
	at junit.framework.Assert.failNotEquals(Assert.java:280)
	at junit.framework.Assert.assertEquals(Assert.java:64)
	at junit.framework.Assert.assertEquals(Assert.java:198)
	at org.apache.lucene.analysis.BaseTokenStreamTestCase.assertTokenStreamContents(BaseTokenStreamTestCase.java:83)
	...




"
1,"spi2dav: ItemInfoCache causes failure of (Workspace)RestoreTest#testRestoreWithUUIDConflict and variants. while running the API version tests i found the (Workspace)RestoreTest.testRestoreWithUUIDConfict and variants failing. to be precise the test passes but
transiently removing the versionableNode2 in the teardown fails upon removal of the jcr:uuid property of the moved childnode.

having a closer look at it revealed that the problem is caused in the WorkspaceItemStateFactory where the property entry is retrieved from the
cache and subsequently checking if the path really matches fails. for test purposes i prevented the usage of the cached entry by returning false in WorkspaceItemStateFactory.isUpToDate  => the tests passed. 

as far as i know the same tests pass with spi2jcr.
michael, could it be that this is caused by a flaw in the iteminfo-cache logic? or is there something specific that needs to be adjusted in spi2dav?"
1,"fix analyzer bugs found by MockTokenizer. In LUCENE-3064, we beefed up MockTokenizer with assertions, and I've switched over the analysis tests to use MockTokenizer for better coverage.

However, this found a few bugs (one of which is LUCENE-3106):
* incrementToken() after it returns false in CommonGramsQueryFilter, HyphenatedWordsFilter, ShingleFilter, SynonymFilter
* missing end() implementation for PrefixAwareTokenFilter
* double reset() in QueryAutoStopWordAnalyzer and ReusableAnalyzerBase
* missing correctOffset()s in MockTokenizer itself.

I think it would be nice to just fix all the bugs on one issue... I've fixed everything except Shingle and Synonym"
1,"redirect not handled correctly if location header doesn't have a protocol. Http redirect is not handled correctly if the location header doesn't have a 
protocol, e.g.:

Location: web/tbghome.nsf/pages/index

a java.net.MalformedURLException is throw in this case. The correct behavior is 
to inherit the protocol from current URL.

The relevant code is in HttpMethodBase.execute()"
1,"Cluster sync not always done when calling session.refresh(..). Session.refresh(..) is supposed to synchronize cluster changes, but this doesn't always happen, specially if the syncDelay is low. The reason is a wrong assumption in ClusterNode.sync: The code there to avoid duplicate sync calls doesn't always work as expected. The following algorithm is used:

        int count = syncCount;
        syncLock.acquire();
        if (count == syncCount) {
            journalSync();
            syncCount++;
        }
        syncLock.release();

The problem is that the background thread might be at the line ""syncCount++"" when Session.refresh(..) is called, so that the main thread believes journalSync was already called and thus doesn't call it."
1,"""Index already present"" exception when opening a restored repository. I have created a new repository, added one node, then copied all files while Jackrabbit is running.
Then closed the repository, restored the backup, and tried to open the repository.
Unfortunately, this resulted in the following exception:

javax.jcr.RepositoryException: Index already present: Index already present: Index already present
	at org.apache.jackrabbit.core.SearchManager.initializeQueryHandler(SearchManager.java:575)
	at org.apache.jackrabbit.core.SearchManager.<init>(SearchManager.java:255)
	at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.getSearchManager(RepositoryImpl.java:1613)
	at org.apache.jackrabbit.core.RepositoryImpl.initWorkspace(RepositoryImpl.java:606)

The backup contains the following index file:
workspaces/default/index/redo.log
There are no other files or directories or files in that directory (also no _n directories). The content of redo.log is:

-1 STR
-1 ADD cafebabe-cafe-babe-cafe-babecafebabe
-1 COM
0 STR
0 DEL cafebabe-cafe-babe-cafe-babecafebabe
0 ADD fa87759b-f9fe-4ba8-986c-d1914ffce3de
0 ADD eda04b36-9c21-4712-bedf-206c36f0e3d2
0 ADD ae917cca-a0bb-4ac0-a16d-805aac6c7b10
0 ADD cafebabe-cafe-babe-cafe-babecafebabe
0 COM
1 STR
1 ADD 0121f271-bbe7-4f71-a793-5f4380f3c487
1 COM
"
1,"When using QueryImpl.setLimit() and QueryImpl.setOffset(), then NodeIterator.getSize() reports wrong size. When using QueryImpl.setLimit() and QueryImpl.setOffset(), then NodeIterator.getSize() reports wrong size. Returned size seems to be allways the same as the limit."
1,"NoSuchItemStateException if Node.checkin() is invoked within a transaction. When you run a code that takes versionning outside transactions - everything goes ok. But when you run it inside transaction, it fails:
Here's the stacktrace:

15:41:14,434 ERROR (TransactionalItemStateManager.java:114) -
java.lang.Exception: Cannot commit transaction.
[...]
Caused by: org.apache.jackrabbit.core.state.TransactionException: Unable
to commit transaction.:
31f78b39-6422-4ec8-b41e-2571b6807b05/{http://www.jcp.org/jcr/1.0}isCheckedOut
[...]
Caused by: org.apache.jackrabbit.core.state.NoSuchItemStateException:
31f78b39-6422-4ec8-b41e-2571b6807b05/{http://www.jcp.org/jcr/1.0}isCheckedOut
[...]

When you dont checkin the node transaction commits well, but the node is left checked out..."
1,"hrefs in dav responses should be url-escaped. the url in an href element of a dav response should be url-escaped. currently at least one webdav client (os x's webdavfs) chokes on unescaped urls (such as /home/bcm/file with spaces in its name.txt).
"
1,JcrValueType#typeFromContentType throws IllegalArgumentException for type weak-ref and uri. 
1,"StandardTokenizer disposes of Hiragana combining mark dakuten instead of attaching it to the character it belongs to. Lucene 3.3 (possibly 3.1 onwards) exhibits less than great behaviour for tokenising hiragana, if combining marks are in use.

Here's a unit test:

{code}
    @Test
    public void testHiraganaWithCombiningMarkDakuten() throws Exception
    {
        // Hiragana 'S' following by the combining mark dakuten
        TokenStream stream = new StandardTokenizer(Version.LUCENE_33, new StringReader(""\u3055\u3099""));

        // Should be kept together.
        List<String> expectedTokens = Arrays.asList(""\u3055\u3099"");
        List<String> actualTokens = new LinkedList<String>();
        CharTermAttribute term = stream.addAttribute(CharTermAttribute.class);
        while (stream.incrementToken())
        {
            actualTokens.add(term.toString());
        }

        assertEquals(""Wrong tokens"", expectedTokens, actualTokens);

    }
{code}

This code fails with:
{noformat}
java.lang.AssertionError: Wrong tokens expected:<[]> but was:<[]>
{noformat}

It seems as if the tokeniser is throwing away the combining mark entirely.

3.0's behaviour was also undesirable:
{noformat}
java.lang.AssertionError: Wrong tokens expected:<[]> but was:<[, ]>
{noformat}

But at least the token was there, so it was possible to write a filter to work around the issue.

Katakana seems to be avoiding this particular problem, because all katakana and combining marks found in a single run seem to be lumped into a single token (this is a problem in its own right, but I'm not sure if it's really a bug.)
"
1,"max connections per host setting does not work. When using the MultiThreadedHttpConnectionManager the default maximal
connections per host/port cannot be exceeded (allowed maximum is 2 by default).
Attempts to exceed this by manually setting the max connections using
HttpConnectionManagerParams#setMaxConnectionsPerHost fail. This is caused by a
bug in the MultiThreadedHttpConnectionManager."
1,"Inflater.end() method not always called in FieldsReader. 
We've just found an insidious memory leak in our own application as we did not always call Deflater.end() and Inflater.end(). As documented here;

http://bugs.sun.com/view_bug.do?bug_id=4797189

The non-heap memory that the native zlib code uses is not freed in a timely manner.

FieldsWriter appears safe as no exception can be thrown between the Deflater's creation and end() as it uses a ByteArrayOutputStream

FieldsReader, however, is not safe. In the event of a DataFormatException the call to end() will not occur."
1,Stackoverflow when calling deprecated CharArraySet.copy(). Calling CharArraySet#copy(set) without the version argument (deprecated) with an instance of CharArraySet results in a stack overflow as this method checks if the given set is a CharArraySet and then calls itself again. This was accidentially introduced due to an overloaded alternative method during LUCENE-2169 which was not used in the final patch.
1,"ConcurrentModificationException in CacheManager.. Using the test code below, I was able to produce this stack:

java.util.ConcurrentModificationException
	at java.util.WeakHashMap$HashIterator.nextEntry(WeakHashMap.java:762)
	at java.util.WeakHashMap$KeyIterator.next(WeakHashMap.java:795)
	at org.apache.jackrabbit.core.cache.CacheManager.logCacheStats(CacheManager.java:164)
	at org.apache.jackrabbit.core.cache.CacheManager.cacheAccessed(CacheManager.java:137)
	at org.apache.jackrabbit.core.cache.AbstractCache.recordCacheAccess(AbstractCache.java:122)
	at org.apache.jackrabbit.core.cache.ConcurrentCache.get(ConcurrentCache.java:122)
	at org.apache.jackrabbit.core.state.MLRUItemStateCache.retrieve(MLRUItemStateCache.java:71)
	at org.apache.jackrabbit.core.state.ItemStateReferenceCache.retrieve(ItemStateReferenceCache.java:139)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.getNonVirtualItemState(SharedItemStateManager.java:1716)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.getItemState(SharedItemStateManager.java:268)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.getNodeState(LocalItemStateManager.java:110)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.getItemState(LocalItemStateManager.java:175)
	at org.apache.jackrabbit.core.state.XAItemStateManager.getItemState(XAItemStateManager.java:260)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:161)
	at org.apache.jackrabbit.core.ItemManager.getItemData(ItemManager.java:382)
	at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:328)
	at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:622)
	at org.apache.jackrabbit.core.ItemManager.getRootNode(ItemManager.java:531)
	at org.apache.jackrabbit.core.SessionImpl.getRootNode(SessionImpl.java:760)
	at test.JackrabbitTest$1.run(JackrabbitTest.java:37)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)

-------------------------

package test;

import java.io.File;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicInteger;

import javax.jcr.Repository;
import javax.jcr.RepositoryException;
import javax.jcr.Session;
import javax.jcr.SimpleCredentials;

import org.apache.jackrabbit.core.TransientRepository;

public class JackrabbitTest {

 public static void main(final String[] args) throws Exception {
   File dir = File.createTempFile(""jackrabbit-test"", """");
   dir.delete();
   dir.mkdir();
   System.out.println(""created temporary directory: "" +
       dir.getAbsolutePath());
   dir.deleteOnExit();

   final Repository jcrRepo = new TransientRepository(dir);
   final AtomicBoolean passed = new AtomicBoolean(true);
   final AtomicInteger counter = new AtomicInteger(0);
   ExecutorService executor = Executors.newFixedThreadPool(50);
   Runnable runnable = new Runnable() {

     @Override
     public void run() {
       try {
         Session session = jcrRepo.login(
             new SimpleCredentials(""admin"",
                 ""admin"".toCharArray()));
         session.getRootNode().addNode(""n"" +
                 counter.getAndIncrement()); //unique name
         session.save();
         session.logout();
       } catch (RepositoryException e) {
         e.printStackTrace();
         passed.set(false);
       }
     }

   };
   System.out.println(""Running threads"");
   for (int i = 0; i<  500; i++) {
     executor.execute(runnable);
   }
   executor.shutdown(); //Disable new tasks from being submitted
   if (!executor.awaitTermination(120, TimeUnit.SECONDS)) {
     System.err.println(""timeout"");
     System.exit(1);
   }
   if (!passed.get()) {
     System.err.println(""one or more threads got an exception"");
     System.exit(1);
   } else {
     System.out.println(""all threads ran with no exceptions"");
     System.exit(0);
   }

 }

}
"
1,"ClusterNode not properly shutdown when repository has shutdown. Sometimes when the repository is shutdown the ClusterNode is not shutdown and it therefore tries to update records or access a closed Journal file.  The setup that generated the exception is I have 3 VMs each with a Repository that are all connected to the same database.  In the below stack trace one of the repositories is being shutdown however the ClusterNode thread is also trying to update the repository journal at the same time.  Below is a copy of the stack trace.

[4/23/08 9:58:52:496 CDT] 00000061 SystemOut     O 89811653 [WebContainer : 2] INFO  org.apache.jackrabbit.core.RepositoryImpl  - Shutting down repository...
[4/23/08 9:58:52:511 CDT] 0000054c SystemOut     O 89811621 [ClusterNode-b06e4fe7-a602-4a93-b106-e0834046ae0f] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7174
[4/23/08 9:58:52:527 CDT] 00000061 SystemOut     O 89811684 [WebContainer : 2] INFO  org.apache.jackrabbit.core.RepositoryImpl  - shutting down workspace 'default'...
[4/23/08 9:58:52:574 CDT] 00000061 SystemOut     O 89811715 [WebContainer : 2] INFO  org.apache.jackrabbit.core.observation.ObservationDispatcher  - Notification of EventListeners stopped.
[4/23/08 9:58:53:058 CDT] 00000061 SystemOut     O 89812215 [WebContainer : 2] INFO  org.apache.jackrabbit.core.RepositoryImpl  - workspace 'default' has been shutdown
[4/23/08 9:58:53:308 CDT] 00000308 SystemOut     O 91641048 [ClusterNode-e609e8a6-320e-44ea-be0f-ab8c5cb89662] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7165
[4/23/08 9:58:53:324 CDT] 00000308 SystemOut     O 91641064 [ClusterNode-e609e8a6-320e-44ea-be0f-ab8c5cb89662] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7166
[4/23/08 9:58:53:324 CDT] 00000308 SystemOut     O 91641064 [ClusterNode-e609e8a6-320e-44ea-be0f-ab8c5cb89662] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7167
[4/23/08 9:58:53:339 CDT] 00000308 SystemOut     O 91641079 [ClusterNode-e609e8a6-320e-44ea-be0f-ab8c5cb89662] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7168
[4/23/08 9:58:53:339 CDT] 00000308 SystemOut     O 91641079 [ClusterNode-e609e8a6-320e-44ea-be0f-ab8c5cb89662] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7169
[4/23/08 9:58:53:355 CDT] 00000308 SystemOut     O 91641095 [ClusterNode-e609e8a6-320e-44ea-be0f-ab8c5cb89662] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7170
[4/23/08 9:58:53:371 CDT] 00000308 SystemOut     O 91641111 [ClusterNode-e609e8a6-320e-44ea-be0f-ab8c5cb89662] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7171
[4/23/08 9:58:53:386 CDT] 00000308 SystemOut     O 91641126 [ClusterNode-e609e8a6-320e-44ea-be0f-ab8c5cb89662] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7172
[4/23/08 9:58:53:417 CDT] 00000308 SystemOut     O 91641157 [ClusterNode-e609e8a6-320e-44ea-be0f-ab8c5cb89662] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7173
[4/23/08 9:58:53:433 CDT] 00000308 SystemOut     O 91641173 [ClusterNode-e609e8a6-320e-44ea-be0f-ab8c5cb89662] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7174
[4/23/08 9:58:53:433 CDT] 00000308 SystemOut     O 91641173 [ClusterNode-e609e8a6-320e-44ea-be0f-ab8c5cb89662] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7175
[4/23/08 9:58:53:496 CDT] 00000308 SystemOut     O 91641236 [ClusterNode-e609e8a6-320e-44ea-be0f-ab8c5cb89662] INFO  org.apache.jackrabbit.core.journal.AbstractJournal  - Synchronized to revision: 7175
[4/23/08 9:58:54:292 CDT] 00000131 SystemOut     O 89171473 [ClusterNode-4930503b-ab33-4444-999e-c87fb3681bf7] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7173
[4/23/08 9:58:54:308 CDT] 00000131 SystemOut     O 89171504 [ClusterNode-4930503b-ab33-4444-999e-c87fb3681bf7] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7174
[4/23/08 9:58:54:308 CDT] 00000131 SystemOut     O 89171504 [ClusterNode-4930503b-ab33-4444-999e-c87fb3681bf7] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7175
[4/23/08 9:58:54:386 CDT] 00000131 SystemOut     O 89171582 [ClusterNode-4930503b-ab33-4444-999e-c87fb3681bf7] INFO  org.apache.jackrabbit.core.journal.AbstractJournal  - Synchronized to revision: 7175
[4/23/08 9:58:55:417 CDT] 00000061 SystemOut     O 89814574 [WebContainer : 2] INFO  org.apache.jackrabbit.core.RepositoryImpl  - Repository has been shutdown
[4/23/08 9:58:56:089 CDT] 0000054c SystemOut     O 89815199 [ClusterNode-b06e4fe7-a602-4a93-b106-e0834046ae0f] ERROR org.apache.jackrabbit.core.cluster.ClusterNode  - Unable to read revision '7174'.
org.apache.jackrabbit.core.journal.JournalException: I/O error while reading string.
	at org.apache.jackrabbit.core.journal.ReadRecord.readString(ReadRecord.java:169)
	at org.apache.jackrabbit.core.cluster.ClusterNode.consume(ClusterNode.java:979)
	at org.apache.jackrabbit.core.journal.AbstractJournal.doSync(AbstractJournal.java:198)
	at org.apache.jackrabbit.core.journal.AbstractJournal.sync(AbstractJournal.java:173)
	at org.apache.jackrabbit.core.cluster.ClusterNode.sync(ClusterNode.java:303)
	at org.apache.jackrabbit.core.cluster.ClusterNode.run(ClusterNode.java:274)
	at java.lang.Thread.run(Thread.java:797)
Caused by: 
java.io.IOException: Closed Connection
	at oracle.jdbc.driver.DatabaseError.SQLToIOException(DatabaseError.java:517)
	at oracle.jdbc.driver.OracleBlobInputStream.needBytes(OracleBlobInputStream.java:187)
	at oracle.jdbc.driver.OracleBufferedStream.readInternal(OracleBufferedStream.java:130)
	at oracle.jdbc.driver.OracleBufferedStream.read(OracleBufferedStream.java:108)
	at java.io.DataInputStream.readBoolean(DataInputStream.java:246)
	at org.apache.jackrabbit.core.journal.ReadRecord.readString(ReadRecord.java:161)
	... 6 more
[4/23/08 9:58:56:261 CDT] 0000054c SystemOut     O 89815355 [ClusterNode-b06e4fe7-a602-4a93-b106-e0834046ae0f] ERROR org.apache.jackrabbit.core.journal.DatabaseJournal  - Error while moving to next record.
java.sql.SQLException: Closed Connection: next
	at oracle.jdbc.driver.DatabaseError.throwSqlException(DatabaseError.java:112)
	at oracle.jdbc.driver.DatabaseError.throwSqlException(DatabaseError.java:146)
	at oracle.jdbc.driver.OracleResultSetImpl.next(OracleResultSetImpl.java:181)
	at org.apache.jackrabbit.core.journal.DatabaseRecordIterator.fetchRecord(DatabaseRecordIterator.java:136)
	at org.apache.jackrabbit.core.journal.DatabaseRecordIterator.hasNext(DatabaseRecordIterator.java:85)
	at org.apache.jackrabbit.core.journal.AbstractJournal.doSync(AbstractJournal.java:190)
	at org.apache.jackrabbit.core.journal.AbstractJournal.sync(AbstractJournal.java:173)
	at org.apache.jackrabbit.core.cluster.ClusterNode.sync(ClusterNode.java:303)
	at org.apache.jackrabbit.core.cluster.ClusterNode.run(ClusterNode.java:274)
	at java.lang.Thread.run(Thread.java:797)
[4/23/08 9:58:56:402 CDT] 0000054c SystemOut     O 89815418 [ClusterNode-b06e4fe7-a602-4a93-b106-e0834046ae0f] WARN  org.apache.jackrabbit.core.cluster.ClusterNode  - Unable to set current revision to 7174.
org.apache.jackrabbit.core.journal.JournalException: Revision file closed.
	at org.apache.jackrabbit.core.journal.FileRevision.set(FileRevision.java:100)
	at org.apache.jackrabbit.core.cluster.ClusterNode.setRevision(ClusterNode.java:1073)
	at org.apache.jackrabbit.core.journal.AbstractJournal.doSync(AbstractJournal.java:211)
	at org.apache.jackrabbit.core.journal.AbstractJournal.sync(AbstractJournal.java:173)
	at org.apache.jackrabbit.core.cluster.ClusterNode.sync(ClusterNode.java:303)
	at org.apache.jackrabbit.core.cluster.ClusterNode.run(ClusterNode.java:274)
	at java.lang.Thread.run(Thread.java:797)

"
1,"JCARepositoryManager.createNonTransientRepository throws NPE with no JCAManagedConnectionFactory.CONFIGFILE_KEY. JCARepositoryManager.createNonTransientRepository fails if

String configFile = parameters.get(JCAManagedConnectionFactory.CONFIGFILE_KEY);

is null, because

config = RepositoryConfig.create(configFile, homeDir);

will always throw an NPE, perhaps the call should just be

config = RepositoryConfig.create(homeDir);

"
1,BoostingNearQuery doesn't have hashCode/equals. 
1,"NativeFSLockFactory.makeLock(...).isLocked() does not work. IndexWriter.isLocked() or IndexReader.isLocked() do not work with NativeFSLockFactory.

The problem is, that the method NativeFSLock.isLocked() just checks if the same lock instance was locked before (lock != null). If the LockFactory created a new lock instance, this always returns false, even if its locked."
1,"shouldn't automatically set Content-Length in request header. currently, httpclient automatically add Content-Length: 0 in the request 
header, this is causing problems with some web servers, particularly, with

ar.atwola.com

Try the following URL
http://ar.atwola.com/file/adsWrapper.js

It will block indefinitely. This problem can be fixed by not sending the 
Content-Length header, this is the browser's behavior. I'm not sure why this 
casue problem, but let's conform to a standard browser's practice and avoid 
troubles."
1,"addIndexes(IndexReader) incorrectly applies existing deletes. If you perform these operations:
# deleteDocuments(Term) for all the new documents
# addIndexes(IndexReader)
# commit

Then addIndexes applies the previous deletes on the incoming indexes as well, which is incorrect. If you call addIndexes(Directory) instead, the deletes are applied beforehand, as they should. The solution, as Mike indicated here: http://osdir.com/ml/general/2011-03/msg20876.html is to add *flush(false,true)* to addIndexes(IndexReader).

I will create a patch with a matching unit test shortly."
1,"jackrabbit wrongly think nodetype is changed on nodetype re-registration. When trying node type re-registration with jackrabbit 2.0, it wrongly detects a nodetype as having changed, with non-trivial changes. Example nodetype definition;

[nen:profile] > mix:referenceable mixin orderable
- nen:dn (string)
- nen:cn (string)
- * (string)
+ * multiple

Exception on nodetype re-registration;

javax.jcr.RepositoryException: The following nodetype change contains
non-trivial changes.Up until now only trivial changes are supported.
(see javadoc for org.apache.jackrabbit.core.nodetype.NodeTypeDefDiff):
org.apache.jackrabbit.core.nodetype.NodeTypeDefDiff[
       nodeTypeName={http://netenviron.com/nen/1.0}profile,
       mixinFlagDiff=NONE,
       supertypesDiff=NONE,
       propertyDifferences=[
               org.apache.jackrabbit.core.nodetype.NodeTypeDefDiff$PropDefDiff[itemName={http://netenviron.com/nen/1.0}dn,
type=TRIVIAL, operation=MODIFIED],
               org.apache.jackrabbit.core.nodetype.NodeTypeDefDiff$PropDefDiff[itemName={http://netenviron.com/nen/1.0}cn,
type=TRIVIAL, operation=MODIFIED],
               org.apache.jackrabbit.core.nodetype.NodeTypeDefDiff$PropDefDiff[itemName={}*,
type=TRIVIAL, operation=MODIFIED]
       ],
       childNodeDifferences=[
               org.apache.jackrabbit.core.nodetype.NodeTypeDefDiff$ChildNodeDefDiff[itemName={}*,
type=MAJOR, operation=REMOVED],
               org.apache.jackrabbit.core.nodetype.NodeTypeDefDiff$ChildNodeDefDiff[itemName={}*,
type=TRIVIAL, operation=ADDED]
       ]
]

       at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.reregisterNodeType(NodeTypeRegistry.java:442)
       at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.reregisterNodeType(NodeTypeRegistry.java:363)
       at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.registerNodeTypes(NodeTypeManagerImpl.java:589)
       at org.apache.jackrabbit.commons.cnd.CndImporter.registerNodeTypes(CndImporter.java:118)
       at com.netenviron.content.manager.SessionManager.checkRepositorySchema(SessionManager.java:355)
       at com.netenviron.content.manager.SessionManager.afterPropertiesSet(SessionManager.java:199)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1288)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1257)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:438)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory$1.run(AbstractAutowireCapableBeanFactory.java:383)
       at java.security.AccessController.doPrivileged(Native Method)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:353)
       at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:245)
       at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:169)
       at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:242)
       at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:164)
       at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:269)
       at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:104)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1172)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:940)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:437)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory$1.run(AbstractAutowireCapableBeanFactory.java:383)
       at java.security.AccessController.doPrivileged(Native Method)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:353)
       at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:245)
       at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:169)
       at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:242)
       at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:164)
       at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:269)
       at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:104)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1172)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:940)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:437)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory$1.run(AbstractAutowireCapableBeanFactory.java:383)
       at java.security.AccessController.doPrivileged(Native Method)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:353)
       at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:245)
       at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:169)
       at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:242)
       at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:164)
       at org.springframework.beans.factory.support.AbstractBeanFactory.getTypeForFactoryBean(AbstractBeanFactory.java:1223)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.getTypeForFactoryBean(AbstractAutowireCapableBeanFactory.java:582)
       at org.springframework.beans.factory.support.AbstractBeanFactory.isTypeMatch(AbstractBeanFactory.java:438)
       at org.springframework.beans.factory.support.DefaultListableBeanFactory.getBeanNamesForType(DefaultListableBeanFactory.java:214)
       at org.springframework.beans.factory.support.DefaultListableBeanFactory.getBeanNamesForType(DefaultListableBeanFactory.java:189)
       at org.springframework.beans.factory.BeanFactoryUtils.beanNamesForTypeIncludingAncestors(BeanFactoryUtils.java:143)
       at org.springframework.beans.factory.support.DefaultListableBeanFactory.findAutowireCandidates(DefaultListableBeanFactory.java:614)
       at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:572)
       at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredMethodElement.inject(AutowiredAnnotationBeanPostProcessor.java:496)
       at org.springframework.beans.factory.annotation.InjectionMetadata.injectMethods(InjectionMetadata.java:87)
       at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessPropertyValues(AutowiredAnnotationBeanPostProcessor.java:250)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:928)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:437)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory$1.run(AbstractAutowireCapableBeanFactory.java:383)
       at java.security.AccessController.doPrivileged(Native Method)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:353)
       at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:245)
       at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:169)
       at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:242)
       at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:164)
       at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:269)
       at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:104)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1172)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:940)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:437)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory$1.run(AbstractAutowireCapableBeanFactory.java:383)
       at java.security.AccessController.doPrivileged(Native Method)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:353)
       at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:245)
       at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:169)
       at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:242)
       at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:164)
       at org.springframework.beans.factory.support.DefaultListableBeanFactory.getBeansOfType(DefaultListableBeanFactory.java:299)
       at org.springframework.context.support.AbstractApplicationContext.getBeansOfType(AbstractApplicationContext.java:955)
       at org.springframework.context.support.AbstractApplicationContext.registerListeners(AbstractApplicationContext.java:712)
       at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:366)
       at org.springframework.web.context.ContextLoader.createWebApplicationContext(ContextLoader.java:261)
       at org.springframework.web.context.ContextLoader.initWebApplicationContext(ContextLoader.java:199)
       at org.springframework.web.context.ContextLoaderListener.contextInitialized(ContextLoaderListener.java:45)
       at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:3972)
       at org.apache.catalina.core.StandardContext.start(StandardContext.java:4467)
       at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:791)
       at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:771)
       at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:526)
       at org.apache.catalina.startup.HostConfig.deployWAR(HostConfig.java:905)
       at org.apache.catalina.startup.HostConfig.deployWARs(HostConfig.java:740)
       at org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:500)
       at org.apache.catalina.startup.HostConfig.check(HostConfig.java:1345)
       at org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:303)
       at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:119)
       at org.apache.catalina.core.ContainerBase.backgroundProcess(ContainerBase.java:1337)
       at org.apache.catalina.core.ContainerBase$ContainerBackgroundProcessor.processChildren(ContainerBase.java:1601)
       at org.apache.catalina.core.ContainerBase$ContainerBackgroundProcessor.processChildren(ContainerBase.java:1610)
       at org.apache.catalina.core.ContainerBase$ContainerBackgroundProcessor.run(ContainerBase.java:1590)
       at java.lang.Thread.run(Thread.java:637)
"
1,"IndexWriter does not release its write lock when trying to open an index which does not yet exist. In version 2.0.0, the private IndexWriter constructor does not properly remove its write lock in the event of an error. This can be seen when one attempts to open (not create) an index in a directory which exists, but in which there is no segments file. Here is the offending code:

    247   private IndexWriter(Directory d, Analyzer a, final boolean create, boolean closeDir)
    248     throws IOException {
    249       this.closeDir = closeDir;
    250       directory = d;
    251       analyzer = a;
    252 
    253       Lock writeLock = directory.makeLock(IndexWriter.WRITE_LOCK_NAME);
    254       if (!writeLock.obtain(writeLockTimeout)) // obtain write lock
    255         throw new IOException(""Index locked for write: "" + writeLock);
    256       this.writeLock = writeLock;                   // save it
    257 
    258       synchronized (directory) {        // in- & inter-process sync
    259         new Lock.With(directory.makeLock(IndexWriter.COMMIT_LOCK_NAME), commitLockTimeout) {
    260             public Object doBody() throws IOException {
    261               if (create)
    262                 segmentInfos.write(directory);
    263               else
    264                 segmentInfos.read(directory);
    265               return null;
    266             }
    267           }.run();
    268       }
    269   }

On line 254, a write lock is obtained by the constructor. If an exception is raised inside the doBody() method (on line 260), then that exception is propagated, the constructor will fail, but the lock is not released until the object is garbage collected. This is typically an issue except when using the IndexModifier class.

As of the filing of this bug, this has not yet been fixed in the trunk (IndexWriter.java#472959):

    251   private IndexWriter(Directory d, Analyzer a, final boolean create, boolean closeDir)
    252     throws IOException {
    253       this.closeDir = closeDir;
    254       directory = d;
    255       analyzer = a;
    256 
    257       Lock writeLock = directory.makeLock(IndexWriter.WRITE_LOCK_NAME);
    258       if (!writeLock.obtain(writeLockTimeout)) // obtain write lock
    259         throw new IOException(""Index locked for write: "" + writeLock);
    260       this.writeLock = writeLock;                   // save it
    261 
    262       synchronized (directory) {        // in- & inter-process sync
    263         new Lock.With(directory.makeLock(IndexWriter.COMMIT_LOCK_NAME), commitLockTimeout) {
    264             public Object doBody() throws IOException {
    265               if (create)
    266                 segmentInfos.write(directory);
    267               else
    268                 segmentInfos.read(directory);
    269               return null;
    270             }
    271           }.run();
    272       }
    273   }"
1,NotQuery does not implement extractTerms(). If the not() function is used in query together with the rep:excerpt() function an UnsupportedOperationException is thrown.
1,"NullPointerException in AbstractVersionManager.createVersionHistory(). Running ConcurrentCheckinMixedTransactionTest with 200 threads results in NullPointerExceptions in AbstractVersionManager.

Exception in thread ""Thread-16"" java.lang.NullPointerException
	at org.apache.jackrabbit.core.version.AbstractVersionManager.createVersionHistory(AbstractVersionManager.java:309)
	at org.apache.jackrabbit.core.version.XAVersionManager.createVersionHistory(XAVersionManager.java:145)
	at org.apache.jackrabbit.core.ItemImpl.initVersionHistories(ItemImpl.java:785)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1221)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:897)
	at org.apache.jackrabbit.core.ConcurrentCheckinMixedTransactionTest$1$1.execute(ConcurrentCheckinMixedTransactionTest.java:66)
	at org.apache.jackrabbit.core.AbstractConcurrencyTest$Executor.run(AbstractConcurrencyTest.java:110)
	at java.lang.Thread.run(Thread.java:619)

I'm not sure why the node that is created by the current thread is not available. I assume that some other thread using XA transactions is committing changes while the current thread creates the node. The changes from the committing thread then overwrite the node that has been modified by the current thread. The write lock is somewhat bypassed in that case."
1,"FastVectorHighlighter: small fragCharSize can cause StringIndexOutOfBoundsException . If fragCharSize is smaller than Query string, StringIndexOutOfBoundsException is thrown."
1,StackOverflowError if too many versions of a node are created. In org.apache.jackrabbit.core.version.VersionIteratorImpl addVersion() is called recursively which can cause StackOverflowErrors if there are too many versions.
1,"SessionImpl.createSession uses same Subject/LoginContext. SessionImpl.createSession(String) uses the same loginctx/subject to create a new session.
this will cause problems if Session.logout() is called on the original instance.

i suggest to fix that by creating a new subject for the new session instance."
1,"XMLPersistanceManager doesn't preserve a property's 'multiValued' attribute. when a multi-valued property is persisted and later read using the XMLPersistenceManager the 'multiValued' attribute is lost, i.e. PropertyState.isMultiValued() returns always false."
1,"Web client/WebDAV fails to unescape workspace names. It seems that when you try to access to a workspace through the web client/webdav, and the workspace has special characteres like spaces on its name like (""my workspace""), then the web cliente/webdav is not able to load it.

Probablly the only thing to do is to unscape the workspace name."
1,"checkIfNodeLocked()  in jcr mapping layer  does not behave properly when open scoped locks are used. I am planning to use open-scoped lock.  For which , I need to persist the locktoken along with the node  so that it can be used by another session for unlocking.Tested with Jackrabbit RMI client and it works fine.

But I am using jcr-mapping layer to achieve the above in my project.Here I want that  as soon as a node is checked out, it gets locked by the session and the lock is stored in ""lockToken"" property of node ""Document"". For that I need to update the Document node after locking .

*public void checkout(String path)throws CMSException {
       pm = getPersistenceManager();
        try{
           pm.checkout(path);*
*            String lockToken = pm.lock(path,true,false);   **        
    Document doc = this.getDocument(path);**           
  doc.setLockToken(lockToken);     //for persisting lockToken
           doc.update();
        }catch(LockedException le){
          System.out.println(le.getLockedNodePath() + ""is locked by"" + le.getLockOwner());         }catch(Exception e){
            throw new CMSException(e.getMessage(),e.getCause());
        }
   }*


Here doc.update() fails with Locked Exception.  The problem here is PersistenceManagerImpl has a method checkIfNodeLocked(path)  which returns LockException if node is locked. This method is checked before every update/insert. So, I am not able to update a locked node. I need to persist the locktoken in the node . What is the reason of checking  for a lock before saving ? Ideally , it should throw error only if node is locked and session does not hold the lockToken .

If the session who has locked the node tries to save the node without unlocking, it should be allowed .

p.s.
I am able to achieve the above by simple Jackrabbit RMI client.
<code>
               ClientRepositoryFactory factory = new ClientRepositoryFactory();
               Repository repository = factory.getRepository(""rmi://localhost:1101/jackrabbit"");
               Session session = repository.login(new SimpleCredentials(""superuser"", ""superuser"".toCharArray()),""Portal"");                        String user = session.getUserID();
               String name = repository.getDescriptor(Repository.REP_NAME_DESC);
               System.out.println(
                       ""Logged in as "" + user + "" to a "" + name + "" repository."");

               /* Testing the locks functionality */
               Node n = session.getRootNode().getNode(""cms/childfolder1/check.txt"");

              * Lock lck = n.lock(true, false); // deeplock,open-scoped
               n.setProperty(""ps:locktoken"",lck.getLockToken());
               n.setProperty(""ps:language"", ""sanskrit"");
*
               System.out.println(""Lock#isLive="" + lck.isLive());
               System.out.println(""Node#isLocked="" +  session.getRootNode().getNode(""cms/childfolder1/check.txt"").isLocked());
               session.save();
               session.logout();
     <code> "
1,"StandardTokenizer splitting all of Korean words into separate characters. StandardTokenizer splits all those Korean words inth separate character tokens. For example, ""?????"" is one Korean word that means ""Hello"", but StandardAnalyzer separates it into five tokens of ""?"", ""?"", ""?"", ""?"", ""?""."
1,"StatusLine IndexOutOfBounds. Reported by Sam Berlin on the developers mailing list:

I'm not sure if this problem is still on CVS HEAD, but we're seeing it  
against 2.0rc2.  In StatusLine (line 139 in my version), when it walks  
through the spaces, it is possible that the entire line was spaces (and  
thus a malformed response).  The code will throw an  
StringIndexOutOfBoundsException now instead of the correct  
HttpException.  See the following bug:

http://bugs.limewire.com/bugs/ 
searching.jsp?disp1=l&disp2=c&disp3=o&disp4=j&l=151&c=204&m=416_205

Thanks,
  Sam"
1,"Revision 949509 (LUCENE-2480) causes IOE ""read past EOF"" when processing older format SegmentInfo data when JVM assertion processing is disabled.. At revision 949509 in org.apache.lucene.index.SegmentInfo at line 155, there is the following code:
{noformat} 
    if (format > SegmentInfos.FORMAT_4_0) {
      // pre-4.0 indexes write a byte if there is a single norms file
      assert 1 == input.readByte();
    }
{noformat} 
Note that the assert statement invokes input.readByte().
If asserts are disabled for the JVM, input.readByte() will not be invoked, causing the following readInt() to return a bogus value, and then causing an IOE during the (mistakenly entered) loop at line 165.
This can occur when processing old format (format ""-9"") index data under Tomcat (whose startup scripts by default do not turn on asserts).

Full stacktrace:
{noformat} 
SEVERE: java.lang.RuntimeException: java.io.IOException: read past EOF
	at org.apache.solr.core.SolrCore.getSearcher(SolrCore.java:1066)
	at org.apache.solr.core.SolrCore.<init>(SolrCore.java:581)
	at org.apache.solr.core.CoreContainer.create(CoreContainer.java:431)
	at org.apache.solr.core.CoreContainer.load(CoreContainer.java:286)
	at org.apache.solr.core.CoreContainer$Initializer.initialize(CoreContainer.java:125)
	at org.apache.solr.servlet.SolrDispatchFilter.init(SolrDispatchFilter.java:86)
	at org.apache.catalina.core.ApplicationFilterConfig.getFilter(ApplicationFilterConfig.java:275)
	at org.apache.catalina.core.ApplicationFilterConfig.setFilterDef(ApplicationFilterConfig.java:397)
	at org.apache.catalina.core.ApplicationFilterConfig.<init>(ApplicationFilterConfig.java:108)
	at org.apache.catalina.core.StandardContext.filterStart(StandardContext.java:3800)
	at org.apache.catalina.core.StandardContext.start(StandardContext.java:4450)
	at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:791)
	at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:771)
	at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:526)
	at org.apache.catalina.startup.HostConfig.deployWAR(HostConfig.java:850)
	at org.apache.catalina.startup.HostConfig.deployWARs(HostConfig.java:724)
	at org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:493)
	at org.apache.catalina.startup.HostConfig.start(HostConfig.java:1206)
	at org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:314)
	at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:119)
	at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1053)
	at org.apache.catalina.core.StandardHost.start(StandardHost.java:722)
	at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1045)
	at org.apache.catalina.core.StandardEngine.start(StandardEngine.java:443)
	at org.apache.catalina.core.StandardService.start(StandardService.java:516)
	at org.apache.catalina.core.StandardServer.start(StandardServer.java:710)
	at org.apache.catalina.startup.Catalina.start(Catalina.java:583)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:288)
	at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:413)
Caused by: java.io.IOException: read past EOF
	at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:154)
	at org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:39)
	at org.apache.lucene.store.ChecksumIndexInput.readByte(ChecksumIndexInput.java:40)
	at org.apache.lucene.store.DataInput.readInt(DataInput.java:76)
	at org.apache.lucene.store.DataInput.readLong(DataInput.java:99)
	at org.apache.lucene.index.SegmentInfo.<init>(SegmentInfo.java:165)
	at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:230)
	at org.apache.lucene.index.DirectoryReader$1.doBody(DirectoryReader.java:91)
	at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:649)
	at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:87)
	at org.apache.lucene.index.IndexReader.open(IndexReader.java:415)
	at org.apache.lucene.index.IndexReader.open(IndexReader.java:294)
	at org.apache.solr.core.StandardIndexReaderFactory.newReader(StandardIndexReaderFactory.java:38)
	at org.apache.solr.core.SolrCore.getSearcher(SolrCore.java:1055)
	... 32 more
{noformat} "
1,Index migration fails for property names that are prefixes of others. The automatic index migration (JCR-1363) introduced in Jackrabbit version 1.5.0 replaces the separator char for named term text in PROPERTIES field. Util 1.4.x '\uFFFF' was used and after migration '[' is used. This changes the overall order of  PROPERTIES terms and leads to assertion failures. See attached test.
1,"NamespaceAdder.addNamespace throws ClassCastException. Here's the method

public void addNamespaces(NamespaceMapping nsm)
            throws NamespaceException, 
UnsupportedRepositoryOperationException, RepositoryException {
        Map m = nsm.getPrefixToURIMapping();
        for (Iterator i = m.values().iterator(); i.hasNext();) {
            Map.Entry e = (Map.Entry) i.next();
            String prefix = (String) e.getKey();
            String uri = (String) e.getKey();
            registry.registerNamespace(prefix, uri);
        }
    }


should be the entrySet iterator, and uri should come from the value, patch fixes this.
Occurs in both spi-commons and jcr-commons (duplicate code)"
1,"Repository Home locked not released despite RepositoryException being thrown.. When an exception is thrown when calling RepositoryImpl.create(...) a .lock file is created in the repository home directory and not removed despite there no longer being an active connection.  If the user attempts to create the repository again (e.g recover from the exception because the url of the repository was temporarily unavailable) a RepositoryException is thrown again indicating that the repository home is locked by another process because there is a .lock file.  If a Repository is not successfully created then the repository home should not be locked.

The lock is only released when the repository is shutdown but in this case the Repository object is never created successfully for that method to be called.

"
1,"MockRandomMergePolicy optimizes segments not in the Set passed in. The test class MockRandomMergePolicy shuffles the whole SegmentInfos passed to the optimize callback and returns random segments for optimizing. This is fine, but it also returns segments, that are not listed in the Set<SegmentInfo> that is also passed in, containing the subset of segments to optimize.

This bug was found when writing a testcase for LUCENE-3082: The wrapper MergePolicy (when wrapped around MockRandomMergePolicy) only passes a subset of the segments to the delegate (the ones that are in old index format). But MockRandom created OneMerge in its return MergeSpecification having segments outside this set."
1,"FSDirectory.getDirectory always creates index path. This was reported to me as a Luke bug, but going deeper it proved to be a non-intuitive (broken?) behavior of FSDirectory.

If you use FSDirectory.getDirectory(File nonexistent) on a nonexistent path, but one that is located under some existing parent path, then FSDirectory:174 uses file.mkdirs() to create this directory. One would expect a variant of the method with a boolean flag to decide whether or not to create the output path. However, the API with ""create"" flag is now deprecated, with a comment that points to IndexWriter's ""create"" flag. This comment is misleading, because the indicated path is created anyway in the file system just by calling FSDirectory.getDirectory().

I propose to do one of the following:

* reinstate the variant of the method with ""create"" flag. In case if this flag is false, and the index directory is missing, either return null or throw an IOException,

* keep the API as it is now, but either return null or throw IOException if the index dir is missing. This breaks the backwards compatibility, because now users are required to do file.mkdirs() themselves prior to calling FSDirectory.getDirectory()."
1,"Some equals methods do not check for null argument. The equals methods in the following classes do not check for a null argument and thus would incorrectly fail with a null pointer exception if passed null:

- org.apache.lucene.index.SegmentInfo
- org.apache.lucene.search.function.CustomScoreQuery
- org.apache.lucene.search.function.OrdFieldSource
- org.apache.lucene.search.function.ReverseOrdFieldSource
- org.apache.lucene.search.function.ValueSourceQuery

If a null parameter is passed to equals() then false should be returned."
1,"Cannot move a first-level node. Given /nodeA,
session.move(""/nodeA"", ""/nodeB"")

throws this exception:

javax.jcr.PathNotFoundException: no ancestor at degree 1: {}
	at org.apache.jackrabbit.spi.commons.name.PathFactoryImpl$PathImpl.getAncestor(PathFactoryImpl.java:481)
	at org.apache.jackrabbit.core.retention.RetentionRegistryImpl.hasEffectiveRetention(RetentionRegistryImpl.java:291)
	at org.apache.jackrabbit.core.ItemValidator.hasRetention(ItemValidator.java:426)
	at org.apache.jackrabbit.core.ItemValidator.checkCondition(ItemValidator.java:328)
	at org.apache.jackrabbit.core.ItemValidator.checkRemove(ItemValidator.java:281)
	at org.apache.jackrabbit.core.SessionImpl.move(SessionImpl.java:1075)
	at org.apache.jackrabbit.core.MoveAtRootTest.testMoveAtRoot(MoveAtRootTest.java:54)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at org.apache.jackrabbit.test.AbstractJCRTest.run(AbstractJCRTest.java:456)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:130)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)

"
1,"While indexing Turkish web pages, ""Parse Aborted: Lexical error...."" occurs. When I try to index Turkish page if there is a Turkish specific character in the HTML specific tag HTML parser gives ""Parse Aborted: Lexical error.on ... line"" error.
For this case ""<IMG SRC=""../images/head.jpg"" WIDTH=570 HEIGHT=47 BORDER=0 ALT="""">"" exception address """" character (which has 351 ascii value) as an error. OR  character in title tag.
<a title=""()"">

Turkish character in the content do not create any problem."
1,"fix compound-file/NoSuchDirectoryException bugs in NRTCachingDir. found some bugs over on LUCENE-3374, but we should fix these separately from whether or not we move it to core,
and the bugs apply to 3.x too.

here we can just add explicit tests for the problems."
1,"spi2dav: Accessing moved referenceble nodes results in PathNotFoundException. the following code fragment causes a PathNotFoundException on an existing path
and there seems to be no way to recover the session from this incorrect state:

	// assuming an existing nt:file node at path /apps/foo/bar.txt
	Node n1 = session.getNode(""/apps/foo/bar.txt"");
	Node n2 = n1.getNode(""jcr:content"");
	n2.setProperty(""jcr:data"", new java.io.ByteArrayInputStream(((String)(""blahblah"")).getBytes()));
	n2.save();
	Workspace ws0 = session.getWorkspace();
	ws0.move(""/apps/foo"", ""/apps/foo1"");
	Node n3 = session.getNode(""/apps/foo1/bar.txt"");
	Node n4 = n3.getNode(""jcr:content"");
	n4.refresh(false);
	Node n5 = n3.getNode(""jcr:content"");     // => PathNotFoundException

Please note that the preceeding Node.refresh() call seems to cause the inconsistency.
the problem doesn't occur when omitting this call."
1,"initVersions crashes with NPE. After delete some old versions. I get serious problems accessing the version history.
This is the stacktrace:
java.lang.NullPointerException
	at org.apache.jackrabbit.core.version.VersionIteratorImpl.initVersions(VersionIteratorImpl.java:169)
	at org.apache.jackrabbit.core.version.VersionIteratorImpl.<init>(VersionIteratorImpl.java:87)
	at org.apache.jackrabbit.core.version.VersionIteratorImpl.<init>(VersionIteratorImpl.java:72)
	at org.apache.jackrabbit.core.version.VersionHistoryImpl.getAllVersions(VersionHistoryImpl.java:92)

I stepped threw the code and see that the Method 
    currentVersion.getSuccessors() 
returns an empty Array.

After all the VersionHistory seems to be corrupt!!"
1,"Readers wrapping other readers don't prevent usage if any of their subreaders was closed. On recent trunk test we got this problem:
org.apache.lucene.index.TestReaderClosed.test
fails because the inner reader is closed but the wrapped outer ones are still open.

I fixed the issue partially for SlowCompositeReaderWrapper and ParallelAtomicReader but it failed again. The cool thing with this test is the following:

The test opens an DirectoryReader and then creates a searcher, closes the reader and executes a search. This is not an issue, if the reader is closed that the search is running on. This test uses LTC.newSearcher(wrap=true), which randomly wraps the passed Reader with SlowComposite or ParallelReader - or with both!!! If you then close the original inner reader, the close is not detected when excuting search. This can cause SIGSEGV when MMAP is used.

The problem in (in Slow* and Parallel*) is, that both have their own Fields instances thats are kept alive until the reader itsself is closed. If the child reader is closed, the wrapping reader does not know and still uses its own Fields instance that delegates to the inner readers. On this step no more ensureOpen checks are done, causing the failures.

The first fix done in Slow and Parallel was to call ensureOpen() on the subReader, too when requesting fields(). This works fine until you wrap two times: ParallelAtomicReader(SlowCompositeReaderWrapper(StandardDirectoryReader(segments_1:3:nrt _0(4.0):C42)))

One solution would be to make ensureOpen also check all subreaders, but that would do the volatile checks way too often (with n is the total number of subreaders and m is the number of hierarchical levels this is n^m) - we cannot do this. Currently we only have n*m which is fine.

The proposal how to solve this (closing subreaders under the hood of parent readers is to use the readerClosedListeners. Whenever a composite or slow reader wraps another readers, it registers itself as interested in readerClosed events. When a subreader is then forcefully closed (e.g by a programming error or this crazy test), we automatically close the parents, too.

We should also fix this in 3.x, if we have similar problems there (needs investigation)."
1,"Explanation.toHtml outputs invalid HTML. If you want an HTML representation of an Explanation, you might call the toHtml() method.  However, the output of this method looks like the following:

<ul>
  <li>some value = some description</li>
  <ul>
    <li>some nested value = some description</li>
  </ul>
</ul>

As it is illegal in HTML to nest a UL directly inside a UL, this method will always output unparseable HTML if there are nested explanations.

What Lucene probably means to output is the following, which is valid HTML:

<ul>
  <li>some value = some description
    <ul>
      <li>some nested value = some description</li>
    </ul>
  </li>
</ul>
"
1,"SegmentReader.setNorm can fail to remove separate norms file, on Windows. 
While working through LUCENE-710 I hit this bug: on Windows
only, when SegmentReader.setNorm is called, but separate norms
(_X_N.sY) had already been previously saved, then, on closing the
reader, we will write the next gen separate norm file correctly
(_X_N+1.sY) but fail to delete the current one.

It's quite minor because the next writer to touch the index will
remove the stale file.

This is because the Norm class still holds the IndexInput open when
the reader commits."
1,"DescendantSelfAxisQuery may fail with IOException when session has limited access. The DescendantSelfAxisQuery uses the current session to look up nodes by id. When the session does not have access to a node the exception is incorrectly re-thrown an IOException. Instead, any ItemNotFoundException should be caught and ignored. This is probably a regression caused by JCR-1365 introduced with Jackrabbit 1.5."
1,"EventConsumer.canRead() should rely on AccessManager.isGranted(). The current implementation of EventConsumer.canRead() uses
AccessManager.canRead(), which might cause issues if the item
does not exist anymore. AccessManager.isGranted() explicitly
mentions and supports checks on paths for items that do not
yet exist or not exist anymore.

See also JCR-3271."
1,"Intermittent failure in TestIndexWriterMergePolicy.testMaxBufferedDocsChange. Last night's build failed from it: http://hudson.zones.apache.org/hudson/job/Lucene-trunk/1019/changes

Here's the exc:

{code}
    [junit] Testcase: testMaxBufferedDocsChange(org.apache.lucene.index.TestIndexWriterMergePolicy):	FAILED
    [junit] maxMergeDocs=2147483647; numSegments=11; upperBound=10; mergeFactor=10
    [junit] junit.framework.AssertionFailedError: maxMergeDocs=2147483647; numSegments=11; upperBound=10; mergeFactor=10
    [junit] 	at org.apache.lucene.index.TestIndexWriterMergePolicy.checkInvariants(TestIndexWriterMergePolicy.java:234)
    [junit] 	at org.apache.lucene.index.TestIndexWriterMergePolicy.testMaxBufferedDocsChange(TestIndexWriterMergePolicy.java:164)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:208)
{code}

Test doesn't fail if I run on opensolaris nor os X machines..."
1,"TermAttributeImpl.equals() does not check termLength. If you look at the code from equals(), I think it misses this check :

other.termLength==this.termLength

This check must be before the comparison of the arrays."
1,"NodeReferences are lost when deleting and setting the same reference in the same save() cycle. I've written the following snippet to illustrate the issue :

        Node root = session.getRootNode();
        
        Node a = root.addNode(""a"");
        Node b = root.addNode(""b"");
        b.addMixin(""mix:referenceable"");
        
        a.setProperty(""p"", b);
        
        root.save();
        
        System.out.println(b.getReferences().getSize());     // --> correctly returns 1
        
        a.setProperty(""p"", (Node) null);
        a.setProperty(""p"", b);
        
        root.save();
        
        System.out.println(b.getReferences().getSize());    // --> returns 0 !

When the ChangeLog is processed, added references are processed before deleted ones, so the persisted NodeReferences is finally wrong.

I've set the priority of this issue to critical, because the persisted references count is corrupted.

A simple workaround is to first remove the property, then save, then add the property again, but it not satisfying.
"
1,"SSL Tunneling does not work with MultiThreadedHttpConnectionManager. The HttpConnection is released prematurely when doing SSL tunneling with the
MultiThreadedHttpConnectionManager.  The ConnectMethod releases the connection
in responseBodyConsumed() before it can be used by the real method."
1,"RowIterator view of result for query '//*' only returns jcr:path column. The RowIterator view of a query result for '//*' only returns the jcr:path column. The spec states that this query is equivalent to:
select * from nt:base. Furthermore a query that selects * properties must return all non-residual properties that are declared for this node type and are not multi-valued. The pseudo properties jcr:path and jcr:score must always be available.

For nt:base this is:
- jcr:primaryType
- jcr:path
- jcr:score"
1,"Netscape proxy problem wtih POST. Description:

When using httpClient to POST to a http url through a Netscape proxy server, 
the httpClient failed due to read error when reading status line.  The log seem 
to indicate that the proxy is talking HTTP/1.0 and does not expect the POST 
data to come.  I am using a modified version of the ClientApp from examples.  I 
will attach both the test program and log files.

Workaround:

If use PostMethod.setUseExpect (true), it will work.  But in many cases, it 
would be slower.

Related issues:

In doing the test, I also found out that the httpClient PostMehtod does not 
work when the request body is NOT set (not calling setRequestBody).  It also 
does not work with empty body (setRequestBody ("""")).  The attached 
clientApp.properties file has flags to test each case and I will attach the 
logs as well.  Excuse my ignorance, I do not know for sure what the HTTP spec. 
says about the body in the POST method.  But at least if the caller/app is 
wrong in not setting the body, some exception should be thrown.  It could also 
be my server's problem, please let me know if that is the case (I am using 
weblogic server 6.1)."
1,"[PATCH] BooleanScorer2 ArrayIndexOutOfBoundsException + alternative NearSpans. From Erik's post at java-dev: 
 
>   [java] Caused by: java.lang.ArrayIndexOutOfBoundsException: 4 
>   [java]   at org.apache.lucene.search.BooleanScorer2  
> $Coordinator.coordFactor(BooleanScorer2.java:54) 
>   [java]   at org.apache.lucene.search.BooleanScorer2.score  
> (BooleanScorer2.java:292) 
... 
 
and my answer: 
 
Probably nrMatchers is increased too often in score() by calling score() 
more than once."
1,"DataStore: garbage collection fails if a workspace is not initialized. The test case GCEventListenerTest fails with the following exception:

testEventListener(org.apache.jackrabbit.core.data.GCEventListenerTest)  Time elapsed: 10.235 sec  <<< ERROR!
java.lang.IllegalStateException: workspace 'test' not initialized
	at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.getPersistenceManager(RepositoryImpl.java:1703)
	at org.apache.jackrabbit.core.SessionImpl.createDataStoreGarbageCollector(SessionImpl.java:694)
	at org.apache.jackrabbit.core.data.GCEventListenerTest.doTestEventListener(GCEventListenerTest.java:75)
	at org.apache.jackrabbit.core.data.GCEventListenerTest.testEventListener(GCEventListenerTest.java:49)

"
1,"In modules/analysys/icu, ant gennorm2 does not work. 
Command to run gennorm2 does not work at present.  Also, icupkg needs to be called to convert the binary file to big-endian.

I will attach a patch."
1,"Don't commit an empty segments_N when IW is opened with create=true. If IW is opened with create=true, it forcefully commits an empty
segments_N.  But really it should not: if autoCommit is false, nothing
should be committed until commit or close is explicitly called.

Spinoff from http://www.nabble.com/no-segments*-file-found:-files:-Error-on-opening-index-td23219520.html
"
1,"Path returned by FileSystemBLOBStore.createId() is not absolute. Hi,

I have developed my own FileSystem in which I call FileSystemPathUtil.checkFormat(path) for every operation on the file system.
When the file system is called to store a BLOB value, the path I get is always relative, resulting in a ""not an absolute path"" FileSystemException.

The problem has been traced back to org.apache.jackrabbit.core.state.util.FileSystemBLOBStore.creatId().
I think there should be a:
   sb.append(FileSystem.SEPARATOR_CHAR);
before the for loop.

Thanks."
1,"need to ensure that sims that use collection-level stats (e.g. sumTotalTermFreq) handle non-existent field. Because of things like queryNorm, unfortunately similarities have to handle the case where they are asked to computeStats() for a term, where the field does not exist at all.
(Note they will never have to actually score anything, but unless we break how queryNorm works for TFIDF, we have to deal with this case).

I noticed this while doing some benchmarking, so i created a test to test some cases like this across all the sims."
1,"NullPointerException during indexing in DocumentsWriter$ThreadState$FieldData.addPosition. In my case during indexing sometimes appear documents with unusually large ""words"" - text-encoded images in fact.
Attempt to add document that contains field with such token produces java.lang.IllegalArgumentException:
java.lang.IllegalArgumentException: term length 37944 exceeds max term length 16383
        at org.apache.lucene.index.DocumentsWriter$ThreadState$FieldData.addPosition(DocumentsWriter.java:1492)
        at org.apache.lucene.index.DocumentsWriter$ThreadState$FieldData.invertField(DocumentsWriter.java:1321)
        at org.apache.lucene.index.DocumentsWriter$ThreadState$FieldData.processField(DocumentsWriter.java:1247)
        at org.apache.lucene.index.DocumentsWriter$ThreadState.processDocument(DocumentsWriter.java:972)
        at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:2202)
        at org.apache.lucene.index.DocumentsWriter.addDocument(DocumentsWriter.java:2186)
        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1432)
        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1411)

This is expected, exception is caught and ignored. The problem is that after this IndexWriter becomes somewhat corrupted and subsequent attempts to add documents to the index fail as well, this time with NPE:
java.lang.NullPointerException
        at org.apache.lucene.index.DocumentsWriter$ThreadState$FieldData.addPosition(DocumentsWriter.java:1497)
        at org.apache.lucene.index.DocumentsWriter$ThreadState$FieldData.invertField(DocumentsWriter.java:1321)
        at org.apache.lucene.index.DocumentsWriter$ThreadState$FieldData.processField(DocumentsWriter.java:1247)
        at org.apache.lucene.index.DocumentsWriter$ThreadState.processDocument(DocumentsWriter.java:972)
        at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:2202)
        at org.apache.lucene.index.DocumentsWriter.addDocument(DocumentsWriter.java:2186)
        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1432)
        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1411)

This is 100% reproducible."
1,"IW.getReader() returns inconsistent reader on RT Branch. I extended the testcase TestRollingUpdates#testUpdateSameDoc to pull a NRT reader after each update and asserted that is always sees only one document. Yet, this fails with current branch since there is a problem in how we flush in the getReader() case. What happens here is that we flush all threads and then release the lock (letting other flushes which came in after we entered the flushAllThread context, continue) so that we could concurrently get a new segment that transports global deletes without the corresponding add. They sneak in while we continue to open the NRT reader which in turn sees inconsistent results.

I will upload a patch soon"
1,"DirectSpellChecker throws NPE if field doesn't exist. DirectSpellchecker doesn't check that the resulting Terms is null,
it should return an empty list here."
1,"DbDataStore connection does not always reconnect. If a DbDataStore connection is closed due to an error all subsequent addRecord calls will fail with 'connection has been closed and autoReconnect == false'
 after getRecord is called and the connection is reconnected addRecord will succeed.

the connection should be validated before setting autoReconnect = false or on retrieval from the pool."
1,Prevent persistence of faulty back-references. The SharedItemStateManager updates reference data. Sometimes the back-references to reference properties are not updated correctly with the result that nodes cannot removed anymore. The attached patch contains JUnit test cases and a possible solution.
1,"FSDirectory.openFile(String) causes ClassCastException. When you call FSDirectory.openFile(String) you get a ClassCastException since FSIndexInput is not an org.apache.lucene.store.InputStream

The workaround is to reimplement using openInput(String). I personally don't need this to be fixed but wanted to document it here in case anyone else runs into this for any reason.

The reason I'm calling this is that I have a requirement on my project to create read only indexes and name the index segments consistently from one build to the next. So, after creating and optimizing the index, I rename the files and rewrite the segments file. It would be nice if I had an API that would allow me to say ""I only want one segment and I want its name to be 'foo'"". For instance IndexWriter.optimize(String segmentName)"
1,"RFC4918IfHeaderTest.testPutIfLockToken could fail with 412 Precondition Failed. In org.apache.jackrabbit.webdav.server.RFC4918IfHeaderTest:110 (webdav-test), 
the lock request is initialized with a timeout of 1800 milliseconds, which is rounded as Timeout: Second-1 (at org.apache.jackrabbit.webdav.header.TimeoutHeader:46).

The assertion in the finally block MUST fail (412, Precondition Failed) if the lock has expired (cf. RFC 4918, Section 10.4.10).

The lock request should be initialized with a higher timeout, at least several seconds."
1,"Failed CONNECT leaves connection in an inconsistent state. Opening a HTTPS Connection over an authenticating Proxy (Basic auth. scheme) 
fails, if proxy credentials are not provided at the first try. 

The following example code will fail:

HttpClient client = new HttpClient(new MultiThreadedHttpConnectionManager());
URL url = new URL(""https://examplehttpsurl"");
  
//first try 
GetMethod get = new GetMethod(url.toExternalForm());
HostConfiguration hc = new HostConfiguration();
hc.setHost(url.getHost(), 443, ""https"");
hc.setProxy(""proxyhost"", 4711);

try {
  client.executeMethod(hc, get);
} catch (Exception e){
  LOG.error("""",e);
} finally {
  get.releaseConnection();
}

//returns 407 (expected)
LOG.debug(""Answer: "" + get.getStatusLine().toString()); 

//retry with credentials (normally requested from the user)
client.getState().setProxyCredentials(new AuthScope(""proxyhost"",4711),
      new NTCredentials(""USER"", ""PASS"", """", """"));

get = new GetMethod(url.toExternalForm());

try {
  client.executeMethod(hc, get);
} catch (Exception e) {
  e.printStackTrace();
} finally {
  get.releaseConnection();
}
//should be 200 but is 407
LOG.debug(""Answer: "" + get.getStatusLine().toString());



----------


From what I see from HttpMethodDirector.executeWithRetry(final
HttpMethod method), the cause is, that the connection is kept open, and
thus the connect is never retried:


if (!this.conn.isOpen()) {
  // this connection must be opened before it can be used
  // This has nothing to do with opening a secure tunnel
  this.conn.open();
  if (this.conn.isProxied() && this.conn.isSecure() 
      && !(method instanceof ConnectMethod)) {
    // we need to create a secure tunnel before we can execute the real method
    if (!executeConnect()) {
      // abort, the connect method failed
      return;
    }
  }
}


If I add a conn.close() before returning on !executeConnect(), the
above code will work, the CONNECT is reattempted."
1,"Socket streams are closed in the incorrect order.. HttpConnection should close the streams/socket in the following order:

OutputStream
InputStream
Socket

<http://java.sun.com/docs/books/tutorial/networking/sockets/readingWriting.html>"
1,"JCR-2523 break the transaction handling in container managed environment. during the cleanup (returning to the pool) of an jca managed connection,  an new internal session is created in the object JCAManagedConnection in the method cleanup, this is supposed to fix JCR-2523, The sideeffect is, that the XA-Resource (variable-xaResource) in JCAManagedConnection is not anymore the same XASessionImpl Object like the session Object. Subsequent calls on this connection, lead that the internal session variable is not anymore informed about the current transaction context. (XAItemStateManager, variables tx and txLog are null), because only the xaResource is informed about the new transaction context. Result is that the complete transaction handling does not work anymore.
I attached a sample project which shows this behaviour.
"
1,Node.canAddMixin(String). after the spec this method must return false if the node is locked.
1,"FileRequestEntity in SVN does not close input file. FileRequestEntity.java in SVN does not close input file - however the version on the web page:

http://jakarta.apache.org/commons/httpclient/performance.html

has a finally clause that closes the file ;-) - perhaps the source code should too...!"
1,"o.a.l.analysis.de.GermanStemmer crashes on some inputs. See the tests from LUCENE-2560. 

GermanAnalyzer no longer uses this stemmer by default, but we should fix it."
1,"MultiReader should make a private copy of the subReaders array. Spinoff from here:

    http://mail-archives.apache.org/mod_mbox/lucene-java-user/200806.mbox/%3C88F3F6A4-FBFB-43DF-890D-DB5F0D9A2461@gmail.com%3E

Because MultiReader just holds a reference to the array that was passed in, it's possible to hit scary exceptions (that look like index corruption) if that array is later altered eg by reopening some of the readers.

The fix is trivial: just make a private copy."
1,"WebDav MKCOL on a directory that already exists generates a IllegalStateException. when performing a MKCOL on a resource that already exists, following is thrown.

31.03.2010 16:14:10.760 *ERROR* [127.0.0.1 [1270012450463] MKCOL /org.apache.sling.launchpad.testing-6-SNAPSHOT/apps HTTP/1.1] org.apache.sling.engine.impl.SlingMainServlet service: Uncaught Throwable java.lang.IllegalStateException: Response has already been committed
       at org.apache.sling.engine.impl.SlingHttpServletResponseImpl.checkCommitted(SlingHttpServletResponseImpl.java:398)
       at org.apache.sling.engine.impl.SlingHttpServletResponseImpl.setStatus(SlingHttpServletResponseImpl.java:265)
       at org.apache.jackrabbit.webdav.WebdavResponseImpl.setStatus(WebdavResponseImpl.java:276)
       at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.doMkCol(AbstractWebdavServlet.java:548)
       at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.execute(AbstractWebdavServlet.java:256)
       at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.service(AbstractWebdavServlet.java:196)
       at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
       at org.apache.sling.engine.impl.request.RequestData.service(RequestData.java:523)
....


I think a return after the sendError is required ?
in AbstractWebdavServlet.doMkCol(...) 


   protected void doMkCol(WebdavRequest request, WebdavResponse response,
                          DavResource resource) throws IOException, DavException {

       DavResource parentResource = resource.getCollection();
       if (parentResource == null || !parentResource.exists() || !parentResource.isCollection()) {
           // parent does not exist or is not a collection
           response.sendError(DavServletResponse.SC_CONFLICT);
           return;
       }
       // shortcut: mkcol is only allowed on deleted/non-existing resources
       if (resource.exists()) {
           response.sendError(DavServletResponse.SC_METHOD_NOT_ALLOWED);
+          return;
       }

       if (request.getContentLength() > 0 || request.getHeader(""Transfer-Encoding"") != null) {
           parentResource.addMember(resource, getInputContext(request, request.getInputStream()));
       } else {
           parentResource.addMember(resource, getInputContext(request, null));
       }
       response.setStatus(DavServletResponse.SC_CREATED);
   }



"
1,"Request/Response race condition when doing multiple requests on the same connection.. If one tries to do multiple request over the same socket connection a race 
condition occurs in the input/output streams.
eg. 
-- Some request -->
<- HTTP/1.1 200 OK
<- Some: Headers
<- 
<- The body.

-- Next request -->
<- HTTP/1.1 200 OK
<- More: Headers
<- 
<- Some data.

If the second request is sent, but the second response isn't yet received 
before the client starts to try to read it, it'll get 
a ""org.apache.commons.httpclient.HttpRecoverableException: Error in parsing the 
status  line from the response: unable to find line starting with ""HTTP/"""" 
exception (it will think ""The body."" is part of the second response).

The following code will reproduce the problem:

import java.io.*;
import java.net.*;
import java.util.*;
import org.apache.commons.httpclient.*;
import org.apache.commons.httpclient.methods.*;

public class HttpClientRaceBug {
    public static void main(String[] args) {
        try {
            SimpleHttpServer.listen(8987);
            HttpClient client = new HttpClient();
            client.startSession(""localhost"", 8987);
            client.getState().setCredentials(""Test Realm"",  
                new UsernamePasswordCredentials(""foo"", ""bar""));
            
            for (int i = 0; i < 100; i++) {
                GetMethod meth = new GetMethod();
                client.executeMethod(meth);
            }
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
    
    private static final class SimpleHttpServer implements Runnable {
        private Socket socket;
        public SimpleHttpServer(Socket socket) {
            this.socket = socket;
        }
        public static void listen(final int port) {
            Thread server = new Thread() {
                public void run() {
                    try {
                        ServerSocket ss = new ServerSocket(port);
                        while (true) {
                            new Thread(new 
                                SimpleHttpServer(ss.accept())).start();
                        }
                    } catch (Exception e) {
                        e.printStackTrace();
                    }
                }
            };
            
            server.setDaemon(true);
            server.start();
        }
        public void run() {
            try {
                BufferedReader in = new BufferedReader(new 
                    InputStreamReader(this.socket.getInputStream()));
                
                int len = 0;
                boolean auth = false;
                String line;
                while ((line = in.readLine()) != null) {
                    System.out.println(""> "" + line);
                    
                    if (line.trim().equals("""")) {
                        in.read(new char[len]);
                        doOutput(auth);
                        auth = false;
                        len = 0;
                        
                    } else if (line.indexOf(':') > -1) {
                        StringTokenizer tok = new StringTokenizer(line, "":"");
                        String key = tok.nextToken().toLowerCase();
                        if (key.equals(""content-length"")) {
                            len = Integer.parseInt(tok.nextToken().trim());
                        } else if (key.equals(""authorization"")) {
                            auth = true;
                        }
                    }
                }
            } catch (Exception e) {}
        }
        private static int count = 0;
        public void doOutput(boolean authorized) throws IOException {
            Writer out = new OutputStreamWriter(this.socket.getOutputStream());
            count++;
            
            String id = (count < 100) ? 
                ((count < 10) ? ""00"" + count : ""0"" + count) : """" + count;
            if (authorized) {
                write(out, ""HTTP/1.1 200 OK\r\n"");
            } else {
                write(out, ""HTTP/1.1 401 Unauthorized\r\n"");
            }
            write(out, ""WWW-Authenticate: Basic realm=\""Test Realm\""\r\n"");
            write(out, ""Response-Id: "" + id + ""\r\n"");
            write(out, ""Content-Type: text/html; charset=iso-8859-1\r\n"");
            write(out, ""Content-Length: 17\r\n\r\n"");
            write(out, ""My Response ("" + id + "")"");
            out.close();
        }
        private void write(Writer out, String text) throws IOException {
            System.out.print(""< "" + text);
            out.write(text);
        }
    }
}"
1,"BLOB Store: only open a stream when really necessary. Currently, PropertyImpl.getValue() opens a FileInputStream if the BLOBStore is used.
If the application doesn't use the value, this stream is never closed. 

See also JCR-2067 (FileDataStore)"
1,"SegmentMerger doesn't set payload bit in new optimized code. In the new optimized code in SegmentMerger the payload bit is not set correctly
in the merged segment. This means that we loose all payloads during a merge!

The Payloads unit test doesn't catch this. Now that we have the new
DocumentsWriter we buffer much more docs by default then before. This means
that the test cases can't assume anymore that the DocsWriter flushes after 10
docs by default. TestPayloads however falsely assumed this, which means that no
merges happen anymore in TestPayloads. We should check whether there are
other testcases that rely on this.

The fixes for TestPayloads and SegmentMerger are very simple, I'll attach a patch
soon."
1,"LLRect.createBox returned box does not contains all points in (center,distance) disc. LLRect,createBox computation of a bouding box for a disc given center and distance doest not contains all the point in the distance.

Example : the point north by distance doest not have Lat inferior of Lat of the UpperRight corner of the returned box"
1,"TestOmitTf.testMixedMerge random seed failure. Version: trunk r1091638

ant test -Dtests.seed=-6595054217575280191:5576532348905930588


    [junit] ------------- Standard Error -----------------
    [junit] WARNING: test method: 'testDeMorgan' left thread running: Thread[NRT search threads-1691-thread-2,5,main]
    [junit] RESOURCE LEAK: test method: 'testDeMorgan' left 1 thread(s) running
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestBooleanQuery -Dtestmethod=testDeMorgan -Dtests.seed=-6595054217575280191:5576532348905930588
    [junit] ------------- ---------------- ---------------
    [junit] Testsuite: org.apache.lucene.index.TestNorms
    [junit] Tests run: 2, Failures: 0, Errors: 0, Time elapsed: 5.064 sec
    [junit] 
    [junit] Testsuite: org.apache.lucene.index.TestOmitTf
    [junit] Testcase: testMixedMerge(org.apache.lucene.index.TestOmitTf):	Caused an ERROR
    [junit] CheckIndex failed
    [junit] java.lang.RuntimeException: CheckIndex failed
    [junit] 	at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:152)
    [junit] 	at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:138)
    [junit] 	at org.apache.lucene.index.TestOmitTf.testMixedMerge(TestOmitTf.java:155)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1232)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1160)
    [junit] 
    [junit] 
    [junit] Tests run: 5, Failures: 0, Errors: 1, Time elapsed: 0.851 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] CheckIndex failed
    [junit] Segments file=segments_1 numSegments=1 version=FORMAT_4_0 [Lucene 4.0]
    [junit]   1 of 1: name=_12 docCount=60
    [junit]     codec=SegmentCodecs [codecs=[MockRandom, MockVariableIntBlock(baseBlockSize=112)], provider=RandomCodecProvider: {f1=MockRandom, f2=MockVariableIntBlock(baseBlockSize=112)}]
    [junit]     compound=false
    [junit]     hasProx=false
    [junit]     numFiles=16
    [junit]     size (MB)=0,01
    [junit]     diagnostics = {optimize=true, mergeFactor=2, os.version=2.6.37-gentoo, os=Linux, lucene.version=4.0-SNAPSHOT, source=merge, os.arch=amd64, java.version=1.6.0_24, java.vendor=Sun Microsystems Inc.}
    [junit]     no deletions
    [junit]     test: open reader.........OK
    [junit]     test: fields..............OK [2 fields]
    [junit]     test: field norms.........OK [2 fields]
    [junit]     test: terms, freq, prox...ERROR: java.io.IOException: Read past EOF
    [junit] java.io.IOException: Read past EOF
    [junit] 	at org.apache.lucene.store.RAMInputStream.switchCurrentBuffer(RAMInputStream.java:90)
    [junit] 	at org.apache.lucene.store.RAMInputStream.readByte(RAMInputStream.java:63)
    [junit] 	at org.apache.lucene.store.MockIndexInputWrapper.readByte(MockIndexInputWrapper.java:105)
    [junit] 	at org.apache.lucene.store.DataInput.readVInt(DataInput.java:94)
    [junit] 	at org.apache.lucene.index.codecs.sep.SepSkipListReader.readSkipData(SepSkipListReader.java:188)
    [junit] 	at org.apache.lucene.index.codecs.MultiLevelSkipListReader.loadNextSkip(MultiLevelSkipListReader.java:142)
    [junit] 	at org.apache.lucene.index.codecs.MultiLevelSkipListReader.skipTo(MultiLevelSkipListReader.java:112)
    [junit] 	at org.apache.lucene.index.codecs.sep.SepPostingsReaderImpl$SepDocsEnum.advance(SepPostingsReaderImpl.java:454)
    [junit] 	at org.apache.lucene.index.CheckIndex.testTermIndex(CheckIndex.java:782)
    [junit] 	at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:495)
    [junit] 	at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:148)
    [junit] 	at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:138)
    [junit] 	at org.apache.lucene.index.TestOmitTf.testMixedMerge(TestOmitTf.java:155)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit] 	at java.lang.reflect.Method.invoke(Method.java:597)
    [junit] 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit] 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit] 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit] 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit] 	at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1232)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1160)
    [junit] 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    [junit] 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    [junit] 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    [junit] 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    [junit] 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    [junit] 	at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:422)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:931)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:758)
    [junit]     test: stored fields.......OK [60 total field count; avg 1 fields per doc]
    [junit]     test: term vectors........OK [120 total vector count; avg 2 term/freq vector fields per doc]
    [junit] FAILED
    [junit]     WARNING: fixIndex() would remove reference to this segment; full exception:
    [junit] java.lang.RuntimeException: Term Index test failed
    [junit] 	at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:508)
    [junit] 	at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:148)
    [junit] 	at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:138)
    [junit] 	at org.apache.lucene.index.TestOmitTf.testMixedMerge(TestOmitTf.java:155)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit] 	at java.lang.reflect.Method.invoke(Method.java:597)
    [junit] 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit] 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit] 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit] 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit] 	at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1232)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1160)
    [junit] 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    [junit] 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    [junit] 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    [junit] 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    [junit] 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    [junit] 	at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:422)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:931)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:758)
    [junit] 
    [junit] WARNING: 1 broken segments (containing 60 documents) detected
    [junit] 
    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestOmitTf -Dtestmethod=testMixedMerge -Dtests.seed=-6595054217575280191:5576532348905930588
    [junit] NOTE: test params are: codec=RandomCodecProvider: {noTf=MockSep, tf=Standard, f1=MockRandom, f2=MockVariableIntBlock(baseBlockSize=112)}, locale=cs_CZ, timezone=Chile/Continental
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestAssertions, TestCachingTokenFilter, TestDocument, TestDirectoryReader, TestFlex, TestIndexWriterConfig, TestIndexWriterMerging, TestIndexWriterOnJRECrash, TestMultiReader, TestNewestSegment, TestNorms, TestOmitTf]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=8,threads=1,free=94021800,total=126484480
    [junit] ------------- ---------------- ---------------"
1,"ConstantScoreRangeQuery - fixes ""too many clauses"" exception. ConstantScoreQuery wraps a filter (representing a set of documents) and returns
a constant score for each document in the set.

ConstantScoreRangeQuery implements a RangeQuery that works for any number of
terms in the range.  It rewrites to a ConstantScoreQuery that wraps a RangeFilter.

Still needed:
  - unit tests (these classes have been tested and work fine in-house, but the
current tests rely on too much application specific code)
  - code review of Weight() implementation (I'm unsure If I got all the score
normalization stuff right)
  - explain() implementation

NOTE: requires Java 1.4 for BitSet.nextSetBit()"
1,"Removing a mixin that adds a same-name-sibling child node throws an ItemNotFoundException. Apologies in advance if this has previously been noted in JIRA or on the lists but I couldn't find anything. When removing a mixin that defines a same-name-sibling child an ItemNotFoundException is thrown due to child node indicies not being maintained. A similar method of NodeImpl (onRemove) recongnized this and amended to remove from the tail but the removeMixin also has this issue.

Simple test reproduction:

Types:

[mix:foo] mixin
	+ bar (nt:bar) multiple
	
[nt:bar] > nt:unstructured, mix:referenceable

Code:

public class RemoveMixinTest extends AbstractServerTest {
    private static final String MIXIN = ""mix:foo"";
    private static final String CHILD = ""bar"";
    private static final String PTYPE = ""nt:bar"";
    
    public void testRemoveMixin() throws RepositoryException {
        Session session = getSession();
        Node root = session.getRootNode().addNode(""root"");
        root.addMixin(MIXIN);
        
        root.addNode(CHILD, PTYPE);
        root.addNode(CHILD, PTYPE);
        root.addNode(CHILD, PTYPE);
        
        session.save();
        
        for (NodeIterator it = root.getNodes(); it.hasNext(); ) {
            Node node = it.nextNode();
            System.out.println(node.getPath() + "" : "" + node.getUUID());
        }
        
        try {
            root.removeMixin(MIXIN);
            root.save();
        } catch (RepositoryException ex) {
            ex.printStackTrace();
        }
    }
}

Output:

/root/bar : 0b09e0b4-0727-4194-978a-4eadfbf93fa8
/root/bar[2] : 84d5e556-6f12-43fb-98e3-614bcf1f7bb7
/root/bar[3] : 8db95029-df3b-4e26-affb-438de0206cf5

javax.jcr.ItemNotFoundException: 8db95029-df3b-4e26-affb-438de0206cf5
at org.apache.jackrabbit.core.ItemManager.createItemInstance(ItemManager.java:463)
	at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:319)
	at org.apache.jackrabbit.core.NodeImpl.removeMixin(NodeImpl.java:1212)
	at org.apache.jackrabbit.core.NodeImpl.removeMixin(NodeImpl.java:2624)
	at com.ms.appmw.rcf.server.RemoveMixinTest.testRemoveMixin(RemoveMixinTest.java:48)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at msjava.base.testutils.junit3.MSTestCase.runTest(MSTestCase.java:203)
	at msjava.base.testutils.junit3.TestCaseTearDownEvenIfSetUpFails.runBare(TestCaseTearDownEvenIfSetUpFails.java:92)
	at msjava.base.testutils.junit3.MSTestCase.runBare(MSTestCase.java:170)
	at junit.framework.TestResult$1.protect(TestResult.java:106)

The missing UUID for the last node is the one not found because upon removal of the second the index [2] is still valid. Thanks!"
1,"Corrupted segment file not detected and wipes index contents. Lucene will happily wipe an existing index if presented with a latest generation segments_n file of all zeros. File format documentation says segments_N files should start with a format of -9 but SegmentInfos.read accepts >=0 as valid for backward compatibility reasons.

"
1,"Error creating DB2 table for Journaling. Hi guys,

First of all, congratulations for the fantastic job.

I'm deploying a Jackrattbit-JCA resource adapter to a clustered Websphere environment and using a DB2 for storing data and realized some missing code to add DB2 support.

Here is:

JACKRABBIT-CORE, @ org.apache.jackrabbit.core.util.db, method guessValidationQuery (the last one), I adjusted as follows to add a DB2 validation query:

----------------------------------------------------------------------------------------------------------
    private String guessValidationQuery(String url) {
        if (url.contains(""derby"")) {
            return ""values(1)"";
        } else if (url.contains(""mysql"")) {
            return ""select 1"";
        } else if (url.contains(""sqlserver"") || url.contains(""jtds"")) {
            return ""select 1"";
        } else if (url.contains(""oracle"")) {
            return ""select 'validationQuery' from dual"";
        } else if (url.contains(""h2"")) {
            return ""select 1"";
        } else if (url.contains(""db2"")) {
        	return ""values(1)"";
        }
        log.warn(""Failed to guess validation query for URL "" + url);
        return null;
----------------------------------------------------------------------------------------------------------

And as a final touch, a DDL to build the tables:

JACKRABBIT-CORE, @ src/main/resources/org/apache/jackrabbit/core/journal, added a file named db2.dll as follows (actually I copied this from derby.dll)

----------------------------------------------------------------------------------------------------------
#  Licensed to the Apache Software Foundation (ASF) under one or more
#  contributor license agreements.  See the NOTICE file distributed with
#  this work for additional information regarding copyright ownership.
#  The ASF licenses this file to You under the Apache License, Version 2.0
#  (the ""License""); you may not use this file except in compliance with
#  the License.  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an ""AS IS"" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
create table ${schemaObjectPrefix}JOURNAL (REVISION_ID BIGINT NOT NULL, JOURNAL_ID varchar(255), PRODUCER_ID varchar(255), REVISION_DATA blob)
create unique index ${schemaObjectPrefix}JOURNAL_IDX on ${schemaObjectPrefix}JOURNAL (REVISION_ID)
create table ${schemaObjectPrefix}GLOBAL_REVISION (REVISION_ID BIGINT NOT NULL)
create unique index ${schemaObjectPrefix}GLOBAL_REVISION_IDX on ${schemaObjectPrefix}GLOBAL_REVISION (REVISION_ID)
create table ${schemaObjectPrefix}LOCAL_REVISIONS (JOURNAL_ID varchar(255) NOT NULL, REVISION_ID BIGINT NOT NULL)

# Inserting the one and only revision counter record now helps avoiding race conditions
insert into ${schemaObjectPrefix}GLOBAL_REVISION VALUES(0)
----------------------------------------------------------------------------------------------------------


Best regards,


J Marcos"
1,"ClassDescriptor.hasIdField() fails if id is declared in upper class. org.apache.jackrabbit.ocm.mapper.model.ClassDescriptor.hasIdField() looks up only current class and not the whole hierarchy, so it fails when the id field is declared in a upper class.

hasIdField should use getIdFieldDescriptor and not access idFieldDescriptor field directly, as follows :

    public boolean hasIdField() {
   		return (this.getIdFieldDescriptor() != null && this
    				.getIdFieldDescriptor().isId());
    }

Please find patch enclosed.

Sincerely yours,

Stphane Landelle"
1,"Registering nodetypes with empty namespace prefix causes a namespace exception in sync node. Registering a nodetype with empty namespace prefix causes a namespace exception in sync node. Stacktrace looks as follows:

03.03.2008 15:33:50 *ERROR* ClusterNode: Unable to read revision '10618'. (ClusterNode.java, line 1051)
o.a.j.core.journal.JournalException: Parse error while reading node type definition.
        at o.a.j.core.journal.AbstractRecord.readNodeTypeDef(AbstractRecord.java:256)
        at o.a.j.core.cluster.ClusterNode.consume(ClusterNode.java:1026)
        at o.a.j.core.journal.AbstractJournal.doSync(AbstractJournal.java:198)
        at o.a.j.core.journal.AbstractJournal.sync(AbstractJournal.java:173)
        at o.a.j.core.cluster.ClusterNode.sync(ClusterNode.java:303)
        at o.a.j.core.cluster.ClusterNode.run(ClusterNode.java:274)
        at java.lang.Thread.run(Thread.java:595)
Caused by: o.a.j.core.nodetype.compact.ParseException: Error while parsing 'bla' ((internal), line 3)
        at o.a.j.core.nodetype.compact.Lexer.fail(Lexer.java:152)
        at o.a.j.core.nodetype.compact.CompactNodeTypeDefReader.toQName(CompactNodeTypeDefReader.java:653)
        at o.a.j.core.nodetype.compact.CompactNodeTypeDefReader.doNodeTypeName(CompactNodeTypeDefReader.java:265)
        at o.a.j.core.nodetype.compact.CompactNodeTypeDefReader.parse(CompactNodeTypeDefReader.java:215)
        at o.a.j.core.nodetype.compact.CompactNodeTypeDefReader.<init>(CompactNodeTypeDefReader.java:178)
        at o.a.j.core.nodetype.compact.CompactNodeTypeDefReader.<init>(CompactNodeTypeDefReader.java:162)
        at o.a.j.core.journal.AbstractRecord.readNodeTypeDef(AbstractRecord.java:248)
        ... 6 more
Caused by: javax.jcr.NamespaceException: No URI for pefix '' declared.
        at o.a.j.spi.commons.namespace.NamespaceMapping.getURI(NamespaceMapping.java:74)
        at o.a.j.spi.commons.conversion.NameParser.parse(NameParser.java:116)
        at o.a.j.spi.commons.conversion.ParsingNameResolver.getQName(ParsingNameResolver.java:62)
        at o.a.j.spi.commons.conversion.DefaultNamePathResolver.getQName(DefaultNamePathResolver.java:61)
        at o.a.j.core.nodetype.compact.CompactNodeTypeDefReader.toQName(CompactNodeTypeDefReader.java:646)
        ... 11 more

"
1,"[PATCH] Error in GermanStemmer.java,v 1.11. GermanStemmer.java,v 1.11 in  lucene-1.4-final
 at the end of a word is not replaced by ss"
1,"TaxonomyWriter parents array creation is not thread safe, can cause NPE. Following user list thread [TaxWriter leakage? | http://markmail.org/thread/jkkhemfzpnbdzoft] it appears that if two threads or more are asking for the parent array for the first time, a context switch after the first thread created the empty parents array but before it initialized it would cause the other array to use an uninitialized array, causing an NPE. Fix is simple: synchronize the method getParentArray()"
1,"CJK char list. Seems the character list in the CJK section of the StandardTokenizer.jj is not quite complete. Following is a more complete list:

< CJK:                                          // non-alphabets
      [
	   ""\u1100""-""\u11ff"",
       ""\u3040""-""\u30ff"",
       ""\u3130""-""\u318f"",
       ""\u31f0""-""\u31ff"",
       ""\u3300""-""\u337f"",
       ""\u3400""-""\u4dbf"",
       ""\u4e00""-""\u9fff"",
       ""\uac00""-""\ud7a3"",
       ""\uf900""-""\ufaff"",
       ""\uff65""-""\uffdc""       
      ]
  >

"
1,"BoostingTermQuery's explanation should be marked as Match even if the payload part negated or zero'ed it. Since BTQ multiplies the payload on the score it might return a negative score.
The explanation should be marked as ""Match"" otherwise it is not added to container explanations,
See also in LUCENE-1302."
1,"Versioning does not make use of tx. for example:

- begin transaction
- create node
- add mix:versionable
- save --> will create version
- checkin
- cancel transaction

should not leave any traces in versioning"
1,"Jcr-Remoting: PathNotFoundException if item name ends with .json. the jcr-remoting-servlet contains the following commented issue:

    * TODO: TOBEFIXED will not behave properly if resource path (i.e. item name)
    * TODO  ends with .json extension and/or contains a depth-selector pattern."
1,"System-view export/import of multi-value property does not respect JCR 2.0. JCR 2.0 has a defined specification about system-view export of multi-value properties when these property have only one value.
The attribute sv:multiple attribute of sv:property tag is not written in output stream result.

Anyway if I add that one manually and try to import the modified system-view, multi-value properties with an only value are not recognized, and these properties are simply stored like a single-value ones.
"
1,Use of Multi-Args URI Causes URI-Rewriting to improperly unescape characters. See: http://www.nabble.com/unable-to-encode-reserved-characters-using-java.net.URI-multi-arg-constructors-td14954679.html for information from the httpclient-dev thread.  The basic idea is that URI's multi-arg constructors break things.
1,"Calling PropertyDef.getDefaultValue() via RMI results in Exception. hi jukka

30.03.2005 15:28:23 *MARK * servletengine: Servlet threw exception: 
org.apache.jackrabbit.rmi.client.RemoteRuntimeException: java.rmi.UnmarshalException: error unmarshalling return; nested exception is: 
	java.io.WriteAbortedException: writing aborted; java.io.NotSerializableException: javax.jcr.BooleanValue
	at org.apache.jackrabbit.rmi.client.ClientPropertyDef.getDefaultValues(ClientPropertyDef.java:76)

[...]

regards
angela"
1,"offsets issues with multiword synonyms. as reported on the list, there are some strange offsets with FSTSynonyms, in the case of multiword synonyms.

as a workaround it was suggested to use the older synonym impl, but it has bugs too (just in a different way).
"
1,"[Lock] weird number for ""infinite"". (this is a follow-up of JCR-3205)

i am surprised by the davex reply to a lock request with infinite timeout (before and after the fix from JCR-3205):


<D:timeout>Second-2147483</D:timeout>

this number is
2^21+50331

which seems pretty random to me. coincidally, this number is exactly 2^31 - 1 (2147483647) without the last 3 digits. can it be that there are some weird string operations happening on server side?
"
1,"Term vectors missing after addIndexes + optimize. I encountered a problem with addIndexes where term vectors disappeared following optimize(). I wrote a simple test case which demonstrates the problem. The bug appears with both addIndexes() versions, but does not appear if addDocument is called twice, committing changes in between.

I think I tracked the problem down to IndexWriter.mergeMiddle() -- it sets term vectors before merger.merge() was called. In the addDocs case, merger.fieldInfos is already populated, while in the addIndexes case it is empty, hence fieldInfos.hasVectors returns false.

will post a patch shortly."
1,"IndexReader.isCurrent() lies if documents were only removed by latest commit. Usecase is as following:

1. Get indexReader via indexWriter.
2. Delete documents by Term via indexWriter. 
3. Commit indexWriter.
4. indexReader.isCurrent() returns true.

Usually there is a check if index reader is current. If not then it is reopened (re-obtained via writer or ect.). But this causes the problem when documents can still be found through the search after deletion.
Testcase is attached."
1,"JCRUrlConnection relies on nt:file/nt:resource. The JCRUrlConnection class implementing the jcr: URL handler for the JCR class loader relies on the fact that the intended primary type of the jcr:content child node of an nt:file node is of type nt:resource. When writing files with the Jackrabbit WebDAV server this is not the case as the jcr:content child node is of type nt:unstructured.

As a result the JCRUrlConnection.connect method fails with an ItemNotFoundException in the Util.getProperty(Item)  method because the primary item of the nt:unstructured node type is not defined."
1,Query may throw ArrayIndexOutOfBoundsException. There's a bug in DescendantSelfAxisQuery.DescendantSelfAxisScorer.skipTo() that causes the exception.
1,"JCR2SPI: several broken equals() comparisons. Detected by FindBugs:

H C EC: Call to equals() comparing unrelated class and interface in org.apache.jackrabbit.jcr2spi.nodetype.NodeTypeManagerImpl.nodeTypeReRegistered(QName)	xythos-jcr/src/main/java/org/apache/jackrabbit/jcr2spi/nodetype	NodeTypeManagerImpl.java	line 218	1190978573312	1664752
H C EC: Call to equals() comparing unrelated class and interface in org.apache.jackrabbit.jcr2spi.nodetype.NodeTypeManagerImpl.nodeTypeReRegistered(QName)	xythos-jcr/src/main/java/org/apache/jackrabbit/jcr2spi/nodetype	NodeTypeManagerImpl.java	line 227	1190978573312	1664753
H C EC: Call to equals() comparing unrelated class and interface in org.apache.jackrabbit.jcr2spi.nodetype.NodeTypeManagerImpl.nodeTypeUnregistered(QName)	xythos-jcr/src/main/java/org/apache/jackrabbit/jcr2spi/nodetype	NodeTypeManagerImpl.java	line 255	1190978573312	1664754
H C EC: Call to equals() comparing unrelated class and interface in org.apache.jackrabbit.jcr2spi.nodetype.NodeTypeManagerImpl.nodeTypeUnregistered(QName)	xythos-jcr/src/main/java/org/apache/jackrabbit/jcr2spi/nodetype	NodeTypeManagerImpl.java	line 264	1190978573312	1664755
H C EC: org.apache.jackrabbit.jcr2spi.WorkspaceManager.canAccess(String) uses equals to compare an array and nonarray	
"
1,"CMS fails to cleanly stop threads. When you close IW, it waits for (or aborts and then waits for) all running merges.

However, it's wait criteria is wrong -- it waits for the threads to be done w/ their merges, not for the threads to actually die.

CMS already has a sync() method, to wait for running threads, which we can call from CMS.close.  However it has a thread hazard because a MergeThread removes itself from mergeThreads before it actually exits.  So sync() is able to return even while a merge thread is still running.

This was uncovered by LUCENE-2819 on the test case TestCustomScoreQuery.testCustomExternalQuery, though I expect other test cases would show it."
1,"CachingHttpClient leaks connections with stale-if-error. If you are using the ""stale-if-error"" Cache-control header and CachingHttpClient decides to use a stale cached response it does not clean up the existing backend response.

This bug causes connections to leak from the connection pool each time the stale-if-error flow is executed.
"
1,"Merge error during add to index (IndexOutOfBoundsException). I've been batch-building indexes, and I've build a couple hundred indexes with 
a total of around 150 million records.  This only happened once, so it's 
probably impossible to reproduce, but anyway... I was building an index with 
around 9.6 million records, and towards the end I got this:

java.lang.IndexOutOfBoundsException: Index: 54, Size: 24
        at java.util.ArrayList.RangeCheck(ArrayList.java:547)
        at java.util.ArrayList.get(ArrayList.java:322)
        at org.apache.lucene.index.FieldInfos.fieldInfo(FieldInfos.java:155)
        at org.apache.lucene.index.FieldInfos.fieldName(FieldInfos.java:151)
        at org.apache.lucene.index.SegmentTermEnum.readTerm(SegmentTermEnum.java
:149)
        at org.apache.lucene.index.SegmentTermEnum.next
(SegmentTermEnum.java:115)
        at org.apache.lucene.index.SegmentMergeInfo.next
(SegmentMergeInfo.java:52)
        at org.apache.lucene.index.SegmentMerger.mergeTermInfos
(SegmentMerger.java:294)
        at org.apache.lucene.index.SegmentMerger.mergeTerms
(SegmentMerger.java:254)
        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:93)
        at org.apache.lucene.index.IndexWriter.mergeSegments
(IndexWriter.java:487)
        at org.apache.lucene.index.IndexWriter.maybeMergeSegments
(IndexWriter.java:458)
        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:310)
        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:294)"
1,"JCA Concurrent Modification Exception when JCAManagedConnection.cleanup() called. The JCAManagedConnection.closeHandles() method causes a ConcurrentModificationException if the handles list is not empty.
This is caused by modification of the handles list by removeHandle(), while closeHandles() is iterating over the list.

Under SunOne AppServer 7 this can be caused simply by not closing the Session handle before the transaction commits.

It is probably not even necessary to send connectionClosed events during cleanup().  According to the API for connectionClosed, the event indicates that an application component has closed  the connection handle.  cleanup() is a container initiated action, and so the connectionClosed event is not applicable.


java.util.ConcurrentModificationException
    at java.util.LinkedList$ListItr.checkForComodification(LinkedList.java:552)
    at java.util.LinkedList$ListItr.next(LinkedList.java:488)
    at org.apache.jackrabbit.jca.JCAManagedConnection.closeHandles(JCAManagedConnection.java:382)
    at org.apache.jackrabbit.jca.JCAManagedConnection.cleanup(JCAManagedConnection.java:145)
    at com.sun.enterprise.resource.IASPoolObjectImp.cleanup(IASPoolObjectImp.java:243)
    at com.sun.enterprise.resource.IASGenericPoolObjects.transactionCompleted(IASGenericPoolObjects.java:794)
    at com.sun.enterprise.resource.ResourcePoolManagerImpl.transactionCompleted(ResourcePoolManagerImpl.java:347)
    at com.sun.enterprise.resource.ResourcePoolManagerImpl$SynchronizationListener.afterCompletion(ResourcePoolManagerImpl.java:644)
    at com.sun.jts.jta.SynchronizationImpl.after_completion(SynchronizationImpl.java:70)

"
1,"SegmentTermEnum.next() doesn't maintain prevBuffer at end. When you're iterating a SegmentTermEnum and you go past the end of the docs, you end up with a state where the nextBuffer = null and the prevBuffer is the penultimate term, not the last term.  This patch fixes it.  (It's also required for my Prefetching bug [LUCENE-506])

Index: java/org/apache/lucene/index/SegmentTermEnum.java
===================================================================
--- java/org/apache/lucene/index/SegmentTermEnum.java	(revision 382121)
+++ java/org/apache/lucene/index/SegmentTermEnum.java	(working copy)
@@ -109,6 +109,7 @@
   /** Increments the enumeration to the next element.  True if one exists.*/
   public final boolean next() throws IOException {
     if (position++ >= size - 1) {
+      prevBuffer.set(termBuffer);
       termBuffer.reset();
       return false;
     }
"
1,TestPagedBytes failure. ant test -Dtestcase=TestPagedBytes -Dtestmethod=testDataInputOutput -Dtests.seed=268db1f3329b70d:3125365bc9c56c90:116e02aa4a70ec2f -Dtests.multiplier=5
1,"DatabaseFileSystem's logger references the wrong class. In DatabaseFileSystem, the logger is constructed as
private static Logger log = LoggerFactory.getLogger(DbFileSystem.class);

It should be constructed as:
private static Logger log = LoggerFactory.getLogger(DatabaseFileSystem.class);"
1,"TestSimpleExplanations failure. {noformat}
ant test -Dtestcase=TestSimpleExplanations -Dtestmethod=testDMQ8 -Dtests.seed=-7e984babece66153:3e3298ae627b33a9:3093059db62bcc71
{noformat}

fails w/ this on current trunk... looks like silly floating point precision issue:

{noformat}

    [junit] Testsuite: org.apache.lucene.search.TestSimpleExplanations
    [junit]   1.4508595 = (MATCH) sum of:
    [junit]     1.4508595 = (MATCH) weight(field:yy in 2) [DefaultSimilarity], result of:
    [junit]       1.4508595 = score(doc=2,freq=1.0 = termFreq=1
    [junit] ), product of:
    [junit]         1.287682 = queryWeight, product of:
    [junit]           1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]           1.0 = queryNorm
    [junit]         1.1267219 = fieldWeight in 2, product of:
    [junit]           1.0 = tf(freq=1.0), with freq of:
    [junit]             1.0 = termFreq=1
    [junit]           1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]           0.875 = fieldNorm(doc=2)
    [junit]   145085.95 = (MATCH) weight(field:xx^100000.0 in 2) [DefaultSimilarity], result of:
    [junit]     145085.95 = score(doc=2,freq=1.0 = termFreq=1
    [junit] ), product of:
    [junit]       128768.2 = queryWeight, product of:
    [junit]         100000.0 = boost
    [junit]         1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]         1.0 = queryNorm
    [junit]       1.1267219 = fieldWeight in 2, product of:
    [junit]         1.0 = tf(freq=1.0), with freq of:
    [junit]           1.0 = termFreq=1
    [junit]         1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]         0.875 = fieldNorm(doc=2)
    [junit]  expected:<145086.66> but was:<145086.69>)
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.544 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestSimpleExplanations -Dtestmethod=testDMQ8 -Dtests.seed=144152895b276837:eb7ba4953db943f:33373b79a971db02
    [junit] NOTE: test params are: codec=PreFlex, sim=RandomSimilarityProvider(queryNorm=false,coord=false): {field=DefaultSimilarity, alt=DFR I(ne)LZ(0.3), KEY=IB LL-D2}, locale=en_IN, timezone=Pacific/Samoa
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestSimpleExplanations]
    [junit] NOTE: Linux 2.6.33.6-147.fc13.x86_64 amd64/Sun Microsystems Inc. 1.6.0_21 (64-bit)/cpus=24,threads=1,free=130426744,total=189988864
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testDMQ8(org.apache.lucene.search.TestSimpleExplanations):	FAILED
    [junit] ((field:yy field:w5^100.0) | field:xx^100000.0)~0.5: score(doc=2)=145086.66 != explanationScore=145086.69 Explanation: 145086.69 = (MATCH) max plus 0.5 times others of:
    [junit]   1.4508595 = (MATCH) sum of:
    [junit]     1.4508595 = (MATCH) weight(field:yy in 2) [DefaultSimilarity], result of:
    [junit]       1.4508595 = score(doc=2,freq=1.0 = termFreq=1
    [junit] ), product of:
    [junit]         1.287682 = queryWeight, product of:
    [junit]           1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]           1.0 = queryNorm
    [junit]         1.1267219 = fieldWeight in 2, product of:
    [junit]           1.0 = tf(freq=1.0), with freq of:
    [junit]             1.0 = termFreq=1
    [junit]           1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]           0.875 = fieldNorm(doc=2)
    [junit]   145085.95 = (MATCH) weight(field:xx^100000.0 in 2) [DefaultSimilarity], result of:
    [junit]     145085.95 = score(doc=2,freq=1.0 = termFreq=1
    [junit] ), product of:
    [junit]       128768.2 = queryWeight, product of:
    [junit]         100000.0 = boost
    [junit]         1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]         1.0 = queryNorm
    [junit]       1.1267219 = fieldWeight in 2, product of:
    [junit]         1.0 = tf(freq=1.0), with freq of:
    [junit]           1.0 = termFreq=1
    [junit]         1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]         0.875 = fieldNorm(doc=2)
    [junit]  expected:<145086.66> but was:<145086.69>
    [junit] junit.framework.AssertionFailedError: ((field:yy field:w5^100.0) | field:xx^100000.0)~0.5: score(doc=2)=145086.66 != explanationScore=145086.69 Explanation: 145086.69 = (MATCH) max plus 0.5 times others of:
    [junit]   1.4508595 = (MATCH) sum of:
    [junit]     1.4508595 = (MATCH) weight(field:yy in 2) [DefaultSimilarity], result of:
    [junit]       1.4508595 = score(doc=2,freq=1.0 = termFreq=1
    [junit] ), product of:
    [junit]         1.287682 = queryWeight, product of:
    [junit]           1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]           1.0 = queryNorm
    [junit]         1.1267219 = fieldWeight in 2, product of:
    [junit]           1.0 = tf(freq=1.0), with freq of:
    [junit]             1.0 = termFreq=1
    [junit]           1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]           0.875 = fieldNorm(doc=2)
    [junit]   145085.95 = (MATCH) weight(field:xx^100000.0 in 2) [DefaultSimilarity], result of:
    [junit]     145085.95 = score(doc=2,freq=1.0 = termFreq=1
    [junit] ), product of:
    [junit]       128768.2 = queryWeight, product of:
    [junit]         100000.0 = boost
    [junit]         1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]         1.0 = queryNorm
    [junit]       1.1267219 = fieldWeight in 2, product of:
    [junit]         1.0 = tf(freq=1.0), with freq of:
    [junit]           1.0 = termFreq=1
    [junit]         1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]         0.875 = fieldNorm(doc=2)
    [junit]  expected:<145086.66> but was:<145086.69>
    [junit] 	at org.apache.lucene.search.CheckHits.verifyExplanation(CheckHits.java:324)
    [junit] 	at org.apache.lucene.search.CheckHits$ExplanationAsserter.collect(CheckHits.java:494)
    [junit] 	at org.apache.lucene.search.Scorer.score(Scorer.java:60)
    [junit] 	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:580)
    [junit] 	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:363)
    [junit] 	at org.apache.lucene.search.CheckHits.checkExplanations(CheckHits.java:302)
    [junit] 	at org.apache.lucene.search.QueryUtils.checkExplanations(QueryUtils.java:92)
    [junit] 	at org.apache.lucene.search.QueryUtils.check(QueryUtils.java:126)
    [junit] 	at org.apache.lucene.search.QueryUtils.check(QueryUtils.java:122)
    [junit] 	at org.apache.lucene.search.QueryUtils.check(QueryUtils.java:106)
    [junit] 	at org.apache.lucene.search.CheckHits.checkHitCollector(CheckHits.java:89)
    [junit] 	at org.apache.lucene.search.TestExplanations.qtest(TestExplanations.java:99)
    [junit] 	at org.apache.lucene.search.TestSimpleExplanations.testDMQ8(TestSimpleExplanations.java:224)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$2$1.evaluate(LuceneTestCase.java:611)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:148)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:50)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.search.TestSimpleExplanations FAILED
{noformat}"
1,"Deadlock when executing Version operations. This only happens when a XA transaction is committed without changes. In XAVersionManager there is a check in the InternalXAResource returned by getXAResourceBegin(), which only acquires the write lock on the version manager if there are version related changes in the transaction. This kind of check is missing in the methods XAVersionManager.prepare/commit/rollback()."
1,"MaxCount not working correctly in user/group query when restricting to group members. For user/group queries having a scope *and* a limit clause maxCount does not work correctly.

    builder.setScope(""contributors"", false);
    builder.setLimit(0, 50);

In the above case, the result might contain to few results. 

This is related to JCR-2829"
1,"Merging implemented by codecs must catch aborted merges. This is a regression (we lost functionality on landing flex).

When you close IW with ""false"" (meaning abort all running merges), IW asks the merge threads to abort.  The threads are supposed to periodically check if they are aborted and throw an exception if so.

But on the cutover to flex, where the codec can override how merging is done (but a default impl is in the base enum classes), we lost this."
1,"CloseableThreadLocal should allow null Objects. CloseableThreadLocal does not allow null Objects in its get() method, but does nothing to prevent them in set(Object). The comment in get() before assert v != null is irrelevant - the application might have passed null.

Null is an important value for Analyzers. Since tokenStreams (a ThreadLocal private member in Analyzer) is not accessible by extending classes, the only way for an Analyzer to reset the tokenStreams is by calling setPreviousTokenStream(null).

I will post a patch w/ a test"
1,"thread starving in MultiThreadedHttpConnectionManager. Hi folks,

I might have found a bug in MTHCM. It has to do with removing HostConnectionPool instances that have no more connections in them. That was a fix for a memory leak we previously had. There are two cases where the pools get deleted. One is in handleLostConnection: (excerpt)
  ...
  if (hostPool.numConnections == 0) mapHosts.remove(config);
  notifyWaitingThread(config);
  ...

Could this delete a pool in which there is still a thread waiting to get a connection? If so, the thread would remain in the global pool. But even if it is interrupted there, it would still use the old HostConnectionPool in which no connection will ever become available again.

I suggest to change the removal check in both cases to:
  if ((hostPool.numConnections < 1) && hostPool.waitingThreads.isEmpty)

What do you think?"
1,checkindex fails if docfreq >= skipInterval and term is indexed more than once at same position. This is a bad check in the skipping verification logic
1,"if index is too old you should hit an exception saying so. If you create an index in 2.3.x (I used demo's IndexFiles) and then try to read it in 4.0.x (I used CheckIndex), you hit a confusing exception like this:
{noformat}
java.io.IOException: read past EOF
        at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:154)
        at org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:39)
        at org.apache.lucene.store.ChecksumIndexInput.readByte(ChecksumIndexInput.java:40)
        at org.apache.lucene.store.DataInput.readInt(DataInput.java:76)
        at org.apache.lucene.index.SegmentInfo.<init>(SegmentInfo.java:171)
        at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:230)
        at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:269)
        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:649)
        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:484)
        at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:265)
        at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:308)
        at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:287)
        at org.apache.lucene.index.CheckIndex.main(CheckIndex.java:930)
{noformat}

I think instead we should throw an IndexTooOldException or something like that?"
1,"when opening the merged SegmentReader, IW attempts to open store files that were deleted. The issue happens when a merge runs that does not merge the doc stores, those doc stores are still being written to, IW is using CFS, and while the merge is running the doc stores get closed and turned into a cfx file.

When we then try to open the reader (for warming), which as of LUCENE-2311 will now [correctly] open the doc stores, we hit FNFE because the SegmentInfo for the merge does not realize that the doc stores were turned into  a cfx.

This issue does affect trunk; if you crank up the #docs in the test, it happens consistently (I will tie this to _TestUtil.getRandomMultiplier!)."
1,"make spell checker test case work again. See attached path which makes the spellchecker test case work again. The problem without the patch is that consecutive calls to indexDictionary() will create a spelling index with duplicate words. Does anybody see a problem with this patch? I see that the spellchecker code is now used in Solr, isn't it? I didn't have time to test this patch inside Solr.

Also see http://issues.apache.org/jira/browse/LUCENE-632, but the null check is included in this patch so the NPE described there cannot happen anymore.
"
1,"Incorrect CND for mix:etag. Jackrabbit currently defined mix:etag as follows:

[mix:etag]
  mixin
  // currently has a default value because auto-creation not handled see JCR-2116
  - jcr:etag (STRING) = '' protected autocreated

I think this violates the spec, which says:

[mix:etag] mixin
  - jcr:etag (STRING) protected autocreated

This also affects the predefined node type test in jackrabbit-jcr-tests where mix-etag.txt is:

NodeTypeName
  mix:etag
Supertypes
  []
IsMixin
  true
HasOrderableChildNodes
  false
PrimaryItemName
  null
PropertyDefinition
  Name jcr:etag
  RequiredType STRING
  DefaultValues []
  AutoCreated true
  Mandatory false
  OnParentVersion COPY
  Protected true
  Multiple false

but should rather be:

NodeTypeName
  mix:etag
Supertypes
  []
IsMixin
  true
HasOrderableChildNodes
  false
PrimaryItemName
  null
PropertyDefinition
  Name jcr:etag
  RequiredType STRING
  DefaultValues null               <===
  AutoCreated true
  Mandatory false
  OnParentVersion COPY
  Protected true
  Multiple false
"
1,"In JCAConnectionRequestInfo, equals() and hashCode() implementations are inconsistent. JCAConnectionRequestInfo behaves differently in its equals() and hashCode() methods. The former is aware about SimpleCredentials structure, so two instances of JCAConnectionRequestInfo were supplied SimpleCredentials instances with same userID, password and attributes, they are considered equal.
But JCAConnectionRequestInfo.hashCode() just delegates to SimpleCredentials.hashCode() which is same as Object's method. This breaks session pooling."
1,"String literal must not interpret entity references. The ampersand character in a string literal is interpreted as a start character for an entity reference. This is because Jackrabbit uses an XQuery parser where a string literal is slightly more constraint than in XPath.

Example:

//element(*, nt:base)[jcr:contains(., 'max&moritz')]

throws a parse exception. Instead the parser should simply recognize the ampersand as regular character."
1,"[PATCH] ClassDescriptor.hasIdField uses faulty logic. hasIdField tries to compare a FieldDescriptor to an empty string, which doesn't make sense, here:

     public boolean hasIdField() {
        return (this.idFieldDescriptor != null && ! this.idFieldDescriptor.equals(""""));
     }


i'm assuming it should be

       return (this.idFieldDescriptor != null && this.idFieldDescriptor.isId());

patch does this

"
1,"PROPFIND response to a request for a property that does not exist reports an empty DAV:prop element  . A PROPFIND response to a request for a property that does not exist reports an empty DAV:prop element:

Request:

<propfind xmlns=""DAV:""><prop><doesnotexist/></prop></propfind>

Response:

<D:multistatus xmlns:xml='http://www.w3.org/XML/1998/namespace' xmlns:D='DAV:'>
  <D:response>
    <D:href>...</D:href>
    <D:propstat>
      <D:prop/>
      <D:status>HTTP/1.1 200 OK</D:status>
    </D:propstat>
    <D:propstat>
      <D:prop><D:doesnotexist/></D:prop>
      <D:status>HTTP/1.1 404 Not Found</D:status>
    </D:propstat>
  </D:response>
</D:multistatus>

  "
1,"Can not set the ""Proxy-connection"" header. When using a proxy the HttpClient refuses to set the ""Proxy-connection"" header
to the value ""close"". The value will be converted to ""keep-alive"" when the final
request is sent to network.

The following code snippet can be used to replicate the defect. Method is GET:
...
method.removeRequestHeader(""Proxy-Connection"");
logger.debug(""Proxy-Connection header removed."");
method.addRequestHeader(""Proxy-Connection"", ""close"");
logger.debug(""Proxy-Connection header set to: "" +
method.getRequestHeader(""Proxy-Connection"") );
try {
  	int statusCode = httpclient.executeMethod( method );
...

Now if you look at the wire log, you will notice that the actual value will be
""keep-alive""."
1,"Buffered deletes are not flushed by RAM or count. When a segment is flushed, we will generally NOT flush the deletes, ie we simply buffer up the pending delete terms/queries, and the only apply them if 1) a segment is going to be merged (so we can remove the del docs in that segment), or 2) the buffered deletes' RAM exceeds 1/2 of IW's RAM limit when we are flushing a segment, or 3) the buffered deletes count exceeds IWC's maxBufferedDeleteTerms.

But the latter 2 triggers are currently broken on trunk; I suspect (but I'm not sure) when we landed DWPT we introduced this bug."
1,"SSL connections cannot be established using the IP address. HttpClient 4.x introduced a regression in establishing SSL connections to remote peers. The AbstractVerifier class only checks for matches in CN and SubjectAlternative->DNSName. But, when an IP (instead of a hostname) is used, the check should be done on CN and SubjectAlternative->IPAddress."
1,"Lucene is not fsync'ing files on commit. Thanks to hurricane Irene, when Mark's electricity became unreliable, he discovered that on power loss Lucene could easily corrumpt the index, which of course should never happen...

I was able to easily repro, by pulling the plug on an Ubuntu box during indexing.  On digging, I discovered, to my horror, that Lucene is failing to fsync any files, ever!

This bug was unfortunately created when we committed LUCENE-2328... that issue added tracking, in FSDir, of which files have been closed but not sync'd, so that when sync is called during IW.commit we only sync those files that haven't already been sync'd.

That tracking is done via the FSDir.onIndexOutputClosed callback, called when an FSIndexOutput is closed.  The bug is that we only call it on exception during close:

{noformat}

    @Override
    public void close() throws IOException {
      // only close the file if it has not been closed yet
      if (isOpen) {
        boolean success = false;
        try {
          super.close();
          success = true;
        } finally {
          isOpen = false;
          if (!success) {
            try {
              file.close();
              parent.onIndexOutputClosed(this);
            } catch (Throwable t) {
              // Suppress so we don't mask original exception
            }
          } else
            file.close();
        }
      }
    }
{noformat}

And so FSDir thinks no files need syncing when its sync method is called....

I think instead we should call it up-front; better to over-sync than under-sync.

The fix is trivial (move the callback up-front), but I'd love to somehow have a test that can catch such a bad regression in the future.... still I think we can do that test separately and commit this fix first.

Note that even though LUCENE-2328 was backported to 2.9.x and 3.0.x, this bug wasn't, ie the backport was a much simpler fix (to just address the original memory leak); it's 3.1, 3.2, 3.3 and trunk when this bug is present."
1,"JCR2SPI: Workspace.getImportHandler creates a handler which doesn't work properly under JDK 1.4.. JCR2SPI returns an import handler which delegates work to a SAXTransformerHandler. In JDK, that one has a known issue not processing namespace prefix mappings properly (will attach a separate test case).

Proposals:

- drop JDK 1.4 support
- tune the JCR2SPI handler to create namespace attributes when needed
- use an entirely different serializer

My personal preference would be just to drop JDK 1.4 support, but that may not be acceptable for everyone.
"
1,"MultiThreadedHttpConnectionManager daemon Thread never GC'd. One of my colleagues was invoking HttpClient by way of a loop something like this:

for (int i = 0; i < 300; i++) {
    GetMethod method = new
GetMethod(""http://cvs.apache.org/viewcvs/jakarta-commons/httpclient/"");
    try {
        HttpClient httpClient = new HttpClient(new
MultiThreadedHttpConnectionManager());
        httpClient.executeMethod(method);
        byte[] bytes = method.getResponseBody();
    } finally {
        // always release the connection after we're done
        method.releaseConnection();
    }
}

He's in the process of revising his code so that he doesn't do this loop, which
other developers might point out as a non-optimal use, but along the way, he
discovered that the daemon thread that the MultiThreadedHttpConnectionManager
makes does not get garbage collected.  Of course, the connection manager itself
is also never gc'd.  While I think we can avoid this problem in our code, in the
more general case, clients may not actually be able to control the number of
MultiThreadedConnectionManagers they create, which could eventually cause
problems.  This makes me think the problem is deserving of a patch.

We found this problem with 2.0rc2, although presumably it also exists with the
CVS HEAD.

Patch to follow."
1,"Access to VirtualNodeTypeStateManager.virtualProvider should be guarded. The virtualProvider field of the VirtualNodeTypeStateManager class is dynamically created by the getter method. Two methods of the class access that field directly though risking NullPointerException.

Access should be guarded against the field being not assigned yet."
1,NamespaceRegistryImpl.getNameResolver/getPathResolver always return null. This seems to be a left over from restructuring commons classes: JCR-1169. Those methods should be removed.
1,"[PATCH] Javadoc improvements and minor fixes. Javadoc improvements for Scorer.java and Weight.java. 
This also fixes some recent changes introduced minor warnings when building 
the javadocs and adds a small comment in Similarity.java. 
The individual patches will be attached."
1,"jcr2spi: versionmanager#checkout(NodeState) should not forward to checkout(NodeState, NodeId). VersionManager#checkout(NodeState nodeState) is called if activity is not supported and thus should call the
corresponding SPI method instead of checkout(NodeState, NodeId activityId)"
1,"[jcr-rmi] xpath queries don't work when the underlying QueryResult doesn't return the rows/nodes size. o.a.j.rmi.server.ServerQueryResult assumes the underlying NodeIterator always return the number of elements, but it migth return -1 in some cases [1], AFAIK depending on the jcr impl. When -1 is returned jcr-rmi fails to return the QueryResult. 

e.g. it fails with the following xpath query //*

[1] http://www.day.com/maven/jsr170/javadocs/jcr-1.0/javax/jcr/RangeIterator.html#getSize()"
1,"index corruption autoCommit=false. In both Lucene 2.3 and trunk, the index becomes corrupted when autoCommit=false"
1,"SQL2 Join with OR clause still has some issues. There are still some issues with Joins that have OR clauses in them. I changed the test, so that it reflects the changes"
1,"System view XML uses hardcoded sv: prefix. Jackrabbit enforces that the xml docment that imported into repository through the use of  ContentHandler have attributte ""name"" with specific prefix (""sv""), instead of specific namespace (""com.cisco.topos.jcr.sv"").

Example of wrong behavior:
Calling
marshaller.marshal(entry, session.getImportContentHandler(session.getNodeByUUID(channelId).getPath(), ImportUUIDBehavior.IMPORT_UUID_CREATE_NEW));

where entry is object that represent xml structure with namespace ""com.cisco.topos.jcr.sv"" assigned to prefix other then ""sv"" or as default namespace  will cause  exception
java.lang.RuntimeException: javax.xml.bind.MarshalException

 javax.jcr.InvalidSerializedDataException: missing mandatory sv:name attribute of element sv:node
	at org.apache.jackrabbit.core.xml.SysViewImportHandler.startElement(SysViewImportHandler.java:122)
	at org.apache.jackrabbit.core.xml.ImportHandler.startElement(ImportHandler.java:192)
	at com.sun.xml.bind.v2.runtime.output.SAXOutput.endStartTag(SAXOutput.java:80)
	at com.sun.xml.bind.v2.runtime.XMLSerializer.endAttributes(XMLSerializer.java:273)
	at com.sun.xml.bind.v2.runtime.XMLSerializer.childAsSoleContent(XMLSerializer.java:531)
	at com.sun.xml.bind.v2.runtime.ClassBeanInfoImpl.serializeRoot(ClassBeanInfoImpl.java:283)
	at com.sun.xml.bind.v2.runtime.XMLSerializer.childAsRoot(XMLSerializer.java:461)
	at com.sun.xml.bind.v2.runtime.MarshallerImpl.write(MarshallerImpl.java:292)
	... 24 more


"
1,"The class from cotrub directory org.apache.lucene.index.IndexSplitter creates a non correct index. When using the method IndexSplitter.split(File destDir, String[] segs) from the Lucene cotrib directory (contrib/misc/src/java/org/apache/lucene/index) it creates an index with segments descriptor file with wrong data. Namely wrong is the number representing the name of segment that would be created next in this index.
If some of the segments of the index already has this name this results either to impossibility to create new segment or in crating of an corrupted segment."
1,"DefaultRedirectHandler does not access correct HttpParams. In the getLocationURI(HttpResponse, HttpContext) method, the HttpParams for determining REJECT_RELATIVE_REDIRECT and ALLOW_CIRCULAR_REDIRECTS are retrieved with:

HttpParams params = response.getParams();

The response HttpParams do not contain these values, however the request HttpParams do. The correct implementation is:

HttpRequest request = (HttpRequest) context.getAttribute(HttpExecutionContext.HTTP_REQUEST);
HttpParams params = request.getParams();

"
1,"AccessManager + CachingHierarchyManager problem. The problem we have is the implementation of the CachingHierarchyManager,
to which the SimpleAccessManager holds a reference.

Let's consider following example:
i add 3 subnodes (a,b,c) to a node and after that i reorder b and c ..
so i have a,c,b. in the process of reordering (using the function
orderBefore of javax.jcr.Node) our AccessManager is called several times to check the permissions of the nodes. In this AccessManager we use some
functions of the CachingHierarchyManager, f.ex.

Path itemPath = hierMgr.getPath(id);
return itemPath.denotesRoot();

or

Path itemPath = hierMgr.getPath(itemId);
Path parentPath = itemPath.getAncestor(1);
return hierMgr.resolvePath(parentPath);

the problem is, that when calling the methods of the
CachingHierarchyManager the nodes i ask for will be cached in the idCache in a wrong state (i. e.: before actually reordering the elements).
so if i want f.ex. delete the node b after reordering, the node will
be looked up in the idCache. in the cache the index of node b is still 2
(actually it should be 3) and so the wrong node will be deleted! "
1,"Missing a null check in BooleanQuery.toString(String). Our queryParser/tokenizer in some situations creates null query and was added as a clause to Boolean query.
When we try to log the query, NPE is thrown from log(booleanQuery).

In BooleanQuery.toString(String), a simple null check is overlooked.
"
1,"MemoryIndex doesn't call TokenStream.reset() and TokenStream.end(). MemoryIndex from contrib/memory does not honor the contract for a consumer of a TokenStream

will work up a patch right quick"
1,"AbstractClientConnAdapter prone to concurrency issues. AbstractClientConnAdapter is currently prone to all sorts of concurrency issues. (1) Access to internal state is not properry synchronized making the class prone  to race conditions. Presently none of the instance variables is even declared volatile. (2) AbstractClientConnAdapter treats aborted connection as one in an illegal state, which is not quite right.

Oleg"
1,"java.lang.IllegalStateException: Connection already open.. I am seeing many of the same problems noted in HTTPCLIENT-741 using the latest builds from the maven repo.

java.lang.IllegalStateException: Connection already open.
        at org.apache.http.impl.conn.AbstractPoolEntry.open(AbstractPoolEntry.java:150)
        at org.apache.http.impl.conn.AbstractPooledConnAdapter.open(AbstractPooledConnAdapter.java:119)
        at org.apache.http.impl.client.DefaultClientRequestDirector.execute(DefaultClientRequestDirector.java:308)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:501)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:456)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:422)
        at com.hi5.os.Hi5RemoteContentFetcher.fetch(Hi5RemoteContentFetcher.java:279)
"
1,HTML Text Extractor does not extract or index numerics. Numerics such as addresses/dates/financial figures are not extracted or indexed by the current HTML Extractor.  These values are handled properly and searchable when done via the PlainTextExtractor
1,"Denying a primaryType does not work in XPath. The following query does not work:

//element(*, my:type)[jcr:contains(.,'foo') and @jcr:primaryType != 'nt:frozenNode')

The jcr:primaryType predicate does not respect the 'not equal' operation."
1,"Exception in deleteDocument, undeleteAll or setNorm in IndexReader can fail to release write lock on close. I hit this while working on LUCENE-140

We have 3 cases in the IndexReader methods above where we have this pattern:

  if (directoryOwner) acquireWriteLock();
  doSomething();
  hasChanges = true;

The problem is if you hit an exception in doSomething(), and hasChanges was not already true, then hasChanges will not have been set to true yet the write lock is held.  If you then try to close the reader without making any other changes, then the write lock is not released because in IndexReader.close() (well, in commit()) we only release write lock if hasChanges is true.

I think the simple fix is to swap the order of hasChanges = true and doSomething().  I already fixed one case of this under LUCENE-140 commit yesterday; I will fix the other two under this issue."
1,"CachingHieraarchyManager may serve moved items. There is a problem with weak referenced item states and event notification in the
LocalItemStateManager.

consider the following:
- Session A traverses some nodes and fills-up the cache of the ChachingHierarchyManager
- This also fills the weak-ref cache in Session A LocalItemStateManager.
- Session B does some operations
- At some point, GC decides to remove the weakly refferenced ItemStates in Session As
  LocalItemStateManager
- Session B moves a node and saves the changes.
- The SharedItemStateManager notifies all listeners that a node was modified
- The LocalItemStateManager of Session A receives the event, but does not bubble it,
  because it does not have the item anymore in its cache
- The CachingHierarchyManager of Session A never receives the modification event and still
  servers the items at the old location.

Solution A:
reconnect missing states in the LocalItemStateManager when an event is received. this has
the drawback that a lot of state would be generated that are not needed.

Solution B:
add a new event 'nodeModified' that is only sent by the LocalItemStateManager if a
'stateModified' was received for which it does not have the item aymore. this has the
drawback that alot more events are generated.

Will implement solution B
"
1,rep:excerpt() may return malformed XML. The rep:excerpt() function does not encode the prefined XML entities but writes them as is into the excerpt XML. This may produce malformed XML.
1,"Save fails after setting a binary property twice. Setting a binary property twice discards the blob value of the first property state but does not remove the change from the changelog, resulting in an error on save:

javax.jcr.RepositoryException: this BLOBFileValue has been disposed
	at org.apache.jackrabbit.core.value.RefCountingBLOBFileValue.copy(RefCountingBLOBFileValue.java:105)

will attach patch that adds the respective test to the jcr2spi tests."
1,BitVector never skips fully populated bytes when writing ClearedDgaps. When writing cleared DGaps in BitVector we compare a byte against 0xFF (255) yet the byte is casted into an int (-1) and the comparison will never succeed. We should mask the byte with 0xFF before comparing or compare against -1
1,"Sloppy Phrase Scorer matches the doc ""A B C D E"" for query = ""B C B""~2. This is an extension of https://issues.apache.org/jira/browse/LUCENE-697

In addition to abnormalities Yonik pointed out in 697, there seem to be other issues with slopy phrase search and scoring.

1) A phrase with a repeated word would be detected in a document although it is not there.
I.e. document = A B D C E , query = ""B C B"" would not find this document (as expected), but query ""B C B""~2 would find it. 
I think that no matter how large the slop is, this document should not be a match.

2) A document containing both orders of a query, symmetrically, would score differently for the queru and for its reveresed form.
I.e. document = A B C B A would score differently for queries ""B C""~2 and ""C B""~2, although it is symmetric to both.

I will attach test cases that show both these problems and the one reported by Yonik in 697. "
1,"Invalid query results when using jcr:like with a case transform function and a pattern not starting with a wildcard. If the repository contains nodes with the following value for the property name :
john
JOhn
joe
Joey

and we run the following query :
//element(*, document)/*[jcr:like(fn:lower-case(@name), 'joh%')]"")
then all the previous nodes will match especially the last 2 nodes.

The reason is the use of two range scans from the lucene term index:
..._name_jOH
..................
..._name_joh_

and

..._name_JOH
..................
..._name_Joh_

The first range will contains ..._name_joe property and the second will contains ..._name_Joey.
But the pattern 'joh%' and so the regexp '.*' because of the range scan will match
the substring values of the properties ('' in the first range and 'y' in the second range).

The solution is to use the full pattern (ie 'joh.*') for matching each properties."
1,"Error while restoring OPV=Version childnodes (Restore of root version not allowed). when restoring a version of a node (by name) that has opv=version childnodes, the following error is thrown, if such a version does not exist in the child nodes versionhistory:

Error while restoring nodes: javax.jcr.version.VersionException: Restore of root version not allowed."
1,"2.4.x index cannot be opened with 2.9-dev. Sorry for the lack of proper testcase.

In 2.4.1, if you created an index with the (stupid) options below, then it will not create a .prx file. 2.9 expects this file and will not open the index.
The reason i used these stupid options is because i changed the field from indexed=yes to indexed=no, but forgot to remove the .setOmitTf()

{code}
public class Testcase {
	public static void main(String args[]) throws Exception {
		/* run this part with lucene 2.4.1 */
		IndexWriter iw = new IndexWriter(""test"", new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
		iw.setUseCompoundFile(false);
		Document doc = new Document();
		Field field1 = new Field(""field1"", ""foo"", Field.Store.YES, Field.Index.NO);
		field1.setOmitTf(true); // 2.9 will create a 0-byte .prx file, but 2.4.x will NOT. This is the problem. 2.9 expects this file!
		doc.add(field1);
		iw.addDocument(doc);
		iw.close(); 
		/* run this with lucene 2.9 */
		IndexReader ir = IndexReader.open(FSDirectory.getDirectory(""test""), true); 
	}
}
{code}"
1,"New LaxRedirectStrategy class should probably call the super method first.. LaxRedirectStrategy extends the defaulRedirect class but does not call the super method as one would expect.

Just adding a patch to make sure it gets called."
1,"Change Primitive Data Types from int to long in class SegmentMerger.java. Hi

We are getting an exception while optimize. We are getting this exception ""mergeFields produced an invalid result: docCount is 385282378 but fdx file size is 3082259028; now aborting this merge to prevent index corruption""
 
I have  checked the code for class SegmentMerger.java and found this check 

***********************************************************************************************************************************************************************
if (4+docCount*8 != fdxFileLength)
        // This is most likely a bug in Sun JRE 1.6.0_04/_05;
        // we detect that the bug has struck, here, and
        // throw an exception to prevent the corruption from
        // entering the index.  See LUCENE-1282 for
        // details.
        throw new RuntimeException(""mergeFields produced an invalid result: docCount is "" + docCount + "" but fdx file size is "" + fdxFileLength + ""; now aborting this merge to prevent index corruption"");
}
***********************************************************************************************************************************************************************

In our case docCount is 385282378 and fdxFileLength size is 3082259028, even though 4+385282378*8 is equal to 3082259028, the above code will not work because number 3082259028 is out of int range. So type of variable docCount needs to be changed to long

I have written a small test for this 

************************************************************************************************************************************************************************

public class SegmentMergerTest {
public static void main(String[] args) {
int docCount = 385282378; 
long fdxFileLength = 3082259028L; 
if(4+docCount*8 != fdxFileLength) 
System.out.println(""No Match"" + (4+docCount*8));
else 
System.out.println(""Match"" + (4+docCount*8));
}
}

************************************************************************************************************************************************************************

Above test will print No Match but if you change the data type of docCount to long, it will print Match

Can you please advise us if this issue will be fixed in next release?

Regards
Deepak







 



"
1,"Registering cyclic dependent nodetypes does not work. when registering the followin 2 nodetypes:

[foo] 
+ mybar (bar)

[bar]
+ myfoo (foo)

NodeTypeRegistry.registerNodeTypes(Collection) throws:

 org.apache.jackrabbit.core.nodetype.InvalidNodeTypeDefException: the following node types could not be registered because of unresolvable dependencies: {}foo {}bar 
"
1,"score and explain don't match. I've faced this problem recently. I'll attach a program to reproduce the problem soon. The program outputs the following:

{noformat}
** score = 0.10003257
** explain
0.050016284 = (MATCH) product of:
  0.15004885 = (MATCH) sum of:
    0.15004885 = weight(f1:""note book"" in 0), product of:
      0.3911943 = queryWeight(f1:""note book""), product of:
        0.61370564 = idf(f1: note=1 book=1)
        0.6374299 = queryNorm
      0.38356602 = fieldWeight(f1:""note book"" in 0), product of:
        1.0 = tf(phraseFreq=1.0)
        0.61370564 = idf(f1: note=1 book=1)
        0.625 = fieldNorm(field=f1, doc=0)
  0.33333334 = coord(1/3)
{noformat}
"
1,"Automatic type conversion no longer works. String values are no longer converted to binary when required. Example:

Node n = testRootNode.addNode(""testConvert"", ""nt:file"");
Node content = n.addNode(""jcr:content"", ""nt:resource"");
content.setProperty(""jcr:lastModified"", Calendar.getInstance());
content.setProperty(""jcr:mimeType"", ""text/html"");
content.setProperty(""jcr:data"", ""Hello"");
n.getSession().save();

This used to work in a previous 2.0 build, but now throws:

javax.jcr.nodetype.ConstraintViolationException: no matching property definition found for {http://www.jcp.org/jcr/1.0}data
at org.apache.jackrabbit.core.nodetype.EffectiveNodeType.getApplicablePropertyDef(EffectiveNodeType.java:782)
at org.apache.jackrabbit.core.NodeImpl.getApplicablePropertyDefinition(NodeImpl.java:747)
at org.apache.jackrabbit.core.ItemManager.getDefinition(ItemManager.java:241)
at org.apache.jackrabbit.core.ItemData.getDefinition(ItemData.java:101)
at org.apache.jackrabbit.core.ItemManager.canRead(ItemManager.java:409)
at org.apache.jackrabbit.core.ItemManager.getItemData(ItemManager.java:383)
at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:316)
at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:595)
at org.apache.jackrabbit.core.NodeImpl.removeChildProperty(NodeImpl.java:554)
at org.apache.jackrabbit.core.NodeImpl.removeChildProperty(NodeImpl.java:534)
at org.apache.jackrabbit.core.NodeImpl.setProperty(NodeImpl.java:2303)
at org.apache.jackrabbit.core.nodetype.ConvertDataTypeTest.testStringToBinary(ConvertDataTypeTest.java:36)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)"
1,"hotspot bug in readvint gives wrong results. When testing the 3.1-RC1 made by Yonik on the PANGAEA (www.pangaea.de) productive system I figured out that suddenly on a large segment (about 5 GiB) some stored fiels suddenly produce a strange deflate decompression problem (CompressionTools) although the stored fields are no longer pre-3.0 compressed. It seems that the header of the stored field is read incorrectly at the buffer boundary in MultiMMapDir and then FieldsReader just incorrectly detects a deflate-compressed field (CompressionTools).

The error occurs reproducible on CheckIndex with MMapDirectory, but not with NIODir or SimpleDir. The FDT file of that segment is 2.6 GiB, on Solaris the chunk size is Integer.MAX_VALUE, so we have 2 MultiMMap IndexInputs.

Robert and me have the index ready as a tar file, we will do tests on our local machines and hopefully solve the bug, maybe introduced by Robert's recent changes to MMap."
1,"Lucene fails to close file handles under certain situations. As a followon to LUCENE-820, I've added a further check in
MockRAMDirectory to assert that there are no open files when the
directory is closed.

That check caused a few unit tests to fail, and in digging into the
reason I uncovered these cases where Lucene fails to close file
handles:

  * TermInfosReader.close() was setting its ThreadLocal enumerators to
    null without first closing the SegmentTermEnum in there.  It looks
    like this was part of the fix for LUCENE-436.  I just added the
    call to close.

    This is somewhat severe since we could leak many file handles for
    use cases that burn through threads and/or indexes.  Though,
    FSIndexInput does have a finalize() to close itself.

  * Flushing of deletes in IndexWriter opens SegmentReader to do the
    flushing, and it correctly calls close() to close the reader.  But
    if an exception is hit during commit and before actually closing,
    it will leave open those handles.  I fixed this first calling
    doCommit() and then doClose() in a finally.  The ""disk full"" tests
    we now have were hitting this.

  * IndexWriter's addIndexes(IndexReader[]) method was opening a
    reader but not closing it with a try/finally.  I just put a
    try/finally in.

I've also changed some unit tests to use MockRAMDirectory instead of
RAMDirectory to increase testing coverage of ""leaking open file
handles"".
"
1,"UUID compareTo and hashCode. The current UUID.compareTo implementation is not correct. Usually, 'equals' is used so this is not a big problem, but I need to create an ordered list of UUIDs and for this I need compareTo. The current implementation is based on subtraction, but this doesn't always work. Example:

//long a = 10, b = 20, c = 0;
long a = Long.MAX_VALUE, b = Long.MIN_VALUE, c = 0;
System.out.println((a - b) < 0 ? ""a < b"" : ""a >= b"");
System.out.println((c - a) < 0 ? ""c < a"" : ""c >= a"");
System.out.println((b - c) < 0 ? ""b < c"" : ""b >= c"");

The hashCode implementation is OK, but the multiplication is not required."
1,"BundlePersistenceManager.externalBLOBs can not be configured. If you try to configure the property externalBLOBs through the workspace.xml it does not work.
The BundlePersistenceManager has not Method setExternalBLOBs(boolean externalBLOBs) so it can not be configured
because its not bean conform. See the DatabasePersistenceManager which has such a Method
"
1,"NTLM Proxy and basic host authorization. Using a Microsoft proxy with NTLM validation enabled the authorization against a
remote host does not work. This, of course, assuming that the page is correctly
fetched (which currently is not), see the NTLM authentication bug number 24327"
1,"incorrect jcr:uuid on frozen subnode. The following program:

import javax.jcr.Repository;
import javax.jcr.Session;
import javax.jcr.SimpleCredentials;
import javax.jcr.Node;
import org.apache.jackrabbit.core.TransientRepository;

public class debug2 {
    public static void main(String[] args) throws Exception {
        Repository repository = new TransientRepository();
        Session session = repository.login(
                new SimpleCredentials(""username"", ""password"".toCharArray()));
        try {
            Node root = session.getRootNode();

            Node foo = root.addNode(""foo"");
            foo.addMixin(""mix:versionable"");

            Node bar = foo.addNode(""bar"");
            bar.addMixin(""mix:referenceable"");
            System.out.println(""bar:            "" + bar.getUUID());

            session.save();
            foo.checkin();

            Node frozenbar = foo.getBaseVersion().getNode(""jcr:frozenNode"").getNode(""bar"");
            System.out.println(""frozenbar UUID: "" + frozenbar.getUUID());
            System.out.println(""jcr:uuid:       "" + frozenbar.getProperty(""jcr:uuid"").getValue().getString());
            System.out.println(""jcr:frozenUuid: "" + frozenbar.getProperty(""jcr:frozenUuid"").getValue().getString());

        } finally {
            session.logout();
        }
    }
}

Gives as sample output:
bar:            fcf0affb-7476-4a64-a480-3039e8c53d53
frozenbar UUID: ed9fece9-9837-4ecc-9b7e-55bdfb8284e2
jcr:uuid:       fcf0affb-7476-4a64-a480-3039e8c53d53
jcr:frozenUuid: fcf0affb-7476-4a64-a480-3039e8c53d53

The jcr:uuid of the frozen bar is incorrect (althoug getUUID() returns the correct value).
"
1,"FieldCache.getStringIndex should not throw exception if term count exceeds doc count. Spinoff of LUCENE-2133/LUCENE-831.

Currently FieldCache cannot handle more than one value per field.
We may someday want to fix that... but until that day:

FieldCache.getStringIndex currently does a simplistic check to try to
catch when you've accidentally allowed more than one term per field,
by testing if the number of unique terms exceeds the number of
documents.

The problem is, this is not a perfect check, in that it allows false
negatives (you could have more than one term per field for some docs
and the check won't catch you).

Further, the exception thrown is the unchecked RuntimeException.

So this means... you could happily think all is good, until some day,
well into production, once you've updated enough docs, suddenly the
check will catch you and throw an unhandled exception, stopping all
searches [that need to sort by this string field] in their tracks.
It's not gracefully degrading.

I think we should simply remove the test, ie, if you have more terms
than docs then the terms simply overwrite one another.
"
1,"Bundle persistence name index not case-sensitive in MySQL and MS SQL. As reported by Martijn Hendriks on the dev mailing list (see http://www.nabble.com/Bundle-persistence-managers---db-collation-tf3571522.html), the NAME column of the NAMES table in the bundle persistence manager needs to be case-sensitive."
1,"FastVectorHighlighter SimpleBoundaryScanner does not work well when highlighting at the beginning of the text . The SimpleBoundaryScanner still breaks text not based on characters provided when highlighting text that end up scanning to the beginning of the text to highlight. In this case, just use the start of the text as the offset."
1,"JCR2SPI NodeEntryImpl throws NPE during reorderNodes. Two folder nodes are created below root. From the root node, the 2nd folder is ordered before the first node. The request is batched up correctly, but upon save, NodeEntryImpl throws a NullPointerException in the first line of the completeTransientChanges method, because revertInfo.oldParent is null.

Test code:

		final String FOLDER1 = ""folder1"", FOLDER2 = ""folder2"";
		
		// Create folder 1 on server in root
		Session serverSession = login(repository, creds);
		Node serverRootNode = serverSession.getRootNode();
		Node serverFolder1 = serverRootNode.addNode(FOLDER1, ""nt:folder"");
		
		// Create folder 2 on server in root
		Node serverFolder2 = serverRootNode.addNode(FOLDER2, ""nt:folder"");
		serverSession.save();
		
		// Validate order (TODO)
		
		// Perform reorder via client
		Session clientSession = login(clientRepository, creds);
		Node clientRootNode = clientSession.getRootNode();
		clientRootNode.orderBefore(FOLDER2, FOLDER1);
		clientSession.save(); <== Throws NPE

Call Stack:

    [junit] java.lang.NullPointerException
    [junit]     at org.apache.jackrabbit.jcr2spi.hierarchy.NodeEntryImpl.completeTransientChanges(NodeEntryImpl.java:1354)
    [junit]     at org.apache.jackrabbit.jcr2spi.hierarchy.NodeEntryImpl.access$1100(NodeEntryImpl.java:60)
    [junit]     at org.apache.jackrabbit.jcr2spi.hierarchy.NodeEntryImpl$RevertInfo.statusChanged(NodeEntryImpl.java:1465)
    [junit]     at org.apache.jackrabbit.jcr2spi.state.ItemState.setStatus(ItemState.java:257)
    [junit]     at org.apache.jackrabbit.jcr2spi.state.NodeState.adjustNodeState(NodeState.java:554)
    [junit]     at org.apache.jackrabbit.jcr2spi.state.NodeState.persisted(NodeState.java:276)
    [junit]     at org.apache.jackrabbit.jcr2spi.state.ChangeLog.persisted(ChangeLog.java:135)
    [junit]     at org.apache.jackrabbit.jcr2spi.WorkspaceManager.execute(WorkspaceManager.java:479)
    [junit]     at org.apache.jackrabbit.jcr2spi.state.SessionItemStateManager.save(SessionItemStateManager.java:149)
    [junit]     at org.apache.jackrabbit.jcr2spi.ItemImpl.save(ItemImpl.java:239)
    [junit]     at org.apache.jackrabbit.jcr2spi.SessionImpl.save(SessionImpl.java:317)
    [junit]     at TestWsNodeReorder.testReorderNodes(TestWsNodeReorder.java:72)


I'm using an SPI I implemented, in conjunction with the jcr2spi and spi2jcr bridges, coupled with a back-end Jackrabbit in-memory filesystem. So there's always the possibility that node or property SPI calls inject errors and cause this downstream problem."
1,"DocumentWriter closes TokenStreams too early. The DocumentWriter closes a TokenStream as soon as it has consumed its tokens. The javadoc of TokenStream.close() says that it releases resources associated with the stream. However, the DocumentWriter keeps references of the resources (i. e. payload byte arrays, term strings) until it writes the postings to the new segment, which means that DocumentWriter should call TokenStream.close() after it has written the postings.

This problem occurs in multithreaded applications where e. g. pooling is used for the resources. My patch adds a new test to TestPayloads which shows this problem. Multiple threads add documents with payloads to an index and use a pool of byte arrays for the payloads. TokenStream.close() puts the byte arrays back into the pool. The test fails with the old version but runs successfully with the patched version. 

All other units tests pass as well.
"
1,"jackrabbit-server.war is missing the slf4j-log4j12 library. Reported by Martin Perez:

But I found a bug on the .war file. It is missing the slf4j-log4j12-1.0.jar. It's in someway tricky to detect it because if you do not include it a ClassNotFoundException will be thrown but poiting to the JCR class with the log statement. Anyways, if you include the .jar file on the WEB-INF/lib directory, then the exception goes to exception's hell.

"
1,"ObjectIterator may return null, which is not readily expected from an Iterator. The ObjectIterator class implements an Iterator of objects mapped from an underlying NodeIterator. This ObjectIterator may return null from next() if no mapping for a node in the iterator exists. Rather than returning null, the iterator should probably just ignore the unmappable node and return an object from the next node in the underlying iterator which is mappable."
1,"BlockJoinQuery advance fails on an assert in case of a single parent with child segment. The BlockJoinQuery will fail on an assert when advance in called on a segment with a single parent with a child. The call to parentBits.prevSetBit(parentTarget - 1) will cause -1 to be returned, and the assert will fail, though its valid. Just removing the assert fixes the problem, since nextDoc will handle it properly.

Also, I don't understand the ""assert parentTarget != 0;"", with a comment of each parent must have one child. There isn't really a reason to add this constraint, as far as I can tell..., just call nextDoc in this case, no?"
1,"Checking of stale connections is broken. HttpConnections that went stale (dropped by server) throw SocketExceptions
instead of silently re-opening themselves, as has been the case with earlier
versions of HttpClient.

I think the problem for this can be found in HttpConnection:

  public boolean closeIfStale() throws IOException {
    if (used && isOpen && isStale()) {
      LOG.debug(""Connection is stale, closing..."");
      close();
      return true;
    }
    return false;
  }

staleness is only checked if used = true, but there is no code in HttpConnection
that sets the used flag. In other words: used is always false and isStale() is
never called."
1,"ConnectionRecoveryManager is created twice in DBDataStore init method. It seems that after introducing pool, old initizialization of ConnectionRecoveryManager has not been removed.

Index: DbDataStore.java
===================================================================
--- DbDataStore.java	(revision 605626)
+++ DbDataStore.java	(working copy)
@@ -479,8 +479,6 @@
             initDatabaseType();
             connectionPool = new Pool(this, maxConnections);
             ConnectionRecoveryManager conn = getConnection();
-            conn = new ConnectionRecoveryManager(false, driver, url, user, password);
-            conn.setAutoReconnect(true);
             DatabaseMetaData meta = conn.getConnection().getMetaData();
             log.info(""Using JDBC driver "" + meta.getDriverName() + "" "" + meta.getDriverVersion());
             meta.getDriverVersion();

Duplicated initialization should be removed , but i've never run this code yet."
1,"IndexReader.isCurrent incorrectly returns false after writer.prepareCommit has been called. Spinoff from thread ""2 phase commit with external data"" on java-user.

The IndexReader should not see the index as changed, after a prepareCommit has been called but before commit is called."
1,"Problems with custom nodes in journal. I have an application that uses custom node types and I am having problems in a clustered configuration.

Issue 1: the following definition in a nodetype is incorrectly read from the journal:
  + * (nt:hierarchyNode) version

The * is stored in the journal as _x002a_ since it should be a QName and it gets escaped.
When read, the code ...core.nodetype.compact.CompactNodeTypeDefReader.doChildNodeDefinition does the following test:

        if (currentTokenEquals('*')) {
            ndi.setName(ItemDef.ANY_NAME); 
        } else {
            ndi.setName(toQName(currentToken));
        }

Since currentToken is _x002a_ and not * toQName(currentToken) is called but it fails.
I changed the test to:
        if (currentTokenEquals('*') || currentTokenEquals(""_x002a_""))
            ....
and that fixes the problem.

Issue 2: when storing a nodeType in the journal the superclass nt:base is not store, but when reading I get an error saying the node should be a subclass of nt:base.

The code in...core.nodetype.compact.CompactNodeTypeDefWriter.writeSupertypes skips nt:base when writing the node.

When reading the nodetype definition from the journal the following exception is thrown:

Unable to deliver node type operation: [{http://namespace/app/repository/1.0}resource] all primary node types except nt:base itself must be (directly or indirectly) derived from nt:base

probably because nt:base is not re-added to the nodetype definition

 "
1,"bad normalization in sorted search returning TopDocs. FieldSortedHitQueue.maxscore is maintained in the lessThan method (which never gets called if a single document is added to the queue).

I've checked in a test to TestSort.testTopDocsScores() with the final assertion commented out."
1,"LockOperation - In a clustered environment, a lock created on one server is not sent to the other servers.. The cluster operation for a lock (LockOperation) always has the isLock variable set to false.
The operation to represent a lock should set the isLock variable to true.

    /**
     * Flag indicating whether this is a lock. 
     */
    private boolean isLock;
"
1,"ClassCastException when updating properties using WebDAV. When issuing PROPPATCH commands, a ClassCastException is raised.

e.g. 

PROPPATCH /jackrabbit-webapp-1.4/repository/default/test/test_file_v.txt HTTP/1.1
Host: localhost:9000
Connection: TE
TE: trailers, deflate, gzip, compress
User-Agent: UCI DAV Explorer/0.91 RPT-HTTPClient/0.3-3E
Translate: f
Authorization: Basic Y3Jvc3NqYTp0ZXN0
Accept-Encoding: deflate, gzip, x-gzip, compress, x-compress
Content-type: text/xml
Content-length: 170

<A:propertyupdate xmlns:A=""DAV:"">
<A:set>
<A:prop>
<A:auto-version>checkout-checkin</A:auto-version>
</A:prop>
</A:set>
</A:propertyupdate>


results in



24.01.2008 15:38:34 *ERROR* [Webdav]: Servlet.service() for servlet Webdav threw
 exception (StandardWrapperValve.java, line 257)
java.lang.ClassCastException: org.apache.jackrabbit.webdav.property.DefaultDavPr
operty
        at org.apache.jackrabbit.webdav.simple.DavResourceImpl.alterProperties(D
avResourceImpl.java:456)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.doPropPatch
(AbstractWebdavServlet.java:457)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.execute(Abs
tractWebdavServlet.java:234)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.service(Abs
tractWebdavServlet.java:192)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:803)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(Appl
icationFilterChain.java:269)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationF
ilterChain.java:188)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperV
alve.java:210)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextV
alve.java:174)
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.j
ava:127)
        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.j
ava:117)
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineVal
ve.java:108)
        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.jav
a:151)
        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java
:870)
        at org.apache.coyote.http11.Http11BaseProtocol$Http11ConnectionHandler.p
rocessConnection(Http11BaseProtocol.java:665)
        at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(PoolTcpEndpo
int.java:528)
        at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt(LeaderFol
lowerWorkerThread.java:81)
        at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadP
ool.java:685)
        at java.lang.Thread.run(Thread.java:595)
24.01.2008 15:38:34 *ERROR* [Webdav]: Servlet.service() for servlet Webdav threw
 exception (SLF4JLocationAwareLog.java, line 174)
java.lang.ClassCastException: org.apache.jackrabbit.webdav.property.DefaultDavPr
operty
        at org.apache.jackrabbit.webdav.simple.DavResourceImpl.alterProperties(D
avResourceImpl.java:456)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.doPropPatch
(AbstractWebdavServlet.java:457)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.execute(Abs
tractWebdavServlet.java:234)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.service(Abs
tractWebdavServlet.java:192)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:803)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(Appl
icationFilterChain.java:269)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationF
ilterChain.java:188)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperV
alve.java:210)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextV
alve.java:174)
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.j
ava:127)
        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.j
ava:117)
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineVal
ve.java:108)
        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.jav
a:151)
        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java
:870)
        at org.apache.coyote.http11.Http11BaseProtocol$Http11ConnectionHandler.p
rocessConnection(Http11BaseProtocol.java:665)
        at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(PoolTcpEndpo
int.java:528)
        at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt(LeaderFol
lowerWorkerThread.java:81)
        at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadP
ool.java:685)
        at java.lang.Thread.run(Thread.java:595)
24.01.2008 15:53:54 *ERROR* [Webdav]: Servlet.service() for servlet Webdav threw
 exception (StandardWrapperValve.java, line 257)
java.lang.ClassCastException: org.apache.jackrabbit.webdav.property.DefaultDavPr
operty
        at org.apache.jackrabbit.webdav.simple.DavResourceImpl.alterProperties(D
avResourceImpl.java:456)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.doPropPatch
(AbstractWebdavServlet.java:457)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.execute(Abs
tractWebdavServlet.java:234)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.service(Abs
tractWebdavServlet.java:192)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:803)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(Appl
icationFilterChain.java:269)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationF
ilterChain.java:188)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperV
alve.java:210)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextV
alve.java:174)
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.j
ava:127)
        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.j
ava:117)
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineVal
ve.java:108)
        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.jav
a:151)
        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java
:870)
        at org.apache.coyote.http11.Http11BaseProtocol$Http11ConnectionHandler.p
rocessConnection(Http11BaseProtocol.java:665)
        at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(PoolTcpEndpo
int.java:528)
        at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt(LeaderFol
lowerWorkerThread.java:81)
        at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadP
ool.java:685)
        at java.lang.Thread.run(Thread.java:595)
24.01.2008 15:53:54 *ERROR* [Webdav]: Servlet.service() for servlet Webdav threw
 exception (SLF4JLocationAwareLog.java, line 174)
java.lang.ClassCastException: org.apache.jackrabbit.webdav.property.DefaultDavPr
operty
        at org.apache.jackrabbit.webdav.simple.DavResourceImpl.alterProperties(D
avResourceImpl.java:456)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.doPropPatch
(AbstractWebdavServlet.java:457)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.execute(Abs
tractWebdavServlet.java:234)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.service(Abs
tractWebdavServlet.java:192)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:803)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(Appl
icationFilterChain.java:269)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationF
ilterChain.java:188)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperV
alve.java:210)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextV
alve.java:174)
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.j
ava:127)
        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.j
ava:117)
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineVal
ve.java:108)
        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.jav
a:151)
        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java
:870)
        at org.apache.coyote.http11.Http11BaseProtocol$Http11ConnectionHandler.p
rocessConnection(Http11BaseProtocol.java:665)
        at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(PoolTcpEndpo
int.java:528)
        at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt(LeaderFol
lowerWorkerThread.java:81)
        at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadP
ool.java:685)
        at java.lang.Thread.run(Thread.java:595) "
1,"SpanNotQuery.hashCode ignores exclude. filing as bug for tracking/refrence...

On May 16, 2006, at 3:33 AM, Chris Hostetter wrote:

> SpanNodeQuery's hashCode method makes two refrences to  
> include.hashCode(),
> but none to exclude.hashCode() ... this is a mistake yes/no?

Date: Tue, 16 May 2006 05:57:15 -0400
From: Erik Hatcher
To: java-dev@lucene.apache.org
Subject: Re: SpanNotQuery.hashCode cut/paste error?

Yes, this is a mistake.  I'm happy to fix it, but looks like you have  
other patches in progress.

"
1,"Enable the use of NoMergePolicy and NoMergeScheduler by Benchmark. Benchmark allows one to set the MP and MS to use, by defining the class name and then use reflection to instantiate them. However NoMP and NoMS are singletons and therefore reflection does not work for them. Easy fix in CreateIndexTask. I'll post a patch soon."
1,"DocViewSaxEventGenerator may generate non-NS-wellformed XML. The XML serialization code relies on the fact that all required prefix-to-uri mappings are known beforehand (actually, when serializing the root node). So there's an assumption that the permanent namespace registry will never change during serialization, which may be incorrect when another client adds namespace registrations while the XML export is in progress.

To fix this, ""addNamespacePrefixes"" should ensure that namespace declarations have been written for all prefixes used on the current node (node name + properties), potentially going back to the namespace resolver when needed.

(Should there be consensus for that change I'm happy to give it a try)"
1,"Jackrabbit fails to shutdown properly when tomcat is shutting down. This is the same issue already discudded in http://issues.apache.org/jira/browse/JCR-57

The problem only occurs when Jackrabbit is deployed in the WEB-INF/lib directory of a web application in Tomcat.
During dispose() jackrabbit tries to instantiate a few objects from classes which were not previously loaded by the webapp classloader, but tomcat doesn't allow to load new classes while shutting down.
This causes the repository not to be closed properly, and an annoying set of stack traces are written to the log.

It seems that there are only two classes which are loaded in this situation: org.apache.jackrabbit.core.observation.EventListenerIteratorImpl and org.apache.jackrabbit.core.fs.FileSystemPathUtil. This is the log from the server standard output:

org.apache.catalina.loader.WebappClassLoader loadClass
INFO: Illegal access: this web application instance has been stopped already.  Could not load org.apache.jackrabbit.core.observation.EventListenerIteratorImpl.  The eventual following stack trace is caused by an error thrown for debugging purposes as well as to attempt to terminate the thread which caused the illegal access, and has no functional impact.
[repeaded more times at each shutdown]

org.apache.catalina.loader.WebappClassLoader loadClass
INFO: Illegal access: this web application instance has been stopped already.  Could not load org.apache.jackrabbit.core.fs.FileSystemPathUtil.  The eventual following stack trace is caused by an error thrown for debugging purposes as well as to attempt to terminate the thread which caused the illegal access, and has no functional impact.


A quick fix is to force preloading of classes normally needed only during shutdown, simply adding a static block to caller classes. The following patch makes tomcat happy, causing classes to be loaded by the webapp classloaded when still allowed  (probably not really elegant, but perfectly working...)




Index: org/apache/jackrabbit/core/fs/FileSystemResource.java
===================================================================
--- src\java\org\apache\jackrabbit\core\fs\FileSystemResource.java	(revision 169503)
+++ src\java\org\apache\jackrabbit\core\fs\FileSystemResource.java	(working copy)
@@ -30,6 +30,11 @@
 
     protected final String path;
 
+    static {
+        // preload FileSystemPathUtil to prevent classloader issues during shutdown
+        FileSystemPathUtil.class.hashCode();
+    }
+
     /**
      * Creates a new <code>FileSystemResource</code>
      *
Index: org/apache/jackrabbit/core/observation/ObservationManagerImpl.java
===================================================================
--- src\java\org\apache\jackrabbit\core\observation\ObservationManagerImpl.java	(revision 169503)
+++ src\java\org\apache\jackrabbit\core\observation\ObservationManagerImpl.java	(working copy)
@@ -54,6 +54,11 @@
      */
     private final ObservationManagerFactory obsMgrFactory;
 
+    static {
+        // preload EventListenerIteratorImpl to prevent classloader issues during shutdown
+        EventListenerIteratorImpl.class.hashCode();
+    }
+
     /**
      * Creates an <code>ObservationManager</code> instance.
      *


"
1,"ClassCastException when registering custom node by XML file. When trying to register node type from XML file using following code:

		JackrabbitNodeTypeManager nodeTypeManager = (JackrabbitNodeTypeManager)workspace.getNodeTypeManager();
		for(Resource resource : nodeDefinitions){
			System.out.println(""** registering node:""+resource);
			nodeTypeManager.registerNodeTypes(resource.getInputStream(), JackrabbitNodeTypeManager.TEXT_XML);
		}

we receive such surprise:

Caused by: java.lang.ClassCastException: com.sun.org.apache.xerces.internal.dom.DeferredDocumentImpl
	at org.apache.jackrabbit.core.util.DOMWalker.iterateElements(DOMWalker.java:215)
	at org.apache.jackrabbit.core.nodetype.xml.NodeTypeReader.getNodeTypeDefs(NodeTypeReader.java:121)
	at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.registerNodeTypes(NodeTypeManagerImpl.java:257)
	at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.registerNodeTypes(NodeTypeManagerImpl.java:499)
	at pl.codeservice.jcr.JcrCustomNodeRegister.registerNodes(JcrCustomNodeRegister.java:41)
	at pl.codeservice.jcr.JcrCustomNodeRegister.init(JcrCustomNodeRegister.java:27)
	...


Registering nodes by .cnd files works fine."
1,"DefaultRedirectHandler not resolving relative location URI wrt the request URI. The adjustment of a relative URI in the Location header value does not take the request URI into account. So you may want to replace ...
------------------------------
try {
    uri = new URI(
            target.getSchemeName(),
            null,
            target.getHostName(),
            target.getPort(),
            uri.getPath(),
            uri.getQuery(),
            uri.getFragment());
------------------------------
... with ...
------------------------------
HttpRequest request = (HttpRequest) context.getAttribute(ExecutionContext.HTTP_REQUEST);
try {
    URI requestURI = new URI(request.getRequestLine().getUri());
    URI absoluteRequestURI = new URI(
            target.getSchemeName(),
            null,
            target.getHostName(),
            target.getPort(),
            requestURI.getPath(),
            requestURI.getQuery(),
            requestURI.getFragment());
    uri = absoluteRequestURI.resolve(uri);
------------------------------
... or get the request URI from somewhere else."
1,"Wrong implementation of DocIdSetIterator.advance . Implementations of {{DocIdSetIterator}} behave differently when advanced is called. Taking the following test for {{OpenBitSet}}, {{DocIdBitSet}} and {{SortedVIntList}} only {{SortedVIntList}} passes the test:
{code:title=org.apache.lucene.search.TestDocIdSet.java|borderStyle=solid}
...
	public void testAdvanceWithOpenBitSet() throws IOException {
		DocIdSet idSet = new OpenBitSet( new long[] { 1121 }, 1 );  // bits 0, 5, 6, 10
		assertAdvance( idSet );
	}

	public void testAdvanceDocIdBitSet() throws IOException {
		BitSet bitSet = new BitSet();
		bitSet.set( 0 );
		bitSet.set( 5 );
		bitSet.set( 6 );
		bitSet.set( 10 );
		DocIdSet idSet = new DocIdBitSet(bitSet);
		assertAdvance( idSet );
	}

	public void testAdvanceWithSortedVIntList() throws IOException {
		DocIdSet idSet = new SortedVIntList( 0, 5, 6, 10 );
		assertAdvance( idSet );
	}	

	private void assertAdvance(DocIdSet idSet) throws IOException {
		DocIdSetIterator iter = idSet.iterator();
		int docId = iter.nextDoc();
		assertEquals( ""First doc id should be 0"", 0, docId );

		docId = iter.nextDoc();
		assertEquals( ""Second doc id should be 5"", 5, docId );

		docId = iter.advance( 5 );
		assertEquals( ""Advancing iterator should return the next doc id"", 6, docId );
	}
{code}

The javadoc for {{advance}} says:
{quote}
Advances to the first *beyond* the current whose document number is greater than or equal to _target_.
{quote}
This seems to indicate that {{SortedVIntList}} behaves correctly, whereas the other two don't. 
Just looking at the {{DocIdBitSet}} implementation advance is implemented as:
{code}
bitSet.nextSetBit(target);
{code}
where the docs of {{nextSetBit}} say:
{quote}
Returns the index of the first bit that is set to true that occurs *on or after* the specified starting index
{quote}
"
1,"TestDocValuesIndexing reproducible  test failure. docvalues branch: r1131275

{code}
    [junit] Testsuite: org.apache.lucene.index.values.TestDocValuesIndexing
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.81 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestDocValuesIndexing -Dtestmethod=testAddIndexes -Dtests.seed=-3253978684351194958:-8331223747763543724
    [junit] NOTE: test params are: codec=RandomCodecProvider: {id=Standard, BYTES_VAR_STRAIGHT=Pulsing(freqCutoff=12), BYTES_FIXED_SORTED=MockRandom}, locale=es_MX, timezone=Pacific/Chatham
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestDocValuesIndexing]
    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=89168480,total=125632512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testAddIndexes(org.apache.lucene.index.values.TestDocValuesIndexing):     FAILED
    [junit] [first=BYTES_FIXED_SORTED, second=BYTES_VAR_STRAIGHT] expected:<9> but was:<10>
    [junit] junit.framework.AssertionFailedError: [first=BYTES_FIXED_SORTED, second=BYTES_VAR_STRAIGHT] expected:<9> but was:<10>
    [junit]     at org.apache.lucene.index.values.TestDocValuesIndexing.testAddIndexes(TestDocValuesIndexing.java:208)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1348)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1266)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.index.values.TestDocValuesIndexing FAILED
{code}"
1,"MsPowerPointTextExtractor does not extract from PPTs with  sign. The MsPowerPointTextExtractor class has a problem when reading PPTs when an  sign is contained. All text following that sign is ignored. Perhaps the POI PowerPointExtractor should be used instead of parsing the data by hand. As a side effect, this would simply the code. Extracting could be done as follows:

	public Reader extractText(InputStream stream, String type, String encoding) throws IOException {
		try {
			PowerPointExtractor extractor = new PowerPointExtractor(stream);
			return new StringReader(extractor.getText(true,true));
		} catch (RuntimeException e) {
			logger.warn(""Failed to extract PowerPoint text content"", e);
			return new StringReader("""");
		} finally {
			try { stream.close(); } catch (IOException ignored) {}
		}
	}
"
1,"DefaultHttpRequestRetryHandler must not retry non-idempotent http methods (violates RFC 2616). In DefaultHttpRequestRetryHandler, in case of NoHttpResponseException, the request is retried, without taking into account whether the http method is idempotent or not. This violates RFC 2616 section 8.1.4 which states :
{quote}
This means that clients, servers, and proxies MUST be able to recover
   from asynchronous close events. Client software SHOULD reopen the
   transport connection and retransmit the aborted sequence of requests
   without user interaction so long as the request sequence is
   idempotent (see section 9.1.2). Non-idempotent methods or sequences
   MUST NOT be automatically retried, although user agents MAY offer a
   human operator the choice of retrying the request(s).
{quote}

The fix is simple : at line 94, just remove the {{if (exception instanceof NoHttpResponseException) }} block. This way the idempotency of the method will be taken into account a bit further in the same method."
1,"Query throws UnsupportedOperationException. When executing an absolute XPath statement where the first location step is not jcr:root the Query may throw an UnsupportedOperationException:

Query: /foo//element(*, nt:unstructured)[@prop = 'bar']

Stacktrace:
java.lang.UnsupportedOperationException
        at org.apache.jackrabbit.core.query.lucene.CachingMultiReader$MultiTermDocs.skipTo(CachingMultiReader.java:281)
        at org.apache.lucene.search.TermScorer.skipTo(TermScorer.java:88)
        at org.apache.lucene.search.ConjunctionScorer.doNext(ConjunctionScorer.java:53)
        at org.apache.lucene.search.ConjunctionScorer.next(ConjunctionScorer.java:48)
        at org.apache.lucene.search.Scorer.score(Scorer.java:37)
        at org.apache.jackrabbit.core.query.lucene.ChildAxisQuery$ChildAxisScorer.calculateChildren(ChildAxisQuery.java:291)
        at org.apache.jackrabbit.core.query.lucene.ChildAxisQuery$ChildAxisScorer.next(ChildAxisQuery.java:251)
        at org.apache.lucene.search.Scorer.score(Scorer.java:37)
        at org.apache.jackrabbit.core.query.lucene.DescendantSelfAxisQuery$DescendantSelfAxisScorer.calculateSubHits(DescendantSelfAxisQuery.java:302)
        at org.apache.jackrabbit.core.query.lucene.DescendantSelfAxisQuery$DescendantSelfAxisScorer.next(DescendantSelfAxisQuery.java:237)
        at org.apache.lucene.search.Scorer.score(Scorer.java:37)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:92)
        at org.apache.lucene.search.Hits.getMoreDocs(Hits.java:64)
        at org.apache.lucene.search.Hits.<init>(Hits.java:43)
        at org.apache.lucene.search.Searcher.search(Searcher.java:33)
        at org.apache.lucene.search.Searcher.search(Searcher.java:27)
        at org.apache.jackrabbit.core.query.lucene.SearchIndex.executeQuery(SearchIndex.java:287)
        at org.apache.jackrabbit.core.query.lucene.QueryImpl.execute(QueryImpl.java:179)
        at org.apache.jackrabbit.core.query.QueryImpl.execute(QueryImpl.java:132)"
1,"[PATCH] NullPointerException when using nested SpanOrQuery in SpanNotQuery. Overview description: 
I'm using the span query classes in Lucene to generate higher scores for 
search results where the search terms are closer together. In certain 
situations I want to exclude terms from the span. When I attempt to exclude 
more than one term I get an error. 
 
The example query I'm using is:  
 
'brighton AND tourism' -pier -contents 
 
I construct the query objects and the toString() version is: 
 
spanNot(spanNear([contents:brighton contents:tourism], 10, false), 
spanOr([contents:pier, contents:road])) 
  
 
Steps to reproduce: 
1. Construct a SpanNearQuery (must have at least one term, but at least two 
makes more sense) 
2. Construct a SpanOrQuery containing two or more terms 
3. Construct a SpanNotQuery to include the first query object and exclude the 
second (SpanOrQuery) 
4. Execute the search 
 
 
Actual Results: 
A null pointer exception is thrown while generating the scores within the 
search. 
 
Stack trace:  
java.lang.NullPointerException   
        at   
org.apache.lucene.search.spans.SpanOrQuery$1.doc(SpanOrQuery.java:174)   
        at   
org.apache.lucene.search.spans.SpanNotQuery$1.next(SpanNotQuery.java:75)   
        at org.apache.lucene.search.spans.SpanScorer.next(SpanScorer.java:50)   
        at org.apache.lucene.search.Scorer.score(Scorer.java:37)   
        at   
org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:92)   
        at org.apache.lucene.search.Hits.getMoreDocs(Hits.java:64)   
        at org.apache.lucene.search.Hits.<init>(Hits.java:43)   
        at org.apache.lucene.search.Searcher.search(Searcher.java:33)   
        at org.apache.lucene.search.Searcher.search(Searcher.java:27)   
        at   
com.runtimecollective.search.LuceneSearch.search(LuceneSearch.java:362)   
 
 
Expected Resuts: 
It executes the search and results where the first search terms (near query) 
are close together but without the second terms (or query) appearing."
1,"Duplicate attribute in BeanDescriptor and CollectionDescriptor. 2 different attributes are used in BeanDescriptor and CollectionDescriptor to store the jcr type (jcrType and jcrNodeType). JcrNodeType can be removed. 
This imply modifications in  the DTD, the pm implementation and the different mapping xml files used for the unit tests. 

Furthermore, it should be nice to use the same name (jcrType)  in all descriptors.  There is a lot of confusion across the different descriptors. sometime jcrType is used, sometime jcrNodeType is used. I propose to use only jcrType with the following purpose : 

* In the ClassDescriptor, it  is used to store the primary node type of the class. 
* In the FieldDescriptor, it  is used to store the property type. 
* In the BeanDescriptor  it is used to defined the child node type. 
* In the CollectionDescriptor , it is used to defined the child node type. "
1,"NPE in PredefinedNodeTypeTest.getPropertyDefSpec. Occurs when PropertyDefinition.getValueConstraints returns null, which is allowed by the spec.
"
1,"MS Excel Mime Type missing in MsExcelTextExtractor . The MsExcelTextExtractor listens to mime type ""application/vnd.ms-excel"", but storing excels will result in mime type ""application/msexcel"", too. Such tagged files will not be indexed by the MsExcelTextExtractor. The class should register itself to both mime types like the MsWordTextExtractor does. "
1,"3.x indexes have the wrong normType set in fieldinfos. 3.x codec claims the single byte norms are BYTES_VAR_STRAIGHT in FieldInfos,
but the norms implementation itself then has the type as FIXED_INTS_8."
1,"WorkspaceAccessManager defined with SecurityManager that keeps users per workspace must test if user exists. the WorkspaceAccessManager defined with the security manager keeping users per workspace currently returns true upon calls to grant(Set, String) if
a workspace with the given name exists.

while this is fine for the initial check upon session creation, it obviously isn't for all method calls that test for accessible workspace names, such as
Workspace#getAccessibleWorkspaceName, Workspace#clone and copy across workspaces.

instead it should test if any of the specified principals corresponds to a valid user within the workspace identified by the given workspaceName."
1,"Oracle JDBC Class Cast Exception. When utilizing the OraclePersistenceManager (package org.apache.jackrabbit.core.persistence.db) (I realize this is marked as deprecated) we noticed during our migration from Jackrabbit 1.6.1 to 2.2.10/11 that when starting the application server an error message is displayed to us that indicates that the Connection object passed to the createTemporaryBlob method of the BLOB class can't be cast to oracle.jdbc.OracleConnection

Here the interesting lines from our log:
2012-03-15 17:15:47,926 ERROR [org.apache.jackrabbit.core.persistence.db.OraclePersistenceManager] failed to write node state: cafebabe-cafe-babe-cafe-babecafebabe
java.lang.ClassCastException: org.apache.commons.dbcp.PoolingDataSource$PoolGuardConnectionWrapper cannot be cast to oracle.jdbc.OracleConnection
	at oracle.sql.BLOB.createTemporary(BLOB.java:708)
	at org.apache.jackrabbit.core.persistence.db.OraclePersistenceManager.createTemporaryBlob(OraclePersistenceManager.java:375)

I want to highlight at this point that the do not see the issue when using the Oracle Bundled persistence manager, however due to the fact that we haven't used the bundled version in the past we have a lot of customers with repo layouts that can not be used by the bundled persistence manager - we ran some tests and noticed that the consistency check fails.
-> At the moment there is no good upgrade path to move a repo to the bundled structure, the paths provided thus far are shaky at best.

I did find a solution to the problem that has shown no issues thus far and wanted to share this with you:

It is a one line change that can be made before the wrapped connection is passed to the Oracle driver:
org.apache.jackrabbit.core.util.db.ConnectionFactory.unwrap(con);

This then solves the problem, I also wanted to share that we are using an XA datasource."
1,"Repository is not unlocked if version manager init failed and assertions are enabled. The following test case will work as expected, except when assertions are enabled (java -ea ...):

Connection conn = DriverManager.getConnection(
        ""jdbc:derby:repository/version/db;create=true"");
Statement stat = conn.createStatement();
stat.execute(""create table version_bundle(id int)"");
TransientRepository rep = new TransientRepository();
try {
    rep.login(new SimpleCredentials("""", new char[0]));
} catch (Exception e) {
    // ignore
}
rep.shutdown();
stat.execute(""drop table version_bundle"");
new TransientRepository().login(new SimpleCredentials("""", new char[0]));

The reason is the assertion in RepositoryContext.getInternalVersionManager. Because of this assertion, the repository lock is not released during the repository shutdown.
"
1,"Be consistent about negative vInt/vLong. Today, write/readVInt ""allows"" a negative int, in that it will encode and decode correctly, just horribly inefficiently (5 bytes).

However, read/writeVLong fails (trips an assert).

I'd prefer that both vInt/vLong trip an assert if you ever try to write a negative number... it's badly trappy today.  But, unfortunately, we sometimes rely on this... had we had this assert in 'since the beginning' we could have avoided that.

So, if we can't add that assert in today, I think we should at least fix readVLong to handle negative longs... but then you quietly spend 9 bytes (even more trappy!)."
1,Lucene can incorrectly set the position of tokens that start a field with positonInc 0.. More info in LUCENE-1465
1,"DefaultHttpMethodRetryHandler does not check whether the failed method has been aborted. DefaultHttpMethodRetryHandler does not check whether the failed method has been
aborted."
1,"Core: WEAKREFERENCE properties object have type REFERENCE when being read from the persistent layer. it seems to me that WEAKREFERENCE properties are properly created and stored as such but are read as REFERENCE 
properties when built again from the persistent layer.

how to reproduce:

- create a new WEAKREFERENCE property and save the changes
- force reading from the persistent layer  (in my case I used Day's CRX and restartet the server)
- the former WEAKREFERENCE will now be displayed as REFERENCE.

"
1,"PostgreSQL: Failed to guess validation query. When using PostgreSQL, the following warning appears in the log file:

> *WARN  [org.apache.jackrabbit.core.util.db.ConnectionFactory] (main)
> Failed to guess validation query for URL
> jdbc:postgresql:..."
1,"wrong stats/scoring from MemoryCodec. I hit some random failures in the flexscoring branch: wierd because its not a random test.

I noticed the test always failed with memorycodec, and wrote a specific test for it.

I haven't traced thru it yet, but I think its likely the issue that memorycodec is somehow returning wrong stats here?"
1,"HttpMethodDirector.executeWithRetry method fails to close the underlying connection if a RuntimeException is thrown. The following code snippet is from the end of the HttpMethodDirector.executeWithRetry method:

        } catch (IOException e) {
            if (this.conn.isOpen()) {
                LOG.debug(""Closing the connection."");
                this.conn.close();
            }
            releaseConnection = true;
            throw e;
        } catch (RuntimeException e) {
            if (this.conn.isOpen) {                                             <<<===========================   BAD!  :-)
                LOG.debug(""Closing the connection."");
                this.conn.close();
            }
            releaseConnection = true;
            throw e;
        }

When an IOException is caught, you can see that the ""open"" status of the connection is accurately checked by calling the ""isOpen()"" method.

When a RuntimeException is caught, however, the code mistakenly checks only the ""isOpen"" member field.  In the case where ""conn"" is, for example, a MultiThreadedHttpConnectionManager, the ""isOpen()"" method is overridden to check for a wrapped connection and returns the ""isOpen"" status of that connection.  In cases like that checking the ""isOpen"" member field is obviously wrong and we end up not calling ""close()"" and the connection is not cleaned up.  This causes issues with later calls.

A very difficult bug to diagnose and <steps up on soapbox> one that could have been easily avoided by making member variables private! <steps down>  thank you.  :-)


"
1,"Redirect to a relative URL fails. Request the url 
http://commerce1.cera.net/discount-pcbooks/catalog/categories.asp?
search_str=0782128092

On a browser the redirect works, while with HttpClient it doesn't."
1,"BundleFsPersistenceManager has no property called: minBlobSize. 2008-04-03 16:48:51,ERROR,org.apache.jackrabbit.core.RepositoryImpl,Thread-237 failed to start Repository: Cannot instantiate persistence manager org.apache.jackrabbit.core.persistence.bundle.BundleFsPersistenceManager: The bean of type: org.apache.jackrabbit.core.persistence.bundle.BundleFsPersistenceManager has no property called: minBlobSize
javax.jcr.RepositoryException: Cannot instantiate persistence manager org.apache.jackrabbit.core.persistence.bundle.BundleFsPersistenceManager: The bean of type: org.apache.jackrabbit.core.persistence.bundle.BundleFsPersistenceManager has no property called: minBlobSize: The bean of type: org.apache.jackrabbit.core.persistence.bundle.BundleFsPersistenceManager has no property called: minBlobSize
	at org.apache.jackrabbit.core.RepositoryImpl.createPersistenceManager(RepositoryImpl.java:1176)
	at org.apache.jackrabbit.core.RepositoryImpl.createVersionManager(RepositoryImpl.java:390)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:294)
	at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:557)
	at pps.jcr.util.RepositoryManager.createRepository(RepositoryManager.java:117)
	at pps.jcr.util.RepositoryManager.startRepository(RepositoryManager.java:43)
	at pps.jcr.ejb.session.JcrUtilFacade.startRepository(JcrUtilFacade.java:62)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.sun.enterprise.security.application.EJBSecurityManager.runMethod(EJBSecurityManager.java:1067)
	at com.sun.enterprise.security.SecurityUtil.invoke(SecurityUtil.java:176)
	at com.sun.ejb.containers.BaseContainer.invokeTargetBeanMethod(BaseContainer.java:2895)
	at com.sun.ejb.containers.BaseContainer.intercept(BaseContainer.java:3986)
	at com.sun.ejb.containers.EJBLocalObjectInvocationHandler.invoke(EJBLocalObjectInvocationHandler.java:197)
	at com.sun.ejb.containers.EJBLocalObjectInvocationHandlerDelegate.invoke(EJBLocalObjectInvocationHandlerDelegate.java:127)
	at $Proxy181.startRepository(Unknown Source)
	at pps.jcr.web.JcrLifecycleListener.contextInitialized(JcrLifecycleListener.java:55)
	at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:4523)
	at org.apache.catalina.core.StandardContext.start(StandardContext.java:5184)
	at com.sun.enterprise.web.WebModule.start(WebModule.java:326)
	at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:973)
	at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:957)
	at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:688)
	at com.sun.enterprise.web.WebContainer.loadWebModule(WebContainer.java:1584)
	at com.sun.enterprise.web.WebContainer.loadWebModule(WebContainer.java:1222)
	at com.sun.enterprise.web.WebContainer.loadJ2EEApplicationWebModules(WebContainer.java:1147)
	at com.sun.enterprise.server.TomcatApplicationLoader.doLoad(TomcatApplicationLoader.java:141)
	at com.sun.enterprise.server.AbstractLoader.load(AbstractLoader.java:244)
	at com.sun.enterprise.server.ApplicationManager.applicationDeployed(ApplicationManager.java:336)
	at com.sun.enterprise.server.ApplicationManager.applicationDeployed(ApplicationManager.java:210)
	at com.sun.enterprise.server.ApplicationManager.applicationDeployed(ApplicationManager.java:645)
	at com.sun.enterprise.admin.event.AdminEventMulticaster.invokeApplicationDeployEventListener(AdminEventMulticaster.java:928)
	at com.sun.enterprise.admin.event.AdminEventMulticaster.handleApplicationDeployEvent(AdminEventMulticaster.java:912)
	at com.sun.enterprise.admin.event.AdminEventMulticaster.processEvent(AdminEventMulticaster.java:461)
	at com.sun.enterprise.admin.event.AdminEventMulticaster.multicastEvent(AdminEventMulticaster.java:176)
	at com.sun.enterprise.admin.server.core.DeploymentNotificationHelper.multicastEvent(DeploymentNotificationHelper.java:308)
	at com.sun.enterprise.deployment.phasing.DeploymentServiceUtils.multicastEvent(DeploymentServiceUtils.java:226)
	at com.sun.enterprise.deployment.phasing.ServerDeploymentTarget.sendStartEvent(ServerDeploymentTarget.java:298)
	at com.sun.enterprise.deployment.phasing.ApplicationStartPhase.runPhase(ApplicationStartPhase.java:132)
	at com.sun.enterprise.deployment.phasing.DeploymentPhase.executePhase(DeploymentPhase.java:108)
	at com.sun.enterprise.deployment.phasing.PEDeploymentService.executePhases(PEDeploymentService.java:919)
	at com.sun.enterprise.deployment.phasing.PEDeploymentService.start(PEDeploymentService.java:591)
	at com.sun.enterprise.deployment.phasing.PEDeploymentService.start(PEDeploymentService.java:635)
	at com.sun.enterprise.admin.mbeans.ApplicationsConfigMBean.start(ApplicationsConfigMBean.java:744)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.sun.enterprise.admin.MBeanHelper.invokeOperationInBean(MBeanHelper.java:375)
	at com.sun.enterprise.admin.MBeanHelper.invokeOperationInBean(MBeanHelper.java:358)
	at com.sun.enterprise.admin.config.BaseConfigMBean.invoke(BaseConfigMBean.java:464)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
	at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.sun.enterprise.admin.util.proxy.ProxyClass.invoke(ProxyClass.java:90)
	at $Proxy1.invoke(Unknown Source)
	at com.sun.enterprise.admin.server.core.jmx.SunoneInterceptor.invoke(SunoneInterceptor.java:304)
	at com.sun.enterprise.interceptor.DynamicInterceptor.invoke(DynamicInterceptor.java:174)
	at com.sun.enterprise.deployment.client.DeploymentClientUtils.startApplication(DeploymentClientUtils.java:145)
	at com.sun.enterprise.deployment.client.DeployAction.run(DeployAction.java:537)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.IllegalArgumentException: The bean of type: org.apache.jackrabbit.core.persistence.bundle.BundleFsPersistenceManager has no property called: minBlobSize
	at org.apache.commons.collections.BeanMap.put(BeanMap.java:367)
	at org.apache.jackrabbit.core.config.BeanConfig.newInstance(BeanConfig.java:109)
	at org.apache.jackrabbit.core.RepositoryImpl.createPersistenceManager(RepositoryImpl.java:1171)
	... 64 more

<?xml version=""1.0""?>
<Repository>
    <DataStore class=""org.apache.jackrabbit.core.data.FileDataStore"">
        <param name=""path"" value=""${rep.home}/datastore""/>
        <param name=""minRecordLength"" value=""100""/>
    </DataStore>    
    <FileSystem class=""org.apache.jackrabbit.core.fs.local.LocalFileSystem"">
        <param name=""path"" value=""${rep.home}""/>
    </FileSystem>    
    <Security appName=""Jackrabbit"">
        <AccessManager class=""org.apache.jackrabbit.core.security.SimpleAccessManager"" />
        <LoginModule class=""org.apache.jackrabbit.core.security.SimpleLoginModule"">
            <param name=""anonymousId"" value=""anonymous"" />
        </LoginModule>
    </Security>
    <Workspaces rootPath=""${rep.home}/workspaces"" defaultWorkspace=""default"" />
    <Workspace name=""${wsp.name}"">
        <FileSystem class=""org.apache.jackrabbit.core.fs.local.LocalFileSystem"">
            <param name=""path"" value=""${rep.home}/${wsp.name}""/>
        </FileSystem>            
        <!--  <PersistenceManager class=""org.apache.jackrabbit.core.persistence.obj.ObjectPersistenceManager""/>  -->
        <PersistenceManager class=""org.apache.jackrabbit.core.persistence.bundle.BundleFsPersistenceManager"">
            <param name=""bundleCacheSize"" value=""8""/> 
            <param name=""blobFSBlockSize"" value=""0""/> 
            <param name=""minBlobSize"" value=""4096""/> 
            <param name=""errorHandling"" value=""""/>             
        </PersistenceManager>        
        <SearchIndex class=""org.apache.jackrabbit.core.query.lucene.SearchIndex"">
            <param name=""path"" value=""${wsp.home}/index""/>    
            <param name=""textFilterClasses"" value=""
                   org.apache.jackrabbit.extractor.MsExcelTextExtractor,
                   org.apache.jackrabbit.extractor.MsPowerPointTextExtractor,
                   org.apache.jackrabbit.extractor.MsWordTextExtractor,
                   org.apache.jackrabbit.extractor.PdfTextExtractor,
                   org.apache.jackrabbit.extractor.PlainTextExtractor,
                   org.apache.jackrabbit.extractor.HTMLTextExtractor,
                   org.apache.jackrabbit.extractor.XMLTextExtractor,
                   org.apache.jackrabbit.extractor.RTFTextExtractor,
            org.apache.jackrabbit.extractor.OpenOfficeTextExtractor""/>
        </SearchIndex>
    </Workspace>
    <Versioning rootPath=""${rep.home}/version"">
        <FileSystem class=""org.apache.jackrabbit.core.fs.local.LocalFileSystem"">
            <param name=""path"" value=""${rep.home}/version""/>
        </FileSystem>                
        <!-- <PersistenceManager class=""org.apache.jackrabbit.core.persistence.obj.ObjectPersistenceManager""/> -->
        <PersistenceManager class=""org.apache.jackrabbit.core.persistence.bundle.BundleFsPersistenceManager"">
            <param name=""bundleCacheSize"" value=""8""/> 
            <param name=""blobFSBlockSize"" value=""0""/> 
            <param name=""minBlobSize"" value=""4096""/> 
            <param name=""errorHandling"" value=""""/>             
        </PersistenceManager>            
    </Versioning>
    <SearchIndex class=""org.apache.jackrabbit.core.query.lucene.SearchIndex"">
        <param name=""path"" value=""${rep.home}/index""/>    
        <param name=""textFilterClasses"" value=""
               org.apache.jackrabbit.extractor.MsExcelTextExtractor,
               org.apache.jackrabbit.extractor.MsPowerPointTextExtractor,
               org.apache.jackrabbit.extractor.MsWordTextExtractor,
               org.apache.jackrabbit.extractor.PdfTextExtractor,
               org.apache.jackrabbit.extractor.PlainTextExtractor,
               org.apache.jackrabbit.extractor.HTMLTextExtractor,
               org.apache.jackrabbit.extractor.XMLTextExtractor,
               org.apache.jackrabbit.extractor.RTFTextExtractor,
        org.apache.jackrabbit.extractor.OpenOfficeTextExtractor""/>
    </SearchIndex>    
</Repository>
"
1,"ParallelTermEnum is BROKEN. ParallelTermEnum.next() fails to advance properly to new fields.  This is a serious bug. 

Christian Kohlschuetter diagnosed this as the root problem underlying LUCENE-398 and posted a first patch there.

I've addressed a couple issues in the patch (close skipped field TermEnum's, generate field iterator only once, integrated Christian's test case as a Lucene test) and packaged in all the revised patch here.

All Lucene tests pass, and I've further tested in this in my app, which makes extensive use of ParallelReader.
"
1,"MockRAMDirectory (used only by unit tests) has some synchronization problems. Coming out of a failure that Earwin noted on java-dev this morning, I reworked the synchronization on MockRAMDirectory."
1,"BlockJoinCollector only allows retrieving groups for only one BlockJoinQuery. Spinoff from Mark Harwood's email (subject ""BlockJoin concerns"") to
dev list.

It's fine to use multiple nested joins in a single query, and
BlockJoinCollector should let you retrieve the top groups for all of
them.

But currently it always returns null after the first query's groups
have been retrieved, because of a silly bug.
"
1,"Parameters 'idleTime' and 'queryClass' cause QueryHandler to fail. This issue does not occur in a released jackrabbit-core version. With the changes from JCR-1462 jackrabbit now fails to startup if there is an unknown parameter in a bean configuration.

The parameters 'idleTime' and 'queryClass' are not used by the QueryHandler but by the SearchManager, which instantiates the QueryHandler. Therefore the parameters do not show up in the QueryHandler.

I suggest we introduce them in the common base class AbstractQueryHandler."
1,"NPE crash in case of out of memory. The attached class makes Lucene crash with an NPE when starting it with -Xmx10M, although there's probably an OutOfMemory problem. The stacktrace:

Exception in thread ""main"" java.lang.NullPointerException
	at java.util.Arrays.fill(Unknown Source)
	at org.apache.lucene.index.DocumentsWriter$ByteBlockPool.reset(DocumentsWriter.java:2873)
	at org.apache.lucene.index.DocumentsWriter$ThreadState.resetPostings(DocumentsWriter.java:637)
	at org.apache.lucene.index.DocumentsWriter.resetPostingsData(DocumentsWriter.java:458)
	at org.apache.lucene.index.DocumentsWriter.abort(DocumentsWriter.java:423)
	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:2433)
	at org.apache.lucene.index.DocumentsWriter.addDocument(DocumentsWriter.java:2397)
	at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1445)
	at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1424)
	at LuceneCrash.myrun(LuceneCrash.java:32)
	at LuceneCrash.main(LuceneCrash.java:19)

The documents are quite big (some hundred KB each), I cannot attach them but I can send them via private mail if needed. The crash happens the first time reset() is called, after indexing 10 documents. I assume the bug is just that the error is misleading, there maybe should be an OOM error.
"
1,"Static variables need to be final (or access should be synchronised):. Static variables need to be final (or access should be synchronised):

Index: module-client/src/main/java/org/apache/http/conn/params/HttpConnectionManagerParams.java
===================================================================
--- module-client/src/main/java/org/apache/http/conn/params/HttpConnectionManagerParams.java	(revision 652021)
+++ module-client/src/main/java/org/apache/http/conn/params/HttpConnectionManagerParams.java	(working copy)
@@ -53,7 +53,7 @@
     public static final int DEFAULT_MAX_TOTAL_CONNECTIONS = 20;
 
     /** The default maximum number of connections allowed per host */
-    private static ConnPerRoute DEFAULT_CONN_PER_ROUTE = new ConnPerRoute() {
+    private static final ConnPerRoute DEFAULT_CONN_PER_ROUTE = new ConnPerRoute() {
         
         public int getMaxForRoute(HttpRoute route) {
             return ConnPerRouteBean.DEFAULT_MAX_CONNECTIONS_PER_ROUTE;
"
1,"IndexWriter applies wrong deletes during concurrent flush-all. Yonik uncovered this with the TestRealTimeGet test: if a flush-all is
underway, it is possible for an incoming update to pick a DWPT that is
stale, ie, not yet pulled/marked for flushing, yet the DW has cutover
to a new deletes queue.  If this happens, and the deleted term was
also updated in one of the non-stale DWPTs, then the wrong document is
deleted and the test fails by detecting the wrong value.

There's a 2nd failure mode that I haven't figured out yet, whereby 2
docs are returned when searching by id (there should only ever be 1
doc since the test uses updateDocument which is atomic wrt
commit/reopen).

Yonik verified the test passes pre-DWPT, so my guess is (but I
have yet to verify) this test also passes on 3.x.  I'll backport
the test to 3.x to be sure.
"
1,".war distribution should be configurable, prompting you to setup JNDI with the Repository Home and Config locations.. The Embedded Deployment Model documentation (http://jackrabbit.apache.org/doc/deploy/howto-model1.html) on the jackrabbit page describes how to package up a .war file so that you can use JNDI Resource settings to change the location of the repository home and the repository configuration xml file.

Unfortunately, the .war file that is provided as part of the Jackrabbit distribution doesn't behave like this. Instead, it has an inbuilt repository.xml file and settings in web.xml that act as defaults. These defaults are not useful and force a user to act like a developer and modify the files within the .war file.

The current situation is that we have a .war that's not going to be useful to anyone without modification. The repository.xml file that is contained within the .war makes the repository home to be the Tomcat/bin/repository directory. This is not a useful default. It's better to have no default setup and a clear error message that JNDI needs to be setup. It would be even better if the web application could recognise when the JNDI wasn't configured and could prompt the user with an instructional webpage, describing how to setup the required JNDI settings on Tomcat, JBoss etc.

----

The .war distribution for Jackrabbit ignores the JNDI settings that are described in the documentation. I am using this Tomcat config.xml snippet to configure Tomcat 5.5:

{{{
<?xml version='1.0' encoding='utf-8'?>
<Context displayName=""Ark"" docBase=""c:\dev\ark\jackrabbit-server-1.1.1.war"" path=""/ark"" 
         useNaming=""false"" workDir=""work\Catalina\localhost\ark"" unpackWAR=""false"">

<Resource name=""jcr/repository""
          auth=""Container""
          type=""javax.jcr.Repository""
          factory=""org.apache.jackrabbit.core.jndi.BindableRepositoryFactory""
          configFilePath=""c:/dev/ark/src/main/resources/repository.xml""
          repHomeDir=""c:/jackrabbitrepo""/>

</Context>
}}}

Jackrabbit loads fine. However, the logs show:

{{{
02.01.2007 10:33:00 *INFO * RepositoryStartupServlet: RepositoryStartupServlet initializing... (RepositoryStartupServlet.java, line 190)
02.01.2007 10:33:00 *INFO * RepositoryStartupServlet:   repository-home = C:\Program Files\Apache Software Foundation\Tomcat 5.5\bin\jackrabbit\repository (RepositoryStartupServlet.java, line 242)

...
...

02.01.2007 10:33:00 *INFO * LocalFileSystem: LocalFileSystem initialized at path C:\Program Files\Apache Software Foundation\Tomcat 5.5\bin\jackrabbit\repository\repository (LocalFileSystem.java, line 166)
}}}






----

My use case is that I want to use Jackrabbit to host a Maven 2 repository within my company. So, ideally I want to:
   * Download the Jackrabbit .war file and mount it on my Tomcat server as context ""/maven2"".
   * Configure Tomcat to use LDAP authentication and point it at my company's LDAP server. This is a standard J2EE feature, of course.
   * Create my own repository.xml file which points to my AccessManager implementation (which goes to my company's SingleSignOn service for authorization). My AccessManager implementation will be placed on the Tomcat shared classpath.
   * Set the repository home directory, where all the working files will be placed and the location of the repository.xml file. Ideally, this would be done in JNDI.

If I have to put together my own Jackrabbit .war file, I consider that I have my ""developer"" hat on when I only really want to have my ""Jackrabbit user"" hat on.
"
1,"Host request header does not contain port. The Host request header is always added with just the hostname used for the 
connection.  If the port is different than 80 it needs to be included as well, 
with a colon separating it from the hostname.  This problem is especially 
apparent when you use the httpclient to connect to tomcat 4 and then use 
HttpUtils to create a full URL representing the request.  HttpUtils pulls the 
host and port from the Host header.  When commons-httpclient is used HttpUtils 
never includes the port since it was never in the Host header."
1,"StandardTokenizer loses Korean characters. While using StandardAnalyzer, exp. StandardTokenizer with Korean text stream, StandardTokenizer ignores the Korean characters. This is because the definition of CJK token in StandardTokenizer.jj JavaCC file doesn't have enough range covering Korean syllables described in Unicode character map.
This patch adds one line of 0xAC00~0xD7AF, the Korean syllables range to the StandardTokenizer.jj code."
1,"CartesianPolyFilterBuilder doesn't properly account for which tiers actually exist in the index . In the CartesianShapeFilterBuilder, there is logic that determines the ""best fit"" tier to create the Filter against.  However, it does not account for which fields actually exist in the index when doing so.  For instance, if you index tiers 1 through 10, but then choose a very small radius to restrict the space to, it will likely choose a tier like 15 or 16, which of course does not exist."
1,"Journal log file rotation overwrites old files. Journal log files are rotated as follows:

  journal.log.N -> journal.log.(N+1)

Because the list of files to be rotated is created with alphanumeric sort order
(descending), it may destroy files in the following situation:

  journal.log.9  -> journal.log.10
  ..
  journal.log.2  -> journal.log.3
  journal.log.10 -> journal.log.11 (!)
  journal.log.1  -> journal.log.2

i.e. journal.log.10 is overwritten by journal.log.9 and then moved."
1,"QueryResult's RowIterator.getSize returned the wrong size of the results after I implemented my own AccessManager. The background is I have implemented my own AccessManager. After executing a query and get back the RowIterator from the result, if I call rowiterator.getSize, it will return the size of all nodes matching my query (without honoring the access control) . But if I iterate through the result, I find lots of duplicates in the results; and if I filter out those duplicate, the final result is quite off the original number from RowIteartor.getSize()

BTW, I also disabled Doc Order sorting.

 "
1,"ClassCastException in GroupImpl.isCyclicMembership. Given three groups and one user with the following membership relation

group1 > group2 > group3
group2 > user

where x > y means x contains y.

group3.addMember(group1) throws a ClassCastException.

The reason is that the search type (i.e. UserManager.SEARCH_TYPE_GROUP) is not honored correctly when constructing the transitive membership relation. "
1,"import must not ignore xml prefixed attributes. XML import currently ignores attributes that are in the xml namespace.
e.g., DocViewImportHandler's startElement():

                if (atts.getQName(i).startsWith(""xml:"")) {
                    // skipping xml:space, xml:lang, etc.
                    log.debug(""skipping reserved/system attribute "" + atts.getQName(i));
                    continue;
                }

That is a significant loss of information, since xml:base, xml:lang, and xml:id attributes are critical to the content.  We should register the xml prefix as a reserved namespace (not needing an xmlns declaration) and then treat it like any other attribute.

Here are some useful XML examples:

http://xformsinstitute.com/essentials/browse/ch03s02.php
http://www.zvon.org/HowTo/Output/
http://www.w3.org/Math/testsuite/testsuite/TortureTests/Complexity/complex1.xml
http://intertwingly.net/wiki/pie/EchoExample
http://support.sas.com/onlinedoc/913/getDoc/en/engxml.hlp/a002973381.htm

"
1,Remove double quote as illegal XPathSearchChar from helper method in Text. Since Lucene 2.4 the double quote character at the end of a search term is no more illegal and must not be escaped
1,"webdav: nullpointer exception while getting the tikka detector . seems to be introduced by https://issues.apache.org/jira/browse/JCR-2334

05.11.2009 14:28:27 *MARK * servletengine: Servlet threw exception: 
java.lang.NullPointerException
	at org.apache.jackrabbit.server.io.DefaultHandler.detect(DefaultHandler.java:668)
	at org.apache.jackrabbit.server.io.XmlHandler.canExport(XmlHandler.java:152)
	at org.apache.jackrabbit.server.io.DefaultHandler.canExport(DefaultHandler.java:557)
	at org.apache.jackrabbit.server.io.PropertyManagerImpl.exportProperties(PropertyManagerImpl.java:58)
	at org.apache.jackrabbit.webdav.simple.DavResourceImpl.initProperties(DavResourceImpl.java:320)
	at org.apache.jackrabbit.webdav.simple.DeltaVResourceImpl.initProperties(DeltaVResourceImpl.java:248)
	at org.apache.jackrabbit.webdav.simple.VersionControlledResourceImpl.initProperties(VersionControlledResourceImpl.java:320)
	at org.apache.jackrabbit.webdav.simple.DavResourceImpl.getProperties(DavResourceImpl.java:300)
	at org.apache.jackrabbit.webdav.MultiStatusResponse.<init>(MultiStatusResponse.java:181)
	at org.apache.jackrabbit.webdav.MultiStatus.addResourceProperties(MultiStatus.java:62)
	at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.doPropFind(AbstractWebdavServlet.java:447)
	at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.execute(AbstractWebdavServlet.java:235)
	at com.day.crx.j2ee.CRXDavServlet.service(CRXDavServlet.java:76)
	at com.day.crx.j2ee.ResourceServlet.service(ResourceServlet.java:97)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
	at com.day.j2ee.servletengine.ServletRuntimeEnvironment.service(ServletRuntimeEnvironment.java:228)
	at com.day.j2ee.servletengine.RequestDispatcherImpl.doFilter(RequestDispatcherImpl.java:315)
	at com.day.j2ee.servletengine.RequestDispatcherImpl.service(RequestDispatcherImpl.java:334)
	at com.day.j2ee.servletengine.RequestDispatcherImpl.service(RequestDispatcherImpl.java:378)
	at com.day.j2ee.servletengine.ServletHandlerImpl.execute(ServletHandlerImpl.java:313)
	at com.day.j2ee.servletengine.DefaultThreadPool$DequeueThread.run(DefaultThreadPool.java:134)
	at java.lang.Thread.run(Thread.java:613)"
0,add test case for recovering from broken version history hierarchy. The test should exercise recovery from a missing parent node of a VHR.
0,"Session#importXML can't handle properly uuid collision if user has insufficient permission. When importing referenceable nodes, if there are nodes with the same uuid in the workspace but the session has no sufficient permission to read them then the import will fail no matter what ImportUUIDBehavior is chosen. 
But the same xml will be imported successfully in another repository or if the user have read access.

SessionImpl.java :
 public NodeImpl getNodeById(NodeId id) ...{
...
 try {
            return (NodeImpl) getItemManager().getItem(id);
        } catch (AccessDeniedException ade) {
            throw new ItemNotFoundException(id.toString());
        }
}

SessionImporter.java :

 public void startNode(NodeInfo nodeInfo, List propInfos)...{
...
  if (node == null) {
            // create node
            if (id == null) {
            ...
            } else {
                // potential uuid conflict
                NodeImpl conflicting;
                try {
                    conflicting = session.getNodeById(id);
                } catch (ItemNotFoundException infe) {
                    conflicting = null;
                }
                if (conflicting != null) {
                    // resolve uuid conflict
                 ...
               }
...
}

In the JCR 1.0 spec says ""lack of read access to an item blocks access to both information about the content of that item and information about the existence of the item"" but this should probably not be true, internally, when doing an import. 
Otherwise it means that read access to an entire workspace must be granted to a user so that it could successfully use the IMPORT_UUID_CREATE_NEW behaviour.

"
0,"Provide possibility to import protected items using Session import functionality. SessionImporter and WorkspaceImporter currently skip all protected items encountered during import except for some special cases
(see JCR-2172 and WorkspaceImporter#postProcessNode).
The specification only mandates that protected content is treated in a consistent manner, but allows the implementation to either import or ignore it.

Find attached a patch containing some initials steps to allow to extend the default import behavior:
Instead of skipping protected items (and in case of nodes the complete tree below it), they should be passed to a separate handler,
that may or may not be able to deal with them and needs to assert, that they are in a valid format.

The patch includes:

- Abstract classes for that protected item import
- Default implementations that never import protected nodes (same behavior as we have today)
- An example implementation for the AC-content (just to see if it works for simple cases) + some trivial tests.
- Changes to SessionImporter to demonstrate how import of protected items would be enabled.

The patch doesn't include yet:

- Changes to WorkspaceImporter (would +- be according to SessionImporter)
- Changes to WorkspaceImpl/SessionImpl as well as configuration that would allow to modify the default behavior.
- Examples for import of protected properties.
- Examples for workspace import.

The patch has the following limitations or TODOs:

- Proper handling of protected references properties or non-protected ref properties with the tree defined by a protected node.
- Test / Careful review if the various ImportUUIDBehaviors are/can properly be covered, specially in case of ""replace-existing"".

The patch in addition addresses:

- An inconsistency in the SessionImporter:
  > Attempt to import protected content below an existing protected node => skipped
  > Attempt to import protected content that doesn't yet exist => first node is imported, ConstraintViolationException for child-nodes.
  > This behavior is also reflected in the Node-stack... where in the first case 'null' is pushed, in the second case the first protected node.
     (see also JCR-2172 for details).
"
0,"Minor typo in org.apache.commons.httpclient.Wire 2.0-rc1. Minor typo ""...may noy be null"" in 
public static final void output(final String s) and
public static final void input(final String s)
of org.apache.commons.httpclient.Wire."
0,"filter jcr properties in jcr-server. attached is a patch that implements jcr property filtering in jcr-server in the same way that nodes and resources are filtered. with the default filter configuration, this has the effect of filtering jcr:created, jcr:mixinTypes, and jcr:primaryType from nt:folder and nt:file nodes. 

this is likely the expected default behavior for most webdav servers - they want to return the normal dav properties, live properties defined themselves, and dead properties defined by clients, but not jcr-internal properties which are for all intents and purposes implementation-specific."
0,Snowball package contains BSD licensed code with ASL header. All classes in org.tartarus.snowball (but not in org.tartarus.snowball.ext) has for some reason been given an ASL header. These classes are licensed with BSD. Thus the ASL header should be removed. I suppose this a misstake or possible due to the ASL header automation tool.
0,"Reduce log level in MultiIndex for deleting obsolete index. The MultiIndex class issues a logging message (with info level) that the obsolete index cannot be deleted (quite often).
As the segments are deleted later (with a retry) and this ""warning"" can be ignored (http://dev.day.com/kb/content/wiki/kb/Crx/Troubleshooting/UnableToDeleteObsoleteIndex.html ), it would be nice to reduce the logging level to debug. People, who are maintaining projects, are not aware of Jackarabbit details and are sometimes scared about this ""warning"" :-)

Thank you in advance!

Kind regards
Sergiy"
0,"Reusable Repository access and bind servlets. As discussed in http://mail-archives.apache.org/mod_mbox/jackrabbit-dev/200705.mbox/%3C510143ac0705151453t7a0eb4cam859a40fb106e81f5@mail.gmail.com%3E and JCR-955, it would be useful to have a reusable set of servlet components for accessing and exposing repositories in various configurable ways.

My plan is to refactor the current RepositoryAccessServlet from jackrabbit-webapp and place the resulting servlet components in jackrabbit-jcr-commons, with servlet-api as a new optional (or provided) dependency."
0,Make inspection of BooleanQuery more efficient. Just attempting to inspect a BooleanQuery allocates two new arrays.  This could be cheaper.
0,JCR2SPI: Use namespace decl. present in imported xml to resolve Name/Path values. 
0,"HyphenationCompoundWordTokenFilter fails to load DTD in Crimson parser (JDK 1.4). HyphenationCompoundWordTokenFilter loads the DTD in its XML parser from memory by supplying EntityResolver. In Java 1.4 (affects Lucene 2.9, but also later versions if not Apache Xerces is used as XML parser) this does not work, because Cromson does not even ask the entity resolver, if no base URI is known. As the hyphenation file is loaded from Reader/InputStream no base URI is known. Crimson needs at least a non-null systemId to proceed.

This patch (Lucene 2.9 only)  fakes this by supplying a fake systemId to the InputSource."
0,"New Analyzer for buffering tokens. In some cases, it would be handy to have Analyzer/Tokenizer/TokenFilters that could siphon off certain tokens and store them in a buffer to be used later in the processing pipeline.

For example, if you want to have two fields, one lowercased and one not, but all the other analysis is the same, then you could save off the tokens to be output for a different field.

Patch to follow, but I am still not sure about a couple of things, mostly how it plays with the new reuse API.

See http://www.gossamer-threads.com/lists/lucene/java-dev/54397?search_string=BufferingAnalyzer;#54397"
0,"optimizations for bufferedindexinput. along the same lines as LUCENE-2816:
* the readVInt/readVLong/readShort/readInt/readLong are not optimal here since they defer to readByte. for example this means checking the buffer's bounds per-byte in readVint instead of per-vint.
* its an easy win to speed this up, even for the vint case: its essentially always faster, the only slower case is 1024 single-byte vints in a row, in this case we would do a single extra bounds check (1025 instead of 1024)
"
0,"exceptions from other threads in beforeclass/etc do not fail the test. Lots of tests create indexes in beforeClass methods, but if an exception is thrown from another thread
it won't fail the test... e.g. this test passes:
{code}
public class TestExc extends LuceneTestCase {
  @BeforeClass
  public static void beforeClass() {
    new Thread() {
      public void run() {
        throw new RuntimeException(""boo!"");
      }  
    }.start();
  }
  
  public void test() { }
}
{code}

this is because the uncaught exception handler is in setup/teardown"
0,"User/Developer Documentation. - quotes from user's on why they used it.
- better project docs, including checkstyle and clover reports, and changelog
- examples showing how to configure logging and why you may want to
- Links on HttpClient to 'sister' projects such as Slide, Cactus and Latka to
show where how it's being used."
0,"spi2dav Improve performance for large binary properties. Sending large binary properties over spi2dav is slow and requires a lot of heap space in both client and server.
One problematic part is base64 conversion of the property value.

On the contrary, using 'normal' webdav interface (/repository/default/ instead of /server) for uploading a file (through traditional webdav client) it is pretty fast and don't have such impact on heap space.

Some suggestions from the previous discussion:
 - avoid temporary copies of the data, and persist large objects as early as possible. 
 - transfer large objects in blocks from the Jackrabbit SPI client to the server (and back).
 - make usage of the global data store (JCR-926). 
 - straight forward PUT for single-valued properties

Link to discussion: http://www.mail-archive.com/dev@jackrabbit.apache.org/msg09481.html
"
0,"System Reqs page should be release specific. The System Requirements page, currently under the Main->Resources section of the website should be part of a given version's documentation, since it will be changing for a given release.  

I will ""deprecate"" the existing one, but leave it in place(with a message) to cover the existing releases that don't have this, but will also add it to the release docs for future releases."
0,"Can not subscribe. Hello, 
I have sent email to lucene-dev-subscribe@jakarta.apache.org and it always 
returns failed: 
 
<lucene-dev-subscribe@jakarta.apache.org>: 
Sorry, no mailbox here by that name. (#5.1.1) 
 
Please help me subscribe."
0,"create a simple test that indexes and searches byte[] terms. Currently, the only good test that does this is Test2BTerms (disabled by default)

I think we should test this capability, and also have a simpler example for how to do this.
"
0,"Upgrade to latest Apache parent POM. We're quite a bit behind the latest and greatest of the Apache parent POMs (org.apache:apache), mostly since we're inheriting it through the now mostly unused org.apache.jackrabbit:parent POM.

I'd like to move things back from the o.a.j:parent POM to the jackrabbit-parent POM that's located inside trunk. This will allow us to upgrade to the latest Apache parent POM without the trouble of an extra release of the o.a.j:parent POM."
0,"Change Visibility of fields[] in MultiFieldQueryParser. In MultiFieldQueryParser the two methods 

  protected Query getFuzzyQuery(String field, String termStr, float minSimilarity) throws ParseException
  protected Query getWildcardQuery(String field, String termStr) throws ParseException

are intended to be overwritten if one would like to avoid fuzzy and wildcard queries. However, the String[] fields attribute of this class is private and hence it is not accessible in subclasses of MFQParser. If you just change it to protected this issue should be solved."
0,"remove 1.5 only unit test code that snuck in. I just tried to run unit tests w/ Java 1.4.2, but hit this:

{code}
common.compile-test:
    [mkdir] Created dir: /lucene/src/diagnostics.1654/build/classes/test
    [javac] Compiling 191 source files to /lucene/src/diagnostics.1654/build/classes/test
    [javac] /lucene/src/diagnostics.1654/src/test/org/apache/lucene/index/TestIndexWriterReader.java:26: package java.util.concurrent.atomic does not exist
    [javac] import java.util.concurrent.atomic.AtomicInteger;
    [javac]                                    ^
    [javac] /lucene/src/diagnostics.1654/src/test/org/apache/lucene/index/TestIndexWriterReader.java:262: cannot resolve symbol
    [javac] symbol  : class AtomicInteger 
    [javac] location: class org.apache.lucene.index.TestIndexWriterReader.DeleteThreads
    [javac]     AtomicInteger delCount = new AtomicInteger();
    [javac]     ^
    [javac] /lucene/src/diagnostics.1654/src/test/org/apache/lucene/index/TestIndexWriterReader.java:332: cannot resolve symbol
    [javac] symbol  : class AtomicInteger 
    [javac] location: class org.apache.lucene.index.TestIndexWriterReader.AddDirectoriesThreads
    [javac]     AtomicInteger count = new AtomicInteger(0);
    [javac]     ^
    [javac] /lucene/src/diagnostics.1654/src/test/org/apache/lucene/index/TestIndexWriterReader.java:333: cannot resolve symbol
    [javac] symbol  : class AtomicInteger 
    [javac] location: class org.apache.lucene.index.TestIndexWriterReader.AddDirectoriesThreads
    [javac]     AtomicInteger numAddIndexesNoOptimize = new AtomicInteger(0);
    [javac]     ^
    [javac] /lucene/src/diagnostics.1654/src/test/org/apache/lucene/index/TestIndexWriterReader.java:262: cannot resolve symbol
    [javac] symbol  : class AtomicInteger 
    [javac] location: class org.apache.lucene.index.TestIndexWriterReader.DeleteThreads
    [javac]     AtomicInteger delCount = new AtomicInteger();
    [javac]                                  ^
    [javac] /lucene/src/diagnostics.1654/src/test/org/apache/lucene/index/TestIndexWriterReader.java:332: cannot resolve symbol
    [javac] symbol  : class AtomicInteger 
    [javac] location: class org.apache.lucene.index.TestIndexWriterReader.AddDirectoriesThreads
    [javac]     AtomicInteger count = new AtomicInteger(0);
    [javac]                               ^
    [javac] /lucene/src/diagnostics.1654/src/test/org/apache/lucene/index/TestIndexWriterReader.java:333: cannot resolve symbol
    [javac] symbol  : class AtomicInteger 
    [javac] location: class org.apache.lucene.index.TestIndexWriterReader.AddDirectoriesThreads
    [javac]     AtomicInteger numAddIndexesNoOptimize = new AtomicInteger(0);
    [javac]                                                 ^
    [javac] /lucene/src/diagnostics.1654/src/test/org/apache/lucene/search/TestSort.java:932: cannot resolve symbol
    [javac] symbol  : method getSimpleName ()
    [javac] location: class java.lang.Class
    [javac]           assertEquals(actualTFCClasses[j], tdc.getClass().getSimpleName());
    [javac]                                                         ^
    [javac] /lucene/src/diagnostics.1654/src/test/org/apache/lucene/search/TestTopScoreDocCollector.java:70: cannot resolve symbol
    [javac] symbol  : method getSimpleName ()
    [javac] location: class java.lang.Class
    [javac]         assertEquals(actualTSDCClass[i], tdc.getClass().getSimpleName());
    [javac]                                                      ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -deprecation for details.
    [javac] 9 errors
{code}"
0,"PersistenceManager API change breaks backward compatibility. Persistence Manager API change introduced in JCR-1428 breaks backward compatibility. although this is not a public visible API it renders 3rd party PMs invalid that do not extend from AbstractPersistenceManager.
at least for the 1.4.3 patch release, we should not do this.

suggest to revert the API change for the next 1.4.4 release, but leave the method on the abstract pm, and introduce it only for 1.5.
"
0,"Changes.html formatting improvements. Some improvements to the Changes.html generated by the changes2html.pl script via the 'changes-to-html' ant task:

# Simplified the Simple stylesheet (removed monospace font specification) and made it the default.  The Fancy stylesheet is really hard for me to look at (yellow text on light blue background may provide high contrast with low eye strain, but IMHO it's ugly).
# Moved the monospace style from the Simple stylesheet to a new stylesheet named ""Fixed Width""
# Fixed syntax errors in the Fancy stylesheet, so that it displays as intended.
# Added <span style=""attrib"">  to change attributions.
# In the Fancy and Simple stylesheets, change attributions are colored dark green.
# Now properly handling change attributions in CHANGES.txt that have trailing periods.
# Clicking on an anchor to expand its children now changes the document location to show the children.
# Hovering over anchors now causes a tooltip to be displayed - either ""Click to expand"" or ""Click to collapse"" - the tooltip changes appropriately after a collapse or expansion."
0,"Some Lucene tests try and use a Junit Assert in new threads. There are a few cases in Lucene tests where JUnit Asserts are used inside a new threads run method - this won't work because Junit throws an exception when a call to Assert fails - that will kill the thread, but the exception will not propagate to JUnit - so unless a failure is caused later from the thread termination, the Asserts are invalid.

TestThreadSafe
TestStressIndexing2
TestStringIntern"
0,"Random Failure TestSizeBoundedOptimize#testFirstSegmentTooLarge. I am seeing this on trunk  

{noformat}

[junit] Testsuite: org.apache.lucene.index.TestSizeBoundedOptimize
    [junit] Testcase: testFirstSegmentTooLarge(org.apache.lucene.index.TestSizeBoundedOptimize):	FAILED
    [junit] expected:<2> but was:<1>
    [junit] junit.framework.AssertionFailedError: expected:<2> but was:<1>
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:882)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:848)
    [junit] 	at org.apache.lucene.index.TestSizeBoundedOptimize.testFirstSegmentTooLarge(TestSizeBoundedOptimize.java:160)
    [junit] 
    [junit] 
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.658 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestSizeBoundedOptimize -Dtestmethod=testFirstSegmentTooLarge -Dtests.seed=7354441978302993522:-457602792543755447 -Dtests.multiplier=3
    [junit] NOTE: test params are: codec=Standard, locale=sv_SE, timezone=Mexico/BajaNorte
    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestSizeBoundedOptimize]
    [junit] ------------- ---------------- ---------------
    [junit] Test org.apache.lucene.index.TestSizeBoundedOptimize FAILED
{noformat}

when running with this seed
ant test -Dtestcase=TestSizeBoundedOptimize -Dtestmethod=testFirstSegmentTooLarge -Dtests.seed=7354441978302993522:-457602792543755447 -Dtests.multiplier=3"
0,"Make ShingleAnalyzerWrapper and PerFieldAnalyzerWrapper immutable. Both ShingleAnalyzerWrapper and PerFieldAnalyzerWrapper have setters which change some state which impacts their analysis stack.  If these are going to become reusable, then the state must be immutable as changing it will have no effect.

Process will be similar to QueryAutoStopWordAnalyzer, I will remove in trunk and deprecate in 3x."
0,"provide an ehcache implementation for HttpCache. Provide an implementation of the HttpCache interface that stores cache entries in ehcache.
"
0,"Move tika-parsers dependency to deployment packages. As discussed on the mailing list, it would be better if the tika-parsers dependency (and all the parser libraries it pulls in transitively) was included in our deployment packages but not directly in jackrabbit-core. This would make it easier for people to set up custom lightweight deployments with no or only partial full text extraction functionality.

To do this we'll first need to wait for Tika 0.9, as we currently have a custom PDFParser class in jackrabbit-core as a workaround to a problem in Tika 0.8.

At the same time we should do a more thorough review of the transitive parser dependencies we include. At least the rome and bouncycastle libraries were flagged as potentially unnecessary."
0,"Remove GCJ IndexReader specializations. These specializations are outdated, unsupported, most probably pointless due to the speed of modern JVMs and, I bet, nobody uses them (Mike, you said you are going to ask people on java-user, anybody replied that they need it?). While giving nothing, they make SegmentReader instantiation code look real ugly.

If nobody objects, I'm going to post a patch that removes these from Lucene."
0,"jcr2spi: avoid unnecessary roundtrips with NodeEntry.getPropertyEntry. Since NodeInfo.getPropertyIds always returns the complete set of property names, there is no need for an extra round trip to the SPI upon NodeEntry.getPropertyEntry. The corresponding code could be simplified."
0,"changed behavior of javax.jcr.Value get* methods. stream handling semantics of javax.jcr.Value has been simplified in JCR 2.0:

Section '5.10.5.1 Deprecated Binary Behavior'

[...]
Unlike in JCR 1.0, calling a get method other than getStream before 
calling getStream on the same Value object will never cause an 
IllegalStateException. 


see https://jsr-283.dev.java.net/issues/show_bug.cgi?id=658"
0,"Lucene-core 2.9.0 missing from Maven Central Repository. Sub-projects like lucene-demos, lucene-contrib, etc. exist in central, and depend on 2.9.0 of lucene-core. However, the lucene-core 2.9.0 artifact itself is missing."
0,"Build.xml - add log level definitions. The default log level is debug, which produces quite a lot of output when testing.

The patch allows separate definition of wire and other log levels (assuming SimpleLog is used)"
0,"String constants should be final. RFC2109Spec - SET_COOKIE_KEY

RFC2965Spec - SET_COOKIE2_KEY

both should be final."
0,"Create merge policy that doesn't periodically inadvertently optimize. The current merge policy, at every maxBufferedDocs *
power-of-mergeFactor docs added, will do a fully cascaded merge, which
is the same as an optimize.

I think this is not good because at that ""optimization poin"", the
particular addDocument call is [surprisingly] very expensive.  While,
amortized over all addDocument calls, the cost is low, the cost is
paid ""up front"" and in a very ""bunched up"" manner.

I think of this as ""pay it forward"": you are paying the full cost of
an optimize right now on the expectation / hope that you will be
adding a great many more docs.  But, if you don't add that many more
docs, then, the amortized cost for your index is in fact far higher
than it should have been.  Better to ""pay as you go"" instead.

So we could make a small change to the policy by only merging the
first mergeFactor segments once we hit 2X the merge factor.  With
mergeFactor=10, when we have created the 20th level 0 (just flushed)
segment, we merge the first 10 into a level 1 segment.  Then on
creating another 10 level 0 segments, we merge the second set of 10
level 0 segments into a level 1 segment, etc.

With this new merge policy, an index that's a bit bigger than a
current ""optimization point"" would then have a lower amortized cost
per document.  Plus the merge cost is less ""bunched up"" and less ""pay
it forward"": instead you pay for what you are actually using.

We can start by creating this merge policy (probably, combined with
with the ""by size not by doc count"" segment level computation from
LUCENE-845) and then later decide whether we should make it the
default merge policy.
"
0,"Replace license headers with new policy text. We need to replace all of the license headers with a new template
that replaces the Copyright and license lines with

---BEGIN PROPOSED SOURCE FILE HEADER---
  Licensed to the Apache Software Foundation (ASF) under one or more
  contributor license agreements.  The ASF licenses this file to You
  under the Apache License, Version 2.0 (the ""License""); you may not
  use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an ""AS IS"" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
---END PROPOSED SOURCE FILE HEADER---

The copyright line is being removed from files due to legal advice from ASF attorneys.
It is replaced with a statement that the copyright owners have licensed it to the ASF.
"
0,"common interface for HttpRoute and RouteTracker. Classes HttpRoute and RouteTracker have many identical getters. There should be a common interface, for example RouteInfo, to define these getters and a toRoute() method that returns an unmodifiable representation. Some portions of the API may then accept the interface instead of the specific class HttpRoute.
"
0,"Trim whitespace from parameter names in configuration files. We've had a couple of issues with extra whitespace in parameter names causing those configuration options being lost. Now with the more strict validation of configuration settings such mistakes can even prevent the repository from starting. On one hand that's a good thing, as the user would then explicitly need to fix such broken configurations, but it would be nice if no user intervention was needed.

Since leading and trailing whitespace is never allowed in parameter names, we can just as well trim it automatically."
0,"Add more methods to manipulate QueryNodeProcessorPipeline elements. QueryNodeProcessorPipeline allows the user to define a list of processors to process a query tree. However, it's not very flexible when the user wants to extend/modify an already created pipeline, because it only provides an add method, which only allows the user to append a new processor to the pipeline.

So, I propose to add new methods to manipulate the processor in a pipeline. I think the methods should not consider an index position when modifying the pipeline, hence the index position in a pipeline does not mean anything, a processor has a meaning when it's after or before another processor. Therefore, I suggest the methods should always consider another processor when inserting/modifying the pipeline. For example, insertAfter(processor, newProcessor), which will insert the ""newProcessor"" after the ""processor""."
0,"improve windows defaults in FSDirectory. Currently windows defaults to SimpleFSDirectory, but this is a problem due to the synchronization.

I have been benchmarking queries *sequentially* and was pretty surprised at how much faster
MMapDirectory is, for example for cases that do many seeks.

I think we should change the defaults for windows as such:

if (WINDOWS and UNMAP_SUPPORTED and 64-bit)
  use MMapDirectory
else
  use SimpleFSDirectory 

I think we should just consider doing this for 4.0 only and see how it goes.
"
0,"Cookie.compare(...) uses single instance STRING_COLLATOR to do blocking compares. I am using a MultiThreadedHttpConnectionManager with a single HttpClient instance and multiple GetMethod objects.  I have a 500 thread max.  I recently noticed that all 500 threads are in the same place and seem to be blocking each other - the stack trace is below.  I dug into the Cookie.compare(...) method and saw that it is using STRING_COLLARTOR.compare(c1.getPath(), c2.getPath()).  STRING_COLLATOR is defined as a single instance object, 'private static final RuleBasedCollator STRING_COLLATOR = (RuleBasedCollator) RuleBasedCollator.getInstance(new Locale(""en"", ""US"", """"));'.  I also saw that RuleBasedCollator.compare is synchronized.  That means that every thread that is trying to make a request is getting blocked while it tries to add cookies to the request method.  I do not see a workaround because this is the same static final object in every Cookie instance.  So, the more threads, the more synchronized comparisons.  At times I am fetching URLs all from the same site so I am going through this code a lot.  I need it to be much faster than it currently is because all of my threads are getting eaten up on this call and backlogging my system.  Can a different RuleBasedCollator be used for each compare (use the RuleBasedCollator.getInstance() for every compare?  I think that would solve things.

Name: pool-1-thread-1443: 72.21.206.5
State: BLOCKED on java.text.RuleBasedCollator@190330a owned by: pool-1-thread-1867: 72.21.206.5
Total blocked: 9,598  Total waited: 381

Stack trace: 
java.text.RuleBasedCollator.compare(RuleBasedCollator.java:396)
org.apache.commons.httpclient.Cookie.compare(Cookie.java:484)
org.apache.commons.httpclient.cookie.CookieSpecBase.addInPathOrder(CookieSpecBase.java:578)
org.apache.commons.httpclient.cookie.CookieSpecBase.match(CookieSpecBase.java:557)
org.apache.commons.httpclient.HttpMethodBase.addCookieRequestHeader(HttpMethodBase.java:1179)
org.apache.commons.httpclient.HttpMethodBase.addRequestHeaders(HttpMethodBase.java:1305)
org.apache.commons.httpclient.HttpMethodBase.writeRequestHeaders(HttpMethodBase.java:2036)
org.apache.commons.httpclient.HttpMethodBase.writeRequest(HttpMethodBase.java:1919)
org.apache.commons.httpclient.HttpMethodBase.execute(HttpMethodBase.java:993)
org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:397)
org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:170)
org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:396)
org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:324)"
0,"Redesign SPI observation. With the current SPI observation design it may happen that events are lost while the filter for an event listener is changed.

See:
- http://www.nabble.com/SPI-observation%3A-EventFilter-lifecycle-tf4732281.html
- http://people.apache.org/~mreutegg/spi-event/problem.png

My proposal is to introduce a Subscription interface. See attached patch and:
http://people.apache.org/~mreutegg/spi-event/proposal.png"
0,"FST should allow controlling how hard builder tries to share suffixes. Today we have a boolean option to the FST builder telling it whether
it should share suffixes.

If you turn this off, building is much faster, uses much less RAM, and
the resulting FST is a prefix trie.  But, the FST is larger than it
needs to be.  When it's on, the builder maintains a node hash holding
every node seen so far in the FST -- this uses up RAM and slows things
down.

On a dataset that Elmer (see java-user thread ""Autocompletion on large
index"" on Jul 6 2011) provided (thank you!), which is 1.32 M titles
avg 67.3 chars per title, building with suffix sharing on took 22.5
seconds, required 1.25 GB heap, and produced 91.6 MB FST.  With suffix
sharing off, it was 8.2 seconds, 450 MB heap and 129 MB FST.

I think we should allow this boolean to be shade-of-gray instead:
usually, how well suffixes can share is a function of how far they are
from the end of the string, so, by adding a tunable N to only share
when suffix length < N, we can let caller make reasonable tradeoffs. 
"
0,"Land DocValues on trunk. Its time to move another feature from branch to trunk. I want to start this process now while still a couple of issues remain on the branch. Currently I am down to a single nocommit (javadocs on DocValues.java) and a couple of testing TODOs (explicit multithreaded tests and unoptimized with deletions) but I think those are not worth separate issues so we can resolve them as we go. 
The already created issues (LUCENE-3075 and LUCENE-3074) should not block this process here IMO, we can fix them once we are on trunk. 

Here is a quick feature overview of what has been implemented:
 * DocValues implementations for Ints (based on PackedInts), Float 32 / 64, Bytes (fixed / variable size each in sorted, straight and deref variations)
 * Integration into Flex-API, Codec provides a PerDocConsumer->DocValuesConsumer (write) / PerDocValues->DocValues (read) 
 * By-Default enabled in all codecs except of PreFlex
 * Follows other flex-API patterns like non-segment reader throw UOE forcing MultiPerDocValues if on DirReader etc.
 * Integration into IndexWriter, FieldInfos etc.
 * Random-testing enabled via RandomIW - injecting random DocValues into documents
 * Basic checks in CheckIndex (which runs after each test)
 * FieldComparator for int and float variants (Sorting, currently directly integrated into SortField, this might go into a separate DocValuesSortField eventually)
 * Extended TestSort for DocValues
 * RAM-Resident random access API plus on-disk DocValuesEnum (currently only sequential access) -> Source.java / DocValuesEnum.java
 * Extensible Cache implementation for RAM-Resident DocValues (by-default loaded into RAM only once and freed once IR is closed) -> SourceCache.java
 
PS: Currently the RAM resident API is named Source (Source.java) which seems too generic. I think we should rename it into RamDocValues or something like that, suggestion welcome!   


Any comments, questions (rants :)) are very much appreciated."
0,cutover oal.index.* tests to use a random IWC to tease out bugs. 
0,"SimpleSelectionTest assumes RowIterator.getSize() not to return -1. Test case ""testSingleProperty"" assumes that RowIterator.getSize() will not return -1. This is an incorrect assumption, according to the JavaDoc for RangeIterator.

Suggested change:

        long size = result.getRows().getSize();
        if (size != -1) {
            assertEquals(""Should have only 1 result"", 1, size);
        }
"
0,"Clone support. It would be nice to have a clone method for some of the classes that don't have getters & setters exposed for all of their fields. Where relevant, the clone method could be in the interface, so that it doesn't matter which implementing class is being used. The main interfaces that I would like to clone are HttpRequest and Cookie. I know that HttpRequest is technically part of HttpCore, but the primary implementations of it are in HttpClient, so I thought I would post it here. 

Thanks,
David Byrne"
0,"MultiThreadedConnectionManager should provide a shutdown. MultiThreadedConnectionManager should provide a shutdown() method to release 
all its resources, it is currently using daemon threads that cannot be stopped 
and HTTP connections that cannot be released.
This is annoying when the pool of connection is created within a web 
application that is undeployed and re-deployed (i.e. the JVM is not restarted) 
consuming resources on local and remote servers."
0,"[PATCH] retain exception stack traces. Code catches one exception, and throws another, losing the stack trace information of the causal exception. This makes it more difficult to understand what happened when an exception occurs. This patch retains all stack traces that occur that causes a thrown exception."
0,"Automatic license header checking using the Apache Rat Plugin. To avoid problems with incorrect license headers, we should include some automated header check in the Maven build and have Hudson run the check whenever changes are committed."
0,"Cleanup use of EncodingUtil and HttpConstants. HttpConstants has become somewhat irrelevant.  Deprecate HttpConstants and move any existing 
functionality to EncodingUtil."
0,"RemoveVersionTest.testReferentialIntegrityException assumes availability of ref properties and same name sibilings. This test case:

- assumes availability of Reference properties (should throw NotExecutable when not available), and

- takes advantage if same name siblings (the child node identified by the nodename2 config variable has already been created by the test setup code)"
0,Provide base classes to simplify read only RepositoryService implementations. There should be base classes that simplify the implementation of a RepositoryService for read only access.
0,Remove (deprecated) ExtendedFieldCache and Auto/Custom caches and lot's of deprecated sort logic. Remove (deprecated) ExtendedFieldCache and Auto/Custom caches and sort
0,"davex remoting has  a performance bottleneck due limit of 2 http connections. The spi2dav service implementation use of HttpClient did not support configuration of the maximum amount of http connections to the server.  The default value, in the HttpClient code, is two. This was a performance bottleneck.  This work makes the number of connections configurable via a parameter to the map passed to the repository factory.  

It also fixes a concurrency issue which was exposed by the increased concurrency effected by this work.  This fix is a replacement of a HashMap cache of client connections with a ConcurrentHashMap, thanks to the java 1.5 available in Jackrabbit 2.x

USAGE: 
Set the number of connections (Spi2davRepositoryServiceFactory.PARAM_MAX_HTTP_CONNECTIONS) when creating a factory via the dav or davex rep factories.  Default is 20.

NOTE: 
See also the server side fixes: JCR-3027  The patch on that ticket allows configuration of the concurrency level on the server, which should be tuned in conjunction with the client side connection levels.  
 "
0,"David Spencer Spell Checker improved. hy,
i developed a SpellChecker based on the David Spencer code (DSc) but more flexible.
the structure of the index is inspired of the DSc (for a 3-4 gram):
word:
gram3:
gram4:
 
3start:
4start:
..
3end:
4end:
..
transposition:
 
This index is a dictonary so there isn't the ""freq"" field like with DSc version.
it's independant of the user index. So we can add words becoming to several
fields of several index for example or, why not, to a file with a list of words.
The suggestSimilar method return a list of suggests word sorted by the
Levenshtein distance and optionaly to the popularity of the word for a specific
field in a user index. More of that, this list can be restricted only to words
present in a specific field of a user index.
 
See the test case.
 
i hope this code will be put in the lucene sandbox. 
 
Nicolas Maisonneuve"
0,"Make IndexReader/Searcher ctors readOnly=true by default. Another ""change the defaults"" in 3.0.

Right now you get a read/write reader from IndexReader.open and new IndexSearcher(...), and reserving the right to write causes thread contention (on isDeleted).

In 3.0 let's make readOnly reader the default, but still allow opening a read/write IndexReader."
0,"Please add maven-notice-plugin to gump-trunk. Hi,

the httpclient build currently fails in Gump because it cannot find the maven-notice-plugin.  We could grab it from http://svn.apache.org/repos/asf/httpcomponents/maven-notice-plugin/trunk/ but since you have a directory with externals just for Gump it would be a lot easier if you added an external for it as well.

Thanks

Stefan"
0,"Performance of AC Evaluation. 1. Performance in access control evaluation
=====================================================================

 - main focus on
   > read performance
   > resource-based access control in .a.j.c/s/authorization/acl/*

 - comparison admin vs. anonymous with full permissions
 - comparision between shortcut and ACL-evaluation.
 - comparison JR 1.4 vs JR 2.0 [actually i will compare Day's CRX as it already provided
   some custom AC stuff with JR 1.4]


2. Potential Problems
=====================================================================

   I would expect the most significant problems to be found in

a) ACLProvider#retrieveResultEntries: calculating effective ACEs
     for each session separately.

b) AclPermission:
     Each instance registering an event listener in order to
     keep the result cache up to date

c) AclPermission:
     Resolution form Path to Item or to nearest existing Item "
0,"UUIDDocId should check IndexReader using equals(). The method UUIDDocId.getDocumentNumber(IndexReader) tests the passed index reader using its object identity.

This is a left over when there was one index per workspace and no system index. When the system index was introduced each query execution will create a new CombinedIndexReader covering the workspace index and the system index. The method should now use the equals() method to test the passed IndexReader."
0,"Jar manifest should not contain ${user.name} of the person building. Not sure if it is a big deal, but I don't particularly like that my user id for my build machine is in the manifest of the JAR that I constructed.  It's a stretch, security-wise, I know, but I don't see how it serves any useful purpose.  We have signatures/logs/SVN tags so we know who built the particular item w/o needing to know what their local user account name is.

The fix is:

{code}
Index: common-build.xml
===================================================================
--- common-build.xml    (revision 661027)
+++ common-build.xml    (working copy)
@@ -281,7 +281,7 @@
                <attribute name=""Implementation-Title"" value=""org.apache.lucene""/>
                <!-- impl version can be any string -->
                <attribute name=""Implementation-Version""
-                          value=""${version} ${svnversion} - ${user.name} - ${DSTAMP} ${TSTAMP}""/>
+                          value=""${version} ${svnversion} - ${DSTAMP} ${TSTAMP}""/>
                <attribute name=""Implementation-Vendor""
                           value=""The Apache Software Foundation""/>
                <attribute name=""X-Compile-Source-JDK"" 
{code} "
0,"Improve maven artifacts. There are a couple of things we can improve for the next release:
- ""*pom.xml"" files should be renamed to ""*pom.xml.template""
- artifacts ""lucene-parent"" should extend ""apache-parent""
- add source jars as artifacts
- update <generate-maven-artifacts> task to work with latest version of maven-ant-tasks.jar
- metadata filenames should not contain ""local"""
0,"Selective disabling of checks in ItemValidator. I would like to be able to selectively disable checks in ItemValidator in the scope of an operation performed through methods of the SessionState class. Doing so would provide simple means for internally modifying (for example) protected items. Currently such modifications must be done 'manually' on the item state level. This approach is very error prone and not very DRY.

With my upcoming patch in place, setting a protected property would look like this:

final Node parent = ...
final Value value = ...
SessionState sessionState = sessionContext.getSessionState();

Property property = sessionState.performUnchecked(new SessionOperation<Property>() {
    public Property perform(SessionContext context) throws RepositoryException {
        return parent.setProperty(""foo"", value);
    }
}, ItemValidator.CHECK_CONSTRAINTS);

That is, users need to have access to the session context in order to disable checks which makes this only usable from inside Jackrabbit. "
0,"Make open scoped locks recoverable. The lock tokens for open scoped locks are currently tied to the session which created the lock. If the session dies (for whatever reason) there is no way to recover the lock and unlock the node.
There is a theoretical way of adding the lock token to another session, but in most cases the lock token is not available.

Fortunately, the spec allows to relax this behaviour and I think it would make sense to allow all sessions from the same user to unlock the node - this is still in compliance with the spec but would make unlocked locked nodes possible in a programmatic way."
0,"Position Checking Span Queries. I've created a bunch of new SpanQuery classes that allow one to do things like check to see if a SpanQuery falls between two positions (which is a more general form of SpanFirstQuery) and I've also added one that only includes a match if the payload located at the span match also matches a given payload.  With the latter, one can do queries for items w/ specific payloads."
0,"Callback for intercepting merging segments in IndexWriter. For things like merging field caches or bitsets, it's useful to
know which segments were merged to create a new segment.

"
0,"Add reopen(IndexCommit) methods to IndexReader. Add reopen(IndexCommit) methods to IndexReader to be able to reopen an index on any previously saved commit points with all advantages of LUCENE-1483.

Similar to open(IndexCommit) & company available in 2.4.0.
"
0,"Add a NamespaceHelper in jcr-commons. We have a number of code snippets in jackrabbit-core and many JCR clients that do something like the following:

* get the prefix/URI for a given namespace URI/prefix without throwing an exception if the namespace is not found (return null instead)
* get a Map containing all current namespace prefix->URI mappings
* get the prefixed name for a given URI + local name pair in a given session (without a dependency to the SPI)
* safely register a given namespace (don't throw if the namespace is already registered, automatically select an unused prefix if needed, etc.)

I'd like to introduce a NamespaceHelper class in jcr-commons to cover such common code."
0,"Pass a context struct to Weight#scorer instead of naked booleans. Weight#scorer(AtomicReaderContext, boolean, boolean) is hard to extend if another boolean like ""needsScoring"" or similar flags / information need to be passed to Scorers. An immutable struct would make such an extension trivial / way easier. "
0,Remove QueryResultImpl and rename LazyQueryResultImpl to QueryResultImpl. QueryResultImpl isn't used in Jackrabbit anymore. Instead LazyQueryResultImpl is now used. See the discussion in JCR-1073.
0,"WordListLoader.java should be able to read stopwords from a Reader. WordListLoader should be able to read the stopwords from a Reader.

This would (for example) allow stopword lists to be stored as a resource in the
jar file of a Lucene application.

Diff is attached."
0,AbstractQueryTest does not handle unknown result size properly. If Node/RowIterator.getSize() returns -1 the method AbstractQueryTest.checkResult() should count the nodes in the iterator manually.
0,"TestIndexWriter.testOptimizeTempSpaceUsage fails w/ SimpleText codec. {noformat}
   [junit] Testsuite: org.apache.lucene.index.TestIndexWriter
   [junit] Testcase: testOptimizeTempSpaceUsage(org.apache.lucene.index.TestIndexWriter):      FAILED
   [junit] optimized used too much temporary space: starting usage was 117867 bytes; max temp usage was 363474 but should have been 353601 (= 3X starting usage)
   [junit] junit.framework.AssertionFailedError: optimized used too much temporary space: starting usage was 117867 bytes; max temp usage was 363474 but should have been 353601 (= 3X starting usage)
   [junit]     at org.apache.lucene.index.TestIndexWriter.testOptimizeTempSpaceUsage(TestIndexWriter.java:662)
   [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)
   [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)
   [junit]
   [junit]
   [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 5.284 sec
   [junit]
   [junit] ------------- Standard Output ---------------
   [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testOptimizeTempSpaceUsage -Dtests.seed=-3299990090561349208:2824386407253661541
   [junit] NOTE: test params are: codec=SimpleText, locale=el_GR, timezone=Africa/Dar_es_Salaam
{noformat}

It's not just SimpleText (because -Dtests.codec=SimpleText, alone, sometimes passes)... there must be something else about the RIWC settings.
"
0,"Provide Programmatic Access to CheckIndex. Would be nice to have programmatic access to the CheckIndex tool, so that it can be used in applications like Solr.  

See SOLR-566"
0,"Support system properties in ${...} vars in XML config files. The variable replacement (${...}) in config files like repository.xml currently only allows for the special variables introduced by Jackrabbit, eg. ${wsp.name} or ${rep.home}. But it would be useful to support all java system properties here as it is some kind of a standard in Java XML config files (see Spring for an example).

This makes it easier to inject variables from outside the config file, eg. by setting them on the command line or injecting them programmatically in test cases. Typical parameters for that include database connection credentials, which one wants to avoid to put into repository.xml files that are often checked into SVN.

This is especially true for test cases, eg. I currently work on a persistence manager component and I want to include the repository.xml in the source tree (under applications/test) but without my specific credentials. These are applied by loading a user-specific properties file through the test case before the repository is started and the config is read.
"
0,"[PATCH] Remove equals() from internal Comparator of ConjunctionScorer. As written, the equals() method is not used. 
The docs of java.util.Comparator have an equals() with a single 
arg to compare the Comparator itself to another one, which is 
hardly ever useful. 
Patch follows"
0,Error reporting page for jackrabbit-webapp. The Jackrabbit webapp should have an error reporting page that would help users collect all relevant environment and log information for inclusion in bug reports.
0,"BooleanQuery add public method that returns number of clauses this query. BooleanQuery add public method getClausesCount() that returns number of clauses this query.

current ways of getting clauses count are:
1).
 int clausesCount  = booleanQuery.getClauses().length;

or 

"
0,"Add OSGi bundle metadata to manifest. See this discussion on the Felix mailing list:
http://www.mail-archive.com/felix-dev@incubator.apache.org/msg04041.html

If there is an easy way to generate the information required for an OSGi bundle into the manifest of our distributable JAR, we should do it.
The runtime and API are not affected, it is only a modification of the build process.
I'll follow up on this when I have information about the required tooling.

cheers,
  Roland
"
0,"Move TrieRange to core. TrieRange was iterated many times and seems stable now (LUCENE-1470, LUCENE-1582, LUCENE-1602). There is lots of user interest, Solr added it to its default FieldTypes (SOLR-940) and if possible I want to move it to core before release of 2.9.
Before this can be done, there are some things to think about:
# There are now classes called LongTrieRangeQuery, IntTrieRangeQuery, how should they be called in core? I would suggest to leave it as it is. On the other hand, if this keeps our only numeric query implementation, we could call it LongRangeQuery, IntRangeQuery or NumericRangeQuery (see below, here are problems). Same for the TokenStreams and Filters.
# Maybe the pairs of classes for indexing and searching should be moved into one class: NumericTokenStream, NumericRangeQuery, NumericRangeFilter. The problem here: ctors must be able to pass int, long, double, float as range parameters. For the end user, mixing these 4 types in one class is hard to handle. If somebody forgets to add a L to a long, it suddenly instantiates a int version of range query, hitting no results and so on. Same with other types. Maybe accept java.lang.Number as parameter (because nullable for half-open bounds) and one enum for the type.
# TrieUtils move into o.a.l.util? or document or?
# Move TokenStreams into o.a.l.analysis, ShiftAttribute into o.a.l.analysis.tokenattributes? Somewhere else?
# If we rename the classes, should Solr stay with Trie (because there are different impls)?
# Maybe add a subclass of AbstractField, that automatically creates these TokenStreams and omits norms/tf per default for easier addition to Document instances?"
0,"Add ""direct"" PackedInts.Reader impl, that reads directly from disk on each get. Spinoff from LUCENE-3518.

If we had a direct PackedInts.Reader impl we could use that instead of
the RandomAccessReaderIterator.
"
0,Documentation - jackrabbit features beyond the spec. 
0,"Improper deprecation of Locked class. The Locked class in the jcr-commons package has been deprecated with 1.4 and moved to the spi-commons.
However as this is a common class which does not depend on the spi, it should rather stay in jcr-commons.
The dependencies to spi can simply be removed again."
0,"Explore streaming Viterbi search in Kuromoji. I've been playing with the idea of changing the Kuromoji viterbi
search to be 2 passes (intersect, backtrace) instead of 4 passes
(break into sentences, intersect, score, backtrace)... this is very
much a work in progress, so I'm just getting my current state up.
It's got tons of nocommits, doesn't properly handle the user dict nor
extended modes yet, etc.

One thing I'm playing with is to add a double backtrace for the long
compound tokens, ie, instead of penalizing these tokens so that
shorter tokens are picked, leave the scores unchanged but on backtrace
take that penalty and use it as a threshold for a 2nd best
segmentation...
"
0,"Add Highlighting benchmark support to contrib/benchmark. I would like to be able to test the performance (speed, initially) of the Highlighter in a standard way.  Patch to follow that adds the Highlighter as a dependency benchmark and adds in tasks extending the ReadTask to perform highlighting on retrieved documents."
0,"jcr:deref and parent axis in xpath predicates. Currently, the jcr:deref() function is not allowed in a xpath query predicate. Example :
book holds a reference property on its author(s)
authors have a name

We want all books from a specific author :

/jcr:root/element(*, bookType)[jcr:deref(@author, 'authorType')/@name = 'King']

This fails with an InvalidQueryException currently (not supported).

The error is raised in the XPathQueryBuilder class, in function : private QueryNode createFunction(SimpleNode node, QueryNode queryNode), in the block :
else if (NameFormat.format(JCR_DEREF, resolver).equals(fName))

Problem is that with this query, when evaluating the jcr:deref() function, then in this method at this point, queryNode.getType() is 0 and tests raise the exception if queryNode.getType() is neither QueryNode.TYPE_LOCATION nor QueryNode.TYPE_PATH.

I think this is a useful place to put a deref function in a query, as I don't know how we could test the referenced node properties another way.

Frederic Esnault"
0,"Make IndexReader.decRef() call refCount.decrementAndGet instead of getAndDecrement. IndexReader.decRef() has this code:

{code}
    final int rc = refCount.getAndDecrement();
    if (rc == 1) {
{code}

I think it will be clearer if it was written like this:

{code}
    final int rc = refCount.decrementAndGet();
    if (rc == 0) {
{code}

It's a very simple change, which makes reading the code (at least IMO) easier. Will post a patch shortly."
0,Improved READMEs for building sequence of trunk and jcr-server. 
0,"Two-stage state expansion for the FST: distance-from-root and child-count criteria.. In the current implementation FST states are expanded into a binary search on labels (from a vector of transitions) when the child count of a state exceeds a given predefined threshold (NUM_ARCS_FIXED_ARRAY). This threshold affects automaton size and traversal speed (as it turns out when benchmarked). For some degenerate (?) data sets, close-to-the-root nodes could have a small number of children (below the threshold) and yet be traversed on every single seek.

A fix of this is to introduce two control thresholds: 
  EXPAND state if (distance-from-root <= MIN_DISTANCE || children-count >= NUM_ARCS_FIXED_ARRAY)

My plan is to create a data set that will prove this first and then to implement the workaround above."
0,Add supported for Wikipedia English as a corpus in the benchmarker stuff. Add support for using Wikipedia for benchmarking.
0,"Provide BestMatch cookie policy. Presently HttpClient uses a single cookie policy for all target hosts, which is suboptimal for two reasons:
(1) the user needs to know beforehand what kind of HTTP cookie support the target host provides
(2) does not work well with multiple sites with different level of HTTP cookie support 

Introduce new cookie policy that dynamically picks up a CookieSpec (browser compatibility | Netscape draft | RFC2109 | RFC2965) based on properties of the response received from the target host"
0,"WebDAV: Allow for Extensions of MimeResolver in the Configuration.. Currently mime type detection is done using the content type header or (if missing) using a static MimeResolver instance in 
the IOUtil class. The MimeResolver itself reads from a properties file, that obviously does not list all possible extensions and
mimetypes.

This could be improved by:

- extending the resource configuration.
- extend the ImportContext and ExportContext interfaces
- replacing the current usages of IOUti#MIMERESOLVER by the corresponding calls on the Context classes which 
  themselves get a MimeResolver that is retrieved from the resource configuration."
0,"Remove jcr-commons dependency from jackrabbit-webdav. while looking at JCR-2896 i just happen to see that jackrabbit-webdav contains a dependency to jcr-commons.
this was never intended to be and i want to get rid of it again... the webdav library should not have any dependency to JCR."
0,Improve performance for queries with large result sets. The current implementation of QueryResult requires that access rights are checked on all NodeIds before they are passed ot the QueryResult. This handling should be improved to a more lazy approach where result nodes are checked in configurable batches. Usually a client is only interested in the top scoring results.
0,"removing properties through SPI: two ways to do it. Batch currently provides two ways to delete a property, similarly to JCR:

- Batch.remove()
- Batch.setValue(..., null)

JCR2SPI currently uses (AFAIK) Batch.remove().

Proposal:

- clarify that the QValue argument in setValue must be non-null (same for setValues)

"
0,"[HttpClient] Better proxy support in HttpMultiClient. If proxy requires authentication, it sends status 407 (Proxy Authentication 
Required) and the response header ""Proxy-Authenticate"" (see RFC2616; e.g. Squid 
can be configured to do so).
HttpClient doesn't yet process this response.
Behavior should be similar to processing of status 401 (Unauthorized)."
0,"improve performance of contrib/TestCompoundWordTokenFilter. contrib/analyzers/compound has some tests that use a hyphenation grammar file.

The tests are currently for german, and they actually are nice, they show how the combination of the hyphenation rules and dictionary work in tandem.
The issue is that the german grammar file is not apache licensed: http://offo.sourceforge.net/hyphenation/licenses.html
So the test must download the entire offo zip file from sourceforge to execute.

I happen to think the test is a great example of how this thing works (with a language where it matters), but we could consider using a different grammar file, for a language that is apache licensed.
This way it could be included in the source with the test and would be more practical.
"
0,"Binary value may leave temp file behind. The following call leaves a temp file behind that is never deleted:

InputStream in = ...
ValueFactory vf = ....
vf.createBinary(in).dispose();

Only happens when the datastore is disabled."
0,"DelimitedPayloadTokenFilter copies the bufer over itsself. Instead it should only set the length. Also optimize logic.. This is a small improvement I found when looking around. It is also a bad idea to copy a array over itsself.

All tests pass, will commit later!"
0,"Cleanup Test TokenStreams so they are reusable. Many TokenStreams created in tests are not reusable.  Some do some really messy things which prevent their reuse so we may have to change the tests themselves.

We'll target back porting this to 3x."
0,"smartcn HHMM doc translation. My coworker Patricia Peng translated the documentation and code comments for smartcn HHMM package.
"
0,"Site search powered by Lucene/Solr. For a number of years now, the Lucene community has been criticized for not eating our own ""dog food"" when it comes to search. My company has built and hosts a site search (http://www.lucidimagination.com/search) that is powered by Apache Solr and Lucene and we'd like to donate it's use to the Lucene community. Additionally, it allows one to search all of the Lucene content from a single place, including web, wiki, JIRA and mail archives. See also http://www.lucidimagination.com/search/document/bf22a570bf9385c7/search_on_lucene_apache_org

You can see it live on Mahout, Tika and Solr

Lucid has a fault tolerant setup with replication and fail over as well as monitoring services in place. We are committed to maintaining and expanding the search capabilities on the site.

The following patch adds a skin to the Forrest site that enables the Lucene site to search Lucene only content using Lucene/Solr. When a search is submitted, it automatically selects the Lucene facet such that only Lucene content is searched. From there, users can then narrow/broaden their search criteria.


I plan on committing in a 3 or 4 days."
0,"Uploade Lucene 2.1 to ibiblio. Please uploaded Lucene (specifically lucene-core) 2.1.0 to ibiblio. I see 2.0.0 but not 2.1.0.

Thanks!"
0,"jcr-server: add possibility to PROPFIND the JCR_NODETYPES_CND_LN property. patch by uwe jaeger see JCR-2454

in didn't include in the resolution of JCR-2454 since i would like to have JCR-2946 fixed as a prerequisite."
0,"contrib/benchmark build doesn't handle checking if content is properly extracted. The contrib/benchmark build does not properly handle checking to see if the content (such as Reuters coll.) is properly extracted.  It only checks to see if the directory exists.  Thus, it is possible that the directory gets created and the extraction fails.  Then, the next time it is run, it skips the extraction part and tries to continue on running the benchmark.

The workaround is to manually delete the extraction directory."
0,"Add support for the Ingres RDBMS. Hi Folks,
    I've put together all the stuff I can figure out that is required to add support for using the Ingres RBMS. I'll upload a svn diff for what I've done. It is against the 1.5.2 version from the tags repository.

    I was looking around but couldn't see if there was a way of running a test suite using Ingres as the DBMS provider. Is that possible with the current environment?

Cheers"
0,"MultipartPostMethod does not check for error messages. If a MultipartPost request is sent to a server which requires authentication, 
the server may respond to the request with an unauthorized header and close the 
connection before all of the data is sent.  HttpClient should monitor the 
incoming stream and cease transmitting the body if an error message is received 
(section 8.2.2 of rfc2616, see below).

At the very least HttpClient should check for a response when catching the 
HttpRecoverableException and retrying.  This probably should be done in 
HttpMethodBase so that we are in a known state when starting to retry the 
connection (ie: there isn't an existing response in the socket buffer to cause 
problems).

Ideally, HttpClient should also implement the 100 (Continue) status as 
specified in section 8.2.3 of rfc2616.

Finally, PostMethod should be tested to ensure that it does not exhibit this 
bug as well.

-------------
8.2.2 Monitoring Connections for Error Status Messages

   An HTTP/1.1 (or later) client sending a message-body SHOULD monitor
   the network connection for an error status while it is transmitting
   the request. If the client sees an error status, it SHOULD
   immediately cease transmitting the body. If the body is being sent
   using a ""chunked"" encoding (section 3.6), a zero length chunk and
   empty trailer MAY be used to prematurely mark the end of the message.
   If the body was preceded by a Content-Length header, the client MUST
   close the connection."
0,"Enhance test data. Running the test cases currently results in a number of test cases that cannot be run. Mostly because not enough test data is in the repository to execute the test.

You can find out the affected test cases by setting the log level for 'org.apache.jackrabbit.test' to DEBUG and grep for 'executable' in the jcr.log file.

This returns the following list:

testGetDeclaringNodeType(org.apache.jackrabbit.test.api.nodetype.PropertyDefTest) not executable
testIsMandatory(org.apache.jackrabbit.test.api.nodetype.NodeDefTest) not executable
testValueConstraintNotSatisfied(org.apache.jackrabbit.test.api.nodetype.CanSetPropertyBinaryTest) not executable
testValueConstraintNotSatisfied(org.apache.jackrabbit.test.api.nodetype.CanSetPropertyBooleanTest) not executable
testValueConstraintNotSatisfied(org.apache.jackrabbit.test.api.nodetype.CanSetPropertyDoubleTest) not executable
testGetSize(org.apache.jackrabbit.test.api.observation.EventIteratorTest) not executable
testGetAttribute(org.apache.jackrabbit.test.api.SessionReadMethodsTest) not executable
testReferenceableRootNode(org.apache.jackrabbit.test.api.ReferenceableRootNodesTest) not executable
testBinaryProperty(org.apache.jackrabbit.test.api.SetValueConstraintViolationExceptionTest) not executable
testBooleanProperty(org.apache.jackrabbit.test.api.SetValueConstraintViolationExceptionTest) not executable
testDoubleProperty(org.apache.jackrabbit.test.api.SetValueConstraintViolationExceptionTest) not executable
testReferenceProperty(org.apache.jackrabbit.test.api.SetValueConstraintViolationExceptionTest) not executable
testMultipleReferenceProperty(org.apache.jackrabbit.test.api.SetValueConstraintViolationExceptionTest) not executable
testBinaryProperty(org.apache.jackrabbit.test.api.SetPropertyConstraintViolationExceptionTest) not executable
testBooleanProperty(org.apache.jackrabbit.test.api.SetPropertyConstraintViolationExceptionTest) not executable
testDoubleProperty(org.apache.jackrabbit.test.api.SetPropertyConstraintViolationExceptionTest) not executable
testReferenceProperty(org.apache.jackrabbit.test.api.SetPropertyConstraintViolationExceptionTest) not executable

Test data in Jackrabbit should be enhanced in order to run those test cases successfully."
0,"port url+email tokenizer to standardtokenizerinterface (or similar). We should do this so that we can fix the LUCENE-3358 bug there, and preserve backwards.
We also want this mechanism anyway, for upgrading to new unicode versions in the future.

We can regenerate the new TLD list for 3.4 but, we should ensure the existing one is used for the urlemail33 or whatever,
so that its exactly the same."
0,"TCK: NodeMixinUtil should exclude for mix:shareable. The mixin test NodeRemoveMixinTest#testRemoveSuccessfully tries to remove a mixin:

what it does: retrieve an addable mixin (NodeTypeUtil#getAddableMixinName), adds it and tries to remove it later on.
the addable mixins are retrieve from the complete set of mixin, testing node.canAddMixin.

However: with jackrabbit-core ""mix:shareable"" can be added but not removed.
if the test by chance gets exactly that mixin the test fails with exception (there is an explicit check the core for exactly that case).

the tck should exclude that special case, shouldn't it?

((michael found the issue)) "
0,"CoordConstrainedBooleanQuery + QueryParser support. Attached 2 new classes:

1) CoordConstrainedBooleanQuery
A boolean query that only matches if a specified number of the contained clauses
match. An example use might be a query that returns a list of books where ANY 2
people from a list of people were co-authors, eg:
""Lucene In Action"" would match (""Erik Hatcher"" ""Otis Gospodneti&#263;"" ""Mark Harwood""
""Doug Cutting"") with a minRequiredOverlap of 2 because Otis and Erik wrote that.
The book ""Java Development with Ant"" would not match because only 1 element in
the list (Erik) was selected.

2) CustomQueryParserExample
A customised QueryParser that allows definition of
CoordConstrainedBooleanQueries. The solution (mis)uses fieldnames to pass
parameters to the custom query."
0,"Allow SnapshotDeletionPolicy to be reused across writer close/open. If you re-use the same instance of SnapshotDeletionPolicy across a
close/open of your writer, and you had a snapshot open, it can still
be removed when the 2nd writer is opened.  This is because SDP is
comparing IndexCommitPoint instances.

The fix is to instead compare segments file names.

I've also changed the inner class IndexFileDeleter.CommitPoint to be
static so an instance of SnapshotDeletionPolicy does not hold
references to IndexFileDeleter, DocumentsWriter, etc.

Spinoff from here:

  http://markmail.org/message/bojgqfgyxkkv4fyb
"
0,"Move core QueryParsers to queryparser module. Move the contents of lucene/src/java/org/apache/lucene/queryParser to the queryparser module.

To differentiate these parsers from the others, they are going to be placed a 'classic' package.  We'll rename QueryParser to ClassicQueryParser as well."
0,"Index sorter. A tool to sort index according to a float document weight. Documents with high weight are given low document numbers, which means that they will be first evaluated. When using a strategy of ""early termination"" of queries (see TimeLimitedCollector) such sorting significantly improves the quality of partial results.

(Originally this tool was created by Doug Cutting in Nutch, and used norms as document weights - thus the ordering was limited by the limited resolution of norms. This is a pure Lucene version of the tool, and it uses arbitrary floats from a specified stored field)."
0,"CachingHttpClient should have similar behavior as AbstractHttpClient when executing with ResponseHandler. When calling execute on the AbstractHttpClient with a  ResponseHandler, the AbstractHttpClient will attempt to Consume the Entity and close any open connections before returning. This behavior is not currently in the CachingHttpClient. 

This can lead to connection leaks when switching to CachingHttpClient, becuase the responsibility to fully consume the entity is now on the ResponseHandler instead on the HttpClient.

Here is the code that does the existing 'auto-close' behavior: ""org.apache.http.impl.client.AbstractHttpClient.java"" lines 1080-1111"
0,"make max size of CachingEntryCollector's cache configurable. To assist in analyzing the bottleneck it would be good if it was easy to change the max size, currently hard-wired to 5000. Suggest to be pragmatic and do that through a system property."
0,"refactor spatial contrib ""Filter"" ""Query"" classes. From erik's comments in LUCENE-1387

    * DistanceQuery is awkwardly named. It's not an (extends) Query.... it's a POJO with helpers. Maybe DistanceQueryFactory? (but it creates a Filter also)

    * CartesianPolyFilter is not a Filter (but CartesianShapeFilter is)
"
0,"Incorrect error message in AnalyzingQueryParser.getPrefixQuery. The error message of  getPrefixQuery is incorrect when tokens were added, for example by a stemmer. The message is ""token was consumed"" even if tokens were added.
Attached is a patch, which when applied gives a better description of what actually happened."
0,"Multiple PropertyDefs with same name not possible. when adding property defs with the same name but different settings, for example one singlevalued, one multivalued, only the last is respected when creating a property.

problem is the 'namedItemDefs' HashMap in the EffectiveNodeType which rather must contain a list of defs rather than the def itself.
"
0,"FastVectorHighlighter: some classes and members should be publicly accessible to implement FragmentsBuilder. I intended to design custom FragmentsBuilder can be written and pluggable, though, when I tried to write it out of the FVH package, it came out that some classes and members should be publicly accessible."
0,"Implement jcr-jackrabbit://... repository URIs. The current file://... URIs used by the Jackrabbit RepositoryFactoryImpl class make it hard to support extra ?parameters and prevent other JCR implementations from using similar repository URI patterns.

Thus I propose that we start supporting a jcr-jackrabbit://... URI pattern in addition to the current file://... pattern."
0,"Text Search Syntax Deviates from Spec. Original JSR 170 EG Email by David B Victor 2005/03/23:

For Query test XPathQueryLevel2Test.java (src\java\org\apache\jackrabbit\test\api\query) in the TCK, method getFullTextStatement() (used by testFullTextSearch()) uses the word ""AND"" in the syntax in its test that is not in the spec (/*[jcrfnContains(""'quick brown' AND -cat"")]...).  Section ""6.6.4.2 contains function"" of v0.16.3, page 100, outlines the EBNF, which does not include the word ""AND"".  Additionally, the paragraphs here go out of their way to explain that AND is implicit.

At this point, I think it would be best to omit ""AND"" from the TCK method and let it test the implicit AND.

------------------------------------------------------------
David Neuscheler Reply 2005/03/24:

thanks for pointing that out.

i think we should probably track all the tck bugs in jackrabbit jira.
http://issues.apache.org/jira/browse/JCR

could you open a bug for that?

this actually is because we used an non-spec compliant query
parser in the RI, so it actually is even a bug in the RI and the TCK.

thanks again.

regards,
david"
0,"Wrap IllegalArgumentException from UUID when bad ID passed to Session.getNodeByUUID. Hi,

On 6/30/06, David Kennedy <davek@us.ibm.com> wrote:
> When invoking session.getNodeByUUID and passing an invalid ID, an
> IllegalArgumentException is thrown.  Should this be wrapped in an
> ItemNotFoundException or RepositoryException by SessionImpl?

Good point, an ItemNotFoundException would probably be best. Could you
please file a Jira issue for this?

BR,

Jukka Zitting"
0,"MultiStatus response for PROPPATCH (copied from JCR-175). Rob Owen commented on JCR-175:
--------------------------------------------------

doPropPatch in AbstractWebdavServlet still needs to send back a multistatus (207) response even in the successful case.

I didn't see a way to collect the success/failure status for each property, but instead created a multistatus response and added a propstat (SC_OK) for each of the properties in the setProperties and removeProperties. This allowed a WebDAV client, which expected a multistatus response from PROPPATCH, to work correctly with jcr-server. In the more general case the actual property status code will need to be used ."
0,"Use the assembly plugin for packaging -tests jars. The spi and jcr2spi components have test code that currently packaged using the jar:test-jar goal and used as a test dependency by other components.

The extra jar plugin invocation creates extra Maven build numbers and causes problems for snapshot dependencies from the repository.apache.org repository that we recently started using. Using the assembly plugin to create the test jars should avoid this problem."
0,"Add sort-by-term with DocValues. There are two sorted byte[] types with DocValues (BYTES_VAR_SORTED,
BYTES_FIXED_SORTED), so you can index this type, but you can't yet
sort by it.

So I added a FieldComparator just like TermOrdValComparator, except it
pulls from the doc values instead.

There are some small diffs, eg with doc values there are never null
values (see LUCENE-3504).
"
0,"Consolidate type safe wrappers for commons-collection classes. Various places define their own type safe wrappers for classes from commons-collections (i.e. FilterIterator, TransformIterator and the like). I would like to consolidate them into one single place. "
0,AbstractQueryTest.evaluateResultOrder() should fail if workspace does not contain enough content. The method AbstractQueryTest.evaluateResultOrder() currently throws a NotExecutableException if the workspace contains less than two nodes that can be used for ordering. It should rather fail with an error message telling that the workspace does not contain sufficient content to run the test.
0,"Change defaultValues format in NodeTypes XML to jcr value. currently, the defaultValues serialization in the nodetypes.xml is the only one that uses internal value serialization, rather than the jcr string serialization.
eg:

<propertyDef name=""jcr:requiredPrimaryTypes"" ..... >
  <defaultValues>
    <defaultValue>{http://www.jcp.org/jcr/nt/1.0}base</defaultValue>
  </defaultValues>
</propertyDef>

this in not very handy, when the custom_nodetypes.xml should be written automatically.
i suggest to change the serialization to use the jcr value one:

<propertyDef name=""jcr:requiredPrimaryTypes"" ..... >
  <defaultValues>
    <defaultValue>nt:base</defaultValue>
  </defaultValues>
</propertyDef>
"
0,"maxDoc should be explicitly stored in the index, not derived from file length. This is a spinoff of LUCENE-140

In general we should rely on ""as little as possible"" from the file system.  Right now, maxDoc is derived by checking the file length of the FieldsReader index file (.fdx) which makes me nervous.  I think we should explicitly store it instead.

Note that there are no known cases where this is actually causing a problem. There was some speculation in the discussion of LUCENE-140 that it could be one of the possible, but in digging / discussion there were no specifically relevant JVM bugs found (yet!).  So this would be a defensive fix at this point."
0,"Static index pruning by in-document term frequency (Carmel pruning). This module provides tools to produce a subset of input indexes by removing postings data for those terms where their in-document frequency is below a specified threshold. The net effect of this processing is a much smaller index that for common types of queries returns nearly identical top-N results as compared with the original index, but with increased performance. 

Optionally, stored values and term vectors can also be removed. This functionality is largely independent, so it can be used without term pruning (when term freq. threshold is set to 1).

As the threshold value increases, the total size of the index decreases, search performance increases, and recall decreases (i.e. search quality deteriorates). NOTE: especially phrase recall deteriorates significantly at higher threshold values. 

Primary purpose of this class is to produce small first-tier indexes that fit completely in RAM, and store these indexes using IndexWriter.addIndexes(IndexReader[]). Usually the performance of this class will not be sufficient to use the resulting index view for on-the-fly pruning and searching. 

NOTE: If the input index is optimized (i.e. doesn't contain deletions) then the index produced via IndexWriter.addIndexes(IndexReader[]) will preserve internal document id-s so that they are in sync with the original index. This means that all other auxiliary information not necessary for first-tier processing, such as some stored fields, can also be removed, to be quickly retrieved on-demand from the original index using the same internal document id. 

Threshold values can be specified globally (for terms in all fields) using defaultThreshold parameter, and can be overriden using per-field or per-term values supplied in a thresholds map. Keys in this map are either field names, or terms in field:text format. The precedence of these values is the following: first a per-term threshold is used if present, then per-field threshold if present, and finally the default threshold.

A command-line tool (PruningTool) is provided for convenience. At this moment it doesn't support all functionality available through API."
0,Move PersistenceManagerTest from o.a.j.core to o.a.j.core.persistence. The subject pretty much sums it up. The PMTest should be placed together with the other PM related tests in o.a.j.core.persistence.
0,"Make FieldSortedHitQueue public. Currently, those who utilize the ""advanced"" search API cannot sort results using
the handy FieldSortedHitQueue. I suggest making this class public to facilitate
this use, as I can't think of a reason not to."
0,"Merge jcr-benchmark into the performance test suite. The jackrabbit-jcr-benchmark component currently lives in the JCR Commons area, but there have been no active plans to release the component and AFAIUI it's so far only been used for the performance test suite we set up in JCR-2695. To avoid the extra complexity of spreading the test code over multiple components and trunks, I'd like to merge the jcr-benchmark component back to Jackrabbit trunk into the performance test suite we have in tests/performance."
0,"Support for embedded index aggregates. Index aggregates could contain other index aggregates. JR should be able to handle a complete hierarchy of aggregates. 

I'm working on a patch."
0,Un-deprecate QueryParser and remove documentation that says it will be replaced in 3.0. This looks like the consensus move at first blush. We can (of course) re-evaluate if things change.
0,Rename package names. Rename package names  *graffito* into *jackrabbit*
0,"Add a ContextAwareAuthScheme that has access to the HttpContext in the authenticate method. The interface to be added would be:

/**
 * This interface represents an extended  authentication scheme
 * that requires access to {@link HttpContext} in order to
 * generate an authorization string.
 *
 * @since 4.1
 */

public interface ContextAwareAuthScheme extends AuthScheme {

    /**
     * Produces an authorization string for the given set of
     * {@link Credentials}.
     *
     * @param credentials The set of credentials to be used for athentication
     * @param request The request being authenticated
     * @param context HTTP context
     * @throws AuthenticationException if authorization string cannot
     *   be generated due to an authentication failure
     *
     * @return the authorization string
     */
    Header authenticate(
            Credentials credentials,
            HttpRequest request,
            HttpContext context) throws AuthenticationException;

}

Binary compatibility can be maintained by doing an instanceof check at the location where AuthScheme.authenticate() is called at the moment, and calling the context aware version if available.

This interface is necessary for the NegotiateScheme authentication scheme because the service names for the authentication tickets are based on the hostname of the target host or proxy host, depending on whether it's normal or proxy authentication, and this information is only available from the HttpContext.

Without the HttpContext there is a workaround that works most of the time, which looks like this:

	String host;
	if (isProxy()) {
		// FIXME this should actually taken from the HttpContext.
		HttpHost proxy = ConnRouteParams.getDefaultProxy(request.getParams());
		host = proxy.getHostName();
	} else {
		host = request.getLastHeader(""Host"").getValue();
	}

"
0,Incorrect link on Apache Jackrabbit Welcome homepage. Link to JSR283 in welcome text points to http://jcp.org/en/jsr/detail?id=170 instead of http://jcp.org/en/jsr/detail?id=283
0,"extensibility patch for simple WebDAV servlet. attaching a patch that makes the simple WebDAV servlet more extensible - subclasses can now provide their own support objects such as DavResourceFactory and DavLocatorFactory. also patches DavResourceImpl to make importXml and importFile methods protected, for subclass use.
"
0,"Lower log level in o.a.j.jcr2spi.query.NodeIteratorImpl. NodeIteratorImpl.fetchNext() logs an error when it cannot load a node and skips that node. Since this is not an error condition (the node could have been deleted by another session), logging should occur at the warn level."
0,"Coarser granularity of node type unregistration notifications. When unregistering multiple node types at a time, the internal notification methods are called separately for each type. This causes some problems as the first notifications triggers the regeneration of the full virtual node type tree, and later notification calls will fail (logging an error) in VirtualNodeTypeStateManager because the removed type is no longer there. A better approach would be to send the names of all the unregistered node types as a collection."
0,"Provide a clean mechanism to attatch user define attributes to connections. It would be nice to have a way to attach user defined attributes to a connection.
Ideally it'd be nice if such support could be added to HttpClientConnection, but understandably this may not be possible due to back-compatibility issues.
So, we could have something like HttpConnectionContext perhaps (or similar) with:

HttpConnectionContext#setAttribute(String name, Object value)
Object HttpConnectionContext#getAttribute(String name)

This would be made available in the HttpContext of a request (like the connection is today):

HttpConnectionContext connectionContext = (HttpConnectionContext) httpContext.getAttribute(ExecutionContext.HTTP_CONNECTION_CONTEXT);

This would make a few things much cleaner to implement than they are today: The most obvious being my current use case of wanting connection isolated cookies.

Currently to achieve this goal we need to provide custom client connection + connection operator + connection manager implementations. Then there is no clean way to currently obtain the actual connection instance created by a custom operator in the HttpContext: As it's wrapped by the connection pool and #getWrappedConnection is protected - so we need to resort to reflection in interceptors.

Providing a clean mechanism for attaching user defined attributes to a connection instance as described above would make such implementations far far simpler.
"
0,Parsing mixed inclusive/exclusive range queries. The current query parser doesn't handle parsing a range query (i.e. ConstantScoreRangeQuery) with mixed inclusive/exclusive bounds.
0,Keep jackrabbit jar/pom's updated at ibiblio. Please keep jackrabbit updated on ibiblio or some other maven repository.  jackrabbit-1.1 needs to be added.
0,"Allow means force a Repository to synchronize with the cluster. Based on the thread on the user mailing list I'm logging this to propose adding a sync() method to force cluster synchronization using the JackrabbitRepository extension API.

The purpose of the method is such that in a distributed clustered environment sometime cluster synchronization does or has not occurred such that certain repositories are in a stale state.  This method would provide a means to force a repository to update pull in possible changes made by other Jackrabbit repositories.

"
0,"Very inefficient implementation of MultiTermDocs.skipTo. In our application anytime the index was unoptimized/contained more than one segment there was a sharp drop in performance, which amounted to over 50ms per search on average.  We would consistently see this drop anytime an index went from an optimized state to an unoptimized state.

I tracked down the issue to the implementation of MultiTermDocs.skipTo function (found in MultiReader.java).  Optimized indexes do not use this class during search but unoptimized indexes do.  The comment on this function even explicitly states 'As yet unoptimized implementation.'  It was implemented just by calling 'next' over and over so even if it knew it could skip ahead hundreds of thousands of hits it would not.

So I re-implemented the function very similar to how the MultiTermDocs.next function was implemented and tested it out on or application for correctness and performance and it passed all our tests and the performance penalty of having multiple segments vanished.  We have already put the new jar onto our production machines.

Here is my implementation of skipTo, which closely mirrors the accepted implementation of 'next', please feel free to test it and commit it.

  /** Much more optimized implementation. Could be
   * optimized fairly easily to skip entire segments */
  public boolean skipTo(int target) throws IOException {
    if (current != null && current.skipTo(target-base)) {
      return true;
    } else if (pointer < readers.length) {
      base = starts[pointer];
      current = termDocs(pointer++);
      return skipTo(target);
    } else
      return false;
  }"
0,"Setup nightly build website links and docs. Per discussion on mailing list, we are going to setup a Nightly Build link on the website linking to the docs (and javadocs) generated by the nightly build process.  The build process may need to be modified to complete this task.

Going forward, the main website will, for the most part, only be updated per releases (I imagine exceptions will be made for News items and per committer's discretion).  The Javadocs linked to from the main website will always be for the latest release."
0,"Avoid element arrays in PathImpl. The path handling code in spi-commons shows quite often in thread dumps and profiling results, as the current implementation does quite a bit of repetitive allocating and copying of path element arrays. We should be able to streamline and simplify the path handling code by only tracking the latest path element and a reference to the parent path. To do this efficiently we may need to adjust some of the Path and PathFactory method declarations (that currently assume element array -based paths) also in the SPI.
"
0,"Testing for indexable properties should check the default indexable properties first. org.apache.jackrabbit.core.query.lucene.NodeIndexer#addValue, uses the following condition for a PropertyType.NAME type of property:

if (isIndexed(name)
                    || name.equals(NameConstants.JCR_PRIMARYTYPE)
                    || name.equals(NameConstants.JCR_MIXINTYPES)) {
                addNameValue(doc, fieldName, value.getQName());
}

It'd be more efficient to test the default properties first (which are on every node anyway) than to query the custom indexing rules every time. "
0,"another highlighter. I've written this highlighter for my project to support bi-gram token stream (general token stream (e.g. WhitespaceTokenizer) also supported. see test code in patch). The idea was inherited from my previous project with my colleague and LUCENE-644. This approach needs highlight fields to be TermVector.WITH_POSITIONS_OFFSETS, but is fast and can support N-grams. This depends on LUCENE-1448 to get refined term offsets.

usage:
{code:java}
TopDocs docs = searcher.search( query, 10 );
Highlighter h = new Highlighter();
FieldQuery fq = h.getFieldQuery( query );
for( ScoreDoc scoreDoc : docs.scoreDocs ){
  // fieldName=""content"", fragCharSize=100, numFragments=3
  String[] fragments = h.getBestFragments( fq, reader, scoreDoc.doc, ""content"", 100, 3 );
  if( fragments != null ){
    for( String fragment : fragments )
      System.out.println( fragment );
  }
}
{code}

features:
- fast for large docs
- supports not only whitespace-based token stream, but also ""fixed size"" N-gram (e.g. (2,2), not (1,3)) (can solve LUCENE-1489)
- supports PhraseQuery, phrase-unit highlighting with slops
{noformat}
q=""w1 w2""
<b>w1 w2</b>
---------------
q=""w1 w2""~1
<b>w1</b> w3 <b>w2</b> w3 <b>w1 w2</b>
{noformat}
- highlight fields need to be TermVector.WITH_POSITIONS_OFFSETS
- easy to apply patch due to independent package (contrib/highlighter2)
- uses Java 1.5
- looks query boost to score fragments (currently doesn't see idf, but it should be possible)
- pluggable FragListBuilder
- pluggable FragmentsBuilder

to do:
- term positions can be unnecessary when phraseHighlight==false
- collects performance numbers
"
0,HttpState should have methods for clearing all cookies and credentials.  
0,"TokenFilter should implement reset(). TokenFilter maintains a private member of TokenStream.
It should implement reset() and call its member TokenStream's reset() method. Otherwise, that TokenStream never gets reset.
Patch applied."
0,"Deprecate BLOBStore (use the DataStore instead). I believe the blob store should be deprecated in favor of the data store (in the source code, and in the documentation). The blob store should still be supported in version 2.x of course."
0,"Move MutableValues to Common Module. Solr makes use of the MutableValue* series of classes to improve performance of grouping by FunctionQuery (I think).  As such they are used in ValueSource implementations.  Consequently we need to move these classes in order to move the ValueSources.

As Yonik pointed out, these classes have use beyond just FunctionQuerys and might be used by both Solr and other modules.  However I don't think they belong in Lucene core, since they aren't really related to search functionality.  Therefore I think we should put them into a Common module, which can serve as a dependency to Solr and any module."
0,"[PATCH] jackrabbit-webapp pom.xml patch to create an additional jar artifact. Modifies the jackrabbit-webapp pom.xml to create a jar artifact in addition to the existing war artifact, to allow the jackrabbit-webapp utility servlets to be reused in other modules.

The right way would be to create a separate jar module for the servlets (or move them to jackrabbit-jcr-commons?), and reuse that jar as a dependency in the jackrabbit-webapp. So I'm not sure if this patch deserves to be applied to the trunk, but it can be useful as a workaround before a cleaner solution is implemented.

See also http://mail-archives.apache.org/mod_mbox/jackrabbit-dev/200705.mbox/%3C510143ac0705151453t7a0eb4cam859a40fb106e81f5@mail.gmail.com%3E which discusses possible improvements to these jackrabbit-webapp utility servlets.

"
0,"Wrong exeption returned from Repository.login(Credentials, String). According to specification, calling Repository.login(Credentials, String) with a non-existent wokspaceName should return NoSuchWorkspaceException.

In fact it returns RepositoryException."
0,SQL2: Implement LIKE support for node names. Doing a LIKE constraint on the local name of a node that throws javax.jcr.UnsupportedRepositoryOperationException.
0,"PersistentVersionManager contains grow-only cache. The PersistentVersionManager class contains a private HashMap ""histories"" which contains references to InternalVersionHistory objects. The bad thing about this cache is, that it only grows, but is not being managed to forget about ""unused"" histories. This is even badder, as the class has support for on-demand loading of version histories.

Example: A repository which is filled with 9350 nodes and 52813 properties grows this histories map to 1'222'030 (!) entries. In this concrete case, the VM allocates 213MB to the heap of which 41MB is referenced by the PersistentVersionManager.histories map."
0,Change NumericRangeQuery to generics (for the numeric type). NumericRangeQuery/Filter can use generics for more type-safety: NumericRangeQuery<T extends Number>
0,Javadocs clean-up. Before Httpclient can be released Javadocs need to be updated and proof-read
0,"allow case insensitive searches. would be nice to be able to search specific properties like a fulltext search, e.g. with an ignore-case flag, so you could find a subset of the results of

  select * from nt:base where contains('bla')

using something like

  select * from nt:base where jcr:bla like '%bla%'

(currently, the value must contain 'bla' exactly as it is to be found by the second query)

i suggest to extend the contains function with an additional argument for the property to search in, e.g.

  select * from nt:base where contains('bla',jcr:bla)

this could then also easily be used in XPath.
"
0,"Indonesian Analyzer. This is an implementation of http://www.illc.uva.nl/Publications/ResearchReports/MoL-2003-02.text.pdf

The only change is that I added an option to disable derivational stemming, 
in case you want to just remove inflectional particles and possessive pronouns.

"
0,"Unnecessary null check in EffectiveNodeType.getApplicableChildNodeDef(). This is just a trivial thing I noticed this while inspecting the code. getApplicableChildNodeDef() says:

        // try named node definitions first
        ItemDef[] defs = getNamedItemDefs(name);
        if (defs != null) {

but getNamedItemDefs() is currently defined to not return null:

    public ItemDef[] getNamedItemDefs(Name name) {
        List defs = (List) namedItemDefs.get(name);
        if (defs == null || defs.size() == 0) {
            return ItemDef.EMPTY_ARRAY;
        }
        return (ItemDef[]) defs.toArray(new ItemDef[defs.size()]);
    }

I didn't check to see if there were any other unnecessary null checks."
0,Can't specify AttributeSource for Tokenizer. One can't currently specify the attribute source for a Tokenizer like one can with any other TokenStream.
0,The DisjunctionMaxQuery lacks an implementation of extractTerms().. The DisjunctionMaxQuery lacks an implementation of extractTerms(). 
0,"Jcr-Server: Avoid xml parsing if request body is missing. Originally reported by Brian.

"
0,"[PATCH] Use filter bits for next() and skipTo() in FilteredQuery. This improves performance of FilteredQuery by not calling score() 
on documents that do not pass the filter. 
This passes the current tests for FilteredQuery, but these tests 
have not been adapted/extended."
0,"default behaviour of useExpectHeader. I suggest to set the ExpectContinueMethod.setUseExpectHeader per default to
false or to arrange that per default it is not used. We lost an awfull lot of
time in a project in which we used MultipartPostMethod via a proxy. Everthing
worked fine in dev, however as soon as we started to use the proxy in production
or testing environment we had severe problems. We lost several manday looking
for the problem, including sniffing and logging op the proxy. It ended up to be
the useexpectheader which was true per default. Putting in on false ended our
problems...

In my opinion it is a bit hard to make something default behaviour if the
javadoc warns : <snip>
handshake should be used with caution, as it may cause problems with HTTP
servers and proxies that do not support HTTP/1.1 protocol.
</snip>

regards
dirkp"
0,"Skip deployment of jackrabbit-standalone. The jackrabbit-standalone jar currently can't be deployed to the repository.apache.org server probably because of its size. I'm not sure if there are any good use cases where you'd want to use the standalone jar as a Maven dependency, so having it on Maven central doesn't seem that important. I'd like to make this explicit by configuring the deploy plugin to skip deploying the standalone jar."
0,"All Analysis Consumers should use reusableTokenStream. With Analyzer now using TokenStreamComponents, theres no reason for Analysis consumers to use tokenStream() (it just gives bad performance).  Consequently all consumers will be moved over to using reusableTokenStream().  The only challenge here is that reusableTokenStream throws an IOException which many consumers are not rigged to deal with.

Once all consumers have been moved, we can rename reusableTokenStream() back to tokenStream()."
0,"Enhance QueryUtils and CheckHIts to wrap everything they check in MultiReader/MultiSearcher. methods in CheckHits & QueryUtils are in a good position to take any Searcher they are given and not only test it, but also test MultiReader & MultiSearcher constructs built around them"
0,"NTLM class registers Sun JCE implementation by default. Currently the NTLM class attempts to load and register the Sun JCE implementation unless a 
System property is set to indicate a different JCE to use.  We should remove this entirely and leave 
the installation and configuration of the JCE to the application rather than trying to do it ourselves 
as this could cause problems with other implementations of JCE.  I'll attach an initial patch for this 
in a moment, with a patch for the documentation in the morning.  (Writing docs at 1am is never a 
good idea.)"
0,"remove dead code from oal.util.cache. We have dead cache impls in oal.util.cache*; we only use DBLRUCache.

These are internal APIs; I'd like to remove all but DBLRUcache."
0,Add multi-part post support. Add a new method to support multi-part post.
0,JSR 283 Evaluate Capabilities. 
0,Only load item definition when required. Some item definitions are loaded when an item state is constructed. Whenever possible this should be delayed to a time when the definition is actually used.
0,"FieldCacheImpl cache gets rebuilt every time. FieldCacheImpl uses WeakHashMap to store the cached objects, but since 
there is no other reference to this cache it is getting released every time."
0,"NodeTypeRegistry.unregisterNodeTypes(Collection) missing. when having nodetypes depending on each other, or even have cyclic dependencies, they must be unregistered in the correct sequence. 

i suggest a NodeTypeRegistry.unregisterNodeTypes(Collection) for that case."
0,"Exception may get lost in WorkspaceManager.OperationVisitorImpl.execute(). The method calls Batch.submit() in the finally block. If both the try and finally block throw exceptions, the one from the try block is ignored."
0,"Create IndexWriterConfiguration and store all of IW configuration there. I would like to factor out of all IW configuration parameters into a single configuration class, which I propose to name IndexWriterConfiguration (or IndexWriterConfig). I want to store there almost everything besides the Directory, and to reduce all the ctors down to one: IndexWriter(Directory, IndexWriterConfiguration). What I was thinking of storing there are the following parameters:
* All of ctors parameters, except for Directory.
* The different setters where it makes sense. For example I still think infoStream should be set on IW directly.

I'm thinking that IWC should expose everything in a setter/getter methods, and defaults to whatever IW defaults today. Except for Analyzer which will need to be defined in the ctor of IWC and won't have a setter.

I am not sure why MaxFieldLength is required in all IW ctors, yet IW declares a DEFAULT (which is an int and not MaxFieldLength). Do we still think that 10000 should be the default? Why not default to UNLIMITED and otherwise let the application decide what LIMITED means for it? I would like to make MFL optional on IWC and default to something, and I hope that default will be UNLIMITED. We can document that on IWC, so that if anyone chooses to move to the new API, he should be aware of that ...

I plan to deprecate all the ctors and getters/setters and replace them by:
* One ctor as described above
* getIndexWriterConfiguration, or simply getConfig, which can then be queried for the setting of interest.
* About the setters, I think maybe we can just introduce a setConfig method which will override everything that is overridable today, except for Analyzer. So someone could do iw.getConfig().setSomething(); iw.setConfig(newConfig);
** The setters on IWC can return an IWC to allow chaining set calls ... so the above will turn into iw.setConfig(iw.getConfig().setSomething1().setSomething2()); 

BTW, this is needed for Parallel Indexing (see LUCENE-1879), but I think it will greatly simplify IW's API.

I'll start to work on a patch."
0,Config: make all elements in the security configuration optional. in order not to introduce new mandatory elements in the security configuration.
0,Remove deprecated SpanQuery.getTerms() and generify Query.extractTerms(Set<Term>). 
0,"unexpected session is used  in XATest.testAddNodeCommit(). In org.apache.jackrabbit.core.XATest.java:137

        // assertion: node exists in this session
        try {
            otherSuperuser.getNodeByUUID(n.getUUID());
        } catch (ItemNotFoundException e) {
            fail(""Committed node not visible in this session"");
        }

        // assertion: node also exists in other session
        try {
            otherSuperuser.getNodeByUUID(n.getUUID());
        } catch (ItemNotFoundException e) {
            fail(""Committed node not visible in other session"");
        }

The session instance of 'otherSuperuser' is used two times. In the first case, I think that it is not 'otherSuperuser' but 'superuser'. 
"
0,Test must not fail if mixin cannot be added. Nearly all mixin types are optional and a test must not fail if a mixin cannot be added. It should rather throw a NotExecutableException. Some tests still try to add a mixin without checking whether an implementation supports it.
0,"More details for beginners. Hi everyone,

I'm one of these beginners trying to make their way in Jackrabbit and content repositories universe.

Besides the fact that there exist but very few examples on Jackrabbit use, all of them, including the official web site of Jackrabbit miss few details, simple though essential, for beginners.

My first point is the following: once Jackrabbit source is checked out and built, time to test it using simple examples. But, where to put the example directory from the beginning. How to run it (maven java:compile....) ?

A second point is the fact that there is no help forum on the Jackrabbit web site.

 A third point, taking my case as an example, the example would just not create a new workspace configuration.
And many other troubleshoots - I repeat - basic but essential, that could avoid dozens of wasted hours and discouragment, if just mentioned on the website.

Here it is my wish :-) for the best of Jackrabbit ;-) I hope!
Regards,
Celina"
0,"Multiple DIGEST authentication attempts with same credentials. HttpMethodBase's processAuthenticationResponse uses a set of realms to which
attempts to authenticate have already been made. The elements of the set are a
concatenation of the requested path and the value of the Authentication response
header.

For digest authentication this response header contains a nonce value, which is
uniquely generated by the server each time a 401 response is made. This makes it
impossible to recognize that authentication against this realm has been
attempted before and so all 100 attempts are made before returning. The nonce
should probably not be used in the realmsUsed element

Reported by Rob Owen <Rob.Owen@sas.com>"
0,"Cache also failed principal lookups. The principal cache in Jackrabbit normally does a good job in ensuring good performance in critical areas like ACL evaulation. However, the cache only includes successful principal lookups, so an ACE that references a missing (or mistyped) principal can cause notable performance issues as a new principal lookup is needed whenever the node covered by such an ACL is accessed.

To solve that problem I propose that we extend the principal cache to also cover negative principal lookups."
0,"application-defined routes. Allow applications to specifiy a route as request parameter (or in the context).
This functionality is a replacement for RoutedRequest, which is removed by HTTPCLIENT-715."
0,JCR-RMI depends on commons-logging. The commons-logging dependency in JCR-RMI should be removed.
0,"Fix StandardAnalyzer to not mis-identify HOST as ACRONYM by default. Coming out of the discussion around back compatibility, it seems best to default StandardAnalyzer to properly fix LUCENE-1068, while preserving the ability to get the back-compatible behavior in the rare event that it's desired.

This just means changing the replaceInvalidAcronym = false with = true, and, adding a clear entry to CHANGES.txt that this very slight non back compatible change took place.

Spinoff from here:

    http://www.gossamer-threads.com/lists/lucene/java-dev/57517#57517

I'll commit that change in a day or two."
0,"Observation tests should throw NotExecutableException when repository does not support observation. The observation tests should throw NotExecutableException when repository does not support observation.
"
0,"High memory usage on node with multi-valued string properties. Multi-valued string properties are tokenized per value, which may consume quite some memory when there are lots of small values in on a property. The memory footprint is 2k per value, because each value is tokenized with a separate tokenizer instance. That tokenizer uses a stream buffer of 2k bytes.

Instead the values should be concatenated (whitespace separated) and then tokenized in one go."
0,"Extend Codec to handle also stored fields and term vectors. Currently Codec API handles only writing/reading of term-related data, while stored fields data and term frequency vector data writing/reading is handled elsewhere.

I propose to extend the Codec API to handle this data as well."
0,"Some tests assume that an implementation of javax.jcr.Item overrides equals(). The following 3 tests (followed by the line number containing the bad assertion):

org.apache.jackrabbit.test.api.ReferencesTest.testReferenceTarget:135
org.apache.jackrabbit.test.api.ReferencesTest.testAlterReference:169
org.apache.jackrabbit.test.api.version.VersionHistoryTest:152

assume that an implementation of javax.jcr.Item overrides equals(), such that 

Assert.assertEquals(n1, n2) or 
java.util.Set.contains(n1) 

works for two ""equal"" nodes n1,n2 or for some node n1 that has been previously put into a set. However, there is no section in the specification that would mandate this. The tests above should therefore replace assertEquals() with one of the other mechanism that officially supported, such as javax.jcr.Node.isSame().

"
0,"Switch from log4j to Logback. Logback (http://logback.qos.ch/) is a native SLF4J implementation and is in many ways superior to log4j (see http://logback.qos.ch/reasonsToSwitch.html). Most notably it includes package version information in logged stack traces (http://logback.qos.ch/reasonsToSwitch.html#packagingData), which can be really useful in many cases."
0,"""ant dist"" no longer generates md5's for the top-level artifacts. Mark hit this for 2.9.0, and I just hit it again for 2.9.1.  It used to work..."
0,Land DWPT on trunk. With LUCENE-2956 we have resolved the last remaining issue for LUCENE-2324 so we can proceed landing the DWPT development on trunk soon. I think one of the bigger issues here is to make sure that all JavaDocs for IW etc. are still correct though. I will start going through that first.
0,"Typo in PropertyDefinitionTemplate. setValueConstarints should read setValueConstraints


"
0,"Single-pass grouping collector based on doc blocks. LUCENE-3112 enables adding/updating a contiguous block of documents to
the index, guaranteed (yet, experimental!) to retain adjacent docID
assignment through the full life of the index as long the app doesn't
delete individual docs from the block.

When an app does this, it can enable neat features like LUCENE-2454
(nested documents), post-group facet counting (LUCENE-3097).

It also makes single-pass grouping possible, when you group by
the ""identifier"" field shared by the doc block, since we know we will
see a given group only once with all of its docs within one block.

This should be faster than the fully general two-pass collectors we
already have.

I'm working on a patch but not quite there yet...
"
0,"[PATCH] refactor access to SAXParser and log which parser class is used. This parser collects code that was duplicated in the SessionImpl and WorkspaceImpl class to initialize the SAXParser.

Also, the actual SAXParser class being used is logged (once only) to make it easier to debug problems like JCR-984."
0,"Enhance indexing of binary content. Indexing of binary content should be enhanced in order to allow either configuration what fields are indexed or provide better support for custom NodeIndexer implementations.

The current design has a couple of flaws that should be addressed at the same time:
- Reader instances are requested from the text filters even though the reader might never be used
- only jcr:data properties of nt:resource nodes are fulltext indexed
- It is up to the text filter implementation to decide the lucene field name for the text representation, responsibility should be moved to the NodeIndexer. A text filter should only provide a Reader instance.

With those changes a custom NodeIndexer can then decide if a binary property has one or more representations in the index."
0,"Add test for Node.restore() may throw InvalidStateException. Add a unit test for JCR-1399 in the 1.3 branch.

A test for the original feature in the trunk/1.4 (JCR-1197) needs a separate issue. "
0,"lucenetestcase ease of use improvements. I started working on this in LUCENE-2658, here is the finished patch.

There are some problems with LuceneTestCase:
* a tests beforeClass, or the test itself (its @befores and its method), might have some
  random behavior, but only the latter can be reproduced with -Dtests.seed
* if you want to do things in beforeClass, you have to use a different API: newDirectory(random)
  instead of newDirectory, etc.
* for a new user, the current output can be verbose, confusing and overwhelming.

So, I refactored this class to address these problems. 
A class still needs 2 seeds internally, as the beforeClass will only run once, 
but the methods or setUp() might run many times, especially when increasing iterations.

but lucenetestcase deals with this, and the ""seed"" is 128-bit (UUID): 
the MSB is initialized in beforeClass, the LSB varied for each method run.
if you provide a seed with a -D, they are both fixed to the UUID you provided.

I fixed the API to be consistent, so you should be able to migrate a test from 
setUp() to beforeClass() [junit3 to junit4] without changing parameters.

The codec, locale, timezone is only printed once at the end if any tests fail, 
as its per-class anyway (setup in beforeClass)

finally, when a test fails, you get a single ""reproduce with"" command line you can copy and paste to reproduce.
this way you dont have to spend time trying to figure out what the command line should be.

{noformat}
    [junit] Tests run: 2, Failures: 2, Errors: 0, Time elapsed: 0.197 sec
    [junit]
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestExample -Dtestmethod=testMethodA 
              -Dtests.seed=a51e707b-6550-7800-9f8c-72622d14bf5f
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestExample -Dtestmethod=testMethodB 
              -Dtests.seed=a51e707b-6550-7800-f7eb-2efca3820738
    [junit] NOTE: test params are: codec=PreFlex, locale=ar_LY, timezone=Etc/UCT
    [junit] ------------- ---------------- ---------------
    [junit] Test org.apache.lucene.util.TestExample FAILED
{noformat}
"
0,"TestThreadSafety.testLazyLoadThreadSafety test failure. TestThreadSafety.testLazyLoadThreadSafety failed with this error:

unable to create new native thread

Maybe because of SimpleText

Here is the stacktrace:
{noformat}
 [junit] Testsuite: org.apache.lucene.search.TestThreadSafe
    [junit] Testcase: testLazyLoadThreadSafety(org.apache.lucene.search.TestThreadSafe):	Caused an ERROR
    [junit] unable to create new native thread
    [junit] java.lang.OutOfMemoryError: unable to create new native thread
    [junit] 	at java.lang.Thread.start0(Native Method)
    [junit] 	at java.lang.Thread.start(Thread.java:614)
    [junit] 	at org.apache.lucene.search.TestThreadSafe.doTest(TestThreadSafe.java:129)
    [junit] 	at org.apache.lucene.search.TestThreadSafe.testLazyLoadThreadSafety(TestThreadSafe.java:148)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)
    [junit] 
    [junit] 
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 6.051 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestThreadSafe -Dtestmethod=testLazyLoadThreadSafety -Dtests.seed=-277698010445513699:-89599297372877779
    [junit] NOTE: test params are: codec=SimpleText, locale=zh_SG, timezone=Pacific/Tongatapu
    [junit] ------------- ---------------- ---------------
    [junit] TEST org.apache.lucene.search.TestThreadSafe FAILED
{noformat}"
0,"Improve handling for missing text filter dependency. Using a LazyReader in a TextFilter implementation will not always throw a NoClassDefFoundError if a depending jar file is missing.

The text filter implementations should therefore include a static block that forces an initialization of a depending class."
0,"New PostSOAP example (for src/examples). I have a slightly modified version of PostXML which invokes SOAP requests. The
only difference to PostXML is that PostSOAP takes the SOAPAction as an extra
commndline arg and adds that as a header into the request."
0,"Stored-only fields automatically enable norms and tf when added to document. During updating my internal components to the new TrieAPI, I have seen the following:

I index a lot of numeric fields with trie encoding omitting norms and term frequency. This works great. Luke shows that both is omitted.

As I sometimes also want to have the components of the field stored and want to use the same field name for it. So I add additionally the field again to the document, but stored only (as the Field c'tor using a TokenStream cannot additionally store the field). As it is stored only, I thought, that I can left out explicit setting of omitNorms and omitTermFreqAndPositions. After adding the stored-only-without-omits field, Luke shows all fields with norms enabled. I am not sure, if the norms/tf were really added to the index, but Luke shows a value for the norms and FieldInfo has it enabled.

In my opinion, this is not intuitive, o.a.l.document.Field  should switch both omit* options on when storing fields only (and also disable other indexing-only options). Alternatively the internal FieldInfo.update(boolean isIndexed, boolean storeTermVector, boolean storePositionWithTermVector, boolean storeOffsetWithTermVector, boolean omitNorms, boolean storePayloads, boolean omitTermFreqAndPositions) should only change the omit* and other options, if the isIndexed parameter (not this.isIndexed) is also true, elsewhere leave it as it is.

In principle, when adding a stored-only field, any indexing-specific options should not be changed in FieldInfo. If the field was indexed with norms before, norms should stay enabled (but this would be the default as it is)."
0,"download section of website down. maybe you know this, but http://hc.apache.org/downloads.cgi (the downloads for httpcomponents and for httpclient) is broken (500 internal server error)..."
0,"CMS should default its maxThreadCount to 1 (not 3). From rough experience, I think the current default of 3 is too large.  I think we get the most bang for the buck going from 0 to 1.

I think this will especially impact optimize on an index with many segments -- in this case the MergePolicy happily exposes concurrency (multiple pending merges), and CMS will happily launch 3 threads to carry that out."
0,"Remove verbosity from tests and make configureable. The parent issue added the functionality to LuceneTestCase(J4), this patch applies it to most tests."
0,"TCK: LockTest.testGetLock compares Nodes with equals. i think comparison by 'isSame' would be better.

-> line 545

-        assertTrue(""lock holding node must be parent"", lock.getNode().equals(n1));
+        assertTrue(""lock holding node must be parent"", lock.getNode().isSame(n1));"
0,"ms-sql tablespace support for FileSystem and PersistenceManager. Trunk and released version 1.5.0 does not have complete support for ms-sqlserver tablespaces.  This patch was originally submitted via JCR-1295 but was not applied to the 1.4 trunk.

"
0,"Spatial Filters not Serializable. I am using Lucene in a distributed setup. 

The Filters in the spatial project aren't Serializable even though it inherits it from Filter. Filter is a Serializable class. 

DistanceFilter contains the non-Serializable class WeakHashMap.
CartesianShapeFilter contains the non-Serializable class java.util.logging.Logger
"
0,Upgrade nekohtml dependency. The latest CyberNeko HTML parser versions are ALv2-licensed and have better Maven dependency metadata.
0,Run TCK on Jackrabbit 1.0-rc3. Run TCK on Jackrabbit 1.0-rc3
0,"Add m2e lifecycle mappings for Eclipse Indigo. When importing Jackrabbit to the latest Eclipse release (Indigo) that comes with m2e version 1.0, many of the POMs are flagged red because m2e doesn't know what to do with the custom plugin mappings we use in many components.

This is a pretty contentious issue for m2e (see for example https://bugs.eclipse.org/bugs/show_bug.cgi?id=350414), and ideally it should just work without any custom workarounds on our part.

However, as a workaround until the core issue is solved, the best solution is to explicitly tell m2e what to do with these plugin mappings. The extra org.eclipse.m2e:lifecycle-mapping configuration is only active when used within Eclipse, so it doesn't affect
normal builds."
0,"Benchmark's ContentSource should not rely on file suffixes to be lower cased when detecting file type (gzip/bzip2/text). file.gz is correctly handled as gzip, but file.GZ handled as text which is wrong.
"
0,"junit dependency in pom.xml with default compile scope. The dependency set as defined in:
http://www.ibiblio.org/maven2/commons-httpclient/commons-httpclient/3.0.1/commons-httpclient-3.0.1.pom
includes junit, but not with a <scope>test</test>. I suppose the junit dependency should have a test scope. Could someone fix this? Because of this my application is packaged including junit 3.8.1, which adds 118KiB for nothing.
"
0,ConfigurationException constructors are package private. ConfigurationException constructors are package private which prevents reusing them in other packages. eg. when extending the configuration.
0,JSR 283: JCR Names. 
0,"Refactor DocumentsWriter. I've been working on refactoring DocumentsWriter to make it more
modular, so that adding new indexing functionality (like column-stride
stored fields, LUCENE-1231) is just a matter of adding a plugin into
the indexing chain.

This is an initial step towards flexible indexing (but there is still
alot more to do!).

And it's very much still a work in progress -- there are intemittant
thread safety issues, I need to add tests cases and test/iterate on
performance, many ""nocommits"", etc.  This is a snapshot of my current
state...

The approach introduces ""consumers"" (abstract classes defining the
interface) at different levels during indexing.  EG DocConsumer
consumes the whole document.  DocFieldConsumer consumes separate
fields, one at a time.  InvertedDocConsumer consumes tokens produced
by running each field through the analyzer.  TermsHashConsumer writes
its own bytes into in-memory posting lists stored in byte slices,
indexed by term, etc.

DocumentsWriter*.java is then much simpler: it only interacts with a
DocConsumer and has no idea what that consumer is doing.  Under that
DocConsumer there is a whole ""indexing chain"" that does the real work:

  * NormsWriter holds norms in memory and then flushes them to _X.nrm.

  * FreqProxTermsWriter holds postings data in memory and then flushes
    to _X.frq/prx.

  * StoredFieldsWriter flushes immediately to _X.fdx/fdt

  * TermVectorsTermsWriter flushes immediately to _X.tvx/tvf/tvd

DocumentsWriter still manages things like flushing a segment, closing
doc stores, buffering & applying deletes, freeing memory, aborting
when necesary, etc.

In this first step, everything is package-private, and, the indexing
chain is hardwired (instantiated in DocumentsWriter) to the chain
currently matching Lucene trunk.  Over time we can open this up.

There are no changes to the index file format.

For the most part this is just a [large] refactoring, except for these
two small actual changes:

  * Improved concurrency with mixed large/small docs: previously the
    thread state would be tied up when docs finished indexing
    out-of-order.  Now, it's not: instead I use a separate class to
    hold any pending state to flush to the doc stores, and immediately
    free up the thread state to index other docs.

  * Buffered norms in memory now remain sparse, until flushed to the
    _X.nrm file.  Previously we would ""fill holes"" in norms in memory,
    as we go, which could easily use way too much memory.  Really this
    isn't a solution to the problem of sparse norms (LUCENE-830); it
    just delays that issue from causing memory blowup during indexing;
    memory use will still blowup during searching.

I expect performance (indexing throughput) will be worse with this
change.  I'll profile & iterate to minimize this, but I think we can
accept some loss.  I also plan to measure benefit of manually
re-cycling RawPostingList instances from our own pool, vs letting GC
recycle them.

"
0,"sysview import cannot handle auto-created nodes. when importing a serialized system-view via the Session.importXML() method, an ItemExistsException is thrown, when a nodes has an auto-create child nodes.

when the parent node is created during the import, the repository automatically creates the auto-create child nodes. when then import handler tries to create the respective child node, the exception is thrown.


"
0,Remove the unneeded cqfs dependencies. There's still unneeded dependencies to the cqfs libraries in jcr-server/webapp and jca.
0,"Remove WARN logs for missing text extractors. In jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/lucene/JackrabbitTextExtractor.java, the method extractText logs at WARN level when no indexer is available.

Can we move this to DEBUG, as not all applications need full text indexing of the text nodes and this message fills the logs?

"
0,Add example test case for surround query language. 
0,"SimilarityDelegator is missing a delegating scorePayload() method. The handy SimilarityDelegator method is missing a scoreDelegator() delegating method.
The fix is trivial, add the code below at the end of the class:

  public float scorePayload(String fieldName, byte [] payload, int offset, int length)
  {
      return delegee.scorePayload(fieldName, payload, offset, length);
  }
"
0,"DateTools.stringToDate() can cause lock contention under load. Load testing our application (the JIRA Issue Tracker) has shown that threads spend a lot of time blocked in DateTools.stringToDate().

The stringToDate() method uses a singleton SimpleDateFormat object to parse the dates.
Each call to SimpleDateFormat.parse() is *synchronized* because SimpleDateFormat is not thread safe.

"
0,"Lucene Java Site docs. It would be really nice if the Java site docs where consistent with the rest of the Lucene family (namely, with navigation tabs, etc.) so that one can easily go between Nutch, Hadoop, etc."
0,"Alternative depth-based DOT layout ordering in FST's Utils. Utils.toDot() dumps GraphViz's DOT file, but it can be quite difficult to read. This patch provides an alternative layout that is probably a little bit easier on the eyes (well, as far as larger FSTs can be ;)"
0,"add UnicodeUtil.nextValidUTF16String . In flex branch, TermRef must not contain unpaired surrogates, etc.
But in trunk/previous releases, people could (and do) seek to these.
Also some lucene multitermqueries will generate these invalid seek locations, even now (which we should separately fix)
I think the common case is already handled with a hack in SegmentReader.LegacyTermEnum, but we should clean up this hack and handle all cases.

I would also like to use this nextValidUTF16String in LUCENE-1606, and there might be other places it could be used for better bw compat.
"
0,"Repository requires access to external resource. With the changes from JCR-626 jackrabbit requires access to the resource at the URL http://jackrabbit.apache.org/dtd/repository-1.2.dtd. If no connection to the internet is available jackrabbit will refuse to start. At least that's the case when I run the test cases in the above mentioned environment.

I'm not an XML expert, but shouldn't the doctype declaration use a public identifier? Otherwise the ConfigurationEntityResolver class doesn't make much sense.

The attached patch solve the issue for me, please comment. I've also targetted this issue for 1.2 because it seems rather serious to me if you can't start jackrabbit when you don't have an internet connection, or am I the only one with this issue?"
0,"Index Splitter. If an index has multiple segments, this tool allows splitting those segments into separate directories.  "
0,"Testcase for StandardAnalyzer. As per our discussion on lucene-user, I'm attaching a unit test for 
StandardAnalyzer.  I wrote most of the tests from reading the comments in the 
StandardTokenizer.jj grammar.  Someone familiar with the grammar (and its 
intent) should review the tests."
0,"Change default Directory impl on 64bit linux to MMap. Consistently in my NRT testing on Fedora 13 Linux, 64 bit JVM (Oracle 1.6.0_21) I see MMapDir getting better search and merge performance when compared to NIOFSDir.

I think we should fix the default."
0,"TCK: XPathQueryLevel2Test uses optional column specifier syntax. Test assumes the implementation uses a terminal attribute step as the column specifier.  This is allowed, but not required, by JCR.

Proposal: remove column specifier and process results using getNodes instead of getRows.

--- XPathQueryLevel2Test.java   (revision 422074)
+++ XPathQueryLevel2Test.java   (working copy)
@@ -85,7 +85,7 @@
         checkResult(result, 1);
  
         // evaluate result
-        checkValue(result.getRows(), propertyName1, ""b"");
+        checkValue(result.getNodes(), propertyName1, ""b"");
     }
  
     /**
@@ -101,7 +101,7 @@
         checkResult(result, 1);
  
         // evaluate result
-        checkValue(result.getRows(), propertyName1, ""existence"");
+        checkValue(result.getNodes(), propertyName1, ""existence"");
     }
  
     /**
@@ -147,7 +147,6 @@
         tmp.append(jcrRoot).append(testRoot);
         tmp.append(""/*[@"").append(propertyName2).append("" = 'two'"");
         tmp.append("" and @"").append(propertyName1).append("" = 'existence']"");
-        tmp.append(""/@"").append(propertyName1);
         return new Statement(tmp.toString(), Query.XPATH);
     }
  
@@ -161,7 +160,7 @@
         tmp.append(propertyName1);
         tmp.append("" <= 'b' and @"");
         tmp.append(propertyName1);
-        tmp.append("" > 'a']/@"").append(propertyName1);
+        tmp.append("" > 'a']"");
         return new Statement(tmp.toString(), Query.XPATH);
     }
 }

--- AbstractQueryLevel2Test.java        (revision 422074)
+++ AbstractQueryLevel2Test.java        (working copy)
@@ -19,6 +19,7 @@
 import org.apache.jackrabbit.test.NotExecutableException;
  
 import javax.jcr.nodetype.NodeType;
+import javax.jcr.NodeIterator;
 import javax.jcr.query.RowIterator;
 import javax.jcr.query.Row;
 import javax.jcr.Value;
@@ -115,4 +116,16 @@
                     expectedValue, value.getString());
         }
     }
+
+    protected void checkValue(NodeIterator itr,
+                              String propertyName,
+                              String expectedValue) throws RepositoryException {
+        while (itr.hasNext()) {
+            Node node = itr.nextNode();
+            // check fullText
+            Value value = node.getProperty(propertyName).getValue();
+            assertEquals(""Value in query result row does not match expected value"",
+                    expectedValue, value.getString());
+        }
+    }
 }
"
0,"Calculate MD5 checksums in target <dist-all>. Trivial patch that extends the ant target <dist-all> to calculate
the MD5 checksums for the dist files."
0,"Add TopDocs.merge to merge multiple TopDocs. It's not easy today to merge TopDocs, eg produced by multiple shards,
supporting arbitrary Sort.
"
0,"Incorrect debug message in HttpMethodBase. HttpMethodBase.addContentLengthRequestHeader has the wrong debug message.  See
attached patch."
0,"Flush volatile index when size limit is reached. Currently the volatile index is committed when minMergeDocs is reached. This is inconvenient because it does not take the size of nodes into account account. When lots of small nodes are added the volatile index should be committed less frequently. Similarly when nodes with lots of properties are indexed the volatile index should be committed more frequently.

Instead the size of the volatile index in bytes should trigger a disk write."
0,"Add support to provide custom classloader for class instantiation from configuration. The configuration framework is based around a BaseConfig class, which provides functionality to instantiate a class whose name is configured in the repository configuration file. Examples of such classes are the FileSystem or the PersistenceManager elements.

The current implementation of the BeanConfig.newInstance() method is to use the ""default classloader"" to load configured classes. That is, the class loader of the BeanConfig class is actually used. This is - generally - the class loader which loads the repository. In certain environments, classes may be provided from outside the core repository class loader. An example fo such an environment is an OSGi setup where each bundle gets its own class laoder, which is separate from all other class loaders except declared by configuration.

I propose to enhance the BeanConfig class as follows:

public class BeanConfig {
 ...
 // Current default class loader, default is BeanConfig's class loader
 private static ClassLoader defaultClassLoader =
BeanConfig.class.getClassLoader();
 // Current instance class loader
 private ClassLoader classLoader;
 ...
 // Sets the default class loader for new BeanConfig instances
 public static void setDefaultClassLoader(ClassLoader loader);
 // Returns the default class loader for new BeanConfig instances
 public static ClassLoader getClassLoader();
 // Sets the class loader of this BeanConfig instance
 public void setClassLoader(ClassLoader loader);
 // Returns the class loader of this BeanConfig instance
 public ClassLoader getClassLoader();
}

The BeanConfig.newInstance method would then use the following to use the class:

public Object newInstance() throws ConfigurationException {
 Class clazz = Class.forName(getClassName(), true, getClassLoader());
 ...
}


This has also been discussed on the dev list: http://mail-archives.apache.org/mod_mbox/jackrabbit-dev/200607.mbox/%3cae03024e0607272341l52aff9b2h3957131411790bc9@mail.gmail.com%3e"
0,Properly close resources. Java has exceptions so resources must always be closed on a finally clause
0,Remove duplicate code in QValueFactoryImpl (spi2dav). QValueFactoryImpl in spi2dav contains code duplicated from spi-commons QValueFactoryImpl. Once JCR-2245 has been applied the spi2dav variant can extend from the factory in spi-commons and we can simply the first.
0,"add concurrent merge policy. Provide the ability to handle merges in one or more concurrent threads, i.e., concurrent with other IndexWriter operations.

I'm factoring the code from LUCENE-847 for this."
0,"Remove build.xml from jackrabbit-core. After JCR-1203 the build.xml within jackrabbit-core contains only a single Ant task, that could just as well be moved into the pom.xml file to be run inline with the maven antrun plugin."
0,"CheckIndex prints wrong version number on 3.1 indexes (and posibly also in trunk). When you run CheckIndex on an index created/updated with 3.1, it prints about the SegmentInfos:

{noformat}
Segments file=segments_g19 numSegments=5 version=-11 [Lucene 1.3 or prior]
{noformat}

We should fix CheckIndex and also verify other cases where version numbers are printed out. In trunk the issue may be more complicated!"
0,"Permit using different tablespaces for tables and indexes with Oracle. OracleFileSystem, OraclePersistenceManager and OracleDatabaseJournal already provide a tableSpace parameter to customize the DDL, but the same tablespace is used for both tables and indexes. It is common place to use distinct tablespaces for these. Jackrabbit could provide support for this."
0,"mvn eclipse:eclipse inconsistent. mvn eclipse:eclipse result is inconsistent, due to deprecated avacc-maven-plugin usage.
should use piped ""jjtree-javacc"" goal.
core :
          <execution>
            <id>fulltext-jjtree</id>
            <configuration>
              <sourceDirectory>${basedir}/src/main/javacc/fulltext</sourceDirectory>
              <outputDirectory>${project.build.directory}/generated-src/main/java</outputDirectory>
              <timestampDirectory>${project.build.directory}/generated-src/javacc-timestamp</timestampDirectory>
              <packageName>org.apache.jackrabbit.core.query.lucene.fulltext</packageName>
            </configuration>
            <goals>
              <goal>jjtree-javacc</goal>
            </goals>
          </execution>

spi-commons:
					<execution>
						<id>sql-jjtree-javacc</id>
						<configuration>
							<sourceDirectory>${basedir}/src/main/javacc/sql</sourceDirectory>
							<outputDirectory>${project.build.directory}/generated-src/main/java</outputDirectory>
							<timestampDirectory>${project.build.directory}/generated-src/javacc-timestamp</timestampDirectory>
							<packageName>org.apache.jackrabbit.spi.commons.query.sql</packageName>
						</configuration>
						<goals>
							<goal>jjtree-javacc</goal>
						</goals>
					</execution>
					<execution>
						<id>xpath-jjtree-javacc</id>
						<configuration>
							<sourceDirectory>${basedir}/src/main/javacc/xpath</sourceDirectory>
							<outputDirectory>${project.build.directory}/generated-src/main/java</outputDirectory>
							<timestampDirectory>${project.build.directory}/generated-src/javacc-timestamp</timestampDirectory>
							<packageName>org.apache.jackrabbit.spi.commons.query.xpath</packageName>
						</configuration>
						<goals>
							<goal>jjtree-javacc</goal>
						</goals>
					</execution>"
0,"Remove deprecated Directory stuff and IR/IW open/ctor hell. This patch removes primarily the deprecated Directory stuff. This also removes parts of the ctor/open hell in IR and IW. IndexModifier is completely removed as deprecated, too."
0,"Add more unit tests on BeanConverters. Some BeanConverters are not yet stable. We have to add more unit tests.  It seems that null values for bean attributes are not well supported. We have to test that for all BeanConverters. 

Here is a good scenario to test : 

Model : Class A contains an attribute ""b"" based on class B. 

Create an instance of A with ""b"" = null
insert A / save
Get instance of A
set the attribute of B
update A /save




"
0,"Clean up old JIRA issues in component ""Other"". A list of all JIRA issues in component ""Other"" that haven't been updated in 2007:

   *	 LUCENE-746  	 Incorrect error message in AnalyzingQueryParser.getPrefixQuery   
   *	LUCENE-644 	Contrib: another highlighter approach 
   *	LUCENE-574 	support for vjc java compiler, also known as J# 
   *	LUCENE-471 	gcj ant target doesn't work on windows 
   *	LUCENE-434 	Lucene database bindings 
   *	LUCENE-254 	[PATCH] pseudo-relevance feedback enhancement 
   *	LUCENE-180 	[PATCH] Language guesser contribution 
"
0,"Add support for 3.0 indexes in 2.9 branch. There was a lot of user requests to be able to read Lucene 3.0 indexes also with 2.9. This would make the migration easier. There is no problem in doing that, as the new stored fields version in Lucene 3.0 is only used to mark a segment's stored fields file as no longer containing compressed fields. But index format did not really change. This patch simply allows FieldsReader to pass a Lucene 3.0 version number, but still writes segments in 2.9 format (as you could suddenly turn on compression for added documents).

I added ZIP files for 3.0 indexes for TestBackwards. Without the patch it does not pass, as FieldsReader complains about incorrect version number (although it could read the file easily). If we would release maybe a 2.9.4 release of Lucene we should include that patch."
0,"Speed up hierarchy cache initialization. Initializing a workspace can take quite a long time if there is a big number of nodes and some search indexes involved. The reason is that the setup of the CachingIndexReader is processed using chunks of a certain size (actually 400K) in order to reduce the memory footprint. As soon as the number of documents exceeds this limit some operations (actually traversing complete indexes) are performed again and again.

It seems that the current algorithm ""initializeParents"" in the CachingIndexReader class can't be optimized without increasing the memory consumption. Therefore it should be a promising approach to persist the ""state"" of this class (actually it's main member array and map) and reload it on startup.

The ""load"" of the state can be done implicitly in the initializing phase of the cache. This is obvious. The correct point of time to call the ""save"" operation isn't obvious at all. I tried the ""doClose"" method of the class and it seems sufficient."
0,"Sort and SortField does not have equals() and hashCode(). During developing for my project panFMP I had the following issue:
I have a cache for queries (like Solr has, too)  for query results. This cache also uses the Sort/SortField as key into the cache. The problem is, because Sort/SortField does not implement equals() and hashCode(), you cannot store them as cache keys. To workaround, currently I use Sort.toString() as cache key, but this is not so nice.

In corelation with issue LUCENE-1478, I could fix this there in one patch together with the other improvements."
0,"ShingleFilter benchmark. Spawned from LUCENE-2218: a benchmark for ShingleFilter, along with a new task to instantiate (non-default-constructor) ShingleAnalyzerWrapper: NewShingleAnalyzerTask.

The included shingle.alg runs ShingleAnalyzerWrapper, wrapping the default StandardAnalyzer, with 4 different configurations over 10,000 Reuters documents each.  To allow ShingleFilter timings to be isolated from the rest of the pipeline, StandardAnalyzer is also run over the same set of Reuters documents.  This set of 5 runs is then run 5 times.

The patch includes two perl scripts, the first to output JIRA table formatted timing information, with the minimum elapsed time for each of the 4 ShingleAnalyzerWrapper runs and the StandardAnalyzer run, and the second to compare two runs' JIRA output, producing another JIRA table showing % improvement."
0,Remove deprecated Field.Store.COMPRESS. Also remove FieldForMerge and related code.
0,fix generics violations in contrib/modules. There are some generics violations... we should fix them.
0,"Remove query handler idleTime. The changes included in JCR-415 revealed a synchronization issue with the query handler idle timer task.

See thread on dev-list: http://thread.gmane.org/gmane.comp.apache.jackrabbit.devel/10199

We could either fix the synchronization issue in the SearchManager class or remove the functionality all together.

Because the repository also supports a idle time parameter for the whole workspace (maxIdleTime in Workspaces element) the query handler idle time should be removed."
0,"[PATCH] remove unused variables. Seems I'm the only person who has the ""unused variable"" warning turned on in 
Eclipse :-) This patch removes those unused variables and imports (for now 
only in the ""search"" package). This doesn't introduce changes in 
functionality, but it should be reviewed anyway: there might be cases where 
the variables *should* be used, but they are not because of a bug."
0,"remove IndexDocValuesField. Its confusing how we present CSF functionality to the user, its actually not a ""field"" but an ""attribute"" of a field like  STORED or INDEXED.

Otherwise, its really hard to think about CSF because there is a mismatch between the APIs and the index format."
0,"JCR2SPI: Add specific deep loading of Nodes and Properties. with jcr2spi an item is 'deep' loaded whenever the hierarchy is not complete. while for Session#itemExists or Session#getItem it is ok to try loading a Node first and if not found a Property, this is inconvenient (and generates unnecessary round trips to the SPI) for those cases, where the original JCR call indicates whether a Node or Property is expected.

This is the case for Node.getNode(String relPath) and Node.getProperty(String relPath) ((and maybe others))

Therefore i suggest to add specific methods

HierarchyManager#getNodeEntry
HierarchyManager#getPropertyEntry
NodeEntry#getDeepNodeEntry
NodeEntry#getDeepPropertyEntry

(or something similar)"
0,"TCK: more tests assuming that 'addMixin' immediately taking effect. jsr170 allows an implementation to have Node.addMixin only taking affect upon a save-call.

some tests already got adjusted.
attached patch for additional tests, that make usage of addMixin"
0,"TestTimeLimitedCollector  shuold not fail if the testing machine happens to be slow. The test fails on Hudson about once a month, like this:

{quote}
   [junit] Testcase: testTimeoutNotGreedy(org.apache.lucene.search.TestTimeLimitedCollector):  FAILED
   [junit] lastDoc=21 ,&& allowed=799 ,&& elapsed=900 >= 886 = ( 2*resolution +  TIME_ALLOWED + SLOW_DOWN = 2*20 + 799 + 47)
   [junit] junit.framework.AssertionFailedError: lastDoc=21 ,&& allowed=799 ,&& elapsed=900 >= 886 = ( 2*resolution +  TIME_ALLOWED + SLOW_DOWN = 2*20 + 799 + 47)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.doTestTimeout(TestTimeLimitedCollector.java:189)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.testTimeoutNotGreedy(TestTimeLimitedCollector.java:150)
   [junit]
   [junit]
   [junit] Test org.apache.lucene.search.TestTimeLimitedCollector FAILED
{quote}

Modify the test to just print a warning in this case - but still verify that there was an early termination.
"
0,non-recursive MultiTermDocs. A non-recursive implementation of MultiTermDocs.next() and skipTo() would be nice as it's currently possible to get a stack overflow in very rare situations.
0,"Factor out ByteSliceWriter from DocumentsWriterFieldData. DocumentsWriter uses byte slices into shared byte[]'s to hold the
growing postings data for many different terms in memory.  This is
probably the trickiest (most confusing) part of DocumentsWriter.

Right now it's not cleanly factored out and not easy to separately
test.  In working on this issue:

  http://mail-archives.apache.org/mod_mbox/lucene-java-user/200805.mbox/%3c126142c0805061426n1168421ya5594ef854fae5e4@mail.gmail.com%3e

which eventually turned out to be a bug in Oracle JRE's JIT compiler,
I factored out ByteSliceWriter and created a unit test to stress test
the writing & reading of byte slices.  The test just randomly writes N
streams interleaved into shared byte[]'s, then reads them back
verifying the results are correct.

I created the stress test to try to find any bugs in that code.  The
test ran fine (no bugs were found) but I think the refactoring is
still very much worthwhile.

I expected the changes to reduce indexing throughput, so I ran a test
indexing first 200K Wikipedia docs using this alg:

{code}
analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
doc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker

docs.file=/Volumes/External/lucene/wiki.txt
doc.stored = true
doc.term.vector = true
doc.add.log.step=2000

directory=FSDirectory
autocommit=false
compound=true

ram.flush.mb=256

{ ""Rounds""
  ResetSystemErase
  { ""BuildIndex""
    - CreateIndex
     { ""AddDocs"" AddDoc > : 200000
    - CloseIndex
  }
  NewRound
} : 4

RepSumByPrefRound BuildIndex

{code}

Ok trunk it produces these results:
{code}
Operation   round   runCnt   recsPerRun        rec/s  elapsedSec    avgUsedMem    avgTotalMem
BuildIndex      0        1       200000        791.7      252.63   338,552,096  1,061,814,272
BuildIndex -  - 1 -  -   1 -  -  200000 -  -   793.1 -  - 252.18 - 605,262,080  1,061,814,272
BuildIndex      2        1       200000        794.8      251.63   601,966,528  1,061,814,272
BuildIndex -  - 3 -  -   1 -  -  200000 -  -   782.5 -  - 255.58 - 608,699,712  1,061,814,272
{code}

and with the patch:

{code}
Operation   round   runCnt   recsPerRun        rec/s  elapsedSec    avgUsedMem    avgTotalMem
BuildIndex      0        1       200000        745.0      268.47   338,318,784  1,061,814,272
BuildIndex -  - 1 -  -   1 -  -  200000 -  -   792.7 -  - 252.30 - 605,331,776  1,061,814,272
BuildIndex      2        1       200000        786.7      254.24   602,915,712  1,061,814,272
BuildIndex -  - 3 -  -   1 -  -  200000 -  -   795.3 -  - 251.48 - 602,378,624  1,061,814,272
{code}

So it looks like the performance cost of this change is negligible (in
the noise).

"
0,.NET build scripts for Jackrabbit. Hugo Burm has created build scripts that make it possible to run Jackrabbit under .NET. We should add the scripts and excample client code as a contrib project or integrate them with the Maven build system.
0,"TaxonomyReader/Writer and their Lucene* implementation. The facet module contains two interfaces TaxonomyWriter and TaxonomyReader, with two implementations Lucene*. We've never actually implemented two TaxonomyWriters/Readers, so I'm not sure if these interfaces are useful anymore. Therefore I'd like to propose that we do either of the following:

# Remove the interfaces and remove the Lucene part from the implementation classes (to end up with TW/TR impls). Or,
# Keep the interfaces, but rename the Lucene* impls to Directory*.

Whatever we do, I'd like to make the impls/interfaces impl also TwoPhaseCommit.

Any preferences?"
0,"Persian Analyzer. A simple persian analyzer.

i measured trec scores with the benchmark package below against http://ece.ut.ac.ir/DBRG/Hamshahri/ :

SimpleAnalyzer:
SUMMARY
  Search Seconds:         0.012
  DocName Seconds:        0.020
  Num Points:           981.015
  Num Good Points:       33.738
  Max Good Points:       36.185
  Average Precision:      0.374
  MRR:                    0.667
  Recall:                 0.905
  Precision At 1:         0.585
  Precision At 2:         0.531
  Precision At 3:         0.513
  Precision At 4:         0.496
  Precision At 5:         0.486
  Precision At 6:         0.487
  Precision At 7:         0.479
  Precision At 8:         0.465
  Precision At 9:         0.458
  Precision At 10:        0.460
  Precision At 11:        0.453
  Precision At 12:        0.453
  Precision At 13:        0.445
  Precision At 14:        0.438
  Precision At 15:        0.438
  Precision At 16:        0.438
  Precision At 17:        0.429
  Precision At 18:        0.429
  Precision At 19:        0.419
  Precision At 20:        0.415

PersianAnalyzer:
SUMMARY
  Search Seconds:         0.004
  DocName Seconds:        0.011
  Num Points:           987.692
  Num Good Points:       36.123
  Max Good Points:       36.185
  Average Precision:      0.481
  MRR:                    0.833
  Recall:                 0.998
  Precision At 1:         0.754
  Precision At 2:         0.715
  Precision At 3:         0.646
  Precision At 4:         0.646
  Precision At 5:         0.631
  Precision At 6:         0.621
  Precision At 7:         0.593
  Precision At 8:         0.577
  Precision At 9:         0.573
  Precision At 10:        0.566
  Precision At 11:        0.572
  Precision At 12:        0.562
  Precision At 13:        0.554
  Precision At 14:        0.549
  Precision At 15:        0.542
  Precision At 16:        0.538
  Precision At 17:        0.533
  Precision At 18:        0.527
  Precision At 19:        0.525
  Precision At 20:        0.518

"
0,"GData Server IndexComponent. New Feature added:

-> Indexcomponent.
-> Content extraction from entries.
-> Custom content ext. strategies added.
-> user defined index schema.
-> extended gdata-config.xml schema (xsd)
-> Indexcomponent UnitTests
-> Spellchecking on some JavaDoc.

##############
New jars included:

nekoHTML.jar 
xercesImpl.jar

@yonik: don't miss the '+' button to add directories :)"
0,"org.apache.jackrabbit.core.query.lucene.SearchIndex with in-memory lucene index. If I'm not wrong, there is actually no way to configure SearchIndex in order to use a memory only lucene index.

Since you can configure a repository using a org.apache.jackrabbit.core.state.mem.InMemPersistenceManager, it makes sense that also search index offers a similar configuration.
MultiIndex and PersistentIndex now always use a org.apache.lucene.store.FSDirectory, they should be refactored in order to allow a switching to a org.apache.lucene.store.RAMDirectory for this."
0,"Wikipedia Document Generation Changes. The EnwikiDocMaker currently produces a fair number of documents that are in the download, but are of dubious use in terms of both benchmarking and indexing.  

These issues are:

# Redirect (it currently only handles REDIRECT and redirect, but there are documents as Redirect
# Template files appear to be useless.  These are marked by the term Template: at the beginning of the body.  See for example: http://en.wikipedia.org/wiki/Template:=)
# Image only pages, as in http://en.wikipedia.org/wiki/Image:Sciencefieldnewark.jpg.jpg  These are about as useful as the Redirects and Templates
# Files pending deletion:  This one is a bit trickier to handle, but they are generally marked by ""Wikipedia:Votes for deletion"" or some variation of that depending where along it is in being deleted

I think I can implement this such that it is backward compatible, if there is such a need when it comes to the contrib/benchmark suite.



"
0,"Add support for large number of users in a group. In the current implementation there are several factors which limit the number of users in a group:

- group membership is recorded in a multi valued property which does not scale well
- members of groups are collected eagerly which does not scale well

I propose to add complementary support for recording group membership in a node structure to the current solution. That node structure would - similar to users and groups - add intermediate nodes when a group reaches a certain threshold on the number of its users."
0,"Registering multiple node types with the same name in a single file must fail. Registering node types from a file (XML or CND) containing multiple definitions with the same name will succeed and only the last definition will be used.
The right behavior is to throw an exception as this kind of file is well-formed but invalid."
0,"Correct 2 minor javadoc mistakes in core, javadoc.access=private. Patches Token.java and TermVectorsReader.java"
0,"JMX Bindings for Jackrabbit. There has been a slight interest in the past for adding JMX support.
This would be the first stab at it. It is a Top 15 slow query log. (the 15 part is configurable)

It is not enabled by default, so just being there should not affect the overall performance too much. You can enable it and play a little, tell me what you think.
I've also added a test case that does a duration comparison with and without the logger.

The most important part of this issue is that it should open the way for some proper monitoring tools support around queries, caches, anything/everything Jackrabbit.

As usual, please let me know what you guys think"
0,"Index merging should run in a separate thread. Indexes are merged using the configuration parameters mergeFactor and minMergeDocs. With the default value of 10 for mergeFactor and 100 for minMergeDocs, as soon as 10 index directories exist with less or equal than 100 nodes they are merged into a single one. This process is then repeated by multiplying the minMergeDocs with the mergeFactor. Therefore the second round will merge 10 index directories with less or equal than 1000 nodes.

Because the above process is part of the regular workspace store operation an index merge with more than let's say 10'000 nodes can block the store operation for a couple of seconds. With the current synchronization scheme, all other threads are blocked from writing. This is not acceptable.

Index merging should run in a separate thread in the background.

The process needs to take care of the following:
- While merging indexes, deletes on those indexes must not get lost
- Switching between the indexes that are merged and the new index must be atomic
- Recovery if merging is interrupted, e.g. jackrabbit is shutdown"
0,"IndexWriter.mergeSegments should not hold the commit lock while cleaning up.. Same happens in IndexWriter.addIndexes(IndexReader[] readers).

The commit lock should be obtained whenever the Index structure/version is read or written.  It should be kept for as short a period as possible.

The write lock is needed to make sure only one IndexWriter or IndexReader instance can update the index (multiple IndexReaders can of course use the index for searching).

The list of files that can be deleted is stored in the file ""deletable"".  It is only read or written by the IndexWriter instance that holds the write lock, so there's no need to have the commit lock to to update it.

On my production system deleting the obsolete segment files after a mergeSegments() happens can occasionally take several seconds(!) and the commit lock blocks the searcher machines from updating their IndexReader instance.
Even on a standalone machine, the time to update the segments file is about 3ms, the time to delete the obsolete segments about 30ms.
"
0,Use repository service wide namespace cache. The jcr2spi layer requests namespaces for each new session that is created. It should rather cache them and make them available to other sessions.
0,"Stop using BaseException. The o.a.j.BaseException class is deprecated (since JCR-1169) and not caught anywhere, so there's no need to keep using it."
0,QueryHandler.init() should take a context argument. Currently the QueryHandler.init() method takes a bunch of arguments which are needed by the single jackrabbit implementation for the query handler. To make further extensions easier the arguments should be packaged into a context class which can be extended without effect on the QueryHandler interface.
0,Remove excessive dependencies from jcr-client module. 
0,"Upgrade to Tika 0.8. Apache Tika version 0.8 is now available, and we should upgrade to benefit from the various fixes and improvements included in that version."
0,Move & copy objects. Add new methods in the persistence manager to move and copy objects
0,"Add a field-filtering FilterAtomicReader to 4.0 so ParallelReaders can be better tested (in LTC.maybeWrapReader). In addition to the filters in contrib/misc for horizontally filtering (by doc-id) AtomicReader, it would be good to have the same vertically (by field). For now I will add this implementation to test-framework, as it cannot stay in contrib/misc, because LTC will need it for maybeWrapReader.

LTC will use this FilterAtomicReader to construct a ParallelAtomicReader out of two (or maybe more) FieldFilterAtomicReaders."
0,"Decouple Filter from BitSet. {code}
package org.apache.lucene.search;

public abstract class Filter implements java.io.Serializable 
{
  public abstract AbstractBitSet bits(IndexReader reader) throws IOException;
}

public interface AbstractBitSet 
{
  public boolean get(int index);
}

{code}

It would be useful if the method =Filter.bits()= returned an abstract interface, instead of =java.util.BitSet=.

Use case: there is a very large index, and, depending on the user's privileges, only a small portion of the index is actually visible.
Sparsely populated =java.util.BitSet=s are not efficient and waste lots of memory. It would be desirable to have an alternative BitSet implementation with smaller memory footprint.

Though it _is_ possibly to derive classes from =java.util.BitSet=, it was obviously not designed for that purpose.
That's why I propose to use an interface instead. The default implementation could still delegate to =java.util.BitSet=.

"
0,"openReaderPassed not populated in CheckIndex.Status.SegmentInfoStatus. When using CheckIndex programatically, the openReaderPassed flag on the SegmentInfoStatus is never populated (so it always comes back false)

looking at the code, its clear that openReaderPassed is defined, but never used

furthermore, it appears that not all information that is propagated to the ""InfoStream"" is available via SegmentIinfoStatus

All of the following information should be able to be gather from public properties on the SegmentInfoStatus:
test: open reader.........OK
test: fields, norms.......OK [2 fields]
test: terms, freq, prox...OK [101 terms; 133 terms/docs pairs; 133 tokens]
test: stored fields.......OK [100 total field count; avg 1 fields per doc]
test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
"
0,"Allow readOnly OpenReader task. I'd like to change OpenReader in contrib/benchmark to open a readOnly reader by default, and take readOnly optional param if for some reason a ""writable IndexReader"" becomes necessary in the future."
0,"Improper EOL for text files on Windows. version 2.0-rc3. Improper EOL for text files on Windows for README.txt and
LICENSE.txt, docs/*.txt. The files display as one loooong line in Notepad."
0,Improved error reporting from JcrUtils.getRepository. The service provider mechanism and the null return value used by the RepositoryFactory API makes it a bit difficult to troubleshoot cases where a repository can not be accessed. It would be helpful if the JcrUtils.getRepository methods reported as accurate failure information as possible in case the requested repository is not found.
0,"Sandbox remaining contrib queries. In LUCENE-3271, I moved the 'good' queries from the queries contrib to new destinations (primarily the queries module).  The remnants now need to find their home.  As suggested in LUCENE-3271, these classes are not bad per se, just odd.  So lets create a sandbox contrib that they and other 'odd' contrib classes can go to.  We can then decide their fate at another time."
0,"reusing connections is unreliable. HttpConnection reuse is unreliable. Because of the following:

1) There is currently no way to determine if a connection is still open on the
server side.
2) If an IOException occurs while writing to a connection it cannot be reused."
0,"contrib/benchmark - few improvements and a bug fix. Benchmark byTask was slightly improved:

1. fixed a bug in the ""child-should-not-report"" mechanism. If a task sequence contained only simple tasks it worked as expected (i.e. child tasks did not report times/memory) but if a child was a task sequence, then its children would report - they should not - this was fixed, so this property is now ""penetrating/inherited"" all the way down.

2. doc size control now possible also for the Reuters doc maker. (allowing to index N docs of size C characters each.)

3. TrecDocMaker was added - it reads as input the .gz files used in Trec - e.g. .gov data - this can be handy to benchmark Lucene on these large collections.  Similar to the Reuters collection, the doc-maker scans the input directory for all the files and extracts documents from the files.  Here there are multiple documents in each input file. Unlike the Reuters collection, we cannot provide a 'loader' for these collections - they are available from http://trec.nist.gov - for research purposes.

4. a new BasicDocMaker abstract class handles most of doc-maker tasks, including creating docs with specific size, so adding new doc-makers for other data is now much simpler."
0,"Http Client give sme message when proxy/http endpoint is down. Whether Http sever endpoint is down or the proxy server is down we get the same stack trace as:

java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333)
	at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
	at java.net.Socket.connect(Socket.java:518)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.commons.httpclient.protocol.ReflectionSocketFactory.createSocket(ReflectionSocketFactory.java:139)
	at org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory.createSocket(DefaultProtocolSocketFactory.java:124)
	at org.apache.commons.httpclient.HttpConnection.open(HttpConnection.java:706)
	at org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpConnectionAdapter.open(MultiThreadedHttpConnectionManager.java:1321)
	at org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:386)
	at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:170)
	at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:396)
	at com.approuter.module.http.protocol.HttpTransportSender.perform(HttpTransportSender.java:214)
	at 


It will be good if we can get information whether the proxy was down or the Http endpoint.

"
0,"Node type documentation tool (NTDoc). This is the first time I post a contrib here on Jira. Hope I do this the right way :-) 

Some weeks ago I postet a message on the forum about a node type documentation tool I had made. Now, finally I have cleaned up the code and fixed some bugs. It is now useful for the majority out there. I do not guarantee it to be bug-free, but will do my best to fix any bugs that is reported. 

A readme file is included in the distribution. Must build using maven 2. Have not done it maven 1 compliant."
0,"Adding a factory to QueryParser to instantiate query instances. With the new efforts with Payload and scoring functions, it would be nice to plugin custom query implementations while using the same QueryParser.
Included is a patch with some refactoring the QueryParser to take a factory that produces query instances."
0,"When using QueryWrapperFilter with CachingWrapperFilter, QueryWrapperFilter returns a DocIdSet that creates a Scorer, which gets cached rather than a bit set. there is a large performance cost to this.

The old impl for this type of thing, QueryFilter, recommends :

@deprecated use a CachingWrapperFilter with QueryWrapperFilter

The deprecated QueryFilter itself also suffers from the problem because its now implemented using a CachingWrapperFilter and QueryWrapperFilter.

see http://search.lucidimagination.com/search/document/7f54715f14b8b7a/lucene_2_9_0rc4_slower_than_2_4_1"
0,"Path.getAncestor and Path.isAncestor are not symmetric. Although the method names refer to ancestors they operate on sub-paths. Consider:

PathFactory pf = PathFactoryImpl.getInstance();
Path.Element p = pf.getParentElement();

Path path = pf.create(new Path.Element[]{p, p});
Path ancestor = path.getAncestor(1);

assertFalse(ancestor.isAncestorOf(path) )  

This is not what one would expect from looking an the method signatures. 
I suggest to rename getAncestor to getSubPath, clarify the javadoc, and deprecate getAncestorCount. 

A patch follows.
"
0,"TwoPhaseCommit interface. I would like to propose a TwoPhaseCommit interface which declares the methods necessary to implement a 2-phase commit algorithm:
* prepareCommit()
* commit()
* rollback()

The prepare/commit ones have variants that take a (Map<String,String> commitData) following the ones we have in IndexWriter.

In addition, a TwoPhaseCommitTool which implements a 2-phase commit amongst several TPCs.

Having IndexWriter implement that interface will allow running the 2-phase commit algorithm on multiple IWs or IW + any other object that implements the interface.

We should mark the interface @lucene.internal so as to not block ourselves in the future. This is pretty advanced stuff anyway.

Will post a patch soon"
0,"reopen support for SegmentReader. Reopen for SegmentReader can be supported simply as the following:

  @Override
  public synchronized IndexReader reopen() throws CorruptIndexException,
		IOException {
	return reopenSegment(this.si,false,readOnly);
  }

  @Override
  public synchronized IndexReader reopen(boolean openReadOnly)
		throws CorruptIndexException, IOException {
	return reopenSegment(this.si,false,openReadOnly);
  }
"
0,"Replace HttpState with CredentialsProvier and CookieStore interfaces. Replace HttpState, which is a concrete class, with CredentialsProvier and CookieStore interfaces. Provide default impls of those interfaces."
0,"Support only-if-cached directive. Add support for only-if-cached Cache-Control directive- If the request is not servable from the cache, return a 504 Gateway Timeout.  See http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.4"
0,"Improve parallel tests. As mentioned on the dev@ mailing list here: http://www.lucidimagination.com/search/document/93432a677917b9bd/lucenejunitresultformatter_sometimes_fails_to_lock

It would be useful to not create a lockfactory for each test suite (As they are run sequentially in the same separate JVM).
Additionally, we create a lot of JVMs (26) for each batch, because we have to run one for each letter.
Instead, we use a technique here to divide up the tests with a custom selector: http://blog.code-cop.org/2009/09/parallel-junit.html
(I emailed the blog author and received permission to use this code)

This gives a nice boost to the speed of overall tests, especially Solr tests, as many start with an ""S"", but this is no longer a problem.

"
0,"Clustering configuration documentation for syncDelay doesn't match. There is a bit of mismatch in the current documentation that is available on configuring a Cluster node for a repository.  If you look at the DTD for repository.xml[1] it states that the syncDelay attribute of the Cluster element is in seconds.  However if you read the Javadoc for the ClusterConfig[2] object it states the syncDelay is in milliseconds.  I'm guessing that the value is actually in milliseconds but at the very least the two documents should be telling the same story.


[1] -http://jackrabbit.apache.org/dtd/repository-1.4.dtd
[2] - http://jackrabbit.apache.org/api/1.4/org/apache/jackrabbit/core/config/ClusterConfig.html"
0,tck doesn't compile (use of enum keyword). Use if enum keyword in TestFinder (patch will be attached)
0,"NTLM implementation lacks support for NTLMv1, NTLMv2, and NTLM2 Session forms of NTLM. The current HttpClient implementation lacks support for all enhancements to NTLM after Windows 95.  That includes NTLMv1, NTLMv2, and NTLM2 Session Response varieties of the protocol.

This seriously impacts the usability of HttpClient in enterprise situations, which has required the Lucene Connector Framework team to extend HttpClient to address the issue.

I've attached a patch which contains the implementation used by LCF.
"
0,"Lucene Scorer implementations should handle the 'advance' to NO_MORE_DOCS optimisation better. This is from the lucene Scorer (actually DocIdSetIterator) api:
""NOTE: this method may be called with NO_MORE_DOCS for efficiency by some Scorers. If your implementation cannot efficiently determine that it should exhaust, it is recommended that you check for that value in each call to this method.""

None of the scorer implementations does that currently. Except for ChildAxisScorer thanks to JCR-3082.

This is a worthwhile effort, which can save us from bugs (JCR-3082) but also leverage some performance optimisation hints from the lucene api."
0,"support array/offset/ length setters for Field with binary data. currently Field/Fieldable interface supports only compact, zero based byte arrays. This forces end users to create and copy content of new objects before passing them to Lucene as such fields are often of variable size. Depending on use case, this can bring far from negligible  performance  improvement. 

this approach extends Fieldable interface with 3 new methods   
getOffset(); gettLenght(); and getBinaryValue() (this only returns reference to the array)

   "
0,"FST should differentiate between final vs non-final stop nodes. I'm breaking out this one improvement from LUCENE-2948...

Currently, if a node has no outgoing edges (a ""stop node"") the FST
forcefully marks this as a final node, but it need not do this.  Ie,
whether that node is final or not should be orthogonal to whether it
has arcs leaving or not.
"
0,"httpClient does not support installation of different SSLSocketFactory. Description:

The SSLProtocolSocketFactory class had hard-
coded ""javax.net.ssl.SSLSocketFactory"" as the socket factory.  It does not 
support installation of other socket factory.

Proposed Fix:

We added a setDefaultSSLSocketFactory method to the SSLProtocolSocketFactory 
and modified the code to use the factory it it is set.  The code falls back on 
using ""javax.net.ssl.SSLSocketFactory"" if a default is not set."
0,Make DefaultSecurityManager the default security manager (instead of SimpleSecurityManager). JCR-2164 made DefaultSecurityManager the default security manager for test runs. However the repository.xml included in jackrabbit core and the one for cluster tests still refer to the SimpleSecurityManager. For consistency reasons I think it makes sense to change these places to DefaultSecurityManager.
0,"allow unsetting of DEFAULT_PROXY and FORCED_ROUTE parameters in the client stack. Since we don't want to delay client alpha3 until HTTPCORE-139 is solved in beta2, we need a parameter specific solution for unsetting these client parameters on the request level.
"
0,"When thread is interrupted we should throw a clear exception. This is the 3.0 followon from LUCENE-1573.  We should throw a dedicated exception, not just RuntimeException.

Recent discussion from java-dev ""Thread.interrupt()"" subject: http://www.lucidimagination.com/search/document/8423f9f0b085034e/thread_interrupt"
0,"Deprecate all non-bundle persistence managers. Bundle persistence has been the recommended default since Jackrabbit 1.3, and there is little reason for anyone to be using non-bundle persistence anymore. Thus I'd like to deprecate all non-bundle PMs in Jackrabbit 2.2 and target for their removal in Jackrabbit 3.0."
0,"SetPropertyAssumeTypeTest check for non-protected string array property. SetPropertyAssumeTypeTest.testValuesConstraintViolationExceptionBecauseOfInvalidTypeParameter tries to find a property definition for a writable, multivalued string property. It consults NodeTypeUtil.locatePropertyDef() for that purpose.

In my setup, the property definition being returned is for jcr:valueConstraints, defined on nt:propertyDefinition. Nodes of that type in turn can not be created on the test node, thus the test fails already when trying to create the node.

It seems the test suite tries to be too smart here. Can we change this so that the node type and the property name are configuration parameters?"
0,"Move text extraction into a background thread. Even though text extraction is not done right on save() most of the extraction work is later done by a client thread. There is a mechanism in place that commits the deferred work in a background thread. But the background thread is only triggered by a timer and does not constantly write back pending index changes. For regular index changes this is done on purpose and should not be changed. However text extraction work should be moved completely into a background thread because it often takes a fair amount of time to index a large document.

Outline of a possible solution:
- all text filtering is tasks are put into a work queue
- the work queue is processed by a background thread
- basic indexing of nt:resource without text filtering takes place
- the background thread updates the index when text filtering completed for a nt:resource

There should be a configuration parameter that allows to execute text filtering without the background thread. This way it is possible to get the existing behaviour of Jackrabbit: the fulltext index is always up-to-date and can be used.
With the background process this is no longer the case."
0,"add IndexReader.getUniqueTermCount. Simple API to return number of unique terms (across all fields).  Spinoff from here:

http://www.lucidimagination.com/search/document/536b22e017be3e27/term_limit"
0,Provide change log consolidator. spi-commons should provide base implementations for consolidating change logs (Batch). 
0,"revisit segments.gen sleeping. in LUCENE-3601, i worked up a change where we intentionally crash() all un-fsynced files 
in tests to ensure that we are calling sync on files when we should.

I think this would be nice to do always (and with some fixes all tests pass).

But this is super-slow sometimes because when we corrupt the unsynced segments.gen, it causes
SIS.read to take 500ms each time (and in checkindex for some reason we do this twice, which seems wrong).

I can workaround this for now for tests (just do a partial crash that avoids corrupting the segments.gen),
but I wanted to create this issue for discussion about the sleeping/non-fsyncing of segments.gen, just
because i guess its possible someone could hit this slowness.
 "
0,AbstractRepositoryService should be able to handle GuestCredentials. AbstractRepositoryService.createSessionInfo should handle GuestCredentials. Currently it only handle SimpleCredentials
0,"make similarities/term/collectionstats take long (for > 2B docs). As noted by Yonik and Andrzej on SOLR-1632, this would be useful for distributed scoring.

we can also add a sugar method add() to both of these to make it easier to sum."
0,"Catch exceptions in Threads created by JUnit tasks. On hudson we had several assertions failed in TestRAMDirectory, that were never caught by the error reportier in JUnit (as the test itsself did not fail). This patch adds a handler for uncaught exceptions to LuceneTestCase(J4) that let the test fail in tearDown()."
0,tests for verifying that assertions are enabled do nothing since they ignore AssertionError. Follow-up from LUCENE-3501
0,"move contrib/snowball to contrib/analyzers. to fix bugs in some duplicate, handcoded impls of these stemmers (nl, fr, ru, etc) we should simply merge snowball and analyzers, and replace the buggy impls with the proper snowball stemfilters.
"
0,"Make prefixLength accessible to PrefixTermEnum subclasses. PrefixTermEnum#difference() offers a way to influence scoring based on the difference between the prefix Term and a term in the enumeration. To effectively use this facility the length of the prefix should be accessible to subclasses. Currently the prefix term is private to PrefixTermEnum. I added a getter for the prefix length and made PrefixTermEnum#endEnum(), PrefixTermEnum#termCompare() final for consistency with other TermEnum subclasses.

Patch is attached.

Simon"
0,"when a test Assume fails, display information. Currently if a test uses Assume.assumeTrue, it silently passes.

I think we should output something, at *least* if you have VERBOSE set, maybe always.

Here's an example of what the output might look like:
{noformat}
junit-sequential:
    [junit] Testsuite: org.apache.solr.servlet.SolrRequestParserTest
    [junit] Tests run: 4, Failures: 0, Errors: 0, Time elapsed: 1.582 sec
    [junit]
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: testStreamURL Assume failed (ignored):
    [junit] org.junit.internal.AssumptionViolatedException: got: <java.io.FileNotFoundException: http://www.apdfgdfgache
.org/dist/lucene/solr/>, expected: null
    [junit]     at org.junit.Assume.assumeThat(Assume.java:70)
    [junit]     at org.junit.Assume.assumeNoException(Assume.java:92)
    [junit]     at org.apache.solr.servlet.SolrRequestParserTest.testStreamURL(SolrRequestParserTest.java:123)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)
    [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit]     at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:802)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:775)
    [junit]     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    [junit]     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    [junit]     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:768)
    [junit] Caused by: java.io.FileNotFoundException: http://www.apdfgdfgache.org/dist/lucene/solr/
    [junit]     at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1311)
    [junit]     at org.apache.solr.servlet.SolrRequestParserTest.testStreamURL(SolrRequestParserTest.java:120)
    [junit]     ... 26 more
    [junit] ------------- ---------------- ---------------
{noformat}"
0,"build.xml in cnotrib/benchmark should auto build core java and demo if required. Currently one needs to build core jar and demo jar before building/running benchmark.
This is not very convenient. 
Change it to 
- use core classes and demo classes (instead of jars).
- build core and demo by dependency if required."
0,Upgrade all default socket factories to use SO_REUSEADDR parameter. See HTTPCORE-209
0,"isValid should be invoked after analyze rather than before it so it can validate the output of analyze. The Synonym map has a protected method String analyze(String word) designed for custom stemming.

However, before analyze is invoked on a word, boolean isValid(String str) is used to validate the word - which causes the program to discard words that maybe useable by the custom analyze method. 

I think that isValid should be invoked after analyze rather than before it so it can validate the output of analyze and allow implemters to decide what is valid for the overridden analyze method. (In fact, if you look at code snippet below, isValid should really go after the empty string check)

This is a two line change in org.apache.lucene.index.memory.SynonymMap

      /*
       * Part B: ignore phrases (with spaces and hyphens) and
       * non-alphabetic words, and let user customize word (e.g. do some
       * stemming)
       */
      if (!isValid(word)) continue; // ignore
      word = analyze(word);
      if (word == null || word.length() == 0) continue; // ignore"
0,"Add AttributeSource.copyTo(AttributeSource). One problem with AttributeSource at the moment is the missing ""insight"" into AttributeSource.State. If you want to create TokenStreams that inspect cpatured states, you have no chance. Making the contents of State public is a bad idea, as it does not help for inspecting (its a linked list, so you have to iterate).

AttributeSource currently contains a cloneAttributes() call, which returns a new AttrubuteSource with all current attributes cloned. This is the (more expensive) captureState. The problem is that you cannot copy back the cloned AS (which is the restoreState). To use this behaviour (by the way, ShingleMatrix can use it), one can alternatively use cloneAttributes and copyTo. You can easily change the cloned attributes and store them in lists and copy them back. The only problem is lower performance of these calls (as State is a very optimized class).

One use case could be:
{code}
AttributeSource state = cloneAttributes();
// .... do something ...
state.getAttribute(TermAttribute.class).setTermBuffer(foobar);
// ... more work
state.copyTo(this);
{code}"
0,"Update site level documentation. ugg - a fun one - my brain is sliding to the bottom of my skull from excitement.

Must update all of the site level pages to current API usage."
0,"JCRTest.java (First Steps example code): to few parameters in session.importXML. In the code on the First Steps page:

if (!rn.hasNode(""importxml"")) {
        System.out.println(""importing xml"");
        Node n=rn.addNode(""importxml"", ""nt:unstructured"");
        session.importXML(""/importxml"", new FileInputStream(""repotest/test.xml""));
        session.save();
      }

The importXML needs a third parameter, compare to: 

http://www.day.com/maven/jsr170/javadocs/jcr-1.0/javax/jcr/Session.html

This prevents the code from the First Steps page from compiling."
0,"Add workaround for ICU bug in combination with Java7 to LuceneTestCase. There is a bug in ICU that makes it fail to load it ULocale class in Java7: http://bugs.icu-project.org/trac/ticket/8734

The problem is caused by some new locales in Java 7, that lead to a chicken-and-egg problem in the static initializer of ULocale. It initializes its default locale from the JDK locale in a static ctor. Until the default ULocale instance is created, the default is not set in ULocale. But ULocales ctor itsself needs the default locale to fetch some ressource bundles and throws NPE.

The code in LuceneTestCase that randomizes the default locale should classload ULocale before it tries to set another random locale, using a defined, safe locale (Locale.US). Patch is easy."
0,"XMLReader logs fatal error to system out. Some test cases check if an appropriate exception is thrown when invalid XML is supplied, in that case the build in XMLReader in Java 1.5 logs a fatal error to system out.

This seems to be caused by a missing error handler on the XMLReader."
0,"Move MemoryJournal from test to main. Running our tests with the FileJournal implementation on a windows box can be quite slow because of the many FileDescriptor.sync() calls.

I'd like to move the MemoryJournal in jackrabbit-core test to the main sources. That way we can use it in other test setups."
0,"Open access modifier for RepositoryImpl.doShutdown(). This is required for a subclass of RepositoryImpl that wants to run additional code on shutdown, otherwise a deadlock may occur because the sequence of lock acquisition cannot be ensured.

Jackrabbit requires that the shutdownLock is first acquired and then the actual shutdown code is executed."
0,Cut over SpanQuery#getSpans to AtomicReaderContext. Followup from LUCENE-2831 - SpanQuery#getSpans(IR) seems to be the last remaining artifact that doesn't enforce per-segments context while it should really work on AtomicReaderContext (SpanQuery#getSpans(AtomicReaderContext) instead of a naked IR.
0,"separate IndexDocValues interface from implementation. Currently the o.a.l.index.values contains both the abstract apis and Lucene40's current implementation.

I think we should move the implementation underneath Lucene40Codec, leaving only the abstract apis.

For example, simpletext might have a different implementation, and we might make a int8 implementation
underneath preflexcodec to support norms."
0,"New Token filter for adding payloads ""in-stream"". This TokenFilter is able to split a token based on a delimiter and use one part as the token and the other part as a payload.  This allows someone to include payloads inline with tokens (presumably setup by a pipeline ahead of time).  An example is apropos.  Given a | delimiter, we could have a stream that looks like:
{quote}The quick|JJ red|JJ fox|NN jumped|VB over the lazy|JJ brown|JJ dogs|NN{quote}

In this case, this would produce tokens and payloads (assuming whitespace tokenization):
Token: the
Payload: null

Token: quick
Payload: JJ

Token: red
Pay: JJ.

and so on.

This patch will also support pluggable encoders for the payloads, so it can convert from the character array to byte arrays as appropriate."
0,"remove dependencies of XPathQueryBuilder on core Jackrabbit code. The XPath query parser currently has a single dependency on SearchManager, for the sole purpose of importing two namespace URIs (for XML Schema and XPath 2.0 functions). This makes it harder than it should be to use it stand-alone.

I propose to copy the two namespace URIs into XPathQueryBuilder, getting rid of the dependency.

"
0,"hashCode improvements. It would be nice for all Query classes to implement hashCode and equals to enable them to be used as keys when caching.
"
0,"Document order of result nodes should be configurable. Queries without an order by clause are performed with document order for the result nodes. This is a quite expensive operation, because the document order is available in the search index itself. The document order is calculated with the help of the ItemStateManager and requires loading of all result node states including their ancestors.

Queries with a lot of result nodes become quite expensive, even though the actual query execution is fast. Because most use cases will not care for the document order, this feature should be made configurable. Some parameter for the QueryHandler that disables the document order on result nodes."
0,"org.apache.http.impl.client.cache.memcached.MemcachedHttpCacheStorage should allow client to specify custom prefix string for keys. org.apache.http.impl.client.cache.memcached.MemcachedHttpCacheStorage should allow client to specify custom prefix string for keys, so as to ensure collision-avoidance with other memcached keys the client may be using."
0,"Fix NOTICE files to match consensus from legal team. As discussed in LEGAL-62 and the related legal-discuss@ threads, the Jackrabbit NOTICE files currently contain information that doesn't really need to be there. A simple ""Copyright (c) ..."" statement is not a required attribution notice."
0,"Optimize SegmentMerger to work on atomic (Segment)Readers where possible. This is a spin-off from LUCENE-2769:

Currently SegmentMerger has some optimizations when it merges segments that are SegmentReaders (e.g. when doing normal indexing or optimizing). But when you do IndexWriter.addIndexes(IndexReader...) the listed IndexReaders may not really be per-segment. SegmentMerger should track down all passed in reads down to the lowest level (Segment)Reader (or other atomic readers like SlowMultiReaderWrapper) and then merge. We can then remove most MultiFields usage (except term merging itsself) and clean up the code.

This especially saves lots of memory for merging norms, as no longer the duplicate norms arrays are created when MultiReaders are used!"
0,"Add Thread-Safety note to IndexWriter JavaDoc. IndexWriter Javadocs should contain a note about thread-safety. This is already mentioned on the wiki FAQ page but such an essential information should be part of the module documentation too.
"
0,decorator enhancements. added some decorating enhancements as we discussed on the mailing list (apparently there is nothing yet in the gmane /marc archives).
0,Collection parameter of CompactNodeTypeDefWriter#write should be covariant. The Collection<QNodeTypeDefinition> parameter of the CompactNodeTypeDefWriter#write methods should have type Collection<? extends QNodeTypeDefinition>. 
0,"Method to create default RepositoryConfig from just the repository directory. It would be useful to have a static method like RepositoryConfig.create(File) that would take the repository directory and expect to find the repository configuration in a ""repository.xml"" file inside that directory.

If the directory does not exist, it would be created. And if the repository configuration file does not exist, then it would be created from the default configuration included in Jackrabbit."
0,Remove deprecated TokenStream API. I looked into clover analysis: It seems to be no longer used since I removed the tests yesterday - I am happy!
0,"remove support for event bundle IDs. Event bundle IDs currently are not used. We can re-add them later in case we need them.
"
0,"Incomplete lucene-core-1.9.1 in Maven2 repository. I'm new to Lucene and am setting up a project using v1.9.1 to use Maven2 instead of ANT.
The project would not build with Maven2 due to lacking lucene classes.
I tracked the problem down to that the lucene-core-1.9.1 jar file that Maven2 downloaded from the repository was smaller (2.3KB) than the one I got from the local ANT repository (408KB).
Can you please update the v1.9.1 file on the Maven2 [1], [2] repositories so other developers don't get frustrated by the incomplete jar?


[1] http://repo1.maven.org/maven2/org/apache/lucene/lucene-core/1.9.1/
[2] http://mirrors.ibiblio.org/pub/mirrors/maven2/org/apache/lucene/lucene-core/1.9.1/

This issue is a copy of a mail sent to the java-dev@lucene.apache.org list april 4. 2007."
0,Retrieve row path via hierarchy manager instead of node. Jackrabbit currently loads the node associated with a result row to retrieve the path. Loading the node may be prevented by retrieving the path from a caching hierarchy manager.
0,"[OCM] Add unit tests with BundleDbPersistenceManager. Until now, we have not yet check the ocm framework with the BundleDbPersistenceManager"
0,"Elision filter for simple french analyzing. If you don't wont to use stemming, StandardAnalyzer miss some french strangeness like elision.
""l'avion"" wich means ""the plane"" must be tokenized as ""avion"" (plane).
This filter could be used with other latin language if elision exists."
0,Child axis support for XPath predicates. It seems that Jackrabbit currently only supports the attribute axis in XPath predicates. Support for the child axis would be a nice addition.
0,"Add generics to DocumentsWriterDeleteQueue.Node. DocumentsWriterDeleteQueue.Note should be generic as the subclasses hold different types of items. This generification is a little bit tricks, but the generics policeman can't wait to fix this *g*."
0,"LocalTestServer and supporting classes should be available as a separate jar. LocalTestServer and it's supporting classes are useful to anyone who wants to easily ""mock""/test simple http calls without having to embed a full jetty or something.
It would be awesome if these were available in a separate http-localtestserver.jar that could be used in projects outside of httpclient."
0,ant test won't run in 'out of the box' installation. one possible solution would be to remove 'lib' from the junit.classpath
0,"Make IndexReader really read-only in Lucene 4.0. As we change API completely in Lucene 4.0 we are also free to remove read-write access and commits from IndexReader. This code is so hairy and buggy (as investigated by Robert and Mike today) when you work on SegmentReader level but forget to flush in the DirectoryReader, so its better to really make IndexReaders readonly.

Currently with IndexReader you can do things like:
- delete/undelete Documents -> Can be done by with IndexWriter, too (using deleteByQuery)
- change norms -> this is a bad idea in general, but when we remove norms at all and replace by DocValues this is obsolete already. Changing DocValues should also be done using IndexWriter in trunk (once it is ready)"
0,"TCK: NamespaceRegistryTest#testRegisterNamespace doesn't remove node in new namespace. The test creates a node in the new namespace, but doesn't remove it.  This prevents tearDown from unregistering the namespace.

Proposal: the test should remove the new node before returning.

--- NamespaceRegistryTest.java  (revision 422074)
+++ NamespaceRegistryTest.java  (working copy)
@@ -138,6 +138,10 @@
  
         testRootNode.addNode(namespacePrefix + "":root"");
         testRootNode.save();
+
+        // Need to remove it here, otherwise teardown can't unregister the NS.
+        testRootNode.getNode(namespacePrefix + "":root"").remove();
+        testRootNode.save();
     }
"
0,"Add Bundle Persistence Managers. we (day software) offer our set of bundle persistence managers to the jackrabbit project. those pms combine the node and property states into a single bundle and store them together. this improves performance and reduces storage-memory overhead (no exact numbers available). The bundle pms also have a ""bundle-cache"" that does a memory sensitive caching of the bundles and a negative cache for non-existent bundles. small binary properties are inlined into the bundle rather than stored in the blobstore."
0,"[API Doc] Compile performance optimization guide. Performance optimization guide is long overdue and badly needed. The more people
start using HttpClient in all sorts of creative ways the more we are going to
need it.

Oleg"
0,RepositoryUtil moved outside of main source tree. It appears that the RepositoryUtil class was moved from src/main to src/test. This class is used by the ocm-spring project.
0,"Store all metadata in human-readable segments file. Various index-reading components in Lucene need metadata in addition to data.
This metadata is presently stored in arbitrary binary headers and spread out
over several files.  We should move to concentrate it in a single file, and 
this file should be encoded using a human-readable, extensible, standardized 
data serialization language -- either XML or YAML.

* Making metadata human-readable makes debugging easier.  Centralizing it
  makes debugging easier still.  Developers benefit from being able to scan
  and locate relevant information quickly and with less debug printing.  Users
  get a new window through which to peer into the index structure.
* Since metadata is written to a separate file, there would no longer be a 
  need to seek back to the beginning of any data file to finish a header, 
  solving issue LUCENE-532.
* Special-case parsing code needed for extracting metadata supplied by 
  different index formats can be pared down.  If a value is no longer 
  necessary, it can just be ignored/discarded.
* Removing headers from the data files simplifies them and makes the file
  format easier to implement. 
* With headers removed, all or nearly all data structures can take the
  form of records stacked end to end, so that once a decoder has been
  selected, an iterator can read the file from top to tail.  To an extent,
  this allows us to separate our data-processing algorithms from our
  serialization algorithms, decoupling Lucene's code base from its file
  format.  For instance, instead of further subclassing TermDocs to deal with
  ""flexible indexing"" formats, we might replace it with a PostingList which
  returns a subclass of Posting.  The deserialization code would be wholly
  contained within the Posting subclass rather than spread out over several
  subclasses of TermDocs.
* YAML and XML are equally well suited for the task of storing metadata, 
  but in either case a complete parser would not be needed -- a small subset 
  of the language will do.  KinoSearch 0.20's custom-coded YAML parser 
  occupies about 600 lines of C -- not too bad, considering how miserable C's 
  string handling capabilities are. "
0,"Connection not closed after ""Connection: close"" request. In HTTP specification at http://www.w3.org/Protocols/rfc2616/rfc2616-
sec8.html , under chapter ""Negotiation"", it is stated :
""If either the client or the server sends the close token in the Connection 
header, that request becomes the last one for the connection.""

HttpClient (v2.0.2 and v3.0 alpha2) is currently closing connection only if 
server has sent ""Connection: close"" header, and not when request contains it."
0,Exclude JavaCC-generated code from static analysis. The JavaCC-generated code we have in spi-commons should be excluded from static analysis done by tools like Sonar.
0,JCR mapping: Upgrade to Maven 2. Upgrade the JCR Mapping components to Maven 2 before a release.
0,"Document SINGLE_COOKIE_HEADER param in the cookie guide. Included is some sample code that shows the behaviour when loading pages from a phpBB powered 
site. Here are the results as i see them on my machine:

==== start results

==================================
Policy: rfc2109
==================================


        URL: http://www.sgboards.com/forums/viewtopic.php?t=12&view=next&mforum=str
        Response status code: 200
        Present cookies: 
                ForumSetCookie=str
                phpbb_str_data=a%3A0%3A%7B%7D
                phpbb_str_sid=c8da590cc4b1683b9079da3d82f4efa6

        URL: http://www.sgboards.com/forums/viewtopic.php?p=24&mforum=str
        Response status code: 200
        Present cookies: 
                phpbb_str_data=a%3A0%3A%7B%7D
                phpbb_str_sid=c8da590cc4b1683b9079da3d82f4efa6
                ForumSetCookie=str

        URL: http://www.sgboards.com/forums/posting.php?mode=quote&p=24&mforum=str
        Response status code: 200
        Present cookies: 
                phpbb_str_data=a%3A0%3A%7B%7D
                phpbb_str_sid=c8da590cc4b1683b9079da3d82f4efa6
                ForumSetCookie=str

        URL: http://www.sgboards.com/forums/viewtopic.php?p=25&mforum=str
        Response status code: 200
        Present cookies: 
                phpbb_str_data=a%3A0%3A%7B%7D
                phpbb_str_sid=c8da590cc4b1683b9079da3d82f4efa6
                ForumSetCookie=str

==================================
Policy: netscape
==================================


        URL: http://www.sgboards.com/forums/viewtopic.php?t=12&view=next&mforum=str
        Response status code: 200
        Present cookies: 
                phpbb_str_sid=e2604334a0022283333153f6879feb70

        URL: http://www.sgboards.com/forums/viewtopic.php?p=24&mforum=str
        Response status code: 200
        Present cookies: 
                phpbb_str_sid=e2604334a0022283333153f6879feb70

        URL: http://www.sgboards.com/forums/posting.php?mode=quote&p=24&mforum=str
        Response status code: 200
        Present cookies: 
                phpbb_str_sid=e2604334a0022283333153f6879feb70

        URL: http://www.sgboards.com/forums/viewtopic.php?p=25&mforum=str
        Response status code: 200
        Present cookies: 
                phpbb_str_sid=e2604334a0022283333153f6879feb70

==================================
Policy: compatibility
==================================


        URL: http://www.sgboards.com/forums/viewtopic.php?t=12&view=next&mforum=str
        Response status code: 200
        Present cookies: 
                ForumSetCookie=str
                phpbb_str_data=a%3A0%3A%7B%7D
                phpbb_str_sid=d156f6dbfa605320b5a250129fa0b22e

        URL: http://www.sgboards.com/forums/viewtopic.php?p=24&mforum=str
        Response status code: 200
        Present cookies: 
                ForumSetCookie=str
                phpbb_str_data=a%3A0%3A%7B%7D
                phpbb_str_sid=d5d5a46fd27fd783cdb4e324992bc9d2

        URL: http://www.sgboards.com/forums/posting.php?mode=quote&p=24&mforum=str
        Response status code: 200
        Present cookies: 
                phpbb_str_data=a%3A0%3A%7B%7D
                phpbb_str_sid=b4312fee4250f767cd1b34b11afadb3d
                ForumSetCookie=str

        URL: http://www.sgboards.com/forums/viewtopic.php?p=25&mforum=str
        Response status code: 200
        Present cookies: 
                ForumSetCookie=str
                phpbb_str_data=a%3A0%3A%7B%7D
                phpbb_str_sid=daf72685d35d851c3eec68b6b3bc3705

==== end results

As you can see the only cookie policy that ISN'T successfully tracking sessions is the COMPATIBILITY 
setting. There are a lot of these phpBB sites around, so that's where I've noticed the behaviour most. 
Trying another random php powered site I see that all policies work as expected.

It would be nice to know what's messing up the cookie handing on these phpBB sites. If you can't rely 
on the compatibility setting to reliably maintain session variables (and hence truly imitate a browser) 
then life get's a little complicated.

Both 3.0beta1 and the CVS version show the same behaviour.

Many thanks,

Garry

Example code below.

====== begin code
import org.apache.commons.httpclient.Cookie;
import org.apache.commons.httpclient.HttpClient;
import org.apache.commons.httpclient.HttpState;
import org.apache.commons.httpclient.cookie.CookiePolicy;
import org.apache.commons.httpclient.methods.GetMethod;


public class CookieProbe {
	static final String[] urls = {
		""http://www.sgboards.com/forums/viewtopic.php?t=12&view=next&mforum=str"",
		""http://www.sgboards.com/forums/viewtopic.php?p=24&mforum=str"",
		""http://www.sgboards.com/forums/posting.php?mode=quote&p=24&mforum=str"",
		""http://www.sgboards.com/forums/viewtopic.php?p=25&mforum=str""
	};
	static final String[] urls2 = {
		""http://www.virginmobilelouder.com/live/index.php"",
		""http://www.virginmobilelouder.com/live/index.php?page_id=214"",
		""http://www.virginmobilelouder.com/live/index.php?page_id=3"",
		""http://www.virginmobilelouder.com/live/index.php?page_id=116""
	};
	
	static final String[] policies = {
		CookiePolicy.RFC_2109, 
		CookiePolicy.NETSCAPE, 
		CookiePolicy.BROWSER_COMPATIBILITY, 
	};
	
	
	public static void main(String[] args) {
		try {
			for (int i = 0; i < policies.length; i++) {
				System.out.println(""\n=================================="");
				System.out.println(""Policy: "" + policies[i]);
				System.out.println(""==================================\n"");
				tryPolicy(policies[i]);
			}
		} catch (Exception e) {
			e.printStackTrace(System.err);
		}
	}
	
	public static void tryPolicy(String policy) throws Exception {
		HttpState initialState = new HttpState();
		HttpClient httpclient = new HttpClient();
		httpclient.getHttpConnectionManager().
			getParams().setConnectionTimeout(30000);
		httpclient.setState(initialState);
		
		httpclient.getParams().setCookiePolicy(policy);
		for (int i = 0; i < urls.length; i++) {
			System.out.println(""\n\tURL: "" + urls[i]);
			tryURL(httpclient, urls[i]);
			Thread.sleep(1000); // give server a break
		}
			
	}

	public static void tryURL(HttpClient httpclient, String strURL) throws Exception {
		GetMethod httpget = new GetMethod(strURL);
		int result = httpclient.executeMethod(httpget);
		System.out.println(""\tResponse status code: "" + result);
		// Get all the cookies
		Cookie[] cookies = httpclient.getState().getCookies();
		System.out.println(""\tPresent cookies: "");
		for (int i = 0; i < cookies.length; i++) {
			System.out.println(""\t\t"" + cookies[i].toExternalForm());
		}
		// Release current connection to the connection pool once you are done
		httpget.releaseConnection();
	}
}
====== end code"
0,"ParallelReader is now atomic, rename to ParallelAtomicReader and also add a ParallelCompositeReader (that requires LogDocMergePolicy to have identical subreader structure). The plan is:
- Move all subreaders to ctor (builder-like API. First build reader-set, then call build)
- Rename ParallelReader to ParallelAtomicReader
- Add a ParallelCompositeReader with same builder API, but taking any CompositeReader-set and checks them that they are aligned (docStarts identical). The subreaders are ParallelAtomicReaders."
0,"random sampler is not random (and so facet SamplingWrapperTest occasionally fails). RandomSample is not random at all:
It does not even import java.util.Random, and its behavior is deterministic.

in addition, the test testCountUsingSamping() never retries as it was supposed to (for taking care of the hoped-for randomness)."
0,"UserManagement: make membership a weakreferences. the 2.0 user management code still contains a TODO asking for changing group-membership from reference to weakreference. 

now that weakreferences are implemented in jackrabbit-core i'd like to make that change both in the node type definitions and the code.

i'm opening this issue in order to give us the chance to keep track of node type change that may cause troubles...."
0,"rev. 169301: wrong directory name in build.xml. build.xml mentions non-existing directory contrib/WordNet/ which should read
contrib/wordnet in line 418.

below the result of svn diff against the corrected and working version of build.xml

--- build.xml   (revision 169301)
+++ build.xml   (working copy)
@@ -415,7 +415,7 @@
         <!-- TODO: find a dynamic way to do include multiple source roots -->
         <packageset dir=""src/java""/>
         <packageset dir=""contrib/analyzers/src/java""/>
-        <packageset dir=""contrib/WordNet/src/java""/>
+        <packageset dir=""contrib/wordnet/src/java""/>
         <packageset dir=""contrib/highlighter/src/java""/>
         <packageset dir=""contrib/similarity/src/java""/>
         <packageset dir=""contrib/spellchecker/src/java""/>"
0,"Speed up SegementDocsEnum by making it more friendly for JIT optimizations. Since we moved the bulk reading into the codec ie. make all  bulk reading codec private in LUCENE-3584 we have seen some performance [regression|http://people.apache.org/~mikemccand/lucenebench/Term.html] on different CPUs. I tried to optimize the implementation to make it more eligible for runtime optimizations, tried to make loops JIT friendly by moving out branches where I can, minimize member access in all loops, use final members where possible and specialize the two common cases With & Without LiveDocs.

I will attache a patch and my benchmark results in a minute."
0,"Make the extraction of Session UserIDs from Subjects configurable. The SessionImpl class must extract a string name from the Prinicpals in a Subject to use as the Session userID.  In 1.4 the SessionImpl class directly selects the first available Principal.  In 1.5, this is delegated to the SecurityManager, which chooses the first  non-group principal.

It would be useful to be able to configure specific selection criteria for the Principal used for the Session userID.  A simple mechanism would involve specifying a Principal implementation classname in the configuration, and the first instance of that class found in the Subject would be used for the userID.  One way to implement this in 1.4 would be to extend AuthContext to include a method getSessionPrincipal() which encapsulates the selection logic, and adding an option the LoginModuleConfig to specify the class name of the Principal to select.

A particular use case is using the LDAP LoginModule from Sun JDK 6 with the repository.  The first Principal LdapLoginModule populates into the Subject is an instance of LdapPrincipal, which renders the userID as the full DN of the user.  The LoginModule also adds an instance of UserPrincipal, whose name is the simple username/uid attribute, which would be more appropriate as the Session userId since it corresponds to the username provided by the user to application authentication mechanisms (the provided username is expanded into the full DN prior to authentication by the login module).  If the above configuration mechanism were available, one could configure the LdapLoginModule, and specify that the userID be extracted from the first instance of com.sun.security.auth.UserPrincipal.  Since rewriting LoginModules is not always possible or desirable, this change would enable the stable integration of 3rd-party login modules that may populate the Subject with several principals."
0,"RangeQuery and RangeFilter should use collation to check for range inclusion. See [this java-user discussion|http://www.nabble.com/lucene-farsi-problem-td16977096.html] of problems caused by Unicode code-point comparison, instead of collation, in RangeQuery.

RangeQuery could take in a Locale via a setter, which could be used with a java.text.Collator and/or CollationKey's, to handle ranges for languages which have alphabet orderings different from those in Unicode."
0,"Deployment of webdav servlet on Jboss problem - logging. Tested two different installs of JBoss to verify problem is not related to a specific version. There is a problem with the jackrabbit-server.war when deploying on jboss.  Here are the details during deployment:

=======================
13:20:48,654 INFO  [TomcatDeployer] deploy, ctxPath=/jackrabbit-server, warUrl=.../deploy/jackrabbit-server.war/
13:20:48,857 INFO  [STDOUT] log4j:ERROR A ""org.jboss.logging.util.OnlyOnceErrorHandler"" object is not assignable to a ""o rg.apache.log4j.spi.ErrorHandler"" variable.
13:20:48,857 INFO  [STDOUT] log4j:ERROR The class ""org.apache.log4j.spi.ErrorHandler"" was loaded by
13:20:48,857 INFO  [STDOUT] log4j:ERROR [WebappClassLoader
  delegate: false
  repositories:
    /WEB-INF/classes/
----------> Parent Classloader:
java.net.FactoryURLClassLoader@19d277e
] whereas object of type
13:20:48,857 INFO  [STDOUT] log4j:ERROR ""org.jboss.logging.util.OnlyOnceErrorHandler"" was loaded by [org.jboss.system.se rver.NoAnnotationURLClassLoader@ab95e6].
13:20:48,904 INFO  [STDOUT] log4j:ERROR Could not create an Appender. Reported error follows.
13:20:48,904 INFO  [STDOUT] java.lang.ClassCastException: org.jboss.logging.appender.DailyRollingFileAppender
13:20:48,904 INFO  [STDOUT]     at org.apache.log4j.xml.DOMConfigurator.parseAppender(DOMConfigurator.java:165)
13:20:48,904 INFO  [STDOUT]     at org.apache.log4j.xml.DOMConfigurator.findAppenderByName(DOMConfigurator.java:140)
13:20:48,904 INFO  [STDOUT]     at org.apache.log4j.xml.DOMConfigurator.findAppenderByReference(DOMConfigurator.java:153
)
13:20:48,904 INFO  [STDOUT]     at org.apache.log4j.xml.DOMConfigurator.parseChildrenOfLoggerElement(DOMConfigurator.jav
a:415)
13:20:48,904 INFO  [STDOUT]     at org.apache.log4j.xml.DOMConfigurator.parseRoot(DOMConfigurator.java:384)
13:20:48,904 INFO  [STDOUT]     at org.apache.log4j.xml.DOMConfigurator.parse(DOMConfigurator.java:783)
13:20:48,904 INFO  [STDOUT]     at org.apache.log4j.xml.DOMConfigurator.doConfigure(DOMConfigurator.java:666)
13:20:48,904 INFO  [STDOUT]     at org.apache.log4j.xml.DOMConfigurator.doConfigure(DOMConfigurator.java:616)
13:20:48,904 INFO  [STDOUT]     at org.apache.log4j.xml.DOMConfigurator.doConfigure(DOMConfigurator.java:602)
13:20:48,904 INFO  [STDOUT]     at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:460)

13:20:48,904 INFO  [STDOUT]     at org.apache.log4j.LogManager.<clinit>(LogManager.java:113)
13:20:48,904 INFO  [STDOUT]     at org.apache.log4j.xml.DOMConfigurator.configure(DOMConfigurator.java:543)
13:20:48,904 INFO  [STDOUT]     at org.apache.jackrabbit.j2ee.LoggingServlet.configureXML(LoggingServlet.java:148)
13:20:48,904 INFO  [STDOUT]     at org.apache.jackrabbit.j2ee.LoggingServlet.configure(LoggingServlet.java:115)
13:20:48,904 INFO  [STDOUT]     at org.apache.jackrabbit.j2ee.LoggingServlet.init(LoggingServlet.java:86)
======================
 
Unlike most logging problems, this is having an impact during runtime - when trying to do DASL searches, will return a 500 error as the server was unable to log correctly.
"
0,"Index Writer constructor flags unclear - and annoying in certain cases. Wouldn't it make more sense if the constructor for the IndexWriter always
created an index if it doesn't exist - and the boolean parameter should be
""clear"" (instead of ""create"")

So instead of this (from javadoc):

IndexWriter

public IndexWriter(Directory d,
                   Analyzer a,
                   boolean create)
            throws IOException

    Constructs an IndexWriter for the index in d. Text will be analyzed with a.
If create is true, then a new, empty index will be created in d, replacing the
index already there, if any.

Parameters:
    d - the index directory
    a - the analyzer to use
    create - true to create the index or overwrite the existing one; false to
append to the existing index 
Throws:
    IOException - if the directory cannot be read/written to, or if it does not
exist, and create is false


We would have this:

IndexWriter

public IndexWriter(Directory d,
                   Analyzer a,
                   boolean clear)
            throws IOException

    Constructs an IndexWriter for the index in d. Text will be analyzed with a.
If clear is true, and a index exists at location d, then it will be erased, and
a new, empty index will be created in d.

Parameters:
    d - the index directory
    a - the analyzer to use
    clear - true to overwrite the existing one; false to append to the existing
index 
Throws:
    IOException - if the directory cannot be read/written to, or if it does not
exist.



Its current behavior is kind of annoying, because I have an app that should
never clear an existing index, it should always append.  So I want create set to
false.  But when I am starting a brand new index, then I have to change the
create flag to keep it from throwing an exception...  I guess for now I will
have to write code to check if a index actually has content yet, and if it
doesn't, change the flag on the fly."
0,"some legal jcr names cause unneccessary server-roundtrips . assume the following legal qualified jcr names:

""{foo}""
""{foo} bar""

when items with such names are read from the spi layer, they are first interpreted as expanded form names.
a prefix lookup for namespace 'foo' fails and the name is treated as qualified jcr name.

=> depending on the spi implementation, a server-roundtrip is required in order to determine that 'foo' is not a
registered namespace. "
0,"Investigate ways to compile the refactored jcr-mapping for Java 1.4. The last refactoring of the jcr-mapping project included the annotation based mapping description into the main code based thus requiring compilation with Java 5 or higher.

There are still some use cases, which require Java 1.4. The goal is to investigate, whether it would be possible to define a build profile in the pom, which compiles for 1.4 by ignoring the annotation classes."
0,"Add Japanese filter to replace term attribute with readings. Koji and Robert are working on LUCENE-3888 that allows spell-checkers to do their similarity matching using a different word than its surface form.

This approach is very useful for languages such as Japanese where the surface form and the form we'd like to use for similarity matching is very different.  For Japanese, it's useful to use readings for this -- probably with some normalization."
0,"Update 3.x legacy homepage with end of life notification and pointer to HTTPComponents. If you google for ""http client"" or ""httpclient"" or ""apache http client"" the first result is always legacy Commons-HttpClient page.  If you follow the link, there's no indication whatsoever on the legacy page that its not the latest version, or that it's end-of-lifed.  Unless you're already aware of HttpComponents, there's no obvious way to find it from the legacy page.  The only reference to HttpComponents is actually in the 'History' section.  

The source for the legacy page should be updated to indicate Commons-HttpClient is an end-of-lifed project and should provide a pointer to the new HttpComponents page in a more prominent location."
0,Update POI dependency to 3.0.2-FINAL. 3.0.2-FINAL is the most recent POI release.
0,"DbDataStore: tablePrefix not accomodated during init test for existing DATASTORE table. we are providing a test db deployment with prepopulated data, including jackrabbit DataStore. I tried specifying the tablePrefix when creating the initial repository, and tables are created properly.

However, when a clean installation is run with a fresh database that has an existing DataStore (stored at JACKRABBIT_DS_DATASTORE table), the startup fails, because during init, the meta data only checks for tables name matching the tableSQL property:
ResultSet rs = meta.getTables(null, null, tableSQL, null);

but the tableSQL property is never modified based on the tablePrefix property (other uses of tableSQL modify queries based on the prefix).

I think the init method should modify the tested table name based on the tablePrefix.

Note: I assume different JDBC drivers may handle this differently (we are using SQL Server 2007 and jTDS driver), since the DatabaseMetaData is API is unclear on the parameter to getTables being a ""tableNamePattern"" - should wildcards work? Or should a specific table be specified? 

My DataStore config is below:

    <DataStore class=""org.apache.jackrabbit.core.data.db.DbDataStore"">
      <param name=""className"" value=""org.apache.jackrabbit.core.data.db.DbDataStore""/>
      <param name=""url"" value=""jdbc:jtds:SQLServer://localhost:1433/nga_admin;prepareSQL=2;responseBuffering=adaptive""/>
      <param name=""user"" value=""sa""/>
      <param name=""password"" value=""""/>
      <param name=""databaseType"" value=""sqlserver""/>
      <param name=""driver"" value=""net.sourceforge.jtds.jdbc.Driver""/>
      <!-- a bug in jackrabbit makes tablePrefix not work -->
      <param name=""tablePrefix"" value=""JACKRABBIT_DS_"">
      <param name=""minRecordLength"" value=""1""/>
      <param name=""maxConnections"" value=""2""/>
      <param name=""copyWhenReading"" value=""true""/>         
    </DataStore>
"
0,"org.apache.jackrabbit.server.SessionProvider needs 'releaseSession()'. the SessionProvider does not have a 'releaseSession()' method, thus the DavSessionProviderImpl just calls repSession.logout() after is DavSession is released. This should rather be handled over to the given SessionProvider."
0,"Post grouping faceting. This issues focuses on implementing post grouping faceting.
* How to handle multivalued fields. What field value to show with the facet.
* Where the facet counts should be based on
** Facet counts can be based on the normal documents. Ungrouped counts. 
** Facet counts can be based on the groups. Grouped counts.
** Facet counts can be based on the combination of group value and facet value. Matrix counts.   

And properly more implementation options.

The first two methods are implemented in the SOLR-236 patch. For the first option it calculates a DocSet based on the individual documents from the query result. For the second option it calculates a DocSet for all the most relevant documents of a group. Once the DocSet is computed the FacetComponent and StatsComponent use one the DocSet to create facets and statistics.  

This last one is a bit more complex. I think it is best explained with an example. Lets say we search on travel offers:
|||hotel||departure_airport||duration||
|Hotel a|AMS|5
|Hotel a|DUS|10
|Hotel b|AMS|5
|Hotel b|AMS|10

If we group by hotel and have a facet for airport. Most end users expect (according to my experience off course) the following airport facet:
AMS: 2
DUS: 1

The above result can't be achieved by the first two methods. You either get counts AMS:3 and DUS:1 or 1 for both airports."
0,"Lucene 2.0 requirements - Remove all deprecated code. Per the move to Lucene 2.0 from 1.9, remove all deprecated code and update documentation, etc.

Patch to follow shortly."
0,"Possible Memory Leak in StoredFieldsWriter. StoredFieldsWriter creates a pool of PerDoc instances

this pool will grow but never be reclaimed by any mechanism

furthermore, each PerDoc instance contains a RAMFile.
this RAMFile will also never be truncated (and will only ever grow) (as far as i can tell)

When feeding documents with large number of stored fields (or one large dominating stored field) this can result in memory being consumed in the RAMFile but never reclaimed. Eventually, each pooled PerDoc could grow very large, even if large documents are rare.

Seems like there should be some attempt to reclaim memory from the PerDoc[] instance pool (or otherwise limit the size of RAMFiles that are cached) etc
"
0,"contrib/memory: PatternAnalyzerTest is a very, very, VERY, bad unit test. while working on something else i was started getting consistent IllegalStateExceptions from PatternAnalyzerTest -- but only when running the test from the top level.

Digging into the test, i've found numerous things that are very scary...
* instead of using assertions to test that tokens streams match, it throws an IllegalStateExceptions when they don't, and then logs a bunch of info about the token streams to System.out -- having assertion messages that tell you *exactly* what doens't match would make a lot more sense.
* it builds up a list of files to analyze using patsh thta it evaluates relative to the current working directory -- which means you get different files depending on wether you run the tests fro mthe contrib level, or from the top level build file
* the list of files it looks for include: ""../../*.txt"", ""../../*.html"", ""../../*.xml"" ... so not only do you get different results when you run the tests in the contrib vs at the top level, but different people runing the tests via the top level build file will get different results depending on what types of text, html, and xml files they happen to have two directories above where they checked out lucene.
* the test comments indicates that it's purpose is to show that PatternAnalyzer produces the same tokens as other analyzers - but points out this will fail for WhitespaceAnalyzer because of the 255 character token limit WhitespaceTokenizer imposes -- the test then proceeds to compare PaternAnalyzer to WhitespaceTokenizer, garunteeing a test failure for anyone who happens to have a text file containing more then 255 characters of non-whitespace in a row somewhere in ""../../"" (in my case: my bookmarks.html file, and the hex encoded favicon.gif images)
"
0,"Add getVersion method to IndexCommit. Returns the equivalent of IndexReader.getVersion for IndexCommit

{code}
public abstract long getVersion();
{code}"
0,"Minor refactoring to IndexFileNameFilter. IndexFileNameFilter looks like it's designed to be a singleton, however its constructor is public and its singleton member is package visible. The proposed patch changes the constructor and member to private. Since it already has a static getFilter() method, and no code in Lucene references those two, I don't think it creates any problems from an API perspective."
0,"Cutover remaining usage of pre-flex APIs. A number of places still use the pre-flex APIs.

This is actually healthy, since it gives us ongoing testing of the back compat emulation layer.

But we should at some point cut them all over to flex.  Latest we can do this is 4.0, but I'm not sure we should do them all for 3.1... still marking this as 3.1 to ""remind us"" :)"
0,"Support DateTools in QueryParser. The QueryParser currently uses the deprecated class DateField to create RangeQueries with date values. However, most users probably use DateTools to store date values in their indexes, because this is the recommended way since DateField has been deprecated. In that case RangeQueries with date values produced by the QueryParser won't work with those indexes.

This patch replaces the use of DateField in QueryParser by DateTools. Because DateTools can produce date values with different resolutions, this patch adds the following methods to QueryParser:

  /**
   * Sets the default date resolution used by RangeQueries for fields for which no
   * specific date resolutions has been set. Field specific resolutions can be set
   * with {@link #setDateResolution(String, DateTools.Resolution)}.
   *  
   * @param dateResolution the default date resolution to set
   */
  public void setDateResolution(DateTools.Resolution dateResolution);
  
  /**
   * Sets the date resolution used by RangeQueries for a specific field.
   *  
   * @param field field for which the date resolution is to be set 
   * @param dateResolution date resolution to set
   */
  public void setDateResolution(String fieldName, DateTools.Resolution dateResolution);

(I also added the corresponding getter methods).

Now the user can set a default date resolution used for all fields or, with the second method, field specific date resolutions.
The initial default resolution, which is used if the user does not set a different resolution, is DateTools.Resolution.DAY. 

Please let me know if you think we should use a different resolution as default.

I extended TestQueryParser to test this new feature.

All unit tests pass.
"
0,"TCK: PredefinedNodeTypeTest does not allow additions to the predefined node types hierarchy. As explained in section 6.7.22.2 (page 147) of the JSR 170 specification, an implementation is allowed to customize a predefined noe type definition with additional supertypes. The tests in PredefinedNodeTypeTest do not account for that and expect an exact match with the predefined node types."
0,"Modify LazyQueryResultImpl to allow resultFetchSize to be set programmatically. In our application we have a search which only shows part of a query result. We always know which part of the result needs to be shown. This means we know in advance how many results need to be fetched. I would like to be able to programmatically set resultFetchSize to minimize the number of loaded lucene docs and therefore improve the performance.
I know it is already possible to the set the resultFetchSize via the index configuration, but this number is fixed and doesn't work well in environments where you use paging for your results because if you set this number too low the query will be executed multiple times and if you set it too high too many lucene docs are loaded."
0,"Copy/Paste-Typo in toString() for SpanQueryFilter.    public String toString() {
-    return ""QueryWrapperFilter("" + query + "")"";
+    return ""SpanQueryFilter("" + query + "")"";
   }

says it all."
0,"CheckIndex API changed without backwards compaitibility. The API of CheckIndex changed. The Check function returns a CheckIndexStatus and not boolean. And JavaDocs notes the boolean return value.

I am not sure if it works, but it would be good to have the check method that returns boolean available @Deprecated, i.e.
@Deprecated public static CheckIndexStatus check(Directory dir, boolean doFix) throws IOException {
 final CheckIndexStatus stat=this.check(dir,doFix);
 return stat.clean;
}

I am not sure, if it can be done with the same method name, but it prevents drop-in-replacements of Lucene to work."
0,"Database Data Store. We want to have a database backed data store implementation.
An implementation using files is already available as part of JCR-926.
"
0,"WriteLineDocTask improvements. Make WriteLineDocTask and LineDocSource more flexible/extendable:
* allow to emit lines also for empty docs (keep current behavior as default)
* allow more/less/other fields"
0,"Inconsistent, order dependant behaviour in HttpMethodBase.getResponse*. `getReponseBodyAsString` is storing the body  and may therefore provide a valid result if the code is requesting the body as stream afterwards. If you switch the order and first call getResponseBodyAsStream and afterwards try to `getReponseBodyAsString`, the result will be `null`.

I wrote a unittest which hopefully describes the IMHO confusing behaviour:

    public void testHttpClientBodyVsStream() throws HttpException, IOException {
        final HttpClient httpClient = new HttpClient();
        final GetMethod getMethod = new GetMethod(""http://www.heise.de/"");
        final String bodyFromStream;
        final String body;
        try {
            httpClient.executeMethod(getMethod);
            body = getMethod.getResponseBodyAsString();
            bodyFromStream = IOUtils.toString(getMethod
                    .getResponseBodyAsStream());
        } finally {
            getMethod.releaseConnection();
        }
        assertEquals(body, bodyFromStream);
    }
    
    public void testHttpClientStreamVsBody() throws HttpException, IOException {
        final HttpClient httpClient = new HttpClient();
        final GetMethod getMethod = new GetMethod(""http://www.heise.de/"");
        final String bodyFromStream;
        final String body;
        try {
            httpClient.executeMethod(getMethod);
            bodyFromStream = IOUtils.toString(getMethod
                    .getResponseBodyAsStream());
            body = getMethod.getResponseBodyAsString();
        } finally {
            getMethod.releaseConnection();
        }
        // ** This will fail **
        assertEquals(body, bodyFromStream);
    }

Searching http://svn.apache.org/repos/asf/jakarta/commons/proper/httpclient/trunk/src/java/org/apache/commons/httpclient/HttpMethodBase.java I understand the outcome, but this is confusing.

I would expect the body data to be gone after calling one of the getResponse*-Methods and calling them again not to return null but even to throw an IllegalStateException. I would not store the body at all in the method.
"
0,"Change MergePolicy & MergeScheduler to be abstract base classes instead of an interfaces. This gives us freedom to add methods with default base implementation over time w/o breaking backwards compatibility.

Thanks to Hoss for raising this!"
0,DEFAULT spelled DEFALT in MoreLikeThis.java. DEFAULT is spelled DEFALT in contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis.java
0,"Hindi Analyzer. An analyzer for hindi.

below are MAP values on the FIRE 2008 test collection.
QE means expansion with morelikethis, all defaults, on top 5 docs.

||setup||T||T(QE)||TD||TD(QE)||TDN||TDN(QE)||
|words only|0.1646|0.1979|0.2241|0.2513|0.2468|0.2735|
|HindiAnalyzer|0.2875|0.3071|0.3387|*0.3791**|0.3837|0.3810|
|improvement|74.67%|55.18%|51.14%|50.86%|55.47%|39.31%|

* TD was the official measurement, highest score for this collection in FIRE 2008 was 0.3487: http://www.isical.ac.in/~fire/paper/mcnamee-jhu-fire2008.pdf

needs a bit of cleanup and more tests"
0,"+ - operators allow any amount of whitespace. As an example, (foo - bar) is treated like (foo -bar).
It seems like for +- to be treated as unary operators, they should be immediately followed by the operand."
0,Make contrib analyzers final. The analyzers in contrib/analyzers should all be marked final. None of the Analyzers should ever be subclassed - users should build their own analyzers if a different combination of filters and Tokenizers is desired.
0,"Remove synchronization in CompoundFileReader. Currently there is what seems to be unnecessary synchronization in CompoundFileReader.  This is solved by cloning the base IndexInput.  Synchronization in low level IO classes creates lock contention on highly multi threaded Lucene installations, so much so that in many cases the CPU utilization never reaches the maximum without using something like ParallelMultiSearcher."
0,"add offsets into lucene40 postings. LUCENE-3684 added support for IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS, but
only SimpleText implements it.

I think we should implement it in the other 4.0 codecs (starting with Lucene40PostingsFormat)."
0,"javacc skeleton files not regenerated. Copies of the the character stream files for javacc are checked into svn. These files were generated under javacc 3.0 (at least that's what they say, though javacc 3.2 says this too). javacc 4 complains that they are out of date but won't replace them; they must be removed before it will regenerate them.

There is one side effect of removing them: local changes are lost.  r387550 removed a couple of deprecated methods. By using the files as generated by javacc, these deprecated  methods will be readded (at least until the javacc team removes them totally). There are other changes being made to the stream files, so I woudl think it's better to live with them unmodified than to keep local versions just for this change.

If we want javacc to recreate the files, the attached patch will remove them before running javacc.

All the tests pass using both javacc3.2 and 4.0.


"
0,"Spatial uses java util logging that causes needless minor work (multiple string concat, a method call) due to not checking log level. Not sure there should be logging here - just used in two spots and looks more for debug - but if its going to be there, should check for isFineEnabled."
0,"Data Store: remove kill switch ""InternalValue.USE_DATA_STORE"". There is still a ""kill switch"" (public static final boolean USE_DATA_STORE) in the class org.apache.jackrabbit.core.value.InternalValue. In version 2.0 this constant should be removed. Also, the system property ""org.jackrabbit.useDataStore"" will no longer be used. 

It is still possible to disable the DataStore (don't include a DataStore configuration in repository.xml)."
0,"Searchability settings in PropertyDefinition. Related to JCR-1591, the new JCR 2.0 property definitions contain settings for searchability of properties.

I'm not sure how deeply we want to implement these settings (perhaps we should just hard-code the values), but in any case the relevant methods need to be implemented."
0,"Fix incorrect IndexingQueueTest logic. The IndexingQueueTest class assumes that a Session.save() call will push all pending text extraction tasks to the indexing queue, when in fact those can still be kept waiting in the VolatileIndex."
0,"automaton spellchecker. The current spellchecker makes an n-gram index of your terms, and queries this for spellchecking.
The terms that come back from the n-gram query are then re-ranked by an algorithm such as Levenshtein.

Alternatively, we could just do a levenshtein query directly against the index, then we wouldn't need
a separate index to rebuild.
"
0,Remove ItemInfo.getName() since it is redundant. I propose to remove the method getName() from org.apache.jackrabbit.spi.ItemInfo since it is redundant. The name is always the last element of the path which is available via the getPath() method.
0,"wildcardquery rewrite improvements. wildcardquery has logic to rewrite to termquery if there is no wildcard character, but
* it needs to pass along the boost if it does this
* if the user asked for a 'constant score' rewriteMethod, it should rewrite to a constant score query for consistency.

additionally, if the query is really a prefixquery, it would be nice to rewrite to prefix query.
both will enumerate the same number of terms, but prefixquery has a simpler comparison function."
0,Remove MultiTermQuery.getTerm(). Removes the field and methods in MTQ that return the pattern term.
0,"clean up serialization in the codebase. We removed contrib/remote, but forgot to cleanup serialization hell everywhere.

this is no longer needed, never really worked (e.g. across versions), and slows 
development (e.g. i wasted a long time debugging stupid serialization of 
Similarity.idfExplain when trying to make a patch for the scoring system).
"
0,"java.util.UUID.fromString() too slow. Benchmarking shows that the java.util.UUID.fromString() method is 10 times slower than the previous version we used from jackrabbit-jcr-commons. This method is quite heavily used in the query section or more generally whenever a NodeId is created from a String.

I'd like to introduce the custom String UUID parsing code again that we had in the jackrabbit-jcr-commons UUID class and use it in the NodeId(String) constructor.

WDYT?"
0,"Missing jackrabbit-rmi-service.xml from jackrabbit-jcr-rmi-1.2.1.jar. The file jackrabbit-rmi-service.xml is missing from the jackrabbit-jcr-rmi-1.2.1.jar.

The cause of the issue appears that the directory structure of the jackrabbit-jcr-rmi sub-project doesn't match the Maven 2 standard.  

To fix: src/resources should be moved to src/main/resources."
0,"Make TrieRange completely independent from Document/Field with TokenStream of prefix encoded values. TrieRange has currently the following problem:
- To add a field, that uses a trie encoding, you can manually add each term to the index or use a helper method from TrieUtils. The helper method has the problem, that it uses a fixed field configuration
- TrieUtils currently creates per default a helper field containing the lower precision terms to enable sorting (limitation of one term/document for sorting)
- trieCodeLong/Int() creates unnecessarily String[] and char[] arrays that is heavy for GC, if you index lot of numeric values. Also a lot of char[] to String copying is involved.

This issue should improve this:
- trieCodeLong/Int() returns a TokenStream. During encoding, all char[] arrays are reused by Token API, additional String[] arrays for the encoded result are not created, instead the TokenStream enumerates the trie values.
- Trie fields can be added to Documents during indexing using the standard API: new Field(name,TokenStream,...), so no extra util method needed. By using token filters, one could also add payload and so and customize everything.

The drawback is: Sorting would not work anymore. To enable sorting, a (sub-)issue can extend the FieldCache to stop iterating the terms, as soon as a lower precision one is enumerated by TermEnum. I will create a ""hack"" patch for TrieUtils-use only, that uses a non-checked Exceptionin the Parser to stop iteration. With LUCENE-831, a more generic API for this type can be used (custom parser/iterator implementation for FieldCache). I will attach the field cache patch (with the temporary solution, until FieldCache is reimplemented) as a separate patch file, or maybe open another issue for it."
0,"Some typos in the English Manual. in section 2.8.4
Per default this implementation will create no more than than 2 concurrent connections per given route and no more 20 connections in total.
Here are 2 ""than"" in this statement.

in section 3.1
Netscape engineers used to refer to it as as a ""magic cookie"" and the name stuck.
Also, here are 2 ""as"" in the sentence.

in section 5.2 'http.protocol.handle-redirects'
If this parameter is not HttpClient will handle redirects automatically.
here, a ""set"" should be put after not

in section 6.1
In certain situations it may be necessary to customize the way HTTP messages get transmitted across
the wire beyond what is possible possible using HTTP parameters in order to be able to deal nonstandard,
non-compliant behaviours.
here are 2 ""possible""."
0,"deprecate IndexWriter.addIndexes(Directory[]). Since addIndexesNoOptimize accomplishes the same thing, more efficiently, and you can always then call optimize() if you really wanted to, I think we should deprecate the older addIndexes(Directory[])."
0,"IndexReader's add/removeCloseListener should not use ConcurrentHashMap, just a synchronized set. The use-case for ConcurrentHashMap is when many threads are reading and less writing to the structure. Here this is just funny: The only reader is close(). Here you can just use a synchronized HashSet. The complexity of CHM is making this just a joke :-)"
0,"Integrate IndexReader with IndexWriter . The current problem is an IndexReader and IndexWriter cannot be open
at the same time and perform updates as they both require a write
lock to the index. While methods such as IW.deleteDocuments enables
deleting from IW, methods such as IR.deleteDocument(int doc) and
norms updating are not available from IW. This limits the
capabilities of performing updates to the index dynamically or in
realtime without closing the IW and opening an IR, deleting or
updating norms, flushing, then opening the IW again, a process which
can be detrimental to realtime updates. 

This patch will expose an IndexWriter.getReader method that returns
the currently flushed state of the index as a class that implements
IndexReader. The new IR implementation will differ from existing IR
implementations such as MultiSegmentReader in that flushing will
synchronize updates with IW in part by sharing the write lock. All
methods of IR will be usable including reopen and clone. 
"
0,"incompatible with newers versions of xml-apis. Apparently the newer version of the xml-api does not play nice with file objects passed in via a streamresult. So to get around this, I have modified the jackrabbit code RepositoryConfig.java:311 to read:
 
               transformer.transform(
                    //new DOMSource(template), new StreamResult(xml)); COMMENTED OUT! 
                    new DOMSource(template), new StreamResult(directory+""\\""+WORKSPACE_XML));

A similar issue can be found here: http://forum.java.sun.com/thread.jspa?forumID=34&threadID=563077 and a somewhat similar issue can be found logged as a bug at sun: http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=5077403"
0,"Allow to configure DB persistence managers through JDNI. Currently, DB persistence managers have hardcoded urls. Even more, they will use a single connection with the drawbacks that this have regarding concurrency, performance and transactionality. 

It would be fairly better to allow to configure DB persistence managers through JDNI references to DataSource. So giving responsability to application server. Concurrency, performance and transactionability will be highly boosted with this approach. 

This could be a sample configuration :

<PersistenceManager class=""org.apache.jackrabbit.core.state.db.SimpleDbPersistenceManager"">
       <param name=""dataSource"" value=""jdbc/JackrabbitDS""/>

        
       .... think also about a way to pass params to data source, it should be simple ....

       <param name=""schema"" value=""mysql""/>
       <param name=""schemaObjectPrefix"" value=""${
wsp.name}_""/>
       <param name=""externalBLOBs"" value=""false""/>
   </PersistenceManager>"
0,"Allow easy extensions of TopDocCollector. TopDocCollector's members and constructor are declared either private or package visible. It makes it hard to extend it as if you want to extend it you can reuse its *hq* and *totatlHits* members, but need to define your own. It also forces you to override getTotalHits() and topDocs().
By changing its members and constructor (the one that accepts a PQ) to protected, we allow users to extend it in order to get a different view of 'top docs' (like TopFieldCollector does), but still enjoy its getTotalHits() and topDocs() method implementations."
0,"Confusing Javadoc in Searchable.java. In Searchable.java, the javadoc for maxdoc() is:

  /** Expert: Returns one greater than the largest possible document number.
   * Called by search code to compute term weights.
   * @see org.apache.lucene.index.IndexReader#maxDoc()

The qualification ""expert"" and the statement ""called by search code to compute term weights"" is a bit confusing, It implies that maxdoc() somehow computes weights, which is obviously not true (what it does is explained in the other sentence). Maybe it is used as one factor of the weight, but do we really need to mention this here? "
0,"QueryWrapperFilter should not do scoring. The purpose of QueryWrapperFilter is to simply filter to include the docIDs that match the query.

Its implementation is wasteful now because it computes scores for those matching docs even though the score is unused.  We could fix this by getting a Scorer and iterating through the docs without asking for the score:

{code}
Index: src/java/org/apache/lucene/search/QueryWrapperFilter.java
===================================================================
--- src/java/org/apache/lucene/search/QueryWrapperFilter.java	(revision 707060)
+++ src/java/org/apache/lucene/search/QueryWrapperFilter.java	(working copy)
@@ -62,11 +62,9 @@
   public DocIdSet getDocIdSet(IndexReader reader) throws IOException {
     final OpenBitSet bits = new OpenBitSet(reader.maxDoc());
 
-    new IndexSearcher(reader).search(query, new HitCollector() {
-      public final void collect(int doc, float score) {
-        bits.set(doc);  // set bit for hit
-      }
-    });
+    final Scorer scorer = query.weight(new IndexSearcher(reader)).scorer(reader);
+    while(scorer.next())
+      bits.set(scorer.doc());
     return bits;
   }
{code}

Maybe I'm missing something, but this seams like a simple win?
"
0,"Throw exception for ""Multi-SortedSource"" instead of returning null. Spinoff of LUCENE-3623: currently if you addIndexes(FIR) or similar, you get a NPE deep within codecs during merge.

I think the NPE is confusing, it looks like a bug but a clearer exception would be an improvement."
0,Generate jar containing test classes.. The test classes are useful for writing unit tests for code external to the Lucene project. It would be helpful to build a jar of these classes and publish them as a maven dependency.
0,"FieldCache should include a BitSet for matching docs. The FieldCache returns an array representing the values for each doc.  However there is no way to know if the doc actually has a value.

This should be changed to return an object representing the values *and* a BitSet for all valid docs."
0,"repository-1.5.dtd: change order of main elements. Currently the order of elements in repository.xml is:
<!ELEMENT Repository (FileSystem,Security,Workspaces,Workspace,Versioning,SearchIndex?,Cluster?,DataStore?)>

I would like to change it to
<!ELEMENT Repository (Cluster?,FileSystem,DataStore?,Security,Workspaces,Workspace,Versioning,SearchIndex?)>
because I think that makes more sense.

Currently XML validation is disabled, and therefore the order of elements in the DTD does not need to match the repository.xml file. However as soon as XML validation is enabled, repository.xml files that use the wrong order will no longer work (the repository can not be started).

There is a request to enable XML validation at http://issues.apache.org/jira/browse/JCR-1462
"
0,"Support synonym searches. Jackrabbit should support synonym searches in the jcr:contains function like Google does.

Example:

//element(*, nt:resource)[jcr:contains(., '~food')]

-> finds all nt:resource nodes that contain the word food or synonyms for food."
0,optimize MultiTermEnum/MultiTermDocs. Optimize MultiTermEnum and MultiTermDocs to avoid seeks on TermDocs that don't match the term.
0,"Unnecessary hasItemState() call in SessionItemStateManager. At the end of  SessionItemStateManager.getItemState(ItemId) the underlying item state manager is first asked whether it contains the item and then it is retrieved. In case the item state manager does not know the item a NoSuchItemStateException is thrown.

The initial check is unnecessary because getItemState() on the underlying manager will also throw the exception if the item does not exist."
0,"Need stopwords and stoptags lists for default Japanese configuration. Stopwords and stoptags lists for Japanese needs to be developed, tested and integrated into Lucene."
0,"A replacement for AsciiFoldingFilter that does a more thorough job of removing diacritical marks or non-spacing modifiers.. The ISOLatin1AccentFilter takes Unicode characters that have diacritical marks and replaces them with a version of that character with the diacritical mark removed.  For example  becomes e.  However another equally valid way of representing an accented character in Unicode is to have the unaccented character followed by a non-spacing modifier character (like this:    )    The ISOLatin1AccentFilter doesn't handle the accents in decomposed unicode characters at all.    Additionally there are some instances where a word will contain what looks like an accented character, that is actually considered to be a separate unaccented character  such as    but which to make searching easier you want to fold onto the latin1  lookalike  version   L  .   

The UnicodeNormalizationFilter can filter out accents and diacritical marks whether they occur as composed characters or decomposed characters, it can also handle cases where as described above characters that look like they have diacritics (but don't) are to be folded onto the letter that they look like (   -> L )"
0,"Implement ""ignoreCookies"" CookieSpec. It would be useful to Implement an ""ignoreCookies"" CookieSpec, as was done in Commons HC 3.1

This should be registered by DefaultHttpClient.createCookieSpecRegistry().

Patch to follow."
0,"Avoid path resolution in case of non-wildcard ACEs (follow-up to JCR-2573). adding the ability to specify wildcard-ac-entries in the default resource based access control management lead to
always resolving the id passed to AccessControlProvider#canRead in order to be able to properly evaluate
any wildcard-aces present.

this could be improved with minor refactoring that postpones the path resolution and omitting it if there are no
wildcard-aces to compare with."
0,"cache should invalidate obsoleted entries mentioned in Content-Location. From http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.6:

If a cache receives a successful response whose Content-Location field matches that of an existing cache entry for the same Request-URI, whose entity-tag differs from that of the existing entry, and whose Date is more recent than that of the existing entry, the existing entry SHOULD NOT be returned in response to future requests and SHOULD be deleted from the cache.

Current caching module doesn't do this (yet). As this is a recommendation (SHOULD) and not a requirement (MUST) I am marking this as an improvement rather than a bug.
"
0,"Data Store: garbage collection should ignore removed items. The GCConcurrentTest fails sometimes. The problem is that
the garbage collector stops if a node or property was removed
while scanning. Instead, the garbage collector should ignore the
removed item and continue.

-------------------------------------------------------------------------------
Test set: org.apache.jackrabbit.core.data.TestAll
-------------------------------------------------------------------------------
Tests run: 19, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 21.282 sec <<< FAILURE!
testGC(org.apache.jackrabbit.core.data.GCConcurrentTest)  Time elapsed: 0.578 sec  <<< ERROR!
javax.jcr.InvalidItemStateException: 5fc4130b-aee4-4bef-b51d-21420d78f315/{}data: the item does not exist anymore
	at org.apache.jackrabbit.core.ItemImpl.sanityCheck(ItemImpl.java:144)
	at org.apache.jackrabbit.core.PropertyImpl.getPropertyState(PropertyImpl.java:89)
	at org.apache.jackrabbit.core.PropertyImpl.getType(PropertyImpl.java:773)
	at org.apache.jackrabbit.core.data.GarbageCollector.recurse(GarbageCollector.java:310)
	at org.apache.jackrabbit.core.data.GarbageCollector.recurse(GarbageCollector.java:327)
	at org.apache.jackrabbit.core.data.GarbageCollector.recurse(GarbageCollector.java:327)
	at org.apache.jackrabbit.core.data.GarbageCollector.scanNodes(GarbageCollector.java:193)
	at org.apache.jackrabbit.core.data.GarbageCollector.scan(GarbageCollector.java:177)
	at org.apache.jackrabbit.core.data.GCThread.run(GCThread.java:52)
	at java.lang.Thread.run(Thread.java:619)
"
0,"Occur incompletely implemented for remote use.. Occur does not implement readResolve() creating problems for
ParallelMultiSearcher y."
0,"Mark Fieldable as allowing some changes in 2.x future releases. See http://lucene.markmail.org/message/4k2gqs3n7coh4lmd?q=Fieldable

1. We mark Fieldable as being subject to change. We heavily advertise (on java-dev and java-user and maybe general) that in the next minor release of Lucene (2.4), Fieldable will be changing. It is also marked at the top of CHANGES.txt very clearly for all the world to see. Since 2.4 is probably at least a month away, I think this gives anyone with a pulse enough time to react. "
0,"highlight-vs-vector-highlight.alg is unfair. highlight-vs-vector-highlight.alg uses EnwikiQueryMaker which makes SpanQueries, but FastVectorHighlighter simply ignores SpanQueries."
0,"Replace commons-logging with jcl-over-slf4j in jackrabbit-webdav. In JCR-1631 we moved the exclusion rule against the transitive commons-logging dependency from commons-httpclient to higher level components like jackrabbit-jca and jackrabbit-webapp.

However, I'm having some trouble with commons-logging showing up in other downstream projects that depend directly on jackrabbit-webapp or jackrabbit-jcr-server. Thus I'd like to revert the JCR-1631 solution and push the exclusion rule back to jackrabbit-webapp and replace the commons-logging dependency with jcl-over-slf4j. Downstream projects that already use commons-logging can still exclude the jcl-over-slf4j dependency."
0,"Automatic repository shutdown. Currently Jackrabbit relies on two mechanisms for safely shutting down a repository:

    1) client application invoking RepositoryImpl.shutdown(), or
    2) the shutdown hook installed by RepositoryImpl being run

Both of these mechanisms have problems:

    1) The shutdown() method is not a part of the JCR API, thus making the client application depend on a Jackrabbit-specific feature
    2) In some cases the shutdown hook is not properly run (see issues JCR-120 and JCR-233)

I think the JCR spec thinks of the Repository and Session interfaces as being somewhat similar to the JDBC DataSource and Connection interfaces. The Repository instances have no real lifecycle methods while the Session instances have clearly specified login and logout steps. (DataSource.getConnection() = Repository.login(), Session.logout() = Connection.close()) However the Jackrabbit implementation defines an explicit lifecycle for the RepositoryImpl instances.

This causes problems especially for container environments (JNDI, Spring) where it is hard or even impossible to specify a shutdown mechanism for resource factories like the Repository instances. The current solution for such environments is to use a shutdown hook, but as reported this solution does not work perfectly in all cases.

How about if we bound the RepositoryImpl lifecycle to the lifecycles of the instantiated Sessions. A RepositoryImpl instance could initialize (and lock) the repository when the first session is opened and automatically shut down when the last session has logged out. As long as the sessions are properly logged out (or finalized by the garbage collector) there would be no need for an explicitly RepositoryImpl.shutdown() call. The current behaviour of pre-initializing the repository and shutting down during a shutdown hook could be enabled with a configuration option for environments (like global JNDI resources) in which the shutdown hooks work well.
"
0,"Implementation of Delete method. The HTTP request method, Delete, had not been implemented. I needed it and created an HttpDelete class modeled after HttpGet."
0,"Clicking on the ""More Results"" link in luceneweb.war demo results in ArrayIndexOutOfBoundsException. Summary says it all."
0,"replace UUID strings by UUID classes in NodeId, etc... Currently the UUIDs of the nodes are stored as Strings in the ItemIds and ItemStates and cause alot of overhead throughout jackrabbit. they should be replaced by a fast implementation of a UUID class."
0,"Improve jcr decorator in jcr-ext. The jcr decorator in jcr-ext does not cover all the necessary interfaces of the jcr api. It may happen that a client loses the decoration layer when accessing properties.

I've added decoration for several jcr interfaces to ensure that the decoration layer is never left.

The attached patch also removes the classes related to decorator chaining. I found it hard to understand the purpose of those classes and decided to remove them from the default implementation. If we want to keep those classes they should be less intrusive.

I've also noticed that there are class name clashes, specifically the package org.apache.jackrabbit.name contains classes that are also present in the jackrabbit and jackrabbit-commons jar file. I propose to move the respective classes in jcr-ext to a different package or remove them in favor of the jackrabbit-commons classes.

Let me know if I should commit the the patch.

Thanks"
0,"Optimize PhraseQuery. Looking the scorers for PhraseQuery, I think there are some speedups
we could do:

  * The AND part of the scorer (which advances to the next doc that
    has all the terms), in PhraseScorer.doNext, should do the same
    optimizing as BooleanQuery's ConjunctionScorer, ie sort terms from
    rarest to most frequent.  I don't think it should use a linked
    list/firstToLast() that it does today.

  * We do way too much work now when .score() is not called, because
    we go and find all occurrences of the phrase in the doc, whereas
    we should stop only after finding the first and then go and count
    the rest if .score() is called.

  * For the exact case, I think we can use two int arrays to find the
    matches.  The first array holds the count of how many times a term
    in the phrase ""matched"" a phrase starting at that position.  When
    that count == the number of terms in the phrase, it's a match.
    The 2nd is a ""gen"" array (holds docID when that count was last
    touched), to avoid clearing.  Ie when incrementing the count, if
    the docID != gen, we reset count to 0.  I think this'd be faster
    than the PQ we now use.  Downside of this is if you have immense
    docs (position gets very large) we'd need 2 immense arrays.

It'd be great to do LUCENE-1252 along with this, ie factor
PhraseScorer into two AND'd sub-scorers (LUCENE-1252 is open for
this).  The first one should be ConjunctionScorer, and the 2nd one
checks the positions (ie, either the exact or sloppy scorers).  This
would mean if the PhraseQuery is AND'd w/ other clauses (or, a filter
is applied) we would save CPU by not checking the positions for a doc
unless all other AND'd clauses accepted the doc.
"
0,"Add Searcher.search(Query, int). Now that we've deprecated Hits (LUCENE-1290), I think we should add this trivial convenience method to Searcher, which is just sugar for Searcher.search(Query, null, int) ie null filter, returning a TopDocs.

This way there is a simple API for users to retrieve the top N results for a Query.
"
0,Lower log level for index updates from queue. The log level is currently at info and should be lowered to debug.
0,"A handy utility class for tracking deprecated overridden methods. This issue provides a new handy utility class that keeps track of overridden deprecated methods in non-final sub classes. This class can be used in new deprecations.

See the javadocs for an example."
0,Systemrequirements should say 1.5 instead of 1.4. The website still says Java 1.4 but it should say 1.5
0,"Caching in QueryHandler does not scale well. Caching in class CachingIndexReader uses too much memory. It uses around 500 bytes per node and does not use any strategy to limit the cache.

This improvement covers two goals:
- lower per-node memory cost for caching
- implement a caching strategy using e.g LRU algorithm"
0,"intermittent failures of  TestTimeLimitedCollector.testTimeoutMultiThreaded in nightly tests. Occasionly TestTimeLimitedCollector.testTimeoutMultiThreaded fails. e.g. with this output:

{noformat}
   [junit] ------------- Standard Error -----------------
   [junit] Exception in thread ""Thread-97"" junit.framework.AssertionFailedError: no hits found!
   [junit]     at junit.framework.Assert.fail(Assert.java:47)
   [junit]     at junit.framework.Assert.assertTrue(Assert.java:20)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.doTestTimeout(TestTimeLimitedCollector.java:152)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.access$100(TestTimeLimitedCollector.java:38)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector$1.run(TestTimeLimitedCollector.java:231)
   [junit] Exception in thread ""Thread-85"" junit.framework.AssertionFailedError: no hits found!
   [junit]     at junit.framework.Assert.fail(Assert.java:47)
   [junit]     at junit.framework.Assert.assertTrue(Assert.java:20)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.doTestTimeout(TestTimeLimitedCollector.java:152)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.access$100(TestTimeLimitedCollector.java:38)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector$1.run(TestTimeLimitedCollector.java:231)
   [junit] ------------- ---------------- ---------------
   [junit] Testcase: testTimeoutMultiThreaded(org.apache.lucene.search.TestTimeLimitedCollector):      FAILED
   [junit] some threads failed! expected:<50> but was:<48>
   [junit] junit.framework.AssertionFailedError: some threads failed! expected:<50> but was:<48>
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.doTestMultiThreads(TestTimeLimitedCollector.java:255)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.testTimeoutMultiThreaded(TestTimeLimitedCollector.java:220)
   [junit]
{noformat}

Problem either in test or in TimeLimitedCollector."
0,"NRTManager shouldn't expose its private SearcherManager. Spinoff from LUCENE-3769.

To actually obtain an IndexSearcher from NRTManager, it's a 2-step process now.

You must .getSearcherManager(), then .acquire() from the returned SearcherManager.

This is very trappy... because if the app incorrectly calls maybeReopen on that private SearcherManager (instead of NRTManager.maybeReopen) then it can unexpectedly cause threads to block forever, waiting for the necessary gen to become visible.  This will be hard to debug... I don't like creating trappy APIs.

Hopefully once LUCENE-3761 is in, we can fix NRTManager to no longer expose its private SM, instead subclassing ReferenceManaager.

Or alternatively, or in addition, maybe we factor out a new interface (SearcherProvider or something...) that only has acquire and release methods, and both NRTManager and ReferenceManager/SM impl that, and we keep NRTManager's SM private."
0,"Custom build.xml for binary distributions. The binary files of a distribution come with the demo sources
and a build.xml file. However, the build.xml doesn't work for
the binary distribution, so it can't be used to build the 
demos.

This problem was notices the first time when release 2.1 was
made. Before we ship 2.2 we should fix this."
0,"Logging into a repository with a big version history takes a long time. Wenn a SessionImpl instance is created, the VersionManager.getVirtualItemStateProvider method is called. This method - amongst other things - loads the complete (!) version history into memory and walks through it to do some mapping.

Besides taking a long time (near 1 minute just to get the version history through PersistentVersionManager.getVersionHistories()) mapping the version histories ultimately results in an ""OutOfMemoryError"".

Currently there are 768 version histories and this is only a very small fraction of the expected final number of version histories in my application"
0,"move contrib/benchmark to modules/benchmark. I think we should move lucene/contrib/benchmark to a shared modules/benchmark, so you can easily benchmark anything (lucene, solr, other modules like analysis or whatever).

For example, if you want to do some benchmarking of something in solr (LUCENE-2844) you should be able to do this.
Another example is simply being able to benchmark an analyzer definition from a schema.xml, its more convenient than writing the equivalent java analyzer just for benchmarking.

"
0,"Remove remaining deprecations in document package. Remove different deprecated APIs:
- Field.Index.NO_NORMS, etc.
- Field.binaryValue()
- getOmitTf()/setOmitTf()
"
0,"test case (TCK) maintenance for JCR 2.0. Umbrella issue for changes/additions to JUnit test cases, setup and config."
0,"Changes for TrieRange in FilteredTermEnum and MultiTermQuery improvement. This is a patch, that is needed for the MultiTermQuery-rewrite of TrieRange (LUCENE-1602):
- Make the private members protected, to have access to them from the very special TrieRangeTermEnum 
- Fix a small inconsistency (docFreq() now only returns a value, if a valid term is existing)
- Improvement of MultiTermFilter.getDocIdSet to return DocIdSet.EMPTY_DOCIDSET, if the TermEnum is empty (less memory usage) and faster.
- Add the getLastNumberOfTerms() to MultiTermQuery for statistics on different multi term queries and how may terms they affect, using this new functionality, the improvement of TrieRange can be shown (extract from test case there, 10000 docs index, long values):
{code}
[junit] Average number of terms during random search on 'field8':
[junit]  Trie query: 244.2
[junit]  Classical query: 3136.94
[junit] Average number of terms during random search on 'field4':
[junit]  Trie query: 38.3
[junit]  Classical query: 3018.68
[junit] Average number of terms during random search on 'field2':
[junit]  Trie query: 18.04
[junit]  Classical query: 3539.42
{code}

All core tests pass.
"
0,"HttpParams doesn't document its key values. http://hc.apache.org/httpcomponents-core/httpcore/apidocs/org/apache/http/params/HttpParams.html should either list the meaningful parameter names with the meanings of their values, or should link to the other classes like HttpClientParams and AuthParams that make it usable. Probably, each class that uses HttpParams should also describe the meaningful values so that human readers find the descriptions however we look for them."
0,"Reducing buffer sizes for TermDocs.. From java-dev: 
 
On Friday 09 September 2005 00:34, Doug Cutting wrote: 
> Paul Elschot wrote: 
> > I suppose one of these cases are when many terms are used in a query.  
> > Would it be easily possible to make the buffer size for a term iterator 
> > depend on the numbers of documents to be iterated? 
> > Many terms only occur in a few documents, so this could be a  
> > nice win on total buffer size for the many terms case. 
>  
> This would not be too difficult. 
>  
> Look in SegmentTermDocs.java. The buffer may be allocated when the  
> parent's stream is first cloned, but clone() won't allocate a buffer if  
> the source hasn't had a buffer allocated yet, and nothing should perform  
> i/o directly on the parent's freqStream, so in practice a buffer should  
> not be allocated until the first read is performed on the clone. 
 
I tried delaying the buffer allocation in BufferedIndexInput by 
using this clone() method: 
 
 public Object clone() { 
  BufferedIndexInput clone = (BufferedIndexInput)super.clone(); 
  clone.buffer = null; 
  clone.bufferLength = 0; 
  clone.bufferPosition = 0; 
  clone.bufferStart = getFilePointer();  
  return clone; 
 } 
 
With this all term document iterators seem to be empty, no 
query in the test cases gives any results, for example TestDemo 
and TestBoolean2. 
As far as I can see, this delaying should work, but it doesn't and 
I have no idea why. 
 
End of quote from java-dev. 
 
Doug replied that at a glance this clone method looks good. 
Without this delayed buffer allocation, a reduced buffer size 
for TermDocs cannot be implemented easily."
0,"benchmark tests always fail on windows because directory cannot be removed. This seems to be a bug recently introduced. I have no idea what's wrong. Attached is a log file, reproduces everytime.

"
0,"benchmark pkg: specify trec_eval submission output from the command line. the QueryDriver for the trec benchmark currently requires 4 command line arguments.
the third argument is ignored (i typically populate this with ""bogus"")
Instead, allow the third argument to specify the submission.txt file for trec_eval.

while I am here, add a usage() documenting what the arguments to this driver program do."
0,"allow tests to use different Directory impls. Now that all tests use MockRAMDirectory instead of RAMDirectory, they are all picky like windows and force our tests to
close readers etc before closing the directory.

I think we should do the following:
# change new MockRAMDIrectory() in tests to .newDirectory(random)
# LuceneTestCase[J4] tracks if all dirs are closed at tearDown and also cleans up temp dirs like solr.
# factor out the Mockish stuff from MockRAMDirectory into MockDirectoryWrapper
# allow a -Dtests.directoryImpl or simpler to specify the default Directory to use for tests: default being ""random""

i think theres a chance we might find some bugs that havent yet surfaced because they are easier to trigger with FSDir
Furthermore, this would be beneficial to Directory-implementors as they could run the entire testsuite against their Directory impl, just like codec-implementors can do now.
"
0,"HttpMethodBase does not compile on JDK prior to 1.3. reason is the use of URL.getPath() and URL.getQuery() within method
processRedirectResponse.

should use URIUtil.getPath and URIUtil.getQuery instead.

so, HttpMethodBase around line 952:

//update the current location with the redirect location
setPath(URIUtil.getPath(redirectUrl.toString()));
setQueryString(URIUtil.getQuery(redirectUrl.toString()));

thanks,

marius"
0,"TokenStream API javadoc improvements. - Change or remove experimental warnings of new TokenStream API
- Improve javadocs for deprecated Token constructors
- javadocs for TeeSinkTokenStream.SinkFilter"
0,Add plugable mechanism for import/export of webdav-server. add plugable mechanism to improve flexible configuration of the jcr-server
0,"Support multi-selector OR constraints in join queries. Our current join implementation doesn't support OR constraints that refer to more than one selector. For example the following query is not possible:

    SELECT a.* FROM [my:type] AS a INNER JOIN [my:type] as b ON a.foo = b.bar WHERE a.baz = 'x' OR b.baz = 'y'

This limitation is a result of the way the join execution splits the query into per-selector components and merges the result based on the given join condition.

A simple but often inefficient solution would be to process such OR constraints as post-processing filters like we already do for some other more complex constraint types."
0,"javadoc often has <code> without </code>. Just to mention it:
there are a lot of javadoc comments that read like
 <tt>false</ff> otherwise.
which prints the rest of the document in <tt>
can't give a complete list, it happens very often.
To check, just go to the bottom of a page and see if it appears in <tt> or 
<code>"
0,"NearSpansOrdered does not lazy load payloads as the PayloadSpans javadoc implies. Best would be to lazy load, but I don't see how with the current algorithm. Short that, we should add an option to ignore payloads - otherwise, if you are doing non payload searching, but the payloads are present, they will be needlessly loaded.

Already added this to LUCENE-1748, but spinning from that issue to this - patch to follow when LUCENE-1748 is committed."
0,"Add optional packing to FST building. The FSTs produced by Builder can be further shrunk if you are willing
to spend highish transient RAM to do so... our Builder today tries
hard not to use much RAM (and has options to tweak down the RAM usage,
in exchange for somewhat lager FST), even when building immense FSTs.

But for apps that can afford highish transient RAM to get a smaller
net FST, I think we should offer packing.
"
0,"Core Test should not have dependencies on the Demo code. The TestDoc.java Test file has a dependency on the Demo FileDocument code.  Some of us don't keep the Demo code around after downloading, so this breaks the build.

Patch will be along shortly"
0,"Fix Document.getFieldables and others to never return null. Document.getFieldables (and other similar methods) returns null if there are no fields matching the name.  We can avoid NPE in consumers of this API if instead we return an empty array.

Spinoff from http://markmail.org/message/g2nzstmce4cnf3zj"
0,"Deprecate org.apache.jackrabbit.api.JackrabbitNodeTypeManager in 2.0 and 1.6. The JackrabbitNodeTypeManager defines 3 methods that can be removed for version 2.0 since they are no longer needed:

    NodeType[] registerNodeTypes(InputSource in)  throws SAXException, RepositoryException;
    NodeType[] registerNodeTypes(InputStream in, String contentType)  throws IOException, RepositoryException;

those deal with directly register nodetypes from a XML or CND source. since we don't want to support XML serialization any longer, and the CND import can be easily done using the spi-commons CompactNodeTypeDefReader with the new JCR2.0 node type registration. if the XML is to be supported, i suggest to detach the reader similar to the CND one.

    boolean hasNodeType(String name) throws RepositoryException;

this is now in the JCR2.0 api
"
0,"Convert NumericUtils and NumericTokenStream to use BytesRef instead of Strings/char[]. After LUCENE-2302, we should use TermToBytesRefAttribute to index using NumericTokenStream. This also should convert the whole NumericUtils to use BytesRef when converting numerics."
0,Apply the supplied patch. Sets 2 variable in the base class to protected. The patch attached to the main task contains minimal changes to allow the HttpMethodBase class to be overloaded by base class.
0,Move Content-Type to the RequestEntity. The content type is really a property of the RequestEntity.  It should be moved there.
0,"Arabic Analyzer: Stopwords list needs enhancement. The provided Arabic stopwords list needs some enhancements (e.g. it contains a lot of words that not stopwords, and some cleanup) . patch will be provided with this issue."
0,[PATCH] Javadoc correction for Scorer.java.  
0,spi2dav: create RepositoryFactory implementation. 
0,"RAMDirectory.close() should have a comment about not releasing any resources. I wrongly assumed that calling RAMDirectory.close() would free up the memory occupied by the RAMDirectory.
It might be helpful to add a javadoc comment that warns users that RAMDirectory.close() is a no-op, since it might be a common assumption that close() would release resources."
0,"Disk full during addIndexes(Directory[]) can corrupt index. This is a spinoff of LUCENE-555

If the disk fills up during this call then the committed segments file can reference segments that were not written.  Then the whole index becomes unusable.

Does anyone know of any other cases where disk full could corrupt the index?

I think disk full should worse lose the documents that were ""in flight"" at the time.  It shouldn't corrupt the index."
0,"if the build fails to download JARs for contrib/db, just skip its tests. Every so often our nightly build fails because contrib/db is unable to download the necessary BDB JARs from http://downloads.osafoundation.org.  I think in such cases we should simply skip contrib/db's tests, if it's the nightly build that's running, since it's a false positive failure."
0,BytesHash. This issue will have the BytesHash separated out from LUCENE-2186
0,Add missing license headers. The RAT tool (http://code.google.com/p/arat/) points out a few files within Jackrabbit trunk that are currently missing the correct license header. We should fix those.
0,"TimeLimitingCollector starts thread in static {} with no way to stop them. See the comment in LuceneTestCase.

If you even do Class.forName(""TimeLimitingCollector"") it starts up a thread in a static method, and there isn't a way to kill it.

This is broken."
0,"Remove unused imports. With all of the churn recently, now seems like the opportune time to do some import cleanup"
0,"war missing jcr jar . dropping the latest war (from latest svn) presents with this error when i point my browser to http://localhost:8080/jackrabbit-webapp-1.4-SNAPSHOT/. Simple solution is to make sure the jcr-1.0.jar is added to the generated war.

org.apache.jasper.JasperException: Unable to compile class for JSP: 

An error occurred at line: 1 in the generated java file
The type javax.jcr.Repository cannot be resolved. It is indirectly referenced from required .class files

An error occurred at line: 9 in the generated java file
The import javax.jcr.Repository cannot be resolved

An error occurred at line: 27 in the jsp file: /index.jsp
Repository cannot be resolved to a type
24: </head>
25: <body style=""font-family:monospace"">
26: <%
27:     Repository rep;
28:     try {
29:         rep = RepositoryAccessServlet.getRepository(pageContext.getServletContext());
30:     } catch (Throwable e) {


An error occurred at line: 84 in the jsp file: /index.jsp
Repository.REP_VENDOR_URL_DESC cannot be resolved to a type
81:     </li>
82: </ol>
83: <p/>
84: <hr size=""1""><em>Powered by <a href=""<%= rep.getDescriptor(Repository.REP_VENDOR_URL_DESC) %>""><%= rep.getDescriptor(Repository.REP_NAME_DESC)%></a> version <%= rep.getDescriptor(Repository.REP_VERSION_DESC) %>.</em>
85: </body>
86: </html>


An error occurred at line: 84 in the jsp file: /index.jsp
Repository.REP_NAME_DESC cannot be resolved to a type
81:     </li>
82: </ol>
83: <p/>
84: <hr size=""1""><em>Powered by <a href=""<%= rep.getDescriptor(Repository.REP_VENDOR_URL_DESC) %>""><%= rep.getDescriptor(Repository.REP_NAME_DESC)%></a> version <%= rep.getDescriptor(Repository.REP_VERSION_DESC) %>.</em>
85: </body>
86: </html>


An error occurred at line: 84 in the jsp file: /index.jsp
Repository.REP_VERSION_DESC cannot be resolved to a type
81:     </li>
82: </ol>
83: <p/>
84: <hr size=""1""><em>Powered by <a href=""<%= rep.getDescriptor(Repository.REP_VENDOR_URL_DESC) %>""><%= rep.getDescriptor(Repository.REP_NAME_DESC)%></a> version <%= rep.getDescriptor(Repository.REP_VERSION_DESC) %>.</em>
85: </body>
86: </html>


Stacktrace:
	org.apache.jasper.compiler.DefaultErrorHandler.javacError(DefaultErrorHandler.java:92)
	org.apache.jasper.compiler.ErrorDispatcher.javacError(ErrorDispatcher.java:330)
	org.apache.jasper.compiler.JDTCompiler.generateClass(JDTCompiler.java:423)
	org.apache.jasper.compiler.Compiler.compile(Compiler.java:308)
	org.apache.jasper.compiler.Compiler.compile(Compiler.java:286)
	org.apache.jasper.compiler.Compiler.compile(Compiler.java:273)
	org.apache.jasper.JspCompilationContext.compile(JspCompilationContext.java:566)
	org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:317)
	org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:320)
	org.apache.jasper.servlet.JspServlet.service(JspServlet.java:266)
	javax.servlet.http.HttpServlet.service(HttpServlet.java:803)

note The full stack trace of the root cause is available in the Apache Tomcat/6.0.14 logs."
0,"TestIndexReaderReopen nightly build failure. An interesting failure in last night's build (http://hudson.zones.apache.org/hudson/job/Lucene-trunk/920).

I think the root cause wast he AIOOB exception... all the ""lock obtain timed out"" exceptions look like they cascaded.

{code}
    [junit] Testsuite: org.apache.lucene.index.TestIndexReaderReopen
    [junit] Lock obtain timed out: org.apache.lucene.store.SingleInstanceLock@6ac615: write.lock)
    [junit] Tests run: 15, Failures: 1, Errors: 0, Time elapsed: 31.087 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] java.lang.ArrayIndexOutOfBoundsException: Array index out of range: 148
    [junit] 	at org.apache.lucene.util.BitVector.getAndSet(BitVector.java:74)
    [junit] 	at org.apache.lucene.index.SegmentReader.doDelete(SegmentReader.java:908)
    [junit] 	at org.apache.lucene.index.IndexReader.deleteDocument(IndexReader.java:1122)
    [junit] 	at org.apache.lucene.index.DirectoryReader.doDelete(DirectoryReader.java:521)
    [junit] 	at org.apache.lucene.index.IndexReader.deleteDocument(IndexReader.java:1122)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen$8.modifyIndex(TestIndexReaderReopen.java:638)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen.refreshReader(TestIndexReaderReopen.java:840)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen.access$400(TestIndexReaderReopen.java:47)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen$9.run(TestIndexReaderReopen.java:681)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen$ReaderThread.run(TestIndexReaderReopen.java:822)
    [junit] org.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: org.apache.lucene.store.SingleInstanceLock@88d319: write.lock
    [junit] 	at org.apache.lucene.store.Lock.obtain(Lock.java:85)
    [junit] 	at org.apache.lucene.index.DirectoryReader.acquireWriteLock(DirectoryReader.java:666)
    [junit] 	at org.apache.lucene.index.IndexReader.setNorm(IndexReader.java:994)
    [junit] 	at org.apache.lucene.index.IndexReader.setNorm(IndexReader.java:1020)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen$8.modifyIndex(TestIndexReaderReopen.java:634)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen.refreshReader(TestIndexReaderReopen.java:840)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen.access$400(TestIndexReaderReopen.java:47)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen$9.run(TestIndexReaderReopen.java:681)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen$ReaderThread.run(TestIndexReaderReopen.java:822)
    ...
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testThreadSafety(org.apache.lucene.index.TestIndexReaderReopen):	FAILED
    [junit] Error occurred in thread Thread-36:
    [junit] Lock obtain timed out: org.apache.lucene.store.SingleInstanceLock@6ac615: write.lock
    [junit] junit.framework.AssertionFailedError: Error occurred in thread Thread-36:
    [junit] Lock obtain timed out: org.apache.lucene.store.SingleInstanceLock@6ac615: write.lock
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen.testThreadSafety(TestIndexReaderReopen.java:764)
    [junit] 
    [junit] 
{code}"
0,"Improved background text extraction. As recently discussed on the mailing list (see http://markmail.org/message/syt7lc2guzapt7la), the current approach to text extraction in background threads doesn't work that well especially with the Tika-based extractors that support streamed parsing of many document types.

Also, we currently *all* of the extracted text streams are buffered into Strings before being passed into the Lucene index. It would be good if we could somehow get back to passing just Readers to Lucene."
0,FST serialization and deserialization from plain DataInput/DataOutput streams.. Currently the automaton can be saved only to a Directory instance (IndexInput/ IndexOutput).
0,"Add IndexReader.acquire() and release() methods using IndexReader's ref counting. From: http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200807.mbox/%3cPine.OSX.4.64.0807170752080.1708@c5850-a3-2-62-147-22-102.dial.proxad.net%3e

I have a server where a bunch of threads are handling search requests. I
have a another process that updates the index used by the search server and
that asks the searcher server to reopen its index reader after the updates
completed.

When I reopen() the index reader, I also close the old one (if the reopen()
yielded a new instance). This causes problems for the other threads that
are currently in the middle of a search request.

I'd like to propose the addition of two methods, acquire() and release() 
(attached to this bug report), that increment/decrement the ref count that IndexReader 
instances currently maintain for related purposes. That ref count prevents 
the index reader from being actually closed until it reaches zero.

My server's search threads, thus acquiring and releasing the index reader 
can be sure that the index reader they're currently using is good until 
they're done with the current request, ie, until they release() it.
"
0,"Improve Greek analysis. * Changed tokenstreams to CharTermAttribute
* Moved stopwords out of private String[] to a txt file (for use by Solr, etc)
* Removed TODO / fixed unicode conformance of GreekLowerCaseFilter
* Reformatted touched files to normal indentation
* Added inflectional stemmer (Ntais algorithm)

all the changes are backwards compatible with Version."
0,Nightly Builds. Nightly builds for Lucene are HUGE due to the inclusion of the contrib/benchmark temp and work directories.  These directories should be excluded.
0,"Clustering. Implement basic clustering, i.e. make two or more repositories available at the same time,  allowing them to stay in sync with changes applied to only one of them."
0,"Jcr-Server Contrib: Remove JDOM dependencies. JDOM has been replace throught the jackrabbit project except the jcr-server contrib.

"
0,"add support to caching module for RFC 5861 (stale-on-error and stale-while-revalidate). These are Cache-Control extensions that allow an origin server to specify some additional behavior for stale cache entries. Stale-on-error configurations allow a cache to continue serving stale content for a certain period of time if a revalidation fails, and stale-while-revalidate similarly allows revalidation to occur asynchronously. Some reverse proxies such as Squid can be configured to understand these headers, which means that some origin servers are probably sending them, and that we can likewise take advantage of them.
"
0,"Links in Section ""Example Code"" are broken. Steps to Reproduce: Go to http://hc.apache.org/user-docs.html
Click of one of the Links in the Section ""Example Code"""
0,"BundleFsPersistenceManager: remove deprecated settings. Some settings of the BundleFsPersistenceManager are not used internally and should be removed:

blobFSInitialCache, blobFSMaximumCache, itemFSBlockSize, itemFSInitialCache, itemFSMaximumCache"
0,"LuceneTestCase should check for modifications on System properties. - fail the test if changes have been detected.
- revert the state of system properties before the suite.
- cleanup after the suite."
0,"repository.xml: throw an exception on error. Currently, unsupported parameters in repository.xml and workspace.xml are ignored.
To find problems earlier, such problems should result in an exception,
and starting such a repository should not be possible.
The same should happen for unsupported values.

For currently unavailable options
(such as text extraction filter classes if the class is not in the classpath),
at least a warning should be written to the error log, or an error should be thrown.
"
0,"TSCCM code cleanup. The ThreadSafeClientConnectionManager, or rather it's ConnPoolByRoute, needs plenty of cleanup.
- use long + TimeUnit for timeout intervals (Java 5 style)
- compute timeout end date once instead of remaining interval
- review which methods should acquire the pool lock,
  and which should expect the caller to have done that
- use factory methods to instantiate some of the helper objects
"
0,"Add method getID to interface ItemInfo. ItemInfo is the base for NodeInfo and PropertyInfo both of which declare a method getId with return type NodeId and PropertyId, respectively. With Java 1.5. it is now possible to override a method with a covariant return type. I thus propose to introduce a method getId on ItemInfo with return type ItemId which is the common base type of NodeId and PropertyId."
0,"Real In-memory Repository. For unit tests it is desirable to have an in-memory repository which holds its whole data(even PropertyType.BINARY) in memory.
The actual implementation of org.apache.jackrabbit.core.persistence.mem.InMemPersistenceManager uses the FileSystemBLOBStore along with LocalFileSystem.
The binary properties are serialized to the OS-Filesystem.
"
0,"SimpleJbossAccessManager. http://wiki.apache.org/jackrabbit/SimpleJbossAccessManager

Code contribution
"
0,"Core Tests should call Version based ctors instead of deprecated default ctors. LUCENE-2183 introduced new ctors for all CharTokenizer subclasses. Core - tests should use those ctors with Version.LUCENE_CURRENT instead of the the deprecated ctors. Yet, LUCENE-2240 introduces more Version ctors For WhitespaceAnalyzer and SimpleAnalyzer. Test should also use their Version ctors instead the default ones."
0,"ReferencesPropertyTest can't deal with multivalued reference properties. The setUp() method uses prop.getNode(), thus assuming that the reference property is not multivalued.
"
0,"SearcherManager and NRTManager should be in the same package. I didnt even know NRTManager was still around, because its in the .index package, whereas SearcherManager is in the .search package.

Separately, I don't like that this stuff is so 'hard' with core lucene... would it be so bad if this stuff was added to core?

I suspect a lot of people have issues with this stuff (see http://www.lucidimagination.com/search/document/37964e5f0e5d733b) for example.

Worst case is just that, combine mistakes with trying to manage this stuff with MMap unmapping and total lack of error detection
for searching closed readers (LUCENE-3439) and its a mess.
"
0,"refactoring of DavSession acquisition in jcr-server. i'm subclassing WebdavServer, and i want to use my own logic for finding credentials in the request, logging into the repository and instantiating a DavSession.

unfortunately, WebdavServlet.getSession and its friend the inner class DavSessionImpl are declared private. i changed WebdavServlet.getSession to be protected so that i could override it, but even so, i have no access to DavSessionImpl, so for now, i've copied and pasted it as an inner class in my subclass. yuck.

here's a proposal for making this more extensible:

1) create the interface DavSessionProvider in org.apache.jackrabbit.server with these methods:

  public void acquireSession(WebdavRequest request) throws DavException;
  public void releaseSession(WebdavRequest request);

2) make JCRWebdavServer implement DavSessionProvider (it already includes the above methods)

3) move WebdavServlet$DavSessionImpl to DavSessionImpl in org.apache.jackrabbit.server.simple

4) create a DavSessionProviderImpl in org.apache.jackrabbit.server.simple implementing DavSessionProvider which returns instances of DavSessionImpl

5) change WebdavServlet to use a DavSessionProvider rather than its own getSession method, and use a DavSessionProviderImpl by default. subclasses can override with setDavSessionProvider().
"
0,"IndexWriter should prune 100% deleted segs even in the NRT case. We now prune 100% deleted segs on commit from IW or IR (LUCENE-2010),
but this isn't quite aggressive enough, because in the NRT case you
rarely call commit.

Instead, the moment we delete the last doc of a segment, it should be
pruned from the in-memory segmentInfos.  This way, if you open an NRT
reader, or a merge kicks off, or commit is called, the 100% deleted
segment is already gone.
"
0,"Extract the public API interfaces from o.a.j.core to o.a.j.api. To better document and track the public JCR extensions and component API provided by Jackrabbit and to allow more room for refactoring within the Jackrabbit core, we shoud move (or create) the supported API interfaces to a new org.apache.jackrabbit.api package.

At least the following interfaces should be moved along with any supporting implementation-independent classes:

    * PersistenceManager
    * FileSystem
    * AccessManager
    * QueryHandler
    * TextFilter

Possible dependencies to implementation-specific classes should preferably be abstracted using extra interfaces.

Also the workspace and node type administration methods should be published as Jackrabbit-specific extensions to the JCR API interfaces.
"
0,"new merge policy. New merge policy developed in the course of 
http://issues.apache.org/jira/browse/LUCENE-565
http://issues.apache.org/jira/secure/attachment/12340475/newMergePolicy.Sept08.patch"
0,"convert automaton to char[] based processing and TermRef / TermsEnum api. The automaton processing is currently done with String, mostly because TermEnum is based on String.
it is easy to change the processing to work with char[], since behind the scenes this is used anyway.

in general I think we should make sure char[] based processing is exposed in the automaton pkg anyway, for things like pattern-based tokenizers and such.
"
0,SPI2DAV: setup automated test execution. task copied from JCR-1877
0,"Updated Snowball package. Updated Snowball contrib package

 * New org.tartarus.snowball java package with patched SnowballProgram to be abstract to avoid using reflection.
 * Introducing Hungarian, Turkish and Romanian stemmers
 * Introducing constructor SnowballFilter(SnowballProgram)

It is possible there have been some changes made to the some of there stemmer algorithms between this patch and the current SVN trunk of Lucene, an index might thus not be compatible with new stemmers!

The API is backwards compatibile and the test pass."
0,"Demo: DeleteFiles doesn't delete files by their path names. It appears that delete(term) fails to delete the last document containing term, which
for a unique match means that you can't remove an individual document.

Code attempting to remove document with specific 'path' (slightly modified version of demo code):

Directory directory = FSDirectory.getDirectory(""index"", false);
IndexReader reader = IndexReader.open(directory);
Term term = new Term(""path"", args[0]);  // path passed via command line arg
int deleted = reader.delete(term);
reader.close();
directory.close();

System.out.println(""deleted "" + deleted + "" documents containing "" + term);

Executing this always returns ""deleted 0 documents containing <path entered>""

In IndexReader.java, delete() has:

public final int delete(Term term) throws IOException {
  TermDocs docs = termDocs(term);
  if (docs == null) return 0;
  int n = 0;
  try {
    while (docs.next()) {
      delete(docs.doc());
      n++;
    }
  } finally {
    docs.close();
  }
  return n;
}

It appears that docs.next() always returns false when there is only one doc, hence
delete() is never called and 0 is always returned.  I assume that this also means that
if there are multiple matches, the last doc will not be deleted either, but I have not tested
that.

I modified the code as follows:

    boolean more = true;
    try {
      docs.next();
      while (more) {
        delete(docs.doc());
        n++;
        more = docs.next();
      }
    } finally {
      docs.close();
    }

and then it worked as expected (at least attempts to delete a single document from the
index succeeded whereas previously they did not)."
0,"Misleading exception message for jcr:deref(). If the type of the second argument in a jcr:deref() function is not a String an InvalidQueryException is thrown with a misleading message: ""Wrong second argument type for jcr:like""

It should be rather something like: ""Second argument for jcr:deref must be a String"""
0,"Jackrabbit depends on Oracle driver for BLOB support in Oracle versions previous than 10.2. In Oracle versions previous to 10.2, Jackrabbit explicitly uses a class from the Oracle driver to provide BLOB support (see OracleFileSystem.init()). This special handling is no longer necesary for Oracle 10.2+, so we should provide a new implementation. As discussed on the list, we can create a new class for Oracle 10.2+, make it inherit from DbFileSystem, and override the createSchema(), and table space related methods, which are the ones that need special handling. Furthermore, we could refactor the current OracleFileSystem and break it into two clases, one of them to keep the current behavior and a new one to keep the common code (which we could rename to OracleBaseFileSystem or similar, to maintain compatiblity with code that uses OracleFileSystem for versions previous to 10.2). Then we make the Oracle10FileSystem inherit from the latter."
0,"XSLT pretty-printer for JCR document view export files. The attached XSLT pretty-prints Jackrabbit XML document view export files.

I'm uploading it here so others can use it or improve it.

For now I'm using it standalone to document content structures, later I might create a servlet that applies it live to repository content."
0,"Allow TaskSequence to run for certain time. To help the perf testing for LUCENE-1483, I added simple ability to specify a fixed run time (seconds) for a task sequence, eg:
{code}
{ ""XSearchWithSort"" SearchWithSort(doctitle:string) > : 2.7s
{code}
iterates on that subtask until 2.7 seconds have elapsed, and then sets the repetition count to how many iterations were done.  This is useful when you are running searches whose runtime may vary drastically."
0,"Create OSGi Bundle Manifest Headers. To be able to easily uses libraries from Jackrabbit inside an OSGi framework, for example in Apache Sling, it would be very helpfull if some of the Jackrabbit libraries include OSGi Bundle Manifest headers. It will of course not be possible to define such manifest header definition for all libraries, but jackrabbit-api, jackrabbit-jcr-commons and jackrabbit-jcr-rmi are certainly good candidates."
0,"Make SlowMultiReaderWrapper wrap always so close() is safe. The overhead when wrapping an atomic reader using SlowMultiReaderWrapper is very low, the work done in the static wrap method is much higher (instantiate ArrayList, recusively went through all subreaders), just to check the number of readers than simply always wrapping.

MultiFields already is optimized when called by one-segment or atomic readers, so there is no overhead at all. So this patch removes the static wrap method and you simply wrap like a TokenFilter with ctor: new SlowMultiReaderWrapper(reader)

When this is done, there is also no risk to close a SegmentReader (which you should not do), when wrap() returns a single SegmentReader. This help in parent issue with cleaning up the case in close().

The patch also removes the now useless mainReader/reader variables and simply closes the wrapper."
0,"H2PersistenceManager: no need to call shutdown; javadoc bugs. The H2PersistenceManager implementation calls ""shutdown"" to force closing the database when using file based databases. There is no need to do that when using the H2 database engine: the database is closed automatically when the last connection is closed.

Also, the javadocs of the H2PersistenceManager need to be fixed.
"
0,"implement reusableTokenStream for all contrib analyzers. most contrib analyzers do not have an impl for reusableTokenStream

regardless of how expensive the back compat reflection is for indexing speed, I think we should do this to mitigate any performance costs. hey, overall it might even be an improvement!

the back compat code for non-final analyzers is already in place so this is easy money in my opinion."
0,"The token types of the standard tokenizer is not accessible. The StandardTokenizerImpl not being public, these token types are not accessible :

{code:java}
public static final int ALPHANUM          = 0;
public static final int APOSTROPHE        = 1;
public static final int ACRONYM           = 2;
public static final int COMPANY           = 3;
public static final int EMAIL             = 4;
public static final int HOST              = 5;
public static final int NUM               = 6;
public static final int CJ                = 7;
/**
 * @deprecated this solves a bug where HOSTs that end with '.' are identified
 *             as ACRONYMs. It is deprecated and will be removed in the next
 *             release.
 */
public static final int ACRONYM_DEP       = 8;

public static final String [] TOKEN_TYPES = new String [] {
    ""<ALPHANUM>"",
    ""<APOSTROPHE>"",
    ""<ACRONYM>"",
    ""<COMPANY>"",
    ""<EMAIL>"",
    ""<HOST>"",
    ""<NUM>"",
    ""<CJ>"",
    ""<ACRONYM_DEP>""
};
{code}

So no custom TokenFilter can be based of the token type. Actually even the StandardFilter cannot be writen outside the org.apache.lucene.analysis.standard package.
"
0,"src builds fail because of no ""lib"" directory. I just downloaded http://mirrors.ibiblio.org/pub/mirrors/apache/lucene/java/lucene-2.0.0-src.tar.gz and noticed that you can't compile and run the tests from that src build because it doesn't inlcude the lib dir (and the build file won't attempt to make it if it doesn't exist) ...

hossman@coaster:~/tmp/l2$ tar -xzvf lucene-2.0.0-src.tar.gz
  ...
hossman@coaster:~/tmp/l2$ cd lucene-2.0.0/
hossman@coaster:~/tmp/l2/lucene-2.0.0$ ant test
  ...
test:
    [mkdir] Created dir: /home/hossman/tmp/l2/lucene-2.0.0/build/test

BUILD FAILED
/home/hossman/tmp/l2/lucene-2.0.0/common-build.xml:169: /home/hossman/tmp/l2/lucene-2.0.0/lib not found.

(it's refrenced in junit.classpath, but i'm not relaly sure why)

"
0,"test granularity for calendar (date) properties. There are repositories out there that do support properties of type Date, but not Calendar (the main difference being that Calendar also captures the time zone). Also, some repositories may not be able to store timestamps with millisecond resolution.

Although both these restrictions make a repository non-compliant, it would be useful for the tests to test these aspects as separate issues. Thus I propose to simplify the existing tests so that they just compare timestamps (factoring out the time zone), and do not require resolution finer than 1s. These two aspects then should be tested in a separate test case (thinking of it, they currently may not test sub-second resolution, in which case I propose to leave things as they are with respect to this).

"
0,Consolidate Solr's and Lucene's OpenBitSet classes. See SOLR-875 for details.
0,"Upgrade to PDFBox 0.7.3. while trying to upload a PDF document (which I can view fine with Acrobat Reader once it is loaded) I get the following exception: 

01.05.2008 12:24:44 *WARN * PdfTextExtractor: Failed to extract PDF text content (PdfTextExtractor.java, line 91)
java.io.IOException: Error: Expected an integer type, actual='%%EOF'
        at org.pdfbox.pdfparser.BaseParser.readInt(BaseParser.java:1159)
        at org.pdfbox.pdfparser.PDFParser.parseObject(PDFParser.java:349)
        at org.pdfbox.pdfparser.PDFParser.parse(PDFParser.java:132)
        at org.apache.jackrabbit.extractor.PdfTextExtractor.extractText(PdfTextExtractor.java:69)
        at org.apache.jackrabbit.extractor.CompositeTextExtractor.extractText(CompositeTextExtractor.java:90)
        at org.apache.jackrabbit.core.query.lucene.JackrabbitTextExtractor.extractText(JackrabbitTextExtractor.java:195)
        at org.apache.jackrabbit.core.query.lucene.NodeIndexer.addBinaryValue(NodeIndexer.java:393)
 ....

I replaced the version of pdfbox (0.6.4) that is bundled with the jackrabbit war file with a more recent version (0.7.3 and fontbox 01.) and it worked fine. The bundled versions should be upgraded.

On the other hand, this software appears to be inactive. Probably a different package should be selected in the long run, but for now, a simple upgrade will do the trick."
0,"deprecated method used in fieldsReader / setOmitTf(). setOmitTf(boolean) is deprecated and should not be used by core classes. One place where it appears is FieldsReader , this patch fixes it. It was necessary to change Fieldable to AbstractField at two places, only local variables.   "
0,Jcr-Server: remove jcr depedency from dav-library. 
0,promote TestExternalCodecs.PerFieldCodecWrapper to core. PerFieldCodecWrapper lets you set the Codec for each field; I'll promote to core & mark experimental.
0,"CustomScoreQuery should support multiple ValueSourceQueries. CustomScoreQuery's constructor currently accepts a subQuery, and a ValueSourceQuery.  I would like it to accept multiple ValueSourceQueries.  The workaround of nested CustomScoreQueries works for simple cases, but it quickly becomes either cumbersome to manage, or impossible to implement the desired function.

This patch implements CustomMultiScoreQuery with my desired functionality, and refactors CustomScoreQuery to implement the special case of a CustomMultiScoreQuery with 0 or 1 ValueSourceQueries.  This keeps the CustomScoreQuery API intact.

This patch includes basic tests, more or less taken from the original implementation, and customized a bit to cover the new cases."
0,"Add insertWithOverflow to PriorityQueue. This feature proposes to add an insertWithOverflow to PriorityQueue so that callers can reuse the objects that are being dropped off the queue. Also, it changes heap to protected for easier extensibility of PQ"
0,"Add Payload retrieval to Spans. It will be nice to have access to payloads when doing SpanQuerys.

See http://www.gossamer-threads.com/lists/lucene/java-dev/52270 and http://www.gossamer-threads.com/lists/lucene/java-dev/51134

Current API, added to Spans.java is below.  I will try to post a patch as soon as I can figure out how to make it work for unordered spans (I believe I have all the other cases working).

{noformat}
 /**
   * Returns the payload data for the current span.
   * This is invalid until {@link #next()} is called for
   * the first time.
   * This method must not be called more than once after each call
   * of {@link #next()}. However, payloads are loaded lazily,
   * so if the payload data for the current position is not needed,
   * this method may not be called at all for performance reasons.<br>
   * <br>
   * <p><font color=""#FF0000"">
   * WARNING: The status of the <b>Payloads</b> feature is experimental.
   * The APIs introduced here might change in the future and will not be
   * supported anymore in such a case.</font>
   *
   * @return a List of byte arrays containing the data of this payload
   * @throws IOException
   */
  // TODO: Remove warning after API has been finalized
  List/*<byte[]>*/ getPayload() throws IOException;

  /**
   * Checks if a payload can be loaded at this position.
   * <p/>
   * Payloads can only be loaded once per call to
   * {@link #next()}.
   * <p/>
   * <p><font color=""#FF0000"">
   * WARNING: The status of the <b>Payloads</b> feature is experimental.
   * The APIs introduced here might change in the future and will not be
   * supported anymore in such a case.</font>
   *
   * @return true if there is a payload available at this position that can be loaded
   */
  // TODO: Remove warning after API has been finalized
  public boolean isPayloadAvailable();
{noformat}"
0,"Wrap messages output with a check of InfoStream != null. I've found several places in the code where messages are output w/o first checking if infoStream != null. The result is that in most of the time, unnecessary strings are created but never output (because infoStream is not set). We should follow Java's logging best practices, where a log message is always output in the following format:
if (logger.isLoggable(leve)) {
    logger.log(level, msg);
}

Log messages are usually created w/o paying too much attention to performance (such as string concatenation using '+' instead of StringBuffer). Therefore, at runtime it is important to avoid creating those messages, if they will be discarded eventually.

I will add a method to IndexWriter messagesEnabled() and then use it wherever a call to iw.message() is made.

Patch will follow"
0,"contrib/orm-persistence Node ordering not supported. Due to a limitation in the implementation, node ordering isn't supported (it is optional in the specification but Jackrabbit provides support for it) in the ORM persistence manager. This is due to the fact that in the database, although same-name sibling ordering is supported, no guarantee is given for the ordering of nodes in the child list.

Jackrabbit has some tests that use node ordering, such as the DerefQueryLevel1Test, where we look for the first property of type reference by using the following code :

        Property refProp = PropertyUtil.searchProp(session, testRootNode, PropertyType.REFERENCE);

The searchProp method traverses the tree and stops at the first property of the type specified. If node ordering is not correct, we return a property that is not the expected one (in the case of the DerefTest we were returning a multi-values reference property), which can could test failures."
0,"PROPPATCH error marshalling when the resource can't be modified in general. Litmus test case ""notowner_modify"" (see <http://mail-archives.apache.org/mod_mbox/jackrabbit-dev/200604.mbox/%3c4432A7CF.30008@gmx.de%3e>) complains about a 423 (Locked) status code being sent back inside a 207 Multistatus:

  9. notowner_modify....... WARNING: PROPPATCH failed with 0 not 423
     ...................... pass (with 1 warning)

I think that warning is correct, as this is an error condition that doesn't need to be marshalled inside multistatus (1: it affects the resource at the Request URI and only that, 2: the operation failed completely). Let me also note that none of the other servers I tested with do return a 207 here (MS IIS, Apache/moddav, Xythos, SAP Netweaver KM),

RFC2518bis will hopefully clarify error marshalling for PROPPATCH. 

From the source code, the current server behaviour is fully intentional (by specifically catching the DavException and using it in MultiStatus). Removing that code seems to fix the issue.
"
0,"Demo and contrib jars should contain NOTICE.TXT and LICENSE.TXT. We should include NOTICE.TXT and LICENSE.TXT not only in the core jar but also
in the demo and contrib jars."
0,"Make Authorizable.setProperty more noisy in case of failures. setProperty fails with an unspecific warning, when there's an exception, but it doesn' print any useful information from this exception."
0,"Create a grouping convenience class. Currently the grouping module has many collector classes with a lot of different options per class. I think it would be a good idea to have a GroupUtil (Or another name?) convenience class. I think this could be a builder, because of the many options (sort,sortWithinGroup,groupOffset,groupCount and more) and implementations (term/dv/function) grouping has."
0,"SPI: Testsuite for the SPI Interfaces. now that people start writing SPI implementations we should provide a test-suite that runs on the SPI directly in order to provide the developers a way to assert basic compliance of their implementation without having the JCR api in between.
"
0,"JSR 283: Access Property/Node from Session. New methods to access properties and nodes from the Session:

- getNode(String absPath) Node
- getNodeByIdentifier(String id) Node
- getProperty(String absPath) Property

... test for their existence:

- nodeExists(String absPath) boolean
- propertyExists(String absPath) boolean

... and remove them:

- removeItem(String absPath) void

The functionality has been added at rev. 571494 and rev. 712984 but apart from Session.removeItem no
test cases are present so far."
0,"TestBasicCookieAttribHandlers fails on non-english Locale systems. The Test checks for written dates in the format for cookies which unfortunately includes a two character abbreviation of the day. This differs by locale, so the dateformat has to be constructed with Locale.US (as in DateUtils)"
0,"Fixed Spelling mailinglist.xml. Just fixed some spelling in the mailinglist.xml in /java/trunk/xdocs



"
0,"Update site lucene-sandbox page. The page has misleading/bad info. One thing I would like to do - but I won't attempt now (prob good for the modules issue) - is commit to one word - contrib or sandbox. I think sandbox should be purged myself.

The current page says that the sandbox is kind of a rats nest with various early stage software that one day may make it into core - that info is outdated I think. We should replace it, and also specify how the back compat policy works in contrib eg each contrib can have its own policy, with the default being no policy.

We should also drop the piece about being open to Lucene's committers and others - a bit outdated.

We should also either include the other contribs, or change the wording to indicate that the list is only a sampling of the many contribs."
0,"Add some ligatures (ff, fi, fl, ft, st) to ISOLatin1AccentFilter. ISOLatin1AccentFilter removes common diacritics and some ligatures. This patch adds support for additional common ligatures: ff, fi, fl, ft, st."
0,"AdministratorTest.testAdminNodeCollidingWithRandomNode failure. I see the following test failure with the latest trunk. It seems to affect also Sbastien as commented in JCR-2389. However, it doesn't break the Hudson build or Angela's checkout.

I'm filing this as a bug and will disable the test for now to be able to cut the 2.0-beta3 release. We can look at this later in more detail.

The detailed failure message is:

-------------------------------------------------------------------------------
Test set: org.apache.jackrabbit.core.security.user.TestAll
-------------------------------------------------------------------------------
Tests run: 144, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 4.078 sec <<< FAILURE!
testAdminNodeCollidingWithRandomNode(org.apache.jackrabbit.core.security.user.AdministratorTest)  Time elapsed: 0.072 sec  <<< FAILURE!
junit.framework.AssertionFailedError
        at junit.framework.Assert.fail(Assert.java:47)
        at junit.framework.Assert.assertTrue(Assert.java:20)
        at junit.framework.Assert.assertFalse(Assert.java:34)
        at junit.framework.Assert.assertFalse(Assert.java:41)
        at org.apache.jackrabbit.core.security.user.AdministratorTest.testAdminNodeCollidingWithRandomNode(AdministratorTest.java:205)
"
0,Stack traces from failed tests are messed up on ANT 1.7.x. 
0,"Contrib: ThaiAnalyzer to enable Thai full-text search in Lucene. Thai text don't have space between words. Usually, a dictionary-based algorithm is used to break string into words. For Lucene to be usable for Thai, an Analyzer that know how to break Thai words is needed.

I've implemented such Analyzer, ThaiAnalyzer, using ICU4j DictionaryBasedBreakIterator for word breaking. I'll upload the code later.

I'm normally a C++ programmer and very new to Java. Please review the code for any problem. One possible problem is that it requires ICU4j. I don't know whether this is OK."
0,"User-defined ProtocolSocketFactory for secure connection through proxy. I use a custom socket implementation with HttpClient, and am having problems 
getting secure connections through a proxy working.

HttpClient requires that my SecureProtocolSocketFactory be able to create a 
secure socket layered over an existing insecure socket, but does not specify 
how that insecure socket is created. Currently, for secure proxied 
connections, the insecure connection is always created using a 
DefaultProtocolSocketFactory (HttpConnection.java, line 702). I wish to be 
able to override this behaviour, so that I can create the insecure socket 
using my own custom implementation.

The problem with the default behaviour is that my custom socket implementation 
is written in C++ using JNI, and the SSL implementation is handled at the 
native level. Hence, layering over an existing JDK socket will not work.

My proposed solution is to add an HTTP connection parameter to specify the 
socket factory to use, perhaps http.connection.insecuresocketfactory of type 
Class."
0,"Per thread DocumentsWriters that write their own private segments. See LUCENE-2293 for motivation and more details.

I'm copying here Mike's summary he posted on 2293:

Change the approach for how we buffer in RAM to a more isolated
approach, whereby IW has N fully independent RAM segments
in-process and when a doc needs to be indexed it's added to one of
them. Each segment would also write its own doc stores and
""normal"" segment merging (not the inefficient merge we now do on
flush) would merge them. This should be a good simplification in
the chain (eg maybe we can remove the *PerThread classes). The
segments can flush independently, letting us make much better
concurrent use of IO & CPU."
0,Jcr2spiRepositoryFactory: make class loading more robust. Currently Jcr2spiRepositoryFactory loads a RepositoryServiceFactory from the context class loader. In an OSGi environment this fails with a ClassNotFoundException. I suggest to fall back to the class loader of a the class (Jcr2spiRepositoryFactory) in this case. 
0,"SorterTemplate.quickSort stack overflows on broken comparators that produce only few disticnt values in large arrays. Looking at Otis's sort problem on the mailing list, he said:
{noformat}
* looked for other places where this call is made - found it in
MultiPhraseQuery$MultiPhraseWeight and changed that call from
ArrayUtil.quickSort to ArrayUtil.mergeSort
* now we no longer see SorterTemplate.quickSort in deep recursion when we do a
thread dump
{noformat}

I thought this was interesting because PostingsAndFreq's comparator
looks like it needs a tiebreaker.

I think in our sorts we should add some asserts to try to catch some of these broken comparators."
0,"Mark pending nodes in IndexingQueue directly in index. The index currently writes an indexing_queue.log file which contains all nodes that timed out while text was extracted. Instead, the index itself should mark an indexed node as pending. This is more robust because no additional file must be written."
0,"Add getPath method to Authorizable interface. currently the only way to retrieve the path of the item associated with an authorizable is to check if the
principal obtained through Authorizable#getPrincipal() is an ItemBasedPrincipal.

having a getPath method would provide a convenient shortcut and would in addition allow
to determine of there is really an item associated with a authorizable that is accessible for the editing
session (which is not necessarily the case for ItemBasedPrincipal#getPath"
0,"Checks for optional features in test cases are wrong. Reported by David Sanders:

The TCK for JSR-170 Final Release
(http://jcp.org/aboutJava/communityprocess/final/jsr170/index.html)
checks for level 2 and optional features by comparing
Repository.getDescriptor to null.  According to the
spec and javadoc, getDescriptor must return either
""true"" or ""false"" for the ""capability"" keys.

Example in AbsractJCRest.java:

        // setup custom namespaces
        if
(helper.getRepository().getDescriptor(Repository.LEVEL_2_SUPPORTED)
!= null) {
            NamespaceRegistry nsReg =
superuser.getWorkspace().getNamespaceRegistry();


I think the above if statement should be:
       if
(helper.getRepository().getDescriptor(Repository.LEVEL_2_SUPPORTED)
.equals(""true"")) "
0,"Child Axis support in order by clause. Hi, 

since child axis is supported in XPath predicates, it would be nice to support it in order by clause as well

Queries of type

//element(*, type) [ foo/@bar ]  order by foo/@bar asc

can become very useful

BR, 

Savvas"
0,Review test cases and cross check with 1.0 specification. This jira task is meant to collect issues with the TCK test cases.
0,Convert Batch implementation in spi-rmi from remote object into a local one. The current implementation of the Batch interface in spi-rmi is very simple and just uses remotes to the server side batch. This should be changed to a local object on the client and only transmit the changes in a single call to the server on save.
0,"Javadocs should explain possible causes for IOExceptions. 
Most methods in Lucene reserve the right to throw an IOException.  This can occur for nearly all methods from low level problems like wrong permissions, transient IO errors, bad hard drive or corrupted file system, corrupted index, etc, but for some methods there are also more interesting causes that we should try to document.

Spinoff of this thread:

    http://www.gossamer-threads.com/lists/lucene/java-user/44929"
0,"ClientPNames.VIRTUAL_HOST is used as is; if not provided, the port should be derived from the target URL. The parameter ClientPNames.VIRTUAL_HOST allows the default Host header to be overridden.

Currently the code uses the HttpHost entry as provided, and does not automatically add the port suffix.
This means that user code has to provide the port - but only if it's not the default for the protocol.

It would be simpler for the user if the port were automatically added.

If the user does not provide the port, the code should derive it from the target URL.

If the user does provide a port number, then that should be used (as is done currently). 
This allows the user to override the port (if that should ever prove necessary)."
0,"Some improvements to contrib/benchmark. I've made some small improvements to the contrib/benchmark, mostly
merging in the ad-hoc benchmarking code I've been using in LUCENE-843:

  - Fixed thread safety of DirDocMaker's usage of SimpleDateFormat

  - Print the props in sorted order

  - Added new config ""autocommit=true|false"" to CreateIndexTask

  - Added new config ""ram.flush.mb=int"" to AddDocTask

  - Added new configs ""doc.term.vector.positions=true|false"" and
    ""doc.term.vector.offsets=true|false"" to BasicDocMaker

  - Added WriteLineDocTask.java, so you can make an alg that uses this
    to build up a single file containing one document per line in a
    single file.  EG this alg converts the reuters-out tree into a
    single file that has ~1000 bytes per body field, saved to
    work/reuters.1000.txt:

      docs.dir=reuters-out
      doc.maker=org.apache.lucene.benchmark.byTask.feeds.DirDocMaker
      line.file.out=work/reuters.1000.txt
      doc.maker.forever=false
      {WriteLineDoc(1000)}: *

    Each line has tab-separted TITLE, DATE, BODY fields.

  - Created feeds/LineDocMaker.java that creates documents read from
    the file created by WriteLineDocTask.java.  EG this alg indexes
    all documents created above:

      analyzer=org.apache.lucene.analysis.SimpleAnalyzer
      directory=FSDirectory
      doc.add.log.step=500

      docs.file=work/reuters.1000.txt
      doc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker
      doc.tokenized=true
      doc.maker.forever=false

      ResetSystemErase
      CreateIndex
      {AddDoc}: *
      CloseIndex

      RepSumByPref AddDoc

I'll attach initial patch shortly.
"
0,"Impl toString() in MergePolicy and its extensions. These can be important to see for debugging.

We lost them in the cutover to IWC.

Just opening this issue to remind us to get them back, before releasing..."
0,"Allow concurrent index updates and queries. Currently the query handler either allows one modification or multiple queries at a time. That is, write operations are separated from read operations. With this synchronization scheme multiple queries may run concurrently, but only in the absence of a write operation.

There is one major drawback with this synchronization: a single long running query is able to block the whole workspace from committing changes. Because the query handler is coupled to the Workspace via a synchronous event listener, further processing is blocked until the query handler has finished its event processing (reflecting the changes in the index).

Instead, each index modification should be non-blocking, in the sense that an index modification should not have to wait for any queries to complete."
0,"Remove oal.util.MapBackedSet (Java 6 offsers Collections.newSetFromMap()). Easy search and replace job. In 3.x we still need the class, as Java 5 does not have Collections.newSetFromMap()."
0,"Add Apache RAT (Release Audit Tool) target to build.xml. 
Apache RAT is a useful tool to check for common mistakes in our source code (eg missing copyright headers):

    http://incubator.apache.org/rat/

I'm just copying the patch Grant worked out for Solr (SOLR-762).  I plan to commit to 2.4 & 2.9."
0,"Exclude PrecedenceQueryParser from build or disable failing test cases. As Erik commented in LUCENE-885 the PrecendenceQueryParser is currently
unmaintained. Since some tests are failing we should either exclude PQP from the 
build or simply disable the failing tests."
0,"[PATCH] LockFactory implementation based on OS native locks (java.nio.*). The current default locking for FSDirectory is SimpleFSLockFactory.
It uses java.io.File.createNewFile for its locking, which has this
spooky warning in Sun's javadocs:

    Note: this method should not be used for file-locking, as the
    resulting protocol cannot be made to work reliably. The FileLock
    facility should be used instead.

So, this patch provides a LockFactory implementation based on FileLock
(using java.nio.*).

All unit tests pass with this patch, on OS X (10.4.8), Linux (Ubuntu
6.06), and Windows XP SP2.

Another benefit of native locks is the OS automatically frees them if
the JVM exits before Lucene can free its locks.  Many people seem to
hit this (old lock files still on disk) now.

I've created this new class:

  org.apache.lucene.store.NativeFSLockFactory

and added a couple test cases to the existing TestLockFactory.

I've left SimpleFSLockFactory as the default locking for FSDirectory
for now.  I think we should get some usage / experience with
NativeFSLockFactory and then later on make it the default locking
implementation?

I also tested changing FSDirectory's default locking to
NativeFSLockFactory and all unit tests still pass (on the above
platforms).

One important note about locking over NFS: some NFS servers and/or
clients do not support it, or, it's a configuration option or mode
that must be explicitly enabled.  When it's misconfigured it's able to
take a long time (35 seconds in my case) before throwing an exception.
To handle this, I acquire & release a random test lock on creating the
NativeFSLockFactory to verify locking is configured properly.

A few other small changes in the patch:

    - Added a ""failure reason"" to Lock.java so that in
      obtain(lockWaitTimeout), if there is a persistent IOException
      in trying to obtain the lock, this can be messaged & included in
      the ""Lock obtain timed out"" that's raised.

    - Corrected javadoc in SimpleFSLockFactory: it previously said the
      wrong system property for overriding lock class via system
      properties

    - Fixed unhandled IOException when opening an IndexWriter for
      create, if the locks dir does not exist (just added
      lockDir.exists() check in clearAllLocks method of
      SimpleFSLockFactory & NativeFSLockFactory.

    - Fixed a few small unrelated issues with TestLockFactory, and
      also fixed tests to accept NativeFSLockFactory as the default
      locking implementation for FSDirectory.

    - Fixed a typo in javadoc in FieldsReader.java

    - Added some more javadoc for the LockFactory.setLockPrefix
"
0,"Fix rawtypes warnings for Java 7 compiler. Java 7 changed the warnings a little bit:
- Java 6 only knew ""unchecked"" warning type, applying for all types of generics violations, like missing generics (raw types)
- Java 7 still knows ""unchecked"" but only emits warning if the call is really unchecked. Declaration of variables/fields or constructing instances without type param now emits ""rawtypes"" warning.

The changes above causes the Java 7 compile now emit lots of ""rawtypes"" warnings, where Java 6 is silent. The easy fix is to suppres both warning types: @SuppressWarnings({""unchecked"",""rawtypes""}) for all those places. Changes are easy to do, will provide patch later!"
0,"Enable native per-field codec support . Currently the codec name is stored for every segment and PerFieldCodecWrapper is used to enable codecs per fields which has recently brought up some issues (LUCENE-2740 and LUCENE-2741). When a codec name is stored lucene does not respect the actual codec used to encode a fields postings but rather the ""top-level"" Codec in such a case the name of the top-level codec is  ""PerField"" instead of ""Pulsing"" or ""Standard"" etc. The way this composite pattern works make the indexing part of codecs simpler but also limits its capabilities. By recoding the top-level codec in the segments file we rely on the user to ""configure"" the PerFieldCodecWrapper correctly to open a SegmentReader. If a fields codec has changed in the meanwhile we won't be able to open the segment.

The issues LUCENE-2741 and LUCENE-2740 are actually closely related to the way PFCW is implemented right now. PFCW blindly creates codecs per field on request and at the same time doesn't have any control over the file naming nor if a two codec instances are created for two distinct fields even if the codec instance is the same. If so FieldsConsumer will throw an exception since the files it relies on are already created.

Having PerFieldCodecWrapper AND a CodecProvider overcomplicates things IMO. In order to use per field codec a user should on the one hand register its custom codecs AND needs to build a PFCW which needs to be maintained in the ""user-land"" an must not change incompatible once a new IW of IR is created. What I would expect from Lucene is to enable me to register a codec in a provider and then tell the Field which codec it should use for indexing. For reading lucene should determ the codec automatically once a segment is opened. if the codec is not available in the provider that is a different story. Once we instantiate the composite codec in SegmentsReader we only have the codecs which are really used in this segment for free which in turn solves LUCENE-2740. 

Yet, instead of relying on the user to configure PFCW I suggest to move composite codec functionality inside the core an record the distinct codecs per segment in the segments info. We only really need the distinct codecs used in that segment since the codec instance should be reused to prevent additional files to be created. Lets say we have the follwing codec mapping :
{noformat}
field_a:Pulsing
field_b:Standard
field_c:Pulsing
{noformat}

then we create the following mapping:
{noformat}
SegmentInfo:
[Pulsing, Standard]

PerField:
[field_a:0, field_b:1, field_c:0]
{noformat}

that way we can easily determ which codec is used for which field an build the composite - codec internally on opening SegmentsReader. This ordering has another advantage, if like in that case pulsing and standard use really the same type of files we need a way to distinguish the used files per codec within a segment. We can in turn pass the codec's ord (implicit in the SegmentInfo) to the FieldConsumer on creation to create files with segmentname_ord.ext (or something similar). This solvel LUCENE-2741). 
"
0,Add workspace population tool. Add a simple tool to jackrabbit-webapp to populate the workspace with content.
0,"HTTPClient per default relentlessly spams to stderr. HTTPClient relentlessly spams to stderr when including it into a project via maven. This is not a decent default behaviour for a libary. Libaries should, per default, communicate their internal state solely and adequatly via their API and let it be up to the application to react to that state (logging it is one such reaction). From some replies to tickets in the same vain I can see that this is perhaps a sensitive topic as some see logging to be a core concern of HTTPClient. I do agree it's helpful as a debugging tool but as such it needs to be opt-in. As a standard error output, the logging of HTTPClient is absolutely useless because it does not and can not describe what the application is trying to do.

Why this improvement when there is a way to disable HTTClient logging (in fact, there seem to be many ways ... always a bad sign ..)?

Do a google search for: httpclient ""console spam""
204 hit for this harsh phrasing alone. Search this phrase for any other libary you like to use and compare the number of hits. Ask youself, how often have you seen the java standard libary write to stdout or stderr?

Personally, I tried to disable it via JDK14 getLogger(""org.apache"").setLevel(Level.OFF)  which wouldn't work and now am using a solution I found on Stackoverflow which is:

System.setProperty(""org.apache.commons.logging.Log"", ""org.apache.commons.logging.impl.NoOpLog""); }

The problem I have is that I include this lib and suddently my console is useless because httpclient is all over it (writing a system monitor ...). I have to search google to find a solution (http://hc.apache.org/httpcomponents-client-ga/logging.html does not tell you how to turn logging off ...) and the logical one ""turn of the JDK logger"" does not work right away.

It's really a matter of following the principal of least suprise (a libary is not expected to write to the console which is the observable default behaviour of HTTPClient) and the principal of separation of concerns (logging is a concern for applications and not for libaries).

Following at least one of these would substantially increase the joy of working with the HTTPClient libary.

"
0,"Setting CONNECTION_TIMEOUT and SO_TIMEOUT on a per-method basis. The capability of setting connection timeout and socket timeout on a per-method
basis should be provided. This would enable different threads, sharing the same
HttpClient, to set different timeouts for their methods executions."
0,Remove System.out left in SpanHighlighter code. A System.out debug was left in the code when a Query is not supported by the SpanHighlighter. This issue simply removes it.
0,"Change Log-Level in DefaultIOListener. Please change loglevel for method onEnd(IOHandler handler, IOContext ioContext, boolean success) to debug"
0,"waitForResponse is using busy wait. In HttpConnection, the method waitForResponse is using busywait, instead of 
blocking until the response is arriving.

Is this on purpose, or shouldn't it handle this by blocking instead ??"
0,"BlockJoinQuery/Collector. I created a single-pass Query + Collector to implement nested docs.
The approach is similar to LUCENE-2454, in that the app must index
documents in ""join order"", as a block (IW.add/updateDocuments), with
the parent doc at the end of the block, except that this impl is one
pass.

Once you join at indexing time, you can take any query that matches
child docs and join it up to the parent docID space, using
BlockJoinQuery.  You then use BlockJoinCollector, which sorts parent
docs by provided Sort, to gather results, grouped by parent; this
collector finds any BlockJoinQuerys (using Scorer.visitScorers) and
retains the child docs corresponding to each collected parent doc.

After searching is done, you retrieve the TopGroups from a provided
BlockJoinQuery.

Like LUCENE-2454, this is less general than the arbitrary joins in
Solr (SOLR-2272) or parent/child from ElasticSearch
(https://github.com/elasticsearch/elasticsearch/issues/553), since you
must do the join at indexing time as a doc block, but it should be
able to handle nested joins as well as joins to multiple tables,
though I don't yet have test cases for these.

I put this in a new Join module (modules/join); I think as we
refactor join impls we should put them here.
"
0,"Add support for terms in BytesRef format to Term, TermQuery, TermRangeQuery & Co.. It would be good to directly allow BytesRefs in TermQuery and TermRangeQuery (as both queries convert the strings to BytesRef internally). For NumericRange support in Solr it will be needed to support numerics as ByteRef in single-term queries.

When this will be added, don't forget to change TestNumericRangeQueryXX to use the BytesRef ctor of TRQ."
0,"Split up IndexInput and IndexOutput into DataInput and DataOutput. I'd like to introduce the two new classes DataInput and DataOutput
that contain all methods from IndexInput and IndexOutput that actually
decode or encode data, such as readByte()/writeByte(),
readVInt()/writeVInt().

Methods like getFilePointer(), seek(), close(), etc., which are not
related to data encoding, but to files as input/output source stay in
IndexInput/IndexOutput.

This patch also changes ByteSliceReader/ByteSliceWriter to extend
DataInput/DataOutput. Previously ByteSliceReader implemented the
methods that stay in IndexInput by throwing RuntimeExceptions.

See also LUCENE-2125.

All tests pass."
0,"CheckIndex should verify numUniqueTerms == recomputedNumUniqueTerms. Just glancing at the code it seems to sorta do this check, but only in the hasOrd==true case maybe (which seems to be testing something else)?

It would be nice to verify this also for terms dicts that dont support ord.

we should add explicit checks per-field in 4.x, and for-all-fields in 3.x and preflex"
0,"Optimizations to TopScoreDocCollector and TopFieldCollector. This is a spin-off of LUCENE-1575 and proposes to optimize TSDC and TFC code to remove unnecessary checks. The plan is:
# Ensure that IndexSearcher returns segements in increasing doc Id order, instead of numDocs().
# Change TSDC and TFC's code to not use the doc id as a tie breaker. New docs will always have larger ids and therefore cannot compete.
# Pre-populate HitQueue with sentinel values in TSDC (score = Float.NEG_INF) and remove the check if reusableSD == null.
# Also move to use ""changing top"" and then call adjustTop(), in case we update the queue.
# some methods in Sort explicitly add SortField.FIELD_DOC as a ""tie breaker"" for the last SortField. But, doing so should not be necessary (since we already break ties by docID), and is in fact less efficient (once the above optimization is in).
# Investigate PQ - can we deprecate insert() and have only insertWithOverflow()? Add a addDummyObjects method which will populate the queue without ""arranging"" it, just store the objects in the array (this can be used to pre-populate sentinel values)?

I will post a patch as well as some perf measurements as soon as I have them."
0,"Architecture Diagrams needed for Lucene, Solr and Nutch. "
0,TestIndexwriterWithThreads#testCloseWithThreads hangs if a thread hit an exception before indexing its first document. in TestIndexwriterWithThreads#testCloseWithThreads we loop until all threads have indexed a single document but if one or more threads fail on before they index the first doc the test hangs forever. We should check if the thread is still alive unless it has indexed a document and fail if it already died.
0,"Utility code for filtering and packaging trees. The attached zip contains new utility code for filtering and packaging trees in the repository.

A tree can be traversed by the provided tree walker. During the traversal configurable filters can be applied. The filters have influence on the traversal, like skipping nodes or properties.
Included filters test the node name, node type etc. Custom filters are possible as well.
A tree walker notifies a tree walker listener (interface) whenever it traverses an item.

The second utility code is able to package a whole tree (through a description) and export this in some way - the exporter is an interface and could e.g. be an exporter serializing the tree into a zip archiv etc."
0,"Lock-less commits. This is a patch based on discussion a while back on lucene-dev:

    http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200608.mbox/%3c44E5B16D.4010805@mikemccandless.com%3e

The approach is a small modification over the original discussion (see
Retry Logic below).  It works correctly in all my cross-machine test
case, but I want to open it up for feedback, testing by
users/developers in more diverse environments, etc.

This is a small change to how lucene stores its index that enables
elimination of the commit lock entirely.  The write lock still
remains.

Of the two, the commit lock has been more troublesome for users since
it typically serves an active role in production.  Whereas the write
lock is usually more of a design check to make sure you only have one
writer against the index at a time.

The basic idea is that filenames are never reused (""write once""),
meaning, a writer never writes to a file that a reader may be reading
(there is one exception: the segments.gen file; see ""RETRY LOGIC""
below).  Instead it writes to generational files, ie, segments_1, then
segments_2, etc.  Besides the segments file, the .del files and norm
files (.sX suffix) are also now generational.  A generation is stored
as an ""_N"" suffix before the file extension (eg, _p_4.s0 is the
separate norms file for segment ""p"", generation 4).

One important benefit of this is it avoids files contents caching
entirely (the likely cause of errors when readers open an index
mounted on NFS) since the file is always a new file.

With this patch I can reliably instantiate readers over NFS when a
writer is writing to the index.  However, with NFS, you are still forced to
refresh your reader once a writer has committed because ""point in
time"" searching doesn't work over NFS (see LUCENE-673 ).

The changes are fully backwards compatible: you can open an old index
for searching, or to add/delete docs, etc.  I've added a new unit test
to test these cases.

All units test pass, and I've added a number of additional unit tests,
some of which fail on WIN32 in the current lucene but pass with this
patch.  The ""fileformats.xml"" has been updated to describe the changes
to the files (but XXX references need to be fixed before committing).

There are some other important benefits:

  * Readers are now entirely read-only.

  * Readers no longer block one another (false contention) on
    initialization.

  * On hitting contention, we immediately retry instead of a fixed
    (default 1.0 second now) pause.

  * No file renaming is ever done.  File renaming has caused sneaky
    access denied errors on WIN32 (see LUCENE-665 ).  (Yonik, I used
    your approach here to not rename the segments_N file(try
    segments_(N-1) on hitting IOException on segments_N): the separate
    "".done"" file did not work reliably under very high stress testing
    when a directory listing was not ""point in time"").

  * On WIN32, you can now call IndexReader.setNorm() even if other
    readers have the index open (fixes a pre-existing minor bug in
    Lucene).

  * On WIN32, You can now create an IndexWriter with create=true even
    if readers have the index open (eg see
    www.gossamer-threads.com/lists/lucene/java-user/39265) .


Here's an overview of the changes:

  * Every commit writes to the next segments_(N+1).

  * Loading the segments_N file (& opening the segments) now requires
    retry logic.  I've captured this logic into a new static class:
    SegmentInfos.FindSegmentsFile.  All places that need to do
    something on the current segments file now use this class.

  * No more deletable file.  Instead, the writer computes what's
    deletable on instantiation and updates this in memory whenever
    files can be deleted (ie, when it commits).  Created a common
    class index.IndexFileDeleter shared by reader & writer, to manage
    deletes.

  * Storing more information into segments info file: whether it has
    separate deletes (and which generation), whether it has separate
    norms, per field (and which generation), whether it's compound or
    not.  This is instead of relying on IO operations (file exists
    calls).  Note that this fixes the current misleading
    FileNotFoundException users now see when an _X.cfs file is missing
    (eg http://www.nabble.com/FileNotFound-Exception-t6987.html).

  * Fixed some small things about RAMDirectory that were not
    filesystem-like (eg opening a non-existent IndexInput failed to
    raise IOException; renames were not atomic).  I added a stress
    test against a RAMDirectory (1 writer thread & 2 reader threads)
    that uncovered these.

  * Added option to not remove old files when create=true on creating
    FSDirectory; this is so the writer can do its own [more
    sophisticated because it retries on errors] removal.

  * Removed all references to commit lock, COMMIT_LOCK_TIMEOUT, etc.
    (This is an API change).

  * Extended index/IndexFileNames.java and index/IndexFileNameFilter.java
    with logic for computing generational file names.

  * Changed index/IndexFileNameFilter.java to use a HashSet to check
    file extentsions for better performance.

  * Fixed the test case TestIndexReader.testLastModified: it was
    incorrectly (I think?) comparing lastModified to version, of the
    index.  I fixed that and then added a new test case for version.


Retry Logic (in index/SegmentInfos.java)

If a reader tries to load the segments just as a writer is committing,
it may hit an IOException.  This is just normal contention.  In
current Lucene contention causes a [default] 1.0 second pause then
retry.  With lock-less the contention causes no added delay beyond the
time to retry.

When this happens, we first try segments_(N-1) if present, because it
could be segments_N is still being written.  If that fails, we
re-check to see if there is now a newer segments_M where M > N and
advance if so.  Else we retry segments_N once more (since it could be
it was in process previously but must now be complete since
segments_(N-1) did not load).

In order to find the current segments_N file, I list the directory and
take the biggest segments_N that exists.

However, under extreme stress testing (5 threads just opening &
closing readers over and over), on one platform (OS X) I found that
the directory listing can be incorrect (stale) by up to 1.0 seconds.
This means the listing will show a segments_N file but that file does
not exist (fileExists() returns false).

In order to handle this (and other such platforms), I switched to a
hybrid approach (originally proposed by Doron Cohen in the original
thread): on committing, the writer writes to a file ""segments.gen"" the
generation it just committed.  It writes 2 identical longs into this
file.  The retry logic, on detecting that the directory listing is
stale falls back to the contents of this file.  If that file is
consistent (the two longs are identical), and, the generation is
indeed newer than the dir listing, it will use that.

Finally, if this approach is also stale, we fallback to stepping
through sequential generations (up to a maximum # tries).  If all 3
methods fail, we throw the original exception we hit.

I added a static method SegmentInfos.setInfoStream() which will print
details of retry attempts.  In the patch it's set to System.out right
now (we should turn off before a real commit) so if there are problems
we can see what retry logic had done.
"
0,"The 1.5.0 webapp points to 1.4 javadocs. There's a ""Jackrabbit API"" link on the Jackrabbit webapp 1.5.0 that points to http://jackrabbit.apache.org/api/1.4/. It should be updated to point to http://jackrabbit.apache.org/api/1.5/."
0,"Deprecate and replace TestWebapp with the SimpleHttpServer based testing framework. Basically TestWebapp based testcases test functionality of Tomcat, rather than
that of HttpClient. They tend to get broken with every major release of Tomcat
and have proven more of a burden than any good"
0,"Support for NTLM authentication. A late write in for this would be support for NTLM authenticatin as well as
basic and digest.  Obviously non-trivial but it would be a very big feature.

Adrian Sutton, Software Engineer
Ephox Corporation
www.ephox.com <http://www.ephox.com>"
0,"Consistency check/fix doesn't work with PSQL persistence manager. PSQL doesn't save blobs directly into table row, instead saves a link there and puts the binary stream somewhere else. The general consistency check method in BundleDBPersistenceManager doesn't take this into account.
I've fixed this by changing getBytes(Blob) method in BundleDbPersistenceManager to getBytes(ResultSet) and overriding it for PSQL."
0,"Fold AuthSSLProtocolSocketFactory into HttpClient proper. Include the functionality of the AuthSSLProtocolSocketFactory class into the
main distribution of HttpClient

http://svn.apache.org/repos/asf/jakarta/commons/proper/httpclient/trunk/src/contrib/org/apache/commons/httpclient/contrib/ssl/"
0,"ServerQuery does not use RemoteAdapterFactory for creating ServerQueryResult. The ServerQuery sould use the Factory for creating ServerQueryResult.

Siehe the method ServerQuery.execute():

{code}
public RemoteQueryResult execute() throws RepositoryException, RemoteException {
        return new ServerQueryResult(query.execute(), getFactory());
    }
{code}

it should be:
{code}
    public RemoteQueryResult execute() throws RepositoryException, RemoteException {
        return getFactory().getRemoteQueryResult(this.query.execute());
    }
{code}"
0,"When authentication is invalidated during redirection, proxy authentication also should be invalidated. This was discovered during use by Lucene Connector Framework, on 3.1.

When a document is fetched through a proxy authenticated with NTLM, and
that document is a redirection (301 or 302), the httpclient fails to
properly use the right proxy credentials on the subsequent document
fetch. This leads to 407 errors on these kinds of documents.

I've attached a proposed patch.
"
0,ReadOnlyIndexReaders are re-created on every access. AbstractIndex.getReadOnlyIndexReader() creates a new instance on every call. The returned index reader should instead be cached and kept open as long as there are no changes on the underlying index.
0,"Lucene Search has poor cpu utilization on a 4-CPU machine. I noticed that the class org.apache.lucene.index.FieldInfos uses private class
members Vector byNumber and Hashtable byName, both of which are synchronized
objects, thus resulting in unessesary locks. 

By changing the Vector byNumber to ArrayList byNumber, and Hashtable byName to
HashMap byName, both are not synchronized objects, I was able to get 110%
improvement in performance (number of searches per second).


Here is a sample of blocked thread
""Thread-32"" daemon prio=1 tid=0x082334c0 nid=0xa66 waiting for monitor entry
[4f385000..4f38687c]
        at java.util.Vector.elementAt(Vector.java:430)
        - waiting to lock <0x452b93a8> (a java.util.Vector)
        at org.apache.lucene.index.FieldInfos.fieldInfo(FieldInfos.java:155)
        at org.apache.lucene.index.FieldInfos.fieldName(FieldInfos.java:151)
        at
org.apache.lucene.index.SegmentTermEnum.readTerm(SegmentTermEnum.java:149)
        at org.apache.lucene.index.SegmentTermEnum.next(SegmentTermEnum.java:115)
        at
org.apache.lucene.index.TermInfosReader.scanEnum(TermInfosReader.java:143)
        at org.apache.lucene.index.TermInfosReader.get(TermInfosReader.java:137)
        at org.apache.lucene.index.SegmentTermDocs.seek(SegmentTermDocs.java:51)
        at org.apache.lucene.index.IndexReader.termDocs(IndexReader.java:364)
        at org.apache.lucene.search.TermQuery$TermWeight.scorer(TermQuery.java:59)
        at
org.apache.lucene.search.BooleanQuery$BooleanWeight.scorer(BooleanQuery.java:165)
        at
org.apache.lucene.search.BooleanQuery$BooleanWeight.scorer(BooleanQuery.java:165)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:154)
        at
gov.gsa.search.SearcherByPageAndSortedField.search(SearcherByPageAndSortedField.java:317)
        at
gov.gsa.search.SearcherByPageAndSortedField.search(SearcherByPageAndSortedField.java:203)
        at
gov.gsa.search.grants.SearchGrants.searchByPageAndSortedField(SearchGrants.java:308)
        at
gov.gsa.search.grants.SearchServlet.searchByIndex(SearchServlet.java:1541)
        at gov.gsa.search.grants.SearchServlet.getResults(SearchServlet.java:1325)
        at gov.gsa.search.grants.SearchServlet.doGet(SearchServlet.java:500)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:740)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:853)
        at
org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:247)
        at
org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:193)
        at
org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:256)
        at
org.apache.catalina.core.StandardPipeline$StandardPipelineValveContext.invokeNext(StandardPipeline.java:643)
        at
org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:480)
        at org.apache.catalina.core.ContainerBase.invoke(ContainerBase.java:995)
        at
org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)
        at
org.apache.catalina.core.StandardPipeline$StandardPipelineValveContext.invokeNext(StandardPipeline.java:643)
        at
org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:480)
        at org.apache.catalina.core.ContainerBase.invoke(ContainerBase.java:995)
        at
org.apache.catalina.core.StandardContext.invoke(StandardContext.java:2415)
        at
org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:180)
        at
org.apache.catalina.core.StandardPipeline$StandardPipelineValveContext.invokeNext(StandardPipeline.java:643)
        at
org.apache.catalina.valves.ErrorDispatcherValve.invoke(ErrorDispatcherValve.java:171)
        at
org.apache.catalina.core.StandardPipeline$StandardPipelineValveContext.invokeNext(StandardPipeline.java:641)
        at
org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:172)
        at
org.apache.catalina.core.StandardPipeline$StandardPipelineValveContext.invokeNext(StandardPipeline.java:641)
        at
org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:480)
        at org.apache.catalina.core.ContainerBase.invoke(ContainerBase.java:995)
        at
org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:174)
        at
org.apache.catalina.core.StandardPipeline$StandardPipelineValveContext.invokeNext(StandardPipeline.java:643)
        at
org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:480)
        at org.apache.catalina.core.ContainerBase.invoke(ContainerBase.java:995)
        at org.apache.coyote.tomcat4.CoyoteAdapter.service(CoyoteAdapter.java:223)
        at org.apache.jk.server.JkCoyoteHandler.invoke(JkCoyoteHandler.java:261)
        at org.apache.jk.common.HandlerRequest.invoke(HandlerRequest.java:360)
        at org.apache.jk.common.ChannelSocket.invoke(ChannelSocket.java:604)
        at
org.apache.jk.common.ChannelSocket.processConnection(ChannelSocket.java:562)
        at org.apache.jk.common.SocketConnection.runIt(ChannelSocket.java:679)
        at
org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadPool.java:619)
        at java.lang.Thread.run(Thread.java:534)"
0,jcr-tests: make property value(s) and property type(s) configurable. test-cases using Node.setProperty or Property.setValue mostly hardcode the value... there should be the possibility to specify type and value in the config.
0,"Improve reliability of canAddMixin. The current implementation of canAddMixin in JCR2SPI lacks flexibility. It only consults the (SPI) node type registry, checking for (1) whether the mixin exists, and (2) whether it is already present and (3) whether it's consistent with the node's type.

This is fine for stores where any legal mixin can be added anywhere. It doesn't work well for stores that are limited in what they can do; for instance when nt:file nodes can be made mix:versionable, but nt:folder nodes can't.

Proposal: enhance QNodeTypeDefinition with

  public Name[] getSupportedMixins();

where the return value is either null (no constraints or no constraints known), or a list of mixin types that are supported for this node type."
0,"Exclude the netcdf dependency. As discussed on the mailing list, the netcdf dependency we get through Tika since version 0.8 is only used in very rare cases and thus does not justify the added size overhead. We should thus exclude it from default installations."
0,"Utility to output total term frequency and df from a lucene index. This is a pair of command line utilities that provide information on the total number of occurrences of a term in a Lucene index.  The first  takes a field name, term, and index directory and outputs the document frequency for the term and the total number of occurrences of the term in the index (i.e. the sum of the tf of the term for each document).   The second reads the index to determine the top N most frequent terms (by document frequency) and then outputs a list of those terms along with  the document frequency and the total number of occurrences of the term. Both utilities are useful for estimating the size of the term's entry in the *prx files and consequent Disk I/O demands. "
0,"Add an explicit method to invoke IndexDeletionPolicy. Today, if one uses an IDP which holds onto segments, such as SnapshotDeletionPolicy, or any other IDP in the tests, those segments are left in the index even if the IDP no longer references them, until IW.commit() is called (and actually does something). I'd like to add a specific method to IW which will invoke the IDP's logic and get rid of the unused segments w/o forcing the user to call IW.commit(). There are a couple of reasons for that:

* Segments take up sometimes valuable HD space, and the application may wish to reclaim that space immediately. In some scenarios, the index is updated once in several hours (or even days), and waiting until then may not be acceptable.
* I think it's a cleaner solution than waiting for the next commit() to happen. One can still wait for it if one wants, but otherwise it will give you the ability to immediately get rid of those segments.
* TestSnapshotDeletionPolicy includes this code, which only strengthens (IMO) the need for such method:
{code}
// Add one more document to force writer to commit a
// final segment, so deletion policy has a chance to
// delete again:
Document doc = new Document();
doc.add(new Field(""content"", ""aaa"", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
writer.addDocument(doc);
{code}

If IW had an explicit method, that code would not need to exist there at all ...

Here comes the fun part - naming the baby:
* invokeDeletionPolicy -- describes exactly what is going to happen. However, if the user did not set IDP at all (relying on default, which I think many do), users won't understand what is it.
* deleteUnusedSegments - more user-friendly, assuming users understand what 'segments' are.

BTW, IW already has deleteUnusedFiles() which only tries to delete unreferenced files that failed to delete before (such as on Windows, due to e.g. open readers). Perhaps instead of inventing a new name, we can change IW.deleteUnusedFiles to call IndexFileDeleter.checkpoint (instead of deletePendingFiles) which deletes those files + calls IDP.onCommit()."
0,"Enable Throttling only during nightly builds. Some of my tests take forever even on a big :) machine. In order to speed up our tests we should default the IO throttling to NEVER and only run in during nightly.

"
0,Add bundle support for PostgreSQL. The class DbNameIndex does not work with this RDBMS since the RETURN_GENERATED_KEYS JDBC feature is not implemented in current PostgreSQL drivers.
0,"[PATCH] remove minor unneeded code stutter. Code has a repeated method call on isOrderable for no reason as such

{code}
public String getSupportedMethods() {
        String ms = super.getSupportedMethods();
        if (isOrderable()) {
            StringBuffer sb = new StringBuffer(ms);
            // Ordering
            if (isOrderable()) {
                sb.append("", "").append(OrderingResource.METHODS);
            }
            return sb.toString();
        } else {
            return ms;
        }
    }
{code}

patch cleans this up."
0,"SimpleWebdavServlet: avoid 404 for the root collection. in order to avoid strange 404 error when accessing the root collection in simple webdav servlet (thus missing workspace name), that request should either be redirected or handled by a create fake root that has no correspondance in the jsr170 repository."
0,Text.escapeIllegalJCRChars should be adjusted to match the 2.0 set of illegal chars. Text.escapeIllegalJCRChars still contains chars that were illegal in JCR 1.0 but were removed from the set for JCR 2.0
0,"Slow performance due to JCR-2138 (Prevent persistence of faulty back-references). In revision 782898, the following code was introduced:

updateReferences() {
    for (Iterator i = local.addedStates(); i.hasNext();) {
        ...
        if (hasItemState(state.getId())) {
            ...
        }
    }
}

This will try to fetch inexistent nodes from the persistence manager.
Depending on the persistence manager implementation, this is very slow.
I hope there is a way to avoid this call, or if not, speed it up.
"
0,Introduce SessionInfo parameter for AbstractRepositoryService.createRootNodeDefinition()  . SPI implementations might require access to the state of the current session in order to fulfill the contract of AbstractRepositoryService.createRootNodeDefinition(). I therefore suggest to add a SessionInfo parameter to this method. 
0,Allow tests to use random codec per field. Since we now have a real per field codec support we should enable to run the tests with a random codec per field. When I change something related to codecs internally I would like to ensure that whatever combination of codecs (except of preflex) I use the code works just fine. I created a RandomCodecProvider in LuceneTestCase that randomly selects the codec for fields when it sees them the first time. I disabled the test by default to leave the old randomize codec support in as it was / is.
0,"Non-standards configuration and tracking. A simple strict or setLenient is likely inadequate.  Each particular
non-standard behaviour should be tagged, and settable from the client.  A mask
for particular behavioural features could be provided, with STRICT meaning none
and LENIENT meaning all."
0,"ShingleFilter improvements. ShingleFilter should allow configuration of minimum shingle size (in addition to maximum shingle size), so that it's possible to (e.g.) output only trigrams instead of bigrams mixed with trigrams.  The token separator used in composing shingles should be configurable too."
0,"Upgrade to easymock 2.5.2. Currently we have a dependency on easymock:easymock:1.1 in the jackrabbit-parent pom's dependency management section. This dependency is not actually used, but I am planning to start using it soon in the core. Upgrading it to 2.5.2 would give us generics support and a lot of improvements and bug fixes.

Note that the upgrade changes the groupId of the dependency."
0,FastVectorHighlighter: support DisjunctionMaxQuery. Add DisjunctionMaxQuery support in FVH. 
0,"New node type namespaces should be automatically registered. A user currently needs to explicitly register any new namespaces used in new node types before registering the node types. See for example the problem on the mailing list:

    http://mail-archives.apache.org/mod_mbox/incubator-jackrabbit-dev/200603.mbox/%3c1142091097.13136.0.camel@localhost.localdomain%3e

The node type registration should be changed so that new namespaces are automatically registered."
0,"Replace deprecated TermAttribute by new CharTermAttribute. After LUCENE-2302 is merged to trunk with flex, we need to carry over all tokenizers and consumers of the TokenStreams to the new CharTermAttribute.

We should also think about adding a AttributeFactory that creates a subclass of CharTermAttributeImpl that returns collation keys in toBytesRef() accessor. CollationKeyFilter is then obsolete, instead you can simply convert every TokenStream to indexing only CollationKeys by changing the attribute implementation."
0,"[PATCH] Avoid checking for TermBuffer in SegmentTermEnum#scanTo. It seems that SegmentTermEnum#scanTo is a critical method which is called very often, especially whenever we iterate over a sequence of terms in an index.

When that method is called, the first thing happens is that it checks whether a temporary TermBuffer ""scratch"" has already been initialized.

In fact, this is not necessary. We can simply declare and initialize the ""scratch""-Buffer at the class-level (right now, the initial value is _null_). Java's lazy-loading capabilities allow this without adding any memory footprint for cases where we do not need that buffer.

The attached patch takes care of this. We now save one comparison per term.
In addition to that, the patch renames ""scratch"" to ""scanBuffer"", which aligns with the naming of the other two buffers that are declared in the class."
0,"Add numDeletedDocs to IndexReader. Add numDeletedDocs to IndexReader. Basically, the implementation is as simple as doing:
public int numDeletedDocs() {
  return deletedDocs == null ? 0 : deletedDocs.count();
}
in SegmentReader.
Patch to follow to include in all IndexReader extensions."
0,"Searchable.java: The info in the @deprecated tags do not refer to the search(Weight, etc...) versions.... E.g.

The javadoc for 
          void search(Query query, Filter filter, HitCollector results)
states:
          Deprecated. use search(Query, Filter, HitCollector) instead.
instead of:
          Deprecated. use search(Weight, Filter, HitCollector) instead.
"
0,"[PATCH] public static members in class TermVectorsWriter. hi all,

looking at the implementation of TermVectorsWriter, you'll find a bunch of
public static final members where the visibility could be reduced to be
protected. I don't see a reason for having them public if the class itself is
protected and all members are final values. May be somebody could check and
either commit or enlighten me ;-)

thx
Bernhard"
0,"Basic tool for checking & repairing an index. This has been requested a number of times on the mailing lists.  Most
recently here:

  http://www.gossamer-threads.com/lists/lucene/java-user/53474

I think we should provide a basic tool out of the box.
"
0,create jcr-browser contrib project. If noone oposes I'll start a contrib project called jcr-browser. A live demo of the initial code is available at http://edgarpoce.dyndns.org:8080/jcr-browser/
0,"Jcr-Server: missing-auth-mapping init parameter should have option for GuestCredential login. the missing-auth-mapping parameter of the servlets contained in jcr-server is defined as follows:

             <param-value>anonymous:anonymous</param-value>
             <description>
                 Defines how a missing authorization header should be handled.
                 1) If this init-param is missing, a 401 response is generated.
                    This is suiteable for clients (eg. webdav clients) for which
                    sending a proper authorization header is not possible if the
                    server never sent a 401.
                 2) If this init-param is present with an empty value,
                    null-credentials are returned, thus forcing an null login
                    on the repository.
                 3) If this init-param has a 'user:password' value, the respective
                    simple credentials are generated.
             </description>

JCR 2.0 introduces GuestCredentials used to obtain a ""anonymous"" session.

Therefore we should probably extend/modify the missing-auth-param in a way that
allows to distinguish between

- null-login
- guest login

in case of missing authorization header."
0,"Provide feedback mechanism to CredentialsProvider. If the remote server is using BASIC or NT authentication and you pass in 
invalid credentials you get stuck in an infinite for loop, repeatedly sending 
the same authentication request again and again to the server.  The for loop is 
in the executeMethod method of the HttpMethodDirector class.

Sample code:
=================================================================


import org.apache.commons.httpclient.Credentials;
import org.apache.commons.httpclient.NTCredentials;
import org.apache.commons.httpclient.UsernamePasswordCredentials;
import org.apache.commons.httpclient.HttpClient;
import org.apache.commons.httpclient.methods.GetMethod;
import org.apache.commons.httpclient.auth.*;

import java.io.IOException;
import java.io.BufferedInputStream;
import java.io.ByteArrayOutputStream;

/**
 * Created by IntelliJ IDEA.
 * User: dmartineau
 * Date: Nov 8, 2005
 * Time: 1:43:21 PM
 */
public class ShowProblem
{

    private String location;
    private String user;
    private String pass;
    private String domain;

    public ShowProblem(String location, String user, String pass, String domain)
    {
        this.location = location;
        this.user=user;
        this.pass=pass;
        this.domain=domain;

    }

    public int getFile()
    {
        int status = 500;
        HttpClient client = new HttpClient();
        client.getParams().setParameter(
            CredentialsProvider.PROVIDER, new CProvider(user,pass,domain));
        GetMethod httpget = new GetMethod(location);
        httpget.setDoAuthentication(true);

        try
        {
            // execute the GET
            status = client.executeMethod(httpget);
            if (status==200)
            {
                BufferedInputStream bin = new BufferedInputStream
(httpget.getResponseBodyAsStream());

                ByteArrayOutputStream bos = new ByteArrayOutputStream();
                int bytesRead = 0;
                byte[] buff = new byte[16384];

                while ( (bytesRead = bin.read(buff)) != -1) {
                    bos.write(buff, 0, bytesRead);
                }

                // display the results.
                System.out.println(new String(bos.toByteArray()));
            }
        }
        catch (Throwable t)
        {
            t.printStackTrace();
        }
        finally
        {
            // release any connection resources used by the method
            httpget.releaseConnection();
        }
        return status;

    }

    public static void main(String[] args)
    {
        ShowProblem showProblem = new ShowProblem(args[0],args[1],args[2],args
[3]);
        int response = showProblem.getFile();
        
    }



    class CProvider implements CredentialsProvider
    {
        private String user;
        private String password;
        private String domain;

        public CProvider(String user, String password, String domain)
        {
            super();
            this.user = user;
            this.password = password;
            this.domain = domain;
        }

        public Credentials getCredentials(final AuthScheme authscheme,final 
String host,int port,boolean proxy)
        throws CredentialsNotAvailableException
        {
            if (authscheme == null)
            {
                return null;
            }
            try
            {
                if (authscheme instanceof NTLMScheme)
                {
                    return new NTCredentials(user, password, host, domain);
                }
                else if (authscheme instanceof RFC2617Scheme)
                {
                    return new UsernamePasswordCredentials(user, password);
                }
                else
                {
                    throw new CredentialsNotAvailableException(""Unsupported 
authentication scheme: "" +
                        authscheme.getSchemeName());
                }
            }
            catch (IOException e)
            {
                throw new CredentialsNotAvailableException(e.getMessage(), e);
            }
        }

    }
}"
0,"Provide a JackrabbitNode#setMixins(String[] mixinNames) method. assume the following scenario:

- mixin A declares the mandatory property p
- mixin A' extends from A
- node n has mixin A'
- we'd like to migrate/downgrade node n from mixin A' to A

currently there's no easy way of replacing the assigned mixins.

assigning A first results in a NOP since A would be redundant.
removing A' first removes the mandatory property p.

a new method setMixins(String[]) would allow to migrate
a node from mixin A' to A while preserving 'shared' content.
the semantics of setMixins(String[]) would be similar to
Node.setPrimaryType(String)."
0,"Make BaseTokenStreamTestCase a bit more evil. Throw an exception from the Reader while tokenizing, stop after not consuming all tokens, sometimes spoon-feed chars from the reader..."
0,"Making Term Vectors more accessible. One of the big issues with term vector usage is that the information is loaded into parallel arrays as it is loaded, which are then often times manipulated again to use in the application (for instance, they are sorted by frequency).

Adding a callback mechanism that allows the vector loading to be handled by the application would make this a lot more efficient.

I propose to add to IndexReader:
abstract public void getTermFreqVector(int docNumber, String field, TermVectorMapper mapper) throws IOException;
and a similar one for the all fields version

Where TermVectorMapper is an interface with a single method:
void map(String term, int frequency, int offset, int position);

The TermVectorReader will be modified to just call the TermVectorMapper.  The existing getTermFreqVectors will be reimplemented to use an implementation of TermVectorMapper that creates the parallel arrays.  Additionally, some simple implementations that automatically sort vectors will also be created.

This is my first draft of this API and is subject to change.  I hope to have a patch soon.

See http://www.gossamer-threads.com/lists/lucene/java-user/48003?search_string=get%20the%20total%20term%20frequency;#48003 for related information."
0,Stop creating huge arrays to represent the absense of field norms. Creating and keeping around huge arrays that hold a constant value is very inefficient both from a heap usage standpoint and from a localility of reference standpoint. It would be much more efficient to use null to represent a missing norms table.
0,"Realm from authentication challenge unavailable. There is currently no way to extract the authentication realm from HttpClient 
except to extract the authentication challenge header and parse it manually.

Either the realm needs to be available to the client or a method in 
Authenticator should extract the realm from a given authentication header.

The same problems occurs with determining which type of authentication is 
being used and what other options there are (basic, digest, NTLM, others)."
0,"Fix pom.xml in jackrabbit core (small fix, big return). Change the following dependency in pom.xml to match what is really available at the maven repos:
FROM:
    <dependency>
      <groupId>jsr170</groupId>
      <artifactId>jcr</artifactId>
      <version>1.0</version>
    </dependency>


TO:
    <dependency>
      <groupId>javax.jcr</groupId>
      <artifactId>jcr</artifactId>
      <version>1.0</version>
    </dependency>
"
0,"Allow configuration of SO_LINGER. There is currently no way to configure the SO_LINGER option on a socket.

Please change the HttpClient class to allow the configuration of the SO_LINGER
option on a socket, similar to the way the SO_TIMEOUT can be configured.

Suggested extension to the interface of the HttpClient class:
- Add method setSoLinger() to set the current setting for SO_LINGER. The method
could accept one argument. A negative value could indicate that the SO_LINGER
should be disabled.
- Add method getSoLinger() that returns the current setting for SO_LINGER. A
negative value would indicate that the SO_LINGER option is disabled.

See:
http://java.sun.com/j2se/1.4.2/docs/api/java/net/Socket.html#setSoLinger(boolean,%20int)"
0,"'ant generate-maven-artifacts' should work for lucene+solr 3.x+. The maven build scripts need to be updated so that solr uses the artifacts from lucene.

For consistency, we should be able to have a different 'maven_version' then the 'version'  That is, we want to build: 3.1-SNAPSHOT with a jar file: 3.1-dev"
0,"TestUTF32ToUTF8 can run forever. Stress testing this particular test uncovered that the testRandomRanges testcase can run forever, depending on the random numbers picked..."
0,"Add maven-eclipse-plugin properties to project.xml for easier configuration in IDE. - add the maven.eclipse.resources.addtoclasspath=true property to project.properties (make the eclipse plugin create source dirs also for resources). 

- add the <eclipse.dependency>true</eclipse.dependency> property to all the jackrabbit internal dependencies in all the POMs (all the dependencies with ""jackrabbit"" groupId) so internal dependencies becomes project dependencies in eclipse."
0,"Rework of the TermInfosReader class to remove the Terms[], TermInfos[], and the index pointer long[] and create a more memory efficient data structure.. Basically packing those three arrays into a byte array with an int array as an index offset.  

The performance benefits are stagering on my test index (of size 6.2 GB, with ~1,000,000 documents and ~175,000,000 terms), the memory needed to load the terminfos into memory were reduced to 17% of there original size.  From 291.5 MB to 49.7 MB.  The random access speed has been made better by 1-2%, load time of the segments are ~40% faster as well, and full GC's on my JVM were made 7 times faster.

I have already performed the work and am offering this code as a patch.  Currently all test in the trunk pass with this new code enabled.  I did write a system property switch to allow for the original implementation to be used as well.

-Dorg.apache.lucene.index.TermInfosReader=default or small

I have also written a blog about this patch here is the link.

http://www.nearinfinity.com/blogs/aaron_mccurry/my_first_lucene_patch.html



"
0,"Cleanup MMapDirectory to use only one MMapIndexInput impl with mapping sized of powers of 2. Robert and me discussed a little bit after Mike's investigations, that using SingleMMapIndexinput together with MultiMMapIndexInput leads to hotspot slowdowns sometimes.

We had the following ideas:
- MultiMMapIndexInput is almost as fast as SingleMMapIndexInput, as the switching between buffer boundaries is done in exception catch blocks. So normal code path is always the same like for Single*
- Only the seek method uses strange calculations (the modulo is totally bogus, it could be simply: int bufOffset = (int) (pos % maxBufSize); - very strange way of calculating modulo in the original code)
- Because of speed we suggest to no longer use arbitrary buffer sizes. We should pass only the power of 2 to the indexinput as size. All calculations in seek and anywhere else would be simple bit shifts and AND operations (the and masks for the modulo can be calculated in the ctor like NumericUtils does when calculating precisionSteps).
- the maximum buffer size will now be 2^30, not 2^31-1. But thats not an issue at all. In my opinion, a buffer size of 2^31-1 is stupid in all cases, as it will no longer fit page boundaries and mmapping gets harder for the O/S.

We will provide a patch with those cleanups."
0,"Allow easy extension of RAMDirectory. RAMDirectory uses RAMFiles to store the data. RAMFile offers a newBuffer() method for extensions to override and allocate buffers differently, from e.g. a pool or something. However, RAMDirectory always allocates RAMFile and doesn't allow allocating a RAMFile extension, which makes RAMFile.newBuffer() unusable.

I think we can simply introduce a newRAMFile() method on RAMDirectory and make the RAMFiles map protected, and it will allow really extending RAMDir.

I will post a patch later."
0,"DateField class should be public. The class org.apache.jackrabbit.core.query.lucene.DateField should be made public.  It has several public methods which are useful but are currently not accessible because the class itself is not accessible outside of its package.  All of the other Field classes in that package are public and accessible (LongField, DoubleField, etc.)"
0,"Spellchecker doesn't need to store ngrams. The spellchecker in contrib stores the ngrams although this doesn't seem to be necessary. This patch changes that, I will commit it unless someone objects. This improves indexing speed and index size. Some numbers on a small test I did:

Input of the original index: 2200 text files, index size 5.3 MB, indexing took 17 seconds

Spell index before patch: about 60.000 documents, index size 13 MB, indexing took 62 seconds
Spell index after patch: about 60.000 documents, index size 6.3 MB, indexing took 52 seconds

BTW, the test case fails even before this patch. I'll probaby submit another issue about how to fix that.
"
0,"TCK: NodeReadMethodsTest#testGetPrimaryItemItemNotFoundException selects wrong test data. Method locateNodeWithoutPrimaryItem is used to locate recursively node which does not define a primary item, but this method calls internally locateNodeWithPrimaryItem instead of locateNodeWithoutPrimaryItem."
0,"test cases relying on Node.equals(). Several test cases rely on Node.equals to compare nodes, where instead isSame() should be used:

org.apache.jackrabbit.test.api.NodeTest.testNodeIdentity(NodeTest.java:751)
org.apache.jackrabbit.test.api.NodeTest.testNodeIdentity(NodeTest.java:753)
org.apache.jackrabbit.test.api.version.VersionHistoryTest.testInitallyGetAllVersionsContainsTheRootVersion(VersionHistoryTest.java:126)
org.apache.jackrabbit.test.api.version.VersionHistoryTest.testGetVersion(VersionHistoryTest.java:180)
org.apache.jackrabbit.test.api.version.CheckinTest.testMultipleCheckinHasNoEffect(CheckinTest.java:93)
org.apache.jackrabbit.test.api.version.VersionGraphTest.testInitialBaseVersionPointsToRootVersion(VersionGraphTest.java:47)
org.apache.jackrabbit.test.api.version.RemoveVersionTest.testRemoveVersionAdjustPredecessorSet(RemoveVersionTest.java:120)
org.apache.jackrabbit.test.api.version.RemoveVersionTest.testRemoveVersionAdjustSucessorSet(RemoveVersionTest.java:144)

 "
0,"Remove/deprecate IndexReader.undeleteAll. This API is rather dangerous in that it's ""best effort"" since it can only un-delete docs that have not yet been merged away, or, dropped (as of LUCENE-2010).

Given that it exposes impl details of how Lucene prunes deleted docs, I think we should remove this API.

Are there legitimate use cases....?"
0,"Rethink LocalizedTestCaseRunner with JUnit 4 - Clover OOM. As a spinn off from this [conversation|http://www.lucidimagination.com/search/document/ae20885bf5baedc5/build_failed_in_hudson_lucene_3_x_116#7ed351341152ee2d] we should rethink the way how we execute testcases with different locals since glover reports appears to throw OOM errors b/c Junit treats each local as a single test case run.

Here are some options:
* select the local at random only run the test with a single local
* set the local via system property -Dtest.locale=en.EN
* run with the default locale only -Dtest.skiplocale=true
* one from the above but only if instrumented with clover (let common tests run all the locale)

"
0,"When adding a large (100MB) binary to the DbDataStore, it fails with an insufficient memory exception. Attached is a small test case. It fails during save(). I think this is related to what I mentioned in http://mail-archives.apache.org/mod_mbox/jackrabbit-dev/200711.mbox/%3c00fc01c832b9$f1f08730$7309240a@goku%3e

The full stacktrace is the following:

javax.jcr.RepositoryException: /: unable to update item.: failed to write property state: cafebabe-cafe-babe-cafe-babecafebabe/{}bin: failed to write property state: cafebabe-cafe-babe-cafe-babecafebabe/{}bin
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1252)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:858)
	at org.apache.jackrabbit.core.data.BigBinaryTest.testBigBinary(BigBinaryTest.java:16)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at org.apache.jackrabbit.test.AbstractJCRTest.run(AbstractJCRTest.java:404)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:128)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)
Caused by: org.apache.jackrabbit.core.state.ItemStateException: failed to write property state: cafebabe-cafe-babe-cafe-babecafebabe/{}bin
	at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.store(DatabasePersistenceManager.java:487)
	at org.apache.jackrabbit.core.persistence.AbstractPersistenceManager.store(AbstractPersistenceManager.java:75)
	at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.store(DatabasePersistenceManager.java:282)
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:687)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:856)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:324)
	at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:313)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:300)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:306)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1244)
	... 21 more
Caused by: org.apache.jackrabbit.core.data.DataStoreException: Can not read identifier a2ada2d96d0b05214288efa03be9005a5bb98c9b: Memoria insuficiente. Utilice cursores del servidor para result sets grandes:Java heap space. Tamao de result set:104.857.723. Tamao memoria total JVM:66.650.112.: Memoria insuficiente. Utilice cursores del servidor para result sets grandes:Java heap space. Tamao de result set:104.857.723. Tamao memoria total JVM:66.650.112.
	at org.apache.jackrabbit.core.data.db.DbDataStore.convert(DbDataStore.java:438)
	at org.apache.jackrabbit.core.data.db.DbDataStore.getInputStream(DbDataStore.java:481)
	at org.apache.jackrabbit.core.data.db.DbDataRecord.getStream(DbDataRecord.java:61)
	at org.apache.jackrabbit.core.value.BLOBInDataStore.getStream(BLOBInDataStore.java:93)
	at org.apache.jackrabbit.core.persistence.util.Serializer.serialize(Serializer.java:198)
	at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.store(DatabasePersistenceManager.java:476)
	... 30 more
Caused by: com.microsoft.sqlserver.jdbc.SQLServerException: Memoria insuficiente. Utilice cursores del servidor para result sets grandes:Java heap space. Tamao de result set:104.857.723. Tamao memoria total JVM:66.650.112.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDriverError(Unknown Source)
	at com.microsoft.sqlserver.jdbc.DBComms.receive(Unknown Source)
	at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.doExecutePreparedStatement(Unknown Source)
	at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement$PreparedStatementExecutionRequest.executeStatement(Unknown Source)
	at com.microsoft.sqlserver.jdbc.CancelableRequest.execute(Unknown Source)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeRequest(Unknown Source)
	at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.execute(Unknown Source)
	at org.apache.jackrabbit.core.persistence.bundle.util.ConnectionRecoveryManager.executeStmtInternal(ConnectionRecoveryManager.java:362)
	at org.apache.jackrabbit.core.persistence.bundle.util.ConnectionRecoveryManager.executeStmtInternal(ConnectionRecoveryManager.java:292)
	at org.apache.jackrabbit.core.persistence.bundle.util.ConnectionRecoveryManager.executeStmt(ConnectionRecoveryManager.java:257)
	at org.apache.jackrabbit.core.persistence.bundle.util.ConnectionRecoveryManager.executeStmt(ConnectionRecoveryManager.java:237)
	at org.apache.jackrabbit.core.data.db.DbDataStore.getInputStream(DbDataStore.java:474)
	... 34 more
org.apache.jackrabbit.core.state.ItemStateException: failed to write property state: cafebabe-cafe-babe-cafe-babecafebabe/{}bin
	at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.store(DatabasePersistenceManager.java:487)
	at org.apache.jackrabbit.core.persistence.AbstractPersistenceManager.store(AbstractPersistenceManager.java:75)
	at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.store(DatabasePersistenceManager.java:282)
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:687)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:856)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:324)
	at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:313)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:300)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:306)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1244)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:858)
	at org.apache.jackrabbit.core.data.BigBinaryTest.testBigBinary(BigBinaryTest.java:16)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at org.apache.jackrabbit.test.AbstractJCRTest.run(AbstractJCRTest.java:404)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:128)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)
Caused by: org.apache.jackrabbit.core.data.DataStoreException: Can not read identifier a2ada2d96d0b05214288efa03be9005a5bb98c9b: Memoria insuficiente. Utilice cursores del servidor para result sets grandes:Java heap space. Tamao de result set:104.857.723. Tamao memoria total JVM:66.650.112.: Memoria insuficiente. Utilice cursores del servidor para result sets grandes:Java heap space. Tamao de result set:104.857.723. Tamao memoria total JVM:66.650.112.
	at org.apache.jackrabbit.core.data.db.DbDataStore.convert(DbDataStore.java:438)
	at org.apache.jackrabbit.core.data.db.DbDataStore.getInputStream(DbDataStore.java:481)
	at org.apache.jackrabbit.core.data.db.DbDataRecord.getStream(DbDataRecord.java:61)
	at org.apache.jackrabbit.core.value.BLOBInDataStore.getStream(BLOBInDataStore.java:93)
	at org.apache.jackrabbit.core.persistence.util.Serializer.serialize(Serializer.java:198)
	at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.store(DatabasePersistenceManager.java:476)
	... 30 more
Caused by: com.microsoft.sqlserver.jdbc.SQLServerException: Memoria insuficiente. Utilice cursores del servidor para result sets grandes:Java heap space. Tamao de result set:104.857.723. Tamao memoria total JVM:66.650.112.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDriverError(Unknown Source)
	at com.microsoft.sqlserver.jdbc.DBComms.receive(Unknown Source)
	at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.doExecutePreparedStatement(Unknown Source)
	at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement$PreparedStatementExecutionRequest.executeStatement(Unknown Source)
	at com.microsoft.sqlserver.jdbc.CancelableRequest.execute(Unknown Source)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeRequest(Unknown Source)
	at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.execute(Unknown Source)
	at org.apache.jackrabbit.core.persistence.bundle.util.ConnectionRecoveryManager.executeStmtInternal(ConnectionRecoveryManager.java:362)
	at org.apache.jackrabbit.core.persistence.bundle.util.ConnectionRecoveryManager.executeStmtInternal(ConnectionRecoveryManager.java:292)
	at org.apache.jackrabbit.core.persistence.bundle.util.ConnectionRecoveryManager.executeStmt(ConnectionRecoveryManager.java:257)
	at org.apache.jackrabbit.core.persistence.bundle.util.ConnectionRecoveryManager.executeStmt(ConnectionRecoveryManager.java:237)
	at org.apache.jackrabbit.core.data.db.DbDataStore.getInputStream(DbDataStore.java:474)
	... 34 more

"
0,"change jdk & icu collation to use byte[]. Now that term is byte[], we should switch collation to use byte[] instead of 'indexablebinarystring'.

This is faster and results in much smaller sort keys.

I figure we can work it out here, and fix termrangequery to use byte in parallel, but we can already test sorting etc now."
0,"improve performance when saving a node with a large number of child nodes (e.g. > 10k child node entries). JCR-307 brought a significant improvement WRT saving nodes with a large number of child nodes

unfortunately JCR-2579 broke part of the optimization (see NodeState.setChildNodeEntries(List))."
0,"[PATCH] cleaner API for Field.Text. Currently there are four methods named Field.Text(). As those methods have 
the same name and a very similar method signature, everyone will think these 
are just convenience methods that do the same thing. But they behave 
differently: the one that takes a Reader doesn't store the data, the one that 
takes a String does. I know that this is documented, but it's still not a nice 
API. Methods that behave differently should have diffent names. The attached 
patch deprecates two of the old methods and adds two new ones named 
Field.StoredText(). I think this is much easier to understand from the 
programmer's point-of-view and will help avoid bugs."
0,"Improve logging in NodeTypeRegistry.persistCustomNodeTypeDefs. When the closing of out in the finally block of persistCustomNodeTypeDefs throws an IOException this is ignored. At least some logging should take place, because an IOException here might still mean that the custom nodetype definitions were not stored correctly. This is the case, for instance, when a DatabaseFileSystem is used: the call to out.close triggers the SQL statement execution which causes an IOException if it fails. "
0,"Unit and integration test cases for the new Similarities. Write test cases to test the new Similarities added in [LUCENE-3220|https://issues.apache.org/jira/browse/LUCENE-3220]. Two types of test cases will be created:
 * unit tests, in which mock statistics are provided to the Similarities and the score is validated against hand calculations;
 * integration tests, in which a small collection is indexed and then searched using the Similarities.

Performance tests will be performed in a separate issue."
0,IndexReader.FieldOption has incomplete Javadocs. IndexReader.FieldOption has no javadocs at all.
0,"Revise internal data structures of ThreadSafeClientConnManager. ThreadSafeClientConnManager internal data structures can be improved:
- keep track of issued connections with weak references
- use class derived from WeakReference instead of a lookup table for callbacks from ReferenceThread
  (or drop ReferenceThread in favor of occasionally polling the issued connections for leaks)
"
0,"[PATCH] Efficiently retrieve sizes of field values. Sometimes an application would like to know how large a document is before retrieving it.  This can be important for memory management or choosing between algorithms, especially in cases where documents might be very large.

This patch extends the existing FieldSelector mechanism with two new FieldSelectorResults:  SIZE and SIZE_AND_BREAK.  SIZE creates fields on the retrieved document that store field sizes instead of actual values.  SIZE_AND_BREAK is especially efficient if one field comprises the bulk of the document size (e.g., the body field) and can thus be used as a reasonable size approximation.
"
0,"remove IndexSearcher.docFreq/maxDoc. As pointed out by Mark on SOLR-1632, these are no longer used by the scoring system.

We've added new stats to Lucene, so having these methods on indexsearcher makes no sense.
Its confusing to people upgrading if they subclassed IndexSearcher to provide distributed stats,
only to find these are not used (LUCENE-3555 has a correct API for them to do this).

So I think we should remove these in 4.0."
0,"Cookie class cannot handle IPv6 literals. When performing requests using IPv6 literals, Cookie.setDomain() will attempt to trim the port number by cutting off the domain string at the first colon. This leads to MalformedCookieExceptions being thrown by CookieSpecBase later on."
0,"norms file can become unexpectedly enormous. 
Spinoff from this user thread:

   http://www.gossamer-threads.com/lists/lucene/java-user/46754

Norms are not stored sparsely, so even if a doc doesn't have field X
we still use up 1 byte in the norms file (and in memory when that
field is searched) for that segment.  I think this is done for
performance at search time?

For indexes that have a large # documents where each document can have
wildly varying fields, each segment will use # documents times # fields
seen in that segment.  When optimize merges all segments, that product
grows multiplicatively so the norms file for the single segment will
require far more storage than the sum of all previous segments' norm
files.

I think it's uncommon to have a huge number of distinct fields (?) so
we would need a solution that doesn't hurt the more common case where
most documents have the same fields.  Maybe something analogous to how
bitvectors are now optionally stored sparsely?

One simple workaround is to disable norms.
"
0,"Contrib Analyzer Setters should be deprecated and replace with ctor arguments. Some analyzers in contrib provide setters for stopword / stem exclusion sets / hashtables etc. Those setters should be deprecated as they yield unexpected behaviour. The way they work is they set the reusable token stream instance to null in a thread local cache which only affects the tokenstream in the current thread. Analyzers itself should be immutable except of the threadlocal. 

will attach a patch soon."
0,"make BaseCharFilter more efficient in performance. Performance degradation in Solr 1.4 was reported. See:

http://www.lucidimagination.com/search/document/43c4bdaf5c9ec98d/html_stripping_slower_in_solr_1_4

The inefficiency has been pointed out in BaseCharFilter javadoc by Mike:

{panel}
NOTE: This class is not particularly efficient. For example, a new class instance is created for every call to addOffCorrectMap(int, int), which is then appended to a private list. 
{panel}
"
0,"Analyzer for Latvian. Less aggressive form of Kreslins' phd thesis: A stemming algorithm for Latvian.
"
0,"Move multipart request to a new RequestEntity type. Multipart posts are currently handled via a separate post method, the MultipartPostMethod.  This 
separate method is unnecessary given the new RequestEntity mechanism."
0,"Deprecate StandardBenchmarker and ""old"" benchmarker code in favor of the Task based approach. We should deprecate the StandardBechmarker code that was the start of the benchmark contribution in favor of the much easier to use/extend byTask benchmark code"
0,Create docvalues based grouped facet collector. Create docvalues based grouped facet collector. Currently only term based collectors have been implemented (LUCENE-3802).
0,"Port 80 is needed to run tests. I'm trying to upgrade to Cactus 1.4.1 and StrutsTest 1.9.  My tests where 
working about fine a month ago with nightly builds of both.  Now I get the 
following error:

    [junit] Testcase: testCreate took 0.2 sec
    [junit]     Caused an ERROR
    [junit] port out of range:-1
    [junit] java.lang.IllegalArgumentException: port out of range:-1
    [junit]     at java.net.InetSocketAddress.<init>
(InetSocketAddress.java:103)
    [junit]     at java.net.Socket.<init>(Socket.java:119)
    [junit]     at org.apache.commons.httpclient.HttpConnection.open
(HttpConnection.java:260)
    [junit]     at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:255)
    [junit]     at 
org.apache.cactus.client.HttpClientConnectionHelper.dispatch24_connect
(HttpClientConnectionHelper.jav
a;org/apache/cactus/util/log/LogAspect.aj(1k):164)
    [junit]     at 
org.apache.cactus.client.HttpClientConnectionHelper.around24_connect
(HttpClientConnectionHelper.java;
org/apache/cactus/util/log/LogAspect.aj(1k):1236)
    [junit]     at org.apache.cactus.client.HttpClientConnectionHelper.connect
(HttpClientConnectionHelper.java;org/apach
e/cactus/util/log/LogAspect.aj(1k):106)
    [junit]     at org.apache.cactus.client.AbstractHttpClient.callRunTest
(AbstractHttpClient.java;org/apache/cactus/uti
l/log/LogAspect.aj(1k):186)
    [junit]     at org.apache.cactus.client.AbstractHttpClient.dispatch2_doTest
(AbstractHttpClient.java;org/apache/cactu
s/util/log/LogAspect.aj(1k):109)
    [junit]     at org.apache.cactus.client.AbstractHttpClient.around2_doTest
(AbstractHttpClient.java;org/apache/cactus/
util/log/LogAspect.aj(1k):1236)
    [junit]     at org.apache.cactus.client.AbstractHttpClient.doTest
(AbstractHttpClient.java;org/apache/cactus/util/log
/LogAspect.aj(1k):104)
    [junit]     at org.apache.cactus.AbstractWebTestCase.runGenericTest
(AbstractWebTestCase.java:260)
    [junit]     at org.apache.cactus.ServletTestCase.runTest
(ServletTestCase.java:133)
    [junit]     at org.apache.cactus.AbstractTestCase.runBare
(AbstractTestCase.java:195)

I found that I had to add port 80 (:80) get it to work with the new stuff:

cactus.properties from: cactus.contextURL = http://localhost/myApp

To: cactus.contextURL = http://localhost:80/myApp

build.xml from:

    <target name=""tomcat.navigationAction"" depends=""deploy"" if=""tomcat.home"">
        <!-- We suppose our webapp is named ""onpoint"" -->
        <runservertests testURL=""http://localhost/${webapp.name}"" 
            startTarget=""start.tomcat"" 
            stopTarget=""stop.tomcat"" 
            testTarget=""test.navigationAction""/>
    </target>

To:

    <target name=""tomcat.navigationAction"" depends=""deploy"" if=""tomcat.home"">
        <!-- We suppose our webapp is named ""onpoint"" -->
        <runservertests testURL=""http://localhost:80/
${webapp.name}/ServletRedirector?Cactus_Service=RUN_TEST"" 
            startTarget=""start.tomcat"" 
            stopTarget=""stop.tomcat"" 
            testTarget=""test.navigationAction""/>
    </target>"
0,"DateParser refactoring; Stateful cookie specs. Presently DateParser is tightly coupled with the DefaultHttpParams class. I find
this sub-optimal from the design standpoint. Moreover, I believe that date
patterns should be specifiable at the method, host, and client levels, not only
global one. Currently this is not the case, and out of sync with the rest of the
preference framework, which can result in quite a bit of confusion.

When refactoring the DateParser class I also realized that the cookie specs were
shared by all the HttpMethod instances and as such had to be stateless. Even
though it is presently the case, technically there's nothing that prevents the
user from implementing a stateful cookie spec, plugging it into HttpClient, and
by doing so potentially causing quite unpleasant concurrency issues. Therefore,
I believe pluggable cookie specs MAY NOT be shared. There should be a cookie
spec instance created per method invocation 

Oleg"
0,"Test compilation error. The Lucence test fails with the following error: (latest revision from SVN)

compile-test:
   [mkdir] Created dir: /opt/lucene/lucene/build/classes/test
   [javac] Compiling 79 source files to /opt/lucene/lucene/build/classes/test
   [javac]
/opt/lucene/lucene/src/test/org/apache/lucene/index/TermInfosTest.java:89:
cannot resolve symbol
   [javac] symbol  : constructor TermInfosWriter
(org.apache.lucene.store.Directory,java.lang.String,org.apache.lucene.index.FieldInfos)
   [javac] location: class org.apache.lucene.index.TermInfosWriter
   [javac]     TermInfosWriter writer = new TermInfosWriter(store,
""words"", fis);

I see that TermInfosWriter was changed two days back to add another
argument (interval) to its constructor.

Mailing lists for Lucene do not seem to be responding (sending emails to
subscribe bounces back and also I do not see any mails after March 2nd being
archived in either the user or dev lists). So I am using the bug database to
inform the test failure and submit a simple patch that uses the value of 128
(the default in the TermInfosWriter class) as interval in the test case."
0,"Resolve JUnit assert deprecations. Many tests use assertEquals methods which have been deprecated.  The culprits are assertEquals(float, float), assertEquals(double, double) and assertEquals(Object[], Object[]).  Although not a big issue, they annoy me every time I see them so I'm going to fix them."
0,"Create ChainingCollector. ChainingCollector allows chaining a bunch of Collectors, w/o them needing to know or care about each other, and be passed into Lucene's search API, since it is a Collector on its own. It is a convenient, yet useful, class. Will post a patch w/ it shortly."
0,"Use Commons IO 1.4. Commons IO contains a number of utility classes and methods for working with files and streams. Many of those utilities would be quite useful in Jackrabbit, so I'd like to introduce commons-io 1.4 as a dependency to jackrabbit-core.

"
0,"add shutdown() or logoutAll() method to TransientRepository. It would be usefull to be able to explicitly ask a TransientRepository to shut down, instead of relying on all sessions to be closed by session.logout()."
0,"[PATCH] Some Field methods use Classcast check instead of instanceof which is slow. I am not sure if this is because Lucene historically needed to work with older
JVM's but with modern JVM's, instanceof is much quicker. 

The Field.stringValue(), .readerValue(), and .binaryValue() methods all use
ClassCastException checking.

Using the following test-bed class, you will see that instanceof is miles quicker:

package com.aconex.index;

public class ClassCastExceptionTest {

    private static final long ITERATIONS = 100000;

    /**
     * @param args
     */
    public static void main(String[] args) {

        runClassCastTest(1); // once for warm up
        runClassCastTest(2);
        
        runInstanceOfCheck(1);
        runInstanceOfCheck(2);

    }
    private static void runInstanceOfCheck(int run) {
        long start = System.currentTimeMillis();

        Object foo = new Foo();
        for (int i = 0; i < ITERATIONS; i++) {
            String test;
            if(foo instanceof String) {
                System.out.println(""Is a string""); // should never print
            }
        }
        long end = System.currentTimeMillis();
        long diff = end - start;
        System.out.println(""InstanceOf checking run #"" + run + "": "" + diff + ""ms"");
        
    }

    private static void runClassCastTest(int run) {
        long start = System.currentTimeMillis();

        Object foo = new Foo();
        for (int i = 0; i < ITERATIONS; i++) {
            String test;
            try {
                test = (String)foo;
            } catch (ClassCastException c) {
                // ignore
            }
        }
        long end = System.currentTimeMillis();
        long diff = end - start;
        System.out.println(""ClassCast checking run #"" + run + "": "" + diff + ""ms"");
    }

    private static final class Foo {
    }

}


Results
=======

Run #1

ClassCast checking run #1: 1660ms
ClassCast checking run #2: 1374ms
InstanceOf checking run #1: 8ms
InstanceOf checking run #2: 4ms


Run #2
ClassCast checking run #1: 1280ms
ClassCast checking run #2: 1344ms
InstanceOf checking run #1: 7ms
InstanceOf checking run #2: 2ms


Run #3
ClassCast checking run #1: 1347ms
ClassCast checking run #2: 1250ms
InstanceOf checking run #1: 7ms
InstanceOf checking run #2: 2ms

This could explain why Documents with more Fields scales worse, as in, for lots
of Documents with lots of Fields, the effect is exacerbated."
0,"Make StopFilter.enablePositionIncrements explicit. I think the default for this should be true, ie, do not lose
information when filtering (preserve the positions of the original
tokens).

But, we can't change this without breaking back-compat.

So, as workaround, we should make the parameter explicit so one must
decide up front.
"
0,"Make cache limits configurable. The cache settings of the CacheManager class (JCR-619) can be adjusted programmatically (JCR-725), but it would be nice if there was also a way to set them with system properties or ideally as a part of the repository configuration."
0,"Changes.html should be visible to users for closed releases. Changes.html is currently available only in the dev page, for trunk. 
See LUCENE-1157 for discussion on where exactly to expose this."
0,"Remove some synchronization on CachingNamespaceResolver. The methods getQName() and getJCRName() are unnecessarily synchronized and cause monitor contention with concurrent calls to the methods of the NameCache interface (those are also synchronized).

I propose the following change:

Index: CachingNamespaceResolver.java
===================================================================
--- CachingNamespaceResolver.java	(revision 488245)
+++ CachingNamespaceResolver.java	(working copy)
@@ -84,7 +84,7 @@
     /**
      * @deprecated use {@link NameFormat#parse(String, NamespaceResolver)}
      */
-    public synchronized QName getQName(String name)
+    public QName getQName(String name)
             throws IllegalNameException, UnknownPrefixException {
         return NameFormat.parse(name, this);
     }
@@ -92,7 +92,7 @@
     /**
      * @deprecated use {@link NameFormat#format(QName, NamespaceResolver)}
      */
-    public synchronized String getJCRName(QName name)
+    public String getJCRName(QName name)
             throws NoPrefixDeclaredException {
         return NameFormat.format(name, this);
     }
"
0,"Change visibility of getComparator method in SortField from protected to public. Hi,

Currently I'm using SortField for the creation of FieldComparators, but I ran into an issue.
I cannot invoke SortField.getComparator(...) directly from my code, which forces me to use a  workaround. (subclass SortField and override the getComparator method with visiblity public)
I'm proposing to make this method public. Currently I do not see any problems changing the visibility to public, I do not know if there are any (and the reason why this method is currently protected)
I think that this is a cleaner solution then the workaround I used and also other developers can benefit from it. I will also attach a patch to this issue based on the code in the trunk (26th of May). place). 
Please let me know your thoughts about this.

Cheers,

Martijn

 "
0,"InternalValue should implement QValue.discard() for BINARY types. currently jackrabbit-core always extracts the BLOBFileValue in order to free resources. Since InternalValue now implements QValue this could be achieved on the InternalValue directly.
However, currently the base implementation is inherited instead of dealing with the BLOBFileValues.

"
0,"variables should be accessed through getters. Some attention should be placed on classes who shared their variables directly (as opposed to through a getter). This is sometimes OK for subclasses, but rarely good for other classes that use the objects. There's a small number of classes that have non-private variables, especially in the impl.conn & impl.conn.tsccm packages.

See HTTPCLIENT-745 ."
0,"BoostingNearQuery class (prototype). This patch implements term boosting for SpanNearQuery. Refer to: http://www.gossamer-threads.com/lists/lucene/java-user/62779

This patch works but probably needs more work. I don't like the use of 'instanceof', but I didn't want to touch Spans or TermSpans. Also, the payload code is mostly a copy of what's in BoostingTermQuery and could be common-sourced somewhere. Feel free to throw darts at it :)
"
0,"SPI POM improvements. While the SPI components were upgraded from the sandbox I didn't pay too much attention to the POM details and thus there still are a number of configuration entries that duplicate stuff from the Jackrabbit parent POM, etc.

I plan to get rid of any such duplication, remove some unneeded dependencies (spi-commons has a compile scope dependency on junit) and generally update the POMs to be in line with the other release components."
0,"Remove unnecessary NodeImpl references from LuceneQueryFactory. LuceneQueryFactory casts to NodeImpl just to get the node id. 
This info is available via the api as well, so the cast seems unnecessary.
I'll attach a patch for this tiny issue."
0,"SQL2 joins on empty sets are not efficient. It seems that in the cases where the LEFT side of the join doesn't contain any hits, the QueryEngine in unable to generate an efficient query for the RIGHT side, so it basically select all the possible nodes.
See this discussion as context [0].

Example:
LEFT side has hits, RIGHT side select is fast given some conditions: 
> SQL2 JOIN LEFT SIDE took 18 ms. fetched 145 rows.
> SQL2 JOIN RIGHT SIDE took 67 ms. fetched 0 rows.

LEFT side has no hits, RIGHT select everything
> SQL2 JOIN LEFT SIDE took 8 ms. fetched 0 rows.
> SQL2 JOIN RIGHT SIDE took 845 ms. fetched 13055 rows.
...so it fetches 130k nodes and doesn't keep any of them.


[0] http://jackrabbit.510166.n4.nabble.com/Strange-Search-Performance-problem-with-OR-td4507121.html
"
0,"QueryParser should use reusable token streams. Just like indexing, the query parser should use reusable token streams"
0,Create a analysis/uima module for UIMA based tokenizers/analyzers. As discussed in SOLR-3013 the UIMA Tokenizers/Analyzer should be refactored out in a separate module (modules/analysis/uima) as they can be used in plain Lucene. Then the solr/contrib/uima will contain only the related factories.
0,Backport FieldCacheTermsFilter code duplication removal to 3.x. In trunk I already cleaned up FieldCacheTermsFilter to not duplicate code of FieldCacheRangeFilter. This issue simply backports this.
0,"Pass potent SR to IRWarmer.warm(), and also call warm() for new segments. Currently warm() receives a SegmentReader without terms index and docstores.
It would be arguably more useful for the app to receive a fully loaded reader, so it can actually fire up some caches. If the warmer is undefined on IW, we probably leave things as they are.

It is also arguably more concise and clear to call warm() on all newly created segments, so there is a single point of warming readers in NRT context, and every subreader coming from getReader is guaranteed to be warmed up -> you don't have to introduce even more mess in your code by rechecking it.

"
0,"Webdav Simple: Delegate PROPPATCH to (extended) IOHandlers. complete description:
http://article.gmane.org/gmane.comp.apache.jackrabbit.devel/6582

Proposed solution
------------------------------------------------------------------------------------------------------------------------------------

The situation described before leads me to the following conclusion:

- IOHandler should not only read/write resource data and properties during GET, PUT, PROPFIND but should
   also take care of setting/removing properties upon PROPPATCH.

- Since the previous suggestion would still limit the properties to (jcr:encoding, jcr:mimeType and
   jcr:lastModified), we may think about changing the default nodetype for the jcr:content node to
   nt:unstructured.

I guess this would meet the requirements for those expecting a webDAV server that is (as a first step)
not limited regarding PROPPATCH. Second it would allow to have a handling of property modifications
which is specific for individual resource types instead of trying to set all properties to the uppermost node."
0,"Remove SortField.AUTO. I'd like to remove SortField.AUTO... it's dangerous for Lucene to
guess the type of your field, based on the first term it encounters.
It can easily be wrong, and, whether it's wrong or right could
suddenly change as you index different documents.

It unexepctedly binds SortField to needing an IndexReader to do the
guessing.

It's caused various problems in the past (most recently, for me on
LUCENE-1656) as we fix other issues/make improvements.

I'd prefer that users of Lucene's field sort be explicit about the
type that Lucene should cast the field to.  Someday, if we have
optional strong[er] typing of Lucene's fields, such type information
would already be known.  But in the meantime, I think users should be
explicit.
"
0,"IndexReader # doCommit - typo nit about v3.0 in trunk. Trunk is already in 3.0.1+ . But the documentation says -  ""In 3.0, this will become ... "".  Since it is already in 3.0, it might as well be removed. 
"
0,"paging collector. http://issues.apache.org/jira/browse/LUCENE-2127?focusedCommentId=12796898&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12796898

Somebody assign this to Aaron McCurry and we'll see if we can get enough votes on this issue to convince him to upload his patch.  :)"
0,"Feature Request: include contributed code for Plugin Proxy Detection. Attached is a zip file containing two classes - PluginProxyUtil and
ProxyDetectionException.  I've tested
PluginProxyUtil.detectProxy(URL) on Windows XP(JRE's 1.3/1.4/1.5 with IE) and
Solaris (JRE 1.4 with Netscape)
and it correctly detects browser plugin settings.  I don't have access to MacOS
X  to try it, but I doubt that
it works there anyway based on Dmitri's comments here:

http://forum.java.sun.com/thread.jspa?threadID=364342&tstart=120

Please change the header and package as necessary to include it in the contrib
section.  I plan to contribute
an example Applet that uses this code at some point - our app is way too
complicated to use as an example.
If you want to wait until that is done to include it, that's fine too.  Just
wanted to offer this up now in case
anyone else is looking for it."
0,"Maintain norms in a single file .nrm. Non-compound indexes are ~10% faster at indexing, and perform 50% IO activity comparing to compound indexes. But their file descriptors foot print is much higher. 

By maintaining all field norms in a single .nrm file, we can bound the number of files used by non compound indexes, and possibly allow more applications to use this format.

More details on the motivation for this in: http://www.nabble.com/potential-indexing-perormance-improvement-for-compound-index---cut-IO---have-more-files-though-tf2826909.html (in particular http://www.nabble.com/Re%3A-potential-indexing-perormance-improvement-for-compound-index---cut-IO---have-more-files-though-p7910403.html).
"
0,"Contributing a High-performance single-document main memory Apache Lucene fulltext search index.. Here is my contribution: a High-performance single-document main memory Apache Lucene fulltext 
search index. I'll try to attach the files, hoping for comments on how to proceed with this..."
0,CND support in jackrabbit-jcr-commons. It would be nice if the CND parsing functionality in spi-commons could be made available in jcr-commons for use by JCR clients that shouldn't have to know anything about the SPI.
0,"Code cleanup from all sorts of (trivial) warnings. I would like to do some code cleanup and remove all sorts of trivial warnings, like unnecessary casts, problems w/ javadocs, unused variables, redundant null checks, unnecessary semicolon etc. These are all very trivial and should not pose any problem.

I'll create another issue for getting rid of deprecated code usage, like LuceneTestCase and all sorts of deprecated constructors. That's also trivial because it only affects Lucene code, but it's a different type of change.

Another issue I'd like to create is about introducing more generics in the code, where it's missing today - not changing existing API. There are many places in the code like that.

So, with you permission, I'll start with the trivial ones first, and then move on to the others."
0,"Update copyright year to 2009. Our normal license headers don't contain copyright years, but the NOTICE.txt files do. We should update the year to 2009 where appropriate.
"
0,New MsOutlook Message Extractor. Sinse we are using poi 3.0.2 it will be useful to have a outlook message extractor
0,"A Property and a Node Can Have the Same Name . according to paragraph ""3.3.4"" of the of the JSR 283 specification (Public Review Draft), a property and a node can have the same name. "
0,"BaseTestRangeFilter can be extremely slow. The tests that extend BaseTestRangeFilter can sometimes be very slow:
TestFieldCacheRangeFilter, TestMultiTermConstantScore, TestTermRangeFilter

for example, TestFieldCacheRangeFilter just ran for 10 minutes on my computer before I killed it,
but i noticed these tests frequently run for over a minute.

I think at the least we should change these to junit4 so the index is built once in @beforeClass"
0,"ShingleMatrixFilter, a three dimensional permutating shingle filter. Backed by a column focused matrix that creates all permutations of shingle tokens in three dimensions. I.e. it handles multi token synonyms.

Could for instance in some cases be used to replaces 0-slop phrase queries with something speedier.

{code:java}
Token[][][]{
  {{hello}, {greetings, and, salutations}},
  {{world}, {earth}, {tellus}}
}
{code}

passes the following test  with 2-3 grams:

{code:java}
assertNext(ts, ""hello_world"");
assertNext(ts, ""greetings_and"");
assertNext(ts, ""greetings_and_salutations"");
assertNext(ts, ""and_salutations"");
assertNext(ts, ""and_salutations_world"");
assertNext(ts, ""salutations_world"");
assertNext(ts, ""hello_earth"");
assertNext(ts, ""and_salutations_earth"");
assertNext(ts, ""salutations_earth"");
assertNext(ts, ""hello_tellus"");
assertNext(ts, ""and_salutations_tellus"");
assertNext(ts, ""salutations_tellus"");
{code}

Contains more and less complex tests that demonstrate offsets, posincr, payload boosts calculation and construction of a matrix from a token stream.

The matrix attempts to hog as little memory as possible by seeking no more than maximumShingleSize columns forward in the stream and clearing up unused resources (columns and unique token sets). Can still be optimized quite a bit though."
0,"ShingleFilter: don't output all-filler shingles/unigrams; also, convert from TermAttribute to CharTermAttribute. When the input token stream to ShingleFilter has position increments greater than one, filler tokens are inserted for each position for which there is no token in the input token stream.  As a result, unigrams (if configured) and shingles can be filler-only.  Filler-only output tokens make no sense - these should be removed.

Also, because TermAttribute has been deprecated in favor of CharTermAttribute, the patch will also convert TermAttribute usages to CharTermAttribute in ShingleFilter."
0,"ant generate-maven-artifacts target broken for contrib. When executing 'ant generate-maven-artifacts' from a pristine checkout of branch_3x/lucene or trunk/lucene the following error is encountered:

{code}
dist-maven:
     [copy] Copying 1 file to /home/drew/lucene/branch_3x/lucene/build/contrib/analyzers/common
[artifact:install-provider] Installing provider: org.apache.maven.wagon:wagon-ssh:jar:1.0-beta-2:runtime
[artifact:pom] An error has occurred while processing the Maven artifact tasks.
[artifact:pom]  Diagnosis:
[artifact:pom] 
[artifact:pom] Unable to initialize POM pom.xml.template: Cannot find parent: org.apache.lucene:lucene-contrib for project: org.apache.lucene:lucene-analyzers:jar:3.1-SNAPSHOT for project org.apache.lucene:lucene-analyzers:jar:3.1-SNAPSHOT
[artifact:pom] Unable to download the artifact from any repository
{code}


The contrib portion of the ant build is executed in a subant task which does not pick up the pom definitions for lucene-parent and lucene-contrib from the main build.xml, so the lucene-parent and lucene-controb poms must be loaded specifically as a part of the contrib build using the artifact:pom task.
"
0,EnwikiQueryMaker. 
0,"TCK: SetPropertyCalendarTest compares Calendar objects. SetPropertyCalendarTest# testNewCalendarPropertySession
SetPropertyCalendarTest# testModifyCalendarPropertySession
SetPropertyCalendarTest# testNewCalendarPropertyParent
SetPropertyCalendarTest# testModifyCalendarPropertyParent

Tests compare Calendar objects.  Calendar.equals(Object) is a stronger test than JSR-170 specifies for Value.equals(Object), leading to false failures.  For the purpose of these tests, even Value.equals(Object) is too strong an equality test, since some repositories may normalize date/time values across a save/read roundtrip (for example, converting ""Z"" to ""+00:00"", or adding/removing trailing zeros in fractional seconds).

Proposal: compare the getTimeInMillis() values.

--- SetPropertyCalendarTest.java        (revision 422074)
+++ SetPropertyCalendarTest.java        (working copy)
@@ -52,8 +52,8 @@
         testNode.setProperty(propertyName1, c1);
         superuser.save();
         assertEquals(""Setting property with Node.setProperty(String, Calendar) and Session.save() not working"",
-                c1,
-                testNode.getProperty(propertyName1).getDate());
+                c1.getTimeInMillis(),
+                testNode.getProperty(propertyName1).getDate().getTimeInMillis());
     }
  
     /**
@@ -66,8 +66,8 @@
         testNode.setProperty(propertyName1, c2);
         superuser.save();
         assertEquals(""Modifying property with Node.setProperty(String, Calendar) and Session.save() not working"",
-                c2,
-                testNode.getProperty(propertyName1).getDate());
+                c2.getTimeInMillis(),
+                testNode.getProperty(propertyName1).getDate().getTimeInMillis());
     }
  
     /**
@@ -78,8 +78,8 @@
         testNode.setProperty(propertyName1, c1);
         testRootNode.save();
         assertEquals(""Setting property with Node.setProperty(String, Calendar) and parentNode.save() not working"",
-                c1,
-                testNode.getProperty(propertyName1).getDate());
+                c1.getTimeInMillis(),
+                testNode.getProperty(propertyName1).getDate().getTimeInMillis());
     }
  
     /**
@@ -92,8 +92,8 @@
         testNode.setProperty(propertyName1, c2);
         testRootNode.save();
         assertEquals(""Modifying property with Node.setProperty(String, Calendar) and parentNode.save() not working"",
-                c2,
-                testNode.getProperty(propertyName1).getDate());
+                c2.getTimeInMillis(),
+                testNode.getProperty(propertyName1).getDate().getTimeInMillis());
     }
"
0,"Generify Security API. The current security api, namely the Authorizable and Group interface use non-generic collections/iterators.
suggest to change this."
0,"add icu-based tokenizer for unicode text segmentation. I pulled out the last part of LUCENE-1488, the tokenizer itself and cleaned it up some.

The idea is simple:
* First step is to divide text into writing system boundaries (scripts)
* You supply an ICUTokenizerConfig (or just use the default) which lets you tailor segmentation on a per-writing system basis.
* This tailoring can be any BreakIterator, so rule-based or dictionary-based or your own.

The default implementation (if you do not customize) is just to do UAX#29, but with tailorings for stuff with no clear word division:
* Thai (uses dictionary-based word breaking)
* Khmer, Myanmar, Lao (uses custom rules for syllabification)

Additionally as more of an example i have a tailoring for hebrew that treats the punctuation special. (People have asked before
for ways to make standardanalyzer treat dashes differently, etc)
"
0,"FieldComparator.TermOrdValComparator compares by value unnecessarily. Digging on LUCENE-2504, I noticed that TermOrdValComparator's compareBottom method falls back on compare-by-value when it needn't.

Specifically, if we know the current bottom ord ""matches"" the current segment, we can skip the value comparison when the ords are the same (ie, return 0) because the ords are exactly comparable.

This is hurting string sort perf especially for optimized indices (and also unoptimized indices), and especially for highly redundant (not many unique values) fields.  This affects all releases >= 2.9.x, but trunk is likely more severely affected since looking up a value is more costly."
0,Use only one scheduler for repository tasks. There are still a few Timer instances being used by Jackrabbit. It would be better if all tasks were scheduled by the central ScheduledExecutorService thread pool of the repository.
0,"WebDAV: pack AbstractWebdavServlet with the jackrabbit-webdav project. suggestion posted by alan cabrera on the dev list:

""Quite a handy servlet.  Too bad it's in jackrabbit-server.  Would this not be better placed in jackrabbit-webdav?  I'm writing my own server bits under WEBDAV and would prefer not to have JCR/Jackrabbit stuff.  I realize that this is a fussy preference.""

"
0,"Move common implementations of SPI interfaces to spi-commons module. Some of the spi modules use nearly duplicate code, which should be moved to the spi-commons module."
0,"Need setURI() methods in HttpMethod interface. I'd like to have the methods setURI( URI ) and setURI( String ) methods. Also a 
method like getRequestURI() because the uri I get now with the method getURI() 
changes if I execute a request which will be automatically forwarded.

The methods setURI can throw an exception if it has already been executed."
0,Make it possible to get multiple nodes in one call via davex. I'm working on this currently
0,"Omit default BatchReadConfig in Spi2davexRepositoryServiceFactory. i'd like to remove the default batchread configuration created in Spi2davexRepositoryServiceFactory (ll 79) and instead pass 
null if the service configuration doesn't define the batch-read-config.

for test execution e.g. the given default isn't really optimal as sessions only have a short life time and only read
a very limited amount of items (in general)... always reading with depth 4 doesn't add any benefit in this case.
running the level1 jcr tests in jcr2dav (that as far as i saw doesn't define an extra batchread-config took 1.5, 2.5 and 13 minutes
from null-config -> depth2 -> depth4.

if there is a strong reason for keeping that default in the factory we should at least change that for the tests.
michael, what do you think?"
0,"DatabaseJournal refactoring for subclassing capability. In the 1.3 upgrade to JackRabbit, the DatabasePersistenceManager class was refactored to allow easy subclassing.  On my project, the subclassing is required because the DBAs have a specific naming convention for database columns, and the default JackRabbit columns don't fit within the naming convention.

At this point, we're cutting over to a clustered setup in preparation for production.  In my design, I would like to use the database for journaling.  But once again, the DBAs will want to change the column names to their own naming convention.  The existing DatabaseJournal class is not set up for the same type of subclassing that the PersistenceManager (or even the FileSystem) hierarchies.  I'd like the DatabaseJournal class to be updated accordingly.

In specific, here are the changes I'm looking for:

* Extract protected instance variables for selectRevisionsStmtSql, updateGlobalStmtSql, selectGlobalStmtSql, and insertRevisionStmtSql.
* Extract method protected void buildSQLStatements() which sets up the above sqls, and allows subclasses to override.
* Update the existing prepareStatements method to use the above instance variables.
* Update the init method to call the buildSQLStatements method before the call to prepareStatements."
0,"HttpRoute.equals(Object o) is quite inefficient, as it does not take full advantage of shortcut logic. HttpRoute.equals(Object o) is quite inefficient, as it does not take full advantage of shortcut logic.

It should return as soon as the first  false is detected.

Patch to follow implements short-circuit checking."
0,"Kuromoji code donation - a new Japanese morphological analyzer. Atilika Inc. () would like to donate the Kuromoji Japanese morphological analyzer to the Apache Software Foundation in the hope that it will be useful to Lucene and Solr users in Japan and elsewhere.

The project was started in 2010 since we couldn't find any high-quality, actively maintained and easy-to-use Java-based Japanese morphological analyzers, and these become many of our design goals for Kuromoji.

Kuromoji also has a segmentation mode that is particularly useful for search, which we hope will interest Lucene and Solr users.  Compound-nouns, such as  (Kansai International Airport) and  (Nikkei Newspaper), are segmented as one token with most analyzers.  As a result, a search for  (airport) or  (newspaper) will not give you a for in these words.  Kuromoji can segment these words into    and   , which is generally what you would want for search and you'll get a hit.

We also wanted to make sure the technology has a license that makes it compatible with other Apache Software Foundation software to maximize its usefulness.  Kuromoji has an Apache License 2.0 and all code is currently owned by Atilika Inc.  The software has been developed by my good friend and ex-colleague Masaru Hasegawa and myself.

Kuromoji uses the so-called IPADIC for its dictionary/statistical model and its license terms are described in NOTICE.txt.

I'll upload code distributions and their corresponding hashes and I'd very much like to start the code grant process.  I'm also happy to provide patches to integrate Kuromoji into the codebase, if you prefer that.

Please advise on how you'd like me to proceed with this.  Thank you.
"
0,"Reduce memory usage of ParentAxisScorer. The ParentAxisScorer keeps a map of non-default scores while it calculates the parent matches of the context scorer. In most cases the scores are not equal to the default score, but still may be all the same.

The scorer should therefore use the first score value as the default instead of the currently used 1.0f."
0,make NamespaceContext#getPrefix(java.lang.String) iterative instead of recursive. Currently the method org.apache.jackrabbit.core.xml.NamespaceContext#getPrefix(java.lang.String) uses recursion. For very large XML files (50 MB Magnolia website exports) this causes a stack overflow. The method can easily be rewritten using iteration.
0,"Download: improve user experience. The download section at http://jackrabbit.apache.org/downloads.html
contains many files. The number of files should be reduced.

Some of them contain other files, for example the .rar files, and the .war files. 
The file jackrabbit-jca-1.4.rar  contains an old version of jackrabbit-core:
jackrabbit-core-1.4.jar - however the newest version of jackrabbit-core is 
jackrabbit-core-1.4.5.jar

This often leads to problems."
0,"Safe namespace registration. The namespace registration methods provided by the JCR NamespaceRegistry API are cumbersome to use and vulnerable to race conditions in the event of conflicting prefix mappings. This problem was discussed lately on the mailing list (see http://article.gmane.org/gmane.comp.apache.jackrabbit.devel/6805) and one symptom of the problem is the new code in NodeTypeManagerImpl (see JCR-349):

    //  Registers a namespace...
    try {
        nsReg.getPrefix(uri);
    } catch (NamespaceException e1) {
        String original = prefix;
        for (int i = 2; true; i++) {
            try {
                nsReg.registerNamespace(prefix, uri);
                return;
            } catch (NamespaceException e2) {
                prefix = original + i;
            }
        }
    }

We should add an internal convenience method like NamespaceRegistryImpl.safeRegisterNamespace(String prefixHint, String uri) that, instead of throwing an exception, would automatically generate and use a unique prefix based on the given hint when a prefix conflict is detected."
0,"Lucene Search not scalling. I've noticed that when doing thousands of searches in a single thread the average time is quite low i.e. a few milliseconds. When adding more concurrent searches doing exactly the same search the average time increases drastically. 
I've profiled the search classes and found that the whole of lucene blocks on 

org.apache.lucene.index.SegmentCoreReaders.getTermsReader
org.apache.lucene.util.VirtualMethod
  public synchronized int getImplementationDistance 
org.apache.lucene.util.AttributeSourcew.getAttributeInterfaces

These cause search times to increase from a few milliseconds to up to 2 seconds when doing 500 concurrent searches on the same in memory index. Note: That the index is not being updates at all, so not refresh methods are called at any stage.


Some questions:
  Why do we need synchronization here?
  There must be a non-lockable solution for these, they basically cause lucene to be ok for single thread applications but disastrous for any concurrent implementation.

I'll do some experiments by removing the synchronization from the methods of these classes."
0,"In J2SDK 1.5.0 (Tiger) enum is a keyword. Hi!

Tiger adds extensions to the Java Programming Language (JSR201). One is
""Enumerations"", which required to add the new keyword enum.

I just made a grep (grep -lrw) over some sources and found some Apache projects
using enum as a word.

To be compliant with the new specification, please check that enum is not used
as a variable, field or method name.

Regards,
Robert"
0,"WikipediaTokenizer. I have extended StandardTokenizer to recognize Wikipedia syntax and mark tokens with certain attributes.  It isn't necessarily complete, but it does a good enough job for it to be consumed and improved by others.

It sets the Token.type() value depending on the Wikipedia syntax (links, internal links, bold, italics, etc.) based on my pass at http://en.wikipedia.org/wiki/Wikipedia:Tutorial

I have only tested it with the benchmarking EnwikiDocMaker wikipedia stuff and it seems to do a decent job.

Caveats:  I am not sure how to best handle testing, since the content is licensed under GNU Free Doc License, I believe I can't copy and paste a whole document into the unit test.  I have hand coded one doc and have another one that just generally runs over the benchmark Wikipedia download.

One more question is where to put it.  It could go in analysis, but the tests at least will have a dependency on Benchmark.  I am thinking of adding a new contrib/wikipedia where this could grow to have other wikipedia things (perhaps we would move EnwikiDocMaker there????) and reverse the dependency on Benchmark.

I will post a patch over the next few days."
0,"org.apache.jackrabbit.server.io.IOUtil getTempFile misses dot in tmp suffix. At line 168:
File tmpFile = File.createTempFile(""__importcontext"", ""tmp"");
Suffix tmp has no use because the dot is missing.

Should be:
File tmpFile = File.createTempFile(""__importcontext"", "".tmp"");

"
0,"ResponseContentEncoding should also handle x-gzip, compress and x-compress. ResponseContentEncoding should also handle x-gzip, compress and x-compress encodings according to specs (http://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html, 3.5 Content Codings).

Also RequestAcceptEncoding should set Accept-Encoding to ""gzip,deflate,identity"". I am not sure about x-gzip, compress and x-compress here though.

Thanks"
0,"Add sort missing first/last ability to SortField and ValueComparator. When SortField and ValueComparator use EntryCreators (from LUCENE-2649) they use a special sort value when the field is missing.

This enables lucene to implement 'sort missing last' or 'sort missing first' for numeric values from the FieldCache.
"
0,"Make not yet final core/contrib TokenStream/Filter implementations final. Lucene's analysis package is designed in a way, that you can plug different *implementations* of analysis in chains of TokenStreams and TokenFilters. An analyzer is build of several TokenStreams/Filters that do the tokenization of text. If you want to modify the behaviour of tokenization, you implement a new subclass of TokenStream/-Filter/Tokenizer.

Most classes in the core are correctly implemented like that. They are itsself final or their implementation methods are final (CharTokenizer).

A lot of problems with backwards-compatibility of LUCENE-1693 are some classes in Lucene's core/contrib not yet final:
- KeywordTokenizer should be declared final or its implementation methods should be final
- StandardTokenizer should be declared final or its implementation methods should be final
- ISOLatin1Filter is deprecated, so it will be removed in 3.0, nothing to do.

CharTokenizer is the abstract base class of several other classes. The design is correct: Child classes cannot override the implementation, they can only change the behaviour of this final implementation.

Contrib should be checked, that all implementation classes are at least final or they are designed in the same way like CharTokenizer."
0,"Improve logging of Session.save() to trace back root cause of externally modified nodes. Currently it's very difficult to find the root cause of error like: javax.jcr.InvalidItemStateException: <UUID> has been modified externally.

To better trace back such issues, it would be nice to add DEBUG logging for the Session.save() call."
0,"include junit JAR in source dist. We recently added the junit JAR under ""lib"" so that we can checkout & run tests, but we fail to include it in the source dist."
0,Use IndexWriterConfig in benchmark. We should use IndexWriterConfig instead of deprecated methods in benchmark. 
0,EntryCollector may log warning for inexistent item. Currently the EntryCollector may log a warning when the node reported in the event does not exist. This may happen when the repository runs in a cluster and a node is created and immediately removed again. This issue is related to JCR-3014. The call to Session.nodeExists() should actually return false when the identifier path cannot be resolved. Currently it throws a RepositoryException.
0,"IndexFormatTooOld/NewExc should try to include fileName + directory when possible. (Spinoff from http://markmail.org/thread/t6s7nn3ve765nojc )

When we throw a too old/new exc we should try to include the full path to the offending file, if possible."
0,"Create Jcr-Client Module. task copied from JCR-1877:

i think it would be wise to create a new module that mainly consists of a RepositoryFactory and combines jackrabbit-jcr2spi with the known (and also any other) spi implementations."
0,"Deprecate IndexModifier. See discussion at http://www.gossamer-threads.com/lists/lucene/java-dev/52017?search_string=deprecating%20indexmodifier;#52017

This is to deprecate IndexModifier before 3.0 and remove it in 3.0.

This patch includes:
  1 IndexModifier and TestIndexModifier are deprecated.
  2 TestIndexWriterModify is added. It is similar to TestIndexModifer but uses IndexWriter and has a few other changes. The changes are because of the difference between IndexModifier and IndexWriter.
  3 TestIndexWriterLockRelease and TestStressIndexing are switched to use IndexWriter instead of IndexModifier."
0,DatabasePersistenceManager & DatabaseFileSystem: try to gracefully recover from connection loss. 
0,"Should SegmentTermPositionVector be public?. I'm wondering why SegmentTermPositionVector is public. It implements the public
interface TermPositionVector. Should we remove ""public""?"
0,Missing third party notices and license info. The components that bundle external libraries (jackrabbit-webapp and jackrabbit-jca) should come with appropriate copyright notices and license information.
0,"TCK: XPath order by test uses non-standard column specifier mechanism. org.apache.jackrabbit.test.api.QueryXPathOrderByTest generates a queries of the following form: /jcr:root/*[@proname]/@propname order by @propname

This syntax for column specifiers is not mandated by the JCR specification and will fail on any implementation that does not support it.

Instead the tests should use the following query: /jcr:root/*[@proname] order by @propname and then read the results using QueryResults.getNodes and not QueryResults.getRows."
0,Jcr2Spi: Unneeded call to getPropertyInfo upon creating a new NodeState. creating a new NodeState may result in additional (but unneeded) calls to getPropertyInfo if a jcr:uuid or jcr:mixinTypes property is present. This can be avoided since the corresponding property values are already present.
0,"Closing a session twice shouldn't write a warning in the log. When closing a session twice the following warning is written to the log file as of JCR-2741:

""This session has already been closed. See the chained exception for a trace of where the session was closed.""

I think the second ""close()"" should simply be ignored, without warning.
"
0,"No documentation on how to use CookieSpec. None of http://hc.apache.org/httpcomponents-client/httpclient/apidocs/org/apache/http/cookie/CookieSpec.html, http://hc.apache.org/httpcomponents-client/httpclient/apidocs/org/apache/http/cookie/CookieSpecFactory.html, http://hc.apache.org/httpcomponents-client/httpclient/apidocs/org/apache/http/cookie/CookieSpecRegistry.html, or http://hc.apache.org/httpcomponents-client/httpclient/apidocs/org/apache/http/impl/client/DefaultHttpClient.html explain how to set the CookieSpec that the HttpClient actually uses. It looks like CookieSpecRegistry might be it, but it doesn't document what the ""names"" mean, so I don't know what to pick to make a factory actually get used."
0,how-to deployment models. how-to with a detailed description of the steps required for each deployment model. 
0,"In AbstractImportXmlTest, a bug in getUnusedUri() causes URI length to grow too quickly, causing test to fail when using ORM-PM. Test fails when using ORM-PM because the URI exceeds the column size in the database.

Here is the current implementation:

    protected String getUnusedURI() throws RepositoryException {
        Set uris = new HashSet(Arrays.asList(nsp.getURIs()));
        String uri = TEST_URI;
        int i = 0;
        while (uris.contains(uri)) {
            uri += i++;
        }
        return uri;
    }

When running the test, the URI grows to become something like this:

When i=50,
""www.apache.org/jackrabbit/test/namespaceImportTest01234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950""

Here is the proposed fix:

    protected String getUnusedURI() throws RepositoryException {
        Set uris = new HashSet(Arrays.asList(nsp.getURIs()));
        String uri = TEST_URI;
        int i = 0;
        while (uris.contains(uri)) {
            uri = TEST_URI + i++;
        }
        return uri;
    }

When i=50,
""www.apache.org/jackrabbit/test/namespaceImportTest50"""
0,"contrib intelligent Analyzer for Chinese. I wrote a Analyzer for apache lucene for analyzing sentences in Chinese language. it's called ""imdict-chinese-analyzer"", the project on google code is here: http://code.google.com/p/imdict-chinese-analyzer/

In Chinese, """"(I am Chinese), should be tokenized as """"(I)   """"(am)   """"(Chinese), not """" """" """". So the analyzer must handle each sentence properly, or there will be mis-understandings everywhere in the index constructed by Lucene, and the accuracy of the search engine will be affected seriously!

Although there are two analyzer packages in apache repository which can handle Chinese: ChineseAnalyzer and CJKAnalyzer, they take each character or every two adjoining characters as a single word, this is obviously not true in reality, also this strategy will increase the index size and hurt the performance baddly.

The algorithm of imdict-chinese-analyzer is based on Hidden Markov Model (HMM), so it can tokenize chinese sentence in a really intelligent way. Tokenizaion accuracy of this model is above 90% according to the paper ""HHMM-based Chinese Lexical analyzer ICTCLAL"" while other analyzer's is about 60%.

As imdict-chinese-analyzer is a really fast and intelligent. I want to contribute it to the apache lucene repository."
0,"Highlighter wraps caching token filters that are not CachingTokenFilter in CachingTokenFilter. I figured this was fine and a rare case that you would have another caching tokenstream to feed the highlighter with - but I guess if its happening to you, especially depending on what you are doing - its not an ideal situation."
0,"FileDataStore should check for lastModified error result. According to javadoc for File.lastModified(), the return value may indicate error: ""...or 0L if the file does not exist or if an I/O error occurs"".

Accordingly, FileDataStore should be checking for this return value, rather than treating it as an actual modification time of ""0"".

Patch to follow.
"
0,"Make license checking/maintenance easier/automated. Instead of waiting until release to check licenses are valid, we should make it a part of our build process to ensure that all dependencies have proper licenses, etc."
0,"String.intern() faster alternative. By using our own interned string pool on top of default, String.intern() can be greatly optimized.

On my setup (java 6) this alternative runs ~15.8x faster for already interned strings, and ~2.2x faster for 'new String(interned)'
For java 5 and 4 speedup is lower, but still considerable."
0,"Allow QP subclasses to support Wildcard Queries with leading ""*"". It would be usefull for some users if the logic that prevents QueryParser from creating WIldcardQueries with leading wildcard characters (""?"" or ""*"") be moved from the grammer into the base implimentation of getWildcardQuery so that it may be overridden in subclasses without needing to modifiy the grammer directly.
"
0,Deprecated API called in o.a.l.store Directories. just ran into NIOFSDirectory and others still call getFile instead of getDirectory
0,"Stats for Queries. Re-enable the stats for queries, as they were disabled during the refactoring phase."
0,"increase maxmemory for unit tests. We have some unit tests that require a fair amount of RAM.  But, sometimes the JRE does not give itself a very large max heap size, by default.  EG on a Mac Pro with 6 GB physical RAM, I see JRE 1.6.0 defaulting to max 80 GB and it always then hits this exception during testing:

    [junit] Testcase: testHugeFile(org.apache.lucene.store.TestHugeRamFile):	Caused an ERROR
    [junit] Java heap space
    [junit] java.lang.OutOfMemoryError: Java heap space
    [junit] 	at java.util.Arrays.copyOf(Arrays.java:2760)
    [junit] 	at java.util.Arrays.copyOf(Arrays.java:2734)
    [junit] 	at java.util.ArrayList.ensureCapacity(ArrayList.java:167)
    [junit] 	at java.util.ArrayList.add(ArrayList.java:351)
    [junit] 	at org.apache.lucene.store.RAMFile.addBuffer(RAMFile.java:69)
    [junit] 	at org.apache.lucene.store.RAMOutputStream.switchCurrentBuffer(RAMOutputStream.java:129)
    [junit] 	at org.apache.lucene.store.RAMOutputStream.writeBytes(RAMOutputStream.java:115)
    [junit] 	at org.apache.lucene.store.TestHugeRamFile.testHugeFile(TestHugeRamFile.java:68)

The fix is simple: add maxmemory=512M into common-build.xml.  I'll commit shortly."
0,"Make ReqExclScorer package private, and use DocIdSetIterator for excluded part.. "
0,"In IndexSearcher class, make subReader and docCount arrays protected so sub classes can access them. Please make these two member variables protected so subclasses can access them, e.g.:

  protected IndexReader[] subReaders;
  protected int[] docStarts;

Thanks"
0,Add cause to ItemStateException in BundleDbPersistenceManager.store(). Currently only the message of the causing exception is given to the ItemStateException. 
0,"Improve random testing. We have quite a few random tests, but there's no way to ""crank"" them.

The idea here is to add a multiplier which can be increased by a sysprop. For example, we could set this to something higher than 1 for hudson."
0,"Fix 2.9 contrib builds to succeed when JDK 1.4 is used (leaving out contribs that require 1.5). When you build and test Lucene 2.9 with Java 1.4, building and testing of contrib fails. This patch fixes this to repect the current compiler version and disables all contribs that need Java 1.5 by checking their javac.source property.

This patch can be ported to 3.x or trunk, when 1.6 contribs will appear."
0,"Regex support and beyond in JavaCC QueryParser. Since the early days the standard query parser was limited to the queries living in core, adding other queries or extending the parser in any way always forced people to change the grammar file and regenerate. Even if you change the grammar you have to be extremely careful how you modify the parser so that other parts of the standard parser are affected by customisation changes. Eventually you had to live with all the limitation the current parser has like tokenizing on whitespaces before a tokenizer / analyzer has the chance to look at the tokens. 
I was thinking about how to overcome the limitation and add regex support to the query parser without introducing any dependency to core. I added a new special character that basically prevents the parser from interpreting any of the characters enclosed in the new special characters. I choose the forward slash  '/' as the delimiter so that everything in between two forward slashes is basically escaped and ignored by the parser. All chars embedded within forward slashes are treated as one token even if it contains other special chars like * []?{} or whitespaces. This token is subsequently passed to a pluggable ""parser extension"" with builds a query from the embedded string. I do not interpret the embedded string in any way but leave all the subsequent work to the parser extension. Such an extension could be another full featured query parser itself or simply a ctor call for regex query. The interface remains quiet simple but makes the parser extendible in an easy way compared to modifying the javaCC sources.

The downsides of this patch is clearly that I introduce a new special char into the syntax but I guess that would not be that much of a deal as it is reflected in the escape method though. It would truly be nice to have more than once extension an have this even more flexible so treat this patch as a kickoff though.

Another way of solving the problem with RegexQuery would be to move the JDK version of regex into the core and simply have another method like:
{code}
protected Query newRegexQuery(Term t) {
  ... 
}
{code}

which I would like better as it would be more consistent with the idea of the query parser to be a very strict and defined parser.

I will upload a patch in a second which implements the extension based approach I guess I will add a second patch with regex in core soon too.

"
0,"javadoc writing and generation with mvn. ""mvn -source 1.5 javadoc:javadoc"" does not work because following lines must be added to pom.xml
<plugin>
      <artifactId>maven-javadoc-plugin</artifactId>
      <configuration>
        <source>1.5</source>
      </configuration>
    </plugin> 
Please also write more comprehensive javadocs to allow better source understanding and developer co-operation"
0,"Add unsigned packed int impls in oal.util. There are various places in Lucene that could take advantage of an
efficient packed unsigned int/long impl.  EG the terms dict index in
the standard codec in LUCENE-1458 could subsantially reduce it's RAM
usage.  FieldCache.StringIndex could as well.  And I think ""load into
RAM"" codecs like the one in TestExternalCodecs could use this too.

I'm picturing something very basic like:
{code}
interface PackedUnsignedLongs  {
  long get(long index);
  void set(long index, long value);
}
{code}

Plus maybe an iterator for getting and maybe also for setting.  If it
helps, most of the usages of this inside Lucene will be ""write once""
so eg the set could make that an assumption/requirement.

And a factory somewhere:

{code}
  PackedUnsignedLongs create(int count, long maxValue);
{code}

I think we should simply autogen the code (we can start from the
autogen code in LUCENE-1410), or, if there is an good existing impl
that has a compatible license that'd be great.

I don't have time near-term to do this... so if anyone has the itch,
please jump!
"
0,"nightly build/javadocs for sandbox. This isn't something i think is crucial, but since i've been on the lucene-users
mailing list (less then 2 months) I've seen several people post questions asking
where they can find documentation on some module available in the sandbox.

the answer of course is usually that they should download the source and build
the javadocs themselves, but since it keeps coming up, I figured i'd suggest
setting up a nightly ""build"" of the whole sandbox, including the javadocs.

if nothing else, it will cut down on the number of questions -- but i think it
may also have an added benefit to the size of the user base.  In my experience,
people tend to be more willing to download/install something and try it out
after they've read the docs online."
0,"spi2davex: use srcWorkspaceName to build srcPath for clone and copy. spi2davex provides as simple workaround for the missing clone and cross-workspace-copy in spi2dav.
however, the src workspace name isn't used to build the src path."
0,"use VersionInfo of core. With core alpha5, a version detection scheme was introduced.
Replace the preliminary version detection of client alpha1 with that in core.
That means new version.properties files, at least one per JAR, maybe one per potential JAR.
"
0,"Improve speed of ThaiWordFilter by CharacterIterator, factor out LowerCasing and also fix some bugs (empty tokens stop iteration). The ThaiWordFilter creates new Strings out of term buffer before passing to The BreakIterator., But BreakIterator can take a CharacterIterator and directly process on it without buffer copying.
As Java itsself does not provide a CharacterIterator implementation in java.text, we can use the javax.swing.text.Segment class, that operates on a char[] and is even reuseable! This class is very strange but it works and is in JDK 1.4+ and not deprecated.

The filter also had a bug: It stopped iterating tokens when an empty token occurred. Also the lowercasing for non-thai words was removed and put into the Analyzer by adding LowerCaseFilter."
0,"Javadoc: does not mention Expires; clarify validate. The Expires attribute processing is not mentioned in any of the Javadoc, as far
as I can tell. 

Also, the public method parseAttribute() actually handles the attributes, but
does not contain the details. It would be useful if there was at least a
backlink to the parse() documentation.

It's not clear from the Javadoc whether parse() automatically calls validate()
or not. It doesn't."
0,"analysis consumers should use reusable tokenstreams. Some analysis consumers (highlighter, more like this, memory index, contrib queryparser, ...) are using Analyzer.tokenStream but should be using Analyzer.reusableTokenStream instead for better performance."
0,"BoostingTermQuery's BoostingSpanScorer class should be protected instead of package access. Currently, BoostingTermScorer, an inner class of BoostingTermQuery is not accessible from outside the search.payloads
making it difficult to write an extension of BoostingTermQuery. The other inner classes are protected already, as they should be."
0,Encapsulate RepositoryHelper field. This is a first step towards a test suite that will run tests with multiple threads concurrently.
0,"OracleDatabaseJournal should assume Oracle defaults. OracleFileSystem and OraclePersistenceManager assumes defaults for certain properties, such as driver and databaseType.  These defaults should be extended to OracleDatabaseJournal for consistency and ease of use."
0,"BalancedSegmentMergePolicy, contributed from the Zoie project for realtime indexing. Written by Yasuhiro Matsuda for Zoie realtime indexing system used to handle high update rates to avoid large segment merges.
Detailed write-up is at:

http://code.google.com/p/zoie/wiki/ZoieMergePolicy
"
0,"Add support for query result highlighting. Highlighting matches in a query result list is regularly needed for an application. The query languages should support a pseudo property or function that allows one to retrieve text fragments with highlighted matches from the content of the matching node.

To support this feature the following enhancements are required:
- define a pseudo property or function that returns the text excerpt and can be used in the select clause
- the index needs to *store* the original text it used when the node was indexed. this also includes extracted text from binary properties.
- text fragments must be created based on the original text, the query and index information"
0,"Node.getReferences() does not properly reflect saved but not yet committed changes. Node.getReferences() currently only returns committed references. The specification however says:

<quote>
Some level 2 implementations may only return properties that have been
saved (in a transactional setting this includes both those properties
that have been saved but not yet committed, as well as properties that
have been committed). Other level 2 implementations may additionally
return properties that have been added within the current Session but are
not yet saved.
</quote>

Jackrabbit does not support the latter, but at least has to support the first."
0,"small speedups to bulk merging. The bulk merging code, for stored fields & term vectors, was calling isDeleted twice for each deleted doc.

Patch also changes DocumentsWriter to use IndexWriter.message for its infoStream messages."
0,"MockCharFilter offset correction is wrong. This is a fake charfilter used in basetokenstreamtestcase.

it occasionally doubles some characters, and corrects offsets.

its used to find bugs where analysis components would fail otherwise with charfilters,
but its correctOffset is actually wrong (harmless to any tests today, but still wrong).
"
0,"left nav of docs/index.html in dist artifacts links to hudson for javadocs. When building the zip or tgz release artifacts, the docs/index.html file contained in that release (the starter point for people to read documentation) links ""API Docs"" to 
http://lucene.zones.apache.org:8080/hudson/job/Lucene-Nightly/javadoc/ instead of to ./api/index.html (the local copy of the javadocs)

this relates to the initial migration to hudson for the nightly builds and a plan to copy the javadocs back to lucene.apache.org that wasn't considered urgent since it was just for transient nightly docs, but a side affect is that the release documentation also links to hudson.

even if we don't modify the nightly build process before the 2.2 release, we should update the link in the left nav in the 2.2 release branch before building the final release."
0,"Rename contrib/queryparser project to queryparser-contrib. Much like with contrib/queries, we should differentiate the contrib/queryparser from the queryparser module.  No directory structure changes will be made, just ant and maven."
0,"remove old static main methods in core. We have a few random static main methods that I think are very rarely used... we should remove them (IndexReader, UTF32ToUTF8, English).

The IndexReader main lets you list / extract the sub-files from a CFS... I think we should move this to a new tool in contrib/misc."
0,Improve Javadoc. 
0,"expose shutdown method in o.a.j.jndi.BindableRepository. see http://thread.gmane.org/gmane.comp.apache.jackrabbit.devel/3680
"
0,"Log at debug level rather that info in CacheManager. Please change org.apache.jackrabbit.core.state.CacheManager#resizeAll to log at DEBUG level rather that INFO.
15:50:01,058 INFO  [CacheManager] resizeAll size=8

--- jackrabbit-core/src/main/java/org/apache/jackrabbit/core/state/CacheManager.java    (revision 565102)
+++ jackrabbit-core/src/main/java/org/apache/jackrabbit/core/state/CacheManager.java    (working copy)
@@ -122,7 +122,7 @@
      * Re-calcualte the maximum memory for each cache, and set the new limits.
      */
     private void resizeAll() {
-        log.info(""resizeAll size="" + caches.size());
+        log.debug(""resizeAll size="" + caches.size());
         // get strong references
         // entries in a weak hash map may disappear any time
         // so can't use size() / keySet() directly

"
0,"Session leak in API test cases. In setUp(). AbstractPropertyTest checks whether it can execute the test and potentially throws an NotExecutableException. In this case, the helper Session is lost (without logging out), and the parent's tearDown() code isn't executed. 

The same problem may be present in more test classes (will check).
"
0,"Cookie Documentation clarifications. The JavaDoc for CookieSpec mentions that the default policy is RFC2109 - it
would be nice if this was mentioned on
http://jakarta.apache.org/commons/httpclient/cookies.html as well
(likewise for 2.0)

The Javadoc for getDefaultPolicy() says to use getCookieSpec(String); it would
be better to say to use getCookieSpec(DEFAULT), and it would be helpful to
mention getDefaultSpec().

CookiePolicy Javadoc does not mention the IGNORE_COOKIES policy in the header
documentation.

The cookies.html page mentions automatic and manual handling of cookies, but
does not provide any links as to how to control these. For example, how does one
turn off automatic cookie handling?"
0,"Revise Weight#scorer & Filter#getDocIdSet API to pass Readers context. Spinoff from LUCENE-2694 - instead of passing a reader into Weight#scorer(IR, boolean, boolean) we should / could revise the API and pass in a struct that has parent reader, sub reader, ord of that sub. The ord mapping plus the context with its parent would make several issues way easier. See LUCENE-2694, LUCENE-2348 and LUCENE-2829 to name some.

"
0,"simple improvements to tests. Simon had requested some docs on what all our test options do, so lets clean it up and doc it.

i propose:
# change all vars to be tests.xxx (e.g. tests.threadspercpu, tests.multiplier, ...)
# ensure all 6 build systems (lucene, solr, each solr contrib) respect these.
# add a simple wiki page listing what these do."
0,"Update StandardTokenizer and UAX29Tokenizer to Unicode 6.0.0. Newly released Unicode 6.0.0 contains some character property changes from the previous release (5.2.0) that affect word segmentation (UAX#29), and JFlex 1.5.0-SNAPSHOT now supports Unicode 6.0.0, so Lucene's UAX#29-based tokenizers should be updated accordingly.

Note that the UAX#29 word break rules themselves did not change between Unicode versions 5.2.0 and 6.0.0."
0,"Upgrade JUnit to 4.10, refactor state-machine of detecting setUp/tearDown call chaining.. Both Lucene and Solr use JUnit 4.7. I suggest we move forward and upgrade to JUnit 4.10 which provides several infrastructural changes (serializable Description objects, class-level rules, various tweaks). JUnit 4.10 also changes (or fixes, depends how you look at it) the order in which @Before/@After hooks and @Rules are applied. This makes the old state-machine in LuceneTestCase fail (because the order is changed).

I rewrote the state machine and used a different, I think simpler, although Uwe may disagree :), mechanism in which the hook methods setUp/ tearDown are still there, but they are empty at the top level and serve only to detect whether subclasses chain super.setUp/tearDown properly (if they override anything).

In the long term, I would love to just get rid of public setup/teardown methods and make them private (so that they cannot be overriden or even seen by subclasses) but this will require changes to the runner itself."
0,"Provide access to port of Host header. We use a load balancer that connects to the HTTP server and the HTTP server
connects to the application server. We use port translation in our load
balancer. So when e.g. a client connects to 90 of the load balancer, the load
balancer connects to port 100 of the HTTP server. The load balancer doesn't
change the Host request header, so in the host request header is still the
original virtual host name and port, in this case port 90. For this reason, the
virtual hosts of the HTTP server and application server are configured based on
the external port numbers, so in this case port 90.
 
For test purposes, we sometimes want to connect directly to the HTTP server or
the application server, bypassing the load balancer. To do this, we need to
connect to the same port as the load balancer would, in this example port 100,
but the host header of this request should be the same as if the request would
go through the load balancer, so in this example port 90, because the HTTP
server and application server's virtual hosts are configured for this port.

The attached patch adds the possibility to specify the port number for virtual
hosts.

Here's a code snippet that uses the patched code:

HttpClient httpClient = new HttpClient();
HttpMethod method = new GetMethod();
HostConfiguration hostConfiguration = new HostConfiguration();
hostConfiguration.setHost(""localhost"", 80, ""http"");
HostParams params = new HostParams();
params.setVirtualHost(""localhost"");
params.setVirtualHostPort(100);
hostConfiguration.setParams(params);
httpClient.executeMethod(hostConfiguration, method);
System.out.println(method.getResponseBodyAsString());
method.releaseConnection();"
0,"Further improvements to contrib/benchmark for testing NRT. Some small changes:

  * Allow specifying a priority for BG threads, after the ""&""
    character; priority increment is + or - int that's added to main
    thread's priority to set child thread's.  For my NRT tests I make
    the reopen thread +2, the indexing threads +1, and leave searching
    threads at their default.

  * Added test case

  * NearRealTimeReopenTask now reports @ the end the full array of
    msec of each reopen latency

  * Added optional breakout of counts by time steps.  If you set
    log.time.step.msec to eg 1000 then reported counts for serial task
    sequence is broken out by 1 second windows.  EG you can use this
    to measure slowdown over time.
"
0,Fix javadocs after deprecation removal. There are a lot of @links in Javadocs to methods/classes that no longer exist. javadoc target prints tons of warnings. We should fix that.
0,"Maintain the cluster revision table. The revision table in which cluster nodes write their changes can potentially become very large. If all cluster nodes are up to date to a certain revision number, then it seems unnecessary to keep the revisions with a lower number."
0,Remove calls to System.out in tests. Two tests write to System.out: QueryTest and HierarchyNodeTest
0,FieldValueFitler should expose the field it uses. FieldValueFitler should expose the field it uses. It currently hides this entirely.
0,"DocValues.type() -> DocValues.getType(). This makes the method easier to find and more clear that it has no side effects... on a
few occasions I've looked for this getter and missed it because of the name.

"
0,"Port to Generics - test cases in contrib . LUCENE-1257 in Lucene 3.0 addressed porting to generics across public api-s . LUCENE-2065 addressed across src/test . 

This would be a placeholder JIRA for any remaining pending generic conversions across the code base. 

Please keep it open after commiting and we can close it when we are near a 3.1 release , so that this could be a placeholder ticket. 

"
0,lucene jars should include LiCENSE and NOTICE. The Lucene jars created by the build should include the LICENSE and NOTICE files in META-INF.
0,"Make behaviour configurable when re-indexing detects workspace inconsistency. Currently the query handler throws an exception and the repository will not start.

There are multiple options how to deal with this situation:

1) fail and repository startup will fail
2) ignore and proceed
3) fix the inconsistency and proceed

1) and 2) can be implemented quite easily. with the recent enhancement JCR-1428, 3) can also be implemented with reasonable effort.

In any case an error message should be written to the log."
0,"UAX29URLEmailTokenizer fails to recognize emails as such when the mailto: scheme is prepended. As [reported by Kai Glzau on solr-user|http://markmail.org/message/n32kji3okqm2c5qn]:

UAX29URLEmailTokenizer seems to split at the wrong place:

{noformat}mailto:test@example.org{noformat} ->
{noformat}mailto:test{noformat}
{noformat}example.org{noformat}

As a workaround I use

{code:xml}
<charFilter class=""solr.PatternReplaceCharFilterFactory"" pattern=""mailto:"" replacement=""mailto: ""/>
{code}"
0,"Log creation impairs performance. Running JProfiler on a program that uses HttpClient with a ThreadSafeClientConnManager, revealed that 5% of the time was spent constructing Log instances in class ClientParamsStack.

Oleg did some further investigation and found that DefaultRequestDirector also has the same problem.

A simple solution would be to make the Log a static member variable, and do this on all classes for consistency.  However this might not be the best solution for interoperating with some frameworks (see http://wiki.apache.org/jakarta-commons/Logging/StaticLog)

Another solution would be to simply remove the Log from the affected classes, although they are presumably there for a reason...
"
0,"SQL2 queries are not logged. SQL2 queries are constructed via QueryObjectModel, and ran via QueryObjectModelImpl which does not log the run time.
I'll attach a run time log similar to the old one."
0,Remove unnecessary array wrapping when calling varargs methods. varargs method callers don't have to wrap args in arrays
0,"fileformats.xml doesn't document compound file streams. Current versions of Lucene generate segments in compound file stream format
files, but the fileformats documentation does not have any description of the
format for those files."
0,"Query Syntax page does not make it clear that wildcard searches are not allowed in Phrase Queries. The queryparsersyntax page which is where I expect most novices (such as myself) start with lucene seems to indicate that wildcards can be used in phrase terms

Quoting:
'Terms: A query is broken up into terms and operators. There are two types of terms: Single Terms and Phrases.
A Single Term is a single word such as ""test"" or ""hello"".
A Phrase is a group of words surrounded by double quotes such as ""hello dolly"".

....

Wildcard Searches
Lucene supports single and multiple character wildcard searches.
To perform a multiple character wildcard search use the ""*"" symbol.
Multiple character wildcard searches looks for 0 or more characters. For example, to search for test, tests or tester, you can use the search:

test*
You can also use the wildcard searches in the middle of a term.

'
there is nothing to indicate in the section on Wildcard Searches that it can be performed only on Single word terms not Phrase terms.

Chris  argues 'that there is nothing in the description of a Phrase to indicate that it can be anything other then what it says ""a group of words surrounded by double quotes"" .. at no point does it
suggest that other types of queries or syntax can be used inside the quotes.  likewise the discussion of Wildcards makes no mention of phrases to suggest that wildcard characters can be used in a phrase.'
but I don't accept this because there is nothing in the description of a Single Term either to indicate it can use wildcards either. Wildcards are only mentioned in the Wildcard section and there it says thay can be used in a term, it does not restrict the type of term


I Propose a simple solution modify:

Lucene supports single and multiple character wildcard searches.

to 

Lucene supports single and multiple character wildcard searches within single terms.

(Chris asked for a patch, but Im not sure how to do this, but the change is simple enough)



"
0,"IndexWriter should detect when it's used after being closed. Spinoff from this thread on java-user:

    http://www.gossamer-threads.com/lists/lucene/java-user/45986

If you call addDocument on IndexWriter after it's closed you'll hit a
hard-to-explain NullPointerException (because the RAMDirectory was
closed).  Before 2.1, apparently you won't hit any exception and the
IndexWrite will keep running but will have released it's write lock (I
think).

I plan to fix IndexWriter methods to throw an IllegalStateException if
it has been closed.
"
0,JSR 283: Configurations and Baselines. 
0,Improve javadoc of User#getCredentials. User#getCredentials is not meant to return the credentials used for a repository login but the javadoc in the interface doesn't make this clear.
0,"Add deleteAllDocuments() method to IndexWriter. Ideally, there would be a deleteAllDocuments() or clear() method on the IndexWriter

This method should have the same performance and characteristics as:
* currentWriter.close()
* currentWriter = new IndexWriter(..., create=true,...)

This would greatly optimize a delete all documents case. Using deleteDocuments(new MatchAllDocsQuery()) could be expensive given a large existing index.

IndexWriter.deleteAllDocuments() should have the same semantics as a commit(), as far as index visibility goes (new IndexReader opening would get the empty index)

I see this was previously asked for in LUCENE-932, however it would be nice to finally see this added such that the IndexWriter would not need to be closed to perform the ""clear"" as this seems to be the general recommendation for working with an IndexWriter now

deleteAllDocuments() method should:
* abort any background merges (they are pointless once a deleteAll has been received)
* write new segments file referencing no segments

This method would remove one of the final reasons i would ever need to close an IndexWriter and reopen a new one 
"
0,"please have spi2dav create a test-jar. for my davex-on-sling integration tests, I need the test classes from spi2dav."
0,"bad test assumptions in org.apache.jackrabbit.test.api.lock. These tests make a lot of assumptions that may not be true for a compliant repository, such as:

- ability to add nodes without specifiying the node type
- assumption that ordering and same name siblings are supported
- assumption that addMixin(lockable) is required on newly added nodes

Furthermore, some repositories may not support shallow locks on leaf nodes. That's not compliant, but failure to do so should not abort a test that tests something else.
"
0,"Remove SVN.exe and revision numbers from build.xml by svn-copy the backwards branch and linking snowball tests by svn:externals. As we often need to update backwards tests together with trunk and always have to update the branch first, record rev no, and update build xml, I would simply like to do a svn copy/move of the backwards branch.

After a release, this is simply also done:
{code}
svn rm backwards
svn cp releasebranch backwards
{code}

By this we can simply commit in one pass, create patches in one pass.

The snowball tests are currently downloaded by svn.exe, too. These need a fixed version for checkout. I would like to change this to use svn:externals. Will provide patch, soon."
0,"XMLPersistenceManager fails after creating too many directories on linux. When using the  XMLPersistenceManager it creates a bunch of directories in jackrabbit/home/version/data. Eventually I reach 32000 directories in the data directory and subsequent writes fail.

I believe this is caused by XMLPersistenceManager.buildNodeFolderPath() method where it does 
   if (cnt == 4 || cnt == 8) {
      sb.append('/');
   }

This causes the subdirectories to be 4 characters, 0-f i.e. 16^4 which is 65536, if what I'm seeing is correct, on linux ext3, it's limited to 32000 entries. If the XMLPersistence manager used 2 or 3 characters this should fix the problem, or if it were configurable it would also solve this (I think).

an 
   ls jackrabbit/home/version/data | wc -l
returns 
   32001

A stack trace for when this happens is as follows :
Caused by: javax.jcr.RepositoryException: failed to write node state: da2cd5d1-9776-4dbe-a42b-842b0134dbfb: failed to write node state: da2cd5d1-9776-4dbe-a42b-842b0134dbfb
        at org.apache.jackrabbit.core.version.AbstractVersionManager.createVersionHistory(AbstractVersionManager.java:181)
        at org.apache.jackrabbit.core.version.VersionManagerImpl$1.run(VersionManagerImpl.java:194)
        at org.apache.jackrabbit.core.version.VersionManagerImpl$DynamicESCFactory.doSourced(VersionManagerImpl.java:526)
        at org.apache.jackrabbit.core.version.VersionManagerImpl.createVersionHistory(VersionManagerImpl.java:191)
        at org.apache.jackrabbit.core.version.XAVersionManager.createVersionHistory(XAVersionManager.java:140)
        at org.apache.jackrabbit.core.ItemImpl.initVersionHistories(ItemImpl.java:754)
        at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1166)
        at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:805)
        ... 166 more
Caused by: org.apache.jackrabbit.core.state.ItemStateException: failed to write node state: da2cd5d1-9776-4dbe-a42b-842b0134dbfb
        at org.apache.jackrabbit.core.state.xml.XMLPersistenceManager.store(XMLPersistenceManager.java:579)
        at org.apache.jackrabbit.core.state.AbstractPersistenceManager.store(AbstractPersistenceManager.java:66)
        at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:574)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:697)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:315)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:291)
        at org.apache.jackrabbit.core.version.AbstractVersionManager.createVersionHistory(AbstractVersionManager.java:174)
        ... 173 more
Caused by: org.apache.jackrabbit.core.fs.FileSystemException: failed to create folder /home/cms/pepsiaccess/jackrabbit/home/version/data/da2c/d5d1/97764dbea42b842b0134dbfb
        at org.apache.jackrabbit.core.fs.local.LocalFileSystem.createFolder(LocalFileSystem.java:208)
        at org.apache.jackrabbit.core.fs.BasedFileSystem.createFolder(BasedFileSystem.java:99)
        at org.apache.jackrabbit.core.fs.FileSystemResource.makeParentDirs(FileSystemResource.java:100)
        at org.apache.jackrabbit.core.state.xml.XMLPersistenceManager.store(XMLPersistenceManager.java:517)
        ... 179 more"
0,"Add an option so skip the ""checkSchema"" methods. Sometimes the ""checkSchema"" methods in the various components (DB filesystem, DB persistence manager, DB journal) fail to detect that the required tables already exist with as result that the startup  fails. (See the mail thread on the dev list: http://jackrabbit.markmail.org/message/jtq2sqis2aceh7ro).
An option to just skip the checkSchema methods on startup would solve this issue."
0,"the jcr:frozenUuid property is of type REFERENCE instead of STRING. The spec says that jcr:frozenUuid is a STRING but jackrabbit 1.0.1 uses a REFERENCE for it.
"
0,"Maintan a stable ordering of properties in xml export. When exporting to xml (system view, not tested with document view) the order of properties is not consistent.
This is not an issue with the jcr specification, since the order of properties is undefined, but keeping the same (whatever) order in xml export could be useful.

At this moment if you try running a few import->export->import->export roundtrips you will notice that the exported xml often changes. This is an example of the differences you can see:

  <sv:property sv:name=""jcr:uuid"" sv:type=""String"">
    <sv:value>59357999-b4fb-45cd-8111-59277caf14b7</sv:value>
  </sv:property>
+  <sv:property sv:name=""title"" sv:type=""String"">
+    <sv:value>test</sv:value>
+  </sv:property>
  <sv:property sv:name=""visible"" sv:type=""String"">
    <sv:value>true</sv:value>
  </sv:property>
-  <sv:property sv:name=""title"" sv:type=""String"">
-    <sv:value>test</sv:value>
-  </sv:property>

If you may need to diff between two exported files that could be pretty annoying, you have no clear way to understand if something has really changed or not.
I would propose to keep ordering consistent between export: an easy way could be sorting properties alphabetically during export.

This behavior has been tested on a recent jackrabbit build from trunk (1.4-SNAPSHOT)



"
0,(Char)TermAttribute cloning memory consumption. The memory consumption problem with cloning a (Char)TermAttributeImpl object was raised on thread http://markmail.org/thread/bybuerugbk5w2u6z
0,"Provide means to display the effective policies for a given set of principals. JSR 283 currently defines AccessControlManager#getEffectivePolicies(String nodePath) that would allow any Permission related UI to
display what policies contribute to a particular set of privileges. In addition the API defines AccessControlManager#getPrivileges(String nodePath) which
returns the privileges the editing session has at the specified path.

In order to have additional flexibility we started to add custom extensions (-> JackrabbitAccessControlManager) that allows e.g. to retrieve the privileges any set
of principals has hat a specified path. I would like to extend this and in addition provide a method that allows to retrieve the effective policies for a set of principals.
Currently this can only be achieved by relying on a specific access control model and making assumptions about it's implementation, which obviously isn't
the desired effect.
I"
0,"Minimize calls to PersistenceManager. In some situations the PersistenceManager is called even though it is not necessary.

E.g. when new items are created the method NodeImpl.getOrCreateProperty() will always check if there is an already existing property state. If the node is new the call will always go down the full item state stack and ask the PersistenceManager if it knows the property id. This is unnessessary because there will never exist properties in the persistence manager for a new node that has not been saved yet.

I propose to add a check to the method to see if  the node is new and does not yet have a property with the given name. In that case the property can be created without further checks.

With the patch applied the time to transiently create 1000 nodes with 4 properties each drops from 1485 ms to 422 ms."
0,"Improve FilteredQuery to shortcut on wrapped MatchAllDocsQuery. Since the rewrite of Lucene trunk to delegate all Filter logic to FilteredQuery, by simply wrapping in IndexSearcher.wrapFilter(), we can do more short circuits and improve query execution. A common use case it to pass MatchAllDocsQuery as query to IndexSearcher and a filter. For the underlying hit collection this is stupid and slow, as MatchAllDocsQuery simply increments the docID and checks acceptDocs. If the filter is sparse, this is a big waste. This patch changes FilteredQuery.rewrite() to short circuit and return ConstantScoreQuery, if the query is MatchAllDocs."
0,"Break the spi2dav dependency to jcr-server. Currently the spi2dav component has a dependency on the jcr-server component, which is troublesome due to the extra transitive dependencies and which strictly speaking should not be necessary from an architectural point of view.

The dependency exists mostly for sharing a number of JCR-specific WebDAV constants. I'd like to push those constants down to jackrabbit-webdav as they are essentially just shared strings and as jackrabbit-webdav already contains a number of constants used by JCR extensions.

In addition to constant values, code in the following classes is shared between jcr-server and spi2dav: JcrValueType, NamespacesProperty, NodeTypesProperty, SearchResultProperty, SubscriptionImpl, ValuesProperty. The shared code in JcrValueType and SubscriptionImpl is mostly just about mapping constant value mappings and could fairly easily be moved to jackrabbit-webdav. The Property classes are a but trickier, but it looks like it would be possible to split the code to separate server- and client-side classes for jcr-server and spi2dav."
0,"ManageableCollectionUtil should not throw ""unsupported"" JcrMapping exception. Many times, the object model'd code cannot be altered for ocm.

To avoid the ""unsupported"" exception in almost all such cases, use a delegating wrapper class to encapsulate a Collection.    The wrapper class implements MaangeableCollection.

Since delegation is a performance hit, make the test below the last resort for *object*  conversion in the method:
public static ManageableCollection getManageableCollection(Object object) 

Proposed ""catchall"" test and program action:

            if (object instanceof Collection) {
                return new ManageableCollectionImpl((Collection)object);
            }

"
0,Automatic upgrade to 2.0. Jackrabbit 2.0 contains some changes that are not compatible with repositories created with earlier versions. It would be nice if Jackrabbit would automatically detect and upgrade repositories created with 1.x versions.
0,"single norm file still uses up descriptors. The new index file format with a single .nrm file for all norms does not decrease file descriptor usage.
The .nrm file is opened once for each field with norms in the index segment."
0,"Some concurrency improvements for NRT. Some concurrency improvements for NRT

I found & fixed some silly thread bottlenecks that affect NRT:

  * Multi/DirectoryReader.numDocs is synchronized, I think so only 1
    thread computes numDocs if it's -1.  I removed this sync, and made
    numDocs volatile, instead.  Yes, multiple threads may compute the
    numDocs for the first time, but I think that's harmless?

  * Fixed BitVector's ctor to set count to 0 on creating a new BV, and
    clone to copy the count over; this saves CPU computing the count
    unecessarily.

  * Also strengthened assertions done in SR, testing the delete docs
    count.

I also found an annoying thread bottleneck that happens, due to CMS.
Whenever CMS hits the max running merges (default changed from 3 to 1
recently), and the merge policy now wants to launch another merge, it
forces the incoming thread to wait until one of the BG threads
finishes.

This is a basic crude throttling mechanism -- you force the mutators
(whoever is causing new segments to appear) to stop, so that merging
can catch up.

Unfortunately, when stressing NRT, that thread is the one that's
opening a new NRT reader.

So, the first serious problem happens when you call .reopen() on your
NRT reader -- this call simply forwards to IW.getReader if the reader
was an NRT reader.  But, because DirectoryReader.doReopen is
synchronized, this had the horrible effect of holding the monitor lock
on your main IR.  In my test, this blocked all searches (since each
search uses incRef/decRef, still sync'd until LUCENE-2156, at least).
I fixed this by making doReopen only sync'd on this if it's not simply
forwarding to getWriter.  So that's a good step forward.

This prevents searches from being blocked while trying to reopen to a
new NRT.

However... it doesn't fix the problem that when an immense merge is
off and running, opening an NRT reader could hit a tremendous delay
because CMS blocks it.  The BalancedSegmentMergePolicy should help
here... by avoiding such immense merges.

But, I think we should also pursue an improvement to CMS.  EG, if it
has 2 merges running, where one is huge and one is tiny, it ought to
increase thread priority of the tiny one.  I think with such a change
we could increase the max thread count again, to prevent this
starvation.  I'll open a separate issue....
"
0,"[PATCH] unnecessary synchronized collections used only in thread safe way. NodeTypeReader uses Vector in only a local variable thread safe way. Thus the synchronized value of Vector is not needed, and just slowing the code down for nothing. this patch switches the collections to ArrayLists."
0,"Allow setting the IndexWriter docstore to be a different directory. Add an IndexWriter.setDocStoreDirectory method that allows doc
stores to be placed in a different directory than the IW default
dir."
0,"HuperDuperSynonymsFilter. The current synonymsfilter uses a lot of ram and cpu, especially at build time.

I think yesterday I heard about ""huge synonyms files"" three times.

So, I think we should use an FST-based structure, sharing the inputs and outputs.
And we should be more efficient with the tokenStream api, e.g. using save/restoreState instead of cloneAttributes()
"
0,Move extensions to the JSR 283 security API  from jackrabbit-core to jackrabbit-api. For the 2.0.0 release i'd like to have the jackrabbit-specific extensions to the JSR 283 security API being part of jackrabbit-api.
0,"integrate snowball stopword lists. The snowball project creates stopword lists as well as stemmers, example: http://svn.tartarus.org/snowball/trunk/website/algorithms/english/stop.txt?view=markup

This patch includes the following:
* snowball stopword lists for 13 languages in contrib/snowball/resources
* all stoplists are unmodified, only added license header and converted each one from whatever encoding it was in to UTF-8
* added getSnowballWordSet  to WordListLoader, this is because the format of these files is very different, for example it supports multiple words per line and embedded comments.

I did not add any changes to SnowballAnalyzer to actually automatically use these lists yet, i would like us to discuss this in a future issue proposing integrating snowball with contrib/analyzers.
"
0,Configure occurrence of property value in excerpt. Jackrabbit currently includes all indexed property values as potential content in an excerpt. This is not always desirable because there may be properties that need to be full-text indexed but should not show up in an excerpt.
0,"Remove lib directory from SVN trunk. build.xml expects to find junit.jar in the lib directory for building and running tests.

The jar is not included in SVN, but nor is the jar ignored, so when it is downloaded it shows up as an unversioned file.

The file should be included in or excluded from SVN.

==

Note: In JMeter we use a lib/opt directory.
This is present in SVN - but all contents are ignored.

This can be used for extra jars that cannot be or are not included in SVN.

Could use the same approach for junit.jar..."
0,AccessControlEntries should be orderable. the entries of an AccessControlList should be orderable if the order of the ac-entries matters for the access control evaluation.
0,"use reusable collation keys in ICUCollationFilter. ICUCollationFilter need not create a new CollationKey object for each token.
In ICU there is a mechanism to use a reusable key.
"
0,"Use FileLock for locking instead of empty file. The FSDirectory uses File.createNewFile to effectively lock a directory (in makeLock), yet the Java Spec says explcitly not to use it for this purpose, and instead use FileLock from nio.

The attached patch shows how this is/could be done (change is internal to the makeLock method only, and functionally equivalent, the same tests apply)."
0,Improve javadocs for Numeric*. I'm working on improving Numeric* javadocs.
0,"Document inheritance of node type attributes such as orderable. Documentation task.

see http://mail-archives.apache.org/mod_mbox/jackrabbit-users/200801.mbox/browser"
0,"Change default write lock file location to index directory (not java.io.tmpdir). Now that readers are read-only, we no longer need to store lock files
in a different global lock directory than the index directory.  This
has been a source of confusion and caused problems to users in the
past.

Furthermore, once the write lock is stored in the index directory, it
no longer needs the big digest prefix that was previously required
to make sure lock files in the global lock directory, from different
indexes, did not conflict.

This way, all files related to an index will appear in a single
directory.  And you can easily list that directory to see if a
""write.lock"" is present to check whether a writer is open on the
index.

Note that this change just affects how FSDirectory creates its default
lockFactory if no lockFactory was specified.  It is still possible
(just no longer the default) to pick a different directory to store
your lock files by pre-instantiating your own LockFactory.

As part of this I would like to remove LOCK_DIR and the no-argument
constructor, in SimpleFSLockFactory and NativeFSLockFactory.  I don't
think we should have the notion of a global default lock directory
anymore.  This is actually an API change.  However, neither
SimpleFSLockFactory nor NativeFSLockFactory haver been released yet,
so I think this API removal is allowed?

Finally I want to deprecate (but not yet remove, because this has been
in the API for many releases) the static LOCK_DIR that's in
FSDirectory.  But it's now entirely unused.

See here for discussion leading to this:

  http://www.gossamer-threads.com/lists/lucene/java-dev/43940
"
0,"Support for handling large files should be added. You should probably change the EntityEnclosingMethod content length field 
to a long in a future release. Currently it's an int."
0,"Do not use deletable anymore. The query handler implementation currently uses a deletable file to keep track of index segments that are not needed anymore and can be deleted. In general index segments are deleted right away when they are not needed anymore, but it may happen that index readers are still open (because of a time consuming query) and the index segment cannot be deleted at the moment. In this case the index segment name is written to the deletable file and the index periodically tries to delete the segment later.

The implementation should rather infer from the indexes file on startup, which segments are still needed and in use."
0,"Enable setting hits queue size in Search*Task in contrib/benchmark. In testing for LUCENE-1483, I'd like to try different collector queue
sizes during benchmarking.  But currently contrib/benchmark uses
deprecated Hits with hardwired ""top 100"" queue size.  I'll switch it to
the TopDocs APIs."
0,"make sure no static loggers are used. Review all loggers used in the component, make sure they are stored in non-static attributes only.
http://wiki.apache.org/jakarta-commons/Logging/StaticLog
"
0,"Disable norms for untokenized fields to reduce memory consumption. For repositories with many indexed fields, the norms cause memory problems both during indexing and querying (see LUCENE-448). Since the fields in question were never boosted they could as well be indexed without norms."
0,"Implement Connection Timeouts. I was writing test code to use the setSoTimeout(int millis) method to set a
timeout value when connecting to a URL.  It appears to me that no matter what I
set the timeout to be a HttpConnection will try to connect but uses some other
timeout value(I'm guessing the OS's default value).  I looked at the code for
HttpConnection and it uses the Socket(host,port) constructor which tries to
connect write away.  I'd like to suggest the following code below so the timeout
is set before first the connection is even made.

/* Compile the code as is and it should timeout within a sec.  If you
 * uncomment the first two lines after the try statement and comment
 * out the other socket connect statements and run the code again you will
 * notice write away that the timeout is something else because it connects
 * right away in the constructor.  Its like the timeout is worthless at this
 * point.  As a matter a fact the code should never get there.
 * This all assumes that 192.168.168.50 is not on your network.
 */

import java.io.*;
import java.net.*;

public class SocketTest {
    public static void main(String[] args) {
        long start = System.currentTimeMillis();

        try {
            //Socket socket = new Socket(""192.168.168.50"",80);
            //socket.setSoTimeout(1000);

            //Setting timeout before the connection is made.
            Socket socket = new Socket();
            InetSocketAddress sAddress =
                new InetSocketAddress(""192.168.168.50"",80);
            socket.connect(sAddress,1000);

        } catch (UnknownHostException e) {
            System.out.println(e);
        } catch (SocketException e) {
            System.out.println(e);
        } catch (IOException e) {
            System.out.println(e);
        }

        System.out.println(System.currentTimeMillis() - start);
    }
}"
0,"IndexOutput.writeString() should write length in bytes. We should change the format of strings written to indexes so that the length of the string is in bytes, not Java characters.  This issue has been discussed at:

http://www.mail-archive.com/java-dev@lucene.apache.org/msg01970.html

We must increment the file format number to indicate this change.  At least the format number in the segments file should change.

I'm targetting this for 2.1, i.e., we shouldn't commit it to trunk until after 2.0 is released, to minimize incompatible changes between 1.9 and 2.0 (other than removal of deprecated features)."
0,"[PATCH] BitSetQuery, FastPrefixQuery, FastWildcardQuery and FastQueryParser. FastPrefixQuery and FastWildcardQuery rewrites to BitSetQuery instead of OR'ed
BooleanQuery's.  A BitSetQuery contains a BitSet that desginates which document
should be included in the search result.  BitSetQuery cannot be used by itself
with MultiSearcher as of now."
0,Remove Searcher from Weight#explain. Explain needs to calculate corpus wide stats in a way that is consistent with MultiSearcher.
0,"remove Byte/CharBuffer wrapping for collation key generation. We can remove the overhead of ByteBuffer and CharBuffer wrapping in CollationKeyFilter and ICUCollationKeyFilter.

this patch moves the logic in IndexableBinaryStringTools into char[],int,int and byte[],int,int based methods, with the previous Byte/CharBuffer methods delegating to these.
Previously, the Byte/CharBuffer methods required a backing array anyway.
"
0,"Refactoring Lucene collectors (HitCollector and extensions). This issue is a result of a recent discussion we've had on the mailing list. You can read the thread [here|http://www.nabble.com/Is-TopDocCollector%27s-collect()-implementation-correct--td22557419.html].

We have agreed to do the following refactoring:
* Rename MultiReaderHitCollector to Collector, with the purpose that it will be the base class for all Collector implementations.
* Deprecate HitCollector in favor of the new Collector.
* Introduce new methods in IndexSearcher that accept Collector, and deprecate those that accept HitCollector.
** Create a final class HitCollectorWrapper, and use it in the deprecated methods in IndexSearcher, wrapping the given HitCollector.
** HitCollectorWrapper will be marked deprecated, so we can remove it in 3.0, when we remove HitCollector.
** It will remove any instanceof checks that currently exist in IndexSearcher code.
* Create a new (abstract) TopDocsCollector, which will:
** Leave collect and setNextReader unimplemented.
** Introduce protected members PriorityQueue and totalHits.
** Introduce a single protected constructor which accepts a PriorityQueue.
** Implement topDocs() and getTotalHits() using the PQ and totalHits members. These can be used as-are by extending classes, as well as be overridden.
** Introduce a new topDocs(start, howMany) method which will be used a convenience method when implementing a search application which allows paging through search results. It will also attempt to improve the memory allocation, by allocating a ScoreDoc[] of the requested size only.
* Change TopScoreDocCollector to extend TopDocsCollector, use the topDocs() and getTotalHits() implementations as they are from TopDocsCollector. The class will also be made final.
* Change TopFieldCollector to extend TopDocsCollector, and make the class final. Also implement topDocs(start, howMany).
* Change TopFieldDocCollector (deprecated) to extend TopDocsCollector, instead of TopScoreDocCollector. Implement topDocs(start, howMany)
* Review other places where HitCollector is used, such as in Scorer, deprecate those places and use Collector instead.

Additionally, the following proposal was made w.r.t. decoupling score from collect():
* Change collect to accecpt only a doc Id (unbased).
* Introduce a setScorer(Scorer) method.
* If during collect the implementation needs the score, it can call scorer.score().
If we do this, then we need to review all places in the code where collect(doc, score) is called, and assert whether Scorer can be passed. Also this raises few questions:
* What if during collect() Scorer is null? (i.e., not set) - is it even possible?
* I noticed that many (if not all) of the collect() implementations discard the document if its score is not greater than 0. Doesn't it mean that score is needed in collect() always?

Open issues:
* The name for Collector
* TopDocsCollector was mentioned on the thread as TopResultsCollector, but that was when we thought to call Colletor ResultsColletor. Since we decided (so far) on Collector, I think TopDocsCollector makes sense, because of its TopDocs output.
* Decoupling score from collect().

I will post a patch a bit later, as this is expected to be a very large patch. I will split it into 2: (1) code patch (2) test cases (moving to use Collector instead of HitCollector, as well as testing the new topDocs(start, howMany) method.
There might be even a 3rd patch which handles the setScorer thing in Collector (maybe even a different issue?)"
0,"Add/change warning comments in the javadocs of Payload APIs. Since the payload API is still experimental we should change the comments
in the javadocs similar to the new search/function package."
0,"FieldInfos should be read-only if loaded from disk. Currently FieldInfos create a private FieldNumberBiMap when they are loaded from a directory which is necessary due to some limitation we need to face with IW#addIndexes(Dir). If we add an index via a directory to an existing index field number can conflict with the global field numbers in the IW receiving the directories. Those field number conflicts will remain until those segments are merged and we stabilize again based on the IW global field numbers. Yet, we unnecessarily creating a BiMap here where we actually should enforce read-only semantics since nobody should modify this FieldInfos instance we loaded from the directory. If somebody needs to get a modifiable copy they should simply create a new one and all all FieldInfo instances to it.

"
0,"jcr2spi: create ChangePolling thread on demand. currently a new ChangePolling thread is created for every single session even if there is neither observation eventlistener nor cachebehaviour#observation.
i think we could create that on demand."
0,"don't silently merge session-local transient changes with external changes before save().. currently, external changes (i.e. changes committed by other sessions) are silently merged with transient changes. this might potentially cause concurrency issues/inconsistent transient state (see e.g. JCR-2632).

it would probably be better to isolate transient changes from external changes until they're saved (true copy-on-write). "
0,"HttpState#PREEMPTIVE_PROPERTY removed.. Our code no longer compiles as HttpState#PREEMPTIVE_PROPERTY has been removed.
Our code compiles with 2.0.1.

See: 
http://jakarta.apache.org/commons/httpclient/apidocs/org/apache/commons/httpclient/HttpState.html#PREEMPTIVE_PROPERTY"
0,"GData - Server wrong commit does not build. The last GData - Server commit  does not build due to a wrong commit.
Yonik did not commit all the files in the diff file. There are several sources and packages missing.
  
The diff - file with the date of 26.06.06 should be applied.
--> http://issues.apache.org/jira/browse/LUCENE-598
26.06.06.diff (644 kb)

could any of the lucene committers apply this patch. Yonik is on the way to Dublin.

Thanks Simon
"
0,"move contrib/analyzers to modules/analysis. This is a patch to move contrib/analyzers under modules/analyzers

We can then continue consolidating (LUCENE-2413)... in truth this will sorta be 
an ongoing thing anyway, as we try to distance indexing from analysis, etc
"
0,"Fix IndexCommit hashCode() and equals() to be consistent. IndexCommit's impl of hashCode() and equals() is inconsistent. One uses Dir + version and the other uses Dir + equals. According to hashCode()'s javadoc, if o1.equals(o2), then o1.hashCode() == o2.hashCode(). Simple fix, and I'll add a test case."
0,"improve test coverage for Multi*. It seems like an easy win that when the test calls newSearcher(), 
it should sometimes wrap the reader with a SlowMultiReaderWrapper.
"
0,"Move Jackrabbit Query Parser from core to spi-commons. The query parser can be used outside jackrabbit-core, for instances in other repository implementations based on JCR2SPI.

Proposal:

- move source and build infrastructure from o.a.j.core.query to o.a.j.spi.commons.query

- switch over jackrabbit.core to use spi-commons for query

- optimally, add specific test cases for the query tree generation. "
0,"wordlistloader is inefficient. WordListLoader is basically used for loading up stopwords lists, stem dictionaries, etc.
Unfortunately the api returns Set<String> and sometimes even HashSet<String> or HashMap<String,String>

I think we should break it and return CharArraySets and CharArrayMaps (but leave the return value as generic Set,Map).

If someone objects to breaking it in 3.1, then we can do this only in 4.0, but i think it would be good to fix it both places.
The reason is that if someone does new FooAnalyzer() a lot (probably not uncommon) i think its doing a bunch of useless copying.

I think we should slap @lucene.internal on this API too, since thats mostly how its being used.
"
0,Make HighFreqTerms.TermStats class public. It's not possible to use public methods in contrib/misc/... /HighFreqTerms from outside the package because the return type has package visibility. I propose to move TermStats class to a separate file and make it public.
0,"cutover FunctionQuery tests to use RandomIndexWriter, for better testing. "
0,Add Compact Namespace and Node Type Definition support to spi-commons. Add support for reading and writing of Compact Namespace and Node Type Definitions (cnd-files) to spi-commons. 
0,"Avoid string concatenation in AbstractBundlePersistenceManager. The following line:

        log.debug(""stored bundle "" + bundle.getId());

should be changed to:

        log.debug(""stored bundle {}"", bundle.getId());
"
0,"Enable setting the terms index divisor used by IndexWriter whenever it opens internal readers. Opening a place holder issue... if all the refactoring being discussed don't make this possible, then we should add a setting to IWC to do so.

Apps with very large numbers of unique terms must set the terms index divisor to control RAM usage.

(NOTE: flex's RAM terms dict index RAM usage is more efficient, so this will help such apps).

But, when IW resolves deletes internally it always uses default 1 terms index divisor, and the app cannot change that.  Though one workaround is to call getReader(termInfosIndexDivisor) which will pool the reader with the right divisor."
0,"add Token.setTermText(), remove final. The Token class should be more friendly to classes not in it's package:
  1) add setTermText()
  2) remove final from class and toString()
  3) add clone()

Support for (1):
  TokenFilters in the same package as Token are able to do things like 
   ""t.termText = t.termText.toLowerCase();"" which is more efficient, but more importantly less error prone.  Without the ability to change *only* the term text, a new Token must be created, and one must remember to set all the properties correctly.  This exact issue caused this bug:
http://issues.apache.org/jira/browse/LUCENE-437

Support for (2):
  Removing final allows one to subclass Token.  I didn't see any performance impact after removing final.
I can go into more detail on why I want to subclass Token if anyone is interested.

Support for (3):
  - support for a synonym TokenFilter, where one needs to make two tokens from one (same args that support (1), and esp important if instance is a subclass of Token)."
0,"Some enhancements to jackrabbit commons. I would like to suggest a couple of  enhancements to the commons module. 

The patch was created against rev. 417443 and the tests did not reveal any 
problems.

Summary of suggestion modifications:

QName
-------------------------------------------------------------------------------------------------------------------------
- reduce QName to its core functionality and put conversion from and to JCR name to
  a separate class 'NameFormat'
- in order not to break existing code, all methods that deal with the conversion in QName
  are marked deprecated.
- add constant for the name of the root node.

Path
-------------------------------------------------------------------------------------------------------------------------
- reduce Path to its core functionality and put conversion from and to JCR path to
  a separate class 'PathFormat'
- in order not to break existing code, all methods that deal with the conversion in Path
  are marked deprecated.
- introduce new constants for UNDEFINED_INDEX (0) and DEFAULT_INDEX (1), that
   are currently hardcoded throughout  the jackrabbit project.
- new method Path.getElement(int) [PathElement]
- make PathElement constants public (used by PathFormat)

Path.PathBuilder
-------------------------------------------------------------------------------------------------------------------------
- additional constructor  PathBuilder(Path)

Path.PathElement
-------------------------------------------------------------------------------------------------------------------------
- add PathElement.getNormalizedIndex() that always asserts a 1-based index.
- change subclasses to be private (no usage within the jackrabbit, except inside Path).

PathMap
-------------------------------------------------------------------------------------------------------------------------
- move o.a.j.core.PathMap  to o.a.j.util.PathMap in order to make it available in the
  commons module.

NamespaceResolver
-------------------------------------------------------------------------------------------------------------------------
- add methods for resolution of paths:
   > getQPath(String jcrPath) [Path]
   > getJCRPath(Path qPath) [String]

NamespaceListener
-------------------------------------------------------------------------------------------------------------------------
- add method 'namespaceRemove(String)'

ValueHelper
-------------------------------------------------------------------------------------------------------------------------
currently  JCR value objects are 'manually' created in the ValueHelper despite the
fact, that JSR170 defines a ValueFactory interface. Consequently the ValueHelper
present in the commons module can only be used by implementations that use
the same value implementations.

- add new helper methods that take a ValueFactory as argument.
- in order not to break existing code the original methods are marked deprecated and
  may be removed at a later time.

consequently:
- modify signature of  InternalValue.create that include a value conversion to take a
  ValueFactory param and adjust all usages inside the core package.

ValueFactoryImpl
-------------------------------------------------------------------------------------------------------------------------
- createValue(String, int ): used to call the conversion on ValueHelper. with the 
   changes suggested to ValueHelper, the code must be changed in order to
   created instances of the Value implementations within the factory.
- together with the modification to ValueHelper, stefan suggested to replace the public 
  constructor with a static 'getInstance' method. All usages within jackrabbit.core, were
   modified accordingly.

Text
-------------------------------------------------------------------------------------------------------------------------
- add getName(String, boolean) where the boolean flag indicates whether  a trailing slash 
   should be ignored.
- add getRelativeParent(String, int, boolean) where the boolean flag indicates whether  a 
  trailing slash should be ignored."
0,Grouping collector that computes grouped facet counts. Spinoff from issue SOLR-2898. 
0,"Allow extendability of RepositoryImpl.WorkspaceInfo. the workspace info has some package private and some protected methods. in order to be able to extend the workspace info, we need to have all methods protected."
0,FileRevision should have a flag to control whether to sync the file on every write.. FileRevision class syncs the underlying revision.log file it uses on every write which could be a performance problem. Add a boolean flag to control whether to sync the file on every write.
0,"HttpClient depends on jcip-annotations.jar. When using Java 5 to compile code that uses HttpClient, jcip-annotations.jar must be in the classpath or else you get a compiler error:

    [javac] /path/to/src/SomeFile.java:129: cannot access net.jcip.annotations.GuardedBy
    [javac] file net/jcip/annotations/GuardedBy.class not found
    [javac]         DefaultHttpClient httpclient = new DefaultHttpClient();
    [javac]                                        ^


With Java 6, you get a bunch of warnings instead.
    [javac] org/apache/http/impl/client/AbstractHttpClient.class(org/apache/http/impl/client:AbstractHttpClient.class): warning: Cannot find annotation method 'value()' in type 'net.jcip.annotations.GuardedBy': class file for net.jcip.annotations.GuardedBy not found


This requirement doesn't seem to be documented anywhere, and jcip-annotations.jar is not included in the ""httpcomponents-client-4.0-bin-with-dependencies"" package.
"
0,"Use ConcurrentHashMap in RAMDirectory. RAMDirectory synchronizes on its instance in many places to protect access to map of RAMFiles, in addition to updating the sizeInBytes member. In many places the sync is done for 'read' purposes, while only in few places we need 'write' access. This looks like a perfect use case for ConcurrentHashMap

Also, syncing around sizeInBytes is unnecessary IMO, since it's an AtomicLong ...

I'll post a patch shortly."
0,"All implementations of SchemeSocketFactory.createSocket(HttpParams params) ignore the params. Only TestTSCCMWithServer.StallingSocketFactory.createSocket(HttpParams params) ever uses the HttpParams parameter.

All non-test implementations of the method ignore the parameter.

Not sure why this version of the method exists if the parameter is never used - the parameterless method from SocketFactory could be used instead."
0,"Incorrect support for java interfaces in typed collection fields. If a typed collection field is defined with an Interface as the type, the following exception is thrown when the main object is inserted : 

org.apache.jackrabbit.ocm.exception.JcrMappingException: Cannot load class interface [name of the interface];

Here is a example : 

@Node
public class EntityA {
       @Field(path=true) String path;
       @Collection List<MyInterface> entityB;
       ....
}

When inserting a new instance of EntityA with a not null entityB, the exception is thrown. 
A workaround is to add the elementClassName on the annotation @Collection. ex. : 

@Collection (elementClassName=MyInterface.class) List<MyInterface> entityB;

elementClassName is used only for untyped collections but if you specify it for a typed collection, the ObjectContentManager will not use reflexion to check the collection class name. 
 
This should be nice to avoid the usage of elementClassName for typed collections. 

"
0,[PATCH] import cleanup. This patch just removes useless imports so you get less warnings in Eclipse.
0,"Upload Lucene 2.0 artifacts in the Maven 1 repository. The Lucene 2.0 artifacts can be found in the Maven 2 repository, but not in the Maven 1 repository. There are still projects using Maven 1 who might be interested in upgrading to Lucene 2, so having the artifacts also in the Maven 1 repository would be very helpful."
0,"[CONTRIB] SSL authenticating protocol socket factory. Here's the long promised SSL client/server authenticating socket factory. This
socket factory can be used to enforce client/server authentication during the
SSL context negotiation. Let me know what you think. Please, please someone
proof-read the accompanying javadocs and let me know if the text is comprehensible 

I have also tweaked EasySSLProtocolSocketFactory a little

The patch is against HTTPCLIENT_2_0_BRANCH

Oleg"
0,Promote ChildNodeEntry and ChildNodeEntries to top level classes.. The current NodeState class is quite heavy weight (source code wise) and the inner class ChildNodeEntry is used in a lot of places outside of NodeState. I think it is useful to have them promoted to top level classes.
0,"provide a (relatively) simple way to disable anonymous access to the security workspace. As discussed in this thread: http://sling.markmail.org/thread/st52jejjuxykfxtj, the security workspace is, by default, configured with an AccessControlProvider which provides a fixed access control policy (i.e. o.a.j.core.security.user.UserAccessControlProvider). In order to prevent anonymous access to security-related nodes requires the use of an alternate AccessControlProvider.

The attached patch provides a simpler mechanism. By adding

<param name=""anonymousAccessToSecurityWorkspace"" value=""false"" />

to the configuration of the DefaultSecurityManager, anonymous access to the security workspace is forbidden.
"
0,"Upgrade to Xerces 2.8.1. Besides a number of bug fixes and new features in Xerces, upgrading to a newer version would also give us better dependency metadata, and thus avoid the need of having a separate xmlParserAPIs dependency in addition to the main xercesImpl dependency. "
0,"Refactor Searchable to not have RMI Remote dependency. Per http://lucene.markmail.org/message/fu34tuomnqejchfj?q=RemoteSearchable

We should refactor Searchable slightly so that it doesn't extend the java.rmi.Remote marker interface.  I believe the same could be achieved by just marking the RemoteSearchable and refactoring the RMI implementation out of core and into a contrib.

If we do this, we should deprecate/denote it for 2.9 and then move it for 3.0"
0,"Allow PackedInts.ReaderIterator to advance more than one value. The iterator-like API in LUCENE-2186 makes effective use of PackedInts.ReaderIterator but frequently skips multiple values. ReaderIterator currently requires to loop over ReaderInterator#next() to advance to a certain value. We should allow ReaderIterator to expose a #advance(ord) method to make use-cases like that more efficient. 

This issue is somewhat part of my efforts to make LUCENE-2186 smaller while breaking it up in little issues for parts which can be generally useful."
0,Add n-gram tokenizers to contrib/analyzers. It would be nice to have some n-gram-capable tokenizers in contrib/analyzers.  Patch coming shortly.
0,"Add SessionImpl#isAdminOrSystem. we have several places in jackrabbit-core where we need to find out if the executing session is either a SystemSession or corresponds to the admin user.
currently the same code (analysing the sessions subject) is copied throughout jackrabbit-core. i would like to replace that by a helper method on SessionImpl."
0,"Add a document describing the HttpClient release process. The commons release process (http://jakarta.apache.org/commons/releases.html) is
a good starting place, but is out of date.  When we do our own releases, there
are some other steps particular to maven, the test-local and the webapp tests
that must be documented."
0,"Pre-analyzed fields. Adds the possibility to set a TokenStream at Field constrution time, available as tokenStreamValue in addition to stringValue, readerValue and binaryValue.

There might be some problems with mixing stored fields with the same name as a field with tokenStreamValue."
0,"WorkspaceRestoreTest extends RestoreTest. WorkspaceRestoreTest extends RestoreTest in order to re-use variables and setUp/tearDown code. 

On the other hand, this causes all tests from RestoreTest that aren't overriden by WorkspaceRestoreTest to be run twice.

Proposal: decouple the classes, copying over the interesting parts from RestoreTest into WorkspaceRestoreTest.
"
0,"Log / trace wrapper for the JCR API. I have implemented the log / trace mechanism for the JCR API. A short summary:

- A wrapper for a Repository. All other objects that where created directly or indirectly (Session, Node and so on) are wrapped as well. 
- The wrappers log all JCR method calls to a file and call the underlying methods. 
- Return values and calling method / line number can be logged as well (optional). 
- The log file itself is mainly Java source code and can be compiled and run.
- Included is a player to re-play log files (for example, if the log file is too big to be compiled).
"
0,"Authentication Mechanism Based on Login Token. implementing an authentication mechanism that apart from simple credentials allows for credentials being built on a login token
could rely on the fact that jackrabbit stores the user data in the repository: adding additional information (generated tokens, expiration time, 
additional security parameters) could be stored in additional subnodes to the user and used for matching during login as alternative
ways to authenticate against the system."
0,"NewAnalyzerTask. NewAnalyzerTask (patch to follow) allows a contrib/benchmark algorithm to change Analyzers during a run.  This is useful when comparing Analyzers

{""NewAnalyzer"" NewAnalyzer(WhitespaceAnalyzer, SimpleAnalyzer, StopAnalyzer, standard.StandardAnalyzer) >

is a sample declaration in an algorithm file."
0,"Standardize on a common mocking framework (either EasyMock or Mockito). We are currently using EasyMock in the caching module and Mockito in the main module. While Mockito appears to have a somewhat nicer API, the sheer number of test cases based on EasyMock in the caching module makes it much simpler to replace Mockito with EasyMock than the other way around."
0,"Problematic exception handling in Jackrabbit WebApp. In this project, the cause of the exception is often ignored, and only the message of the cause is used, as in:

} catch (Exception e) {
    log.error(""Error in configuration: {}"", e.toString());
    throw new ServletException(""Error in configuration: "" + e.toString());
}

An additional problem is that when using ServletException(String message, Throwable rootCause), the rootCause is not used in printStackTrace(), that means the cause is not logged. See also: http://closingbraces.net/2007/11/27/servletexceptionrootcause/

It is therefore better to convert 
  throw new ServletException(""Unable to create RMI repository. jcr-rmi.jar might be missing."", e);
to
  ServletException s = new ServletException(""Unable to create RMI repository. jcr-rmi.jar might be missing."");
  s.initCause(e);
  throw s;




"
0,"Add a OnWorkspaceInconsistency with logging only. If a Workspace performs a re-index on startup with  a inconsistency in it the process will fail now.
The new OnWorkspaceInconsistency ""log"" will only log the inconsistency but the reindex-process will not fail"
0,"Improve handling of inherited mixins. If an abstract class is annotated with a mixin type, the annotation must be repeated in concrete classes.

E.g.
@Node(jcrMixinTypes=""mix:referenceable"", isAbstract=true)
public abstract class Content {
...
...}

/**
* This class will not be referenceable
**/
@Node(extend=Content.class)
public class Page extends Content {
...
...
}

/**
* But this one will
**/
@Node(extend=Content.class, jcrMixinTypes=""mix:referenceable"")
public class Folder extends Content {
...
...
}

It would be nice if the annotation was inherited by default."
0,"Swap URL+Email recognizing StandardTokenizer and UAX29Tokenizer. Currently, in addition to implementing the UAX#29 word boundary rules, StandardTokenizer recognizes email adresses and URLs, but doesn't provide a way to turn this behavior off and/or provide overlapping tokens with the components (username from email address, hostname from URL, etc.).

UAX29Tokenizer should become StandardTokenizer, and current StandardTokenizer should be renamed to something like UAX29TokenizerPlusPlus (or something like that).

For rationale, see [the discussion at the reopened LUCENE-2167|https://issues.apache.org/jira/browse/LUCENE-2167?focusedCommentId=12929325&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12929325]."
0,"javadocs cleanup. basic cleanup in core/contrib: typos, apache license header as javadoc, missing periods that screw up package summary, etc.
"
0,VersionTest.testGetUUID() fails. VersionTest.testGetUUID() fails due to inproper invlaidation of the successor properties after checkin.
0,provide option to automatically dispose idle workspaces. 
0,"request.abort() should interrupt thread waiting for a connection. Calls to HttpRequestBase.abort() will not unblock a thread that is still waiting for a connection and therefore has no ConnectionReleaseTrigger yet.
"
0,"EnwikiDocMaker id field. The EnwikiDocMaker is fairly usable outside of the benchmarking class, but it would benefit from indexing the ID field of the docs.

Patch to follow that adds an ID field."
0,"Extend Codec with a SegmentInfos writer / reader. I'm trying to implement a Codec that works with append-only filesystems (HDFS). It's _almost_ done, except for the SegmentInfos.write(dir), which uses ChecksumIndexOutput, which in turn uses IndexOutput.seek() - and seek is not supported on append-only output. I propose to extend the Codec interface to encapsulate also the details of SegmentInfos writing / reading. Patch to follow after some feedback ;)"
0,"when checking tvx/fdx size mismatch, also include whether the file exists. IndexWriter checks, during flush and during merge, that the size of the index file for stored fields (*.fdx) and term vectors (*.tvx) matches how many bytes it has just written.

This originally was added for LUCENE-1282, ie, as a safety to catch the nasty ""off by 1"" JRE hotspot bug that would otherwise silently corrupt the index.

However, this check also seems to catch a different case, where the size of the file is zero.   The most recent example is LUCENE-1521.  I'd like to improve the message in the exception to include whether or not the file exists, to help understand why users are sometimes hitting this exception.  My best theory at this point is something external is removing the file out from under the IndexWriter.
"
0,"Jcr2Spi: Warning upon reloading property values. tobi reported the following log-warning being written upon reloading property values:

[WARN ] Property data has been discarded

It seems to me that this has been introduced with JCR-1963. Taking a closer look at it, i get the impression that
the 'discarded' flag should only be set if any values are notified accordingly. In addition it seems to me that the MergeResult should have a dispose() method (or something similar) in order to have the replaced (old) property values properly released..."
0,"Make TokenStream Reuse Mandatory for Analyzers. In LUCENE-2309 it became clear that we'd benefit a lot from Analyzer having to return reusable TokenStreams.  This is a big chunk of work, but its time to bite the bullet.

I plan to attack this in the following way:

- Collapse the logic of ReusableAnalyzerBase into Analyzer
- Add a ReuseStrategy abstraction to Analyzer which controls whether the TokenStreamComponents are reused globally (as they are today) or per-field.
- Convert all Analyzers over to using TokenStreamComponents.  I've already seen that some of the TokenStreams created in tests need some work to be reusable (even if they aren't reused).
- Remove Analyzer.reusableTokenStream and convert everything over to using .tokenStream (which will now be returning reusable TokenStreams)."
0,"Fix PayloadProcessorProvider to no longer use Directory for lookup, instead AtomicReader. The PayloadProcessorProvider has a broken API, this should be fixed. The current trunk mimics the old behaviour, but not 100%.

The PayloadProcessorProvider API should return a PayloadProcessor based on the AtomicReader instance that gets merged. As AtomicReader do no longer know the directory they are reside (they could be e.g. FilterIndexReaders, MemoryIndexes,...) a selection by Directory is no longer possible.

The current code in Lucene trunk mimics the old behavior by doing an instanceof SegmentReader check and then asking for a DirProvider. If something else is merged in, Payload processing is not supported. This should be changed, the old API could be kept backwards compatible by moving the instanceof check in a ""convenience class"" DirPayloadProcessorProvider, extending PayloadProcessorProvider."
0,"Helper Method to escape illegal XPath Search Term. If you try to perform a search like this

//element(*, nt:base)[jcr:contains(., 'test!')]

you get this exception

javax.jcr.RepositoryException: Exception building query: org.apache.jackrabbit.core.query.lucene.fulltext.ParseException: Encountered ""<EOF>"" at line 1, column 6.
"
0,"Occasional IndexingQueueTest failures. Every now and then, when doing a clean build of the latest jackrabbit trunk I see the following test failure in jackrabbit-core:

-------------------------------------------------------------------------------
Test set: org.apache.jackrabbit.core.query.lucene.TestAll
-------------------------------------------------------------------------------
Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.665 sec <<< FAILURE!
testQueue(org.apache.jackrabbit.core.query.lucene.IndexingQueueTest)  Time elapsed: 1.654 sec  <<< FAILURE!
junit.framework.AssertionFailedError
        at junit.framework.Assert.fail(Assert.java:47)
        at junit.framework.Assert.assertTrue(Assert.java:20)
        at junit.framework.Assert.assertTrue(Assert.java:27)
        at org.apache.jackrabbit.core.query.lucene.IndexingQueueTest.testQueue(IndexingQueueTest.java:69)

Typically the problem disappears when I rebuild, but the test should still not have failed."
0,"MANIFEST.MF cleanup (main jar and luci customizations). there are several problems with the MANIFEST.MF file used in the core jar, and some inconsistencies in th luci jar:

Lucli's build.xml has an own ""jar"" target and does not use the jar target from common-build.xml. The result
is that the MANIFEST.MF file is not consistent and the META-INF dir does not contain LICENSE.TXT and NOTICE.TXT.

Is there a reason why lucli behaves different in this regard? If not I think we should fix this."
0,"ContentBody doesn't currently have a setMimeType method.. ContentBody and therefore FileBody, StringBody and InputStreamBody do not have a setMimeType method so you can't set the Mime Type, it always defaults. 
Current workaround is to subclass and override getMimeType."
0,"NamespaceRegistryTest uses an invalid URI as namespace URI. The test cases use ""www.apache.org/..."" as a namespace URI, but this is not a URI.

Suggest to fix by using a proper URI, such as by prefixing with ""http://"".

A related question is what our expectation is for JCR implementations. Are they allowed to reject something that doesn't parse as a URI according to RFC3986?
"
0,"Filter to process output of ICUTokenizer and create overlapping bigrams for CJK . The ICUTokenizer produces unigrams for CJK. We would like to use the ICUTokenizer but have overlapping bigrams created for CJK as in the CJK Analyzer.  This filter would take the output of the ICUtokenizer, read the ScriptAttribute and for selected scripts (Han, Kana), would produce overlapping bigrams."
0,Adding FilteredDocIdSet and FilteredDocIdSetIterator. Adding 2 convenience classes: FilteredDocIdSet and FilteredDocIDSetIterator.
0,Use jackrabbit 1.2.1. Use Jackarabit 1.2.1
0,"SpanOrQuery.java: simplification and test. The current SpanOrQuery.java has some unnessary attributes. After removing these, I found that there was no existing test for it, so I added some tests to TestSpans.java."
0,"Contributed ClassLoader project still uses commons-logging for logging.. As of JCR-215 Jackrabbit core code has been migrated from Log4J to SLF4J. The ClassLoader contribution always used commons-logging. It is about time, to also migrate that project to proper SLF4J."
0,"Add the ability to disable inheriting ancestor ACLs. The current ACL implementation will walk the tree from the item being accessed, up to the root, collecting ACL entries for all the ancestors. With this system, there is no easy way to restrict access to subnodes except by adding DENY entries to negate the entries inherited from the parent nodes.

I'd like to request a way to turn this behavior off either at a node level or global level.

The place where recursion is happening is in org.apache.jackrabbit.core.security.authorization.acl.ACLProvider$Entries.collectEntries(NodeImpl node). Inside this method, it could perhaps check a global parameter or the existence of property of the ACL policy node to determine whether to recurse up the tree."
0,"Don't use ensureOpen() excessively in IndexReader and IndexWriter. A spin off from here: http://www.nabble.com/Excessive-use-of-ensureOpen()-td24127806.html.

We should stop calling this method when it's not necessary for any internal Lucene code. Currently, this code seems to hurt properly written apps, unnecessarily.

Will post a patch soon"
0,"Default to anonymous access when no Credentials are given. Even though JCR-348 made easier to start a Jackrabbit repository with default configuration, the user still needs to take care of the JAAS configuration. It would be more user-friendly to log a warning and default to superuser access rather than throwing a LoginException when JAAS has not been configured. This behaviour should be limited to only default credential logins (Session.login() with null Credentials) and it should be possible to disable it with a configuration option. We could even have this behaviour disabled by default, but enabled in the configuration file used with the JCR-348 automatic configuration.

This is a case against the ""secure by default"" design principle, but I think that in this case the benefits in easier setup outweight the security drawbacks, especially if coupled with the above restrictions and a clear documentation note about the insecure default.

[Update: As mentioned by Stefan, this is  not a JAAS configuration issue but a problem in handling null Credentials. A more proper alternative for superuser access would be to default to anonymous access when credentials are not given.]"
0,Allow o.a.j.jca.JCARepositoryManager to load repository configuration from the classpath.. The current implementation of o.a.j.jca.JCARepositoryManager is only able to load configuration files from the file system. It would be useful to allow the configuration to be loaded from the classpath also.
0,"duplicate package.html files in queryParser and analsysis.cn packages. These files conflict with eachother when building the javadocs. there can be only one (of each) ...

{code}
hossman@brunner:~/lucene/java$ find src contrib -name package.html | perl -ple 's{.*src/java/}{}' | sort | uniq -c | grep -v "" 1 ""
   2 org/apache/lucene/analysis/cn/package.html
   2 org/apache/lucene/queryParser/package.html
hossman@brunner:~/lucene/java$ find src contrib -path \*queryParser/package.html
src/java/org/apache/lucene/queryParser/package.html
contrib/queryparser/src/java/org/apache/lucene/queryParser/package.html
hossman@brunner:~/lucene/java$ find src contrib -path \*cn/package.html
contrib/analyzers/common/src/java/org/apache/lucene/analysis/cn/package.html
contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/package.html
{code}

"
0,"Jcr-Server: DavException doesn't allow to specify an exception cause. While DavException extends Exception it does not allow to specify a exception cause in the constructor.
Adding a separate constructor taking status code plus a Throwable would provide the possibility to specify the original cause."
0,"The terms index divisor in IW should be set via IWC not via getReader. The getReader call gives a false sense of security... since if deletions have already been applied (and IW is pooling) the readers have already been loaded with a divisor of 1.

Better to set the divisor up front in IWC."
0,"bulk postings should be codec private. In LUCENE-2723, a lot of work was done to speed up Lucene's bulk postings read API.

There were some upsides:
* you could specify things like 'i dont care about frequency data up front'.
  This made things like multitermquery->filter and other consumers that don't
  care about freqs faster. But this is unrelated to 'bulkness' and we have a
  separate patch now for this on LUCENE-2929.
* the buffersize for standardcodec was increased to 128, increasing performance
  for TermQueries, but this was unrelated too.

But there were serious downsides/nocommits:
* the API was hairy because it tried to be 'one-size-fits-all'. This made consumer code crazy.
* the API could not really be specialized to your codec: e.g. could never take advantage that e.g. docs and freqs are aligned.
* the API forced codecs to implement delta encoding for things like documents and positions. 
  But this is totally up to the codec how it wants to encode! Some codecs might not use delta encoding.
* using such an API for positions was only theoretical, it would have been super complicated and I doubt ever
  performant or maintainable.
* there was a regression with advance(), probably because the api forced you to do both a linear scan thru
  the remaining buffer, then refill...

I think a cleaner approach is to let codecs do whatever they want to implement the DISI
contract. This lets codecs have the freedom to implement whatever compression/buffering they want
for the best performance, and keeps consumers simple. If a codec uses delta encoding, or if it wants
to defer this to the last possible minute or do it at decode time, thats its own business. Maybe a codec
doesn't want to do any buffering at all.
"
0,"Do not consume the remaining response content if the connection is to be closed. I am working on a HttpClient-based application to send and receive potentially large files (up to Gigabytes). When receiving large files the application allows the user to cancel the download, at which time it closes the response input stream behind the scenes.

The input stream currently provided by HttpMethodBase.getResponseBody() for un-chunked responses with a known content length is a ContentLengthInputStream, which automatically reads the remainder of the wrapped response instead of closing it straight away. This behaviour does not work well with very large files as the data is downloaded unnecessarily and the connection is held open for long very periods.

Per the HTTP 1.1 spec section 14.10 it seems to me that either a server or a client in an HTTP 1.1 connection can use the Connection:close directive to signal that a connection will be non-persistent, and will therefore not require that all data be read before the connection can be released (the cleaning up ContentLengthInputStream performs for persistent connections).

http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.10

Could HttpMethodBase be modified to check for this directive, from the server or client, and avoid wrapping the response input stream in ContentLengthInputStream when it is present? It seems straight-forward, though there may be side-effects I am not aware of. 
"
0,"Let Codec consume entire document. Currently the codec API is limited to consume Terms & Postings upon a segment flush. To enable stored fields & DocValues to make use of the Codec abstraction codecs should allow to pull a consumer ahead of flush time and consume all values from a document's field though a consumer API. An alternative to consuming the entire document would be extending FieldsConsumer to return a StoredValueConsumer / DocValuesConsumer like it is done in DocValues - Branch right now side by side to the TermsConsumer. Yet, extending this has proven to be very tricky and error prone for several reasons:
* FieldsConsumer requires SegmentWriteState which might be different upon flush compared to when the document is consumed. SegmentWriteState must therefor be created twice 1. when the first docvalues field is indexed 2. when flushed. 
* FieldsConsumer are current pulled for each indexed field no matter if there are terms to be indexed or not. Yet, if we use something like DocValuesCodec which essentially wraps another codec and creates FieldConsumer on demand the wrapped codecs consumer might not be initialized even if the field is indexed. This causes problems once such a field is opened but missing the required files for that codec. I added some harsh logic to work around this which should be prevented.
* SegmentCodecs are created for each SegmentWriteState which might yield wrong codec IDs depending on how fields numbers are assigned. We currently depend on the fact that all fields for a segment and therefore their codecs are known when SegmentCodecs are build. To enable consuming perDoc values in codecs we need to do that incrementally

Codecs should instead provide a DocumentConsumer side by side with the FieldsConsumer created prior to flush. This is also a prerequisite for LUCENE-2621"
0,"Lucene needs to ship the JUnit jar for testing. In order for Hudson builds, etc. to work properly, Lucene needs to ship the JUnit jar and have it made available in the testing classpath.  Our system reqs say 3.8.1, but I have 3.8.2 laying around, so I will update the system requirements, too."
0,Missing log4j.properties file. The log4j.properties file is missing in the test resources.
0,"Upgrade to Java 5 as the base platform. As discussed on the mailing list, Jackrabbit 2.0 will use Java 5 as the base platform.

Now that 1.x has been branched, we can update the build settings in trunk to use Java 5."
0,"o.a.j.webdav.jcr.DavResourceFactoryImpl#createResource creates VersionControlledResource instances regardless of mix:versionable status. DavResourceFactoryImpl#createResource() first calls createResourceForItem() which threats all nodes as version-controlled. 
it then calls isVersionControlled() which indirectly triggers a call to Node#getVersionHistroy(). 
getVersionHistroy throws a UnsupportedRepositoryException if the node is non-versionable, leading to a DavException further up the call stack.

as a consequence, every request for a non-versionable node leads to unnecessary (and expensive) exception generation which could be avoided by checking the mix:versionable status of a node.


"
0,better explain output. Very simple patch that slightly improves output of idf: show both docFreq and numDocs.
0,"avoid converting property values to strings. QValues currently can not expose properties of types LONG and DOUBLE in a parsed format. Thus, setting/retrieving properties of these types requires roundtripping through Strings, which we should avoid.

Proposal:

1) Add ""long getLong()"" and ""double getDouble()"" to QValue.

2) Add matching create methods to QValueFactory.

3) Take advantage of the new methods in JCR2SPI, for instance by allowing it's own Value implementation to internally just hold the QValue.

"
0,"TRStringDistance uses way too much memory (with patch). The implementation of TRStringDistance is based on version 2.1 of org.apache.commons.lang.StringUtils#getLevenshteinDistance(String, String), which uses an un-optimized implementation of the Levenshtein Distance algorithm (it uses way too much memory). Please see Bug 38911 (http://issues.apache.org/bugzilla/show_bug.cgi?id=38911) for more information.

The commons-lang implementation has been heavily optimized as of version 2.2 (3x speed-up). I have reported the new implementation to TRStringDistance."
0,"Change security configuration from 'simple' to the some reasonable default. I'd like to change the security configuration from 'simple' to a default that enables ac-management and user management (and consequently doesn't skip the corresponding tests).

there is currently an issue with WEAKREFERENCEs (see issue JCR-2135) that prevents me from changing the config.
unless someone objects i would like to change the default config as soon as JCR-2135 is solved.

"
0,Jackrabbit concurrency review and invariants. I've been working on reviewing and verifying the internal concurrency model of Jackrabbit in an attempt to proactively prevent the kinds of deadlock issues we've seen before. Overall things seem pretty good nowadays. I'll be updating the web site with some resulting design and review docs that should help guide future work in this area. I also have created some global invariant checks that I'll be adding to the codebase.
0,"[PATCH] new method expungeDeleted() added to IndexWriter. We make use the docIDs in lucene. I need a way to compact the docIDs in segments
to remove the ""holes"" created from doing deletes. The only way to do this is by
calling IndexWriter.optimize(). This is a very heavy call, for the cases where
the index is large but with very small number of deleted docs, calling optimize
is not practical.

I need a new method: expungeDeleted(), which finds all the segments that have
delete documents and merge only those segments.

I have implemented this method and have discussed with Otis about submitting a
patch. I don't see where I can attached the patch. I will do according to the
patch guidleine and email the lucene mailing list.

Thanks

-John

I don't see a place where I can"
0,"Incorrect outer join TCK tests. The TCK test cases for outer joins seem to be incorrect. More specifically the expected result sets for the testRightOuterJoin1() and testLeftOuterJoin2() test cases in EquiJoinConditionTest are invalid, as shown below:

* testRightOuterJoin1() result set {{null, n1}, {n1, n2}, {n2, n2}} --> The n1 node does not have the propertyName2 property set, so the first tuple can never occur regardless of the join type. And since n2 already matches existing nodes, even {null, n2} can not be included in the result set. The correct result set for this query seems to be {{n1, n2}, {n2, n2}}.

* testLeftOuterJoin2() result set {{n1, null}, {n2, n1}, {n2, n2}} --> Same as above, a tuple with n1 as the leftmost node is not possible. The correct result set would be {{n2, n1}, {n2, n2}}.

Unfortunately the correct result sets here don't actually exercise the outer join functionality, i.e. none of the nodes in the returned tuples are null. We'll need to modify the test case setup to fix this."
0,Change contrib tests to use the special LuceneTestCase(J4) constant for the current version used a matchVersion parameter. Sub issue for contrib changes
0,"StopFilter should not create a new CharArraySet if the given set is already an instance of CharArraySet. With LUCENE-2094 a new CharArraySet is created no matter what type of set is passed to StopFilter. This does not behave as  documented and could introduce serious performance problems. Yet, according to the javadoc, the instance of CharArraySet should be passed to CharArraySet.copy (which is very fast for CharArraySet instances) instead of ""copied"" via ""new CharArraySet()"""
0,"Minor spi2dav ExceptionConverter improvements. It would be nice if the ExceptionConverter class in spi2dav returned UnsupportedRepositoryOperationExceptions instead of the undeclared UnsupportedOperationExceptions for HTTP 501 responses.

Besides that, the ExceptionConverter.generate() methods should be cleaned to always return the generated exception instead of in some cases returning and in others throwing it.

Finally, there's some unused code and chances for Java 5 cleanups.

I'll attach a patch, and commit it unless anyone objects."
0,"contrib/javascript is not packaged into releases. the contrib/javascript directory is (apparently) a collection of javascript utilities for lucene .. but it has not build files or any mechanism to package it, so it is excluded form releases.

"
0,"Inefficient growth of OpenBitSet. Hi, I found a potentially serious efficiency problem with OpenBitSet.

One typical (I think) way to build a bit set is to set() the bits one by one -
e.g., have a HitCollector set() the bit for each matching document.
The underlying array of longs needs to grow as more as more bits are set, of
course.

But looking at the code, it appears to me that the array grows very
ineefficiently - in the worst case (when doc ids are sorted, as they would
normally be in the HitCollector case for example), copying the array again
and again for every added bit... The relevant code in OpenBitSet.java is:

  public void set(long index) {
    int wordNum = expandingWordNum(index);
    ...
  }

  protected int expandingWordNum(long index) {
    int wordNum = (int)(index >> 6);
    if (wordNum>=wlen) {
      ensureCapacity(index+1);
    ...
  }
  public void ensureCapacityWords(int numWords) {
    if (bits.length < numWords) {
      long[] newBits = new long[numWords];
      System.arraycopy(bits,0,newBits,0,wlen);
      bits = newBits;
    }
  }

As you can see, if the bits array is not long enough, a new one is
allocated at exactly the right size - and in the worst case it can grow
just one word every time...

Shouldn't the growth be more exponential in nature, e.g., grow to the maximum
of index+1 and twice the existing size?

Alternatively, if the growth is so inefficient, this should be documented,
and it should be recommended to use the variant of the constructor with the
correct initial size (e.g., in the HitCollector case, the number of documents
in the index). and the fastSet() method instead of set().

Thanks,
Nadav.
"
0,"Restructure codec hierarchy. Spinoff of LUCENE-2621. (Hoping we can do some of the renaming etc here in a rote way to make progress).

Currently Codec.java only represents a portion of the index, but there are other parts of the index 
(stored fields, term vectors, fieldinfos, ...) that we want under codec control. There is also some 
inconsistency about what a Codec is currently, for example Memory and Pulsing are really just 
PostingsFormats, you might just apply them to a specific field. On the other hand, PreFlex actually
is a Codec: it represents the Lucene 3.x index format (just not all parts yet). I imagine we would
like SimpleText to be the same way.

So, I propose restructuring the classes so that we have something like:
* CodecProvider <-- dead, replaced by java ServiceProvider mechanism. All indexes are 'readable' if codecs are in classpath.
* Codec <-- represents the index format (PostingsFormat + FieldsFormat + ...)
* PostingsFormat: this is what Codec controls today, and Codec will return one of these for a field.
* FieldsFormat: Stored Fields + Term Vectors + FieldInfos?

I think for PreFlex, it doesnt make sense to expose its PostingsFormat as a 'public' class, because preflex
can never be per-field so there is no use in allowing you to configure PreFlex for a specific field.
Similarly, I think in the future we should do the same thing for SimpleText. Nobody needs SimpleText for production, it should
just be a Codec where we try to make as much of the index as plain text and simple as possible for debugging/learning/etc.
So we don't need to expose its PostingsFormat. On the other hand, I don't think we need Pulsing or Memory codecs,
because its pretty silly to make your entire index use one of their PostingsFormats. To parallel with analysis:
PostingsFormat is like Tokenizer and Codec is like Analyzer, and we don't need Analyzers to ""show off"" every Tokenizer.

we can also move the baked in PerFieldCodecWrapper out (it would basically be PerFieldPostingsFormat). Privately it would
write the ids to the file like it does today. in the future, all 3.x hairy backwards code would move to PreflexCodec. 
SimpleTextCodec would get a plain text fieldinfos impl, etc."
0,"Root exception not logged in ClusterNode for ClusterException. When our MySQL server is down or failed queries we have the following log :
ERROR (ClusterNode-node1) [org.apache.jackrabbit.core.cluster.ClusterNode] Periodic sync of journal failed: Unable to return record iterater.

So the root exception (SQLException in my case) is missing from the log and this prevent me from quickly finding the reason."
0,"Upgrade to latest SLF4J and Logback. While fixing JCR-2836 I ran into LBCLASSIC-183 [1] that's fixed in a recent Logback release. To get this and other fixes we should upgrade Logback and SLF4J in Jackrabbit 2.3.

[1] http://jira.qos.ch/browse/LBCLASSIC-183"
0,"Add search timeout support to Lucene. This patch is based on Nutch-308. 

This patch adds support for a maximum search time limit. After this time is exceeded, the search thread is stopped, partial results (if any) are returned and the total number of results is estimated.

This patch tries to minimize the overhead related to time-keeping by using a version of safe unsynchronized timer.

This was also discussed in an e-mail thread.
http://www.nabble.com/search-timeout-tf3410206.html#a9501029"
0,Expose namespace registry via workspace instead via session in spi2jcr. spi2jcr/SessionInfoImpl.getNamespaceResolver() returns the namespace registry through the current session of the wrapped repository. Since session scoped namespace remapping is not visible to the SPI I think the method should return the namespace registry through the current workspace. 
0,"HttpClient throws java.net.SocketException instead of org.apache.http.conn.ConnectionTimeoutException when connection timeout occurs. When sending an http request a connection timeout occurs, the HttpClient.execute method throws a java.net.SocketException instead of a org.apache.http.conn.ConnectionTimeoutException.

java.net.SocketTimeoutException: connect timed out
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333)
        at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195)
        at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
        at java.net.Socket.connect(Socket.java:519)
        at org.apache.http.conn.scheme.PlainSocketFactory.connectSocket(PlainSocketFactory.java:119)
        at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:129)
        at org.apache.http.impl.conn.AbstractPoolEntry.open(AbstractPoolEntry.java:164)
        at org.apache.http.impl.conn.AbstractPooledConnAdapter.open(AbstractPooledConnAdapter.java:119)
        at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:349)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:555)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:487)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:465)
"
0,"SPI: RepositoryService.getItemInfos should be allowed to return entries outside of the requested tree.. michael duerig asked for for that extension of the semantic of RepositoryService.getItemInfos.
currently this doesn't work and leads to an inconsistent hierarchy."
0,"JCR unit tests use invalid queries. According to Section 8.5.2.11 of the JCR 1.0 specification:

    It is optional to support properties in the SELECT, WHERE and ORDER BY clauses that are not explicitly
    defined in the node types listed in the FROM clause but which are defined in subtypes of those node types.

    It is optional to support the specifying of properties in the SELECT,WHERE and ORDERBY clauses that 
    are not explicitly defined in the node types listed in the FROM clause but which are defined in mixin 
    node types that may be assigned to node instances of the types that are mentioned in the SELECT clause.

However, two of the test methods in the org.apache.jackrabbit.test.api.query.SQLJoinTest class are producing and executing queries that use in the WHERE clause different properties and node types than those listed in the FROM clause.  The testJoinNtBase() method is producing a query using the following code:

        StringBuffer query = new StringBuffer(""SELECT * FROM "");
        query.append(ntBase).append("", "").append(testMixin);
        query.append("" WHERE "");
        query.append(testNodeType).append(""."").append(jcrPath);
        query.append("" = "");
        query.append(mixReferenceable).append(""."").append(jcrPath);
        query.append("" AND "").append(jcrPath).append("" LIKE "");
        query.append(""'"").append(testRoot).append(""/%'"");

This code will produce a valid query only when ""testNodeType"" is set to ""nt:base"" and ""testMixin"" is set to ""mix:referenceable"":

    SELECT * FROM nt:base, mix:referenceable 
    WHERE nt:base.jcr:path = mix:referenceable.jcr:path AND ...

However, when any other values for ""testNodeType"" and ""testMixin"" are used, this produces an invalid query in which the WHERE criteria references tuple sources that do not exist in the FROM clause.  For example, when ""testNodeType"" is ""my:type"" and ""testMixin"" is ""my:mixin"", the query becomes:

    SELECT * FROM nt:base, my:mixin 
    WHERE my:type.jcr:path = mix:referenceable.jcr:path AND ...

This code can be corrected by simply using the ""testNodeType"" in the FROM clause.

A similar bug is in the testJoinFilterPrimaryType() method, which uses this code:

        StringBuffer query = new StringBuffer(""SELECT * FROM "");
        query.append(testNodeType).append("", "").append(ntBase);
        query.append("" WHERE "");
        query.append(testNodeType).append(""."").append(jcrPath);
        query.append("" = "");
        query.append(mixReferenceable).append(""."").append(jcrPath);
        query.append("" AND "").append(jcrPath).append("" LIKE "");
        query.append(""'"").append(testRoot).append(""/%'"");

This code will really never produce a valid query, since the FROM clause uses the ""testNodeType"" and ""nt:base"" node types, whereas the WHERE clause will use the ""testNodeType"" and ""mix:referenceable"" types.  For example, if ""testNodeType"" has a value of ""my:type"", the query becomes:

    SELECT * FROM my:type, nt:base 
    WHERE my:type.jcr:path = mix:referenceable.jcr:path AND ...
"
0,"Provide general HTTP date parsing. Add generally accessible support for parsing HTTP dates as used in headers/cookies.

Initially submitted to HttpClient dev by Chris Brown."
0,Add the ability to disable the content-type and transfer encoding headers for Parts.  
0,"[patch] add toString for NodeImpl and PropertyImpl. add toString for NodeImpl and PropertyImpl with new format. see how it is liked, before adding more."
0,"Add ""testpackage"" to common-build.xml. One can define ""testcase"" to execute just one test class, which is convenient. However, I didn't notice any equivalent for testing a whole package. I find it convenient to be able to test packages rather than test cases because often it is not so clear which test class to run.

Following patch allows one to ""ant test -Dtestpackage=search"" (for example) and run all tests under the \*/search/\* packages in core, contrib and tags, or do ""ant test-core -Dtestpackage=search"" and execute similarly just for core, or do ""ant test-core -Dtestpacakge=lucene/search/function"" and run all the tests under \*/lucene/search/function/\* (just in case there is another o.a.l.something.search.function package out there which we want to exclude."
0,"Thread safety and visibility Improvements. AbstractAuthenticationHandler.DEFAULT_SCHEME_PRIORITY is not protected against external changes.

Although the field is private, subclasses can obtain a reference to it and so may be able to change it.

Consider making the list read-only, or returning a copy instead."
0,"Separate javadocs for core and contribs. A while ago we had a discussion on java-dev about separating the javadocs
for the contrib modules instead of having only one big javadoc containing 
the core and contrib classes.

This patch:
* Adds new targets to build.xml: 
  ** ""javadocs-all"" Generates Javadocs for the core, demo, and contrib 
    classes
  ** ""javadocs-core"" Generates Javadocs for the core classes
  ** ""javadocs-demo"" Generates Javadocs for the demo classes
  ** ""javadocs-contrib"" Using contrib-crawl it generates the Javadocs for 
    all contrib modules, except ""similarity"" (currently empty) and gdata.
* Adds submenues to the Javadocs link on the Lucene site with links to
  the different javadocs
* Includes the javadocs in the maven artifacts

Remarks:
- I removed the ant target ""javadocs-internal"", because I didn't want to
  add corresponding targets for all new javadocs target. Instead I 
  defined a new property ""javadoc.access"", so now  
  ""ant -Djavadoc.access=package"" can be used in combination with any of
  the javadocs targets. Is this ok?
- I didn't include gdata (yet) because it uses build files that don't 
  extend Lucenes standard build files.
  
Here's a preview:
http://people.apache.org/~buschmi/site-preview/index.html

Please let me know what you think about these changes!"
0,"Add UserManager.createGroup(String groupID) method. As discussed on the dev list [1] I think it would be useful (and consistent inside the API) to have a UserManager.createGroup(String groupID) method.

The specification of the method would be:

    /**
     * Creates a Group for the given groupID must not be <code>null</code>.
     * <br>
     * Same as {@link #createGroup(Principal,String)} where the specified groupID
     * is the name of a simple <code>Principal</code> implementation and the
     * intermediate path is <code>null</code>.
     *
     * @param groupID The id of the new group, must not be <code>null</code>.
     * @return The new <code>Group</code>.
     * @throws AuthorizableExistsException in case the given groupID is already
     * in use or another {@link Authorizable} with the same
     * {@link Authorizable#getID() ID} exists.
     * @throws RepositoryException If another error occurs.
     */
    Group createGroup(String groupID) throws AuthorizableExistsException, RepositoryException;

[1] http://markmail.org/message/rjofzg4t3kiht7xv"
0,"Thread starvation problems in some tests. In some of the tests, a time limit is set and the tests have a ""while (inTime)"" loop. If creation of thread under heavy load is too slow, the tasks are not done. Most tests are only useful, if the task is at least done once (most would even fail).

This thread changes the loops to be do...while, so the task is run at least one time."
0,JSR 283: Activities. 
0,"HTTP Client doesn't support multipart/related content-type. It is not possible to sent data easely as a multipart/related content-type (as 
discribed in rfc 2387) using Http Client."
0,"Refactor ObservationManagerFactory. The current o.a.j.core.observation.ObservationManagerFactory class has two main responsibilities:

    1) Create new ObservationManagerImpl instances as an observation manager factory
    2) Manage event consumers and dispatch events within a workspace

These two responsibilities are quite unrelated and the factory responsibility essentially boils down to the following method that is only ever invoked within WorkspaceImpl.getObservationManager():

    public ObservationManagerImpl createObservationManager(SessionImpl session, ItemManager itemMgr) {
        return new ObservationManagerImpl(this, session, itemMgr);
    }

To simplify the design I'd inline this method and rename ObservationManagerFactory to ObservationDispatcher to better reflect the one remaining responsibility."
0,"Misleading exception message when re-index fails. E.g. the log may say:

19.06.2007 11:25:42 *ERROR* RepositoryImpl: Failed to initialize workspace 'default' (RepositoryImpl.java, line 382)
javax.jcr.RepositoryException: Error indexing root node: 10022d38-c449-4751-b8f0-9d07ac45ead5:
[...]

The mentioned uuid is not the root node and the root cause is missing."
0,"JCR2SPI: several performance improvements pointed out by Findbugs. FindBug report:

M P Bx: Method org.apache.jackrabbit.jcr2spi.nodetype.BitsetENTCacheImpl.getBitNumber(QName) invokes inefficient Integer(int) constructor; use Integer.valueOf(int) instead	src/main/java/org/apache/jackrabbit/jcr2spi/nodetype	BitsetENTCacheImpl.java	line 177	1190981544656	1666284
M P Bx: Method org.apache.jackrabbit.jcr2spi.query.RowIteratorImpl$RowImpl.getValue(String) invokes inefficient Integer(int) constructor; use Integer.valueOf(int) instead	src/main/java/org/apache/jackrabbit/jcr2spi/query	RowIteratorImpl.java	line 247	1190981544671	1666292
M P Bx: Method org.apache.jackrabbit.jcr2spi.WorkspaceManager.onEventReceived(EventBundle[], InternalEventListener[]) invokes inefficient Integer(int) constructor; use Integer.valueOf(int) instead	src/main/java/org/apache/jackrabbit/jcr2spi	WorkspaceManager.java	line 616	1190981544640	1666279
M P WMI: Method org.apache.jackrabbit.jcr2spi.name.NamespaceCache.syncNamespaces(Map) makes inefficient use of keySet iterator instead of entrySet iterator	src/main/java/org/apache/jackrabbit/jcr2spi/name	NamespaceCache.java	line 193	1190981544656	1666283
M P WMI: Method org.apache.jackrabbit.jcr2spi.nodetype.NodeTypeRegistryImpl.internalRegister(Map) makes inefficient use of keySet iterator instead of entrySet iterator	src/main/java/org/apache/jackrabbit/jcr2spi/nodetype	NodeTypeRegistryImpl.java	line 524	1190981544656	1666285
M P WMI: Method org.apache.jackrabbit.jcr2spi.observation.ObservationManagerImpl.onEvent(EventBundle) makes inefficient use of keySet iterator instead of entrySet iterator	src/main/java/org/apache/jackrabbit/jcr2spi/observation	ObservationManagerImpl.java	line 189	1190981544656	1666286
M P WMI: Method org.apache.jackrabbit.jcr2spi.state.NodeState.persisted(ChangeLog) makes inefficient use of keySet iterator instead of entrySet iterator	src/main/java/org/apache/jackrabbit/jcr2spi/state	NodeState.java	line 275	1190981544671	1666297
"
0,"Token implementation needs improvements. This was discussed in the thread (not sure which place is best to reference so here are two):
http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200805.mbox/%3C21F67CC2-EBB4-48A0-894E-FBA4AECC0D50@gmail.com%3E
or to see it all at once:
http://www.gossamer-threads.com/lists/lucene/java-dev/62851

Issues:
1. JavaDoc is insufficient, leading one to read the code to figure out how to use the class.
2. Deprecations are incomplete. The constructors that take String as an argument and the methods that take and/or return String should *all* be deprecated.
3. The allocation policy is too aggressive. With large tokens the resulting buffer can be over-allocated. A less aggressive algorithm would be better. In the thread, the Python example is good as it is computationally simple.
4. The parts of the code that currently use Token's deprecated methods can be upgraded now rather than waiting for 3.0. As it stands, filter chains that alternate between char[] and String are sub-optimal. Currently, it is used in core by Query classes. The rest are in contrib, mostly in analyzers.
5. Some internal optimizations can be done with regard to char[] allocation.
6. TokenStream has next() and next(Token), next() should be deprecated, so that reuse is maximized and descendant classes should be rewritten to over-ride next(Token)
7. Tokens are often stored as a String in a Term. It would be good to add constructors that took a Token. This would simplify the use of the two together.
"
0,"Allow query results with unknown size. To further optimize certain queries the query implementation should be changed to allow for unknown result sizes. Currently there is only one query ( //* ) where the query result returns an unknown size and a special query result implementation is returned. At the same time, this should be fixed that only one implementation is used."
0,"Use an enumeration for QOM operators. The PFD version of QueryObjectModelConstants contains some incorrect constant values that make it unusable as a source of operator constants.

Since we are now using Java 5, I propose that instead of adding our own replacement constant strings, we implement a type-safe Operator enumeration that contains fixed versions of all the operator constants declared in QueryObjectModelConstants."
0,"QueryHandler should use lucene Input-/OutputStream implementations. Currently the QueryHandler uses a jackrabbit specific implementation of the lucene Directory interface to make use of the jackrabbit FileSystem abstraction. Lucene operations on the file system however requires quite often random access on the index files. With the current FileSystem interface / abstraction random access is not possible on a FileSystemResource, therefore it is simulated by re-aquiring the InputStream and then seeking to the desired position. This it not efficient at all.

With respect to performance any other use than file based index storage does not make sense with lucene. Hence, the current abstraction using FileSystem should be dropped in favour of direct file access."
0,"replace the PostMethod parameters HashMap with a List. Propose to change the parameters datastructure in PostMethod from its
current HashMap to an ArrayList (or Vector) of NameValuePair objects. 
This change would lead to simpler and more robust code in the PostMethod
with more deterministic behaviour for the following reasons:

1) HashMap looses any insertion order.  If the client wants to have the
encoded parameters to show up in a particular order, they are unable.

2) Hash map requres unique keys where there is no reason that multiple
parameters with the same name cannot be POSTed.  The current
implementation replaces the string value with a List if there is a
addParameter with a repeated name key.  Every get from the HashMap then
has to do an instanceOf to see if the value is a String or a List.

3) Hash maps are no faster than a Vector for typical operations.  They
both have O(1) insertions and both do O(n) removal operations to
genPropose to change the parameters datastructure in PostMethod from its
current HashMap to an ArrayList (or Vector) of NameValuePair objects. 
This change would lead to simpler and more robust code in the PostMethod
with more deterministic behaviour for the following reasons:

1) HashMap looses any insertion order.  If the client wants to have the
encoded parameters to show up in a particular order, they are unable.

2) Hash map requres unique keys where there is no reason that multiple
parameters with the same name cannot be POSTed.  The current
implementation replaces the string value with a List if there is a
addParameter with a repeated name key.  Every get from the HashMap then
has to do an instanceOf to see if the value is a String or a List.

3) Hash maps are no faster than a Vector for typical operations.  They
both have O(1) insertions and both do O(n) removal operations to
generate the body.  HashMap is only faster when a search is required,
such as for getParameter(String), setParameter(String, String) and
removeParameter(String) which should rarely be called.

I would also move to depricate setParameter(String, String) as it is
confusing in the API.  (setParameter overwrites any existing parameter
of the same name where addParameter accumulates to the list of
parameters).  The setParameter functionality can also be effected by
calling removeParameter then addParameter already.

erate the body.  HashMap is only faster when a search is required,
such as for getParameter(String), setParameter(String, String) and
removeParameter(String) which should rarely be called.

I would also move to depricate setParameter(String, String) as it is
confusing in the API.  (setParameter overwrites any existing parameter
of the same name where addParameter accumulates to the list of
parameters).  The setParameter functionality can also be effected by
calling removeParameter then addParameter already."
0,"Extend FieldCache architecture to multiple Values. I would consider this a bug. It appears lots of people are working around this limitation, 
why don't we just change the underlying data structures to natively support multiValued fields in the FieldCache architecture?

Then functions() will work properly, and we can do things like easily geodist() on a multiValued field.

Thoughts?"
0,"TCK: SetValueDateTest compares Calendar objects. SetValueDateTest#testDateSession
SetValueDateTest#testDateParent

Tests compare Calendar objects.  Calendar.equals(Object) is a stronger test than JSR-170 specifies for Value.equals(Object), leading to false failures.  For the purpose of these tests, even Value.equals(Object) is too strong an equality test, since some repositories may normalize date/time values across a save/read roundtrip (for example, converting ""Z"" to ""+00:00"", or adding/removing trailing zeros in fractional seconds).

Proposal: compare the getTimeInMillis() values.

--- SetValueDateTest.java       (revision 422074)
+++ SetValueDateTest.java       (working copy)
@@ -79,7 +80,8 @@
     public void testDateSession() throws RepositoryException {
         property1.setValue(value);
         superuser.save();
-        assertEquals(""Date node property not saved"", value.getDate(), property1.getValue().getDate());
+        assertEquals(""Date node property not saved"",
+          value.getDate().getTimeInMillis(), property1.getDate().getTimeInMillis());
     }
  
     /**
@@ -89,7 +91,8 @@
     public void testDateParent() throws RepositoryException {
         property1.setValue(value.getDate());
         node.save();
-        assertEquals(""Date node property not saved"", value.getDate(), property1.getValue().getDate());
+        assertEquals(""Date node property not saved"",
+          value.getDate().getTimeInMillis(), property1.getDate().getTimeInMillis());
     }
"
0,"Surround query language. This is a copy of what I posted about a year ago. 
 
The whole thing is hereby licenced under the Apache Licence 2.0, 
copyright 2005 Apache Software Foundation. 
 
For inclusion in Lucene (sandbox perhaps?) it will need 
at least the following adaptations: 
- renaming of package names 
  (org.surround to somewhere org.apache.lucene ) 
- moves of the source files to corresponding directories 
 
Although it uses the identifier sncf in some places 
I'm not associated with French railroads, but I like the TGV. 
 
Regards, 
Paul Elschot"
0,"build.xml: result of ""dist-src"" should support ""build-contrib"". Currently the packed src distribution would fail to run ""ant build-contrib"".
It would be much nicer if that work.
In fact, would be nicer if you could even ""re-pack"" with it.

For now I marked this for 2.1, although I am not yet sure if this is a stopper."
0,"Put everything in jackrabbit-spi-commons under org.apache.jackrabbit.spi.commons. To avoid confusion and naming conflicts, we should put all classes and packages in jackrabbit-spi-commons under org.apache.jackrabbit.spi.commons."
0,"Add automatic default configuration. We should provide a simple way to start a Jackrabbit repository with default configuration. The current First Hops document exposes too much configuration details to be really friendly to first-time users.

I'd like to provide a default TransientRepository constructor that looks for ""repository.xml"" as the configuration file and ""repository"" as the repository home directory. If either of these does not exist, it is automatically created using default settings. This way the repository setup would boil down to:

    Repository repository = new TransientRepository();

As an added feature I'm planning to support system properties ""org.apache.jackrabbit.repository.conf"" and ""org.apache.jackrabbit.repository.home"" for overriding the defaults.

This improvement would make it easier to write and set up ""Hello, World!"" -type applications, thus helping interested people to try out Jackrabbit. This feature will also make it easier to provide a standard template for test classes that exhibit some error condition. Like this:

    import javax.jcr.*;
    import org.apache.jackrabbit.core.TransientRepository;
    public Example {
        public static void main(String[] args) {
            try {
                Repository repository = new TransientRepository();
                Session session = repository.login();
                try {
                    // YOUR CODE HERE
                } finally {
                    session.close();
                }
            } catch (Exception e) {
                e.printStackTRace();
            }
        }
    }

I'm targetting this for inclusion in 1.0 as it affects none of the existing code and it will probably be very helpful for the expected number of new users we are going to see after 1.0 is out."
0,"[SUBMISSION] Amazon S3 Persistence Manager Project. As I noted previously on the dev-list (http://markmail.org/search/?q=amazon+list%3Aorg.apache.jackrabbit.dev#query:amazon%20list%3Aorg.apache.jackrabbit.dev+page:1+mid:qw27gopsn4lnbde5+state:results) I have written an Amazon S3 bundle persistence manager for Jackrabbit. I want to submit the code for the sandbox, the full source is included in the zip file. Licensed under the ASF.

The project also aims to implement a normal persistence manager (which I abandoned in favor of the more efficient bundle pm, which is implemented, but does not work 100%), a file system impl for S3 (only rough structure present) and an SPI impl that connects to S3 (dreaming ;-)). For more infos, I will include the README.txt of the project here:

=================================================================
Welcome to Jackrabbit persistence for Amazon Webservices (ie. S3)
=================================================================

This module contains various persistence options for using
Amazon Webservices as backend for Jackrabbit / JCR. Amazon has
two persistence services: S3 (public) and SimpleDB (still beta).
The following options are available/ideas:

- (1) persistence managers that connects to S3
      (normal + bundle, in work, probably not very efficient)
      
- (2) persistence manager that connects to SimpleDB
      (NOT feasible)
      
- (3) SPI implementation that connects to S3
      (not implemented, very complicated, probably more efficient)
      
See details below and also TODO.txt


Installing / Testing
====================

This needs a patched Jackrabbit 1.3.x version. The patches can
be found in the directory ""patches-for-1.3"". One patch will modify
the pom of jackrabbit-core to generated the jackrabbit test jar
for reuse in this project. To build that customized version, you need
to do the following steps:

1) svn co http://svn.apache.org/repos/asf/jackrabbit/branches/1.3 jackrabbit-1.3
2) cd jackrabbit-1.3
3) apply all patches from the ""patches-for-1.3"" directory:
   patch -p0 < %JR-AMAZON-PATH%/patches-for-1.3/%PATCH%.patch
4) mvn install
5) cd %JR-AMAZON-PATH%
6) change jackrabbit version number in pom.xml to the one you just built
   (eg. project/parent/version = 1.3.4)
7) cp aws.properties.template aws.properties
8) enter your credentials in aws.properties
9) mvn test

For debugging, you can change the logging in applications/test/log4j.properties
and set up proxying (for monitoring the traffic with eg. tcp mon) in
applications/test/jets3t.properties.


Details about Implementations
=============================

(1) org.apache.jackrabbit.persistence.amazon.AmazonS3PersistenceManager

http://www.amazon.com/s3

Stores JCR Nodes and Properties inside S3 Objects. Uses UUID for Nodes and
UUID/Name for Properties as Object names. Node references are stored
via references/UUID.

Configuration parameters:

accessKey
    Amazon AWS access key (aka account user id) [required]

secretKey
    Amazon AWS secret key (aka account password) [required]
    
bucket
    Name of the S3 bucket to use [optional, default uses accessKey]
    Note that bucket names are global, so using your accessKey is
    recommended to prevent conflicts with other AWS users. 
    
objectPrefix
    Prefix used for all object names [optional, default is """"]
    Should include the workspace name (""${wsp.name}"" or ""version"" for
    the versioning PM) to put multiple workspaces into one bucket.

Example XML Config:

<PersistenceManager class=""org.apache.jackrabbit.persistence.amazon.AmazonS3PersistenceManager"">
    <param name=""accessKey""    value=""abcde01234""/>
    <param name=""secretKey""    value=""topsecret""/>
    <param name=""bucket""       value=""abcde01234.jcrstore""/>
    <param name=""objectPrefix"" value=""${wsp.name}/""/>
</PersistenceManager>

-----

(2) AmazonSimpleDBPersistenceManager

This is *not* feasible because of the restrictions that are applied
to SimpleDB. An item can only have up to 256 attributes, each attribute
can only contain a string value and that one can only have 1024 chars.
See this link for more information:

http://docs.amazonwebservices.com/AmazonSimpleDB/2007-11-07/DeveloperGuide/SDB_API_PutAttributes.html

-----

(3) org.apache.jackrabbit.spi2s3

TODO

lots of work...


About
=====

It was originally written by Alexander Klimetschek
(alexander.klimetschek at googlemail dot com) in 2008.

See the Apache Jackrabbit web site (http://jackrabbit.apache.org/)
for documentation and other information. You are welcome to join the
Jackrabbit mailing lists (http://jackrabbit.apache.org/mail-lists.html)
to discuss this component and to use the Jackrabbit issue tracker
(http://issues.apache.org/jira/browse/JCR) to report issues or request
new features.

Apache Jackrabbit is a project of the Apache Software Foundation
(http://www.apache.org).
"
0,"Move generic locking tests from jcr2spi to jackrabbit-jcr-tests. once we touch the jackrabbit-jcr-tests for the JSR 283 implementation, we should also move the generic tests from jackrabbit-jcr2spi to the overall test suite.

opening this issue as a reminder and as a marker issue.
angela"
0,A faster JFlex-based replacement for StandardAnalyzer. JFlex (http://www.jflex.de/) can be used to generate a faster (up to several times) replacement for StandardAnalyzer. Will add a patch and a simple benchmark code in a while.
0,"Use a pre-generated version of XPath.jjt. The workaround described in JCR-46 is still causing extra steps for Java 5 users. I'd like to solve this issue by including a pre-generated version of the XPath.jjt file as a normal source file. This will avoid the need for XSL transformations during normal builds and thus remove the need for the extra steps.

I'll create a patch for this and unless anyone objects, I'm planning to include it in the 1.0 branch as well as the svn trunk."
0,"Lazy initialize ItemDefinition. The item definition is currently set immediately when an ItemData is instantiated. Accessing nodes usually does not require reading the item definition, thus it is not necessary to load/set it that early.

Lazy initialization also has the benefit that content migration in an upgrade scenario becomes easier. Instead of throwing an exception early, jackrabbit could allow access to the item until an item definition is really required for the operation."
0,"Remove deprecated classes in jcr-commons. There are many classes in jackrabbit-jcr-commons that we've replaced with better alternatives in spi-commons.

I'd like to clean things up by removing the deprecated versions from jcr-commons now that we're upgrading to Jackrabbit 2.0"
0,"Contribute Pluggable Permission and User Management to Jackrabbit. Working with a Jackrabbit based appliction I had to extend its security handling.
The aim of this extension has been to allow for a eitable resource based authorization.
The solution ended up in beeing plugable and extendable.
As there have been some questions in the Jackrabbit Developper-list about custom implementation of security or the management of privileges in Jackrabbit, I like to suggest my implementation as contribution with attached patches. 

Below you can find some high-level explanation of the contained files and concepts

I hope the prove to be usable and enhance this great repository.
I welcome your feed-back and like to thank for your kind inspection

Regards
Christian Keller

The patch contains the following:
=========================

1) API [jackrabbit-core-changes.20071010.patch]
-------------------------------------------------------------------
API which allows to implement and configure a mechanisms for Authentication and Authorization. 
The API is ACL- and Principal-based.
ACL and Principals Management is independent of the JCR api, to allow implementations to use different back-end systems like a Directory Server.

2) Changes to current core [jackrabbit-core-changes.20071010.patch]
-----------------------------------------------------------------------------------------------
Some small changes have been necessary to core to enable configuration and access of Management, like session access to UserManager.

3) Implementation [jackrabbit-core-implementation.20071010.patch]
-----------------------------------------------------------------------------------------------
Additionally an implemenation is contained. It is not dependent on any back-end system, and may therefore be used as a default.


Description:
==========
The extensions hook into Jackrabbit bei implementations of the Intefaces: AccessManager and LoginModule. 
Additionally there are changes for configuration, set-up and access of the used Object.

The patch extends the API, in order to allow client inspections of Users and Permission. These are contained in the api.patch

See a short Introduction below:
=========================

The Security extensions of this Patch contain both, Authentication and Authorization extensions for which the follwoing two modells are introduced:

I) The Authorizable
----------------------------
These are User's and Groups of Users. Users can authenticate. 
Authentication in Jackrabbit is done by LoginModules which issue Principals as result of an Authentication.
The Users are the objects which can be represented by such an Principal
They are therfore are the base for the Authorization.

II) The ACL
----------------
The ACL is the Policy for Authorziation. 
The ACL grants or denies a Principal Privileges which are called Actions.

Additional ther is a Management for Principals:

The Principal is the link between User and permission.
A User may related to multiple Principals. As this dependes on the LoginModules verfiying the Idendity of the login-attemp.
The LoginModules may expose their Principals to the Repository via a Provider interface, to allow for usage in ACEs.

All Modells and their Managing Classes API's are abstracted from the fact, that they are used in a JC-Repository. Aka there is no reference to javax.jcr.Items, Sessions etc.
This should allow to implement both for external sources for both without imposing any JCR specific methods. Taken an LDAP as UserBase for example.

The managing classes are UserManger, PrincpalManager and ACLManager. 
They are set-up and maintained by a repsoitory singular SecurityManger. 
Session specific versions of this Managers are exposed via Session.

PrincipalManger and ACLManger are feed by one to multiple Providers. 
PrincipalProviders may exist per LoginModule, ACLProvider per Workspace.

Authentication:
--------------------
The User will be used by the LoginModule. It will be resolved based on the given Credentials. If the Credentials can be validated, the User will be used to resolve Principals according its Group-Membership. As a result the Session's Subject will be extended by this principals.

Authorization:
-------------------
The ACL will be use be an Implementation of the AccessManager-Interface
An ACLManger relates Items to ACLs and the ACL evaluates the Permission for the current Subject's Principals.

Default Implementation
===================
The Default Implementation uses the Repository itself to store its security data.
The Users are stored within a dedicated workspace. 
The ACL are attached to the Nodes they relate to.
The ACLs are inherited along the Item-Hierarchy.
The Principals are taken from the Authorables.

Configuration
===========
The LoginModules may declare their PrincipalProvider class via a property key with the name ""principal_provider.class""

The Workspace specific ACL Providers may be added via a configuration element in Worskspace.xml, called WorkspaceSecurity.
A Factory class can be configured there."
0,"Test failures when running ""mvn cobertura:check"". It looks like the bytecode instrumentation done by Cobertura interferes with the rather complex XPathTokenManager class produced by JavaCC.

The easiest workaround seems to be to simply exclude XPathTokenManager from being instrumented by Cobertura."
0,"Cookie rejected. Hello,

I'm using HttpClient 1.0 rc2 to login in the SourceForge website and perform
some operations, but i'm getting the following error:

Page 1 from https://sourceforge.net/account/login.php
6 dc. 2003 23:45:27 org.apache.commons.httpclient.HttpMethodBase
processResponseHeaders
ATTENTION: Cookie rejected: ""username=l6qpwtK5hpE%3D"". Illegal domain attribute
"".sourceforge.net"". Domain of origin: ""sourceforge.net""
6 dc. 2003 23:45:27 org.apache.commons.httpclient.HttpMethodBase readResponseBody

The cookie returned by Sourceforge is rejected because the domain for the
request was sourceforge.net and the domain for the cookie was .sourceforge.net
I have tried to change the cookie policy to all available options, but none work. 
CookiePolicy.setDefaultPolicy(CookiePolicy.COMPATIBILITY);

What can i do?

Thanks,
Ludovic"
0,"LuceneTestCase.afterClass does not print enough information if a temp-test-dir fails to delete. I've hit an exception from LTC.afterClass when _TestUtil.rmDir failed (on write.lock, as if some test did not release resources). However, I had no idea which test caused that (i.e. opened the temp directory and did not release resources).

I think we should do the following:
* Track in LTC a map from dirName -> StackTraceElement
* In afterClass if _TestUtil.rmDir fails, print the STE of that particular dir, so we know where was this directory created from
* Make tempDirs private and create accessor method, so that we control the inserts to this map (today the Set is updated by LTC, _TestUtils and TestBackwards !)"
0,"Simplify the usage of OCM annotations. If we are using more reflections during the OCM init phase (class descriptor loading), some OCM annotation settings are not necessary : 

@Node(isAbtract=true) : used to specify an abstract classes
@Node(extend=....) : used to specify the ancestor class
@Node(isInterface= ...) : used to specify the entity as an interface
@implement  : used to specify the associated interfaces

If this refactoring is done, we can set them as deprecated.

The performances will not suffer because this is done only once during the application startup (when the ObjectContentManager is initialized). "
0,"o.a.l.messages should be moved to core. contrib/queryParser contains an org.apache.lucene.messages package containing some generallized code that (claims in it's javadocs) is not specific to the queryParser.

If this is truely general purpose code, it should probably be moved out of hte queryParser contrib -- either into it's own contrib, or into the core (it's very small)

*EDIT:* alternate suggestion to rename package to fall under the o.a.l.queryParser namespace retracted due to comments in favor of (eventually) promoting to it's own contrib"
0,"reorder arguments of Field constructor to be more intuitive. I think Field should take (name, value, type) not (name, type, value) ?

This seems more intuitive and consistent with previous releases

Take this change to some code I had for example:
{noformat}
-    d1.add(new Field(""foo"", ""bar"", Field.Store.YES, Field.Index.ANALYZED));
+    d1.add(new Field(""foo"", TextField.TYPE_STORED, ""bar""));
{noformat}

I think it would be better if it was
{noformat}
document.add(new Field(""foo"", ""bar"", TextField.TYPE_STORED));
{noformat}"
0,"Further updates to the site scoring page. update the site scoring page - see Appendix:
{quote}
Class Diagrams
Karl Wettin's UML on the Wiki
{quote}
Karl's diagrams are outdated - I think this link should be pulled for 2.9

{quote}
Sequence Diagrams
FILL IN HERE. Volunteers?
{quote}
I think this should be pulled - I say put something like this as a task in JIRA - not the published site docs."
0,"improved compound file handling. Currently CompoundFileReader could use some improvements, i see the following problems
* its CSIndexInput extends bufferedindexinput, which is stupid for directories like mmap.
* it seeks on every readInternal
* its not possible for a directory to override or improve the handling of compound files.

for example: it seems if you were impl'ing this thing from scratch, you would just wrap the II directly (not extend BufferedIndexInput,
and add compound file offset X to seek() calls, and override length(). But of course, then you couldnt throw read past EOF always when you should,
as a user could read into the next file and be left unaware.

however, some directories could handle this better. for example MMapDirectory could return an indexinput that simply mmaps the 'slice' of the CFS file.
its underlying bytebuffer etc naturally does bounds checks already etc, so it wouldnt need to be buffered, not even needing to add any offsets to seek(),
as its position would just work.

So I think we should try to refactor this so that a Directory can customize how compound files are handled, the simplest 
case for the least code change would be to add this to Directory.java:

{code}
  public Directory openCompoundInput(String filename) {
    return new CompoundFileReader(this, filename);
  }
{code}

Because most code depends upon the fact compound files are implemented as a Directory and transparent. at least then a subclass could override...
but the 'recursion' is a little ugly... we could still label it expert+internal+experimental or whatever.
"
0,"Node.setProperty(String, ...) implementation not according to the specification. to illustrate the issue assume the following  property definition:

name: someText
type: String
non-mandatory
non-autocreate

the following call would throw a ConstraintViolationException
if the property doesn't exist yet:

node.setProperty(""someText"", 12345);

the rules used to find an applicable definition in this case are too strict."
0,"Build with JDK 1.4, get many javadoc warnings. Building httpclient ""dist"" ant target, I get lots of ""warning - The first
sentence is interpreted to be:"".

As the summary says, I get these warnings when I build using JDK 1.4.  Using JDK
1.3.1 yields far fewer problems.  I see this with the latest sources as of this
posting."
0,"Provide Readme's for subprojects jcr-mapping and jcr-nodemanagement. There need to be Readme files in each of the subprojects ""jcr-mapping"" and ""jcr-nodemanagement"" to provide some information about the scope of the subproject, building and hints on how to get started."
0,"First cut at column-stride fields (index values storage). I created an initial basic impl for storing ""index values"" (ie
column-stride value storage).  This is still a work in progress... but
the approach looks compelling.  I'm posting my current status/patch
here to get feedback/iterate, etc.

The code is standalone now, and lives under new package
oal.index.values (plus some util changes, refactorings) -- I have yet
to integrate into Lucene so eg you can mark that a given Field's value
should be stored into the index values, sorting will use these values
instead of field cache, etc.

It handles 3 types of values:

  * Six variants of byte[] per doc, all combinations of fixed vs
    variable length, and stored either ""straight"" (good for eg a
    ""title"" field), ""deref"" (good when many docs share the same value,
    but you won't do any sorting) or ""sorted"".

  * Integers (variable bit precision used as necessary, ie this can
    store byte/short/int/long, and all precisions in between)

  * Floats (4 or 8 byte precision)

String fields are stored as the UTF8 byte[].  This patch adds a
BytesRef, which does the same thing as flex's TermRef (we should merge
them).

This patch also adds basic initial impl of PackedInts (LUCENE-1990);
we can swap that out if/when we get a better impl.

This storage is dense (like field cache), so it's appropriate when the
field occurs in all/most docs.  It's just like field cache, except the
reading API is a get() method invocation, per document.

Next step is to do basic integration with Lucene, and then compare
sort performance of this vs field cache.

For the ""sort by String value"" case, I think RAM usage & GC load of
this index values API should be much better than field caache, since
it does not create object per document (instead shares big long[] and
byte[] across all docs), and because the values are stored in RAM as
their UTF8 bytes.

There are abstract Writer/Reader classes.  The current reader impls
are entirely RAM resident (like field cache), but the API is (I think)
agnostic, ie, one could make an MMAP impl instead.

I think this is the first baby step towards LUCENE-1231.  Ie, it
cannot yet update values, and the reading API is fully random-access
by docID (like field cache), not like a posting list, though I
do think we should add an iterator() api (to return flex's DocsEnum)
-- eg I think this would be a good way to track avg doc/field length
for BM25/lnu.ltc scoring.
"
0,"Initializing SeededSecureRandom may be slow. For systems where reading from /dev/random is very slow (so that the alternative seed algorithm is used), initializing the org.apache.jackrabbit.core.id.SeededSecureRandom singleton may be very slow, because it is not synchronized. Each thread that calls SeededSecureRandom.getInstance() will wait up to 1 second until the singleton is initialized.

At the same time, I would like to add more entropy to the alternative seed algorithm.
"
0,"[HttpClient] Authenticator() - ability to perform alternate authentication. My post to the user group.  The developer replied suggesting I enter an 
enhancement request.

-----Original Message-----
From: Gustafson, Vicki [mailto:vicki.gustafson@us.didata.com]
Sent: Thursday, 12 December 2002 5:03 AM
To: Jakarta Commons Users List
Subject: [HttpClient] Authentication using Basic

Is there a way to specify which authentication scheme you would like the client 
to use if several schemes are returned in the www-auth header?

I'm performing a simple post using the httpClient.  The server returns a 401 at 
which point the httpClient tries to authenticate with the server.  The 
following header is received:

Attempting to parse authenticate header: 'WWW-Authenticate: Negotiate, NTLM, 
Basic realm=""XXXwhateverXXX""

I need to authenticate using Basic, but the Authenticator class will only try 
the most secure scheme:  NTLM.  Is there a setting or parameter I can set to 
force the httpClient to use Basic?

thanks,
Vicki

// determine the most secure request header to add
Header requestHeader = null;
if (challengeMap.containsKey(""ntlm"")) {
    String challenge = (String) challengeMap.get(""ntlm"");
    requestHeader = Authenticator.ntlm(challenge, method, state,
    responseHeader);
} else if (challengeMap.containsKey(""digest"")) {
    String challenge = (String) challengeMap.get(""digest"");
    String realm = parseRealmFromChallenge(challenge);
    requestHeader = Authenticator.digest(realm, method, state,
    responseHeader);
} else if (challengeMap.containsKey(""basic"")) {
    String challenge = (String) challengeMap.get(""basic"");
    String realm = parseRealmFromChallenge(challenge);
    requestHeader = Authenticator.basic(realm, state, responseHeader);
} else if (challengeMap.size() == 0) {
    throw new HttpException(""No authentication scheme found in '""
    + authenticateHeader + ""'"");
} else {
    throw new UnsupportedOperationException(
    ""Requested authentication scheme "" + challengeMap.keySet()
    + "" is unsupported"");
}

--
To unsubscribe, e-mail:   <mailto:commons-user-unsubscribe@jakarta.apache.org>
For additional commands, e-mail: <mailto:commons-user-help@jakarta.apache.org>


--
To unsubscribe, e-mail:   <mailto:commons-user-unsubscribe@jakarta.apache.org>
For additional commands, e-mail: <mailto:commons-user-help@jakarta.apache.org>

**********developer response**********************************



Currently there isn't, however we probably should be more intelligent about 
falling back to other authentication schemes based on the type of credentials 
provided.  Having said this I'm not sure it conforms to the HTTP spec strictly 
(which states that the client must use the strongest authentication scheme it 
supports, there's a grey area here because if your application doesn't provide 
a dialog or similar for the user to enter NTLM credentials it can only support 
basic or digest authentication, despite HTTPClient supporting NTLM).

What I'd like to see happen is:

When NTLM authentication is requested as top priority but only 
UsernamePasswordCredentials are available instead of NTLMCredentials we fall 
back to one of the other schemes.  In general this would mean that:

if an authentication scheme is requested and a credentials object of the wrong 
type is provided, HTTPClient should assume (probably optionally or only in non-
strict mode) that the requested authentication scheme is not supported and fall 
through to other options.

Achieving this would require a reasonably amount of refactoring of the 
Authenticator class but shouldn't be impossible.  Unfortunately I don't have 
time to do it myself at the moment but I'd be happy to help out if you felt 
like doing it, otherwise logging an enhancement bug in Bugzilla would be a good 
way to record this request until someone has time to actually implement it.

Adrian Sutton, Software Engineer
Ephox Corporation
www.ephox.com"
0,"Cleanup DR.getCurrentVersion/DR.getUserData/DR.getIndexCommit().getUserData(). Spinoff from Ryan's dev thread ""DR.getCommitUserData() vs DR.getIndexCommit().getUserData()""... these methods are confusing/dups right now."
0,"Implement getDistance() on DirectSpellChecker.INTERNAL_LEVENSHTEIN. DirectSpellChecker.INTERNAL_LEVENSHTEIN is currently not a full-fledged implementation of StringDistance.  But an full implementation is needed for Solr's SpellCheckComponent.finishStage(), and also would be helpful for those trying to take the advice given in LIA 2nd ed section sect8.5.3."
0,"Implement a way to override or resolve DNS entries defined in the OS. When working with HttpClient in restrictive environments, where the user doesn't have the permissisions to edit the local /etc/hosts file or the DNS configuration, can be eased with an DNS Overrider capability. 

This can be useful with JMeter which can follow redirects automatically and resolve some of the redirected hosts against its configuration. Another example is a custom forward proxy, written in Java and based on httpclient, which can be deployed is such a restricted environment that would ease the development of various web solutions for some developers. "
0,"Performance improvement for TermInfosReader. Currently we have a bottleneck for multi-term queries: the dictionary lookup is being done
twice for each term. The first time in Similarity.idf(), where searcher.docFreq() is called.
The second time when the posting list is opened (TermDocs or TermPositions).

The dictionary lookup is not cheap, that's why a significant performance improvement is
possible here if we avoid the second lookup. An easy way to do this is to add a small LRU 
cache to TermInfosReader. 

I ran some performance experiments with an LRU cache size of 20, and an mid-size index of
500,000 documents from wikipedia. Here are some test results:

50,000 AND queries with 3 terms each:
old:                  152 secs
new (with LRU cache): 112 secs (26% faster)

50,000 OR queries with 3 terms each:
old:                  175 secs
new (with LRU cache): 133 secs (24% faster)

For bigger indexes this patch will probably have less impact, for smaller once more.

I will attach a patch soon."
0,"Data store garbage collection. Currently the data store garbage collection needs to be run manually. It should be simpler to use (maybe tool based), or automatic."
0,"Compressed entities are not being cached properly. org.apache.http.impl.client.cache.CacheValidityPolicy.contentLengthHeaderMatchesActualLength() returns false for entities decompressed by ContentEncodingHttpClient, because the length of decompressed entity stored in cache will be different from the length specified in the response header.
Consequently, gzipped/deflated entities will never be satisfied from the cache.

Proposed fix: introduce new field in HttpCacheEntry() - actualContentLength, and populate it with the actual content length rigth before the cache entry is stored in the cache. Change the org.apache.http.impl.client.cache.CacheValidityPolicy.contentLengthHeaderMatchesActualLength() method to compare
entry.getResource().length() with entry.getActualContentLength()
"
0,"don't reuse byte[] in IndexInput/Output for read/writeString. IndexInput now holds a private ""byte[] bytes"", which it re-uses for reading strings.  Likewise, IndexOutput holds a UTF8Result (which holds ""byte[] bytes""), re-used for writing strings.

These are both dangerous, since on reading or writing immense strings, we never free this storage.

We don't use read/writeString in very perf sensitive parts of the code, so, I think we should not reuse the byte[] at all.

I think this is likely the cause of the recent ""IndexWriter and memory usage"" thread, started by Ross Woolf on java-user@."
0,"contrib/benchmark:  configurable HTML Parser, external classes to path, exhaustive doc maker. ""doc making"" enhancements:

1. Allow configurable html parser, with a new html.parser property.
Currently TrecDocMaker is using the Demo html parser. With this new property this can be overridden.

2. allow to add external class path, so the benchmark can be used with modified makers/parsers without having to add code to Lucene.
Run benchmark with e.g. ""ant run-task -Dbenchmark.ext.classpath=/myproj/myclasses""

3. allow to crawl a doc maker until exhausting all its files/docs once, without having to know in advance how many docs it can make. 
This can be useful for instance if the input data is in zip files."
0,"Make getAttribute(Class attClass) Generic. org.apache.lucene.util.AttributeSource

current:
public Attribute getAttribute(Class attClass) {
    final Attribute att = (Attribute) this.attributes.get(attClass);
    if (att == null) {
      throw new IllegalArgumentException(""This AttributeSource does not have the attribute '"" + attClass.getName() + ""'."");
    }
    return att;
}
sample usage:
TermAttribute termAtt = (TermAttribute)ts.getAttribute(TermAttribute.class)


my improvment:
@SuppressWarnings(""unchecked"")
	public <T> T getAttribute2(Class<? extends Attribute> attClass) {
    final T att = (T) this.attributes.get(attClass);
    if (att == null) {
      throw new IllegalArgumentException(""This AttributeSource does not have the attribute '"" + attClass.getName() + ""'."");
    }
    return att;
 }
sample usage:
TermAttribute termAtt = ts.getAttribute(TermAttribute.class)"
0,"Authentication does not respond to stale nonce. When using digest authentication, HTTP allows the server to mark the nonce value
as stale. The client then must re-authenticate with a new nonce value provided
by the server. Currently, HttpClient does not support this functionality. I've
created a patch that allows HttpClient to support stale nonce values. It is
attached below. The patch should be applied to HttpMethodBase.java


***
/home/scohen/downloads/httpclient-src/commons-httpclient-2.0-rc1/src/java/org/apache/commons/httpclient/HttpMethodBase.java
2003-07-31 22:15:26.000000000 -0400
--- org/apache/commons/httpclient/HttpMethodBase.java   2003-08-20
17:22:52.000000000 -0400
***************
*** 1351,1384 ****
       *
       * @throws IOException when errors occur reading or writing to/from the
       *         connection
       * @throws HttpException when a recoverable error occurs
       */
!     protected void addAuthorizationRequestHeader(HttpState state,
!                                                  HttpConnection conn)
!     throws IOException, HttpException {
!         LOG.trace(""enter HttpMethodBase.addAuthorizationRequestHeader(""
!                   + ""HttpState, HttpConnection)"");
   
          // add authorization header, if needed
!         if (getRequestHeader(HttpAuthenticator.WWW_AUTH_RESP) == null) {
!             Header[] challenges = getResponseHeaderGroup().getHeaders(
!                                                HttpAuthenticator.WWW_AUTH);
!             if (challenges.length > 0) {
!                 try {
!                     AuthScheme authscheme =
HttpAuthenticator.selectAuthScheme(challenges);
                      HttpAuthenticator.authenticate(authscheme, this, conn, state);
!                 } catch (HttpException e) {
!                     // log and move on
!                     if (LOG.isErrorEnabled()) {
!                         LOG.error(e.getMessage(), e);
!                     }
                  }
              }
          }
      }
                                                                                
      /**
       * Adds a <tt>Content-Length</tt> or <tt>Transfer-Encoding: Chunked</tt>
       * request header, as long as no <tt>Content-Length</tt> request header
       * already exists.
       *
--- 1351,1391 ----
       *
       * @throws IOException when errors occur reading or writing to/from the
       *         connection
       * @throws HttpException when a recoverable error occurs
       */
!     protected void addAuthorizationRequestHeader(HttpState state,
HttpConnection conn)
!         throws IOException, HttpException {
!         LOG.trace(""enter HttpMethodBase.addAuthorizationRequestHeader("" +
""HttpState, HttpConnection)"");
                                                                                
          // add authorization header, if needed
!
!         Header[] challenges =
getResponseHeaderGroup().getHeaders(HttpAuthenticator.WWW_AUTH);
!         if (challenges.length > 0) {
!
!             try {
!                 AuthScheme authscheme =
HttpAuthenticator.selectAuthScheme(challenges);
!                 if (getRequestHeader(HttpAuthenticator.WWW_AUTH_RESP) == null
!                     || isNonceStale(authscheme) ) {
                      HttpAuthenticator.authenticate(authscheme, this, conn, state);
!                 }
!             } catch (HttpException e) {
!                 // log and move on
!                 if (LOG.isErrorEnabled()) {
!                     LOG.error(e.getMessage(), e);
                  }
              }
          }
      }
                                                                                
+
+     private boolean isNonceStale(AuthScheme authscheme) {
+         return authscheme.getSchemeName().equalsIgnoreCase(""digest"")
+             && ""true"".equalsIgnoreCase(authscheme.getParameter(""stale""));
+     }
+
+
      /**
       * Adds a <tt>Content-Length</tt> or <tt>Transfer-Encoding: Chunked</tt>
       * request header, as long as no <tt>Content-Length</tt> request header
       * already exists.
       *
***************
*** 2419,2430 ****
                  buffer.append(port);
              }
              buffer.append('#');
              buffer.append(authscheme.getID());
              String realm = buffer.toString();
!
              if (realmsUsed.contains(realm)) {
                  if (LOG.isInfoEnabled()) {
                      LOG.info(""Already tried to authenticate to \""""
                               + realm + ""\"" but still receiving ""
                               + statusCode + ""."");
                  }
--- 2426,2442 ----
                  buffer.append(port);
              }
              buffer.append('#');
              buffer.append(authscheme.getID());
              String realm = buffer.toString();
!
!                       // check to see if the server has made our nonce stale.
!                       // if it has, re-auth
              if (realmsUsed.contains(realm)) {
+               if ( isNonceStale(authscheme)) {
+                       return false;
+               }
                  if (LOG.isInfoEnabled()) {
                      LOG.info(""Already tried to authenticate to \""""
                               + realm + ""\"" but still receiving ""
                               + statusCode + ""."");
                  }"
0,"Move *.log files to target/. The jackrabbit-core component already puts the derby.log file in target/ along with other build  and test artifacts, but many other components don't do that yet. Having all generated files in target/ is good as it makes it very easy to clean things up. Also things like the RAT checks (JCR-1937) are easier when there's no need to worry about such extra files.
"
0,"Configure Maximum Connection Lifetimes. Provide a means of configuring a maximum lifetime for HttpClient connections.  Currently, it would appear as long as a connection is used it may persist indefinitely.

This would be useful for situations where HttpClient needs to react to DNS changes, such as the following situation that may occur when using DNS load balancing:
 - HttpClient maintains connections to example.com which resolves to IP A
 - Machine at IP A fails, and example.com now resolves to backup machine at IP B
 - Since IP A is failing, connections are destroyed, and new connections are made to IP B
 - Machine at IP A recovers, but HttpClient maintains connections to IP B since the connections are still healthy

The desired behavior would be that connections to IP B will reach their connection lifetime, and new connections could be created back to IP A according to the updated DNS settings."
0,Add read acessor for user data to SessionInfoImpl. Add method SessionInfoImpl.getUserData() to retrieve the user data set through SessionInfoImpl.setUserData(String)
0,"AbstractLoginModule logs a warning on anonymous logins. Currently a ""No credentials available -> try default (anonymous) authentication"" warning is logged by AbstractLoginModule whenever an anonymous login is made. Since this is a normal situation, the log message should be at debug or at most info level."
0,"Validate the SearchIndex configuration. The validation of the configuration (repository.xml / workspace.xml) was enabled in JCR-1462, unfortunately a problem with the LoginModule and SearchIndex was found (see JCR-1920), so it was later disabled for those two modules.

Validation for SearchIndex is possible but needs some more changes. To avoid problems, those changes should be done in a major version and not in a minor version."
0,Refrase javadoc 1st sentence for IndexReader.deleteDocuments. 
0,"Clean up old JIRA issues in component ""Analysis"". A list of all JIRA issues in component ""Analysis"" that haven't been updated in 2007:

   *	 LUCENE-760  	 Spellchecker could/should use n-gram tokenizers instead of rolling its own n-gramming   
   *	LUCENE-677 	Italian Analyzer 
   *	LUCENE-571 	StandardTokenizer parses decimal number as <HOST> 
   *	LUCENE-566 	Esperanto Analyzer 
   *	LUCENE-559 	Turkish Analyzer for Lucene 
   *	LUCENE-494 	Analyzer for preventing overload of search service by queries with common terms in large indexes 
   *	LUCENE-424 	[PATCH] Submissiom form simple Romanian Analyzer 
   *	LUCENE-417 	StandardTokenizer has problems with comma-separated values 
   *	LUCENE-400 	NGramFilter -- construct n-grams from a TokenStream 
   *	LUCENE-396 	[PATCH] Add position increment back into StopFilter 
   *	LUCENE-387 	Contrib: Main memory based SynonymMap and SynonymTokenFilter 
   *	LUCENE-321 	[PATCH] Submissiom of my Tswana Analyzer 
   *	LUCENE-233 	[PATCH] analyzer refactoring based on CVS HEAD from 6/21/2004 
   *	LUCENE-210 	[PATCH] Never write an Analyzer again 
   *	LUCENE-205 	[PATCH] Patches for RussianAnalyzer 
   *	LUCENE-185 	[PATCH] Thai Analysis Enhancement 
   *	LUCENE-152 	[PATCH] KStem for Lucene 
   *	LUCENE-82 	[PATCH] HTMLParser: IOException: Pipe closed 

"
0,"Introduce a temprary cache for intermediate query results. Sometimes queries execute the same sub query multiple times.

e.g. //element(*, nt:resource)[@jcr:mimeType != 'text/plain' and @jcr:mimeType != 'text/html']

will result in two internal MatchAllQuery on jcr:mimeType. The query should re-use the previously calculated results when possible."
0,"IndexWriter.setInfoStream should percolate down to mergePolicy & mergeScheduler. Right now *MergePolicy and *MergeScheduler have their own ad-hoc means
of being verbose about their actions.  We should unify these with
IndexWriter's infoStream.  Thanks to Hoss for suggesting this."
0,"Default namespaces in JackrabbitNodeTypeManager.registerNodetypes. It would be nice if it wasn't necessary to always specify all the namespaces in node type definition files passed to JackrabbitNodeTypeManager.registerNodeTypes(). The node type parsers should by default use the persistent namespace mappings, but allow custom mappigns to be specified in the parsed node type definition files."
0,"Add ConstantScore highlighting support to SpanScorer. Its actually easy enough to support the family of constantscore queries with the new SpanScorer. This will also remove the requirement that you rewrite queries against the main index before highlighting (in fact, if you do, the constantscore queries will not highlight)."
0,"Promotion of SPI from Contrib. This has been suggested by Jukka on the dev-list [1] and i would like to start the promotion now. Apart from 
the promotion itself the discussion regarding distribution of common classes [2] and the related issue JCR-996 somehow depends on the promotion and i consider the latter one to be important to follow.

So far nobody objected the promotion. If this is still true, i will start working on that.




[1] http://www.mail-archive.com/dev@jackrabbit.apache.org/msg06433.html
[2] http://www.mail-archive.com/dev@jackrabbit.apache.org/msg06698.html"
0,"Use IOContext.READONCE in VarGapTermsIndexReader to load FST. VarGapTermsIndexReader should pass READONCE context down when it
opens/reads the FST. Yet, it should just replace the ctx passed in, ie if we are merging vs reading we want to differentiate.
"
0,"The values for the Via header are created by httpclient-cache for each cached and backend request. The Via header that gets generated and inserted by the caching layer is done repeatedly in the HTTP conversation, even if the constructed string is constant for each protocol version that is involved.

The proposed patch constructs a map of generated values held in memory with the associated ProtocolVersion as a key and uses read/write locks to access the data. This  solution minimizes the time to generate such a value from several milliseconds to 40-50 microseconds."
0,"Update link for javadocs from 1.0 to 1.3. On this page: http://jackrabbit.apache.org/doc/arch/overview/jcrlevels.html

You see this link:
Browse current Jackrabbit API: http://jackrabbit.apache.org/api-1/index.html

This should point to the latest javadoc version."
0,"contrib/benchmark files need eol-style set. The following files in contrib/benchmark don't have eol-style set to native, so when they are checked out, they don't get converted.

./build.xml:                    
./CHANGES.txt:                                                             
./conf/sample.alg:                                                                                
./conf/standard.alg:                                                                           
./conf/sloppy-phrase.alg:                                                                                 
./conf/deletes.alg:                                                                                         
./conf/micro-standard.alg:                                                                   
./conf/compound-penalty.alg:                                                                          
"
0,"Make retrieveTerms(int docNum) public in MoreLikeThis. It would be useful if 
{code}
private PriorityQueue retrieveTerms(int docNum) throws IOException {
{code}

were public, since it is similar in use to 
{code}
public PriorityQueue retrieveTerms(Reader r) throws IOException {
{code}

It also seems useful to add 
{code}
public String [] retrieveInterestingTerms(int docNum) throws IOException{
{code}
to mirror the one that works on Reader.

"
0,"Add test to check maven artifacts and their poms. As release manager it is hard to find out if the maven artifacts work correct. It would be good to have an ant task that executes maven with a .pom file that requires all contrib/core artifacts (or one for each contrib) that ""downloads"" the artifacts from the local dist/maven folder and builds that test project. This would require maven to execute the build script. Also it should pass the ${version} ANT property to this pom.xml"
0,Use bundle persistence in default configuration. The default repository configuration files in jackrabbit-core and -webapp still use the old simple database persistence. They should be updated to use bundle persistence in the 1.4 release.
0,"Script for checking releases. Script to automate checking of releases, similar to [0] 
(heavily inspired by the sling script)

The command line call will look like:
    ./check-release.sh jukka 1.6.5 4fa1e032f9b641fbc5c9ff8d6ba76fdb58b539ba

This command will also be embedded in the emails announcements that come with each release, so anybody can easily run it.

[0] http://svn.apache.org/repos/asf/sling/trunk/check_staged_release.sh
"
0,"QDefinitionBuilderFactory should auto-subtype from nt:base. Similar to JCR-2066, the QNodeTypeDefinitions build by QDefinitionBuilderFactory should auto subtype from nt:base. "
0,"optimize lev automata construction. in our lev automata algorithm, we compute an upperbound of the maximum possible states (not the true number), and
create some ""useless"" unconnected states ""floating around"".

this isn't harmful, in the original impl we did the Automaton is simply a pointer to the initial state, and all algorithms
traverse this list, so effectively the useless states were dropped immediately. But recently we changed automaton to
cache its numberedStates, and we set them here, so these useless states are being kept around.

it has no impact on performance, but can be really confusing if you are debugging (e.g. toString). Thanks to Dawid Weiss
for noticing this. 

at the same time, forcing an extra traversal is a bit scary, so i did some benchmarking with really long strings and found
that actually its helpful to reduce() the number of transitions (typically cuts them in half) for these long strings, as it
speeds up some later algorithms. 

won't see any speedup for short terms, but I think its easier to work with these simpler automata anyway, and it eliminates
the confusion of seeing the redundant states without slowing anything down.
"
0,"Optimize concurrent queries. There are a number of bottlenecks that prevent scalability of concurrent queries:

- Fake norms are created repeatedly because a new SearchIndex$CombinedIndexReader is created for each query. This prevents caching of fake norms on the level of the CombinedIndexReader. Creating fake norms for index readers that span multiple sub reader is inefficient and should be avoided. Like with other Jackrabbit specific queries, there should be one for TermQuery, which is aware of sub readers. Its weight should then create one scorer for each sub reader. This effectively reuses the fake norms on the sub reader.

- There should be a  UUID cache that maps document number to UUID. This is basically the inverse of the existing DocNumberCache. UUID lookup is regularly a bottleneck in the SegmentReader where the method document() is synchronized and does I/O.

- Queries often contain constraints that limit the result to nodes with a certain flag set to a literal. These constraints should be cached in the query handler."
0,"changes-to-html: better handling of bulleted lists in CHANGES.txt. - bulleted lists
- should be rendered
- as such
- in output HTML"
0,"Setting SSLSocket parameters. In HttpClient 4.0.3, it was easy to subclass SSLSocketFactory, and set SSLSocket options (e.g. setEnabledCipherSuites() or setSSLParameterse()) before the SSL handshake happened. This way it was possible to e.g. restrict cipher suites on per-HttpClient basis (instead of JVM-wide system properties).

In HttpClient 4.1.1, the design has changed quite a lot, and copy-pasting of several long methods is needed. 

Ideally, SSLSocketFactory should support applying SSLParameters to the socket. However, SSLParameters is Java 1.6, so if we want to keep compatibility with 1.5, that's out.

However, it'd be nice to at least have a method (e.g. ""protected SSLSocket prepareSSLSocket(SSLSocket s)"") that would get called immediately after a socket is retrieved from the socket factory. The default implementation could be just ""return s;"", but subclasses could do something like s.setEnabledCipherSuites() s.setSSLParameters()."
0,"Query path constraints like foo//*/bar do not scale. To resolve the * step the LuceneQueryBuilder currently creates a MatchAllQuery and checks every node for a foo ancestor. Instead, it should search for bar nodes and check for foo ancestors with at least one arbitrary hierarchy level in between."
0,"Review the use of BaiscHttpParams and HttpProtocolProcessor in HttpClient. Review the use of BaiscHttpParams and HttpProtocolProcessor in HttpClient and replace with thread-safe implementations where necessary.

Oleg"
0,"Add variable-gap terms index impl.. PrefixCodedTermsReader/Writer (used by all ""real"" core codecs) already
supports pluggable terms index impls.

The only impl we have now is FixedGapTermsIndexReader/Writer, which
picks every Nth (default 32) term and holds it in efficient packed
int/byte arrays in RAM.  This is already an enormous improvement (RAM
reduction, init time) over 3.x.

This patch adds another impl, VariableGapTermsIndexReader/Writer,
which lets you specify an arbitrary IndexTermSelector to pick which
terms are indexed, and then uses an FST to hold the indexed terms.
This is typically even more memory efficient than packed int/byte
arrays, though, it does not support ord() so it's not quite a fair
comparison.

I had to relax the terms index plugin api for
PrefixCodedTermsReader/Writer to not assume that the terms index impl
supports ord.

I also did some cleanup of the FST/FSTEnum APIs and impls, and broke
out separate seekCeil and seekFloor in FSTEnum.  Eg we need seekFloor
when the FST is used as a terms index but seekCeil when it's holding
all terms in the index (ie which SimpleText uses FSTs for).
"
0,"When sorting by field, IndexSearcher should not compute scores by default. In 2.9 we've added the ability to turn off scoring (maxScore &
trackScores, separately) when sorting by field.

I expect most apps don't use the scores when sorting by field, and
there's a sizable performance gain when scoring is off, so I think for
2.9 we should not score by default, and add show in CHANGES how to
enable scoring if you rely on it.

If there are no objections, I'll commit that change in a day or two
(it's trivial).
"
0,"Always use bulk-copy when merging stored fields and term vectors. Lucene has nice optimizations in place during merging of stored fields
(LUCENE-1043) and term vectors (LUCENE-1120) whereby the bytes are
bulk copied to the new segmetn.  This is much faster than decoding &
rewriting one document at a time.

However the optimization is rather brittle: it relies on the mapping
of field name to number to be the same (""congruent"") for the segment
being merged.

Unfortunately, the field mapping will be congruent only if the app
adds the same fields in precisely the same order to each document.

I think we should fix IndexWriter to assign the same field number for
a given field that has been assigned in the past.  Ie, when writing a
new segment, we pre-seed the field numbers based on past segments.
All other aspects of FieldInfo would remain fully dynamic."
0,"Make term offsets work in MemoryIndex. Fix the logic for retrieving term offsets from DocsAndPositionsEnum on a MemoryIndex, and allow subclasses to access them."
0,"Remove circular dependency between VersionManagerImpl and VersionItemStateProvider. From a architectural perspective the VersionManagerImpl (VMgr) is at a higher level as the VersionItemStateProvider (VISP). While the VMgr deals with Items the VISP deals with ItemState object. Nonetheless the VISP has a reference to the VMgr and also calls the method setNodeReferences(), which violates the rule of a strictly layered system. E.g. one negative effect of this was a deadlock as described in JCR-672. It also makes it hard to solve JCR-962.

The attached patch includes the following changes:

- Move method VersionManagerImpl.setNodeReferences() VersionItemStateManager. The method can operate on ItemStates only and does not need to be in VersionManagerImpl. As can be seen in the current method it directly calls the PeristenceManager, which indicates it should be located in a lower layer.
- Promote the class VersionItemStateManager to a top level class
- Change method VersionManagerImpl.createSharedStateManager to return a VersionItemStateManager
- Remove VersionManagerImpl instance variable from VersionItemStateProvider
- In VersionItemStateProvider.setNodeReferences() call VersionItemStateManager.setNodeReferences()
- Instead of using the PersistenceManager in VersionManagerImpl.getItemReferences() use the ItemStateManager. It also seems that locking is not necessary for this method."
0,"DocValuesField should not overload setInt/setFloat etc. See my description on LUCENE-3687. In general we should avoid this for primitive types and give them each unique names.

So I think instead of setInt(byte), setInt(short), setInt(int), setInt(long), setFloat(float) and setFloat(double),
we should have setByte(byte), setShort(short), setInt(int), setLong(long), setFloat(float) and setDouble(double)."
0,"Observation: avoid running out of memory. Jackrabbit uses an unbounded observation queue for event listeners (for asynchronous listeners, which are the default). If an observation listener is very slow, the observation queue gets larger and larger, and the JVM will eventually run out of memory.

I suggest to use a maximum queue size of 100'000 by default. Adding new events to the queue will block until the observation listeners removed an item. I'm not sure if we need a way to configure this option; probably a system property is enough as a start (we can still add a better way to configure this setting if it turns out somebody actually needs a different value).

A special case is observation listeners that themselves write to the repository and therefore cause new events. In this case, it doesn't make sense to block adding an event, because that would block the whole system. However a warning should be written to the log file."
0,"remove MultiTermQuery get/inc/clear totalNumberOfTerms. This method is not correct if the index has more than one segment.
Its also not thread safe, and it means calling query.rewrite() modifies
the original query. 

All of these things add up to confusion, I think we should remove this 
from multitermquery, the only thing that ""uses"" it is the NRQ tests, which 
conditionalizes all the asserts anyway.
"
0,"Deprecate XASession. The XASession interface in jackrabbit-api extends Session with a single getXAResource() method. The idea is that a transactional client (or a transaction manager) will test whether a session implements XASession and can then get the XAResource instance that can be used to bind the session to a distributed transaction. The essential code is:

    if (session instanceof XASession) {
        return ((XASession) session).getXAResource();
    }

This works fine except for the extra dependency to jackrabbit-api that it introduces in code that otherwise would only need the JCR API. Since the link between a transaction-enabled session and the related XAResource instance is always one-to-one, we could avoid this dependency by making the session directly implement XAResource, leading to code like this:

    if (session instanceof XAResource) {
        return (XAResource) session;
    }

This is essentially what jackrabbit-jcr-rmi did in 2.0.0 to avoid the jackrabbit-api dependency while maintaining XA transaction support, and I'd like to extend this solution also to other parts of Jackrabbit."
0,"Genericize DirectIOLinuxDir -> UnixDir. Today DirectIOLinuxDir is tricky/dangerous to use, because you only want to use it for indexWriter and not IndexReader (searching).  It's a trap.

But, once we do LUCENE-2793, we can make it fully general purpose because then a single native Dir impl can be used.

I'd also like to make it generic to other Unices, if we can, so that it becomes UnixDirectory."
0,"indexwriter creates unwanted termvector info. I noticed today that when I build a big index in Solr, I get some unwanted termvector info, even though I didn't request any.
This does not happen on 3x - not sure when it started happening on trunk."
0,"Error from maven when generating the task list. This message scrolls by when runing the site:generate goal.

tasklist:generate:
Non-fatal error while parsing file:
/home/jsdever/cvs-commit/jakarta-commons/httpclient/src/java/org/apache/commons/httpclient/HeaderElement.java
Non-fatal error while parsing file:
/home/jsdever/cvs-commit/jakarta-commons/httpclient/src/java/org/apache/commons/httpclient/URI.java"
0,"Provide Date Header Util Methods. Hello,

It would be really nice to have util methods that help with setting date
headers.  For instance, like the servlet spec's setDateHeader() method.  This
allows for the client of HttpClient to not have to deal with formatting the date
into the correct string.  There is a parseDate method that turns a String into a
Date.  It would be nice to have the opposite method.

Thanks!
Seth"
0,"add ASCIIFoldingFilter and deprecate ISOLatin1AccentFilter. The ISOLatin1AccentFilter is removing accents from accented characters in the ISO Latin 1 character set.
It does what it does and there is no bug with it.

It would be nicer, though, if there was a more comprehensive version of this code that included not just ISO-Latin-1 (ISO-8859-1) but the entire Latin 1 and Latin Extended A unicode blocks.
See: http://en.wikipedia.org/wiki/Latin-1_Supplement_unicode_block
See: http://en.wikipedia.org/wiki/Latin_Extended-A_unicode_block

That way, all languages using roman characters are covered.
A new class, ISOLatinAccentFilter is attached. It is intended to supercede ISOLatin1AccentFilter which should get deprecated."
0,Add a link to the release archive. It would be nice if the [Releases page|http://lucene.apache.org/java/docs/releases.html] contained a link to the release archive at http://archive.apache.org/dist/lucene/java/.
0,"Test tooling updates. To leverage advances in test tooling I'd like to upgrade our JUnit dependency to 4.x and switch to using the Maven Failsafe plugin  [1] instead of our current custom POM settings for integration tests. I'll also upgrade the Easymock dependency to 3.0.

[1] http://maven.apache.org/plugins/maven-failsafe-plugin/"
0,"[PATCH] reduce duplicate conversions from OffsetCharSequence to (lower/upper) strings. code repetitively converts OffsetCharSequence to strings, and then repetitively converts to lower/upper case, when generating search terms.

Patch fixes this."
0,"jcr2spi: reloading of invalidated nodes doesn't benefit from batch-read. issue reported by stefan:

upon reloading of an invalidated node, jcr2spi doesn't use batch-read but only retrieves the NodeInfo.

basically, WorkspaceItemStateFactory ll 92 needs to be changed to omit the extra handling of 
invalidated node entries and always use RepositoryService.getItemInfos when build the NodeState.

i will try it out as soon as possible.

"
0,Disable consistency check per default. There should be a way to disable the consistency check entirely. Currently a consistency check is performed on startup whenever the redo log is applied. For large workspaces this may take a long time and should only be performed when 'requested'.
0,"Audit log. JCR-2031 added the user name and path in debug logs for audit purposes. There are some problems with the fix that I had outlined in the comments for JCR-2031 and provided a patch. Additionally, it would use useful to add an update counter and size information to the debug log as well. Something like this:

17.03.2009 14:43:37 [1] 18216140 admin@/apps/acme/templates/contentpage/thumbnail.png (12343) 
17.03.2009 14:43:37 [2] 18216141 admin@/apps/acme/templates/contentpage/my.png (123) 
17.03.2009 14:43:37 [3] 18216142 admin@/apps/acme/templates/contentpage/blah.png (1423) 
17.03.2009 14:43:37 [4] 18216143 admin@/apps/acme/templates/contentpage/test.png (123423) 
17.03.2009 14:43:37 [5] 18216144 admin@/apps/acme/templates/contentpage/test2.png (123423) 

<date> <time> [<counter>] <txid> <userid>@<path> (<size>)

We should also think about whether we want this log as part of regular jackrabbit log or in a separate audit log. 
"
0,"CompactNodeTypeReader fails to explain why valid JCR names cause errors. for example, you cannot use underscores in node type definitions:
[my:example_breaks2]

In fact only A-Z, a-z, 0-9, : are allowed, unless you quote the name. The error message you see when you make this mistake doesn't give any hint:
Missing ']' delimiter for end of node type name (nodetypes.cnd, line 8)

and the documentation on the website and the javadoc for CompactNodeTypeDefReader both just say:

 * unquoted_string ::= ...a string...

... not helpful. If you made this mistake, you end up needing to look at the source to figure out what you've done wrong. 

A few suggested solutions:
- change the documentation to say unquoted string is '[A-Za-z0-9:]+'
- change the error message to mention the token causing the problem, eg:
if (!currentTokenEquals(Lexer.END_NODE_TYPE_NAME)) {
            lexer.fail(""Missing '"" + Lexer.END_NODE_TYPE_NAME + ""' delimiter for end of node type name, found "" + currentToken);
}
- add ""st.wordChars('_','_');"" to the lexer, its probably going to be the most common cause, and doesnt conflict with other rules.
"
0,"Typo in NodeTypeRegistry. It seems a little typo has been introduced in the NodeTypeRegistry, as illustrated in this stack trace : 

Caused by: javax.jcr.RepositoryException: internal error: invalid resource: nodetypes/custom_nodetypes.xml
	at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.<init>(NodeTypeRegistry.java:703) ~[jackrabbit-core-2.2-SNAPSHOT.jar:2.2-SNAPSHOT]
	at org.apache.jackrabbit.core.RepositoryImpl.createNodeTypeRegistry(RepositoryImpl.java:422) ~[jackrabbit-core-2.2-SNAPSHOT.jar:na]
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:294) ~[jackrabbit-core-2.2-SNAPSHOT.jar:na]

This happens when using a DbFileSystem for the root filesystem. This didn't cause a problem in 2.1.1

The patch attached to this ticket correct the issue."
0,"the demo application does not work as of 3.0. the demo application does not work. QueryParser needs a Version argument.

While I am here, remove @author too"
0,"Weight.scorer() not passed doc offset for ""sub reader"". Now that searching is done on a per segment basis, there is no way for a Scorer to know the ""actual"" doc id for the document's it matches (only the relative doc offset into the segment)

If using caches in your scorer that are based on the ""entire"" index (all segments), there is now no way to index into them properly from inside a Scorer because the scorer is not passed the needed offset to calculate the ""real"" docid

suggest having Weight.scorer() method also take a integer for the doc offset

Abstract Weight class should have a constructor that takes this offset as well as a method to get the offset
All Weights that have ""sub"" weights must pass this offset down to created ""sub"" weights


Details on workaround:
In order to work around this, you must do the following:
* Subclass IndexSearcher
* Add ""int getIndexReaderBase(IndexReader)"" method to your subclass
* during Weight creation, the Weight must hold onto a reference to the passed in Searcher (casted to your sub class)
* during Scorer creation, the Scorer must be passed the result of YourSearcher.getIndexReaderBase(reader)
* Scorer can now rebase any collected docids using this offset

Example implementation of getIndexReaderBase():
{code}
// NOTE: more efficient implementation can be done if you cache the result if gatherSubReaders in your constructor
public int getIndexReaderBase(IndexReader reader) {
  if (reader == getReader()) {
    return 0;
  } else {
    List readers = new ArrayList();
    gatherSubReaders(readers);
    Iterator iter = readers.iterator();
    int maxDoc = 0;
    while (iter.hasNext()) {
      IndexReader r = (IndexReader)iter.next();
      if (r == reader) {
        return maxDoc;
      } 
      maxDoc += r.maxDoc();
    } 
  }
  return -1; // reader not in searcher
}
{code}

Notes:
* This workaround makes it so you cannot serialize your custom Weight implementation
"
0,"PostMethod - Chunked requests are not supported at the moment.. For Apache Axis, we'd like send a POST request without needing to calculate the
content-length for HTTP 1.1 based servers. Of course if the server-side does not
support 1.1 then a fallback mechanism could calculate the total size under the
covers. 

Also see related request from ""Trevor O'Reilly"" <wtrevor@yahoo.com>:
http://marc.theaimsgroup.com/?l=jakarta-commons-user&m=102719653201792&w=2"
0,"Add log information when node/property type determination fails. getQNodeDefinition() and getQPropertyDefinition() of o.a.j.jcr2spi.nodetype.ItemDefinitionProviderImpl silently ignore errors which might occur on determination of node and property types. Instead these methods use RepositoryService.getNodeDefinition() and RepositoryService.getPropertyDefinition(), respectively to determine the types. This might lead to difficult to track down problems when the RepositoryService call occurs because of an error in the node type definition. I suggest to add logging statements to these methods. "
0,Text extraction may congest thread pool in the repository. Text extraction congests the thread pool in the repository when e.g. many PDFs are loaded into the workspace. Tasks submitted by the index merger are delayed because of that and will result in many index segment folders.
0,"Occasional testDataStoreGarbageCollection test failures. In the past few days our Hudson build started failing every now and then with the following jackrabbit-core test failure:

javax.jcr.NoSuchWorkspaceException: security
	at org.apache.jackrabbit.core.RepositoryImpl.getWorkspaceInfo(RepositoryImpl.java:786)
	at org.apache.jackrabbit.core.RepositoryImpl.getSystemSession(RepositoryImpl.java:985)
	at org.apache.jackrabbit.core.RepositoryImpl.getSecurityManager(RepositoryImpl.java:471)
	at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1496)
	at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:380)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:123)
	at org.apache.jackrabbit.core.data.DataStoreAPITest.testDataStoreGarbageCollection(DataStoreAPITest.java:55)"
0,"Make contrib/collation/(ICU)CollationKeyAnalyzer constructors public. In contrib/collation, the constructors for CollationKeyAnalyzer and ICUCollationKeyAnalyzer are package private, and so are effectively unusable."
0,"Revise NIOFSDirectory and its usage due to NIO limitations on Thread.interrupt. I created this issue as a spin off from http://mail-archives.apache.org/mod_mbox/lucene-java-dev/201001.mbox/%3Cf18c9dde1001280051w4af2bc50u1cfd55f85e50914f@mail.gmail.com%3E

We should decide what to do with NIOFSDirectory, if we want to keep it as the default on none-windows platforms and how we want to document this.

"
0,"Add support for simple test cases. As discussed on the mailing list, I'd like to add a simple org.apache.jackrabbit.core.TestRepository helper class that could be used in a simple unit test template."
0,"Back-compat break with non-ascii field names. If a field name contains non-ascii characters in a 2.3.x index, then
on upgrade to 2.4.x unexpected problems are hit.  It's possible to hit
a ""read past EOF"" IOException; it's also possible to not hit an
exception but get an incorrect field name.

This was caused by LUCENE-510, because the FieldInfos (*.fnm) file is
not properly versioned.

Spinoff from http://www.nabble.com/Read-past-EOF-td23276171.html
"
0,Allow for specification of spell checker accuracy when calling suggestSimilar. There is really no need for accuracy to be a class variable in the Spellchecker
0,"Provide access to SSLSession in ManagedClientConnection. Provide access to the wrappedConnection in org.apache.http.impl.conn.AbstractClientConnAdapter via some interface in order to access the socket from within an HttpProcessor. Currently the org.apache.http.conn.OperatedClientConnection has a getSocket() method, but the connection implementation returned by

  context.getAttribute(ExecutionContext.HTTP_CONNECTION) 

(org.apache.http.impl.conn.tsccm.BasicPooledConnAdapter) does not provide access to the wrappedConnection."
0,JSR 283: Built-In Node Types. sync definitions of built-in node types with spec
0,"[PATCH] HTMLParser doesn't parse hexadecimal character references. I recently inherited a project from an ex-colleague; it uses Lucene and in
particular the HTML Parser.  I've found that she had made an amendment to the
parser to allow it to parse and decode hexadecimal character references, which
we depend on, but had not reported a bug.  If she had, someone might have
pointed out that her correction was wrong ...

I don't seem to be able to attach the (fairly trivial) patch to an initial bug
report (and in any case I've failed to find the instructions for generating a
diff file in the right format, even though I'm sure I've seen it somewhere)."
0,IndexWriter.deleteDocuments bug. IndexWriter.deleteDocuments() fails random testing
0,"Various inner classes maintain references to owning class for no reason. Various inner classes maintain references to their owning classes for no reason, as they are independent classes. This issue will change these classes to be static inner classes, so that their footprint decreases, they ease gc work, and potentially reduce the lifetime of the owning classes if they outlive their owner."
0,"Unit tests do not fail if a ConcurrentMergeScheduler thread hits an exception. Now that CMS is the default, it's important to fail any unit test that
hits an exception in a CMS thread.  But they do not fail now.  The
preferred solution (thanks to Erik Hatcher) is to fix all Lucene unit
tests to subclass from a new LuceneTestCase (in o.a.l.util) base that
asserts that there were no such exceptions during the test.
"
0,"Track FieldInfo per segment instead of per-IW-session. Currently FieldInfo is tracked per IW session to guarantee consistent global field-naming / ordering. IW carries FI instances over from previous segments which also carries over field properties like isIndexed etc. While having consistent field ordering per IW session appears to be important due to bulk merging stored fields etc. carrying over other properties might become problematic with Lucene's Codec support.  Codecs that rely on consistent properties in FI will fail if FI properties are carried over.

The DocValuesCodec (DocValuesBranch) for instance writes files per segment and field (using the field id within the file name). Yet, if a segment has no DocValues indexed in a particular segment but a previous segment in the same IW session had DocValues, FieldInfo#docValues will be true  since those values are reused from previous segments. 

We already work around this ""limitation"" in SegmentInfo with properties like hasVectors or hasProx which is really something we should manage per Codec & Segment. Ideally FieldInfo would be managed per Segment and Codec such that its properties are valid per segment. It also seems to be necessary to bind FieldInfoS to SegmentInfo logically since its really just per segment metadata.  "
0,"New Preferences Architecture. An architectural solution is needed to configure various aspects of HttpClient,
Methods and Connections. 

Features:
- can configure certain properties per request / per connection
- all configuration is done in a consistant way 
- do not use system properties
- configuration is completely optional: default values should be used if no
configuration is made

This is a refactoring request / reminder. File configuration issues as
dependencies of this bug."
0,"replace collation/lib/icu4j.jar with a smaller icu jar. Collation does not need all the icu data.
we can shrink the jar file a bit by using the data customizer, and excluding things like character set conversion tables."
0,"Index creates many folders when re-indexing. When the repository is re-indexed the search index creates a lot of directories, which are finally cleaned up. If the repository contains a lot of content the number of directories that are created can be quite high (thousands of directories).

The re-indexing process should clean up unused index folders right away and not wait until the changes are committed."
0,No support for Oracle schemas. Jackrabbit assumes that the table name (f.e.: prefix + fs_entry) is unique across all schemas  used on one oracle instance. So two Jackrabbit instances using two schemas on the same Oracle instance won't work. 
0,"remove TermVectorsWriter (it's no longer used). We should remove TermVectorsWriter: it's no longer used now that
DocumentsWriter writes the term vectors directly to the index."
0,Using explain may double ram reqs for fieldcaches when using ValueSourceQuery/CustomScoreQuery or for ConstantScoreQuerys that use a caching Filter.. 
0,Add a filter returning all document without a value in a field. In some situations it would be useful to have a Filter that simply returns all document that either have at least one or no value in a certain field. We don't have something like that out of the box and adding it seems straight forward.
0,"Contrib: Main memory based SynonymMap and SynonymTokenFilter. - Contrib: Main memory based SynonymMap and SynonymTokenFilter
- applies to SVN trunk as well as 1.4.3"
0,"Poor performance in range queries using dates. I am evaluating migrating from 1.5 to 1.6. I created several test cases that prove the query performance of 1.6 is the same or better than 1.5. That is until I add a date property into my query. The repository has 400,000 nodes. Each node as several string based properties (@property, @property2, ...) and a date based property (@datestart). Every node has a relatively unique datestart and the total date range spans 6 years.

In my tests, my base query is:
//element(*,my:namespace)[@property='value'] order by @datestart descending

The time to run this query in 1.5 and 1.6 is:
1.5 = 1.5 seconds
1.6 = 1.5 seconds

If I add a date property:
//element(*,my:namespace)[@property='value' and @datestart<=xs:dateTime('2009-09-24T11:53:23.293-05:00')] order by @datestart descending

the results are:
1.5 = 1.5 seconds
1.6 = 3.5 seconds 

I have isolated the slow down to the implementation of SortedLuceneQueryHits. SortedLuceneQueryHits is not present in 1.5. I have run versions of the test where the query is run 20 times simultaneously and a different time where the query is run 20 times sequentially. In both tests I do see evidence that caching is taking place, but it provides only very minor performance gains. Also, running the 1.6 query multiple times does not decrease the query time dramatically.

http://www.nabble.com/Date-Property-Performance-in-1.6-td25704607.html"
0,"Add TrieRangeFilter to contrib. According to the thread in java-dev (http://www.gossamer-threads.com/lists/lucene/java-dev/67807 and http://www.gossamer-threads.com/lists/lucene/java-dev/67839), I want to include my fast numerical range query implementation into lucene contrib-queries.

I implemented (based on RangeFilter) another approach for faster
RangeQueries, based on longs stored in index in a special format.

The idea behind this is to store the longs in different precision in index
and partition the query range in such a way, that the outer boundaries are
search using terms from the highest precision, but the center of the search
Range with lower precision. The implementation stores the longs in 8
different precisions (using a class called TrieUtils). It also has support
for Doubles, using the IEEE 754 floating-point ""double format"" bit layout
with some bit mappings to make them binary sortable. The approach is used in
rather big indexes, query times are even on low performance desktop
computers <<100 ms (!) for very big ranges on indexes with 500000 docs.

I called this RangeQuery variant and format ""TrieRangeRange"" query because
the idea looks like the well-known Trie structures (but it is not identical
to real tries, but algorithms are related to it).

"
0,"Small speedups to DocumentsWriter's quickSort. In working on LUCENE-510 I found that DocumentsWriter's quickSort can
be further optimized to handle the common case of sorting only 2
values.

I ran with this alg:

  analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
  
  doc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker
  
  docs.file=/Volumes/External/lucene/wiki.txt
  doc.stored = true
  doc.term.vector = true
  doc.add.log.step=2000
  doc.maker.forever = false
  
  directory=FSDirectory
  autocommit=false
  compound=false
  
  ram.flush.mb=64
  
  { ""Rounds""
    ResetSystemErase
    { ""BuildIndex""
      CreateIndex
      { ""AddDocs"" AddDoc > : 200000
      - CloseIndex
    }
    NewRound
  } : 5
  
  RepSumByPrefRound BuildIndex

Best of 5 was 857.3 docs/sec before the optimization and 881.6 after =
2.8% speedup, on a quad-core Mac Pro with 4-drive RAID 0 array.

The fix is trivial.  I will commit shortly.

"
0,"optimize automatonquery. Mike found a few cases in flex where we have some bad behavior with automatonquery.
The problem is similar to a database query planner, where sometimes simply doing a full table scan is faster than using an index.

We can optimize automatonquery a little bit, and get better performance for fuzzy,wildcard,regex queries.

Here is a list of ideas:
* create commonSuffixRef for infinite automata, not just really-bad linear scan cases
* do a null check rather than populating an empty commonSuffixRef
* localize the 'linear' case to not seek, but instead scan, when ping-ponging against loops in the state machine
* add a mechanism to enable/disable the terms dict cache, e.g. we can disable it for infinite cases, and maybe fuzzy N>1 also.
* change the use of BitSet to OpenBitSet or long[] gen for path-tracking
* optimize the backtracking code where it says /* String is good to go as-is */, this need not be a full run(), I think...
"
0,"Specialize BooleanQuery if all clauses are TermQueries. During work on LUCENE-3319 I ran into issues with BooleanQuery compared to PhraseQuery in the exact case. If I disable scoring on PhraseQuery and bypass the position matching, essentially doing a conjunction match, ExactPhraseScorer beats plain boolean scorer by 40% which is a sizeable gain. I converted a ConjunctionScorer to use DocsEnum directly but still didn't get all the 40% from PhraseQuery. Yet, it turned out with further optimizations this gets very close to PhraseQuery. The biggest gain here came from converting the hand crafted loop in ConjunctionScorer#doNext to a for loop which seems to be less confusing to hotspot. In this particular case I think code specialization makes lots of sense since BQ with TQ is by far one of the most common queries.

I will upload a patch shortly"
0,"TCK: DerefQueryLevel1Test requires support for optional jcr:deref function. Test fails if jcr:deref is not supported.  Per JSR-170, jcr:deref is optional.

Proposal: introduce new configuration property which indicates whether jcr:deref is supported; if not throw NotExecutableException
"
0,"disable atime for DirectIOLinuxDirectory. In Linux's open():
O_NOATIME
    (Since Linux 2.6.8) Do not update the file last access time (st_atime in the inode) when the file is read(2). This flag is intended for use by indexing or backup programs, where its use can significantly reduce the amount of disk activity. This flag may not be effective on all filesystems. One example is NFS, where the server maintains the access time.

So we should do this in our linux-specific DirectIOLinuxDirectory.

Separately (offtopic), it would be better if this was a LinuxDirectory that only uses O_DIRECT when it should :)
It would be nice to think about an optional modules/native for common platforms similar to what tomcat provides
Its easier to test directories like this now (-Dtests.directory)...
"
0,"[PATCH] TermVectorReader and TermVectorWriter. TermVectorReader.close() closes all streams now under any condition. If an
excpetion is catched, it is remembered an thrown when all streams are closed.
Unnecessary variable assignment removed from code. 
Fix typo in TermVectorReader and TermVectorWriter."
0,"jackrabbit-jcr-client should put all test data under ./target. Currently the jackrabbit-jcr-client component drops a ""repository"" directory and the ""repository.xml"" and ""derby.log"" files into the project root when running the test suite. These files should go inside ""target""."
0,"simple webdav server does not support lock timeouts. The ""simple"" WebDAV server still does not support lock timeouts (HTTP trace shows MS word requests 3600s timeout but gets infinity)."
0,"version.properties. If we're not going to split it, there should be only one version.properties in module-client.
module-httpmime is currently missing a version.properties file.
"
0,Add args to test-macro. Add passing args to JUnit.  (Like Solr and mainly for debugging).  
0,"If setConfig(Config config) is called in resetInputs(), you can turn term vectors off and on by round. I want to be able to run one benchmark that tests things using term vectors and not using term vectors.

Currently this is not easy because you cannot specify term vectors per round.

While you do have to create a new index per round, this automation is preferable to me in comparison to running two separate tests.

If it doesn't affect anything else, it would be great to have setConfig(Config config) called in BasicDocMaker.resetInputs(). This would keep the term vector options up to date per round if you reset.

- Mark"
0,"o.a.jackrabbit.spi.commons.conversion.NameParser should not assume that namespace URI's are registered. according to JCR 2.0, ""3.4.3.4 Parsing Lexical Paths"":

<quote>
An otherwise valid path containing an expanded name with an unregistered 
namespace URI will always resolve into a valid internal representation of a path 
</quote>

the current implementation assumes that namespace URIs encountered in 
expanded form names are registered, otherwise the name is treated as
qualified name. "
0,"relax the per-segment max unique term limit. Lucene can't handle more than 2.1B (limit of signed 32 bit int) unique terms in a single segment.

But I think we can improve this to termIndexInterval (default 128) * 2.1B.  There is one place (internal API only) where Lucene uses an int but should use a long."
0,"Add support for ICU's Normalizer2. While there are separate Case Folding, Normalization, and Ignorable-removal filters in LUCENE-1488,
the new ICU Normalizer2 API does this all at once with nfkc_cf (based on the new NFKC_Casefold property in Unicode).

This is great, because it provides a ton of unicode functionality that is really needed.
And the new Normalizer2 API takes CharSequence and writes to Appendable...
"
0,"Handle Null Arguments consistantly. Consider throwing a NullPointerException or InvalidArgumentException for null
argument when they are not allowed.  Be consistant and document behaviour."
0,"spi2davex: unspecific BadRequest error instead of error code matching the RepositoryException. the JsonDiffHandler#NodeHandler in the server part of the jcr remoting may only throw IOException
if an error occurs. this results in unspecific BadRequest responses even if the problem source was
something very specific such as e.g. a locked node.

after having a first glance at this i think that making DiffException a subclass of IOException would
allow to generate much more specific responses codes that even include the original exception
details.

i will attach a patch as i didn't had time to carefully test it.  [the conformance tests passed]."
0,"cache module should handle out-of-order validations properly and unconditionally refresh. There is a protocol recommendation that when we attempt to revalidate a cache entry, but we receive a response that has a Date header that's actually *older* than that of our current entry, we SHOULD revalidate again unconditionally with either max-age=0 or no-cache (since some upstream cache would appear to be out-of-date).

http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.2.6

"
0,"Remove geronimo JTA as a runtime dependency. Geronimo JTA is marked as a dependency for runtime when it should be (at most) a compile time dependency. 
Is it possible to remedy this so when using the war or building your own, you don't get the geronimo jar stowing away?"
0,"Provide factory method to create DefaultHttpClient instances pre-configured based on JSSE and networking system properties. Provide factory method or a factory class intended to create DefaultHttpClient instances pre-configured based on JSSE [1] and networking [2] system properties.

[1] http://download.oracle.com/javase/1,5.0/docs/guide/security/jsse/JSSERefGuide.html
[2] http://download.oracle.com/javase/1.5.0/docs/guide/net/properties.html"
0,Introduce QNodeTypeDefinition cache per userId. Once read from the server a QNodeTypeDefinition can be cached and shared across SessionImpl with the same userId.
0,"Distinct field value count per group. Support a second pass collector that counts unique field values of a field per group.
This is just one example of group statistics that one might want."
0,"Modify confusing javadoc for queryNorm. See http://markmail.org/message/arai6silfiktwcer

The javadoc confuses me as well."
0,"Support inclusive/exclusive for TrieRangeQuery/-Filter, remove default trie variant setters/getters. TrieRangeQuery/Filter is missing one thing: Ranges that have exclusive bounds. For TrieRangeQuery this may not be important for ranges on long or Date (==long) values (because [1..5] is the same like ]0..6[ or ]0..5]). This is not so simple for doubles because you must add/substract 1 from the trie encoded unsigned long.

To be conform with the other range queries, I will submit a patch that has two additional boolean parameters in the ctors to support inclusive/exclusive ranges for both ends. Internally it will be implemented using TrieUtils.incrementTrieCoded/decrementTrieCoded() but makes life simplier for double ranges (a simple exclusive replacement for the floating point range [0.0..1.0] is not possible without having the underlying unsigned long).

In December, when trie contrib was included (LUCENE-1470), 3 trie variants were supplied by TrieUtils. For new APIs a statically configureable default Trie variant does not conform to an API we want in Lucene (currently we want to deprecate all these static setters/getters). The important thing: It does not make code shorter or easier to understand, its more error prone. Before release of 2.9 it is a good time to remove the default trie variant and always force the parameter in TrieRangeQuery/Filter. It is better to choose the variant in the application and do not automatically manage it.

As Lucene 2.9 was not yet released, I will change the ctors and not preserve the old ones."
0,BundleDumper to analyze broken bundles. The BundleReader fails if it can't read a bundle. We should have a tool to analyze broken bundles.
0,"Drop deprecations from trunk. subj.
Also, to each remaining deprecation add release version when it first appeared.

Patch incoming."
0,Allow using FST to hold terms data in DocValues.BYTES_*_SORTED. 
0,"Refactor FieldCacheRangeFilter.FieldCacheDocIdSet to be separate class and fix the dangerous matchDoc() throws AIOOBE requirement. Followup from LUCENE-3593:
The FieldCacheRangeFilter.FieldCacheDocIdSet class has a strange requirement on the abstract matchDoc(): It should throw AIOOBE if the docId is > maxDoc. This check should be done by caller as especially on trunk, e.g. FieldCacheTermsFilter does not seem to always throw this exception correctly (getOrd() is a method and no array in TermsIndex cache).

Also in 3.x the Filter does not correctly respect deletions when a FieldCache based on a reopened reader is used.

This issue will refactor this and fix the bugs and moves the docId check up to the iterator."
0,"Make FSIndexInput and FSIndexOutput inner classes of FSDirectory. I would like make FSIndexInput and FSIndexOutput protected, static, inner classes of FSDirectory. Currently these classes are located in the same source file as FSDirectory, which means that classes outside the store package can not extend them.

I don't see any performance impacts or other side effects of this trivial patch. All unit tests pass."
0,"omitTF is viral, but omitNorms is anti-viral.. omitTF is viral. if you add document 1 with field ""foo"" as omitTF, then document 2 has field ""foo"" without omitTF, they are both treated as omitTF.

but omitNorms is the opposite. if you have a million documents with field ""foo"" with omitNorms, then you add just one document without omitting norms, 
now you suddenly have a million 'real norms'.

I think it would be good for omitNorms to be viral too, just for consistency, and also to prevent huge byte[]'s.
but another option is to make omitTF anti-viral, which is more ""schemaless"" i guess.
"
0,"Code depends on Log4J directly. The code is written against the Log4J APIs, which forces all users of Jackarabbit to pick up log4J dependency and to juggle with JDK logging and Log4J configuration if other components of the project uses JDK 1.4 logging.
If the code is move to depend on Apache commons-logging this issue will be resolved. Also this should be a minor fix."
0,"spi2dav: temp files are not clean up after batch submit. when a batch includes larger binary properties, their backing temp files are not clean up after submit.
suggest to dispose those in any case."
0,"Deprecate Spatial Contrib. The spatial contrib is blighted by bugs.  The latest series, found by Grant and discussed [here|http://search.lucidimagination.com/search/document/c32e81783642df47/spatial_rethinking_cartesian_tiers_implementation] shows that we need to re-think the cartesian tier implementation.

Given the need to create a spatial module containing code taken from both lucene and Solr, it makes sense to deprecate the spatial contrib, and start from scratch in the new module.


"
0,Make CacheEntry use an immutable object to represent cache content . Make CacheEntry use an immutable object to represent cache content similar to HttpEntity
0,"SecureProtocolFactoryWrapper class for using the socket factory created by Java Web Start. As smartcards and SSL are becoming more and more prevelant, Java Web Start has started to become better equiped to handle these situations.  When running an app within webstart, it can access the browser's keystore, which (at least in our case) accessed the users smartcard to make the SSL connection.

I wanted to start using HttpClient, but needed a way to do so while still mainaining access to the browser's keystore.

My initial tests show that getting the default socket factory from the java.net.HttpURLConnection and wrapping it in a class that implements org.apache.commons.httpclient.protocol.SecureProtocolSocketFactory is sufficient."
0,"add LuceneTestCase[J4].newField. I think it would be good to vary the different field options in tests.

For example, we do this with IW settings (newIndexWriterConfig), and directories (newDirectory).

This patch adds newField(), it works just like new Field(), except it will sometimes turns on extra options:
Stored fields, term vectors, additional term vectors data, etc.
"
0,"Small imprecision in Search package Javadocs. Search package Javadocs states that Scorer#score(Collector) will be abstract in Lucene 3.0, which is not accurate anymore."
0,"LengthFilter and other TokenFilters that skip tokens ignore relative positionIncrement. See for reference:
http://www.nabble.com/WordDelimiterFilter%2BLenghtFilter-results-in-termPosition%3D%3D-1-td16306788.html
and http://www.nabble.com/Lucene---Java-f24284.html

It seems that LengthFilter (at least) could produce a stream in which the first Token has a positionIncrement of 0, which make CheckIndex and Luke function ""Reconstruct&Edit"" to generate exception.

Should something be done to avoid this situation, or could the error be ignored (by allowing Term with a position of -1, and relaxing CheckIndex checks?)
"
0,"Error in FSDirectory if java.io.tmpdir incorrectly specified. A user of the JAMWiki project (http://jamwiki.org/) reported an error with the following stack trace:

SEVERE: Unable to create search instance /usr/share/tomcat5/webapps/jamwiki-0.3.4-beta7/test/base/search/indexen
java.io.IOException: Cannot create directory: /temp
        at org.apache.lucene.store.FSDirectory.init(FSDirectory.java:171)
        at org.apache.lucene.store.FSDirectory.getDirectory(FSDirectory.java:141)
        at org.apache.lucene.store.FSDirectory.getDirectory(FSDirectory.java:117)
        at org.jamwiki.search.LuceneSearchEngine.getSearchIndexPath(LuceneSearchEngine.java:318)

The culprit is that the java.io.tmpdir property was incorrectly specified on the user's system.  Lucene could easily handle this issue by modifying the FSDirectory.init() method.  Currently the code uses the index directory if java.io.tmpdir and org.apache.lucene.lockDir are unspecified, but it could use that directory if those values are unspecified OR if they are invalid.  Doing so would make Lucene a bit more robust without breaking any existing installations.
"
0,"Make shutdown hooks in TransientFileFactory removable. TransientFileFactory class always registers shutdown hook. So, if jackrabbit classes were loaded by web-app classloader, they will not be released when web-app is undeployed (if jackrabbit-jcr-commons JAR is inside WAR). This causes classloader leak.
It seems to be useful to have ability to cancel TransientFileFactory's shutdown hook when application is going to be unloaded to avoid classloader leak."
0,"Remove background initialization of hierarchy cache. This is a follow up to JCR-1998.

Rethinking the initialization in a background thread again, I now come to the conclusion that it should be initialized either completely on startup or not at all. A background thread puts additional load on the process, possibly fighting for I/O with other startup procedures.
"
0,"Reduce number of different repository.xml present with jackrabbit-core. while taking a look at the repository configuration and the related test-cases, i saw that there are quite some repository.xml files around... which i think is a bit confusion and probably hard to maintain once we make
changes to the config.

i would to suggest to consolidate that and - if possible - get rid of some of them.
if we can't i would suggest to put some comment in every of the different configuration files indicating
what they are used for.

from what i've seen so far (still missing complete overview)

1) http://svn.apache.org/repos/asf/jackrabbit/trunk/jackrabbit-core/src/main/config/repository.xml

    Current comment: <!-- Example Repository Configuration File -->
    Usage: ??

2) http://svn.apache.org/repos/asf/jackrabbit/trunk/jackrabbit-core/src/main/resources/org/apache/jackrabbit/core/test-repository.xml

   current comment: -  
   Used as repository configuration in org.apache.jackrabbit.core.TestRepository.java
   

3) http://svn.apache.org/repos/asf/jackrabbit/trunk/jackrabbit-core/src/main/resources/org/apache/jackrabbit/core/repository.xml
  
   current comment: <!-- Example Repository Configuration File -->

   Used by org.apache.jackrabbit.core.config.RepositoryConfigTest.java in order to create another repository.xml 
   under target/test-repository.xml. a bit confusing given the fact, that a test-repository.xml exists as well. I would
   suggest to rename the REPOSITORY_XML constant in RepositoryConfigTest.


4) http://svn.apache.org/repos/asf/jackrabbit/trunk/jackrabbit-core/src/test/repository/repository.xml

   current comment: <!-- Example Repository Configuration File -->
   Usage: i assume, that is the one referenced in test/resources/repositoryStubImpl.properties

5) http://svn.apache.org/repos/asf/jackrabbit/trunk/jackrabbit-core/src/test/resources/org/apache/jackrabbit/core/config/repository.xml

   current comment: <!-- Example Repository Configuration File -->
   Usage: ?? 


"
0,"Add private ctors to static utility classes. During development in 3.x and trunk we added some new classes like IOUtils and CodecUtils that are only providing static methods, but have no ctor at all. This adds the default empty public ctor, which is wrong, the classes should never be instantiated.

We should add private dummy ctors to prevent creating instances."
0,Publish the jackrabbit-ocm DTD. The jackrabbit-ocm DTD from jackrabbit-ocm/src/dtd should be made available for reference on the Jackrabbit web site.
0,Rename some remaining tests for new IndexReader class hierarchy. 
0,"Create resource sensitive cache for item states. there is currently a lru-caching strategy for the itemstates in the shared ism, with a hardcoded limit of 1000 entries. the problem is that the size of the states is not respected in the caching strategy; this poses a problem, if the
states are large (i.e. large values in property states, or large number of childnode entries)."
0,"Move backwards compatibility tests to trunk. As discussed on dev@, I'd like to move the backwards compatibility tests from the Jackrabbit sandbox to trunk."
0,"improve arabic analyzer: light8 -> light10. Someone mentioned on the java user list that the arabic analysis was not as good as they would like.

This patch adds the - prefix (light10 algorithm versus light8 algorithm).
In the light10 paper, this improves precision from .390 to .413
They mention this is not statistically significant, but it makes linguistic sense and at least has been shown not to hurt.

In the future, I hope openrelevance will allow us to try some more approaches. 
"
0,"bad assumptions/error handling in SetValueVersionExceptionTest:. SetValueVersionExceptionTest makes several assumptions that may not be true in all repositories:

- nodes can be created without specifying a node type
- nodes are not referenceable by default (thus addMixin fails)

Also, if a repository does not allow creating a reference property, the associated test should be aborted with NotExecutableException.
"
0,Upgrade to Lucene 2.0. We would like to upgrade to Lucene 1.9.1. There are jar conflicts when integrating with other projects such as Liferay Portal --  which uses v 1.9.1.
0,"Change value for SearchIndex#DEFAULT_EXTRACTOR_BACK_LOG. The value is currently 100. This means that once 100 extractor jobs are pending in the indexing queue additional extractor jobs are executed with the current thread. I think it would be more useful to change this value to Integer.MAX_VALUE (or in other words: unbounded).

If the backlog is filled up then this indicates that the repository is very busy and we should not put additional burden on the current thread in that case."
0,"JSR 283 namespace handling. JSR 283 makes namespace handling more flexible and user-friendly (less exceptions, no unexpected mapping changes during a session, etc.). The changes are mostly compliant with JSR 170, so we can implement them already for Jackrabbit 1.x.

TODO: JSR 283 namespace handling + related TCK tests"
0,"set jcr:encoding when importing files into simple webdav server. attached is a patch that sets the jcr:encoding property when importing files into the simple webdav server. it also strips parameters from the content type before setting the jcr:mimetype property.
"
0,"In 3.x branch (starting with 3.4) the IndexFormatTooOldException was backported, but the error message was not modified for 3.x. In 3.x branch (starting with 3.4) the IndexFormatTooOldException was backported, but the error message was not modified for 3.x:

bq. This version of Lucene only supports indexes created with release 3.0 and later.

In 3.x it must be:

bq. This version of Lucene only supports indexes created with release 1.9 and later.

Indexes before 1.9 will throw this exception on reading SegmentInfos (LUCENE-3255)."
0,"FST should allow more than one output for the same input. For the block tree terms dict, it turns out I need this case."
0,"New feature rich higlighter for Lucene.. Well, I refactored (took) some code from two previous highlighters.
This highlighter:
+ use TermPositionVector where available
+ use Analyzer if no TermPositionVector found or is forced to use it.
+ support for all lucene queries (Term, Phrase with slops, Prefix, Wildcard, Range) except Fuzzy Query (can be implemented easly)

- has no support for scoring (yet)
- use same prefix,postfix for accepted terms (yet)

? It's written in Java5

In next release I'd like to add support for Fuzzy, ""coloring"" f.e. diffrent color for terms btw. phrase terms (slops), scoring of fragments

It's apache licensed - I hope so :-) I put licene statement in every file
"
0,"FVH: uncontrollable color tags. The multi-colored tags is a feature of FVH. But it is uncontrollable (or more precisely, unexpected by users) that which color is used for each terms."
0,"LogMergePolicy.findMergesToExpungeDeletes need to get deletes from the SegmentReader. With LUCENE-1516, deletes are carried over in the SegmentReaders
which means implementations of
MergePolicy.findMergesToExpungeDeletes (such as LogMergePolicy)
need to obtain deletion info from the SR (instead of from the
SegmentInfo which won't have the information)."
0,"Provide a non-pooling connection manager. The current implementations of the connection managers all have a connection
pool. For applications requiring only single requests very rarely this is
overkill. We should provide a very simple connection manager that uses a
connection only one time and then closes it right away."
0,"pull CoreReaders out of SegmentReader. Similar to LUCENE-3117, I think we should pull the CoreReaders class out of SR,
to make it easier to navigate the code."
0,"Better error message for non-trivial nodetype changes. Currently Jackrabbit only throws a RepositoryException with the message ""Not yet implemented"". This says almost nothing to newcomers and is very time consuming to debug even if you know what is means. It would be better to use the information already availble through NodeTypeDefDiff to provide a more descriptive error message."
0,"contrib/benchmark unit tests. The need came up in this thread: 
http://www.mail-archive.com/java-dev@lucene.apache.org/msg09260.html

: We might want to start thinking about Unit Tests...  :-)  Seems kind
: of weird to have tests for tests, but this is becoming sufficiently
: complex that it should have some tests.
"
0,"Use StringBuilder instead of StringBuffer in benchmark. Minor change - use StringBuilder instead of StringBuffer in benchmark's code. We don't need the synchronization of StringBuffer in all the places that I've checked.

The only place where it _could_ be a problem is in HtmlParser's API - one method accepts a StringBuffer and it's an interface. But I think it's ok to change benchmark's API, back-compat wise and so I'd like to either change it to accept a String, or remove the method altogether -- no code in benchmark uses it, and if anyone needs it, he can pass StringReader to the other method."
0,"Add a variable-sized int block codec. We already have support for fixed block size int block codecs, making it very simple to create a codec from a int encoder algorithms like FOR/PFOR.

But algorithms like Simple9/16 are not fixed -- they encode a variable number of adjacent ints at once, depending on the specific values of those ints."
0,Allow parent path to be set explicitly in NodeInfoBuilder. Currently there is no way to explicitly set the parent of an NodeInfo build by NodeInfoBuilder. I suggest to add a setParentPath() method to NodeInfoBuilder to fix this.
0,"Fine grained locking in SharedItemStateManager. The SharedItemStateManager (SISM) currently uses a simple read-write lock to ensure data consistency. Store operations to the PersistenceManager (PM) are effectively serialized.

We should think about more sophisticated locking to allow concurrent writes on the PM.

One possible approach:

If a transaction is currently storing data in a PM a second transaction may check if the set of changes does not intersect with the first transaction. If that is the case it can safely store its data in the PM.

This fine grained locking must also be respected when reading from the SISM. A read request for an item that is currently being stored must be blocked until the store is finished."
0,"Line-separator differences cause PredefinedNodeTypeTest to fail on different operating systems.. In testPredefinedNodeType(), the test reads in a test file from the file system and then performs a string comparison, which may fail due to line-separator differences:

    private void testPredefinedNodeType(String name)
            throws NotExecutableException {
        try {
            StringBuffer spec = new StringBuffer();
            String resource =
                ""org/apache/jackrabbit/test/api/nodetype/spec/""
                + name.replace(':', '-') + "".txt"";
            Reader reader = new InputStreamReader(
                    getClass().getClassLoader().getResourceAsStream(resource));
            for (int ch = reader.read(); ch != -1; ch = reader.read()) {
                spec.append((char) ch);
            }

            NodeType type = manager.getNodeType(name);

            assertEquals(
                    ""Predefined node type "" + name,
                    spec.toString(),
                    getNodeTypeSpec(type));
...

The above works when the file being read in has line-separators that match the operating system the test is being run on.  However, if there is a mismatch, the string comparison will fail.

The fix is to replace line-separators in both strings being compared:

Helper method to replace line separators

    /** Standardize line separators around ""\n"". */
    public String replaceLineSeparators(String stringValue) {
        // Replace ""\r\n"" (Windows format) with ""\n"" (Unix format) 
        stringValue = stringValue.replaceAll(""\r\n"", ""\n"");
        // Replace ""\r"" (Mac format) with ""\n"" (Unix format)
        stringValue = stringValue.replaceAll(""\r"", ""\n"");
        
        return stringValue;
    }
    
Updated test method:

    private void testPredefinedNodeType(String name)
            throws NotExecutableException {
        try {
            StringBuffer spec = new StringBuffer();
            String resource =
                ""org/apache/jackrabbit/test/api/nodetype/spec/""
                + name.replace(':', '-') + "".txt"";
            Reader reader = new InputStreamReader(
                    getClass().getClassLoader().getResourceAsStream(resource));
            for (int ch = reader.read(); ch != -1; ch = reader.read()) {
                spec.append((char) ch);
            }

            NodeType type = manager.getNodeType(name);
            
            String nodeTypeSpecValue = replaceLineSeparators(getNodeTypeSpec(type));
            String specValue = replaceLineSeparators(spec.toString());
            
            assertEquals(
                    ""Predefined node type "" + name,
                    specValue,
                    nodeTypeSpecValue);
..."
0,"Database connection pooling. Jackrabbit should use database connection pools instead of a single connection per persistence manager, cluster journal, or database data store."
0,"Remove norms() support from non-atomic IndexReaders. Spin-off from LUCENE-2769:
Currently all IndexReaders support norms(), but the core of Lucene never uses it and its even dangerous because of memory usage. We should do the same like with MultiFields and factor it out and throw UOE on non-atomic readers.

The SlowMultiReaderWrapper can then manage the norms. Also ParallelReader needs to be fixed."
0,"TCK: DocumentViewImportTest does not call refresh after direct-to-workspace import. After performing a direct-to-workspace import, the test does not call refresh to ensure the transient layer doesn't contain stale data.

Proposal: call refresh(false) after performing direct-to-workspace imports.

--- DocumentViewImportTest.java (revision 422074)
+++ DocumentViewImportTest.java (working copy)
@@ -106,6 +106,12 @@
             SAXException, NotExecutableException {
  
         importXML(target, createSimpleDocument(), uuidBehaviour, withWorkspace);
+
+        if (withWorkspace)
+        {
+          session.refresh(false);
+        }
+
         performTests();
     }
  
@@ -127,6 +133,12 @@
             SAXException, IOException, NotExecutableException {
  
         importWithHandler(target, createSimpleDocument(), uuidBehaviour, withWorkspace);
+
+        if (withWorkspace)
+        {
+          session.refresh(false);
+        }
+
         performTests();
     }
"
0,Remove deprecated RangeQuery classes. Remove deprecated RangeQuery classes
0,"DisjunctionScorer. This disjunction scorer can match a minimum nr. of docs, 
it provides skipTo() and it uses skipTo() on the subscorers. 
The score() method is abstract in DisjunctionScorer and implemented 
in DisjunctionSumScorer as an example."
0,"Change the error message in the exception at URIUtils#rewriteURI . The message in URIUtils#rewriteURI is misspelled - ""URI may nor be null"" should be ""URI should not be null"""
0,"Distribution of commons classes. jukka started a discussion regarding distribution of commons classes a while ago:

http://www.mail-archive.com/dev@jackrabbit.apache.org/msg06698.html

"
0,"Improve and promote spi-logger. The spi-logger is very useful for debugging an SPI implementation. However it only supports the RepositoryService interface. Other SPI interfaces are not supported. Also writing log information as string seems a bit restrictive to me. Finally it is not included in the jackrabbit-spi package which introduces an additional dependency for debugging. 

I therefore suggest:
- Add support for the other major SPI interfaces 
- Replace the current way of logging a String to a Writer instance by a more versatile mechanism (i.e. clients have to provide an interface which consumes more structured log data). 
- Provide some default implementation for the above mechanism (i.e. writing to a file, writing to a slf4j based logger, writing to the console...)
- Promote this project to jackrabbit-spi
"
0,"URI reference resolution fails examples in RFC 3986. org.apache.http.client.utils.URIUtils.resolve(final URI baseURI, URI reference) fails to resolve some examples from RFC 3986 section 5.3 correctly. See TestCase."
0,registerNodeType is not implemented in the CLI. registernodetype is listed in the standalone client but not implemented. 
0,"ClientConnectionManager should throw InterruptedException. For historical reasons, ThreadSafeClientConnectionManager throws an IllegalThreadStateException instead of an InterruptedException if the waiting thread is interrupted from outside. This design was chosen since adding InterruptedException to the HttpConnectionManager in 3.x would have broken the API. This is not a concern for HttpClient 4.0.
"
0,Annotation based implementation of jackrabbit ocm. we have created an annotation based implementation of jackrabbit-ocm that can be used instead of the digester one
0,"Webdav: Review usage of command chains. i'd like to review the usage of command chains for import/export within the simple webdav server.

while the concept of command chains offers a lot of flexibility, it showed that the implementation generates some drawbacks. a new mechanism should take advantage of the experiences made with the command chains.

from my point of view the following issues should be taken into consideration:

- provide means to extend and modify the import/export logic with minimal effort

- consistent import/export functionality for both collections and non-collections
  > export/import should not be completely separated.
  > interfaces should encourage consistency
  > increase maintainability, reduce no of errors

- distinction of collections and non-collections for import/export behaviour
  > PUT must result in non-coll, MKCOL in collection

- allow to defined a set of import/export-handlers with a given order.

- the different handlers must not rely on each other.

- an import/export should be completed after the first handler indicates success. there 
  should not be other classes involved in order to complete the import/export.

- avoid huge configuration files and if possible, avoid program flow being defined outside of java code.

- avoid duplicate configuration (e.g. resource-filtering), duplicate code, duplicate logic, that is 
  hard to maintain.

- additonal logic should be defined within a given import/export handler.
  however, in case of webdav i see limited value of using extra logic such as addMixin or checkin, 
  that are covered by  webdav methods (such as LOCK, VERSION-CONTROL or CHECKIN).

regards
angela


"
0,"porting of ProxyClient from 3.1 to 4.x API. With 3.1 version of HttpClient it was possible to establish a tunneled connection to a generic non http server through an authenticated proxy, but since 3.1 version does not support NTLMv2 and Kerberos authentication, that are supported in 4.x version, it is very useful to port the features provided by ProxyClient to 4.x API."
0,"advertise support for RFC4918 (WebDAV) compliance class 3. With the recent changes for PROPFIND/allprop/include (JCR-1769) and the parsing of Destination/If headers (JCR-1770), we can advertise RFC 4918 L3 support (http://greenbytes.de/tech/webdav/rfc4918.html#rfc.section.18.3).

Note we still have test failures for tagged If headers, but this has nothing to do with compliance class 3.

"
0,"Easier way to run benchmark. Move Benchmark.main to a more easily accessible method exec() that can be easily invoked by external programs.
"
0,"re-sync client with changes in core alpha6 snapshot. There have been API changes in core since it's alpha5 release.
Client needs to be adapted so it's alpha2 (snapshot) builds and runs against the current core API.
"
0,"check all tests that use FSDirectory.open. In LUCENE-2471 we were discussing the copyBytes issue, and Shai and I had a discussion about how we could prevent such bugs in the future.

One thing that lead to the bug existing in our code for so long, was that it only happened on windows (e.g. never failed in hudson!)
This was because the bug only happened if you were copying from SimpleFSDirectory, and the test used FSDirectory.open

Today the situation is improving: most tests use newDirectory() which is random by default and never use FSDir.open,
it always uses SimpleFS or NIOFS so that the same random seed will reproduce across both windows and unix.

So I think we need to review all uses of FSDirectory.open in our tests, and minimize these.
In general tests should use newDirectory().
If the test comes with say a zip-file and wants to explicitly open stuff from disk, I think it should open the contents with say SimpleFSDir,
and then call newDirectory(Directory) to copy into a new ""random"" implementation for actual testing. This method already exists:
{noformat}
  /**
   * Returns a new Dictionary instance, with contents copied from the
   * provided directory. See {@link #newDirectory()} for more
   * information.
   */
  public static MockDirectoryWrapper newDirectory(Directory d) throws IOException {
{noformat}
"
0,Reduce memory usage of DocNumberCache. Instead of using the uuid String the a UUID instance should be used as the key in the cache.
0,"[API Doc] Improve the description of the preemptive authentication. HttpClient authentication guide does not reflect the fact that preemptive
authentication requires default credentials to be set. It should also mention
the security implications of preemptive authentication (default credentials sent
with EVERY request to ANY target / proxy server)"
0,"queryparser makes all CJK queries phrase queries regardless of analyzer. The queryparser automatically makes *ALL* CJK, Thai, Lao, Myanmar, Tibetan, ... queries into phrase queries, even though you didn't ask for one, and there isn't a way to turn this off.

This completely breaks lucene for these languages, as it treats all queries like 'grep'.

Example: if you query for f:abcd with standardanalyzer, where a,b,c,d are chinese characters, you get a phrasequery of ""a b c d"". if you use cjk analyzer, its no better, its a phrasequery of  ""ab bc cd"", and if you use smartchinese analyzer, you get a phrasequery like ""ab cd"". But the user didn't ask for one, and they cannot turn it off.

The reason is that the code to form phrase queries is not internationally appropriate and assumes whitespace tokenization. If more than one token comes out of whitespace delimited text, its automatically a phrase query no matter what.

The proposed patch fixes the core queryparser (with all backwards compat kept) to only form phrase queries when the double quote operator is used. 

Implementing subclasses can always extend the QP and auto-generate whatever kind of queries they want that might completely break search for languages they don't care about, but core general-purpose QPs should be language independent.
"
0,"Highlighter should try and use maxDocCharsToAnalyze in WeightedSpanTermExtractor when adding a new field to MemoryIndex as well as when using CachingTokenStream. huge documents can be drastically slower than need be because the entire field is added to the memory index
this cost can be greatly reduced in many cases if we try and respect maxDocCharsToAnalyze

things can be improved even further by respecting this setting with CachingTokenStream

"
0,"ItemManager issues WARN message on Node.checkIn and Node.checkOut. Wenn checking in or checking out a node, the ItemManager.cacheItem method issues a WARN message because a cache entry is being replaced.

While this message might be valuable in certain contexts, in the contetx of checking in or out a node, this is not valuable and harms confidence :-)"
0,"Usage of ""qualified name"" in JavaDoc and Comments. Within jackrabbit the term ""qualified name"" is used, when the internal representation of a JCR name is referred according to the usage of ""fully-qualified name"" within the JSR 170 specification.

Based on input by julian (Issue #449) the phrasing has been adjusted in JSR 283 to match the terminology used in XMLNS.

As of JSR 283

- Qualified Name -> refers to a JCR Name in the form prefix:localname
- Expanded Name -> is used for what was formerly called ""fully-qualified name"".

In order to avoid confusion i would suggest to fix the javadoc throughout the project and eventually add explanations in cases where method or class names are misleading."
0,Discovery of privileges of any set of Principals. jsr 283 defines means to discover the privileges for the editing session. however there is no way to determine the privileges for other principals.
0,"MockDirectoryWrapper should track open file handles of IndexOutput too. MockDirectoryWrapper currently tracks open file handles of IndexInput only. Therefore IO files that are not closed do not fail our tests, which can then lead to test directories fail to delete on Windows. We should make sure all open files are tracked and if they are left open, fail the test. I'll attach a patch shortly."
0,"HTTPClient doesn't send authentication header in threaded environment. Using HTTPClient with multiple threads and basic authentication seems to create a race condition. The request headers sometimes don't contain the authorization entry, which results in a 401 (although the username and password credentials are correctly set). "
0,add framework for performance tests. Add a test suite for running various kinds of performance tests.
0,"QValueFactory improvements. 1) Allow all create methods to throw RepositoryException.

2) Further document that create(value,type) can throw ValueFormatException.

3) Remove special case create(File)
"
0,Improve logging in LazyItemIterator#prefetchNext . When LazyItemIterator#prefetchNext fails an item it should spell out the name of that item in  the respective log message. 
0,"Rename lucene/solr dev jar files to -SNAPSHOT.jar. Currently the lucene dev jar files end with '-dev.jar' this is all fine, but it makes people using maven jump through a few hoops to get the -SNAPSHOT naming convention required by maven.  If we want to publish snapshot builds with hudson, we would need to either write some crazy scripts or run the build twice.

I suggest we switch to -SNAPSHOT.jar.  Hopefully for the 3.x branch and for the /trunk (4.x) branch"
0,"replace text from an online collection (used in few test cases) with text that is surely 100% free.. Text from an online firstaid collection (firstaid . ie . eu . org)  is used as arbitrary text for test documents creation, in:
   o.a.l.analysis.Analyzer.FunctionTestSetup.DOC_TEXT_LINES
   o.a.l.benchmark.byTask.feeds.SimpleDocMaker.DOC_TEXT

I once got this text from Project Gutenberg and was sure that it is free. But now the referred Web site does not seem to respond, and I can no more find that firstaid eBook in the Project Gutenberg site.

Since it doesn't matter what text we use there, I will just replace that with some of my own words..."
0,Return null for optional configuration elements. Two recently introduced configuration elements are optional but the configuration parser still returns an instance when the elements are missing in the configuration. The parser should return null when an element is not there in order to distinguish it from the case where an empty element is present. 
0,"Add JcrUtils.getPropertyTypeNames . for property related ui its common to populate a list with all property type names.
instead of hardcoding at various places that list could be provided by JcrUtils."
0,"Adding norms, properties indexing and writer.infoStream support to benchmark. I would like to add the following support in benchmark:
# Ability to specify whether norms should be stored in the index.
# Ability to specify whether norms should be stored for the body field (assuming norms are usually stored for that field in real life applications, make it explicit)
# Ability to specify an infoStream for IndexWriter
# Ability to specify whether to index the properties returned on DocData (for content sources like TREC, these may include arbitrary <meta> tags, which we may not want to index).

Patch to come shortly."
0,Implement Query.getBindVariableNames(). 
0,"SetValueFormatExceptiontest.testNode() relies on addMixin(), does not allow specification of node type. SetValueFormatExceptiontest.testNode() has two flaws:

- it doesn't cope with cases where the created node already is mix:referenceable.

- it doesn't allow specification of the node typer to be created.
"
0,"remove IndexSearcher.close. Now that IS is never ""heavy"" (since you have to pass in your own IR), IS.close is truly a no-op... I think we should remove it.

"
0,"TestTermInfosReaderIndex failing (always reproducible). Always fails on branch (use reproduce string below):
git clone --depth 1 -b rr git@github.com:dweiss/lucene_solr.git

{noformat}
[junit4] Running org.apache.lucene.codecs.lucene3x.TestTermInfosReaderIndex
[junit4] FAILURE 0.04s J0 | TestTermInfosReaderIndex.testSeekEnum
[junit4]    > Throwable #1: java.lang.AssertionError: expected:<field9:z91ob3wozm6d> but was:<:>
[junit4]    > 	at __randomizedtesting.SeedInfo.seed([C7597DFBBE0B3D7D:C6D9CEDD0700AAFF]:0)
[junit4]    > 	at org.junit.Assert.fail(Assert.java:93)
[junit4]    > 	at org.junit.Assert.failNotEquals(Assert.java:647)
[junit4]    > 	at org.junit.Assert.assertEquals(Assert.java:128)
[junit4]    > 	at org.junit.Assert.assertEquals(Assert.java:147)
[junit4]    > 	at org.apache.lucene.codecs.lucene3x.TestTermInfosReaderIndex.testSeekEnum(TestTermInfosReaderIndex.java:137)
[junit4]    > 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[junit4]    > 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
[junit4]    > 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
[junit4]    > 	at java.lang.reflect.Method.invoke(Method.java:597)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1766)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner.access$1000(RandomizedRunner.java:141)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:728)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:789)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:803)
[junit4]    > 	at org.apache.lucene.util.LuceneTestCase$SubclassSetupTeardownRule$1.evaluate(LuceneTestCase.java:744)
[junit4]    > 	at org.apache.lucene.util.LuceneTestCase$InternalSetupTeardownRule$1.evaluate(LuceneTestCase.java:636)
[junit4]    > 	at org.apache.lucene.util.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:22)
[junit4]    > 	at org.apache.lucene.util.LuceneTestCase$TestResultInterceptorRule$1.evaluate(LuceneTestCase.java:550)
[junit4]    > 	at org.apache.lucene.util.LuceneTestCase$RememberThreadRule$1.evaluate(LuceneTestCase.java:600)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:735)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner.access$600(RandomizedRunner.java:141)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner$3$1.run(RandomizedRunner.java:586)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:605)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:641)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:652)
[junit4]    > 	at org.apache.lucene.util.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:22)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner.runSuite(RandomizedRunner.java:533)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner.access$400(RandomizedRunner.java:141)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner$2.run(RandomizedRunner.java:479)
[junit4]    > 
[junit4]   2> NOTE: reproduce with: ant test -Dtests.filter=*.TestTermInfosReaderIndex -Dtests.filter.method=testSeekEnum -Drt.seed=C7597DFBBE0B3D7D -Dargs=""-Dfile.encoding=UTF-8""
[junit4]   2>
[junit4]    > (@AfterClass output)
[junit4]   2> NOTE: test params are: codec=Appending, sim=DefaultSimilarity, locale=en, timezone=Atlantic/Stanley
[junit4]   2> NOTE: all tests run in this JVM:
[junit4]   2> [TestLock, TestFileSwitchDirectory, TestWildcardRandom, TestVersionComparator, TestTermdocPerf, TestBitVector, TestParallelTermEnum, TestSimpleSearchEquivalence, TestNumericRangeQuery64, TestSort, TestIsCurrent, TestToken, TestIntBlockCodec, TestDocumentsWriterDeleteQueue, TestPagedBytes, TestThreadedForceMerge, TestOmitTf, TestSegmentTermEnum, TestIndexWriterConfig, TestCheckIndex, TestTermVectorsWriter, TestNumericTokenStream, TestSearchAfter, TestRegexpQuery, InBeforeClass, InAfterClass, InTestMethod, NonStringProperties, TestIndexWriterMergePolicy, TestVirtualMethod, TestFieldCache, TestSurrogates, TestSegmentTermDocs, TestMultiValuedNumericRangeQuery, TestBasicOperations, TestCodecs, TestDateSort, TestPositiveScoresOnlyCollector, TestBooleanQuery, TestIndexInput, TestMinimize, TestNumericRangeQuery32, TestBoolean2, TestSloppyPhraseQuery, TestNoDeletionPolicy, TestFieldCacheTermsFilter, TestRandomStoredFields, TestDocBoost, TestTransactionRollback, TestUnicodeUtil, TestIndexWriterLockRelease, TestUTF32ToUTF8, TestFixedBitSet, TestDoubleBarrelLRUCache, TestTimeLimitingCollector, TestSpanFirstQuery, TestDirectory, TestSpansAdvanced2, TestConcurrentMergeScheduler, TestIndexWriterExceptions, TestDocValues, TestCustomNorms, TestFieldValueFilter, TestTermVectors, TestTermInfosReaderIndex]
[junit4]   2> NOTE: Linux 2.6.32-38-server amd64/Sun Microsystems Inc. 1.6.0_20 (64-bit)/cpus=4,threads=1,free=100102360,total=243859456
[junit4]   2> 
{noformat}"
0,"Add contrib/fast-vector-highlighter to Maven central repo. I'm not at all familiar with the Lucene build/deployment process, but it would be very nice if releases of the fast vector highlighter were pushed to the maven central repository, as is done with other contrib modules.

(Issue filed at the request of Grant Ingersoll.)"
0,"Make the Payload Boosting Queries consistent. BoostingFunctionTermQuery should be consistent with BoostingNearQuery -

Renaming to PayloadNearQuery and PayloadTermQuery"
0,Reduce number of compiler warning by adding @Override and generics where appropriate . Add @Override and generics where possible to reduce the number of warnings issued by the compiler.
0,"CloseableThreadLocal is now obsolete. Since Lucene 3 depends on Java 5, we can use ThreadLocal#remove() to take care or resource management."
0,"ChildNodeEntriesImpl.update logs incorrect errors. The ChildNodeEntriesImpl logs errors on a correct update.

""ChildInfo iterator contains multiple entries with the same name|index or uniqueID -> ignore ChildNodeInfo.""  (line 186)"
0,"Hostname verification:  turn off wildcards when CN is an IP address. Hostname verification:   turn off wildcards when CN is an IP address.  This is a further improvement on HTTPCLIENT-613 and HTTPCLIENT-614.

Example - don't allow:
CN=*.114.102.2

I'm thinking of grabbing the substring following the final dot, and running it through ""Integer.parseInt()"".  If the NumberFormatException isn't thrown (so Integer.parseInt() actually worked!), then I'll turn off wildcard matching.  Notice that this won't be a problem with IP6 addresses, since they don't use dots.  It's only a problem with IP4, where the meaning of the dots clashes with dots in domain names.

Note:  when I turn off wildcard matching, I still attempt an exact match with the hostname.  If through some weird mechanism the client is actually able to use a hostname such as ""https://*.114.102.2/"", then they will be okay if that's what the certificate on the server contains."
0,"Redundant calls to RepositoryService.getChildInfos. In some cases jcr2spi issues calls to RepositoryService.getChildInfos for items which haven been returned by the last call to RepositoryService.getItemInfos. 

This happens because WorkspaceItemStateFactory.createDeepPropertyState is asked to create the node states for all items returned by RepositoryService.getChildInfos in the order they are returned by the Iterator. When trying to create an item state for an item which is deeper down the hierarchy than another item which comes later in the iterator, a call to RepositoryService.getChildInfos is issued for the latter. "
0,"Includes new (old) mimetypes that OpenOfficeTextExtractor can handle. The following patch adds the old openoffice (1.0 version) mimetypes to have their contents extracted. 
I've tested with simple files and it worked here. 


$ cat OpenOfficeTextExtractor-mimetype.patch
--- jackrabbit-1.4/jackrabbit-text-extractors/src/main/java/org/apache/jackrabbit/extractor/OpenOfficeTextExtractor.java    2007-12-19 12:57:58.000000000 -0200
+++ jackrabbit-1.4-modified/jackrabbit-text-extractors/src/main/java/org/apache/jackrabbit/extractor/OpenOfficeTextExtractor.java  2008-07-24 15:01:08.000000000 -0300
@@ -54,7 +54,11 @@
                            ""application/vnd.oasis.opendocument.graphics"",
                            ""application/vnd.oasis.opendocument.presentation"",
                            ""application/vnd.oasis.opendocument.spreadsheet"",
-                           ""application/vnd.oasis.opendocument.text""});
+                           ""application/vnd.oasis.opendocument.text"",
+                           ""application/vnd.sun.xml.calc"",
+                           ""application/vnd.sun.xml.draw"",
+                           ""application/vnd.sun.xml.impress"",
+                           ""application/vnd.sun.xml.writer""});
     }

     //-------------------------------------------------------< TextExtractor >
"
0,Namespace handling in AbstractSession should be synchronized. The AbstractSession base class in o.a.j.commons implicitly assume that the session is never accessed concurrently from more than one thread and thus doesn't synchronize access to the namespace map. This causes problems when the session *is* accessed concurrently. Instead of relying on client code we should enforce thread-safety by explicitly synchronizing potentially unsafe operations on the session instance.
0,Use creation tick instead of weak references in DocNumberCache. This avoids the need to keep two maps in CachingMultiIndexReader. And I guess the fewer weak references the better...
0,"Update slf4j. Please update slf4j from 1.3.0 to 1.5.2.

jcl104-over-slf4j has been renamed as jcl-over-slf4j, so if one uses a recent version, he has to exclude jcl104-over-slf4j for every jackrabbit dependency, which is quite a pain...

No impact observed.

Best regards,

Stephane Landelle"
0,"IOUtils.closeSafely should log suppressed Exceptions in stack trace of original Exception (a new feature of Java 7). I was always against Java 6 support, as it brings no really helpful new features into Lucene. But there are several things that make life easier in coming Java 7 (hopefully on July 28th, 2011). One of those is simplier Exception handling and suppression on Closeable, called ""Try-With-Resources"" (see http://docs.google.com/View?id=ddv8ts74_3fs7483dp, by the way all Lucene classes support these semantics in Java 7 automatically, the cool try-code below would work e.g. for IndexWriter, TokenStreams,...).

We already have this functionality in Lucene since adding the IOUtils.closeSafely() utility (which can be removed when Java 7 is the minimum requirement of Lucene - maybe in 10 years):

{code:java}
try (Closeable a = new ...; Closeable b = new ...) {
  ... use Closeables ...
} catch (Exception e) {
  dosomething;
  throw e;
}
{code}

This code will close a and b in an autogenerated finally block and supress any exception. This is identical to our IOUtils.closeSafely:

{code:java}
Exception priorException = null;
Closeable a,b;
try (Closeable a = new ...; Closeable b = new ...) {
  a = new ...;
  b = new ...
  ... use Closeables ...
} catch (Exception e) {
  priorException = e;
  dosomething;
} finally {
  IOUtils.closeSafely(priorException, a, b);
}
{code}

So this means we have the same functionality without Java 7, but there is one thing that makes logging/debugging much nicer:
The above Java 7 code also adds maybe suppressed Exceptions in those Closeables to the priorException, so when you print the stacktrace, it not only shows the stacktrace of the original Exception, it also prints all Exceptions that were suppressed to throw this Exception (all Closeable.close() failures):

{noformat}
org.apache.lucene.util.TestIOUtils$TestException: BASE-EXCEPTION
    at org.apache.lucene.util.TestIOUtils.testSuppressedExceptions(TestIOUtils.java:61)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:601)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1486)
    at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1404)
    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
    at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
    at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:768)
    Suppressed: java.io.IOException: TEST-IO-EXCEPTION-1
            at org.apache.lucene.util.TestIOUtils$BrokenCloseable.close(TestIOUtils.java:36)
            at org.apache.lucene.util.IOUtils.closeSafely(IOUtils.java:58)
            at org.apache.lucene.util.TestIOUtils.testSuppressedExceptions(TestIOUtils.java:62)
            ... 26 more
    Suppressed: java.io.IOException: TEST-IO-EXCEPTION-2
            at org.apache.lucene.util.TestIOUtils$BrokenCloseable.close(TestIOUtils.java:36)
            at org.apache.lucene.util.IOUtils.closeSafely(IOUtils.java:58)
            at org.apache.lucene.util.TestIOUtils.testSuppressedExceptions(TestIOUtils.java:62)
            ... 26 more
{noformat}

For this in Java 7 a new method was added to Throwable, that allows logging such suppressed Exceptions (it is called automatically by the synthetic bytecode emitted by javac). This patch simply adds this functionality conditionally to IOUtils, so it ""registers"" all suppressed Exceptions, if running on Java 7. This is done by reflection: once it looks for this method in Throwable.class and if found, it invokes it in closeSafely, so the exceptions thrown on Closeable.close() don't get lost.

This makes debugging much easier and logs all problems that may occur.

This patch does *not* change functionality or behaviour, it just adds more nformation to the stack trace in a Java-7-way (similar to the way how Java 1.4 added causes). It works here locally on Java 6 and Java 7, but only Java 7 gets the additional stack traces. For Java 6 nothing changes. Same for Java 5 (if we backport to 3.x).

This would be our first Java 7 improvement (a minor one). Next would be NIO2... - but thats not easy to do with reflection only, so we have to wait 10 years :-)"
0,"IndexWriter#addIndexesNoOptimize has redundent try/catch. With the new transaction code, the try/catch clause at the beginning of IndexWriter#addIndexesNoOptimize is redundant."
0,Remove deprecated classes in jackrabbit-webdav and the corresponding impls in jcr-server. 
0,"Implement a cache to perform real request only when needed. Browsers may cache received content according to the values of different
response headers. It would be great if HttpClient could do the same."
0,"[OCM] rename o.a.j.ocm.persistence.PersistenceManager to avoid confusion with core component. PersistenceManager is a well known and established interface in jackrabbit's architecture. the same-named class in the
jcr-mapping contrib project should IMO be renamed in order to avoid confusion in mailing list threads and jira issues,"
0,"[PATCH] Use entrySet iterators to avoid map look ups in loops. Code uses a keySet iterator in a loop, then does a map look up using the key retrieved from the iterator. 

Might as well use an entrySet iterator to avoid n map lookups.

Patch does this."
0,"Remove unnecessary String concatenation in IndexWriter. I've noticed a couple of places in IndexWriter where a boolean string is created by bool + """", or integer by int + """". There are some places (in setDiagonstics) where a string is concatenated with an empty String ...
The patch uses Boolean.toString and Integer.toString, as well as remove the unnecessary str + """"."
0,Evaluate if membershipcache (JCR-2703) obsoletes the cache in DefaultPrincipalProvider. 
0,[PATCH] add term index interval accessors. It should be possible for folks to set the index interval used when writing indexes.
0,"jcr-server should honor a webdav request's Content-Type and Content-Language headers. when processing a PUT or a POST, the DavResource should have access to the Content-Type and Content-Language headers presented in the webdav request, if any.

when the client explicitly communicates these headers, their values should take priority over server calculations (such as that done in SetContentTypeCommand), or at least be input into server calculations

furthermore, the dav getcontentlanguage is not at all supported by at least the simple webdav server.
"
0,"JSR 283: Identifier based event filtering. JSR 283 PFD states:

Only events whose associated parent node has one of the identifiers in the uuid String array will be received. If this parameter is null then no identifier-related restriction is placed on events received. Note that specifying an empty array instead of null results in no nodes being listened to. The uuid is used for backwards compatibility with JCR 1.0.
"
0,"ChineseFilter is inefficient. trivial patch to use CharArraySet, so it can use termBuffer() instead of term()
"
0,"MergeException from CMS threads should record the Directory. When you hit an unhandled exception in ConcurrentMergeScheduler, it
throws a MergePolicy.MergeException, but there's no easy way to figure
out which index caused this (if you have more than one).

I plan to add the Directory to the MergeException.  I also made a few
other small changes to ConcurrentMergeScheduler:

  * Added handleMergeException method, which is called on exception,
    so that you can subclass ConcurrentMergeScheduler to do something
    when an exception occurs.

  * Added getMergeThread() method so you can override how the threads
    are created (eg, if you want to make them in a different thread
    group, use a pool, change priorities, etc.).

  * Added doMerge(...) to actually do this merge, so you can do
    something before starting and after finishing a merge.

  * Changed private -> protected on a few attrs

I plan to commit in a day or two.
"
0,"Built-in way to do auto-retry for certain status codes. The HttpRequestRetryHandler mechanism is great.  It allows API users to plug in their own logic to control whether or not a retry should automatically be done, how many times it should be retried, etc.  That works perfectly in scenarios where an exception is caught while issuing the request.  It falls short, however, in this scenario...

If I'm hitting a service that returns a 503, I want to be able to retry that request automatically as well.  As of right now, I need to write my own logic to accomplish that, and it's clunky trying to integrate it with the httpClient.execute() call, since it's my ResponseHandler impl that ends up getting the 503.  I can see use cases for auto-retrying upon getting other HTTP statuses as well, not just 503.

My request here is...I would love to be able to configure, either on the HttpClient itself or on a wrapping class or something, that a request should automatically be retried if the HTTP status code is among of a set of statuses that I configure.  It would be nice if you could set the max # of retries, an optional sleep time in between retries (perhaps optional incremental backoff if you want to get fancy).  I'm not sure if this is possible, but it would be nice if -- when this type of status-based retry is enabled -- my ResponseHandler wouldn't even get invoked until retry was successful.

Here's an alternative suggestion, possibly simpler to build, but definitely not as elegant:

In my ResponseHandler, you could throw a RetryRequestException or something like that, and the calling code would catch that and do as expected.  That might simplify the mechanism so to speak.

Anyway, I would love not to have to roll my own retry code, since I suspect this is something that hundreds (thousands?) of HttpClient users have had to code.  Seems like providing a standardized, well-written way to do it would go a long way to helping many coders out there.

Thanks!"
0,"Remove RepositoryService.getRootId(). For consistency reasons jcr2spi should use idFactory.createNodeId((String) null, pathFactory.getRootPath()) everywhere to build the NodeId of the root node. having two separate methods is confusing."
0,"add @Deprecated annotations. as discussed on LUCENE-2084, I think we should be consistent about use of @Deprecated annotations if we are to use it.

This patch adds the missing annotations... unfortunately i cannot commit this for some time, because my internet connection does not support heavy committing (it is difficult to even upload a large patch).

So if someone wants to take it, have fun, otherwise in a week or so I will commit it if nobody objects.
"
0,"Fixed README.txt on textfilters project. - Fixed a little mistake: changed org.apache.jackrabbit.core.query..RTFTextFilter to org.apache.jackrabbit.core.query.RTFTextFilter

- Added the OpenOfficeTextFilter to the sample configuration line."
0,"indexing-rules should allow wildcards for (global) property names. eg:

<indexing-rule nodeType=""*"">
  <property>text</property>
  <property>*Text</property>
</indexing-rule>

defines that all properties named 'text' and all that end with 'Text' should be fulltext indexed.
if the property name includes namespace prefixes, wildcards are only allowed for 'any' namespace. eg:

*:title

but not: j*:title
"
0,"Unnecessary parameter in NodeTypeRegistry.persistCustomNodeTypeDefs(NodeTypeDefStore store). The parameter ""store"" is not used within this method. I suggest to delete the parameter to make the code more readable.

"
0,Some Workspace tests require a second workspace. Some workspace test require a second workspace even though it is not used in the test cases.
0,"Lazy Atomic Loading Stopwords in SmartCN . The default constructor in SmartChineseAnalyzer loads the default (jar embedded) stopwords each time the constructor is invoked. 
This should be atomically loaded only once in an unmodifiable set.

"
0,"Add WaitForMergesTask. When building an index, if you just .close the IW, you may leave merges still needing to be done... so a WaitForMerges task lets algs fix this."
0,"use the internal CND file for builtin nodetypes. the jackrabbit node type registry is reading the built in node types from a XML file.
since the CND (compact node type definition notation) is now specified by jsr283,
i would like to drop the builtin .xml file and read the builtin node typesonly from the .cnd file.
this certainly helps the developers. furthermore, all the node types in jsr283 are now speced
in CND, and converting them to XML is a pain and error prone."
0,"Remove write access from SegmentReader and possibly move to separate class or IndexWriter/BufferedDeletes/.... After LUCENE-3606 is finished, there are some TODOs:

SegmentReader still contains (package-private) all delete logic including crazy copyOnWrite for validDocs Bits. It would be good, if SegmentReader itsself could be read-only like all other IndexReaders.

There are two possibilities to do this:
# the simple one: Subclass SegmentReader and make a RWSegmentReader that is only used by IndexWriter/BufferedDeletes/... DirectoryReader will only use the read-only SegmentReader. This would move all TODOs to a separate class. It's reopen/clone method would always create a RO-SegmentReader (for NRT).
# Remove all write and commit stuff from SegmentReader completely and move it to IndexWriter's readerPool (it must be in readerPool as deletions need a not-changing view on an index snapshot).

Unfortunately the code is so complicated and I have no real experience in those internals of IndexWriter so I did not want to do it with LUCENE-3606, I just separated the code in SegmentReader and marked with TODO. Maybe Mike McCandless can help :-)"
0,Support grouping by IndexDocValues. Although IDV is not yet finalized (More particular the SortedSource). I think we already can discuss / investigate implementing grouping by IDV.
0,"Can't build lucene 06/13/2004 CVS under jdk 1.5.0. [root@shilo jakarta-lucene]# /usr/local/ant/bin/ant build
Buildfile: build.xml

BUILD FAILED
Target `build' does not exist in this project. 

Total time: 1 second
[root@shilo jakarta-lucene]# /usr/local/ant/bin/ant 
Buildfile: build.xml

init:
    [mkdir] Created dir: /usr/src/jakarta-lucene/build
    [mkdir] Created dir: /usr/src/jakarta-lucene/dist

compile-core:
    [mkdir] Created dir: /usr/src/jakarta-lucene/build/classes/java
    [javac] Compiling 160 source files to /usr/src/jakarta-
lucene/build/classes/java
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/FilterIndexReader.java:42: as of 
release 1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]     public void seek(TermEnum enum) throws IOException { in.seek
(enum); }
    [javac]                               ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/FilterIndexReader.java:42: as of 
release 1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]     public void seek(TermEnum enum) throws IOException { in.seek
(enum); }
    [javac]                                                                  ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/SegmentTermDocs.java:55: as of release 
1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]   public void seek(TermEnum enum) throws IOException {
    [javac]                             ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/SegmentTermDocs.java:59: as of release 
1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]     if (enum instanceof SegmentTermEnum && ((SegmentTermEnum) 
enum).fieldInfos == parent.fieldInfos)          // optimized case
    [javac]         ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/SegmentTermDocs.java:59: as of release 
1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]     if (enum instanceof SegmentTermEnum && ((SegmentTermEnum) 
enum).fieldInfos == parent.fieldInfos)          // optimized case
    [javac]                                                               ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/SegmentTermDocs.java:60: as of release 
1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]       ti = ((SegmentTermEnum) enum).termInfo();
    [javac]                               ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/SegmentTermDocs.java:62: as of release 
1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]       ti = parent.tis.get(enum.term());
    [javac]                           ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:63: as of release 
1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]     SegmentTermEnum enum = (SegmentTermEnum)enumerators.get();
    [javac]                     ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:64: as of release 
1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]     if (enum == null) {
    [javac]         ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:65: enum types 
must not be local
    [javac]       enum = terms();
    [javac]       ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:65: <identifier> 
expected
    [javac]       enum = terms();
    [javac]            ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:65: '{' expected
    [javac]       enum = terms();
    [javac]                     ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:66: <identifier> 
expected
    [javac]       enumerators.set(enum);
    [javac]                      ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:66: ';' expected
    [javac]       enumerators.set(enum);
    [javac]                       ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:66: <identifier> 
expected
    [javac]       enumerators.set(enum);
    [javac]                           ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:66: '{' expected
    [javac]       enumerators.set(enum);
    [javac]                            ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:68: illegal start 
of type
    [javac]     return enum;
    [javac]     ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:68: as of release 
1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]     return enum;
    [javac]            ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:75: illegal start 
of expression
    [javac]   private final void readIndex() throws IOException {
    [javac]   ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:199: ';' expected
    [javac] }
    [javac] ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:200: '}' expected
    [javac] ^
    [javac] 21 errors

BUILD FAILED
/usr/src/jakarta-lucene/build.xml:140: Compile failed; see the compiler error 
output for details.

Total time: 4 seconds
[root@shilo jakarta-lucene]#"
0,"QueryParser support for MatchAllDocs. It seems like there really should be QueryParser support for MatchAllDocsQuery.
I propose *:* (brings back memories of DOS :-)
"
0,"Stop storing TermsEnum in CloseableThreadLocal inside Terms instance. We have sugar methods in Terms.java (docFreq, totalTermFreq, docs,
docsAndPositions) that use a saved thread-private TermsEnum to do the
lookups.

But on apps that send many threads through Lucene, and/or have many
segments, this can add up to a lot of RAM, especially if the codecs
impl holds onto stuff.

Also, Terms has a close method (closes the CloseableThreadLocal) which
must be called, but we fail to do so in some places.

These saved enums are the cause of the recent OOME in TestNRTManager
(TestNRTManager.testNRTManager -seed
2aa27e1aec20c4a2:-4a5a5ecf46837d0e:-7c4f651f1f0b75d7 -mult 3
-nightly).

Really sharing these enums is a holdover from before Lucene queries
would share state (ie, save the TermState from the first pass, and use
it later to pull enums, get docFreq, etc.).  It's not helpful anymore,
and it can use gobbs of RAM, so I'd like to remove it.
"
0,"Make observation polling time configurable. Currently the polling time is hard coded to 3 seconds in org.apache.jackrabbit.jcr2spi.WorkspaceImpl. I suggest to make it configurable similar to CacheBehaviour. That is, add a respective setting to org.apache.jackrabbit.jcr2spi.config.RepositoryConfig"
0,"InternalValue refactoring. Now that we have use the value factory from spi-commons, the InternalValue code should be simplified."
0,Some implementations require a save() after a mixin has been assigned. Some test cases do not call save() after a mixin has been added.
0,NodeIndexer creates unnecessary string representation of Name value. NodeIndexer.addNameValue() calls Name.toString() but never uses it.
0,"Could we get a way to know if the response has been served from the cache or not ?. Is there a way to know if the response has been served from the cache or not ?
That's an information which might be useful for monitoring the activity of the cache.

If there's no current way, maybe a flag could be added in the request context whenever the response comes from the cache ... ?

"
0,"Test cases not fully initialized at first run. It seems that some test cases in o.a.j.test.api.query and o.a.j.test.api do not properly initialized the test repository before running tests against it. The repository gets initialized by other test cases, and later runs of the troublesome tests report no errors.

Thus the problem appears right after a fresh checkout and a tree cleanup. The command sequence below illustrates this problem. The error messages reported by the initial test runs are ""Workspace does not contain test data at: /testdata/query"" and ""Workspace does not contain test data at: /testdata"".

I tried tracing the cause of this problem, but couldn't find it easily as I'm not yet too familiar with the test setup.

$ svn co
$ maven test

    [junit] Running org.apache.jackrabbit.test.api.query.TestAll
    [junit] Tests run: 77, Failures: 30, Errors: 0, Time elapsed: 5,333 sec
    [junit] [ERROR] TEST org.apache.jackrabbit.test.api.query.TestAll FAILED
    [junit] Running org.apache.jackrabbit.test.api.TestAll
    [junit] Tests run: 534, Failures: 181, Errors: 0, Time elapsed: 16,105 sec
    [junit] [ERROR] TEST org.apache.jackrabbit.test.api.TestAll FAILED

$ maven test

    [junit] Running org.apache.jackrabbit.test.api.query.TestAll
    [junit] Tests run: 77, Failures: 0, Errors: 0, Time elapsed: 5,887 sec
    [junit] Running org.apache.jackrabbit.test.api.TestAll
    [junit] Tests run: 534, Failures: 0, Errors: 0, Time elapsed: 18,427 sec

$ maven clean
$ maven test

    [junit] Running org.apache.jackrabbit.test.api.query.TestAll
    [junit] Tests run: 77, Failures: 30, Errors: 0, Time elapsed: 13,185 sec
    [junit] [ERROR] TEST org.apache.jackrabbit.test.api.query.TestAll FAILED
    [junit] Running org.apache.jackrabbit.test.api.TestAll
    [junit] Tests run: 534, Failures: 181, Errors: 0, Time elapsed: 40,42 sec
    [junit] [ERROR] TEST org.apache.jackrabbit.test.api.TestAll FAILED

$ maven test

    [junit] Running org.apache.jackrabbit.test.api.query.TestAll
    [junit] Tests run: 77, Failures: 0, Errors: 0, Time elapsed: 5,942 sec
    [junit] Running org.apache.jackrabbit.test.api.TestAll
    [junit] Tests run: 534, Failures: 0, Errors: 0, Time elapsed: 17,797 sec
"
0,"Ability to ignore (reject) cookies altogether. I was looking for a way to ignore cookies altogether, but there doesn't appear
to be one.  I could definitely use this capability right now, and I can see
others making use of it at times."
0,UserManagement: Limit set of properties exposed by AuthorizableImpl. AuthorizableImpl currently exposes all properties present on the Node representing the user/group. This should be limited to the properties set through the API (i.e. the non-protected props defined by the rep:Authorizable node type)
0,"QueryManagerImpl hardwires supported query languages. QueryManagerImpl hardwires supported query languages. This seems to be sub-optimal, given the fact that Jackrabbit has a pluggable architecture for additional query languages.
"
0,"javacc-maven-plugin version in jackrabbit-core pom file. Hi, I noticed that the pom.xml file of the jackrabbit-core project needs to specify version ""2.1"" for the javacc-maven-plugin because if it takes the 2.2-SNAPSHOT it won't compile. I put the 2.1 version and it worked fine.

<plugin>
        <groupId>org.codehaus.mojo</groupId>
        <artifactId>javacc-maven-plugin</artifactId>
        <version>2.1</version>
        <executions>


Im working with revision 529712 [April 17, 2007, 15:05 EST]"
0,"All Tokenizer implementations should have constructors that take AttributeSource and AttributeFactory. I have a TokenStream implementation that joins together multiple sub TokenStreams (i then do additional filtering on top of this, so i can't just have the indexer do the merging)

in 2.4, this worked fine.
once one sub stream was exhausted, i just started using the next stream 

however, in 2.9, this is very difficult, and requires copying Term buffers for every token being aggregated

however, if all the sub TokenStreams share the same AttributeSource, and my ""concat"" TokenStream shares the same AttributeSource, this goes back to being very simple (and very efficient)


So for example, i would like to see the following constructor added to StandardTokenizer:
{code}
  public StandardTokenizer(AttributeSource source, Reader input, boolean replaceInvalidAcronym) {
    super(source);
    ...
  }
{code}

would likewise want similar constructors added to all Tokenizer sub classes provided by lucene
"
0,"authentication order has changed from 1.4.x to 1.5.x. In 1.4.x inside RepositoryImpl.login(...) at first the local configuration is checked for configured LoginModules and after it was unsuccessful, the JAAS component is asked:

          AuthContext authCtx;
            LoginModuleConfig lmc = repConfig.getLoginModuleConfig();
            if (lmc == null) {
                        authCtx = new AuthContext.JAAS(repConfig.getAppName(), credentials);
            } else {
...

With 1.5.x this behaviour has moved to SimpleSecurityManager.init(..) and is changed:
        LoginModuleConfig loginModConf = config.getLoginModuleConfig();
        authCtxProvider = new AuthContextProvider(config.getAppName(), loginModConf);
        if (authCtxProvider.isJAAS()) {
            log.info(""init: using JAAS LoginModule configuration for "" + config.getAppName());
        } else if (authCtxProvider.isLocal()) {
...

The problem is with JBoss JAAS implemantation, that authCtxProvider.isJAAS()  is always true.
Because for any reason, the result of Configuration.getAppConfigurationEntry(appName) is never empty,
when a jaas.config is specified for Liferay. Using different appName takes no effect, always the configuration inside the jaas.config is used.

I think still first the local configuration should be concerned, before using JAAS."
0,"Deprecate and remove ShingleMatrixFilter. Spin-off from LUCENE-1391: This filter is unmainatined and no longer up-to-date, has bugs nobody understands and does not work with attributes.

This issue deprecates it as of Lucene 3.1 and removes it from trunk."
0,"DbClusterTest failure due to network configuration. As reported by Serge, the DbClusterTest case fails when run with certain network configuration.

Thomas already suggested a fix:

### Eclipse Workspace Patch 1.0
#P jackrabbit-core
Index: src/test/java/org/apache/jackrabbit/core/cluster/DbClusterTest.java
===================================================================
--- 
src/test/java/org/apache/jackrabbit/core/cluster/DbClusterTest.java (revisi
on 1067983)
+++ 
src/test/java/org/apache/jackrabbit/core/cluster/DbClusterTest.java (workin
g copy)
@@ -37,9 +37,9 @@
     public void setUp() throws Exception {
         deleteAll();
         server1 = Server.createTcpServer(""-tcpPort"", ""9001"", ""-baseDir"",
-                ""./target/dbClusterTest/db1"").start();
+                ""./target/dbClusterTest/db1"", ""-tcpAllowOthers"").start();
         server2 = Server.createTcpServer(""-tcpPort"", ""9002"", ""-baseDir"",
-                ""./target/dbClusterTest/db2"").start();
+                ""./target/dbClusterTest/db2"", ""-tcpAllowOthers"").start();
         FileUtils.copyFile(
                 new
File(""./src/test/resources/org/apache/jackrabbit/core/cluster/repository-h2
.xml""),
                 new File(""./target/dbClusterTest/node1/repository.xml""));
"
0,"MockDirectoryWrapper should wrap the lockfactory. After applying the patch from LUCENE-3147, I added a line to make the test fail if it cannot remove its temporary directory.

I ran 'ant test' on linux 50 times, and it passed all 50 times.
But on windows, it failed often because of write.lock... this is because of unclosed writers in the test.

MockDirectoryWrapper is currently unaware of this write.lock, I think it should wrap the lockfactory so that .close() will fail if there are any outstanding locks.
Then hopefully these tests would fail on linux too.
"
0,"New Jackrabbit site skin. Some time ago Michael Eppelheimer from Day created a new skin for the Jackrabbit web site, and I've now streamlined it a bit and integrated it with the Maven site build mechanism.

The templates should be easy to adapt also for Confluence when we get around to that migration."
0,make spi query code compatible with JCR 2.0. SPI-Commons currently has it's own outdated copy of the new JCR 2.0 query interfaces.
0,JSR 283: Shareable nodes support in query. 
0,"Make MMapDirectory.MAX_BBUF user configureable to support chunking the index files in smaller parts. This is a followup for java-user thred: http://www.lucidimagination.com/search/document/9ba9137bb5d8cb78/oom_with_2_9#9bf3b5b8f3b1fb9b

It is easy to implement, just add a setter method for this parameter to MMapDir."
0,"Re-add SorterTemplate and use it to provide fast ArraySorting and replace BytesRefHash sorting. This patch adds back an optimized and rewritten SorterTemplate back to Lucene (removed after release of 3.0). It is of use for several components:

- Automaton: Automaton needs to sort States and other things. Using Arrays.sort() is slow, because it clones internally to ensure stable search. This component is much faster. This patch adds Arrays.sort() replacements in ArrayUtil that work with natural order or using a Comparator<?>. You can choose between quickSort and mergeSort.
- BytesRefHash uses another QuickSort algorithm without insertionSort for very short ord arrays. This class uses SorterTemplate to provide the same with insertionSort fallback in a very elegant way. Ideally this class can be used everywhere, where the sort algorithm needs to be separated from the underlying data and you can implement a swap() and compare() function (that get slot numbers instead of real values). This also applies to Solr (Yonik?).

SorterTemplate provides quickSort and mergeSort algorithms. Internally for short arrays, it automatically chooses insertionSort (like JDK's Arrays). The quickSort algorith was copied modified from old BytesRefHash. This new class only shares MergeSort with the original CGLIB SorterTemplate, which is no longer maintained."
0,Set jcr and servlet-api dependency scope to provided. The jcr and servlet API libraries should typically be provided by the deployment environment and not included as compile/runtime dependencies of Jackrabbit artifacts. The scope of those dependencies should thus be set to provided in the <dependencyManagement/> section of the Jackrabbit parent POM.
0,"Fix CommitIndexTask to also commit IndexReader changes. I'm setting up a benchmark for LUCENE-1458, and one limitation I hit is that the CommitIndexTask doesn't commit pending changes in the IndexReader (eg via DeleteByPercent), using a named commit point."
0,"Remove all interning of field names from flex API. In previous versions of Lucene, interning of fields was important to minimize string comparison cost when iterating TermEnums, to detect changes in field name. As we separated field names from terms in flex, no query compares field names anymore, so the whole performance problematic interning can be removed. I will start with doing this, but we need to carefully review some places e.g. in preflex codec.

Maybe before this issue we should remove the Term class completely. :-) Robert?"
0,"TieredMergePolicy should expose control over how aggressively segments with deletions are targeted . TMP today always does a linear pro-rating of a merge's score according to what pctg of the documents are deleted; I'd like to 1) put a power factor in (score is multiplicative), and 2) default it to stronger favoring of merging away deletions."
0,"Some house cleaning in addIndexes*. Today, the use of addIndexes and addIndexesNoOptimize is confusing - 
especially on when to invoke each. Also, addIndexes calls optimize() in 
the beginning, but only on the target index. It also includes the 
following jdoc statement, which from how I understand the code, is 
wrong: _After this completes, the index is optimized._ -- optimize() is 
called in the beginning and not in the end. 

On the other hand, addIndexesNoOptimize does not call optimize(), and 
relies on the MergeScheduler and MergePolicy to handle the merges. 

After a short discussion about that on the list (Thanks Mike for the 
clarifications!) I understand that there are really two core differences 
between the two: 
* addIndexes supports IndexReader extensions
* addIndexesNoOptimize performs better

This issue proposes the following:
# Clear up the documentation of each, spelling out the pros/cons of 
  calling them clearly in the javadocs.
# Rename addIndexesNoOptimize to addIndexes
# Remove optimize() call from addIndexes(IndexReader...)
# Document that clearly in both, w/ a recommendation to call optimize() 
  before on any of the Directories/Indexes if it's a concern. 

That way, we maintain all the flexibility in the API - 
addIndexes(IndexReader...) allows for using IR extensions, 
addIndexes(Directory...) is considered more efficient, by allowing the 
merges to happen concurrently (depending on MS) and also factors in the 
MP. So unless you have an IR extension, addDirectories is really the one 
you should be using. And you have the freedom to call optimize() before 
each if you care about it, or don't if you don't care. Either way, 
incurring the cost of optimize() is entirely in the user's hands. 

BTW, addIndexes(IndexReader...) does not use neither the MergeScheduler 
nor MergePolicy, but rather call SegmentMerger directly. This might be 
another place for improvement. I'll look into it, and if it's not too 
complicated, I may cover it by this issue as well. If you have any hints 
that can give me a good head start on that, please don't be shy :). "
0,"MultiStatusResponse should not call resource.getProperties. current constructor MultiStatusResponse() calls resource.getProperties() even if propFindType == PROPFIND_BY_PROPERTY.

This is inconvenient, because some properties are expensive to generate if they are not requested. MultiStatusResponse() constructor with parameter PROPFIND_BY_PROPERTY should do:

===
if (propFindType == PROPFIND_BY_PROPERTY) {
  for (propName : propNameSet) {
    prop = resource.getProperty(propName);
    if (prop != null)
      status200.addContent(prop);
    else
      status404.addContent(propName);
  }
} else {
  ...
}
==="
0,"web.xml refers to 2.2 dtd. the web.xml present in the jackrabbit-webapp project contains the following doctype:

<!DOCTYPE web-app PUBLIC ""-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN"" ""http://java.sun.com/j2ee/dtds/web-app_2_2.dtd"">

what is probably meant is

<!DOCTYPE web-app PUBLIC ""-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN"" ""http://java.sun.com/dtd/web-app_2_3.dtd"">

since the dependency also points to version 2.3 of the servlet-api

if nobody objects, i will adjust it accordingly."
0,"HostConfiguration handling requires cleanup. As discussed on the mailing list, the host configuration handling currently
appears faulty:

http://marc.theaimsgroup.com/?t=109644952000001&r=1&w=2

Oleg"
0,"IndexReader.reopen(). This is Robert Engels' implementation of IndexReader.reopen() functionality, as a set of 3 new classes (this was easier for him to implement, but should probably be folded into the core, if this looks good).
"
0,"Remove timeout handling from TransactionContext. As discussed in JCR-2861, the transaction timeout handling in the TransactionContext class should not be needed since that's the task of the transaction manager, not the context. We should simply remove the timeout handling."
0,"Add support for tablespaces to Oracle related classes. When a user account for an Oracle database has no or a temporary default tablespace, then the appropriate database schemas cannot be created. This is an issue for at least the following packages:
- o.a.j.core.persistence.bundle
- o.a.j.core.persistence.db
- o.a.j.core.fs.db
"
0,Migrate to maven 2 the other OCM subprojects. Migrate to maven 2 the other OCM subprojects : jcr-nodemanagement & spring. 
0,"Missing class JNDIDatabaseJournal. We're dealing to set up a clustered repository and run into some issues and missing features stated to be fixed in the upcoming v1.4. But while scanning the sources of the v1.4-rc1, i still can't find the class

  JNDIDatabaseJournal (org.apache.jackrabbit.core.journal.JNDIDatabaseJournal)

as a silbing to the classes 

  JNDIDatabaseFileSystem (org.apache.jackrabbit.core.fs.db.JNDIDatabaseFileSystem)

and 

  JNDIDatabasePersistenceManager (org.apache.jackrabbit.core.persistence.db.JNDIDatabasePersistenceManager)

The missing one you'll need to configure all parts of a repository handeled in an abstract way by (e.g one common) JNDI database resource. From the shortness and simplicity of the source code of the other ones, i think adding this missing feature takes just about an hour.

Thank you for support"
0,"Deprecation of autoCommit in 2.4 leads to compile problems, when autoCommit should be false. I am currently changing my code to be most compatible with 2.4. I switched on deprecation warnings and got a warning about the autoCommit parameter in IndexWriter constructors.

My code *should* use autoCommit=false, so I want to use the new semantics. The default of IndexWriter is still autoCommit=true. My problem now: How to disable autoCommit whithout deprecation warnings?

Maybe, the ""old"" constructors, that are deprecated should use autoCommit=true. But there are new constructors with this ""IndexWriter.MaxFieldLength mfl"" in it, that appear new in 2.4 but are deprecated:

IndexWriter(Directory d, boolean autoCommit, Analyzer a, boolean create, IndexDeletionPolicy deletionPolicy, IndexWriter.MaxFieldLength mfl) 
          Deprecated. This will be removed in 3.0, when autoCommit will be hardwired to false. Use IndexWriter(Directory,Analyzer,boolean,IndexDeletionPolicy,MaxFieldLength) instead, and call commit() when needed.

What the hell is meant by this, a new constructor that is deprecated? And the hint is wrong. If I use the other constructor in the warning, I get autoCommit=true.

There is something completely wrong.

It should be clear, which constructors set autoCommit=true, which set it per default to false (perhaps new ones), and the Deprecated text is wrong, if autoCommit does not default to false."
0,"MemoryIndex memory estimation in toString inconsistent with getMemorySize(). After LUCENE-3867 was committed, there are some more minor problems with MemoryIndex's estimates. This patch will fix those and also add verbose test output of RAM needed for MemoryIndex vs. RAMDirectory.

Interestingly, the RAMDirectory always takes (according to estimates, so even with buffer overheads) only 2/3 of the MemoryIndex (excluding IndexReaders)."
0,"ConstantScoreQuery should directly support wrapping Query and simply strip off scores. Especially in MultiTermQuery rewrite modes we often simply need to strip off scores from Queries and make them constant score. Currently the code to do this looks quite ugly: new ConstantScoreQuery(new QueryWrapperFilter(query))

As the name says, QueryWrapperFilter should make any other Query constant score, so why does it not take a Query as ctor param? This question was aldso asked quite often by my customers and is simply correct, if you think about it.

Looking closer into the code, it is clear that this would also speed up MTQs:
- One additional wrapping and method calls can be removed
- Maybe we can even deprecate QueryWrapperFilter in 3.1 now (it's now only used in tests and the use-case for this class is not really available) and LUCENE-2831 does not need the stupid hack to make Simon's assertions pass
- CSQ now supports out-of-order scoring and topLevel scoring, so a CSQ on top-level now directly feeds the Collector. For that a small trick is used: The score(Collector) calls are directly delegated and the scores are stripped by wrapping the setScorer() method in Collector

During that I found a visibility bug in Scorer (LUCENE-2839): The method ""boolean score(Collector collector, int max, int firstDocID)"" should be public not protected, as its not solely intended to be overridden by subclasses and is called from other classes, too! This leads to no compiler bugs as the other classes that calls it is mainly BooleanScorer(2) and thats in same package, but visibility is wrong. I will open an issue for that and fix it at least in trunk where we have no backwards-requirement."
0,"Preserving UUID and document version history on repository migration. I have been working I an migration utility for OpenKM and I performed some changes in jackrabit-core to enable version import, preserving
the modification date. Also modified org.apache.jackrabbit.core.NodeImpl to preserve UUID in the migration process.

This migration process is needed because there are changes in repository node definition, and Jackrabbit can't deal with this actually.

I've attache a PDF with the changes needed in Jackrabbit-core. It works and there was no problems with the migrated repository."
0,"Improve indexing performance by increasing internal buffer sizes. In working on LUCENE-843, I noticed that two buffer sizes have a
substantial impact on overall indexing performance.

First is BufferedIndexOutput.BUFFER_SIZE (also used by
BufferedIndexInput).  Second is CompoundFileWriter's buffer used to
actually build the compound file.  Both are now 1 KB (1024 bytes).

I ran the same indexing test I'm using for LUCENE-843.  I'm indexing
~5,500 byte plain text docs derived from the Europarl corpus
(English).  I index 200,000 docs with compound file enabled and term
vector positions & offsets stored plus stored fields.  I flush
documents at 16 MB RAM usage, and I set maxBufferedDocs carefully to
not hit LUCENE-845.  The resulting index is 1.7 GB.  The index is not
optimized in the end and I left mergeFactor @ 10.

I ran the tests on a quad-core OS X 10 machine with 4-drive RAID 0 IO
system.

At 1 KB (current Lucene trunk) it takes 622 sec to build the index; if
I increase both buffers to 8 KB it takes 554 sec to build the index,
which is an 11% overall gain!

I will run more tests to see if there is a natural knee in the curve
(buffer size above which we don't really gain much more performance).

I'm guessing we should leave BufferedIndexInput's default BUFFER_SIZE
at 1024, at least for now.  During searching there can be quite a few
of this class instantiated, and likely a larger buffer size for the
freq/prox streams could actually hurt search performance for those
searches that use skipping.

The CompoundFileWriter buffer is created only briefly, so I think we
can use a fairly large (32 KB?) buffer there.  And there should not be
too many BufferedIndexOutputs alive at once so I think a large-ish
buffer (16 KB?) should be OK.
"
0,"Improve performance of MatchAllScorer. The BitSets created in MatchAllScorer should be cached per IndexReader. This enhancement should also take care that the supplied IndexReader may in fact be a CombinedIndexReader or a CachingMultiReader with multiple contained IndexReaders. To achieve a good cache efficiency the BitSets must be cached per contained IndexReader and combined later.

See also thread on dev list: http://thread.gmane.org/gmane.comp.apache.jackrabbit.devel/10976"
0,Refactoring and slight extension of regex testing code.. 
0,"Add ReverseStringFilter. add ReverseStringFilter and ReverseStringAnalyzer that can be used for backword much. For Example, ""*ry"", ""*ing"", ""*ber""."
0,"Some small javadocs/extra import fixes. Two things that Uwe Schindler caught, plus fixes for javadoc warnings in core.  I plan to commit to trunk & 2.4."
0,"Add meta keywords to HTMLParser. 
It would be good if the HTMLParser could give us the keywords specified in the meta tags, so that we can index them.

In HTMLParser.jj:

  void addMetaTag() {
      metaTags.setProperty(currentMetaTag, currentMetaContent);
      currentMetaTag = null;
      currentMetaContent = null;
      return;
  }

One way to do it:

  void addMetaTag() throws IOException {
      metaTags.setProperty(currentMetaTag, currentMetaContent);
      if (currentMetaTag.equalsIgnoreCase(""keywords"")) {
          pipeOut.write(currentMetaContent);
      }
      currentMetaTag = null;
      currentMetaContent = null;
      return;
  }
"
0,"WikipediaTokenizer needs a way of not tokenizing certain parts of the text. It would be nice if the WikipediaTokenizer had a way of, via a flag, leaving categories, links, etc. as single tokens (or at least some parts of them)

Thus, if we came across [[Category:My Big Idea]] there would be a way of outputting, as a single token ""My Big Idea"".  

Optionally, it would be good to output both ""My Big Idea"" and the individual tokens as well.

I am not sure of how to do this in JFlex, so any insight would be appreciated."
0,Move ReusableAnalyzerBase into core. In LUCENE-2309 it was suggested that we should make Analyzer reusability compulsory.  ReusableAnalyzerBase is a fantastic way to drive reusability so lets move it into core (so that we can then change all impls over to using it).
0,"If test has methods with @Ignore, we should print out a notice. Currently these silently pass, but there is usually a reason they are @Ignore 
(sometimes good, sometimes really a TODO we should fix)

In my opinion we should add reasons for all these current @Ignores like Mike did with Test2BTerms.

Example output:
{noformat}
[junit] Testsuite: org.apache.lucene.index.Test2BTerms
[junit] Tests run: 0, Failures: 0, Errors: 0, Time elapsed: 0.184 sec
[junit]
[junit] ------------- Standard Error -----------------
[junit] NOTE: Ignoring test method 'test2BTerms' Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.
[junit] ------------- ---------------- ---------------

...

[junit] Testsuite: org.apache.solr.handler.dataimport.TestMailEntityProcessor
[junit] Tests run: 0, Failures: 0, Errors: 0, Time elapsed: 0.043 sec
[junit]
[junit] ------------- Standard Error -----------------
[junit] NOTE: Ignoring test method 'testConnection'
[junit] NOTE: Ignoring test method 'testRecursion'
[junit] NOTE: Ignoring test method 'testExclude'
[junit] NOTE: Ignoring test method 'testInclude'
[junit] NOTE: Ignoring test method 'testIncludeAndExclude'
[junit] NOTE: Ignoring test method 'testFetchTimeSince'
[junit] ------------- ---------------- ---------------
{noformat}"
0,"JCR2SPI: error level logging when cleaning up session locks . LockManagerImpl.loggingOut() tries to unlock nodes that have a session lock. If, while doing so, a RepositoryException is thrown, this gets locked on error level.

The TCK tests tearDown code removes test nodes using a separate session; thus we see RepositoryExceptions for the simple reason that the nodes have already been removed by somebody else.

Proposal: handle ItemNotFoundExc and PathNotFoundExc separately, not logging them.
"
0,Spellchecker uses default IW mergefactor/ramMB settings of 300/10. These settings seem odd - I'd like to investigate what makes most sense here.
0,"Maven 2 POM includes junit in default ""compile"" scope, rather than ""test"" scope. The POM at the URL above declares a dependency on JUnit in the default scope, rather than the ""test"" scope."
0,"Making Tokenizer.reset(Reader) public. In order to implement reusableTokenStream and be able to reset a Tokenizer, Tokenizer defines a reset(Reader) method. The problem is that this method is protected. I need to call this reset(Reader) method without having to know in advance what will be the type of the Tokenizer (I plan to have several).
I noticed that almost all Tokenizer extensions define this method as public, and I wonder if this can be changed for Tokenizer also (I can't simply create my general Tokenizer extension and inherit from it because I want to use StandardTokenizer as well). "
0,"Small speedups to DocumentsWriter. Some small fixes that I found while profiling indexing Wikipedia,
mainly using our own quickSort instead of Arrays.sort.

Testing first 200K docs of Wikipedia shows a speedup from 274.6
seconds to 270.2 seconds.

I'll commit in a day or two."
0,"GroupImp#getMembers and #getDeclaredMembers should return RangeIterator. for those cases where the total amount of members can easily be detected/calculated the
implementations of Group#getMembers() and #getDeclaredMembersOf() should return a RangeIterator.

so far i found that Group#declaredMembers() can be easily adjusted for those cases where
the group members are stored in a multivalued property."
0,"Jackrabbit web page scroll is slow with Firefox. When I visit http://jackrabbit.apache.org/ from my Firefox in Ubuntu  the browser scroll is very slow and make CPU go to 100%. 

The problem seems to be the ""fixed"" attribute in the css background definition, so it should be removed.

body {
  background:white url(bg.png) repeat-x fixed center bottom;
  font-family:Verdana,Helvetica,Arial,sans-serif;
  font-size:small;
  margin:0pt;
  padding:0pt;
}"
0,"jira notifications. I have set up Jackrabbit's jira so that notifications will
go to the dev mailing list.  If that gets annoying, I can easily
switch it to a different list or turn off notifications such that
people have to register as watchers.

Let me know what is best for you.

....Roy
"
0,"Upgrade to Tika 0.7. Apache Tika 0.7 is now available. It's probably too late for the 2.1 release, but I'll upgrade the dependency in the trunk for Jackrabbit 2.2."
0,"Directory createOutput and openInput should take an IOContext. Today for merging we pass down a larger readBufferSize than for searching because we get better performance.

I think we should generalize this to a class (IOContext), which would hold the buffer size, but then could hold other flags like DIRECT (bypass OS's buffer cache), SEQUENTIAL, etc.

Then, we can make the DirectIOLinuxDirectory fully usable because we would only use DIRECT/SEQUENTIAL during merging.

This will require fixing how IW pools readers, so that a reader opened for merging is not then used for searching, and vice/versa.  Really, it's only all the open file handles that need to be different -- we could in theory share del docs, norms, etc, if that were somehow possible."
0,"[PATCH] new method: Document.remove(). Here's a patch that adds a remove() method to the Document class (+test case). This 
is very useful if you have converter classes that return a Lucene Document object but 
you need to make changes to that object. 
 
In my case, I wanted to index PDF files that were saved as BLOBs in a database. The 
files need to be saved to a temporary file and that file name is given to the PDF 
converter class. The PDF converter then saves the name of the temporary file name 
as the file name, which doesn't make sense. So my code needs to remove the 
'filename' field and re-add it, this time with the columns primary ID. This is only possible 
with the attached patch."
0,"drop java 5 ""support"". its been discussed here and there, but I think we need to drop java 5 ""support"", for these reasons:
* its totally untested by any continual build process. Testing java5 only when there is a release candidate ready is not enough. If we are to claim ""support"" then we need a hudson actually running the tests with java 5.
* its now unmaintained, so bugs have to either be hacked around, tests disabled, warnings placed, but some things simply cannot be fixed... we cannot actually ""support"" something that is no longer maintained: we do find JRE bugs (http://wiki.apache.org/lucene-java/SunJavaBugs) and its important that bugs actually get fixed: cannot do everything with hacks.
* because of its limitations, we do things like allow 20% slower grouping speed. I find it hard to believe we are sacrificing performance for this.

So, in summary: because we don't test it at all, because its buggy and unmaintained, and because we are sacrificing performance, I think we need to cutover the build system for the next release to require java 6.
"
0,"Remove support for pre-3.0 indexes. We should remove support for 2.x (and 1.9) indexes in 4.0. It seems that nothing can be done in 3x because there is no special code which handles 1.9, so we'll leave it there. This issue should cover:
# Remove the .zip indexes
# Remove the unnecessary code from SegmentInfo and SegmentInfos. Mike suggests we compare the version headers at the top of SegmentInfos, in 2.9.x vs 3.0.x, to see which ones can go.
# remove FORMAT_PRE from FieldInfos
# Remove old format from TermVectorsReader

If you know of other places where code can be removed, then please post a comment here.

I don't know when I'll have time to handle it, definitely not in the next few days. So if someone wants to take a stab at it, be my guest.
"
0,"Move XML QueryParser to queryparser module. The XML QueryParser will be ported across to queryparser module.

As part of this work, we'll move the QP's demo into the demo module."
0,Improve FileRevision extensibility. It'd be nice to make FileRevision more extensible by chaning some of its private variables to protected so it can be extended easier when needed.
0,Deprecate RepositoryService.getNodeInfo method. I suggest to deprecate RepositoryService.getNodeInfo in favor of RepositoryService.getItemInfos. The former method is not called from jsr2spi anymore and thus not required. 
0,"JSR 283: References and Dereferencing of Property Values. References
--------------------------------------------------------------------------------------------------------------------------
new methods are:

- Node.getReferences(String name) PropertyIterator 
- Node.getWeakReferences() PropertyIterator 
- Node.getWeakReferences(String name) PropertyIterator


Derferencing
--------------------------------------------------------------------------------------------------------------------------
As of JSR 283 the following property types may be dereferenced to a Node:

- REFERENCE
- WEAKREFERENCE
- PATH
- any type that can be converted to either of the types above

The new method
- Property.getProperty() returns the Property pointed to by a PATH value.
- any type that can be converted to PATH




"
0,"if a filter can support random access API, we should use it. I ran some performance tests, comparing applying a filter via
random-access API instead of current trunk's iterator API.

This was inspired by LUCENE-1476, where we realized deletions should
really be implemented just like a filter, but then in testing found
that switching deletions to iterator was a very sizable performance
hit.

Some notes on the test:

  * Index is first 2M docs of Wikipedia.  Test machine is Mac OS X
    10.5.6, quad core Intel CPU, 6 GB RAM, java 1.6.0_07-b06-153.

  * I test across multiple queries.  1-X means an OR query, eg 1-4
    means 1 OR 2 OR 3 OR 4, whereas +1-4 is an AND query, ie 1 AND 2
    AND 3 AND 4.  ""u s"" means ""united states"" (phrase search).

  * I test with multiple filter densities (0, 1, 2, 5, 10, 25, 75, 90,
    95, 98, 99, 99.99999 (filter is non-null but all bits are set),
    100 (filter=null, control)).

  * Method high means I use random-access filter API in
    IndexSearcher's main loop.  Method low means I use random-access
    filter API down in SegmentTermDocs (just like deleted docs
    today).

  * Baseline (QPS) is current trunk, where filter is applied as iterator up
    ""high"" (ie in IndexSearcher's search loop)."
0,"need DOAP file for Lucene. Can someone please draft a DOAP file for Lucene, so that we're listed at http://projects.apache.org/?

A DOAP generator is at:

http://projects.apache.org/create.html

Please attach it to this bug report.  Thanks."
0,"Provide Summary Information on the Files in the Lucene index. I find myself often having to remember, by file extension, what is in a particular index file.  The information is all contained in the File Formats, but not summarized.  This patch provides a simple table that describes the extensions and provides links to the relevant section."
0,"Change project names to start with jackrabbit. All the released projects should have artifactId's starting with ""jackrabbit""."
0,Jackrabbit JCR Nodemanagement API implementation. There needs to be a Jackrabbit implementation of the org.apache.portals.graffito.jcr.nodemanagement.NodeTypeManager interface.
0,"Make it easier to run Test2BTerms. Currently, Test2BTerms has an @Ignore annotation which means that the only way to run it as a test is to edit the file.

There are a couple of options to fix this:
# Add a main() so it can be invoked via the command line outside of the test framework
# Add some new annotations that mark it as slow or weekly or something like that and have the test target ignore @slow (or whatever) by default, but can also turn it on."
0,"remove _X.fnx. Currently we store a global (not per-segment) field number->name mapping in _X.fnx

However, it doesn't actually save us any performance e.g on IndexWriter's init because
since LUCENE-2984 we are to loading the fieldinfos anyway to compute files() for IFD, etc, 
as thats where hasProx/hasVectors is.

Additionally in the past global files like shared doc stores have caused us problems,
(recently we just fixed a bug related to this file in LUCENE-3601).

Finally this is trouble for backwards compatibility as its difficult to handle a global
file with the codecs mechanism."
0,"MTQ rewrite + weight/scorer init should be single pass. Spinoff of LUCENE-2690 (see the hacked patch on that issue)...

Once we fix MTQ rewrite to be per-segment, we should take it further and make weight/scorer init also run in the same single pass as rewrite."
0,"NodeTypeRegistry could auto-subtype from nt:base. when tying to register a (primary) nodetype that does not extend from nt:base the following error is
thrown:

""all primary node types except nt:base itself must be (directly or indirectly) derived from nt:base""

since the registry is able to detect this error, it would be easy to auto-subtype all nodetypes from nt:base. imo it's pointless to explzitely add the nt:base to every supperclass set. as an analogy, you don't need to 'extend from java.lang.Object' explicitely - the compiler does that automatically for your."
0,"ProtocolException thrown on slightly broken headers. HTTPClient throws an exception when parsing headers returned by GET from the
following URL:

 http://butler.cit.nih.gov/hembase/hembase.taf

The headers returned are as follows:

HTTP/1.0 200 OK\r\nServer: WebSTAR/1.0 ID/ACGI\r\nMIME-Version:
1.0\r\nContent-Type: text/html\r\nSet-Cookie:
Tango_UserReference=ADC5871C57FABEDEC63DD47B; path=/\n\r\r\n\r\n<!DOCTYPE HTML
PUBLIC ""-//W3C//DTD HTML 4.01 Transitional//EN"">...

Please note the superfluous \r in the line separating headers from the body.
IMHO this type of error should generate a warning, but then it should cause a
graceful recovery. Currently a ProtocolException is thrown.

Standard java.net.HttpURLConnection handles this just fine, without giving any
warning."
0,"Minor changes to SimpleHTMLFormatter. I'd like to make few minor changes to SimpleHTMLFormatter.

1. Define DEFAULT_PRE_TAG and DEFAULT_POST_TAG and use them in the default constructor. This will not trigger String lookups by the JVM whenever the highlighter is instantiated.

2. Create the StringBuffer in highlightTerm with the right number of characters from the beginning. Even though StringBuffer's default constructor allocates 16 chars, which will probably be enough for most highlighted terms (pre + post tags are 7 chars, which leaves 9 chars for terms), I think it's better to allocate SB with the right # of chars in advance, to avoid char[] allocations in the middle."
0,"Incorrect copyright statements. Most of the copyright statements in http-core are for 1999-2004.  These should
be updated with the correct years."
0,"[PATCH] Remove Stutter in NodeState. Code duplicates code for no reason

Index: src/main/java/org/apache/jackrabbit/jcr2spi/state/NodeState.java
===================================================================
--- src/main/java/org/apache/jackrabbit/jcr2spi/state/NodeState.java	(revision 740824)
+++ src/main/java/org/apache/jackrabbit/jcr2spi/state/NodeState.java	(working copy)
@@ -449,7 +449,7 @@
              */
             NodeState parent = getParent();
             NodeId wspId = (NodeId) getWorkspaceId();
-            def = definitionProvider.getQNodeDefinition(getParent().getNodeTypeNames(), getName(), getNodeTypeName(), wspId);
+            def = definitionProvider.getQNodeDefinition(parent.getNodeTypeNames(), getName(), getNodeTypeName(), wspId);
         }
         return def;
     }
"
0,Upgrade to Tika 0.10. Apache Tika 0.10 was released some while ago. It contains lots of improvements and fixes for full text extraction.
0,"Check if a DAV-Request has a Label in the header, before checking if it's version-controlled. When looking at our MySQL logs, I realized that jackrabbit on each DAV Request calls the VERSION table every time I get a new node (which is not cached yet), even if I only do a simple getNode. 
As a versioning table can get pretty large, this may have a performance impact.

I found out, that DavResourceFactoryImpl checks, if a node is versioned to decide, if we have to check for the Label header to later check out another version for the GET request. I re-ordered those checks now so that it first checks, if there's an http Label-header and only then checks, if the node is versioned. The check for a Label header should be much faster than checking a DB, if it's versioned (and scale much better, too)



"
0,"Fix junit scope in maven pom. Please change the junit dependency to

    <dependency>
      <groupId>junit</groupId>
      <artifactId>junit</artifactId>
      <version>3.8.1</version>
      <url>http://www.junit.org/</url>
      <properties>
        <scope>test</scope>
      </properties>
    </dependency>

for better automatic conversion to maven 2"
0,"Introduce cache for frequently used index lookups. Some queries heavily use hierarchy relations to resolve location steps. E.g. ChildAxisQuery or DescendantSelfAxisQuery. Currently those hierarchy relations are looked up from the native lucene index which is not very efficient. The index should maintain a cache of frequently used hierarchy lookups. 
That is, calls like IndexReader.termDocs() on terms with field: UUID or PARENT"
0,"Put resource files in java/{main,test}/resources. The Maven standard directory layout (see http://maven.apache.org/guides/introduction/introduction-to-the-standard-directory-layout.html) suggests that all resource files should be placed in src/main/resources or src/test/resources instead of putting them inside the java subdirectory. Following that guideline would simplify the pom files as we wouldn't need to explicitly configure the resource directories. I'll move the resource files unless anyone argues otherwise."
0,"Deprecating StopAnalyzer ENGLISH_STOP_WORDS - General replacement with an immutable Set. StopAnalyzer and StandartAnalyzer are using the static final array ENGLISH_STOP_WORDS by default in various places. Internally this array is converted into a mutable set which looks kind of weird to me. 
I think the way to go is to deprecate all use of the static final array and replace it with an immutable implementation of CharArraySet. Inside an analyzer it does not make sense to have a mutable set anyway and we could prevent set creation each time an analyzer is created. In the case of an immutable set we won't have multithreading issues either. 
in essence we get rid of a fair bit of ""converting string array to set"" code, do not have a PUBLIC static reference to an array (which is mutable) and reduce the overhead of analyzer creation.

let me know what you think and I create a patch for it.

simon"
0,"Add configuration path to SynonymProvider. This is an enhancement to the SynonymProvider, which will be included in the 1.4 release. The current interface only has a getSynonyms() method but does not allow to initialize the SynonymProvider. It should be possible to initialize the provider with a configuration file. The configuration may then include the synonym definitions or a pointer to a location where the synonyms are defined. The configuration will be implementation dependent.

In addition there should be a simple default implementation in jackrabbit-core. The wordnet-synonyms in the sandbox are only of limited use and must be built manually."
0,"EntryCollectorTest failure on certain Java versions. The testCache test case in the EntryCollectorTest class uses array comparison for a set of permissions, which causes test failures on certain Java versions where the ordering of permissions is different than expected."
0,"optimize contrib/regex for flex. * changes RegexCapabilities match(String) to match(BytesRef)
* the jakarta and jdk impls uses CharacterIterator/CharSequence matching against the utf16result instead.
* i also reuse the matcher for jdk, i don't see why we didnt do this before but it makes sense esp since we reuse the CSQ
"
0,"QueryObjectModelImpl should execute queries as SessionOperation(s). QueryObjectModelImpl doesn't leverage the SessionOperation closure approach (like the QueryImpl does). 

Switching to this style of running a query yields some gains in speed (I ran 50 queries per test):
 - #1. old style   (no code change)       avg was 14.26 ms
 - #2. new style (as session operation) avg was 12.14 ms
 - #3. new style (as session operation) avg was   6.44 ms
 - #4. new style (as session operation) avg was   6.68 ms
 - #5. old style  (no code change)        avg was 11.62 ms
 - #6. old style  (no code change)        avg was 11.66 ms"
0,"Add Warnlog on Extraction Failure. It will be fine to have a feedback if a exception occurs in the TextExtractors.
At the moment only a empty StringReader will be returned.
We had the issue that we updated the content and in the textextractor a exception occured
so the index was not updated and the document was searchable by its old content."
0,"'ant javacc' in root project should also properly create contrib/queryparser Java files. 'ant javacc' in the project root doesn't run javacc in contrib/queryparser
'ant javacc' in contrib/queryparser does not properly create the Java files. What still needs to be done by hand is (partly!) described in contrib/queryparser/README.javacc. I think this process should be automated. Patch provided."
0,"Repository does not start if text filter dependencies are missing. When the search index is configured with a text filter class that requries another jar file and that jar file is missing the repository will not start and log the following misleading error:

Caused by: javax.jcr.RepositoryException
    at org.apache.jackrabbit.core.SearchManager.initializeQueryHandler(SearchManager.java:536)
    at org.apache.jackrabbit.core.SearchManager.<init>(SearchManager.java:278)
    at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.getSearchManager(RepositoryImpl.java:1430)
    at org.apache.jackrabbit.core.RepositoryImpl.initWorkspace(RepositoryImpl.java:538)
    at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:245)
    at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:482)
    at org.jbpm.jcr.impl.JackrabbitJcrService.start(JackrabbitJcrService.java:119)
    ... 63 more
Caused by: java.lang.IllegalArgumentException
    at org.apache.commons.collections.BeanMap.put(BeanMap.java:374)
    at org.apache.jackrabbit.core.config.BeanConfig.newInstance(BeanConfig.java:97)
    at org.apache.jackrabbit.core.SearchManager.initializeQueryHandler(SearchManager.java:530)
    ... 69 more "
0,"[PATCH] Binary stored fields. Provides a binary Field type that can be used to store byte arrays in the Lucene
index. Can be used for a variety of applications from compressed text storage,
image storage or as a basis for implementing typed storage (e.g: Integers,
Floats, etc.)

Based on discussion from lucene-dev list started here:
http://marc.theaimsgroup.com/?l=lucene-dev&m=108455161204687&w=2

Directly based on design fleshed out here:
http://marc.theaimsgroup.com/?l=lucene-dev&m=108456898230542&w=2

Patch includes updated code and unit tests not included in the patch sent do the
lucene-dev list."
0,"Improve exception handling in observation (ChangePolling). Currently when an (internal) event listener throws an exception, all further event listeners are skipped. This happens for move events where the HierarchyListener throws an UnsupportedOperationException. I suggest to move the exception handler up the call chain such that exceptions are caught and logged per listener instead of for all listeners. "
0,"contrib/remote tests fail randomly. The contrib/remote tests will fail randomly.

This is because they use this _TestUtil.getRandomSocketPort() which
simply generates a random number, but if this is already in use, it will fail.

Additionally there is duplicate RMI logic across all 3 test classes."
0,"Reduce usage of String.intern(). String.intern() is used for interning the namespace URI in NameImpl. For some trivial cases the intern() method shouldn't be called but a constant should be
used. E.g. I'm thinking about the empty namespace URI, where calling String.intern() is way more expensive than checking if the length of the URI string is zero. "
0,"Make Token.DEFAULT_TYPE public. Make Token.DEFAULT_TYPE public so that TokenFilters using the reusable Token model have a way of setting the type back to the default.

No patch necessary.  I will commit soon."
0,"deprecate ChineseAnalyzer. The ChineseAnalyzer, ChineseTokenizer, and ChineseFilter (not the smart one, or CJK) indexes chinese text as individual characters and removes english stopwords, etc.

In my opinion we should simply deprecate all of this in favor of StandardAnalyzer, StandardTokenizer, and StopFilter, which does the same thing."
0,"fix jackrabbit groupId. Actually the groupid used in maven project.xml is simply ""jackrabbit"".
However, the new naming policy adopted by maven requires the groupid to mirror the main package name, so it should be changed to ""org.apache.jackrabbit"".
Probably this could be a good moment to fix it, after the recent merge and modification of artifact ids.

A note about the new policy for groupid can be found in http://maven.apache.org/reference/repository-upload.html"
0,"Add possibility to disable automatic authentication header processing.. In the application I'm working on I need the possibility to manually get the 
WWW-Authenticate header instead of letting the HttpClient process it 
automatically. Instead of rewriting the execute() method in a subclass I added 
a configuration setting, similar to the followRedirects flag. Diffs below for 
anyone interested."
0,NodeBasedGroup#isMember(Principal) should have shortcut for the everyone group.. 
0,"Some contribs depend on core tests to be compiled and fail when ant clean was done before. If you do ""ant clean"" on the root level of Lucene and then go to e.g. contrib/queryparser (3.x only) or contrib/misc (3.x and trunk) and call ""ant test"", the build of tests fails:
- contrib/queryparser's ExtendedableQPTests extend a core TestQueryParser (3.x only, in module this works, of course)
- contrib/misc/TestIndexSplitter uses a core class to build its index

To find the root cause: We should first remove the core tests from contrib classpath, so the issue gets visible even without ""ant clean"" before. Then we can fix this."
0,"Make FieldSelector usable from Searchable . Seems reasonable that you would want to be able to specify a FieldSelector from Searchable because many systems do not use IndexSearcher (where you can get a Reader), but instead use Searchable or Searcher so that Searchers and MultiSearchers can be used in a polymorphic manner."
0,"Deprecate/remove language-specific tokenizers in favor of StandardTokenizer. As of Lucene 3.1, StandardTokenizer implements UAX#29 word boundary rules to provide language-neutral tokenization.  Lucene contains several language-specific tokenizers that should be replaced by UAX#29-based StandardTokenizer (deprecated in 3.1 and removed in 4.0).  The language-specific *analyzers*, by contrast, should remain, because they contain language-specific post-tokenization filters.  The language-specific analyzers should switch to StandardTokenizer in 3.1.

Some usages of language-specific tokenizers will need additional work beyond just replacing the tokenizer in the language-specific analyzer.  

For example, PersianAnalyzer currently uses ArabicLetterTokenizer, and depends on the fact that this tokenizer breaks tokens on the ZWNJ character (zero-width non-joiner; U+200C), but in the UAX#29 word boundary rules, ZWNJ is not a word boundary.  Robert Muir has suggested using a char filter converting ZWNJ to spaces prior to StandardTokenizer in the converted PersianAnalyzer."
0,"Access cluster node id. I need to know the cluster node id in my application. I didn't find any other way than to cast to org.apache.jackrabbit.core.RepositoryImpl : ((RepositoryImpl) session.getRepository()).getConfig().getClusterConfig().getId()

I would appreciate it if I could get to this using the system property ClusterNode.SYSTEM_PROPERTY_NODE_ID."
0,Do not log warning when coercing value in query is not possible. The LuceneQueryBuilder currently logs a warning when a String literal cannot be coerced into a type derived from information provided by the node type manager. The log level should be lowered to debug.
0,"move log4j initialization out of RepositoryStartupServlet. the RepositoryStartupServlet initializes/configures the Log4J environment. this might not be desirable since other applications might already done so.

"
0,"Performance improvement for SegmentMerger.mergeNorms(). This patch makes two improvements to SegmentMerger.mergeNorms():

1) When the SegmentMerger merges the norm values it allocates a new byte array to buffer the values for every field of every segment. The size of such an array equals the size of the corresponding segment, so if large segments are being merged, those arrays become very large, too.
We can easily reduce the number of array allocations by reusing a byte array to buffer the norm values that only grows, if a segment is larger than the previous one.

2) Before a norm value is written it is checked if the corresponding document is deleted. If not, the norm is written using IndexOutput.writeByte(byte[]). This patch introduces an optimized case for segments that do not have any deleted docs. In this case the frequent call of IndexReader.isDeleted(int) can be avoided and the more efficient method IndexOutput.writeBytes(byte[], int) can be used.


This patch only changes the method SegmentMerger.mergeNorms(). All unit tests pass."
0,"Node Type Management subproject : Default namespace should be emtpy. When creating node types matching to the class descriptors,  the default namespace should be empty instead of  'ocm'."
0,Inner classes of FilterAtomicReader (trunk) / FilterIndexReader (3.x) do not override all methods to be filtered. This issue adds missing checks in the FilterReader test to also check overridden methods in the enum implementations (inner classes) similar to the checks added by Shai Erea.
0,"Disentangle commons-httpclient from commons in Gump. Currently, the Gump project for HttpClient 3.x is defined as one of the commons projects.
It should be moved to a separate definition, either all by itself or as a new group of HttpComponents Gump projects.
"
0,"FastVectorHighlighter: Make FragmentsBuilder use Encoder. Make FragmentsBuilder use Encoder, as Highlighter does."
0,"Add a LATENT FieldSelectorResult. I propose adding LATENT FieldSelectorResult

this would be similar to LAZY_LOAD except that it would NEVER cache the stored value

This will be useful for very large fields that should always go direct to disk (because they will take so much memory)
when caching Documents returned from a Searcher, the large field may be initially requested as LAZY_LOAD, however once someone reads this field, it will then get locked into memory. if this Document (and others like it) are cached, it can start to use a very large amount of memory for these fields

Contract for FieldSelectorResult.LATENT should be that it will always be pulled direct from the IndexInput and never be persisted in memory as part of a Fieldable

I could prepare a patch if desired

"
0,"SPI: Describe equality requirements of ItemIds. Michael Duerig asked for clarification regarding the equality of  ItemIds.

While discussing this we came to the following conclusion:

Two ItemIds should be considered equal if both the unique part and the path part are equal AND if they denote the same type of id (see #denotesNode).

If nobody objects i would adjust the javadoc of ItemId accordingly."
0,"Remove duplicate code in InternalValueFactory. After JCR-2245 has been applied some of the duplicate code in InternaValueFactory can be removed.

Namely:
- create(String, int)
- all other create methods that are non-binary ?"
0,Add pattern matching for paths. I suggest to add utility classes to spi-commons which can be used to do pattern matching on paths similar to regular expressions. 
0,"RMIC not working in subprojects when compiling parent using maven2. This is because there is a bug such that if you have a child build which uses the ant plugin it inherits the plugin dependencies of the first time the plugin is declared.

The workaround is to put the antrun plugin in the toplevel, and add the java jar to its plugin dependencies.

(reference: http://mail-archives.apache.org/mod_mbox/maven-users/200602.mbox/%3CC2CDEFBECFC9A14892BCCFB4C95F4868044F8230@EX-201.mail.navisite.com%3E)"
0,"Rename Analyzer.reusableTokenStream() to tokenStream(). All Analysis consumers now use reusableTokenStream().  To finally make reuse mandatory, lets rename resuableTokenStream() to tokenStream() (removing the old tokenStream() method)."
0,"Eliminate class HostConfiguration. Remove the target host attribute from the HostConfiguration class. This will allow one HostConfiguration object to be used for different targets.
The problem is that currently MultiThreadedHttpConnectionManager uses HostConfiguration objects as cache keys, which needs to be changed.

This is a followup to HTTPCLIENT-615.

cheers,
  Roland
"
0,"Replace BundleFsPersistenceManager with DerbyPersistenceManager in the JR Core indexing tests. Running the JR Core tests yields a deprecation warning on account of workspace config being outdated for some indexing tests:

  INFO  o.a.j.core.config.BeanConfig - org.apache.jackrabbit.core.persistence.pool.BundleFsPersistenceManager is deprecated. Please use org.apache.jackrabbit.core.persistence.bundle.BundleFsPersistenceManager instead

This shows up 3 times in the logs because there are 3 indexing related workspaces that need this config update."
0,"terms index should not store useless suffixes. This idea came up when discussing w/ Robert how to improve our terms index...

The terms dict index today simply grabs whatever term was at a 0 mod 128 index (by default).

But this is wasteful because you often don't need the suffix of the term at that point.

EG if the 127th term is aa and the 128th (indexed) term is abcd123456789, instead of storing that full term you only need to store ab.  The suffix is useless, and uses up RAM since we load the terms index into RAM.

The patch is very simple.  The optimization is particularly easy because terms are now byte[] and we sort in binary order.

I tested on first 10M 1KB Wikipedia docs, and this reduces the terms index (tii) file from 3.9 MB -> 3.3 MB = 16% smaller (using StandardAnalyzer, indexing body field tokenized but title / date fields untokenized).  I expect on noisier terms dicts, especially ones w/ bad terms accidentally indexed, that the savings will be even more.

In the future we could do crazier things.  EG there's no real reason why the indexed terms must be regular (every N terms), so, we could instead pick terms more carefully, say ""approximately"" every N, but favor terms that have a smaller net prefix.  We can also index more sparsely in regions where the net docFreq is lowish, since we can afford somewhat higher seek+scan time to these terms since enuming their docs will be much faster."
0,"Run 'test-tag' in nightly build. Changes in this trivial patch:
- ant target 'nightly' now also depends on 'test-tag'
- adds property 'compatibility.tag' to common-build.xml that should always point to the last tagged release; its unit tests will be downloaded unless -Dtag="""" is used to override
- 'download-tag' does not fail if the svn checkout wasn't successful; instead 'test-tag' checks if the specified tag is checked-out and available, if not it fails "
0,"Nuke SpanFilters and CachingSpanFilter (maybe move to sandbox). SpanFilters are inefficient and OOM easily (they don't scale at all: Create large Lists of Objects for every match, also filtering deleted docs is a pain). Some talks with Grant on Eurocon and also the fact that caching of them is still broken in 3.x (but fixed on trunk) - I assume nobody uses them, so let's nuke them. They are also in wrong package, so standard statement: ""Die, SpanFilters, die!"""
0,"WorkspaceUpdateChannel.updateCommitted logs too much. On each cluster record update, an info message is logged.

I think this is too much and logging should be reduced to the DEBUG level."
0,"Contrib JCR-Server: improve handing of strong etags. copied from dev-mail:

[..]

so... could we
- delegate the calculation of the etag to the
  commands, letting them decide on whether they are able
  to provide any and whether it would be a strong or a
  weak one?
- remove that additional method in NodeResource that is
  not used? "
0,Buffered I/O in IndexInfos. IndexInfos currently uses plain Input/OutputStreams without buffering.
0,"Get rid of backwards tags. This is a followup on: [http://www.lucidimagination.com/search/document/bb6c23b6e87c0b63/back_compat_folders_in_tags_when_i_svn_update#3000a2232c678031]

Currently we use tags for specifying the revision number in the backwards branch that matches the current development branch revision (in common-build.xml). The idea is to just specify the corresponding revision no of the backwards branch in common-build.xml and the backwards-test target automatically handles up/down/co:

- We just give the rev number in common-build in common-build.xml as a property backwards-rev=""XXXX"". This property is used also in building the command line which is also a property backwards-svn-args=""-r $backwards-rev"". By that you can use ""ant -Dbackwards-svn-args=''"" to force test-backwards to checkout/update to head of branch (recommened for developers).

- we should rename target to ""test-backwards"" and keep a ""test-tag"" with dependency to that for compatibility

- The checkout on backwards creates a directory ""backwards/${backwards-branch}"" and uses ""svn co ${backwards-svn-args} 'http://svn.../${backwards-branch}' 'backwards/${backwards-branch}'"". The cool thing, the dir is checked out if non existent, but if the checkout already exists, svn co implicitely does an svn up to the given revision (it will also downgrade and merge if newer). So the test-backwards target always updates the checkout to the correct revision. I had not tried with local changes, but this should simply merge as an svn up.

The workflow for committing fixes to bw would be:

- First use ""svn up"" to upgrade the backwards working copy to HEAD.
- Commit the changes
- Copy and paste the message ""Committed revision XXXX"" to common-build.xml
- Commit the changes in trunk
"
0,"Remove Lucene core's FunctionQuery impls. As part of the consolidation of FunctionQuerys, we want to remove Lucene core's impls.  Included in this work, we will make sure that all the functionality provided by the core impls is also provided by the new module.  Any tests will be ported across too, to increase the test coverage."
0,new CachingNamespaceResolver introduces dependency from commons-jackrabbit to commons-collections. new CachingNamespaceResolver introduces dependency from commons-jackrabbit to commons-collections which is undesried
0,ValueFactory is not extensible. The Jackrabbit ValueFactory implementation should have a generic base class in jackrabbit-jcr-commons. This base class could be reused in SPI.
0,"Add static readSnapshotsInfo to PersistentSnapshotDeletionPolicy. PSDP persists the snapshots information in a Directory. When you open PSDP, it obtains a write lock on the snapshots dir (by keeping an open IndexWriter), and updates the directory when snapshots are created/released.

This causes problem in the following scenario -- you have two processes, one updates the 'content' index and keeps PSDP open (because it also takes snapshots). Another process wants to read the existing snapshots information and open a read-only IndexReader on the 'content' index. The other process cannot read the existing snapshots information, because p1 keeps a write lock on the snapshots directory.

There are two possible solutions:
# Have PSDP open the IndexWriter over the directory for each snapshot/release. A bit expensive, and unnecessary.
# Introduce a static readSnapshotsInfo on PSDP which accepts a Directory and returns the snapshots information. IMO it's cleaner, and won't have the performance overhead of opening/closing the IW as before.

I'll post a patch (implementing the 2nd approach) shortly. I'd appreciate any comments."
0,"Ensure the features.html and index.html adequately give httpclient enough credit. See the email thread started by Eric Johnson.
http://archives.apache.org/eyebrowse/BrowseList?listId=128&by=thread&from=316092

Initial post:
Based on the recent URI discussion, and some other points, it strikes me that we
could take a little more credit for the work that has gone into HttpClient.

On the HttpClient home page
(http://jakarta.apache.org/commons/httpclient/index.html) four RFCs are listed.

Given all the discussion about URIs being thrown around, I think it might be
reasonable to add RFC 2396 - for URI compliance.  Then there is RFC 1867, for
multipart/form-data POST requests (I think I got the right number there).  Are
there RFCs corresponding to our ""cookie"" compliance? Any other RFCs we can claim
credit for conforming to?

With the recent ""Protocol"" changes, I think we've made it relatively
straightforward for clients of HttpClient to plug in their own secure sockets
implementations, making it easier to use third party, non-Sun solutions."
0,MultiFields not thread safe. MultiFields looks like it has thread safety issues
0,"Change superclass of TrieRangeQuery. This patch changes the superclass of TrieRangeQuery to ConstantScoreQuery. The current implementation is using rewrite() and was copied from early RangeQueries. But this is not needed, the TrieRangeQuery can easily subclassed from ConstantScoreQuery.

If LUCENE-1345 is solved, the whole TrieRangeQuery can be removed, as TrieRangeFilter can be added to BooleanQueries. The whole TrieRangeQuery class is just a convenience class for easier usage of the trie contrib."
0,"Allow indexingConfiguration to be loaded from the classpath. The ""indexingConfiguration"" attribute in the SearchIndex configuration (http://wiki.apache.org/jackrabbit/IndexingConfiguration) actually requires an absolute filesystem path.

It would be nice if SearchIndex would also accept a file available in the classpath... although you can use variables like ${wsp.home} or similar there are many scenarios where a classpath resource would help (for example when creating a new workspace the directory structure is automatically created by jackrabbit and doesn't need to be already available but the indexing configuration file does).

I am attaching a simple patch to SearchIndex that tries to load the file from the classpath if it has not been found. Since priority is given to the old behavior (file before classpath) so it's fully backward compatible.

Diff has been generated against trunk, it would be nice to have this patch also on the 2.0 branch.
 
 "
0,"Public API inconsistency. org.apache.lucene.index.SegmentInfos is public, and contains public methods (which is good for expert-level index manipulation tools such as Luke). However, SegmentInfo class has package visibility. This leads to a strange result that it's possible to read SegmentInfos, but it's not possible to access its details (SegmentInfos.info(int)) from a user application.

The solution is to make SegmentInfo class public."
0,"Add NumericField, make plain text numeric parsers public in FieldCache, move trie parsers to FieldCache. In discussions about LUCENE-1673, Mike & me wanted to add a new NumericField to o.a.l.document specific for easy indexing. An alternative would be to add a NumericUtils.newXxxField() factory, that creates a preconfigured Field instance with norms and tf off, optionally a stored text (LUCENE-1699) and the TokenStream already initialized. On the other hand NumericUtils.newXxxSortField could be moved to NumericSortField.

I and Yonik tend to use the factory for both, Mike tends to create the new classes.

Also the parsers for string-formatted numerics are not public in FieldCache. As the new SortField API (LUCENE-1478) makes it possible to support a parser in SortField instantiation, it would be good to have the static parsers in FieldCache public available. SortField would init its member variable to them (instead of NULL), so making code a lot easier (FieldComparator has this ugly null checks when retrieving values from the cache).

Moving the Trie parsers also as static instances into FieldCache would make the code cleaner and we would be able to hide the ""hack"" StopFillCacheException by making it private to FieldCache (currently its public because NumericUtils is in o.a.l.util)."
0,"Add tests.iter.min to improve controlling tests.iter's behavior. As discussed here: http://lucene.472066.n3.nabble.com/Stop-iterating-if-testsFailed-td2747426.html, this issue proposes to add tests.iter.min in order to allow one better control over how many iterations are run:

* Keep tests.iter as it is today
* Add tests.iter.min (default to tests.iter) to denote that at least N instances of the test should run until there's either a failure or tests.iter is reached.

If one wants to run until the first failure, he can set tests.iter.min=1 and tests.iter=X -- up to X instances of the test will run, until the first failure.

Similarly, one can set tests.iter=N to denote that at least N instances should run, regardless if there were failures, but if after N runs a failure occurred, the test should stop.

Note: unlike what's proposed on the thread, tests.iter.max is dropped from this proposal as it's exactly like tests.iter, so no point in having two similar parameters.

I will work on a patch tomorrow."
0,"Via NTLM proxy to SSL Apache/BasicAuth. - worked in may 22nd, but broken in beta1. Hi there,

This morning I downloaded beta 1 and tried a small piece of code to connect to 
a SSLified apache server (using basic authentication) via a MS-Proxy 2.0 with 
NTLM enabled. The sourcecode of my crashme is based on the 1st attachment for 
HTTPCLIENT-153. It differs from the original in using basic authentication for the 
webserver instead of NTLM.

It failed with this error:

--
10-jun-2003 16:39:05 org.apache.commons.httpclient.HttpMethodBase 
processAuthenticationResponse
INFO: Already tried to authenticate to ""website#"" but still receiving 407.
Status: 407 : Proxy authentication required
--

Then I downloaded a fresh night build (commons-httpclient-20030605) which also 
failed :/

Then I went back to an old build from May (commons-httpclient-20030522) which 
worked like a charm!!!

Using MSIE I can succesfully connect to the apache server. I know it's not a 
problem with typos because I have MSIE ask me for all creds.

Seems somethings got broken along the way. If I can help, please ask!

Cheers."
0,"Add support for Digest authentication to the Authenticator class. Here's some code initially whipped up by Geza for Apache Axis, now adapted to
HTTPClient that adds support for Digest authentication to the Authenticator
class. I have tested this code against tomcat 4.0.4 with a sample code that
calls an Apache Axis Web Service. One caveat according to Geza, the code ""Right
now does not support qop-int""."
0,"please log allocation of new connections to support debugging, testing. I'd like to suggest that the MultiThreaded connection manager emit a trace-level log when it 
allocates a new HttpConnection to support debugging and testing.  I added one while working on 
my integration in Apache Axis (see org.apache.axis.transport.http.CommonsHTTPSender) and 
figured this would be of general use.  I'll attach a patch with the oh-so-minor addition after 
submitting this enhancement request."
0,TestFieldsReader - TestLazyPerformance problems w/ permissions in temp dir in multiuser environment. Was trying to setup some enhancements to the nightly builds and the testLazyPerformance test failed in TestFieldsReader since it couldn't delete the lazyDir directory from someone else's running of the test.  Will change it to append user.name System property to the directory name.
0,"move DocumentStoredFieldsVisitor to o.a.l.document. when examining the changes to the field/document API, i noticed this class was in o.a.l.index

I think it should be in o.a.l.document, its more intuitive packaging"
0,"Test failure: org.apache.jackrabbit.test.TestAll. Subsequent test runs fail unless doing a mvn clean first.

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.jackrabbit.spi2jcr.spi.TestAll
Tests run: 50, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.965 sec
Running org.apache.jackrabbit.test.TestAll
Tests run: 1038, Failures: 11, Errors: 0, Skipped: 0, Time elapsed: 44.925 sec <<< FAILURE!
Running org.apache.jackrabbit.spi2jcr.jcr2spi.TestAll
Tests run: 394, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.416 sec

Results :

Failed tests:
  testOrderByAscending(org.apache.jackrabbit.test.api.query.SQLOrderByTest)
  testOrderByDescending(org.apache.jackrabbit.test.api.query.SQLOrderByTest)
  testOrderByDefault(org.apache.jackrabbit.test.api.query.SQLOrderByTest)
  testDocOrderIndexedNotation(org.apache.jackrabbit.test.api.query.XPathPosIndexTest)
  testDocOrderPositionFunction(org.apache.jackrabbit.test.api.query.XPathDocOrderTest)
  testDocOrderPositionIndex(org.apache.jackrabbit.test.api.query.XPathDocOrderTest)
  testDocOrderLastFunction(org.apache.jackrabbit.test.api.query.XPathDocOrderTest)
  testDocOrderFirstFunction(org.apache.jackrabbit.test.api.query.XPathDocOrderTest)
  testOrderByAscending(org.apache.jackrabbit.test.api.query.XPathOrderByTest)
  testOrderByDescending(org.apache.jackrabbit.test.api.query.XPathOrderByTest)
  testOrderBy(org.apache.jackrabbit.test.api.query.XPathOrderByTest)

"
0,"Behavior on hard power shutdown. When indexing a large number of documents, upon a hard power failure  (e.g. pull the power cord), the index seems to get corrupted. We start a Java application as an Windows Service, and feed it documents. In some cases (after an index size of 1.7GB, with 30-40 index segment .cfs files) , the following is observed.

The 'segments' file contains only zeros. Its size is 265 bytes - all bytes are zeros.
The 'deleted' file also contains only zeros. Its size is 85 bytes - all bytes are zeros.

Before corruption, the segments file and deleted file appear to be correct. After this corruption, the index is corrupted and lost.

This is a problem observed in Lucene 1.4.3. We are not able to upgrade our customer deployments to 1.9 or later version, but would be happy to back-port a patch, if the patch is small enough and if this problem is already solved.
"
0,"User definable default headers support. Provide the ability to set default headers to be sent on every request.  Should
be used whenever an object is created or recycled.  Needs to be user
configurable."
0,"make compoundfilewriter public. CompoundFileReader is public, but CompoundFileWriter is not.

I propose we make it public + @lucene.internal instead (just in case someone 
else finds themselves wanting to manipulate cfs files)
"
0,"Support lower and upper case functions in ""order by"" clause. The query languages should support lower- and upper-case functions within the ""order by"" clause.  This would provide case-insensitive ordering of query results.

Example:  Find all ""nt:base"" nodes ordered by the ""foo"" property, but ignoring case

In XPath:

//element(*,nt:base) order by fn:lower-case(@foo)

In SQL:

SELECT * FROM nt:base ORDER BY lower(foo)

"
0,"Failures during contrib builds, when classes in core were changed without ant clean. From java-dev by Shai Erera:

{quote}
I've noticed that sometimes, after I run test-core and test-contrib, and then change core code, test-contrib fail on NoSuchMethodError and stuff like that. I've noticed that core.jar exists under build, and I assumed it's used by test-contrib, and probably is not recreated after core code has changed.

I verified it when looking in contrib-build.xml, which defines a property lucene.jar.present which is set to true if the jar is ... well, present. Which I believe is the reason for these failures. I've been thinking how to resolve that, and I can think of two ways:

(1) have test-core always delete that file, but that has two issues:
(1.1) It's redundant if the code hasn't changed.
(1.2) It forces you to either jar-core or test-core before you test-contrib, if you want to make sure you run w/ the latest jar.

or

(2) have test-contrib always call jar-core, which will first delete the file and then re-create it by compiling first. Compiling should not do anything if the code hasn't changed. So the only waste would be to create the .jar, but I think that's quite fast?

Does anyone, with more Ant skills than me, know of a better way to detect from test-contrib that core code has changed and only then rebuild the jar?
{quote}"
0,Improved join query performance. Our current implementation of SQL2 join queries does not perform very well on pretty much any non-trivial data set.
0,"Clean caches in node type registry on session logout. When running the JCR tests the memory consumption increases steadily. At the end of the test run it consumes about 300 Mb on my machine. There's not really a memory leak in jcr2spi, because the JUnit tests keep references to Session and Node instances until the end of the test run, but  it would be nice if those instances were a bit more lightweight."
0,"FieldSortedHitQueue.lessThan() should not be final. The final seems to provide little benefit and it takes away the ability to
specialize this method (which I need to do, forcing a customization of Lucene to
remove the final)."
0,"Add IW.add/updateDocuments to support nested documents. I think nested documents (LUCENE-2454) is a very compelling addition
to Lucene.  It's also a popular (many votes) issue.

Beyond supporting nested document querying, which is already an
incredible addition since it preserves the relational model on
indexing normalized content (eg, DB tables, XML docs), LUCENE-2454
should also enable speedups in grouping implementation when you group
by a nested field.

For the same reason, it can also enable very fast post-group facet
counting impl (LUCENE-3097) when you what to
count(distinct(nestedField)), instead of unique documents, as your
""identifier"".  I expect many apps that use faceting need this ability
(to count(distinct(nestedField)) not distinct(docID)).

To support these use cases, I believe the only core change needed is
the ability to atomically add or update multiple documents, which you
cannot do today since in between add/updateDocument calls a flush (eg
due to commit or getReader()) could occur.

This new API (addDocuments(Iterable<Document>), updateDocuments(Term
delTerm, Iterable<Document>) would also further guarantee that the
documents are assigned sequential docIDs in the order the iterator
provided them, and that the docIDs all reside in one segment.

Segment merging never splits segments apart, so this invariant would
hold even as merges/optimizes take place.
"
0,"MergePolicy.OneMerge.segments should be List<SegmentInfo> not SegmentInfos, Remove Vector<SI> subclassing from SegmentInfos & more refactoring. SegmentInfos carries a bunch of fields beyond the list of SI, but for merging purposes these fields are unused.

We should cutover to List<SI> instead.

Also SegmentInfos subclasses Vector<SI>, this should be removed and the collections be hidden inside the class. We can add unmodifiable views on it (asList(), asSet())."
0,"GQLTest fails occasionally. This is again the text extraction, which may hit a time out."
0,"move deletes under codec. After LUCENE-3631, this should be easier I think.

I haven't looked at it much myself but i'll play around a bit, but at a glance:
* SegmentReader to have Bits liveDocs instead of BitVector
* address the TODO in the IW-using ctors so that SegmentReader doesn't take a parent but just an existing core.
* we need some type of minimal ""MutableBits"" or similar subinterface of bits. BitVector and maybe Fixed/OpenBitSet could implement it
* BitVector becomes an impl detail and moves to codec (maybe we have a shared base class and split the 3.x/4.x up rather than the conditional backwards)
* I think the invertAll should not be used by IndexWriter, instead we define the codec interface to say ""give me a new MutableBits, by default all are set"" ?
* redundant internally-consistent checks in checkLiveCounts should be done in the codec impl instead of in SegmentReader.
* plain text impl in SimpleText."
0,JE Directory Implementation. I've created a port of DbDirectory to JE
0,"Add new bit set impl for caching filters. I think OpenBitSet is trying to satisfy too many audiences, and it's
confusing/error-proned as a result.  It has int/long variants of many
methods.  Some methods require in-bound access, others don't; of those
others, some methods auto-grow the bits, some don't.  OpenBitSet
doesn't always know its numBits.

I'd like to factor out a more ""focused"" bit set impl whose primary
target usage is a cached Lucene Filter, ie a bit set indexed by docID
(int, not long) whose size is known and fixed up front (backed by
final long[]) and is always accessed in-bounds.
"
0,"Similarity can only be set per index, but I may want to adjust scoring behaviour at a field level. Similarity can only be set per index, but I may want to adjust scoring behaviour at a field level, to faciliate this could we pass make field name available to all score methods.
Currently it is only passed to some such as lengthNorm() but not others such as tf()"
0,"JavaDoc getConnection methods in Connection Managers. The JavaDoc for the getConnection() methods in the Simple and MultiThreaded
Connection managers is taken from the interface, and so is too generic.

The Javadoc for the doGetConnection() method in the MultiThreaded manager is
fine, but is not visible in the JavaDoc

The Simple Mangager JavaDoc could likewise be improved

[I hope to provide patches shortly]"
0,"Configuration of CacheManager memory sizes. (I already posted this as comments under JCR-619.)

The maximum size for all caches in CacheManager is hardcoded to 16 megabytes and there's no way to change that. It would be nice if this as well as other CacheManager parameters were configurable. It's just a waste running Jackrabbit on a server with gigabytes of memory and only using 16 megabytes for cache...

I have created a really simple and straightforward patch (jackrabbit-cachemanager-config.patch) which enables reaching the CacheManager instance through RepositoryImpl object and setting all three of its memory parameters. The memory parameters are no longer static constants, but instance fields getting initial values from constants (so the default behavior of the class remains the same).

(It would be even nicer if these parameters were configurable via configuration files, but that should probably be implemented by someone close to the project.)"
0,"Add a MBean method to programatically create a new Workspace.. Would be useful to have a mbean method to create a new workspace to use if with a jmx console.

"
0,"don't use finalizers for FSIndexInput clones. finalizers are expensive, and we should avoid using them where possible.
It looks like this helped to tickle some kind of bug (looks like a JVM bug?)
http://www.nabble.com/15-minute-hang-in-IndexInput.clone%28%29-involving-finalizers-tf2826906.html#a7891015"
0,"Path parser does not allow trailing slashes. The current implementation of the Path class violates the spec in that the trailing slashes are not allowed while the path syntax clearly allows trailing slashes.

(Of course the Path class violates the spec in another way, too, in that it allows for the root path ""/"" to be valid while the path syntax in the spec does not allow that, which is clearly an error of the spec of course)."
0,"Add API for selective bundle consistency check (Jackrabbit-specific). Add a jackrabbit-specific API for doing a selective consistencyCheck, ie. on single nodes. The current entire-workspace check can be very slow if there workspace is large enough. Also it should be easy to write a tool to invoke that feature programmatically rather than by configuration + restart (see below).

Existing Implementation:
The current bundle consistencyCheck feature is enabled by setting a bundle PM parameter and restarting Jackrabbit, it will then run upon startup (see JCR-972 for the only issue regarding bundle consistency check). This check looks for broken parent-child relationships, ie. it will remove any child node entries that reference non-existing parent nodes. For non-existing parent UUIDs and other problems in bundles it will log those.

Outlook:
An advanced consistencyCheck could also check for non-existing version nodes and vice-versa (see JCR-630), but this is not the focus of this issue and could be a later addition to the API."
0,"Add method to remove mappings from NamespaceMapping. o.a.j.spi.commons.namespace.NamespaceMapping has currently no means to remove a mapping. I suggest to add a method
public String removeMapping(String uri) "
0,"Remove code duplication in MultiReader/DirectoryReader, make everything inside final. After making IndexReader readOnly (LUCENE-3606) there is no need to have completely different DirectoryReader and MultiReader, the current code is heavy code duplication and violations against finalness patterns. There are only few differences in reopen and things like isCurrent/getDirectory/...

This issue will clean this up by introducing a hidden package-private base class for both and only handling reopen and incRef/decRef different. DirectoryReader is now final and all fields in BaseMultiReader, MultiReader and DirectoryReader are final now. DirectoryReader has now only static factories, no public ctor anymore."
0,"add IndexCommit.isOptimized method. Spinoff from here:

    http://mail-archives.apache.org/mod_mbox/lucene-java-user/200807.mbox/%3C69de18140807010347s6269fea5r12c3212e0ec0a12a@mail.gmail.com%3E"
0,"Change remaining contrib streams/filters to use new TokenStream API. All other contrib streams/filters have already been converted with LUCENE-1460.

The two shingle filters are the last ones we need to convert."
0,JSR 283: QOM and SQL2. Adjusted title. This issue now covers all query changes and enhancements in JSR 283.
0,"improve HttpRoute API. Some of the constructors of HttpRoute have three boolean parameters.
Use enumerations to reduce the potential for confusion.

The flags for tunnelled and layered are not independent, since layered implies tunnelled.
These can be combined to a 3-valued enum.
"
0,"Bulgarian Analyzer. someone asked about bulgarian analysis on solr-user today... http://www.lucidimagination.com/search/document/e1e7a5636edb1db2/non_english_languages
I was surprised we did not have anything.

This analyzer implements the algorithm specified here, http://members.unine.ch/jacques.savoy/Papers/BUIR.pdf

In the measurements there, this improves MAP approx 34%
"
0,"Get rid of NonMatchingScorer from BooleanScorer2. Over in LUCENE-1614 Mike has made a comment about removing NonMatchinScorer from BS2, and return null in BooleanWeight.scorer(). I've checked and this can be easily done, so I'm going to post a patch shortly. For reference: https://issues.apache.org/jira/browse/LUCENE-1614?focusedCommentId=12715064&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12715064.

I've marked the issue as 2.9 just because it's small, and kind of related to all the search enhancements done for 2.9."
0,"IndexWriter has hard limit on max concurrency. DocumentsWriter has this nasty hardwired constant:

{code}
private final static int MAX_THREAD_STATE = 5;
{code}

which probably I should have attached a //nocommit to the moment I
wrote it ;)

That constant sets the max number of thread states to 5.  This means,
if more than 5 threads enter IndexWriter at once, they will ""share""
only 5 thread states, meaning we gate CPU concurrency to 5 running
threads inside IW (each thread must first wait for the last thread to
finish using the thread state before grabbing it).

This is bad because modern hardware can make use of more than 5
threads.  So I think an immediate fix is to make this settable
(expert), and increase the default (8?).

It's tricky, though, because the more thread states, the less RAM
efficiency you have, meaning the worse indexing throughput.  So you
shouldn't up and set this to 50: you'll be flushing too often.

But... I think a better fix is to re-think how threads write state
into DocumentsWriter.  Today, a single docID stream is assigned across
threads (eg one thread gets docID=0, next one docID=1, etc.), and each
thread writes to a private RAM buffer (living in the thread state),
and then on flush we do a merge sort.  The merge sort is inefficient
(does not currently use a PQ)... and, wasteful because we must
re-decode every posting byte.

I think we could change this, so that threads write to private RAM
buffers, with a private docID stream, but then instead of merging on
flush, we directly flush each thread as its own segment (and, allocate
private docIDs to each thread).  We can then leave merging to CMS
which can already run merges in the BG without blocking ongoing
indexing (unlike the merge we do in flush, today).

This would also allow us to separately flush thread states.  Ie, we
need not flush all thread states at once -- we can flush one when it
gets too big, and then let the others keep running.  This should be a
good concurrency gain since is uses IO & CPU resources ""throughout""
indexing instead of ""big burst of CPU only"" then ""big burst of IO
only"" that we have today (flush today ""stops the world"").

One downside I can think of is... docIDs would now be ""less
monotonic"", meaning if N threads are indexing, you'll roughly get
in-time-order assignment of docIDs.  But with this change, all of one
thread state would get 0..N docIDs, the next thread state'd get
N+1...M docIDs, etc.  However, a single thread would still get
monotonic assignment of docIDs.
"
0,"Next steps towards flexible indexing. In working on LUCENE-1410 (PFOR compression) I tried to prototype
switching the postings files to use PFOR instead of vInts for
encoding.

But it quickly became difficult.  EG we currently mux the skip data
into the .frq file, which messes up the int blocks.  We inline
payloads with positions which would also mess up the int blocks.
Skipping offsets and TermInfo offsets hardwire the file pointers of
frq & prox files yet I need to change these to block + offset, etc.

Separately this thread also started up, on how to customize how Lucene
stores positional information in the index:

  http://www.gossamer-threads.com/lists/lucene/java-user/66264

So I decided to make a bit more progress towards ""flexible indexing""
by first modularizing/isolating the classes that actually write the
index format.  The idea is to capture the logic of each (terms, freq,
positions/payloads) into separate interfaces and switch the flushing
of a new segment as well as writing the segment during merging to use
the same APIs.
"
0,"Adding DerbyDataStore to handle proper close of the embedded database. When using embedded Derby in conjunction with DbDataStore, the Derby database is never shutdown, as it requires special code to be executed (creating a Connection with "";shutdown=true"")
We may provide a DerbyDataStore extending standard DbDataStore for handling that."
0,"improve automaton performance by running on byte[]. Currently, when enumerating terms, automaton must convert entire terms from flex's native utf-8 byte[] to char[] first, then step each char thru the state machine.

we can make this more efficient, by allowing the state machine to run on byte[], so it can return true/false faster."
0,"BasicResponseHandler Javadoc Needs Clarification. The class-level javadoc for BasicResponseHandler indicates that it reads the response body before throwing an Exception for responses with status code >= 300, which is not the case."
0,"Improve BufferedIndexInput.readBytes() performance. During a profiling session, I discovered that BufferedIndexInput.readBytes(),
the function which reads a bunch of bytes from an index, is very inefficient
in many cases. It is efficient for one or two bytes, and also efficient
for a very large number of bytes (e.g., when the norms are read all at once);
But for anything in between (e.g., 100 bytes), it is a performance disaster.
It can easily be improved, though, and below I include a patch to do that.

The basic problem in the existing code was that if you ask it to read 100
bytes, readBytes() simply calls readByte() 100 times in a loop, which means
we check byte after byte if the buffer has another character, instead of just
checking once how many bytes we have left, and copy them all at once.

My version, attached below, copies these 100 bytes if they are available at
bulk (using System.arraycopy), and if less than 100 are available, whatever
is available gets copied, and then the rest. (as before, when a very large
number of bytes is requested, it is read directly into the final buffer).

In my profiling, this fix caused amazing performance
improvement: previously, BufferedIndexInput.readBytes() took as much as 25%
of the run time, and after the fix, this was down to 1% of the run time! However, my scenario is *not* the typical Lucene code, but rather a version of Lucene with added payloads, and these payloads average at 100 bytes, where the original readBytes() did worst. I expect that my fix will have less of an impact on ""vanilla"" Lucene, but it still can have an impact because it is used for things like reading fields. (I am not aware of a standard Lucene benchmark, so I can't provide benchmarks on a more typical case).

In addition to the change to readBytes(), my attached patch also adds a new
unit test to BufferedIndexInput (which previously did not have a unit test).
This test simulates a ""file"" which contains a predictable series of bytes, and
then tries to read from it with readByte() and readButes() with various
sizes (many thousands of combinations are tried) and see that exactly the
expected bytes are read. This test is independent of my new readBytes()
inplementation, and can be used to check the old implementation as well.

By the way, it's interesting that BufferedIndexOutput.writeBytes was already efficient, and wasn't simply a loop of writeByte(). Only the reading code was inefficient. I wonder why this happened."
0,"Change all multi-term querys so that they extend MultiTermQuery and allow for a constant score mode. Cleans up a bunch of code duplication, closer to how things should be - design wise, gives us constant score for all the multi term queries, and allows us at least the option of highlighting the constant score queries without much further work."
0,"Upgrade commons-codec 1.4 -> 1.6. commons-codec 1.4 is buggy, see for example https://issues.apache.org/jira/browse/CODEC-99"
0,"Patch to JCR-RMI contribution adding Version/VersionHistory support. Hi Jukka,

You contributed the famous RMI extension to Jackrabbit. Many thanks. On my way to implement an Eclipse plugin to access repositories this provides great help. Unfortunately your contribution does not include support for versioning yet.

I took the freedom to add this missing piece and provide it to you to add it to your contribution. Thanks."
0,"cleanup contrib/demo. I don't think we should include optimize in the demo; many people start from the demo and may think you must optimize to do searching, and that's clearly not the case.

I think we should also use a buffered reader in FileDocument?

And... I'm tempted to remove IndexHTML (and the html parser) entirely.  It's ancient, and we now have Tika to extract text from many doc formats."
0,"SQL Server support in clustering module. I realize the clustering module doesn't specifically support SQL Server yet (there's no mssql.ddl), but I still tried to run the repository against SQL Server with clustering enabled in the hope that the default schema (default.ddl) would suffice. Apparently, it doesn't (unless I'm doing something very wrong), since I kept getting the following error whenever a write operation was attempted:

2007-05-25 14:48:06,757 WARN  [org.apache.jackrabbit.core.journal.DatabaseJournal] Error while rolling back connection: You cannot rollback with autocommit set!
2007-05-25 14:48:06,757 ERROR [org.apache.jackrabbit.core.cluster.ClusterNode] Unable to commit log entry.
org.apache.jackrabbit.core.journal.JournalException: Unable to append revision 1090.
	at org.apache.jackrabbit.core.journal.DatabaseJournal.append
	at org.apache.jackrabbit.core.journal.AppendRecord.update(AppendRecord.java:242)
	at org.apache.jackrabbit.core.cluster.ClusterNode$WorkspaceUpdateChannel.updateCommitted(ClusterNode.java:530)
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:725)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:855)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:326)
	at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:313)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:302)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:306)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1214)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:849)
Caused by: java.sql.DataTruncation: Data truncation
	at net.sourceforge.jtds.jdbc.SQLDiagnostic.addDiagnostic(SQLDiagnostic.java:379)
	at net.sourceforge.jtds.jdbc.TdsCore.tdsErrorToken(TdsCore.java:2781)
	at net.sourceforge.jtds.jdbc.TdsCore.nextToken(TdsCore.java:2224)
	at net.sourceforge.jtds.jdbc.TdsCore.getMoreResults(TdsCore.java:628)
	at net.sourceforge.jtds.jdbc.JtdsStatement.processResults(JtdsStatement.java:525)
	at net.sourceforge.jtds.jdbc.JtdsStatement.executeSQL(JtdsStatement.java:487)
	at net.sourceforge.jtds.jdbc.JtdsPreparedStatement.execute(JtdsPreparedStatement.java:475)
	at org.jboss.resource.adapter.jdbc.WrappedPreparedStatement.execute(WrappedPreparedStatement.java:183)
	at org.apache.jackrabbit.core.journal.DatabaseJournal.append
	... 58 more

However, I think I got things working by using a modified version of default.ddl, with the only change being the type of the REVISION_DATA field (varbinary -> IMAGE)."
0,Add import-export tool. We at <GX> creative online development would like to contribute our command-line import-export tool to the Apache Jackrabbit project. This tool is capable of exporting and importing all kinds of repository content (including custom nodetypes and namespace mappings) in a persistence-layer independent way. 
0,"Build SegmentCodecs incrementally for consistent codecIDs during indexing. currently we build the SegementCodecs during flush which is fine as long as no codec needs to know which fields it should handle. This will change with DocValues or when we expose StoredFields / TermVectors via Codec (see LUCENE-2621 or LUCENE-2935). The other downside it that we don't have a consistent view of which codec belongs to which field during indexing and all FieldInfo instances are unassigned (set to -1). Instead we should build the SegmentCodecs incrementally as fields come in so no matter when a codec needs to be selected to process a document / field we have the right codec ID assigned.

"
0,Remove autoCommit from IndexWriter. IndexWriter's autoCommit is deprecated; in 3.0 it will be hardwired to false.
0,"overhaul connection manager and associated connection interface. MultiThreadedHttpConnectionManager/HttpHostConnection needs to be overhauled to provide a layer on top of OperatedClientConnection.
Preliminary working names: ThreadSafeClientConnManager/ManagedClientConnection

This implies some work on former HttpMethodDirector and HttpClient to verify completeness of the new connection management API.
"
0,"Documentation Error for FilteredTermEnum. As pointed out in 
http://nagoya.apache.org/eyebrowse/ReadMsg?listName=lucene-user@jakarta.apache.org&msgNo=11034
the documentation of FilteredTermEnum.term() is wrong:
it says 
'Returns the current Term in the enumeration. Initially invalid, valid after
next() called for the first time.'
but the implementation of the constructors of the two derived classes
(FuzzyTermEnum and WildcardTermEnum) already initializes the object to point to
the first match. So calling next() before accessing terms will leave out the
first match.

So I suggest to replace the second sentance by something like
'Returns null if no Term matches or all terms have been enumerated.'
(I checked that for WildcardTermEnum only).
Further one might add some note to the docs of the constructors of FuzzyTermEnum
and WildcardTermEnum that they will point to the first element of the
enumeration (if any)."
0,"Precompile JavaCC parsers in jackrabbit-spi-commons. The JavaCC-generated Java source files in jackrabbit-spi-commons require special configuration when importing Jackrabbit sources to an IDE like Eclipse. To make IDE integration smoother it would be nice if precompiled copies of the Java files existed the src/main/java folder.

Precompiling the sources would also allow us to avoid the JavaCC processing step during each Jackrabbit build. Instead we could have a separate profile for explicitly recompiling the JavaCC sources when they have been modified. In the past three years that has happened only once (JCR-952), so I think a bit of extra complexity there is justified by the simplification we can achieve in normal builds and IDE integration."
0,Fix Hits deprecation notice. Just needs to be committed to 2.9 branch since hits is now removed.
0,"RepositoryFactory implementation for jcr2spi. There should be a RepositoryFactory implementation in jcr2spi that also covers acquiring the underlying RepositoryService.

For this purpose I suggest to create:
-  a RepositoryServiceFactory in jackrabbit-spi, which encapsulates the spi implementation specifc instantiation of the RepositoryService. the factory probably just needs a single method that takes a parameters map.
- a RepositoryFactory implementation in jcr2spi, which works with a URI that contains all required information to connect/acquired the RepositoryService.

To use jcr2spi/spi2jcr/jackrabbit-core:
- jcr+file://path/to/repository/home?config=repository.xml

To use jcr2spi/spi2dav:
- jcr+dav://localhost:8080/server/repository/?br=4

To use jcr2spi/spi2davex:
- jcr+davex://localhost:8080/server/repository/

An implementation of RepositoryServiceFactory must check the scheme and decide if it can handle it and create a RepositoryService instance with it, otherwise it must return null. This means there would be a single name for the connect URI for all RepositoryServiceFactory implementations.

"
0,"[PATCH] XPathQueryBuilder reports misleading column numbers for faulty queries. XPathQueryBuilder returns an error string with the column offset where a parsing error occurred. Unfortunately this value is difficult to correlate to the users query string, as XPathQueryBuilder embellishes the query by doing

statement = ""for $v in "" + statement + "" return $v"";

This patch appends the modified statement to the query message so that the user can get the real position of the error."
0,"MMapDirectory speedups. MMapDirectory has some performance problems:
# When the file is larger than Integer.MAX_VALUE, we use MultiMMapIndexInput, 
which does a lot of unnecessary bounds-checks for its buffer-switching etc. 
Instead, like MMapIndexInput, it should rely upon the contract of these operations
in ByteBuffer (which will do a bounds check always and throw BufferUnderflowException).
Our 'buffer' is so large (Integer.MAX_VALUE) that its rare this happens and doing
our own bounds checks just slows things down.
# the readInt()/readLong()/readShort() are slow and should just defer to ByteBuffer.readInt(), etc
This isn't very important since we don't much use these, but I think there's no reason
users (e.g. codec writers) should have to readBytes() + wrap as bytebuffer + get an 
IntBuffer view when readInt() can be almost as fast..."
0,"SPI-commons:  QValueTest.testDateValueEquality2 fails due to changes made with JCR-1018. with the introduction of QValue.getCalendar() the internal value for DATE-properties is now a Calendar (was
String). however, the equals() method has not been adjusted."
0,"randomize skipInterval in tests. we probably don't test the multi-level skipping very well, but skipInterval etc is now private to the codec, so for better test coverage we should parameterize it to the postings writers, and randomize it via mockrandomcodec."
0,"add option to CheckIndex to only check certain segments. Simple patch to add -segment option to CheckIndex tool, to have it only check the particular segment, instead of all segments, from your index."
0,"[PATCH] UpdateTest has two typos. UpdateTest has a typo where it doesn't grab an element out of an array, but uses the array itself to do comparisons. This patch fixes this."
0,"jackrabbit-jca rar archive misses required classes. Since the Maven 2 upgrade, the jackrabbit-jca rar archive doesn't contain the rar-specific classes in a jackrabbit-jca jar file that was previously also included in the archive."
0,"Remove unused (and untested) methods from ReaderUtil that are also veeeeery ineffective. ReaderUtil contains two methods that are nowhere used and not even tested. Additionally those are implemented with useless List->array copying; ineffective docStart calculation for a binary search later instead directly returning the reader while scanning -- and I am not sure if they really work as expected. As ReaderUtil is @lucene.internal we should remove them in 3.x and trunk, alternatively the useless array copy / docStarts handling should be removed and tests added:

{code:java}
public static IndexReader subReader(int doc, IndexReader reader)
public static IndexReader subReader(IndexReader reader, int subIndex)
{code}
"
0,"[PATCH] FSDirectory create() method deletes all files. hi all,

the current implementation of FSDirectory.create(...) silently deletes all files
(even empty directories) within the index directory when setting up a new index
with create option enabled. Lucene doesn't care when deleting files in the index
directory if they  belong to lucene or not. I don't think that this is a real
bug, but it can be a pain if somebody whants to store some private information
in the lucene index directory, e.g some configuration files.

Therefore i implemented a FileFilter which knows about the internal lucene file
extensions, so that all other files would never get touched when creating a new
index. The current patch is an enhancement in FSDirectory only. I don't think
that there is a need to make it available in the Directory class and change all
it's depending classes.

regards
Bernhard"
0,"lazily create SegmentMergeInfo.docMap. Since creating the docMap is expensive, and it's only used during segment merging, not searching, defer creation until it is requested.

SegmentMergeInfo is also used in MultiTermEnum, the term enumerator for a MultiReader.  TermEnum is used by queries such as PrefixQuery, RangeQuery, WildcardQuery, as well as RangeFilter, DateFilter, and sorting the first time (filling the FieldCache).

Performance Results:
  A simple single field index with 555,555 documents, and 1000 random deletions was queried 1000 times with a PrefixQuery matching a single document.

Performance Before Patch:
  indexing time = 121,656 ms
  querying time = 58,812 ms

Performance After Patch:
  indexing time = 121,000 ms
  querying time =         598 ms

A 100 fold increase in query performance!

All lucene unit tests pass."
0,"Automatic staging of the non-Maven release artefacts. Currently the Jackrabbit release process includes the following manual steps in addition to the standard Maven release plugin invocations:

<script>
VERSION=x.y.z  # Release version number

# Prepare the release directory
mkdir target/$VERSION

# Copy the main release artifacts created in the release:perform stage
cp target/checkout/RELEASE-NOTES.txt target/$VERSION
cp target/checkout/target/jackrabbit-$VERSION-src.zip* target/$VERSION
cp target/checkout/jackrabbit-webapp/target/jackrabbit-webapp-$VERSION.war* target/$VERSION
cp target/checkout/jackrabbit-jca/target/jackrabbit-jca-$VERSION.rar* target/$VERSION
cp target/checkout/jackrabbit-standalone/target/jackrabbit-standalone-$VERSION.jar* target/$VERSION

# Add MD5 and SHA1 checksums
for BINARY in target/$VERSION/*.zip target/$VERSION/*ar; do
  openssl md5 < $BINARY > $BINARY.md5
  openssl sha1 < $BINARY > $BINARY.sha
done

# Upload the release directory to people.apache.org
scp -r target/$VERSION people.apache.org:public_html/jackrabbit/$VERSION
</script>

I'd like to automate these steps."
0,"Code coverage reports. Hi all,

We should be able to measure the code coverage of our unit testcases. I believe it would be very helpful for the committers, if they could verify before committing a patch if it does not reduce the coverage. 

Furthermore people could take a look in the code coverage reports to figure out where work needs to be done, i. e. where additional testcases are neccessary. It would be nice if we could add a page to the Lucene website showing the report, generated by the nightly build. Maybe you could add that to your preview page (LUCENE-707), Grant?

I attach a patch here that uses the tool EMMA to generate the code coverage reports. EMMA is a very nice open-source tool released under the CPL (same license as junit). The patch adds three targets to common-build.xml: 
- emma-check: verifys if both emma.jar and emma_ant.jar are in the ant classpath 
- emma-instrument: instruments the compiled code 
- generate-emma-report: generates an html code coverage report 

The following steps are neccessary in order to generate a code coverage report:
- add emma.jar and emma_ant.jar to your ant classpath (download emma from http://emma.sourceforge.net/)
- execute ant target 'emma-instrument' (depends on compile-test, so it will compile all core and test classes)
- execute ant target 'test' to run the unit tests
- execute ant target 'generate-emma-report'

To view the emma report open build/test/emma/index.html"
0,Remove unused method RedoLog.clear(). This method is not used anymore and can be removed.
0,"Unit tests for HttpConn. HttpConn needs more test coverage.
Starting at 0%.
"
0,"Make ObjectIterator implement RangeIterator interface. Currently, it's not possible to skip a part of results returned in the form of ObjectIterator (for example, to implement db-like pagination feature with offset/max parameters).

It would be great if ObjectIterator implement RangeIterator interface, and it's trivial enough since underlying NodeIterator implements this interface."
0,"Field Selection and Lazy Field Loading. The patch to come shortly implements a Field Selection and Lazy Loading mechanism for Document loading on the IndexReader.

It introduces a FieldSelector interface that defines the accept method:
FieldSelectorResult accept(String fieldName);

(Perhaps we want to expand this to take in other parameters such as the field metadata (term vector, etc.))

Anyone can implement a FieldSelector to define how they want to load fields for a Document.  
The FieldSelectorResult can be one of four values: LOAD, LAZY_LOAD, NO_LOAD, LOAD_AND_BREAK.  
The FieldsReader, as it is looping over the FieldsInfo, applies the FieldSelector to determine what should be done with the current field.

I modeled this after the java.io.FileFilter mechanism.  There are two implementations to date: SetBasedFieldSelector and LoadFirstFieldSelector.  The former takes in two sets of field names, one to load immed. and one to load lazily.  The latter returns LOAD_AND_BREAK on the first field encountered.  See TestFieldsReader for examples.

It should support UTF-8 (I borrowed code from Issue 509, thanks!).  See TestFieldsReader for examples

I added an expert method on IndexInput  named skipChars that takes in the number of characters to skip.  This is a compromise on changing the file format of the fields to better support seeking.  It does some of the work of readChars, but not all of it.  It doesn't require buffer storage and it doesn't do the bitwise operations.  It just reads in the appropriate number of bytes and promptly ignores them.  This is useful for skipping non-binary, non-compressed stored fields.

The biggest change is by far the introduction of the Fieldable interface (as per Doug's suggestion from a mailing list email on Lazy Field loading from a while ago).  Field is now a Fieldable.  All uses of Field have been changed to use Fieldable.  FieldsReader.LazyField also implements Fieldable.

Lazy Field loading is now implemented.  It has a major caveat (that is Documented) in that it clones the underlying IndexInput upon lazy access to the Field value.  IT IS UNDEFINED whether a Lazy Field can be loaded after the IndexInput parent has been closed (although, from what I saw, it does work).  I thought about adding a reattach method, but it seems just as easy to reload the document.  See the TestFieldsReader and DocHelper for examples.

I updated a couple of other tests to reflect the new fields that are on the DocHelper document.

All tests pass."
0,"Update the Wiki. The wiki needs updating.  For starters, the URL is still Jakarta.  I think infrastructure needs to be contacted to do this move.  If someone is so inclined, it might be useful to go through and cleanup/organize what is there."
0,"Allow polymorphic use of addParameter. I have some common code (in a reverse proxy server) that uses addParameter on 
instances of both PostMethod and MultipartPostMethod

It would be great if either addParameter were made an abstract method on 
ExpectContinueMethod or both were made to implement a common base class. Here's 
my workaround:

    private void addPostParameter(ExpectContinueMethod method, String name, 
String value) {
        if (method instanceof PostMethod) {
            ((PostMethod)method).addParameter(name, value);
        } else if (method instanceof MultipartPostMethod) {
            ((MultipartPostMethod)method).addParameter(name, value);
        } else {
            throw new IllegalArgumentException(""addPostParameter is only 
defined for PostMethod and MultipartPostMethod"");
        }
        
    }
    // whoa - smells pretty bad"
0,"Allow database as backend for clustering. Currently, clustering (see JCR-623) uses a shared file system folder in order to store modifications and synchronize all nodes in the cluster. Alternatively, a database backend should be available."
0,"[PATCH] BufferedIndexOutput - optimized writeBytes() method. I have created a patch that optimize writeBytes metod:

  public void writeBytes(byte[] b, int length) throws IOException {
    if (bufferPosition > 0) // flush buffer
      flush();
 
    if (length < BUFFER_SIZE) {
      flushBuffer(b, length);
      bufferStart += length;
    } else {
      int pos = 0;
      int size;
      while (pos < length) {
        if (length - pos < BUFFER_SIZE) {
          size = length - pos;
        } else {
          size = BUFFER_SIZE;
        }
        System.arraycopy(b, pos, buffer, 0, size);
        pos += size;
        flushBuffer(buffer, size);
        bufferStart += size;
      }
    }
  }

Its a much more faster now. I know that for indexing this not help much, but for copying files in the IndexStore this is so big improvement. Its about 400% faster that old implementation.

The patch was tested with 300MB data, ""ant test"" sucessfuly finished with no errors."
0,Fix for deprecations in contrib/surround. Fix for deprecations in contrib/surround.
0,"spi2davex: InvalidItemStateException not properly extracted from ambiguous response error. NodeTest#testSaveInvalidStateException
SessionTest#testSaveInvalidStateException

fail with PathNotFoundException instead of InvalidItemStateException.

i remember that i already addressed that issue in spi2dav a long time ago. with the batched writing in
spi2davex it is back: the server isn't aware of the distinction and just isn't able to retrieve that removed
item... either the client side finds a way to distinguish between path-not-found and externally modified
or we have to leave this as known issue...

in spi2dav i added add quick hack: if the operation was some write operation the path-not-found is
simply converted into invaliditemstateexception."
0,Provide additional test coverage for HTTP and HTTPS over proxy. HTTP and HTTPS over proxy test coverage is still insufficient
0,"Trunk fails tests, FSD.open() - related.     [junit] Testcase: testReadAfterClose(org.apache.lucene.index.TestCompoundFile):	FAILED
    [junit] expected readByte() to throw exception
    [junit] junit.framework.AssertionFailedError: expected readByte() to throw exception
    [junit] 	at org.apache.lucene.index.TestCompoundFile.demo_FSIndexInputBug(TestCompoundFile.java:345)
    [junit] 	at org.apache.lucene.index.TestCompoundFile.testReadAfterClose(TestCompoundFile.java:313)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:88)

This one is a non-bug, if you ask me. The test should fail on SimpleFSD, but on my system FSD.open() creates MMapD and that one cannot be closed, so the read succeeds.

    [junit] ------------- Standard Output ---------------
    [junit] Thread[Thread-34,5,main]: exc
    [junit] java.nio.BufferUnderflowException
    [junit] 	at java.nio.Buffer.nextGetIndex(Buffer.java:474)
    [junit] 	at java.nio.DirectByteBuffer.get(DirectByteBuffer.java:229)
    [junit] 	at org.apache.lucene.store.MMapDirectory$MMapIndexInput.readByte(MMapDirectory.java:67)
    [junit] 	at org.apache.lucene.store.ChecksumIndexInput.readByte(ChecksumIndexInput.java:36)
    [junit] 	at org.apache.lucene.store.IndexInput.readInt(IndexInput.java:70)
    [junit] 	at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:238)
    [junit] 	at org.apache.lucene.index.DirectoryIndexReader$1.doBody(DirectoryIndexReader.java:106)
    [junit] 	at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:699)
    [junit] 	at org.apache.lucene.index.DirectoryIndexReader.open(DirectoryIndexReader.java:126)
    [junit] 	at org.apache.lucene.index.IndexReader.open(IndexReader.java:374)
    [junit] 	at org.apache.lucene.index.IndexReader.open(IndexReader.java:260)
    [junit] 	at org.apache.lucene.search.IndexSearcher.<init>(IndexSearcher.java:76)
    [junit] 	at org.apache.lucene.index.TestStressIndexing$SearcherThread.doWork(TestStressIndexing.java:109)
    [junit] 	at org.apache.lucene.index.TestStressIndexing$TimedThread.run(TestStressIndexing.java:52)
    [junit] NOTE: random seed of testcase 'testStressIndexAndSearching' was: -7374705829444180151
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testStressIndexAndSearching(org.apache.lucene.index.TestStressIndexing):	FAILED
    [junit] null
    [junit] junit.framework.AssertionFailedError
    [junit] 	at org.apache.lucene.index.TestStressIndexing.runStressTest(TestStressIndexing.java:155)
    [junit] 	at org.apache.lucene.index.TestStressIndexing.testStressIndexAndSearching(TestStressIndexing.java:178)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:88)

This one suceeds sometimes, sometimes (mostly) fails. Is obviously linked with switch to MMapD, but what is the real cause - I don't know.

    [junit] ------------- Standard Output ---------------
    [junit] NOTE: random seed of testcase 'testSetBufferSize' was: 8481546620770090440
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testSetBufferSize(org.apache.lucene.store.TestBufferedIndexInput):	Caused an ERROR
    [junit] org.apache.lucene.store.MMapDirectory$MMapIndexInput cannot be cast to org.apache.lucene.store.BufferedIndexInput
    [junit] java.lang.ClassCastException: org.apache.lucene.store.MMapDirectory$MMapIndexInput cannot be cast to org.apache.lucene.store.BufferedIndexInput
    [junit] 	at org.apache.lucene.store.TestBufferedIndexInput$MockFSDirectory.tweakBufferSizes(TestBufferedIndexInput.java:226)
    [junit] 	at org.apache.lucene.store.TestBufferedIndexInput.testSetBufferSize(TestBufferedIndexInput.java:181)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:88)

Broken assumptions.
"
0,"msoffice text extractor for office 2007 files. i created a patch that provides a mstextextractor for jackrabbit. this patch will entirely replace all existing ms extractors.
this patch can be applied as soon as poi-3.5 is available. the ms text extractor supports: doc, docx, ppt, pptx,
xls, xlsx. the patch is not fully tested and uses poi code which is not yet available on the maven repo (needs to be 
build locally)"
0,"Add a simple FST impl to Lucene. 
I implemented the algo described at
http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.24.3698 for
incrementally building a finite state transducer (FST) from sorted
inputs.

This is not a fully general FST impl -- it's only able to build up an
FST incrementally from input/output pairs that are pre-sorted.

Currently the inputs are BytesRefs, and the outputs are pluggable --
NoOutputs gets you a simple FSA, PositiveIntOutputs maps to a long,
ByteSequenceOutput maps to a BytesRef.

The implementation has a low memory overhead, so that it can handle a
fairly large set of terms.  For example, it can build the FSA for the
9.8M terms from a 10M document wikipedia index in ~8 seconds (on
beast), using ~256 MB peak RAM, resulting in an FSA that's ~60 MB.

It packs the FST as-it-builds into a compact byte[], and then exposes
the API to read nodes/arcs directly from the byte[].  The FST can be
quickly saved/loaded to/from a Directory since it's just a big byte[].

The format is similar to what Morfologik uses
(http://sourceforge.net/projects/morfologik/).

I think there are a number of possible places we can use this in
Lucene.  For example, I think many apps could hold the entire terms
dict in RAM, either at the multi-reader level or maybe per-segment
(mapping to file offset or to something else custom to the app), which
may possibly be a good speedup for certain MTQs (though, because the
format is packed into a byte[], there is a decode cost when visiting
arcs).

The builder can also prune as it goes, so you get a prefix trie pruned
according to how many terms run through the nodes, which makes it
faster and even less memory consuming.  This may be useful as a
replacement for our current binary search terms index since it can
achieve higher term density for the same RAM consumption of our
current index.

As an initial usage to make sure this is exercised, I cutover the
SimpleText codec, which currently fully loads all terms into a
TreeMap (and has caused intermittent OOME in some tests), to use an FST
instead.  SimpleText uses a PairOutputs which is able to ""pair up"" any
two other outputs, since it needs to map each input term to an int
docFreq and long filePosition.

All tests pass w/ SimpleText forced codec, and I think this is
committable except I'd love to get some help w/ the generics
(confession to the policeman: I had to add
@SuppressWarnings({""unchecked""})) all over!!  Ideally an FST is
parameterized by its output type (Integer, BytesRef, etc.).

I even added a new @nightly test that makes a largeish set of random
terms and tests the resulting FST on different outputs :)

I think it would also be easy to make a variant that uses char[]
instead of byte[] as its inputs, so we could eg use this during analysis
(Robert's idea).  It's already be easy to have a CharSequence
output type since the outputs are pluggable.

Dawid Weiss (author of HPPC -- http://labs.carrotsearch.com/hppc.html -- and
Morfologik -- http://sourceforge.net/projects/morfologik/)
was very helpful iterating with me on this (thank you!).
"
0,"Update SPI locking to match JCR 2.0. jcr2spi currently uses the JSR 170 way to determine whether a given Session owns the lock by checking of the lock token is null.
with JSR 283 a new Lock method has been defined for this, while on the other hand the lock token is always null for session-scoped
locks.

In addition 283-locking allows to specify a timeout hint and hint about the owner info that should be displayed
for information purpose.

Proposed changes to SPI:

- extend org.apache.jackrabbit.spi.LockInfo to cover the new functionality added with JSR 283
- add an variant of RepositoryService.lock that allows to specify timeout and owner hint.

Proposed changes to JCR2SPI:
- change jcr2spi to make use of the new functionality and modify the test for session being lock holder.
  this mainly affects
  > LockOperation
  > LockManager impl
  > Lock impl"
0,"CharTokenizer has bugs for large documents.. Initially found by hudson from additional testing added in LUCENE-3894, but 
currently not reproducable (see LUCENE-3895).

But its easy to reproduce for a simple single-threaded case in TestDuelingAnalyzers."
0,"reorganize xdocs and website. i would like to propose a change to the website and xdocs structure.

the current structure below seems like it could use a brush up.

---
Overview
Architecture Doc
.Overview
..JSR-170 Levels
.Deployment Models
..Application HOWTO
..Model 1 HOWTO
..Model 2 HOWTO
..Model 3 HOWTO
.Core Operations
..Start-up, Initialize
..QueryManager Implementation
First Steps
JCR API Documentation
Layout
Downloads
FAQ
---

i would propose the following, instead:
---
About 
Documentation
.First Steps
.JCR
.API Documentation
.Jackrabbit Architecture
..Start-up, Initialize
..QueryManager Implementation
.Deployment Models
..Application HOWTO
..Model 1 HOWTO
..Model 2 HOWTO
..Model 3 HOWTO
.Nodetype Modelling *new*
Layout
Downloads
FAQ
---

also i would like to introduce a new ""homepage"" with a little bit more attractive content like jackrabbit
news, maybe featured jackrabbit applications, schedules and events.

ideally i would like to re-organize the file structure according to the navigation, which may break
bookmarks and search indexes.

thoughts?"
0,"typo in RFC reference in web site. Quoting from <http://hc.apache.org/httpcomponents-client/index.html>:

""Standards Compliance

HttpClient strives to conform to the following specifications endorsed by the Internet Engineering Task Force (IETF) and the internet at large:

    * RFC 1945 - Hypertext Transfer Protocol -- HTTP/1.0
    * RFC 2116 - Hypertext Transfer Protocol -- HTTP/1.1
    * RFC2617 HTTP Authentication: Basic and Digest Access Authentication
    * RFC2109 HTTP State Management Mechanism (Cookies)
    * RFC2965 HTTP State Management Mechanism (Cookies v2)""

Note the typo in the reference to HTTP/1.1.
"
0,AbstractWebdavServlet: add protected method sendUnauthorized. 
0,Consolidate Solr  & Lucene FunctionQuery into modules. Spin-off from the [dev list | http://www.mail-archive.com/dev@lucene.apache.org/msg13261.html]  
0,"Support for new Resources model in ant 1.7 in Lucene ant task.. Ant Task for Lucene should use modern Resource model (not only FileSet child element).
There is a patch with required changes.

Supported by old (ant 1.6) and new (ant 1.7) resources model:
<index ....> <!-- Lucene Ant Task -->
  <fileset ... />
</index> 

Supported only by new (ant 1.7) resources model:
<index ....> <!-- Lucene Ant Task -->
  <filelist ... />
</index> 

<index ....> <!-- Lucene Ant Task -->
  <userdefinied-filesource ... />
</index> "
0,Precedence query parser using the contrib/queryparser framework. Extend the current StandardQueryParser on contrib so it supports boolean precedence
0,"Let users set Similarity for MoreLikeThis. Let users set Similarity used for MoreLikeThis

For discussion, see:
http://www.nabble.com/MoreLikeThis-API-changes--tf3838535.html"
0,"[PATCH] Enable application-level management of IndexWriter.ramDirectory size. IndexWriter currently only supports bounding of in the in-memory index cache using maxBufferedDocs, which limits it to a fixed number of documents.  When document sizes vary substantially, especially when documents cannot be truncated, this leads either to inefficiencies from a too-small value or OutOfMemoryErrors from a too large value.

This simple patch exposes IndexWriter.flushRamSegments(), and provides access to size information about IndexWriter.ramDirectory so that an application can manage this based on total number of bytes consumed by the in-memory cache, thereby allow a larger number of smaller documents or a smaller number of larger documents.  This can lead to much better performance while elimianting the possibility of OutOfMemoryErrors.

The actual job of managing to a size constraint, or any other constraint, is left up the applicatation.

The addition of synchronized to flushRamSegments() is only for safety of an external call.  It has no significant effect on internal calls since they all come from a sychronized caller.
"
0,"User interaction for authentication. Some actions require user input.  Like forwarding to another host or retrieveing
authentication credentials.  Should have some way for clients to setup listeners
for such events so that they can be handled on the fly.  No gui, programatic
only."
0,"Introduce daily integration test suite. Some time ago we discussed integration tests that would be run on a daily basis. See also comments in issue JCR-1452. It seems we reached consensus that running a daily integration test suite is desirable.

Here's my proposal:

- Introduce a test suite org.apache.jackrabbit.core.integration.daily.DailyIntegrationTest which includes all tests that should be run on a daily basis.
- Configure our continuous integration system to run the test suite on a daily basis. e.g. mvn -Dtest=DailyIntegrationTest package

With this approach we don't need to introduce maven profiles or any other pom magic, yet it's easy for a developer to run the daily tests when needed."
0,"fail build if contrib tests fail to compile. spinoff of LUCENE-885, from Steven's comments...

Looking at the current build (r545324) it looks like the some contrib failures are getting swallowed. Things like lucli are throwing errors along the lines of

 [subant] /home/barronpark/smparkes/work/lucene/trunk/common-build.xml:366: srcdir ""/home/barronpark/smparkes/work/lucene/trunk/contrib/lucli/src/test"" does not exist!

but these don't make it back up to the top level status.

It looks like the current state will bubble up junit failures, but maybe not build failures?

...

It's ""test-compile-contrib"" (if you will) that fails and rather being contrib-crawled, that's only done as the target of ""test"" in each contrib directory, at which point, it's running in the protected contrib-crawl.

Easy enough to lift this loop into another target, e.g., build-contrib-test. And that will start surfacing errors, which I can work through.
"
0,"OOM in TestBeiderMorseFilter.testRandom. This has been OOM'ing a lot... we should see why, its likely a real bug.

ant test -Dtestcase=TestBeiderMorseFilter -Dtestmethod=testRandom -Dtests.seed=2e18f456e714be89:310bba5e8404100d:-3bd11277c22f4591 -Dtests.multiplier=3 -Dargs=""-Dfile.encoding=ISO8859-1"""
0,"TestBackwardsCompatibility needs terms with U+E000 to U+FFFF. we changed sort order in 4.0, and have sophisticated backwards compatibility (e.g. surrogates dance),
but we don't test this at all in TestBackwardsCompatibility.

for example, nothing handles this case for term vectors..."
0,cipher.  
0,"Remove deprecated Scorer.explain(int) method. This is the only remaining deprecation in core, but is not so easy to handle, because lot's of code in core still uses the explain() method in Scorer. So e.g. in PhraseQuery, the explain method has to be moved from Scorer to the Weight."
0,"Performance fix, when deserializing large jcr:binary in ValueHelper.deserialize(). While profiling import of large PDF files into Magnolia 3.6.3 (which uses Jackrabbit 1.4 as JCR repository) we had found that there is large CPU time spent on:

""http-8080-4"" daemon prio=6 tid=0x5569fc00 nid=0x6ec runnable [0x5712d000..0x5712fb14]
   java.lang.Thread.State: RUNNABLE
	at java.io.FileOutputStream.writeBytes(Native Method)
	at java.io.FileOutputStream.write(FileOutputStream.java:260)
	at org.apache.jackrabbit.util.Base64.decode(Base64.java:269)
	at org.apache.jackrabbit.util.Base64.decode(Base64.java:184)
	at org.apache.jackrabbit.value.ValueHelper.deserialize(ValueHelper.java:759)
	at org.apache.jackrabbit.core.xml.BufferedStringValue.getValue(BufferedStringValue.java:258)
	at org.apache.jackrabbit.core.xml.PropInfo.apply(PropInfo.java:132)

Looking into source code of Base64.decode it became obvious, that it writes each 1to3byte chunk into unbuffered FileOutputStream (thus calling OS kernel many times to write just few bytes) which causes lot of CPU usage without disk usage.


Provided fix is quite trivial - just wrap FileOutputStream into BufferedOutputStream.
"
0,"Rewrite TrieRange to use MultiTermQuery. Issue for discussion here: http://www.lucidimagination.com/search/document/46a548a79ae9c809/move_trierange_to_core_module_and_integration_issues

This patch is a rewrite of TrieRange using MultiTermQuery like all other core queries. This should make TrieRange identical in functionality to core range queries."
0,"Add oal.util.Version ctor to QueryParser. This is a followup of LUCENE-1987:

If somebody uses StandardAnalyzer with Version.LUCENE_CURRENT and then uses QueryParser, phrase queries will not work, because the StopFilter enables position Increments for stop words, but QueryParser ignores them per default. The user has to explicitely enable them.

This issue would add a ctor taking the Version constant and automatically enable this setting. The same applies to the contrib queryparser. Eventually also StopAnalyzer should add this version ctor.

To be able to remove the default ctor for 3.0 (to remove a possible trap for users of QueryParser), it must be deprecated and the new one also added to 2.9.1."
0,"Confusing code line. Line 81 of TermScorer:

      if (!(target > docs[pointer])) {

Could be replaced with the more readable:

      if (docs[pointer] >= target) {

Sorry for nit picking!"
0,"DateTools.java general improvements. Applying the attached patch shows the improvements to DateTools.java that I think should be done. All logic that does anything at all is moved to instance methods of the inner class Resolution. I argue this is more object-oriented.

1. In cases where Resolution is an argument to the method, I can simply invoke the appropriate call on the Resolution object. Formerly there was a big branch if/else.
2. Instead of ""synchronized"" being used seemingly everywhere, synchronized is used to sync on the object that is not threadsafe, be it a DateFormat or Calendar instance.
3. Since different DateFormat and Calendar instances are created per-Resolution, there is now less lock contention since threads using different resolutions will not use the same locks.
4. The old implementation of timeToString rounded the time before formatting it. That's unnecessary since the format only includes the resolution desired.
5. round() now uses a switch statement that benefits from fall-through (no break).

Another debatable improvement that could be made is putting the resolution instances into an array indexed by format length. This would mean I could remove the switch in lookupResolutionByLength() and avoid the length constants there. Maybe that would be a bit too over-engineered when the switch is fine."
0,"UserManagerImpl: typo in ""compatibleJR16"" config option constant. "
0,"Add next() and skipTo() variants to DocIdSetIterator that return the current doc, instead of boolean. See http://www.nabble.com/Another-possible-optimization---now-in-DocIdSetIterator-p23223319.html for the full discussion. The basic idea is to add variants to those two methods that return the current doc they are at, to save successive calls to doc(). If there are no more docs, return -1. A summary of what was discussed so far:
# Deprecate those two methods.
# Add nextDoc() and skipToDoc(int) that return doc, with default impl in DISI (calls next() and skipTo() respectively, and will be changed to abstract in 3.0).
#* I actually would like to propose an alternative to the names: advance() and advance(int) - the first advances by one, the second advances to target.
# Wherever these are used, do something like '(doc = advance()) >= 0' instead of comparing to -1 for improved performance.

I will post a patch shortly"
0,"[PATCH] fix compile errors in sandbox. Here's a patch that fixes the compile problems in sandbox/analyzers starting 
shortly before the 1.4 release. The deprecation warnings are also fixed. I 
have not tested the changes (I don't use those analyzers) but the changes 
should be trivial enough so they don't break anything. 
 
Could someone apply the patch and also fix FrenchAnalyzer? It's the same 
change as for the other files, but I didn't manage to make a clean diff 
because of encoding problems."
0,"TCK: observation tests are too restrictive. The basic sequence in all observation tests is:

1) add listener
2) modify workspace
3) remove listener
4) wait for events on listener

This sequence forces an implementation to maintain a logical order for listener registrations and content changes. In the light of the asynchronous nature of observation events this seems too restrictive for certain implementations.

The sequence should be changed to:

1) add listener
2) modify workspace
3) wait for events on listener
4) remove listener

Which is also more intuitive from a user perspective."
0,"Make JCAManagedConnectionFactory non final, so it can be extended. Hello,

Is there a reason why JCAManagedConnectionFactory is final?
I need to build my own one and I'd rather reuse some code of yours.
"
0,Optimize FixedStraightBytes for bytes size == 1. Currently we read all the bytes in a PagedBytes instance wich is unneeded for single byte values like norms. For fast access this should simply be a straight array.
0,OCM test are too verbose. The OCM test cases print quite a bit of stuff on standard output which makes the standard build output harder to read. It would be better if the tests either used explicit assertions to verify correct behaviour or at least redirected free-form output to a log file in the target directory.
0,"Allow to override LockManager creation. Currently, Repository.getLockManager() internally creates a new lock manager if needed.

Jackrabbit should provide an extension point so that a JCR repository that extends it can create a custom lock manager."
0,Expose directory on IndexReader. It would be really useful to expose the index directory on the IndexReader class.
0,"TextFilter implementations in a new project under contrib.  AFAIK the TextFilters sent by Jn Hala?a were not added yet. If that's the case, I think we can create a project under contrib with all the implementations in order to avoid adding new dependencies to the core distribution."
0,SimpleTextCodec needs SimpleText DocValues impl. currently SimpleTextCodec uses binary docValues we should move that to a simple text impl.
0,"[patch] better support gcj compilation. In order to workaround http://gcc.gnu.org/bugzilla/show_bug.cgi?id=15411 the
attached patch is necessary."
0,Add DataInput/DataOutput subclasses that delegate to an InputStream/OutputStream.. Such classes would be handy for FST serialization/deserialization.
0,"Typos in method names in test classes (SetPropertyAssumeTypeTest). Misspelled: ""ConstraintVioloationException"".
"
0,"Use bulk-byte-copy when merging term vectors. Indexing all of Wikipedia, with term vectors on, under the YourKit
profiler, shows that 26% of the time (!!) was spent merging the
vectors.  This was without offsets & positions, which would make
matters even worse.

Depressingly, merging, even with ConcurrentMergeScheduler, cannot in
fact keep up with the flushing of new segments in this test, and this
is on a strong IO system (Mac Pro with 4 drive RAID 0 array, 4 CPU
cores).

So, just like Robert's idea to merge stored fields with bulk copying
whenever the field name->number mapping is ""congruent"" (LUCENE-1043),
we can do the same with term vectors.

It's a little trickier because the term vectors format doesn't quite
make it easy to bulk-copy because it doesn't directly encode the
offset into the tvf file.

I worked out a patch that changes the tvx format slightly, by storing
the absolute position in the tvf file for the start of each document
into the tvx file, just like it does for tvd now.  This adds an extra
8 bytes (long) in the tvx file, per document.

Then, I removed a vLong (the first ""position"" stored inside the tvd
file), which makes tvd contents fully position independent (so you can
just copy the bytes).

This adds up to 7 bytes per document (less for larger indices) that
have term vectors enabled, but I think this small increase in index
size is acceptable for the gains in indexing performance?

With this change, the time spent merging term vectors dropped from 26%
to 3%.  Of course, this only applies if your documents are ""regular"".
I think in the future we could have Lucene try hard to assign the same
field number for a given field name, if it had been seen before in the
index...

Merging terms now dominates the merge cost (~20% over overall time
building the Wikipedia index).

I also beefed up TestBackwardsCompatibility unit test: test a non-CFS
and a CFS of versions 1.9, 2.0, 2.1, 2.2 index formats, and added some
term vector fields to these indices.
"
0,"Analysis package calls Java 1.5 API. I found compile errors when I tried to compile trunk with 1.4 JVM.
org.apache.lucene.analysis.NormalizeCharMap
org.apache.lucene.analysis.MappingCharFilter

uses Character.valueOf() which has been added in 1.5.
I added a CharacterCache (+ testcase) with a valueOf method as a replacement for that quite useful method.

org.apache.lucene.analysis.BaseTokenTestCase

uses StringBuilder instead of the synchronized version StringBuffer (available in 1.4)

I will attach a patch shortly."
0,"Exception root cause is swallowed in various places. When re-throwing an exception, the root cause is swallowed in some places in Jackrabbit, mainly when converting to an IOException."
0,Make WeightedSpanTermExtractor extensible to handle custom query implemenations. Currently if I have a custom query which subclasses query directly I can't use the QueryScorer for highlighting since it does explicit instanceof checks. In some cases its is possible to rewrite the query before passing it to the highlighter to obtain a primitive query. However I had the usecase where this was not possible ie. the original index was not available on the machine which highlights the results. To still use the highlighter I had to copy a bunch of code due to visibility issues in those classes. I think we can make this extensible with minor effort to allow this usecase without massive code duplication.
0,"toplevel exception cleanup. HttpClient.execute should throw only one exception, for easier general use.
HttpMethod constructors (HttpGet, HttpPut, etc..) should throw IllegalArgumentException in the string constructor (imply the string is pre-checked).  People wanting to see a URIException can use 'new HttpGet(new URI(uri))' and trigger the exception from the explicit URI creation."
0,"Added New Token API impl for ASCIIFoldingFilter. I added an implementation of incrementToken to ASCIIFoldingFilter.java and extended the existing  testcase for it.
I will attach the patch shortly.
Beside this improvement I would like to start up a small discussion about this filter. ASCIIFoldingFitler is meant to be a replacement for ISOLatin1AccentFilter which is quite nice as it covers a superset of the latter. I have used this filter quite often but never on a as it is basis. In the most cases this filter does the correct thing (replace a special char with its ascii correspondent) but in some cases like for German umlaut it does not return the expected result. A german umlaut  like '' does not translate to a but rather to 'ae'. I would like to change this but I'n not 100% sure if that is expected by all users of that filter. Another way of doing it would be to make it configurable with a flag. This would not affect performance as we only check if such a umlaut char is found. 
Further it would be really helpful if that filter could ""inject"" the original/unmodified token with the same position increment into the token stream on demand. I think its a valid use-case to index the modified and unmodified token. For instance, the german word ""sd"" would be folded to ""sud"". In a query q:(sd) the filter would also fold to sud and therefore find sud which has a totally different meaning. Folding works quite well but for special cases would could add those options to make users life easier. The latter could be done in a subclass while the umlaut problem should be fixed in the base class.

simon "
0,"SetValueBinaryTest: some repositories have constraints on where binary properties can be set. For repositories that do only support binary properties in the form of jcr:content/jcr:data, the current configurability is not sufficient. To avoid more config params, I'd suggest to check for propertyName1 == ""jcr:data"" && node.hasNode(""jcr:content""), and in that case to automatically navigate down to the ""jcr:content"" child node.

"
0,"Do not fulltext index jcr:uuid property. The UUID value of a referenceable nodes is currently add to the index four times:
- As a system field _:UUID
- As a property value for jcr:uuid
- As part of the node scope fulltext index
- As part of the jcr:uuid property scope fulltext index

In a repository with lots of referenceable nodes this bloates the index and does not add much value. Searching a node by its UUID is preferably done using the equal operator and not by using the jcr:contains() function.

I suggest to remove fulltext indexing of the jcr:uuid property. Existing indexes would still work after this change, new or updated nodes would simply not have the property jcr:uuid fulltext indexed anymore."
0,"Move PatternAnalyzer out of contrib/memory to contrib/analyzers. in the memory index contrib there is a PatternAnalyzer.
i think this analyzer belongs in contrib/analyzers instead, it has no relation to memory index."
0,"Support all of unicode in StandardTokenizer. StandardTokenizer currently only supports the BMP.

If it encounters characters outside of the BMP, it just discards them... 
it should instead implement fully implement UAX#29 across all of unicode."
0,"Add CopyMoveHanlder so that the copy/move behavior can be customized (as this is the case for the IOHandler and PropertyHandler). The IOHandler impls let you define a specific import/export behavior either for specific nodetypes/locations/etc which is just great. this works well while you create or modify a web dav resource but does not work for the copy/move use case.
the attached patch provides a proposal for an additional CopyMoveHandler."
0,"Add deleteByQuery to IndexWriter. This has been discussed several times recently:

  http://markmail.org/message/awlt4lmk3533epbe
  http://www.gossamer-threads.com/lists/lucene/java-user/57384#57384

If we add deleteByQuery to IndexWriter then this is a big step towards
allowing IndexReader to be readonly.

I took the approach suggested in that first thread: I buffer delete
queries just like we now buffer delete terms, holding the max docID
that the delete should apply to.

Then, I also decoupled flushing deletes (mapping term or query -->
actual docIDs that need deleting) from flushing added documents, and
now I flush deletes only when a merge is started, or on commit() or
close().  SegmentMerger now exports the docID map it used when
merging, and I use that to renumber the max docIDs of all pending
deletes.

Finally, I turned off tracking of memory usage of pending deletes
since they now live beyond each flush.  Deletes are now only
explicitly flushed if you set maxBufferedDeleteTerms to something
other than DISABLE_AUTO_FLUSH.  Otherwise they are flushed at the
start of every merge."
0,"MultiReader.norm() takes up too much memory: norms byte[] should be made into an Object. MultiReader.norms() is very inefficient: it has to construct a byte array that's as long as all the documents in every segment.  This doubles the memory requirement for scoring MultiReaders vs. Segment Readers.  Although this is cached, it's still a baseline of memory that is unnecessary.

The problem is that the Normalization Factors are passed around as a byte[].  If it were instead replaced with an Object, you could perform a whole host of optimizations
a.  When reading, you wouldn't have to construct a ""fakeNorms"" array of all 1.0fs.  You could instead return a singleton object that would just return 1.0f.
b.  MultiReader could use an object that could delegate to NormFactors of the subreaders
c.  You could write an implementation that could use mmap to access the norm factors.  Or if the index isn't long lived, you could use an implementation that reads directly from the disk.

The patch provided here replaces the use of byte[] with a new abstract class called NormFactors.  
NormFactors has two methods on it
    public abstract byte getByte(int doc) throws IOException;  // Returns the byte[doc]
    public float getFactor(int doc) throws IOException;            // Calls Similarity.decodeNorm(getByte(doc))

There are four implementations of this abstract class
1.  NormFactors.EmptyNormFactors - This replaces the fakeNorms with a singleton that only returns 1.0
2.  NormFactors.ByteNormFactors - Converts a byte[] to a NormFactors for backwards compatibility in constructors.
3.  MultiNormFactors - Multiplexes the NormFactors in MultiReader to prevent the need to construct the gigantic norms array.
4.  SegmentReader.Norm - Same class, but now extends NormFactors to provide the same access.

In addition, Many of the Query and Scorer classes were changes to pass around NormFactors instead of byte[], and to call getFactor() instead of using the byte[].  I have kept around IndexReader.norms(String) for backwards compatibiltiy, but marked it as deprecated.  I believe that the use of ByteNormFactors in IndexReader.getNormFactors() will keep backward compatibility with other IndexReader implementations, but I don't know how to test that.
"
0,"contrib/Highlighter javadoc example needs to be updated. The Javadoc package.html example code is outdated, as it still uses QueryParser.parse.  

http://lucene.zones.apache.org:8080/hudson/job/Lucene-Nightly/javadoc/contrib-highlighter/index.html"
0,"Deprecate and replace SimpleHttpConnection with the SimpleHttpServer based testing framework. Thanks to Christian Kohlschuetter and Odi we now have a very flexible testing
framework, which enables us to emulate pretty much all the aspects of a HTTP
server functionality including non-compliant behavior and various vendor
specific implementation quirks. 

Many, many thanks go to Christian Kohlschuetter for having contributed the
original code. 

I propose SimpleHttpConnection be deprecated and eventually be phased out. I
took the first steps toward this goal by migrating Basic authentication test
cases. I urge all committers and contributors to use SimpleHttpServer for all
the new cases from now on. Ideally in the future we should even be able to get
rid of Tomcat as a dependency for testing.

I also took liberty of tweaking the SimpleHttpServer API a little. I factored
SimpleRequest and SimpleResponse classes out and provided a new interface called
HttpService, which can be used instead of HttpRequestHandler to implement test
cases in a way very similar to writing servlets. 

I'll commit the patch shortly as it does not really touch any _productive_ code. 

Oleg"
0,"consolidate FieldCache and ExtendedFieldCache instances. It's confusing and error prone having two instances of FieldCache... FieldCache .DEFAULT and ExtendedFieldCache .EXT_DEFAULT.
Accidentally use the wrong one and you silently double the memory usage for that field.  Since ExtendedFieldCache extends FieldCache, there's no reason not to share the same instance across both."
0,"preflex codec doesn't order terms correctly. The surrogate dance in the preflex codec (which must dynamically remap terms from UTF16 order to unicode code point order) is buggy.

To better test it, I want to add a test-only codec, preflexrw, that is able to write indices in the pre-flex format.  Then we should also fix tests to randomly pick codecs (including preflexrw) so we better test all of our codecs."
0,"javadocs very very ugly if you generate with java7. Java7 changes its javadocs to look much nicer, but this involves different CSS styles.

Lucene overrides the CSS with stylesheet+prettify.css which is a combination of java5/6 stylesheet + google prettify:
but there are problems because java7 has totally different styles.

So if you generate javadocs with java7, its like you have no stylesheet at all.

A solution might be to make stylesheet7+prettify.css and conditionalize a property in ant based on java version."
0,"SPI: Javadoc Issue with QNodeTypeDefinition#getPropertyDefs and #getChildNodeDefs. Javadoc of the mentioned methods currently states:

@return an array containing the property definitions or
     *         <code>null</code> if not set.

while the default implementation returns an empty array, which i find much nicer.

if nobody objects, i would fix the javadoc accordingly."
0,"Avoid creation of more than one jackrabbit instance with the same configuration. based on the mailing list archive, it seems new users often run more than one jackrabbit instance with the same configuration. I propose to lock the repository by creating an empty file called "".lock"" at the repository home on startup and remove it on shutdown.
If the lock file is found on jackrabbit startup the following message will be logged:
""The repository home at "" + home.getAbsolutePath() + "" appears to be in use. If you are sure it's not in use please delete the file at  "" + lock.getAbsolutePath() + "". Probably the repository was not shutdown properly.""
"
0,"Set-Cookie2 and Set-Cookie. Acording to RFC2965 9.1:
                                                 User agents that
   receive in the same response both a Set-Cookie and Set-Cookie2
   response header for the same cookie MUST discard the Set-Cookie
   information and use only the Set-Cookie2 information.

this is read that the header for a cetain cookie, but not all cookie.
So, Server can send only Set-Cookie header for some cookies and,
for cookies send Set-Cookie2,Set-cookie both.

But httpclient implementation handles this that if find any set-cookie2 header,
then ignores all Set-cookie header.
I know some sites use set-cookie2 only for cookies which needs
 more flexible exiration handling, and for other cookies use only
Set-Cookie. One of exmaples of such sites I know is 
TDNet Database service provided by Tokyo Stock Exchange.

So, the preferred implementation is that if set-cookie2 header
 found for a certain cookie then cookie value is set from set-cookie2 header, if
not, then from Set-Cookie header."
0,"Some improvements to _TestUtil and its usage. I've started this issue because I've noticed that _TestUtil.getRandomMultiplier() is called from many loops' condition check, sometimes hundreds and thousands of times. Each time it does Integer.parseInt after calling System.getProperty. This really can become a constant IMO, either in LuceneTestCase(J4) or _TestUtil, as it's not expected to change while tests are running ...

I then reviewed the class and spotted some more things that I think can be fixed/improved:
# getTestCodec() can become a constant as well
# arrayToString is marked deprecated. I've checked an no one calls them, so I'll delete them. This is a 4.0 code branch + a test-only class. No need to deprecate anything.
# getTempDir calls new Random(), instead of newRandom() in LuceneTestCaseJ4, which means that if something fails, we won't know the random seed used ...
#* In that regard, we might want to output all the classes that obtained a static seed in reportAdditionalFailures(), instead of just the class that ran the test.
# rmDir(String) can be removed IMO, and leave only rmDir(File)
# I suggest we include some recursion in rmDir(File) to handle the deletion of nested directories.
#* Also, it does not check whether the dir deletion itself succeeds (but it does so for the files). This can bite us on Windows, if some test did not close things properly.

I'll work out a patch."
0,"move JDK collation to core, ICU collation to ICU contrib. As mentioned on the list, I propose we move the JDK-based CollationKeyFilter/CollationKeyAnalyzer, currently located in contrib/collation into core for collation support (language-sensitive sorting)

These are not much code (the heavy duty stuff is already in core, IndexableBinaryString). 

And I would also like to move the ICUCollationKeyFilter/ICUCollationKeyAnalyzer (along with the jar file they depend on) also currently located in contrib/collation into a contrib/icu.

This way, we can start looking at integrating other functionality from ICU into a fully-fleshed out icu contrib.
"
0,Improve DocValues merging. Some DocValues impl. still load all values from merged segments into memory during merge. For efficiency we should merge them on the fly without buffering in memory
0,"Optimize the core tokenizers/analyzers & deprecate Token.termText. There is some ""low hanging fruit"" for optimizing the core tokenizers
and analyzers:

  - Re-use a single Token instance during indexing instead of creating
    a new one for every term.  To do this, I added a new method ""Token
    next(Token result)"" (Doron's suggestion) which means TokenStream
    may use the ""Token result"" as the returned Token, but is not
    required to (ie, can still return an entirely different Token if
    that is more convenient).  I added default implementations for
    both next() methods in TokenStream.java so that a TokenStream can
    choose to implement only one of the next() methods.

  - Use ""char[] termBuffer"" in Token instead of the ""String
    termText"".

    Token now maintains a char[] termBuffer for holding the term's
    text.  Tokenizers & filters should retrieve this buffer and
    directly alter it to put the term text in or change the term
    text.

    I only deprecated the termText() method.  I still allow the ctors
    that pass in String termText, as well as setTermText(String), but
    added a NOTE about performance cost of using these methods.  I
    think it's OK to keep these as convenience methods?

    After the next release, when we can remove the deprecated API, we
    should clean up Token.java to no longer maintain ""either String or
    char[]"" (and the initTermBuffer() private method) and always use
    the char[] termBuffer instead.

  - Re-use TokenStream instances across Fields & Documents instead of
    creating a new one for each doc.  To do this I added an optional
    ""reusableTokenStream(...)"" to Analyzer which just defaults to
    calling tokenStream(...), and then I implemented this for the core
    analyzers.

I'm using the patch from LUCENE-967 for benchmarking just
tokenization.

The changes above give 21% speedup (742 seconds -> 585 seconds) for
LowerCaseTokenizer -> StopFilter -> PorterStemFilter chain, tokenizing
all of Wikipedia, on JDK 1.6 -server -Xmx1024M, Debian Linux, RAID 5
IO system (best of 2 runs).

If I pre-break Wikipedia docs into 100 token docs then it's 37% faster
(1236 sec -> 774 sec), I think because of re-using TokenStreams across
docs.

I'm just running with this alg and recording the elapsed time:

  analyzer=org.apache.lucene.analysis.LowercaseStopPorterAnalyzer
  doc.tokenize.log.step=50000
  docs.file=/lucene/wikifull.txt
  doc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker
  doc.tokenized=true
  doc.maker.forever=false

  {ReadTokens > : *

See this thread for discussion leading up to this:

  http://www.gossamer-threads.com/lists/lucene/java-dev/51283

I also fixed Token.toString() to work correctly when termBuffer is
used (and added unit test).
"
0,"TopTermsScoringBooleanQueryRewrite minscore. when using the TopTermsScoringBooleanQueryRewrite (LUCENE-2123), it would be nice if MultiTermQuery could set an attribute specifying the minimum required score once the Priority Queue is filled. 

This way, FilteredTermsEnums could adjust their behavior accordingly based on the minimal score needed to actually be a useful term (i.e. not just pass thru the pq)

An example is FuzzyTermsEnum: at some point the bottom of the priority queue contains words with edit distance of 1 and enumerating any further terms is simply a waste of time.
This is because terms are compared by score, then termtext. So in this case FuzzyTermsEnum could simply seek to the exact match, then end.

This behavior could be also generalized for all n, for a different impl of fuzzyquery where it is only looking in the term dictionary for words within edit distance of n' which is the lowest scoring term in the pq (they adjust their behavior during enumeration of the terms depending upon this attribute).

Other FilteredTermsEnums could make use of this minimal score in their own way, to drive the most efficient behavior so that they do not waste time enumerating useless terms.
"
0,"ComparatorKey in Locale based sorting. This is a reply/follow-up on Chris Hostetter's message on Lucene developers list (aug 2006):
http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200608.mbox/%3cPine.LNX.4.58.0608211050330.5081@hal.rescomp.berkeley.edu%3e

> perhaps it would be worthwhile for comparatorStringLocale to convert the String[] it gets back from FieldCache.DEFAULT.getStrings to a new CollationKey[]? or maybe even for FieldCache.DEFAULT.getStrings to be deprecated, and replaced with a FieldCache.DEFAULT.getCollationKeys(reader,field,Collator)?

I think the best is to keep the default behavior as it is today. There is a cost of building caches for sort fields which I think not everyone wants. However for some international production environments there are indeed possible performance gains in comparing precalculated keys instead of comparing strings with rulebased collators.

Since Lucene's Sort architecture is pluggable it is easy to create a custom locale-based comparator, which utilizes the built-in caching/warming mechanism of FieldCache, and may be used in SortField constructor.

I'm not sure whether there should be classes for this in Lucene core or not, but it could be nice to have the option of performance vs. memory consumption in localized sorting without having to use additional jars.

"
0,"Remove PropDefId and NodeDefId. the PropDefIds and NodeDefIds are used to quickly lookup a childnode- or property definition in the nodetype registry (or effective nodetype).
this is heavily used during reading, when calling Property.getDefinition() usually when checking the isMultiple() flag. and of course while writing when getting the definition for the property or childnode. 

however, this poses problems when a nodetype is changed that is still used in the content. if a property definition is changed due to an altered nodetype, subsequent accesses to that property result in a ""invalid propdefid"" warning in the log - but the id is recomputed. this is especially a problem when upgrade jackrabbit from 1.x to 2.0, where some of the builtin nodetypes are defined differently.

i think that it should be feasible to remove the propdefids and nodedefids and compute the definition on demand. i think this can be implemented without performance loss, when some sort of 'signatures' of the items are computed to quickly find the definitions in the effective node type. furthermore, the most common usecase for using the property definition is probably the isMultiple() check - which is now on the Property interface itself - which does not need a definition lookup at all.

and last but not least, it saves 8 bytes per item in the persistence layer."
0,"JMX Stats for the Session. I've named them Core stats. This will include:
 - number of sessions currently opened
 - session read / write operations per second

The stats refresh once a minute.
This is disabled by default, so it will not affect performance."
0,"Some improvements to CMS. While running optimize on a large index, I've noticed several things that got me to read CMS code more carefully, and find these issues:

* CMS may hold onto a merge if maxMergeCount is hit. That results in the MergeThreads taking merges from the IndexWriter until they are exhausted, and only then that blocked merge will run. I think it's unnecessary that that merge will be blocked.

* CMS sorts merges by segments size, doc-based and not bytes-based. Since the default MP is LogByteSizeMP, and I hardly believe people care about doc-based size segments anymore, I think we should switch the default impl. There are two ways to make it extensible, if we want:
** Have an overridable member/method in CMS that you can extend and override - easy.
** Have OneMerge be comparable and let the MP determine the order (e.g. by bytes, docs, calibrate deletes etc.). Better, but will need to tap into several places in the code, so more risky and complicated.

On the go, I'd like to add some documentation to CMS - it's not very easy to read and follow.

I'll work on a patch."
0,"Let NameException extend RepositoryException. Since the NameExceptions (IllegalNameException, UnknownPrefixException, etc.) are typically thrown when parsing or formatting JCR names at the JCR API level, it would make sense for the NameException class to extend RepositoryException instead of the internal BaseException. This idea is supported by the fact that the majority of cases where NameExceptions are encountered simply rethrow the exceptions wrapped inside RepositoryException instances. Making NameException extend RepositoryException would reduce the amount of try-catch blocks and wrapped exceptions."
0,"CharArraySet.clear(). I needed CharArraySet.clear() for something I was working on in Solr in a tokenstream.

instead I ended up using CharArrayMap<Boolean> because it supported .clear()

it would be better to use a set though, currently it will throw UOE for .clear() because AbstractSet will call iterator.remove() which throws UOE.

In Solr, the very similar CharArrayMap.clear() looks like this:
{code}
  @Override
  public void clear() {
    count = 0;
    Arrays.fill(keys,null);
    Arrays.fill(values,null);
  }
{code}

I think we can do a similar thing as long as we throw UOE for the UnmodifiableCharArraySet

will submit a patch later tonight (unless someone is bored and has nothing better to do)"
0,"Provide support for unconnected sockets. Overview description:
If Proxy settings are incorrect or host does not reply, the
HttpClient.executeMethod() hangs, and HttpMethod.abort() does not stop it. Thus,
you cannot assert that the entire application will stop immediately on demand.

Expected Results:
During a HttpMethod.executeMethod(), HttpMethod.abort() should cancel
immediately the executeMethod().

Actual Results:
If HttpMethod.executeMethod() freezes because of Proxy bad settings or not
responding hostname (in fact impossible to open the socket), the abort() method
does not do anything.

Platform:
I tested it on Windows XP and Linux Debian with HttpClient 3.0 RC2 (but if you
look further I point the problem and the source code of the nightly build is
identical).

See comments for the dialogue about the problem, and 2 Test cases. The solution
is described at the end, but it may implies a change in the API and works only
since Java 1.4."
0,"NumericField should be stored in binary format in index (matching Solr's format). (Spinoff of LUCENE-3001)

Today when writing stored fields we don't record that the field was a NumericField, and so at IndexReader time you get back an ""ordinary"" Field and your number has turned into a string.  See https://issues.apache.org/jira/browse/LUCENE-1701?focusedCommentId=12721972&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-12721972

We have spare bits already in stored fields, so, we should use one to record that the field is numeric, and then encode the numeric field in Solr's more-compact binary format.

A nice side-effect is we fix the long standing issue that you don't get a NumericField back when loading your document."
0,Consolidate ItemDef/QItemDefinition. There is a great deal of duplicate code in ItemDef (jackrabbit-core) and QItemDefinition (jackrabbit-spi) and their implementing classes.
0,"Align the code base with checkstyle. The style use in HttpClient is still quite inconsistant.  checkstyle should be
used to align the code base to a similar style.

The checkstyle report can be found at:
http://jakarta.apache.org/commons/httpclient/checkstyle-report.html

And can be generated with:
maven checkstyle:generate-report"
0,"Sun hotspot compiler bug in 1.6.0_04/05 affects Lucene. This is not a Lucene bug.  It's an as-yet not fully characterized Sun
JRE bug, as best I can tell.  I'm opening this to gather all things we
know, and to work around it in Lucene if possible, and maybe open an
issue with Sun if we can reduce it to a compact test case.

It's hit at least 3 users:

  http://mail-archives.apache.org/mod_mbox/lucene-java-user/200803.mbox/%3c8c4e68610803180438x39737565q9f97b4802ed774a5@mail.gmail.com%3e
  http://mail-archives.apache.org/mod_mbox/lucene-solr-user/200804.mbox/%3c4807654E.7050900@virginia.edu%3e
  http://mail-archives.apache.org/mod_mbox/lucene-java-user/200805.mbox/%3c733777220805060156t7fdb8fectf0bc984fbfe48a22@mail.gmail.com%3e

It's specific to at least JRE 1.6.0_04 and 1.6.0_05, that affects
Lucene.  Whereas 1.6.0_03 works OK and it's unknown whether 1.6.0_06
shows it.

The bug affects bulk merging of stored fields.  When it strikes, the
segment produced by a merge is corrupt because its fdx file (stored
fields index file) is missing one document.  After iterating many
times with the first user that hit this, adding diagnostics &
assertions, its seems that a call to fieldsWriter.addDocument some
either fails to run entirely, or, fails to invoke its call to
indexStream.writeLong.  It's as if when hotspot compiles a method,
there's some sort of race condition in cutting over to the compiled
code whereby a single method call fails to be invoked (speculation).

Unfortunately, this corruption is silent when it occurs and only later
detected when a merge tries to merge the bad segment, or an
IndexReader tries to open it.  Here's a typical merge exception:

{code}
Exception in thread ""Thread-10"" 
org.apache.lucene.index.MergePolicy$MergeException: 
org.apache.lucene.index.CorruptIndexException:
    doc counts differ for segment _3gh: fieldsReader shows 15999 but segmentInfo shows 16000
        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:271)
Caused by: org.apache.lucene.index.CorruptIndexException: doc counts differ for segment _3gh: fieldsReader shows 15999 but segmentInfo shows 16000
        at org.apache.lucene.index.SegmentReader.initialize(SegmentReader.java:313)
        at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:262)
        at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:221)
        at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3099)
        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:2834)
        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:240)
{code}

and here's a typical exception hit when opening a searcher:

{code}
org.apache.lucene.index.CorruptIndexException: doc counts differ for segment _kk: fieldsReader shows 72670 but segmentInfo shows 72671
        at org.apache.lucene.index.SegmentReader.initialize(SegmentReader.java:313)
        at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:262)
        at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:230)
        at org.apache.lucene.index.DirectoryIndexReader$1.doBody(DirectoryIndexReader.java:73)
        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:636)
        at org.apache.lucene.index.DirectoryIndexReader.open(DirectoryIndexReader.java:63)
        at org.apache.lucene.index.IndexReader.open(IndexReader.java:209)
        at org.apache.lucene.index.IndexReader.open(IndexReader.java:173)
        at org.apache.lucene.search.IndexSearcher.<init>(IndexSearcher.java:48)
{code}

Sometimes, adding -Xbatch (forces up front compilation) or -Xint
(disables compilation) to the java command line works around the
issue.

Here are some of the OS's we've seen the failure on:

{code}
SuSE 10.0
Linux phoebe 2.6.13-15-smp #1 SMP Tue Sep 13 14:56:15 UTC 2005 x86_64 
x86_64 x86_64 GNU/Linux 

SuSE 8.2
Linux phobos 2.4.20-64GB-SMP #1 SMP Mon Mar 17 17:56:03 UTC 2003 i686 
unknown unknown GNU/Linux 

Red Hat Enterprise Linux Server release 5.1 (Tikanga)
Linux lab8.betech.virginia.edu 2.6.18-53.1.14.el5 #1 SMP Tue Feb 19 
07:18:21 EST 2008 i686 i686 i386 GNU/Linux
{code}

I've already added assertions to Lucene to detect when this bug
strikes, but since assertions are not usually enabled, I plan to add a
real check to catch when this bug strikes *before* we commit the merge
to the index.  This way we can detect & quarantine the failure and
prevent corruption from entering the index.

"
0,Expose BootstrapConfig in Servlets. the RepostitoryStartup and RepositroyAccess servlets use a bootstrap config object for initialization. in order to generate diagnostics reports it would be very useful to be able to access them.
0,"Change access to internal maps of HttpState to protected.. To be able to serialize the conversational state of a http session access to the internal maps of HttpState is required. Currently they are all ""private"", so subclasses cannot access them. Changing the access to ""protected"" will allow any subclass to access those maps."
0,"TokenSources.getTokenStream(Document...) . Sometimes, one already has the Document, and just needs to generate a TokenStream from it, so I am going to add a convenience method to TokenSources.  Sometimes, you also already have just the string, so I will add a convenience method for that."
0,Contrib CharTokenizer classes should be instantiated using their new Version based ctors. Contrib CharTokenizer classes should be instantiated using their new Version based ctors introduced by LUCENE-2183 and LUCENE-2240
0,"Per socket SOCKS proxies. HttpClient requires a way of allowing a SOCKS proxy to be used on some
connections without requiring that all created Sockets go through the proxy."
0,"Provide support for non-ASCII charsets in the multipart disposition-content header. Because of the the following line in getAsciiBytes 
 data.getBytes(""US-ASCII"");

The returned string is modified if has Latin Characters.

Ex : Document non-control -> Document non-control?"
0,"TCK: PropertyReadMethodsTest#testGetValues fails if it cannot find a single valued STRING property. If the JCR repository being tested does not contain any single valued property of type STRING, PropertyReadMethodsTest#testGetValues fails with the following exception:

java.lang.NullPointerException at org.apache.jackrabbit.test.api.PropertyReadMethodsTest.testGetValues(PropertyReadMethodsTest.java:273) at org.apache.jackrabbit.test.AbstractJCRTest.run(AbstractJCRTest.java:401)

The test should either try to find a single valued property of any type (it is guaranted that it will at least find jcr:primaryType) or should throw NotExecutableException if is does not find the property it needs."
0,"Most 4.0 PostingsReaders don't obey DISI contract. While trying to do some refactoring, I noticed funky things going on with some codecs.

One problem is that DocIdSetIterator says the following:
{noformat}
Returns the following:
   * <ul>
   * <li>-1 or {@link #NO_MORE_DOCS} if {@link #nextDoc()} or
   * {@link #advance(int)} were not called yet.
{noformat}

But most 4.0 Docs/DocsAndPositionsEnums don't actually do this (e.g. return 0). instead we 
are relying on Scorers to 'cover' for them, which is inconsistent. Some scorers actually rely
upon this behavior, for example look at ReqExclScorer.toNonExcluded(), it calls docId() on the
excluded part before it calls nextDoc()

So we need to either fix these enums, change these enums to not extend DocIdSetIterator (and redefine
what the actual contract should be for these enums), change DocIdSetIterator, or something else.

Fixing the enums to return -1 here when they are uninitialized kinda sucks for the ones summing up
document deltas...
"
0,"Deprecate ""create"" method in FSDirectory.getDirectory in favor of IndexWriter's ""create"". It's confusing that there is a create=true|false at the FSDirectory
level and then also another create=true|false at the IndexWriter
level.  Which one should you use when creating an index?

Our users have been confused by this in the past:

  http://www.gossamer-threads.com/lists/lucene/java-user/4792

I think in general we should try to have one obvious way to achieve
something (like Python: http://en.wikipedia.org/wiki/Python_philosophy).

And the fact that there are now two code paths that are supposed to do
the same (similar?) thing, can more easily lead to sneaky bugs.  One
case of LUCENE-140 (already fixed in trunk but not past releases),
which inspired this issue, can happen if you send create=false to the
FSDirectory and create=true to the IndexWriter.

Finally, as of lockless commits, it is now possible to open an
existing index for ""create"" while readers are still using the old
""point in time"" index, on Windows.  (At least one user had tried this
previously and failed).  To do this, we use the IndexFileDeleter class
(which retries on failure) and we also look at the segments file to
determine the next segments_N file to write to.

With future issues like LUCENE-710 even more ""smarts"" may be required
to know what it takes to ""create"" a new index into an existing
directory.  Given that we have have quite a few Directory
implemenations, I think these ""smarts"" logically should live in
IndexWriter (not replicated in each Directory implementation), and we
should leave the Directory as an interface that knows how to make
changes to some backing store but does not itself try to make any
changes.
"
0,"New tool for  reseting the (length)norm of fields after changing Similarity. I've written a little tool that seems like it can/will be very handy as I tweak my custom similarity.  I think it would make a good addition to contrib/miscellaneous.

Class and Tests to be attached shortly..."
0,"The cluster syncDelay attribute is milliseconds. The repository DTDs document the cluster syncDelay attribute as being the sync interval in seconds, when it really is the interval in milliseconds."
0,"Create ""quick start"" developer bundles for model 1,2,3 deployment. Please create ""quick start"" developer bundles for deployment models 1,2,3 with all dependance libs and application startup code. Some kind of ""Hello, world"" app on models 1,2,3.
Tomcat and jetty bundles would be nice."
0,"No way to get the requestBody out of a PostMethod or use if extending class. Attempting to extend the PostMethod class I discovered that I had no access to 
the requestBody because the member is declared private and there is no get 
method.

I was trying to override the setRequestContentLength() when I discovered the 
problem.

So my enhancement request specifically is:
1. add a get method to be able to get the requestBody (probably a get method to 
the parameters as well)
2. (optionally) make the requestBody and parameters members protected instead 
of private so extending the class is easier.

In case you were wondering, the reason for extending the class was to add the 
ability to set a timeout value on the httpconnection and also the ability to 
set the character encoding of the request body.  I don't know if these are 
worthy of an enhancement request but I require them for what I'm doing.  Nice 
work by the way.

Thank you."
0,"Can't use proxy server with https. There doesn't seem to be a way to configure HttpClient to use both HTTPS and a
proxy server at the same time.  It's not clear if this was just an oversight or
if there was a deliberate decision to not support this combination for some reason.

Assuming that it was an oversight, the fix seems to just require one more
variation of startSession() in HttpClient.java which would be the following:

   public void startSession(String host, int port,
                            String proxyhost, int proxyport, boolean https) {
       connection = new HttpConnection(proxyhost,proxyport,host,port,https);
   }"
0,"Unnecessary assert in org.apache.lucene.index.DocumentsWriterThreadState.trimFields(). In org.apache.lucene.index.DocumentsWriterThreadState.trimFields() is the following code:

      if (fp.lastGen == -1) {
        // This field was not seen since the previous
        // flush, so, free up its resources now

        // Unhash
        final int hashPos = fp.fieldInfo.name.hashCode() & fieldDataHashMask;
        DocumentsWriterFieldData last = null;
        DocumentsWriterFieldData fp0 = fieldDataHash[hashPos];
        while(fp0 != fp) {
          last = fp0;
          fp0 = fp0.next;
        }
        assert fp0 != null;

The assert at the end is not necessary as fp0 cannot be null.  The first line in the above code guarantees that fp is not null by the time the while loop is hit.  The while loop is exited when fp0 and fp are equal.  Since fp is not null then fp0 cannot be null when the while loop is exited, thus the assert is guaranteed to never occur.

This was detected by FindBugs."
0,"TestIndexWriterDelete makes broken segments with payloads on. This could just be a SimpleText problem.... but just in case

Grant added payloads to MockAnalyzer in LUCENE-2692

I wondered what would happen if i turned on his payload filter by default for all tests.

{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestIndexWriterDelete
    [junit] Testcase: testDeletesOnDiskFull(org.apache.lucene.index.TestIndexWriterDelete):     Caused an ERROR
    [junit] CheckIndex failed
    [junit] java.lang.RuntimeException: CheckIndex failed
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:80)
    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.doTestOperationsOnDiskFull(TestIndexWriterDelete.java:5
38)
    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.testDeletesOnDiskFull(TestIndexWriterDelete.java:401)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)
    [junit]
    [junit]
    [junit] Testcase: testUpdatesOnDiskFull(org.apache.lucene.index.TestIndexWriterDelete):     Caused an ERROR
    [junit] CheckIndex failed
    [junit] java.lang.RuntimeException: CheckIndex failed
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:80)
    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.doTestOperationsOnDiskFull(TestIndexWriterDelete.java:5
38)
    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.testUpdatesOnDiskFull(TestIndexWriterDelete.java:405)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)
    [junit]
    [junit]
    [junit] Tests run: 14, Failures: 0, Errors: 2, Time elapsed: 0.322 sec
    [junit]
    [junit] ------------- Standard Output ---------------
    [junit] CheckIndex failed
    [junit] Segments file=segments_2 numSegments=1 version=FORMAT_4_0 [Lucene 4.0]
    [junit]   1 of 1: name=_0 docCount=157
    [junit]     codec=MockFixedIntBlock
    [junit]     compound=true
    [junit]     hasProx=true
    [junit]     numFiles=2
    [junit]     size (MB)=0,017
    [junit]     diagnostics = {os.version=6.0, os=Windows Vista, lucene.version=4.0-SNAPSHOT, source=flush, os.arch=x86,
 java.version=1.6.0_21, java.vendor=Sun Microsystems Inc.}
    [junit]     has deletions [delFileName=_0_1.del]
    [junit]     test: open reader.........OK [13 deleted docs]
    [junit]     test: fields..............OK [2 fields]
    [junit]     test: field norms.........OK [2 fields]
    [junit]     test: terms, freq, prox...ERROR [read past EOF]
    [junit] java.io.IOException: read past EOF
    [junit]     at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:154)
    [junit]     at org.apache.lucene.store.BufferedIndexInput.readBytes(BufferedIndexInput.java:119)
    [junit]     at org.apache.lucene.store.BufferedIndexInput.readBytes(BufferedIndexInput.java:94)
    [junit]     at org.apache.lucene.index.codecs.sep.SepPostingsReaderImpl$SepDocsAndPositionsEnum.getPayload(SepPostin
gsReaderImpl.java:689)
    [junit]     at org.apache.lucene.index.CheckIndex.testTermIndex(CheckIndex.java:693)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:489)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:290)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:286)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:76)
    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.doTestOperationsOnDiskFull(TestIndexWriterDelete.java:5
38)
    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.testDeletesOnDiskFull(TestIndexWriterDelete.java:401)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)
    [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit]     at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)
    [junit]     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    [junit]     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    [junit]     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:743)
    [junit]     test: stored fields.......OK [223 total field count; avg 1,549 fields per doc]
    [junit]     test: term vectors........OK [242 total vector count; avg 1,681 term/freq vector fields per doc]
    [junit] FAILED
    [junit]     WARNING: fixIndex() would remove reference to this segment; full exception:
    [junit] java.lang.RuntimeException: Term Index test failed
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:502)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:290)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:286)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:76)
    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.doTestOperationsOnDiskFull(TestIndexWriterDelete.java:5
38)
    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.testDeletesOnDiskFull(TestIndexWriterDelete.java:401)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)
    [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit]     at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)
    [junit]     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    [junit]     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    [junit]     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:743)
    [junit]
    [junit] WARNING: 1 broken segments (containing 144 documents) detected
    [junit]
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterDelete -Dtestmethod=testDeletesOnDiskFull -Dtests.s
eed=-8128829179004133416:-7192468460505114475
    [junit] CheckIndex failed
    [junit] Segments file=segments_1 numSegments=1 version=FORMAT_4_0 [Lucene 4.0]
    [junit]   1 of 1: name=_0 docCount=157
    [junit]     codec=MockFixedIntBlock
    [junit]     compound=false
    [junit]     hasProx=true
    [junit]     numFiles=14
    [junit]     size (MB)=0,017
    [junit]     diagnostics = {os.version=6.0, os=Windows Vista, lucene.version=4.0-SNAPSHOT, source=flush, os.arch=x86,
 java.version=1.6.0_21, java.vendor=Sun Microsystems Inc.}
    [junit]     no deletions
    [junit]     test: open reader.........OK
    [junit]     test: fields..............OK [2 fields]
    [junit]     test: field norms.........OK [2 fields]
    [junit]     test: terms, freq, prox...ERROR [Read past EOF]
    [junit] java.io.IOException: Read past EOF
    [junit]     at org.apache.lucene.store.RAMInputStream.switchCurrentBuffer(RAMInputStream.java:89)
    [junit]     at org.apache.lucene.store.RAMInputStream.readBytes(RAMInputStream.java:73)
    [junit]     at org.apache.lucene.store.MockIndexInputWrapper.readBytes(MockIndexInputWrapper.java:109)
    [junit]     at org.apache.lucene.index.codecs.sep.SepPostingsReaderImpl$SepDocsAndPositionsEnum.getPayload(SepPostin
gsReaderImpl.java:689)
    [junit]     at org.apache.lucene.index.CheckIndex.testTermIndex(CheckIndex.java:693)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:489)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:290)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:286)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:76)
    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.doTestOperationsOnDiskFull(TestIndexWriterDelete.java:5
38)
    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.testUpdatesOnDiskFull(TestIndexWriterDelete.java:405)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)
    [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit]     at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)
    [junit]     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    [junit]     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    [junit]     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:743)
    [junit]     test: stored fields.......OK [237 total field count; avg 1,51 fields per doc]
    [junit]     test: term vectors........OK [254 total vector count; avg 1,618 term/freq vector fields per doc]
    [junit] FAILED
    [junit]     WARNING: fixIndex() would remove reference to this segment; full exception:
    [junit] java.lang.RuntimeException: Term Index test failed
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:502)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:290)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:286)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:76)
    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.doTestOperationsOnDiskFull(TestIndexWriterDelete.java:5
38)
    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.testUpdatesOnDiskFull(TestIndexWriterDelete.java:405)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)
    [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit]     at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)
    [junit]     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    [junit]     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    [junit]     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:743)
    [junit]
    [junit] WARNING: 1 broken segments (containing 157 documents) detected
    [junit]
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterDelete -Dtestmethod=testUpdatesOnDiskFull -Dtests.s
eed=-8128829179004133416:-5617162412680232634
    [junit] NOTE: test params are: codec=MockFixedIntBlock(blockSize=266), locale=tr_TR, timezone=Africa/Maseru
    [junit] ------------- ---------------- ---------------
    [junit] TEST org.apache.lucene.index.TestIndexWriterDelete FAILED
    [junit] Testsuite: org.apache.lucene.index.TestLazyProxSkipping
    [junit] Tests run: 2, Failures: 0, Errors: 0, Time elapsed: 0.034 sec
    [junit]
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestLazyProxSkipping -Dtestmethod=testLazySkipping -Dtests.seed=-7
119541877291237950:-8499388603775233752
    [junit] NOTE: test params are: codec=SimpleText, locale=da_DK, timezone=Pacific/Guam
    [junit] ------------- ---------------- ---------------
{noformat}"
0,"Remove rest of analysis deprecations (Token, CharacterCache). These removes the rest of the deprecations in the analysis package:
- -Token's termText field-- (DONE)
- -eventually un-deprecate ctors of Token taking Strings (they are still useful) -> if yes remove deprec in 2.9.1- (DONE)
- -remove CharacterCache and use Character.valueOf() from Java5- (DONE)
- Stopwords lists
- Remove the backwards settings from analyzers (acronym, posIncr,...). They are deprecated, but we still have the VERSION constants. Do not know, how to proceed. Keep the settings alive for index compatibility? Or remove it together with the version constants (which were undeprecated)."
0,"JCR2SPI: remove duplicate item states. the original approach with duplicate item state objects connected to each is not required any more 
and can be simplified."
0,jcr mapping layer does not expose node move and node copy via PersistenceManager.java. The PersistenceManagerImpl.java  in jcr-apping layer does not implement move and copy methods for a node.  
0,"RAMDirectory not Serializable. The current implementation of RAMDirectory throws a NotSerializableException when trying to serialize, due to the inner class KeySet of HashMap not being serializable (god knows why)

java.io.NotSerializableException: java.util.HashMap$KeySet
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1081)

Caused by line 43:

private Set fileNames = fileMap.keySet();

EDIT:

while we're at it: same goes for inner class Values 

java.io.NotSerializableException: java.util.HashMap$Values
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1081)

Collection files = fileMap.values();
"
0,"Use Float.floatToRawIntBits over Float.floatToIntBits. Copied From my Email:
  Float.floatToRawIntBits (in Java1.4) gives the raw float bits without
normalization (like *(int*)&floatvar would in C).  Since it doesn't do
normalization of NaN values, it's faster (and hopefully optimized to a
simple inline machine instruction by the JVM).

On my Pentium4, using floatToRawIntBits is over 5 times as fast as
floatToIntBits.
That can really add up in something like Similarity.floatToByte() for
encoding norms, especially if used as a way to compress an array of
float during query time as suggested by Doug."
0,"add new snowball languages. Snowball added new languages. This patch adds support for them.

http://snowball.tartarus.org/algorithms/armenian/stemmer.html
http://snowball.tartarus.org/algorithms/catalan/stemmer.html
http://snowball.tartarus.org/algorithms/basque/stemmer.html
"
0,JSR 283 Observation. 
0,"AuthorizableImpl#memberOf and #declaredMemberOf should return RangeIterator. it would be favorable if the iterator returned by Authorizable#memberOf and #declaredMemberOf
would return a RangeIterator in order to all the caller to determine the size without having to
iterate."
0,JSR 283: Event user data. Implement JSR 283 Event user data.
0,"TestFSDirectory fails on Windows. ""ant test"" generates the following error consistently when run on a Windows machine even when run as user with Administrator privileges

    [junit] Testcase: testTmpDirIsPlainFile(org.apache.lucene.index.store.TestFSDirectory):     Caused an ERROR
    [junit] Access is denied
    [junit] java.io.IOException: Access is denied
    [junit]     at java.io.WinNTFileSystem.createFileExclusively(Native Method)
    [junit]     at java.io.File.createNewFile(File.java:828)
    [junit]     at org.apache.lucene.index.store.TestFSDirectory.testTmpDirIsPlainFile(TestFSDirectory.java:66)"
0,"Global data store for binaries. There are three main problems with the way Jackrabbit currently handles large binary values:

1) Persisting a large binary value blocks access to the persistence layer for extended amounts of time (see JCR-314)
2) At least two copies of binary streams are made when saving them through the JCR API: one in the transient space, and one when persisting the value
3) Versioining and copy operations on nodes or subtrees that contain large binary values can quickly end up consuming excessive amounts of storage space.

To solve these issues (and to get other nice benefits), I propose that we implement a global ""data store"" concept in the repository. A data store is an append-only set of binary values that uses short identifiers to identify and access the stored binary values. The data store would trivially fit the requirements of transient space and transaction handling due to the append-only nature. An explicit mark-and-sweep garbage collection process could be added to avoid concerns about storing garbage values.

See the recent NGP value record discussion, especially [1], for more background on this idea.

[1] http://mail-archives.apache.org/mod_mbox/jackrabbit-dev/200705.mbox/%3c510143ac0705120919k37d48dc1jc7474b23c9f02cbd@mail.gmail.com%3e
"
0,Add Rewriteable Support to SortField.toString. I missed adding support for the new Rewriteable SortField type to toString().
0,"Remove dependency to log4j. Currently two classes in the test cases contain unused references to log4j.
The attached patch removes the unused logger and makes the package independent from lo4j"
0,"Modify ParallelMultiSearcher to use a CompletionService instead of slowly polling for results. Right now, the parallel multi searcher creates an array/list of Future<V> representing each of the searchables that's being concurrently searched (and its corresponding search task).

As it stands, once the tasks are all submitted to the executor, the array is iterated over, FIFO, and Future.get() is called iteratively.  This obviously works, but isn't ideal.  It's entirely possible (a situation I've run into) where one of the first searchables represents a large index that takes a long time to search, so the results of the other searchables can't be processed until the large index is done searching.  In my case, we have two indexes with several million records that get searched in front of some other indexes, the smallest of which has only a few ten thousand entries and I didn't think it was ideal for the results of the other indexes to wait.

I've modified ParallelMultiSearcher to use CompletionServices instead, so that results are processed in the order they are completed, rather than the order that they are submitted.  All the tests still pass, and to the best of my knowledge this won't break anything.  This have several advantages:
1) Speed - the thread owning the executor doesn't have to wait for the first submitted task to finish in order to process the results of the other tasks, which may have finished first
2) Removed several warnings (even if they are annotated away) due to the ugliness of typecasting generic arrays.
3) Decreased the complexity of the code in some cases, usually by removing the necessity of allocating and filling arrays.

With a primed ""cache"" of searchables, I was getting 700-1200 ms per search, and using the same phrases, with this patch, I am now getting 400-500ms per search :)

Patch is attached."
0,"BUILD.txt instructions wrong for JavaCC. The text in BUILD.txt for javacc says to set the property to the bin directory in the javacc installation. It should actually be set to the javacc installation directory, the directory containing the bin directory. The comments common-build.xml correctly state this."
0,"Convert some tests to new TokenStream API, better support of cross-impl AttributeImpl.copyTo(). This patch converts some remaining tests to the new TokenStream API and non-deprecated classes.
This patch also enhances AttributeImpl.copyTo() of Token and TokenWrapper to also support copying e.g. TermAttributeImpl into Token. The target impl must only support all interfaces but must not be of the same type. Token and TokenWrapper use optimized coping without casting to 6 interfaces where possible.
Maybe the special tokenizers in contrib (shingle matrix and so on using tokens to cache may be enhanced by that). Also Yonik's request for optimized copying of states between incompatible AttributeSources may be enhanced by that (possibly a new issue)."
0,"BeanLazyLoader is not Serializable. Class org.apache.jackrabbit.ocm.manager.objectconverter.impl.BeanLazyLoader is not serializable.
In ocm module we can mark some property to be lazy loaded. For example @Bean(..., proxy=true)
In such scenario instead of object we will have here proxy BeanLazyLoader which is not serializable.

It is problematic while using another technologies. 
For example Spring WebFlow requires objects (model) stored in scope to be Serializable.
So when we use proxied model with Spring WebFlow we received exception ""org.springframework.webflow.execution.repository.snapshot.SnapshotCreationException: Could not serialize flow execution; make sure all objects stored in flow or flash scope are serializable.... Caused by: java.io.NotSerializableException: org.apache.jackrabbit.ocm.manager.objectconverter.impl.BeanLazyLoader
...""

Please make BeanLazyLoader Serializable.
"
0,"remove unused benchmark dependencies. Benchmark has a huge number of jar files in its lib/ (some of which even have different versions than the same libs used in e.g. solr)

But the worst thing is, most of these it doesn't even use.
* commons-collection: unused
* commons-beanutils: unused
* commons-logging: unused
* commons-digetser: unused

"
0,"Simplified Repository URI format for JNDI lookups. The JndiRepositoryFactory class (together with JcrUtils) currently supports the following repository URI formats:

    JcrUtils.getRepository(""jndi:name-of-repository"");
    JcrUtils.getRepository(""jndi://ignored?org.apache.jackrabbit.repository.jndi.name=name-of-repository&other-parameters"");

The first uri formats allows no extra JNDI environment settings to be passed in, and the second one is pretty verbose and simply ignores the authority and path parts of the URI.

I'd like to add support for the following simplified format that makes it easy to provide the repository name along with the initial context factory from which the name is to be looked up:

    JcrUtils.getRepository(""jndi://initial-context-factory/name-of-repository"");

Extra JNDI environment settings could still be included as additional query parameters. Backwards compatibility with the previous formats would be guaranteed based on the presence or absence of the org.apache.jackrabbit.repository.jndi.name parameter in hierarchical URIs."
0,"Use Maven dependency management. Many of the Jackrabbit components have dependencies to each other and to external libraries,
whose versions should ideally be the same for all the Jackrabbit components. To guarantee
the use of same depedency versions and to simplify overall depedency management we should
start using the Maven depedencyManagement feature in the Jackrabbit parent pom."
0,"use packed ints for the terms dict index. Terms dict index needs to store large RAM resident arrays of ints, but, because their size is bound & variable (depending on the segment/docs), we should used packed ints for them."
0,New method on NodeTypeManagerImpl to reregister nodetypes. Add a method to NodeTypeManagerImpl to allow reregistering of existing nodetypes. The method takes an inputstream in either XML or CND format and registers all new nodetypes and reregisters existing nodetypes.
0,"SPI: Introduce NodeInfo.getChildInfos(). Improvement suggested by Marcel:

ChildInfo is basically a stripped down NodeInfo. With little effort it would even be possible to have NodeInfo extends ChildInfo. Not sure how useful that is, but since we don't have that inheritance in code and at the same time nearly a 100% overlap it makes me suspicious.

Here's another idea:

introduce a method ChildInfo[] NodeInfo.getChildInfos(). The method either returns:

- all child infos, which also gives the correct number of child nodes. this may also mean that an empty array is returned to indicate there are no child nodes.
- null, to indicate that there are *lots* of child nodes and the method RepositoryService.getChildInfos() with the iterator should be used. 


"
0,Remove remaining deprecations from indexer package. 
0,"File Formats Documentation is not correct for Term Vectors. From Samir Abdou on the dev mailing list:

Hi, 

There is an inconsistency between the files format page (from Lucene
website) and the source code. It concerns the positions and offsets of term
vectors. It seems that documentation (website) is not up to date. According
to the file format page, offsets and positions are not stored! Is that
correct?

Many thanks,

Samir
-----
Indeed, in the file formats term vectors section it doesn't talk about the storing of position and offset info.
"
0,"SystemSession#createSession should return SessionImpl again. a long with the fix of  JCR-2890 (revision 1089436) the behavior of SystemSession#createSession has changed to
return a SystemSession instead of SessionImpl as it used to be.

while i basically consider this move to be correct and the better way of dealing with that session-cloning
mechanism as it prevents the user of this method to convert a SystemSession into a regular session
for extra writing operations (such as e.g. access control editing that is not supported with the
system session to prevent chicken-egg-problems on repo startup).

therefore i would like to revert that change for the 2.4 release in order to prevent regressions.

for the time after 2.4 i would however suggest that we finally take the time to clearly define the
usages, abilities and responsibilities of the system session and also review how and where we
expose them to the individual 'modules' of jackrabbit core..  i started working on this but decided
that this is definitely too risky for 2.4 whereas reverting the change mentioned above should
imo impose very limited risk as all usages of those sessions i am aware of use them as ""Session""
or ""SesssionImpl"", most of them not even having access to the SystemSession class."
0,"Add hit miss statistics and logging to caches. The current caches (ConcurrentCache) doesn't maintain hit and miss statistics. This makes it very hard to know if you need to increase the caches in a deployment. This functionality does exist in the 1.5 and 1.6 branches, but is missing from the 2.x branches. The patch adds these statistics and adds logging on info level. The frequency of the logging is by default configured to maximal once a minute but can be configured with the system property ""org.apache.jackrabbit.cacheLogStatsInterval"" (in ms). 

The log lines look like:

07.10.2011 09:00:39 INFO  [org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.logCacheStats():737] name=defaultBundleCache[ConcurrentCache@54fb02fd] num=21074 mem=34504k max=65536k hits=93352 miss=21074 puts=21135
07.10.2011 09:00:40 INFO  [org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.logCacheStats():737] name=versionBundleCache[ConcurrentCache@47b1de1a] num=10637 mem=250k max=8192k hits=36352 miss=10637 puts=10637

This patch will also make possible to later on expose these statistics over JMX when the initial JMX work has been settled.  "
0,"NamespaceRegistryTest.testRegisterNamespace test assumptions. NamespaceRegistryTest.testRegisterNamespace() makes the assumption that it is possible to create arbitrarily nodes inside the root folder.

This is not required to be the case.

Proposal: 

- get the name of the test node from the config, and

- use a property rather than a child node for the test (as far as I can tell, many repositories will not allow node names in namespaces other than the empty one).
"
0,"configurable MultiTermQuery TopTermsScoringBooleanRewrite pq size. MultiTermQuery has a TopTermsScoringBooleanRewrite, that uses a priority queue to expand the query to the top-N terms.

currently N is hardcoded at BooleanQuery.getMaxClauseCount(), but it would be nice to be able to set this for top-N MultiTermQueries: e.g. expand a fuzzy query to at most only the 50 closest terms.

at a glance it seems one way would be to expose TopTermsScoringBooleanRewrite (it is private right now) and add a ctor to it, so a MultiTermQuery can instantiate one with its own limit."
0,"[GSoC] Implementing State of the Art Ranking for Lucene. Lucene employs the Vector Space Model (VSM) to rank documents, which compares
unfavorably to state of the art algorithms, such as BM25. Moreover, the architecture is
tailored specically to VSM, which makes the addition of new ranking functions a non-
trivial task.

This project aims to bring state of the art ranking methods to Lucene and to implement a
query architecture with pluggable ranking functions.

The wiki page for the project can be found at http://wiki.apache.org/lucene-java/SummerOfCode2011ProjectRanking."
0,"Deprecate / Remove DutchAnalyzer.setStemDictionary. DutchAnalyzer.setStemDictionary(File) prevents reuse of TokenStreams (and also uses a File which isn't ideal).  It should be deprecated in 3x, removed in trunk."
0,"Avoid unnecessary index reader calls when using aggregate definitions. SearchIndex.retrieveAggregateRoot(Set<NodeId> removedIds, Map<NodeId, NodeState> map) identifies aggregate root nodes based on removed nodes and aggregate rules defined in the indexing configuration. This process requires index lookups. The method can be optimized for the case when no nodes are removed and an unnecessary call to the index reader can be avoided."
0,"Improved log message: include path. The cluster logs a message for each appended operation. The log message is currently the revision number. A more interesting log message would be the user name, and the path of the change (the most specific path if the change contains multiple nodes)."
0,"Pass ClientConnectionManager to DefaultHttpClient constructor. Copied from my mailing list post, Oleg suggested I post it to JIRA for 4.1 fix.

I'm trying to find the least verbose way of configuring a DefaultHttpClient with a ThreadSafeClientConnManager.

The example code given for this goes through a manual process of configuring HttpParams and SchemeRegistry objects, which is more or less copied from the DefaultHttpClient.createHttpParams() and createClientConnectionManager() methods.

It's a bit of a chicken and egg situation - DefaultHttpClient can create its own HttpParams and SchemeRegistry, which are themselves fine, but only once its been constructed, and the constructor requires the ThreadSafeClientConnManager, but that in turn requires the HttpParams and SchemeRegistry objects.  The only way out is to manually construct the HttpParams and SchemeRegistry, which is a waste.

It seems to me that DefaultHttpClient's constructor should take a ClientConnectionManagerFactory instead of a ClientConnectionManager. That way, the createClientConnectionManager() method already has the factory reference, and doesn't have to grub around in the HttpParams object to find it.

The code would then become:

new DefaultHttpClient(new ThreadSafeClientConnManagerFactory(), null);

where ThreadSafeClientConnManagerFactory.newInstance() just constructs ThreadSafeClientConnManager.  There's no manual construction of HttpParams and SchemeRegistry, you just leave it up to DefaultHttpClient.
"
0,"JSR 283 lifecycle management. JSR 283 specifies a simple lifecycle management mechanism for nodes, and as the reference implementation we should implement that feature. We also need to implement the related TCK tests.

This feature introduces a few new API methods, but during Jackrabbit 1.x we can just introduce them as custom jsr-283 API extensions.
"
0,"Move to the new URIUtil class. Depricate httpclient.URIUtil class and methods
Move all URIUtil calls to httpclient.util.URIUtil"
0,Release the OCM component. The contrib/jackrabbit-jcr-mapping/jcr-mapping should be promoted from contrib into a jackrabbit-jcr-ocm component.
0,"More clarification, improvements and correct behaviour of backwards tests. Backwards tests are used since 2.9 to assert, that the new Lucene version supports drop-in-replacement over the previous version. For this all tests from the previous version are compiled against the old version but then run against the new JAR file.

At the beginning the test suite was checking out another branch and doing this, but this was replaced in 3.1 by directly embedding the previous source tree and the previous tests into the backwards/ subdirectory of the SVN source. The whole idea has several problems:

- Tests not only check *public* APIs, they also check internals and sometimes even fields or package-private methods. This is allowed to change in later versions, so we must be able to change the tests, to support this behaviour. This can be done by modifying the backwards tests to pass, but still use the public API unchanged. Sometimes we simply comment out tests, that test internals and not public APIs. For those tests, I would like to propose a Java Annotation for trunk tests like @LuceneInternalTest - so we can tell the tests runner for backwards (when this test is moved as backwards layer, e.g in 4.1, that it runs all tests *but* not this marked one. This can be done easily with Junit3/4 in LuceneTestCase(J4). This is not part of this issue, but a good idea.
- Sometimes we break backwards compatibility. Currently we do our best to change the tests to reflect this, but this is unneeded and stupi, as it brings two problems. The backwards tests should be compiled against the old version of Lucene. If we change this old Version in the backwards folder, its suddenly becomes nonsense. At least the JAR artifacts of the previous version should stay *unchanged* in all cases! If we break backwards, the correct way to do this, is to simply disable coresponding tests! There is no need to make them work again, as we broke backwards, wy test plugin? The trunk tests already check the functionality, backwards tests only check API. If we fix the break in backwards, we do the contra of what they are for.

So I propose the following and have implemented in a patch for 3.x branch:

- Only include the *tests* and nothing else into the backwards branch, no source files of previous Lucene Core.
- Add the latest released JAR artifact of lucene-core.jar into backwards/lib, optimally with checksum (md5/sh1). This enforces that it is not changed and exactly do what they are for: To compile the previous tests against. This is the only reason for this JAR file.
- If we break backwards, simply *disable* the tests by commenting out, ideally with a short note and the JIRA issue that shows the break.
- If we change inner behaviour of classes, that are not public, dont fix, disable tests. Its simple: backwards tests are only for API compatibility testsing of public APIs. If a test uses internals it should not be run. For that we should use a new annotation in trunk (see above).

This has several good things:

- we can package backwards tests in src ZIP. Its not a full distrib, only the core tests and the JAR file. This enables people that doenloaded the src ZIP files to also run backwrads tests
- Your SVN checkout is not so big and backwards tests run faster!

There are some problems, with one example in the attached patch:

- If we have mock classes in the tests (e.g. MockRAMDirectory) that extend Lucene classes and have access to their internal APIs, a change in these APIs will make them fail to work unchanged. The above example (MockRAMDir) is used in lots of tests and uses a internal RAMDir field that changed type in 3.1. But we cannot disable all tests using this dir (no tests will survive). As we cannot change the previous versions JAR to reflect this, you have to use some trick in this interbal test class. In this case I removed static linking of this field and replaced by reflection. This enables compilation against old JAR, but supports running in new version. This is really a special case, but works good here.

Any comments?"
0,"LowerCaseFilter for Turkish language. java.lang.Character.toLowerCase() converts 'I' to 'i' however in Turkish alphabet lowercase of 'I' is not 'i'. It is LATIN SMALL LETTER DOTLESS I.

"
0,"Remove shared doc stores. With per-thread DocumentsWriters sharing doc stores across segments doesn't make much sense anymore.

See also LUCENE-2324."
0,"Weird locking behaviour in CachingHierarchyManager. in some of our itegration tests the repository sometime locks-up with a stacktrace like this:

""Thread-38"" daemon prio=5 tid=0x08cb3908 nid=0xdd8 runnable [9fef000..9fefd90]
        at org.apache.jackrabbit.core.CachingHierarchyManager.removeLRU(CachingHierarchyManager.java:540)
        - waiting to lock <0x16a9b0e0> (a java.lang.Object)
        at org.apache.jackrabbit.core.CachingHierarchyManager.cache(CachingHierarchyManager.java:510)
        - locked <0x16a9b0e0> (a java.lang.Object)
        at org.apache.jackrabbit.core.CachingHierarchyManager.buildPath(CachingHierarchyManager.java:163)
        at org.apache.jackrabbit.HierarchyManagerImpl.buildPath(HierarchyManagerImpl.java:296)
[...]

although i think that this sacktrace is valid (a thread holding a monitor waiting to lock it again) this scenario shows up a lot during stress-testing. i assume it's the JIT that messesup synchornization. 

the fix is to remove the double monitor entry in this case."
0,SimpleAnalyzer and WhitespaceAnalyzer should have Version ctors. Due to the Changes to CharTokenizer ( LUCENE-2183 ) WhitespaceAnalyzer and SimpleAnalyzer need a Version ctor. Default ctors must be deprecated
0,OpenDocument files missing in mimetypes.properties. The mime-types from Oasis OpenDocument are missing from mimetypes.properties file. 
0,"SPI implementations currently need to provide implementations of both ValueFactory and QValueFactory. This should be simplified so that an implementation of QValueFactory is sufficient.
"
0,"Generify PriorityQueue. Priority Queue should use generics like all other Java 5 Collection API classes. This very simple, but makes code more readable."
0,"Unable to get the status line from a http method object. The status line (typically the first line returned from a http connection read)
is hidden inside httpclient with no way for client code to retrieve it intact.
readStatusLine() in HttpMethodBase is where the status line is
read, but it is never stored and is not available outside the method.

We could store the status line as a string and add a getStatusLine
method to the HttpMethod interface and HttpMethodBase class.  Alternatively, we
could create a header for it with the name StatusLine (or perhaps just null) so
that it could be retrieved with getHeader(""StatusLine"").  This would preserve
the interface but would be a bit of a kludge."
0,"Set omit term freq positions flag on parent field in the index. The flag to omit term frequencies is set to true by default and it is not changed by any of the constructors on the Field class.
We don't use this info in the index anyway, so it is safe to remove it.

The index size gain for 130K nodes (3 leves, ~50 nodes per level) is around 150kb for a 30mb index."
0,Support system property to define the DefaultTransactionTimeout for a XASession. It should be possible to define the DefaultTransactionTimeout for a XASession by a SystemProperty
0,"Cached filter for a single term field. These classes implement inexpensive range filtering over a field containing a single term. They do this by building an integer array of term numbers (storing the term->number mapping in a TreeMap) and then implementing a fast integer comparison based DocSetIdIterator.

This code is currently being used to do age range filtering, but could also be used to do other date filtering or in any application where there need to be multiple filters based on the same single term field. I have an untested implementation of single term filtering and have considered but not yet implemented term set filtering (useful for location based searches) as well. 

The code here is fairly rough; it works but lacks javadocs and toString() and hashCode() methods etc. I'm posting it here to discover if there is other interest in this feature; I don't mind fixing it up but would hate to go to the effort if it's not going to make it into Lucene.

"
0,"broken test in AddEventListener. Here's the test code, comments inline prefixed with ""reschke""

    /**
     * Tests if {@link javax.jcr.observation.Event#NODE_ADDED} is created only
     * for the specified path if <code>isDeep</code> is <code>false</code>.
     */
    public void testIsDeepFalseNodeAdded() throws RepositoryException {
        EventResult listener = new EventResult(log);

        // reschke: we are listening for changes at testRoot/nodeName1, with isDeep==false 
        obsMgr.addEventListener(listener, Event.NODE_ADDED, testRoot + ""/"" + nodeName1, false, null, null, false);

        // reschke; node at ""testRoot/nodeName1"" being created, the associated parent node for this event is ""testRoot""
        Node n = testRootNode.addNode(nodeName1, testNodeType);

        // reschke: node at ""testRoot/nodeName1/nodeName2"" being created, the associated parent node for this event is ""testRoot/nodeName1""
        n.addNode(nodeName2);
        testRootNode.save();

        Event[] events = listener.getEvents(DEFAULT_WAIT_TIMEOUT);
        obsMgr.removeEventListener(listener);

        // reschke: test case expects event with path ""testRoot/nodeName1""
        checkNodeAdded(events, new String[]{nodeName1});
    }

So, in plain english:

- test case listens for events where the associated parent node equals ""testRoot/nodeName1"", but
- it expects a single event where the Event.getPath() returns ""testRoot/nodeName1"".

This is incorrect (IMHO), because the associated parent node for *that* event is ""testRoot"". 

So the correct test would be to check for:

        checkNodeAdded(events, new String[]{nodeName1 + ""/"" + nodeName2});

Making this change of course leads to a test failure reported against the RI.

Feedback appreciated.
"
0,"Provide a Method getCredentialsProvider to the SimpleWebdavServlet. It will be useful to provide a easy way to change the default CredentialsProvider (BasicCredentialsProvider) when the SessionProvider will be created.
It makes sense to let the SessionProvider return a other CredentialsProvider so that no BasicAuthentication wil be prompt.
thanks
claus"
0,Put JavaDoc resources in src/main/javadoc. The Maven javadoc plugin suggests that Javadoc resources like package.html files and doc-files subdirectories should be placed in the src/main/javadoc folder (see http://maven.apache.org/plugins/maven-javadoc-plugin/faq.html). I'll move the javadoc files unless anyone argues otherwise.
0,"new Arabic Analyzer (Apache license). I've noticed there is no Arabic analyzer for Lucene, most likely because Tim Buckwalter's morphological dictionary is GPL.

However, it is not necessary  to have full morphological analysis engine for a quality arabic search. 
This implementation implements the light-8s algorithm present in the following paper: http://ciir.cs.umass.edu/pubfiles/ir-249.pdf

As you can see from the paper, improvement via this method over searching surface forms (as lucene currently does) is significant, with almost 100% improvement in average precision.

While I personally don't think all the choices were the best, and some easily improvements are still possible, the major motivation for implementing it exactly the way it is presented in the paper is that the algorithm is TREC-tested, so the precision/recall improvements to lucene are already documented.

For a stopword list, I used a list present at http://members.unine.ch/jacques.savoy/clef/index.html simply because the creator of this list documents the data as BSD-licensed.

This implementation (Analyzer) consists of above mentioned stopword list plus two filters:
 ArabicNormalizationFilter: performs orthographic normalization (such as hamza seated on alif, alif maksura, teh marbuta, removal of harakat, tatweel, etc)
 ArabicStemFilter: performs arabic light stemming

Both filters operate directly on termbuffer for maximum performance. There is no object creation in this Analyzer.

There are no external dependencies. I've indexed about half a billion words of arabic text and tested against that.

If there are any issues with this implementation I am willing to fix them. I use lucene on a daily basis and would like to give something back. Thanks.
"
0,"Provide rename method for nodes. Currently renaming a node is a nuisance if the node's parent has orderable child nodes: The parents child nodes must be searched for the successor of the node to be moved, the node must be moved to its new name and then ordered before the successor. Furthermore the case where the to be moved node is the last node must be special cased. 

I thus propose to provide functionality for directly renaming nodes.  "
0,"Lock test assumes that changes in one session are immediately visible in different session. LockTest.testLogout() assumes that a change in one session (logging out, removing a session-scoped lock) is immediately visible in another session.

Proposal: insert a 

 n1.getSession().refresh(true);

call before checking

 assertFalse(""node must not be locked"", n1.isLocked());"
0,"Fix for small syntax omission in TermQuery documentation. A coding example, which could be cut'n'paste by a user, has unbalanced parenthesis.

This fix corrects the documentation, making no changes to functionality, only readability."
0,Show referencing nodes in debug log when trying to delete a node with references. This can be very useful when trying to analyze errors from non-interactive applications in a production environment.
0,"Some files are missing the license headers. Jukka provided the following list of files that are missing the license headers.
In addition there might be other files (like build scripts) that don't have the headers.

src/java/org/apache/lucene/document/MapFieldSelector.java
src/java/org/apache/lucene/search/PrefixFilter.java
src/test/org/apache/lucene/TestHitIterator.java
src/test/org/apache/lucene/analysis/TestISOLatin1AccentFilter.java
src/test/org/apache/lucene/index/TestAddIndexesNoOptimize.java
src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
src/test/org/apache/lucene/index/TestFieldInfos.java
src/test/org/apache/lucene/index/TestIndexFileDeleter.java
src/test/org/apache/lucene/index/TestIndexWriter.java
src/test/org/apache/lucene/index/TestIndexWriterDelete.java
src/test/org/apache/lucene/index/TestIndexWriterLockRelease.java
src/test/org/apache/lucene/index/TestIndexWriterMergePolicy.java
src/test/org/apache/lucene/index/TestNorms.java
src/test/org/apache/lucene/index/TestParallelTermEnum.java
src/test/org/apache/lucene/index/TestSegmentTermEnum.java
src/test/org/apache/lucene/index/TestTerm.java
src/test/org/apache/lucene/index/TestTermVectorsReader.java
src/test/org/apache/lucene/search/TestRangeQuery.java
src/test/org/apache/lucene/search/TestTermScorer.java
src/test/org/apache/lucene/store/TestBufferedIndexInput.java
src/test/org/apache/lucene/store/TestWindowsMMap.java
src/test/org/apache/lucene/store/_TestHelper.java
src/test/org/apache/lucene/util/_TestUtil.java
contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleSloppyPhraseQueryMaker.java
contrib/gdata-server/src/core/src/java/org/apache/lucene/gdata/server/FeedNotFoundException.java
contrib/gdata-server/src/core/src/java/org/apache/lucene/gdata/server/registry/ComponentType.java
contrib/gdata-server/src/core/src/java/org/apache/lucene/gdata/server/registry/RegistryException.java
contrib/gdata-server/src/core/src/java/org/apache/lucene/gdata/storage/lucenestorage/StorageAccountWrapper.java
contrib/gdata-server/src/core/src/test/org/apache/lucene/gdata/storage/lucenestorage/TestModifiedEntryFilter.java
contrib/gdata-server/src/gom/src/test/org/apache/lucene/gdata/gom/core/AtomUriElementTest.java
contrib/gdata-server/src/gom/src/test/org/apache/lucene/gdata/gom/core/GOMEntryImplTest.java
contrib/gdata-server/src/gom/src/test/org/apache/lucene/gdata/gom/core/GOMFeedImplTest.java
contrib/gdata-server/src/gom/src/test/org/apache/lucene/gdata/gom/core/GOMGenereatorImplTest.java
contrib/gdata-server/src/gom/src/test/org/apache/lucene/gdata/gom/core/GOMSourceImplTest.java
contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources.java
contrib/javascript/queryConstructor/luceneQueryConstructor.js
contrib/javascript/queryEscaper/luceneQueryEscaper.js
contrib/javascript/queryValidator/luceneQueryValidator.js
contrib/queries/src/java/org/apache/lucene/search/BooleanFilter.java
contrib/queries/src/java/org/apache/lucene/search/BoostingQuery.java
contrib/queries/src/java/org/apache/lucene/search/FilterClause.java
contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery.java
contrib/queries/src/java/org/apache/lucene/search/TermsFilter.java
contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThisQuery.java
contrib/queries/src/test/org/apache/lucene/search/BooleanFilterTest.java
contrib/regex/src/test/org/apache/lucene/search/regex/TestSpanRegexQuery.java
contrib/snowball/src/java/net/sf/snowball/Among.java
contrib/snowball/src/java/net/sf/snowball/SnowballProgram.java
contrib/snowball/src/java/net/sf/snowball/TestApp.java
contrib/spellchecker/src/test/org/apache/lucene/search/spell/TestSpellChecker.java
contrib/surround/src/test/org/apache/lucene/queryParser/surround/query/BooleanQueryTst.java
contrib/surround/src/test/org/apache/lucene/queryParser/surround/query/ExceptionQueryTst.java
contrib/surround/src/test/org/apache/lucene/queryParser/surround/query/SingleFieldTestDb.java
contrib/surround/src/test/org/apache/lucene/queryParser/surround/query/Test01Exceptions.java
contrib/surround/src/test/org/apache/lucene/queryParser/surround/query/Test02Boolean.java
contrib/surround/src/test/org/apache/lucene/queryParser/surround/query/Test03Distance.java
contrib/wordnet/src/java/org/apache/lucene/wordnet/SynExpand.java
contrib/wordnet/src/java/org/apache/lucene/wordnet/SynLookup.java
contrib/wordnet/src/java/org/apache/lucene/wordnet/Syns2Index.java
"
0,"Scan method signatures and add varargs where possible. I changed a lot of signatures, but there may be more. The important ones like MultiReader and MultiSearcher are already done. This applies also to contrib. Varargs are no backwards break, they stay arrays as before."
0,"Eliminate unnecessary uses of Hashtable and Vector. Lucene uses Vector, Hashtable and Enumeration when it doesn't need to. Changing to ArrayList and HashMap may provide better performance.

There are a few places Vector shows up in the API. IMHO, List should have been used for parameters and return values.

There are a few distinct usages of these classes:
# internal but with ArrayList or HashMap would do as well. These can simply be replaced.
# internal and synchronization is required. Either leave as is or use a collections synchronization wrapper.
# As a parameter to a method where List or Map would do as well. For contrib, just replace. For core, deprecate current and add new method signature.
# Generated by JavaCC. (All *.jj files.) Nothing to be done here.
# As a base class. Not sure what to do here. (Only applies to SegmentInfos extends Vector, but it is not used in a safe manner in all places. Perhaps, implements List would be better.)
# As a return value from a package protected method, but synchronization is not used. Change return type.
# As a return value to a final method. Change to List or Map.

In using a Vector the following iteration pattern is frequently used.
for (int i = 0; i < v.size(); i++) {
  Object o = v.elementAt(i);
}

This is an indication that synchronization is unimportant. The list could change during iteration.

"
0,"the unversioned site points to a dead trunk. The unversioned site needs to point to the new merged trunk.
Currently it points to the closed-off dead trunk in two different places.
"
0,"Provide more Example Code. - better project samples showing how to use HttpClient in a variety of ways. 
There is already a src/examples directory which is excellent.  Its in the right
place and should be build with a full compile, if only to know how any API
changes may effect example code, and that we will be required to keep them
current.
- make sure it uses the 2.0 API and no depricated methods!"
0,"tests-local fails on 3 tests. 1)
testMultiSendCookieGet(org.apache.commons.httpclient.TestWebappCookie)junit.framework.AssertionFailedError:
<html>
<head><title>ReadCookieServlet: GET</title></head>
<body>
<p>This is a response to an HTTP GET request.</p>
<p><tt>Cookie:
$Version=1;simplecookie=value;$Path=/httpclienttest/cookie;$Domain=localhost</tt></p>
<tt>$Version=1</tt><br>
<tt>simplecookie=value</tt><br>
<tt>$Path=/httpclienttest/cookie</tt><br>
<tt>$Domain=localhost</tt><br>
</body>
</html>
    at
org.apache.commons.httpclient.TestWebappCookie.testMultiSendCookieGet(TestWebappCookie.java:348)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
2)
testDeleteCookieGet(org.apache.commons.httpclient.TestWebappCookie)junit.framework.AssertionFailedError:
<html>
<head><title>ReadCookieServlet: GET</title></head>
<body>
<p>This is a response to an HTTP GET request.</p>
<p><tt>Cookie:
$Version=1;simplecookie=value;$Path=/httpclienttest/cookie;$Domain=localhost</tt></p>
<tt>$Version=1</tt><br>
<tt>simplecookie=value</tt><br>
<tt>$Path=/httpclienttest/cookie</tt><br>
<tt>$Domain=localhost</tt><br>
</body>
</html>
    at
org.apache.commons.httpclient.TestWebappCookie.testDeleteCookieGet(TestWebappCookie.java:389)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
3)
testDeleteCookiePut(org.apache.commons.httpclient.TestWebappCookie)junit.framework.AssertionFailedError:
<html>
<head><title>ReadCookieServlet: PUT</title></head>
<body>
<p>This is a response to an HTTP PUT request.</p>
<p><tt>Cookie:
$Version=1;simplecookie=value;$Path=/httpclienttest/cookie;$Domain=localhost</tt></p>
<tt>$Version=1</tt><br>
<tt>simplecookie=value</tt><br>
<tt>$Path=/httpclienttest/cookie</tt><br>
<tt>$Domain=localhost</tt><br>
</body>
</html>
    at
org.apache.commons.httpclient.TestWebappCookie.testDeleteCookiePut(TestWebappCookie.java:464)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
FAILURES!!!
Tests run: 108,  Failures: 3,  Errors: 0
httpclient/build.xml [271] Java returned: -1
BUILD FAILED
Total time: 9 seconds"
0,"Make EasySimilarityProvider a full-fledged class . The {{EasySimilarityProvider}} in {{TestEasySimilarity}} would be a good candidate for a full-fledged class. Both {{DefaultSimilarity}} and {{BM25Similarity}} have their own providers, which are effectively the same,so I don't see why we couldn't add one generic provider for convenience."
0,Enable Java asserts in the Junit tests. For background see http://www.mail-archive.com/java-dev@lucene.apache.org/msg10307.html
0,Implement QueryResult.getSelectorNames(). 
0,"Jcr2Spi: Avoid extra round trip to the SPI upon Node.getNode and Session.getItem. Upon Session.getItem/itemExists and Node.getNode/hasNode JCR2SPI currently tries to load the Node from the persistent layer (SPI) if no corresponding entry exists in the hierarchy.

Since with JCR-1638 a flag has been introduced indicating if the child node entries are complete. In this case, the extra round trip could be omitted."
0,"Add files generated by eclipse or maven to svn:ignore. To make life easier for eclipse and maven users please add the following files to svn:ingore:

jackrabbit-jcr-rmi:
-------------------
.settings
.classpath
.project
jackrabbit-jcr-rmi-pom-snapshot-version
project.xml.md5

jackrabbit-core:
----------------
.settings
.classpath
.project
jackrabbit-core-pom-snapshot-version
project.xml.md5

Maybe there some files missing in this list that could help developers using IDEA?"
0,"AttributeSource's methods for accessing attributes should be final, else its easy to corrupt the internal states. The methods that operate and modify the internal maps of AttributeSource should be final, which is a backwards break. But anybody that overrides such methods simply creates a buggy AS either case.

I want to makeall impls final (in general the class should be final at all, but it is made for extension in TokenStream). So its important that the implementations are final!"
0,"Move SimpleWebdavServlet to jcr-server and make it abstract. In line with isse JCR-417, I suggest to partially move the SimpleWebdavServlet from the jcr-webapp project to the jcr-server project. By partially I mean, that the new (moved) servlet will be abstract and the getRepository() method will be abstract. The jcr-webapp project will still contain a SimpleWebdavServlet (for backwards compatibility maintaing the same name) which just extends the new servlet and implements the getRepository() method using the RepositoryAccess servlet.

This allows for the reuse of the jcr-server project including the abstract SimpleWebdavServlet in other environments. My intention is to include this project (along with the webdav) project in Sling.

Will provide a patch for this proposal

(This issue is separated out of JCR-1262 as suggested by Angela)
"
0,"Simplify StandardTokenizer JFlex grammar. Summary of thread entitled ""Fullwidth alphanumeric characters, plus a question on Korean ranges"" begun by Daniel Noll on java-user, and carried over to java-dev:

On 01/07/2008 at 5:06 PM, Daniel Noll wrote:
> I wish the tokeniser could just use Character.isLetter and
> Character.isDigit instead of having to know all the ranges itself, since
> the JRE already has all this information.  Character.isLetter does
> return true for CJK characters though, so the ranges would still come in
> handy for determining what kind of letter they are.  I don't support
> JFlex has a way to do this...

The DIGIT macro could be replaced by JFlex's predefined character class [:digit:], which has the same semantics as java.lang.Character.isDigit().

Although JFlex's predefined character class [:letter:] (same semantics as java.lang.Character.isLetter()) includes CJK characters, there is a way to handle this using JFlex's regex negation syntax {{!}}.  From [the JFlex documentation|http://jflex.de/manual.html]:

bq. [T]he expression that matches everything of {{a}} not matched by {{b}} is !(!{{a}}|{{b}}) 

So to exclude CJ characters from the LETTER macro:

{code}
    LETTER = ! ( ! [:letter:] | {CJ} )
{code}
 
Since [:letter:] includes all of the Korean ranges, there's no reason (AFAICT) to treat them separately; unlike Chinese and Japanese characters, which are individually tokenized, the Korean characters should participate in the same token boundary rules as all of the other letters.

I looked at some of the differences between Unicode 3.0.0, which Java 1.4.2 supports, and Unicode 5.0, the latest version, and there are lots of new and modified letter and digit ranges.  This stuff gets tweaked all the time, and I don't think Lucene should be in the business of trying to track it, or take a position on which Unicode version users' data should conform to.  

Switching to using JFlex's [:letter:] and [:digit:] predefined character classes ties (most of) these decisions to the user's choice of JVM version, and this seems much more reasonable to me than the current status quo.

I will attach a patch shortly.
"
0,Remove SegmentReader.document synchronization. This is probably the last synchronization issue in Lucene.  It is the document method in SegmentReader.  It is avoidable by using a threadlocal for FieldsReader.  
0,"Add a new TestBackwardsCompatibility index for flex backwards (a 3.0 one with also numeric fields). In flex we change also the encode/decoder for numeric fields (NumericTokenSteam) using BytesRef and also the collation filters. We should add a test index from 3.0 that contains these fields and do some validation, that field contents did not change when read with flex."
0,"Restore mix:referenceable check to SessionImpl.getNodeByUUID. In revision 504623 we commented out the mix:referenceable check in the SessionImpl.getNodeByUUID() method:

    // since the uuid of a node is only exposed through jcr:uuid declared
    // by mix:referenceable it's rather unlikely that a client can possibly
    // know the internal uuid of a non-referenceable node; omitting the
    // check for mix:referenceable seems therefore to be a reasonable
    // compromise in order to improve performance.
    /*
    if (node.isNodeType(Name.MIX_REFERENCEABLE)) {
        return node;
    } else {
        // there is a node with that uuid but the node does not expose it
        throw new ItemNotFoundException(uuid.toString());
    }
    */

This solved a minor performance issue issue with client code that used the node UUID as a quick way to access a node. The downside was a slight incompatibility with the spec that says that the getNodeByUUID method is only supposed to work with mix:referenceable nodes.

Now with JCR 2.0 clients can (and should) use the Session.getNodeByIdentifier method that does not have the mix:referenceable limitation. Thus we can restore the original and correct functionality of the getNodeByUUID method."
0,"Session.importXML and Workspace.importXML throw wrong exception. According to the JCR specification (section 7.3.6 and 7.3.7), if uuidBehaviour is set to IMPORT_UUID_COLLISION_REMOVE_EXISTING, a ConstraintViolationException should be thrown, when an incoming node has the same UUID as the node at parentAbsPath or one of its ancestors.

Currently, a RepositoryException is thrown instead.
"
0,"move o.a.l.index.codecs.* -> o.a.l.codecs.*. These package names are getting pretty long, e.g.:

org.apache.lucene.index.codecs.lucene40.values.XXXXYYYY

I think we should move it to just the codecs package now while it won't cause anyone any trouble."
0,"Node.addNode() does not scale with increasing content. With increasing repository content (and versions), the time to create new nodes increases. For example with around 6500 nodes and 33500 properties, it takes around 3 seconds (!) to just add one single node !

When attaching to the application with a Debugger and delibaretly suspending the VM, this stack trace is displayed all the times :

   [ changing internals of access List iterator ]
   PersistentNodeState(NodeState).getChildNodeEntries(String) line: 362
   PersistentNode.getName() line: 84
   PersistentVersionManager.getVersion(String) line: 278
   VersionManager.getVersion(String) line: 304
   VersionItemStateProvider.getNodeState(NodeId) line: 124
   VersionItemStateProvider.hasPropertyState(PropertyId) line: 154
   VersionItemStateProvider.hasItemState(ItemId) line: 174
   SessionItemStateManager.getItemState(ItemId) line: 246
   ItemManager.createItemInstance(ItemId) line: 563
   ItemManager.getItem(ItemId) line: 332
   NodeImpl.getProperty(QName) line: 876
   NodeImpl.hasProperty(QName) line: 893
   NodeImpl.safeIsCheckedOut() line: 2515
   NodeImpl.internalAddChildNode(QName, NodeTypeImpl, String) line: 527
   NodeImpl.internalAddNode(String, NodeTypeImpl, String) line: 475
   NodeImpl.internalAddNode(String, NodeTypeImpl) line: 436
   NodeImpl.addNode(String, String) line: 1145
   ...

It seems, that virtual item state providers are asked for whatever property is looked for and this in return calls into the version handler, which loops over some child entries (currently around 1100 entries) to find one single entry with a given UUID.

Besides the latter not being optimal and certainly not scaling, the former has its problems in its own right."
0,"Define clear semantics for Directory.fileLength. On this thread: http://mail-archives.apache.org/mod_mbox/lucene-java-dev/201003.mbox/%3C126142c1003121525v24499625u1589bbef4c0792e7@mail.gmail.com%3E it was mentioned that Directory's fileLength behavior is not consistent between Directory implementations if the given file name does not exist. FSDirectory returns a 0 length while RAMDirectory throws FNFE.

The problem is that the semantics of fileLength() are not defined. As proposed in the thread, we'll define the following semantics:

* Returns the length of the file denoted by <code>name</code> if the file exists. The return value may be anything between 0 and Long.MAX_VALUE.
* Throws FileNotFoundException if the file does not exist. Note that you can call dir.fileExists(name) if you are not sure whether the file exists or not.

For backwards we'll create a new method w/ clear semantics. Something like:

{code}
/**
 * @deprecated the method will become abstract when #fileLength(name) has been removed.
 */
public long getFileLength(String name) throws IOException {
  long len = fileLength(name);
  if (len == 0 && !fileExists(name)) {
    throw new FileNotFoundException(name);
  }
  return len;
}
{code}

The first line just calls the current impl. If it throws exception for a non-existing file, we're ok. The second line verifies whether a 0 length is for an existing file or not and throws an exception appropriately."
0,"Current implementation of fuzzy and wildcard queries inappropriately implemented as Boolean query rewrites. The implementation of MultiTermQuery in terms of BooleanQuery introduces several problems:

1) Collisions with maximum clause limit on boolean queries which throws an exception.  This is most problematic because it is difficult to ascertain in advance how many terms a fuzzy query or wildcard query might involve.

2) The boolean disjunctive scoring is not appropriate for either fuzzy or wildcard queries.  In effect the score is divided by the number of terms in the query which has nothing to do with the relevancy of the results.

3) Performance of disjunctive boolean queries for large term sets is quite sub-optimal"
0,Backport JCR-940: add db connection autoConnect for BundleDbPersistenceManager. Backport issue JCR-940 ( add db connection autoConnect for BundleDbPersistenceManager) to 1.3 branch for 1.3.4 (separate issue to avoid re-opening JCR-940 which was already released with 1.4). 
0,"Contrib analyzers need tests. The analyzers in contrib need tests, preferably ones that test the behavior of all the Token 'attributes' involved (offsets, type, etc) and not just what they do with token text.

This way, they can be converted to the new api without breakage."
0,"improve BaseTokenStreamTestCase to test end(). If offsetAtt/end() is not implemented correctly, then there can be problems with highlighting: see LUCENE-2207 for an example with CJKTokenizer.

In my opinion you currently have to write too much code to test this.

This patch does the following:
* adds optional Integer finalOffset (can be null for no checking) to assertTokenStreamContents
* in assertAnalyzesTo, automatically fill this with the String length()

In my opinion this is correct, for assertTokenStreamContents the behavior should be optional, it may not even have a Tokenizer. If you are using assertTokenStreamContents with a Tokenizer then simply provide the extra expected value to check it.

for assertAnalyzesTo then it is implied there is a tokenizer so it should be checked.

the tests pass for core but there are failures in contrib even besides CJKTokenizer (apply Koji's patch from LUCENE-2207, it is correct). Specifically ChineseTokenizer has a similar problem.
"
0,Provide fail-over for multi-home remote servers (if one server in a farm goes down). The HTTP Client does not provide automatic fail-over for multi-home remote servers (web-farm) if one server in a farm goes down
0,"Content-Length & Transfer-Encoding request headers should be handled by entity enclosing methods. Currently 'Content-Length' & 'Transfer-Encoding' request headers are handled by 
the HttpMethodBase class. This is conceptually wrong and error-prone in my 
opinion. Entity enclosing methods should control 'Content-Length' & 'Transfer-
Encoding' request headers instead, as they provide request content and 
encapsulate the requisite content transfer logic."
0,"Unit tests TestBackwardsCompatibility and TestIndexFileDeleter might fail depending on JVM. In the two units tests TestBackwardsCompatibility and TestIndexFileDeleter several index file names are hardcoded. For example, in TestBackwardsCompatibility.testExactFileNames() it is tested if the index directory contains exactly the expected files after several operations like addDocument(), deleteDocument() and setNorm() have been performed. Apparently the unit tests pass on the nightly build machine, but in my environment (Windows XP, IBM JVM 1.5) they fail for the following reason:

When IndexReader.setNorm() is called a new norm file for the specified field is created with the file  ending .sx, where x is the number of the field. The problem is that the SegmentMerger can not guarantee to keep the order of the fields, in other words after a merge took place a field can have a different field number. This specific testcase fails, because it expects the file ending .s0, but the file has the ending .s1.

The reason why the field numbers can be different on different JVMs is the use of HashSet in SegmentReader.getFieldNames(). Depending on the HashSet implementation an iterator might not iterate over the entries in insertion order. When I change HashSet to LinkedHashSet, the two testcases pass.

However, even with a LinkedHashSet the order of the field numbers might change during a merge, because the order in which the SegmentMerger merges the FieldInfos depends on the field options like TERMVECTOR, INDEXED... (see SegmentMerger.mergeFields() for details). 

So I think we should not use LinkedHashSet but rather change the problematic testcases. Furthermore I'm not sure if we should have hardcoded filenames in the tests anyway, because if we change the index format or file names in the future these test cases would fail without modification."
0,"Remove @lucene.experimental from Numeric*. NumericRangeQuery and NumericField are now there since 2.9. It is still marked as experimental. The API stabilized and there are no changes in the public parts (even in Lucene trunk no changes). Also lot's of people ask, if ""experimental"" means ""unstable"" in general, but it means only ""unstable API"".

I will remove the @lucene.experimental from Numeric* classes. NumericUtils* stays with @lucene.internal, as it is not intended for public use. Some people use it to make ""TermQuery"" on a numeric field, but this should be done using a NRQ with upper==lower and included=true, which does not affect scoring (applies also to Solr)."
0," The jackrabbit-ocm DTD 1.5 is missing and has to be publish. 
The jackrabbit-ocm DTD 1.5 is missing and it should be made available for reference on the Jackrabbit web site."
0,Highlighter Documentation updates. Various places in the Highlighter documentation refer to bytes (i.e. SimpleFragmenter) when it should be chars.  See http://www.gossamer-threads.com/lists/lucene/java-user/56986
0,"Configurable actions upon authorizable creation and removal. i would like to add the possibility to configure custom actions that are executed upon user (and group) creation before 
the operation is persisted. this would allow applications to run custom code without the need of subclassing the
usermanager implementation. e.g.: creating additional mandatory properties, setting up permissions, calculating default 
group membership. the same applies for user/group removal."
0,"Basic refactoring of DocumentsWriter. As a starting point for making DocumentsWriter more understandable,
I've fixed its inner classes to be static, and then broke the classes
out into separate sources, all in org.apache.lucene.index package.

"
0,"Consolidate Lucene's QueryParsers into a module. Lucene has a lot of QueryParsers and we should have them all in a single consistent place.  

The following are QueryParsers I can find that warrant moving to the new module:

- Lucene Core's QueryParser
- AnalyzingQueryParser
- ComplexPhraseQueryParser
- ExtendableQueryParser
- Surround's QueryParser
- PrecedenceQueryParser
- StandardQueryParser
- XML-Query-Parser's CoreParser

All seem to do a good job at their kind of parsing with extensive tests.

One challenge of consolidating these is that many tests use Lucene Core's QueryParser.  One option is to just replicate this class in src/test and call it TestingQueryParser.  Another option is to convert all tests over to programmatically building their queries (seems like alot of work)."
0,"FilteredQuery should have getFilter(). Unless you are in the same package, you can't access the filter in a FilteredQuery.
A getFilter() method should be added."
0,BoostingNearQuery must implement clone - now it clones to a NearSpanQuery - cloning is required for Span container classes. 
0,"JSR 283: Repository Compliance. JSR 283 defines a huge set of new (or changed) repository descriptors as well as a couple of new methods on the
Repository interface that allows the API consumer to determine the feature set exposed by an implementation.

The new methods are

- Repository.isStandardDescriptor(String key)
- Repository.isSingleValueDescriptor(String key)
- Repository.getDescriptorValue(String key) Value
- Repository.getDescriptorValues(String key) Value[]"
0,"refactor HttpClientConnection and HttpProxyConnection. Instead of trying to define a full abstraction for client connections, let's define only a minimal interface in HttpCore with only those methods actually needed in the core. In particular, the core does not need to open connections (since HTTPCORE-11), and it does not care whether a connection is direct or through a proxy. An abstraction for client connections can be defined in HttpConn.

(original description:)
As discussed on the mailing list, separating the responsibility for establishing connections from the connection objects could improve the design and help with proxy support.
"
0,"Not getting random-seed/reproduce-with if a test fails from another thread. See https://builds.apache.org/job/Lucene-Solr-tests-only-trunk/12822/console as an example.

This is at least affecting 4.0, maybe 3.x too"
0,"Create a MaxFieldLengthAnalyzer to wrap any other Analyzer and provide the same functionality as MaxFieldLength provided on IndexWriter. A spinoff from LUCENE-2294. Instead of asking the user to specify on IndexWriter his requested MFL limit, we can get rid of this setting entirely by providing an Analyzer which will wrap any other Analyzer and its TokenStream with a TokenFilter that keeps track of the number of tokens produced and stop when the limit has reached.

This will remove any count tracking in IW's indexing, which is done even if I specified UNLIMITED for MFL.

Let's try to do it for 3.1."
0,serialVersionUID for AuthorizableExistsException. 
0,ExtendableQueryParser should allow extensions to access the toplevel parser settings/ properties. Based on the latest discussions in LUCENE-2039 this issue will expose the toplevel parser via the ExtensionQuery so that ExtensionParsers can access properties like getAllowLeadingWildcards() from the top level parser.
0,"SessionImpl#isSupportedOption: Skip descriptor evaluation if descriptor has not been loaded. followup issue for JCR-3076.

as jukka stated changing the jcr-server to serve the repository-descriptor without mandating a successful login would
require quite some changes on the server side as the current flow demands a successful repository login in order
to be access any resource including the root resource that acts as parent for all (available) workspaces. since the
repository-descriptor report has be requested one of the resources it also mandates a successful login although
retrieving descriptors on the jcr-level is possible when just having the repository at hand.

on the other hand i would assume that the descriptor functionality present on the Repository is rarely used.
therefore i would suggest to just relax the check for supported options in the jcr2spi session implementation
and skip the evaluation if the descriptor isn't available at all. consequently the failure of a non-supported
feature would be postponed to the point it reaches the SPI (instead of informing the API consumer upfront). 
on the other hand supported operations would not fail just because the descriptors have not been loaded."
0,"TCK: Test that expect that modifications made by Session1 are automatically visible to Session2. While changes made by session1 are automatically visible to any other session2 with the RI, this is not required by the
specification. Therefore i would suggest to modify the following test cases:

- NodeUUIDTest.testSaveMovedRefNode()
- SessionUUIDTest.testSaveMovedRefNode()

-> no patch. sorry.

- NodeTest.testRemoveInvalidItemStateException()

-> see patch."
0,use AtomicInteger/Boolean to track IR.refCount and IW.closed. Less costly than synchronized methods we have now...
0,"Improved logging for session operations. I'd like to add the following logging features to SessionOperations:

* Use MDC [1] to make it possible to filter and redirect logs based on which session is being used
* Add simple debug-level timing information for executed SessionOperations to help pinpoint performance issues
* Classify SessionOperations as read or write operations, and log warnings (instead of the current debug messages) for concurrent writes on a session

[1] http://logback.qos.ch/manual/mdc.html"
0,"Remove unused ""numSlotsFull"" from FieldComparator.setNextReader. This param is a relic from older optimizations that we've since turned off, and it's quite confusing.  I don't think we need it, and we haven't released the API yet so we're free to remove it now."
0,Remove @author tags in jackrabbit-jcr-rmi. It is a recommendation within Apache not to use @author tags or other means to identify source code with individual developers.  The @author tags in jackrabbit-jcr-rmi should therefore be removed.
0,"Evil up MockDirectoryWrapper.checkIndexOnClose. MockDirectoryWrapper checks any indexes tests create on close(), if they exist.

The problem is the logic it uses to determine if an index exists could mask real bugs (e.g. segments file corrumption):
{code}
if (DirectoryReader.indexExists(this) {
  ...
  // evil stuff like crash()
  ...
  _TestUtil.checkIndex(this)
}
{code}

and for reference DirectoryReader.indexExists is:
{code}
try {
  new SegmentInfos().read(directory);
  return true;
} catch (IOException ioe) {
  return false;
}
{code}

So if there are segments file problems, we just silently do no checkIndex.
"
0,"Make ""ant -projecthelp"" show the javadocs and docs targets as well. Added a description to the targets ""javadocs"" and ""docs"".
This makes ant show them when the executes ""ant -projecthelp""
"
0,"Rename ""Version Managers"". currently there is a VersionManager interface and VersionManagerImpl class that operate on the version storage.
new for JSR283, the is a javax.jcr.version.VersionManager and its implementation JcrVersionManager.

in order to avoid confusion, i would like to rename the following classes:

o.a.j.core.version.VersionManager - > o.a.j.core.version.InternalVersionManager
o.a.j.core.version.VersionManagerImpl -> o.a.j.core.version.InternalVersionManagerImpl
o.a.j.core.version.AbstractVersionManager -> o.a.j.core.version.AbstractInternalVersionManager
o.a.j.core.version.XAVersionManager -> o.a.j.core.version.XAInternalVersionManager

o.a.j.core.version.JcrVersionManagerImpl -> o.a.j.core.version.VersionManagerImpl"
0,"FieldCacheSanityChecker called directly by FieldCache.get*. As suggested by McCandless in LUCENE-1749, we can make FieldCacheImpl a client of the FieldCacheSanityChecker and have it sanity check itself each time it creates a new cache entry, and log a warning if it thinks there is a problem.  (although we'd probably only want to do this if the caller has set some sort of infoStream/warningStream type property on the FieldCache object."
0,spi2dav: JSR 283 NodeType Management. 
0,"tweak AppendingCodec to write segments_N compatible with 'normal' Lucene. Just an easy improvement from LUCENE-3490:

Currently AppendingCodec writes a different segments_N format (it writes no checksum at all in commit())
If you don't configure your codecprovider correctly in IndexReader, you will get read past EOF.
(we have some proposed fixes for this stuff in LUCENE-3490 branch)

But besides this, all it really needs to do is no-op prepareCommit(), it can still write the 'final' checksum
which is a good thing."
0,"Make DocumentsWriter more robust on hitting OOM. I've been stress testing DocumentsWriter by indexing wikipedia, but not
giving enough memory to the JVM, in varying heap sizes to tickle the
different interesting cases.  Sometimes DocumentsWriter can deadlock;
other times it will hit a subsequent NPE or AIOOBE or assertion
failure.

I've fixed all the cases I've found, and added some more asserts.  Now
it just produces plain OOM exceptions.  All changes are contained to
DocumentsWriter.java.

All tests pass.  I plan to commit in a day or two!"
0,"[PATCH] disable coord for generated BooleanQueries. Here's a patch that disables Similiarty.coord() in the scoring of most
automatically generated boolean queries.  The coord() score factor is
appropriate when clauses are independently specified by a user, but is usually
not appropriate when clauses are generated automatically, e.g., by a fuzzy,
wildcard or range query.  Matches on such automatically generated queries are
currently penalized for not matching all terms."
0,Demo HTMLParser compares StringBuffer to an empty String with .equals. 
0,"User configurable cookie policy. Some user configurable how cookies are handled.  Emulate cookie options in web
browsers."
0,"Document number integrity merge policy. This patch allows for document numbers stays the same even after merge of segments with deletions.

Consumer needs to do this:
indexWriter.setSkipMergingDeletedDocuments(false);

The effect will be that deleted documents are replaced by a new Document() in the merged segment, but not marked as deleted. This should probably be some policy thingy that allows for different solutions such as keeping the old document, et c.

Also see http://www.nabble.com/optimization-behaviour-tf3723327.html#a10418880
"
0,"multilingual analyzer based on icu. The standard analyzer in lucene is not exactly unicode-friendly with regards to breaking text into words, especially with respect to non-alphabetic scripts.  This is because it is unaware of unicode bounds properties.

I actually couldn't figure out how the Thai analyzer could possibly be working until i looked at the jflex rules and saw that codepoint range for most of the Thai block was added to the alphanum specification. defining the exact codepoint ranges like this for every language could help with the problem but you'd basically be reimplementing the bounds properties already stated in the unicode standard. 

in general it looks like this kind of behavior is bad in lucene for even latin, for instance, the analyzer will break words around accent marks in decomposed form. While most latin letter + accent combinations have composed forms in unicode, some do not. (this is also an issue for asciifoldingfilter i suppose). 

I've got a partially tested standardanalyzer that uses icu Rule-based BreakIterator instead of jflex. Using this method you can define word boundaries according to the unicode bounds properties. After getting it into some good shape i'd be happy to contribute it for contrib but I wonder if theres a better solution so that out of box lucene will be more friendly to non-ASCII text. Unfortunately it seems jflex does not support use of these properties such as [\p{Word_Break = Extend}] so this is probably the major barrier.

Thanks,
Robert



"
0,"Support more queries (other than just title) in Trec quality pkg. Now that we can properly parse descriptions and narratives from TREC queries (LUCENE-2210), it would be nice to allow the user to easily run quality evaluations on more than just ""Title""

This patch adds an optional commandline argument to QueryDriver (the default is Title as before), where you can specify something like:
T: Title-only
D: Description-only
N: Narrative-only
TD: Title + Description,
TDN: Title+Description+Narrative,
DN: Description+Narrative

The SimpleQQParser has an additional constructor that simply accepts a String[] of these fields, forming a booleanquery across all of them.
"
0,"Missing support for some ""general"" relations in QueryTreeDump and xpath.QueryFormat. The two classes lack support for some of the ""general"" relations in XPath.
"
0,"Allow (or bring back) the ability to setRAMBufferSizeMB on an open IndexWriter. In 3.1 the ability to setRAMBufferSizeMB is deprecated, and removed in trunk. It would be great to be able to control that on a live IndexWriter. Other possible two methods that would be great to bring back are setTermIndexInterval and setReaderTermsIndexDivisor. Most of the other setters can actually be set on the MergePolicy itself, so no need for setters for those (I think)."
0,"Add configuration options for search manager. Right now, if the search manager is active, everything is indexed, even the system branch of a workspace with the versions.

take parameters / conditions into account whether a node should be indexed:
- path
- node type
- property type
- property name


see also http://thread.gmane.org/gmane.comp.apache.jackrabbit.devel/3343"
0,IndexWriter has incomplete Javadocs. A couple of getter methods in IndexWriter have no javadocs.
0,"Jackrabbit utilities. Attached are two utilities for Jackrabbit:

The first one is a DataStore implementation that uses Amazon S3 for storage.
This is fairly straightforward. It is configured by adding a DataStore
section to the repository.xml file, e.g.:
   <DataStore class=""org.jcrutil.S3DataStore"">
       <param name=""awsAccessKey"" value="""" />
       <param name=""awsSecretKey"" value="""" />
       <param name=""bucketName"" value="""" />
       <param name=""minModifiedDate"" value=""0"" />
       <param name=""minRecordLength"" value=""0"" />
   </DataStore>

The second utility is a JCR based Commons VFS filesystem provider. This
allows you to access a JCR repository (nt:file and nt:folder nodes) using
the Commons VFS API. I've also used this with MINA FTP Server and Dctm VFS
(http://dctmvfs.sourceforge.net/) to provide FTP access to a Jackrabbit
repository.
"
0,Add link to irc channel #lucene on the website. We should add a link to #lucene IRC channel on chat.freenode.org. 
0,"Move & rename the terms dict, index, abstract postings out of oal.index.codecs.standard. The terms dict components that current live under Standard codec
(oal.index.codecs.standard.*) are in fact very generic, and in no way
particular to the Standard codec.  Already we have many other codecs
(sep, fixed int block, var int block, pulsing, appending) that re-use
the terms dict writer/reader components.

So I'd like to move these out into oal.index.codecs, and rename them:

  * StandardTermsDictWriter/Reader -> PrefixCodedTermsWriter/Reader
  * StandardTermsIndexWriter/Reader -> AbstractTermsIndexWriter/Reader
  * SimpleStandardTermsIndexWriter/Reader -> SimpleTermsIndexWriter/Reader
  * StandardPostingsWriter/Reader -> AbstractPostingsWriter/Reader
  * StandardPostingsWriterImpl/ReaderImple -> StandardPostingsWriter/Reader

With this move we have a nice reusable terms dict impl.  The terms
index impl is still well-decoupled so eg we could [in theory] explore
a variable gap terms index.

Many codecs, I expect, don't need/want to implement their own terms
dict....

There are no code/index format changes here, besides the renaming &
fixing all imports/usages of the renamed class.
"
0,"[PATCH] Performance improvement to DisjunctionSumScorer. A recent profile of the new BooleanScorer2 showed that 
quite a bit of CPU time is spent in the advanceAfterCurrent method 
of DisjunctionScorer, and in the PriorityQueue of scorers that 
is used there. 
 
This patch reduces the internal overhead of DisjunctionScorer 
to about 70% of the current one (ie. 30% saving in cpu time). 
It also reduces the number of calls to the subscorers, but 
that was not measured. 
 
To get this, it was necessary to specialize the PriorityQueue 
for a Scorer and to add move some code fragments from DisjunctionScorer 
to this specialized queue."
0,"Refactoring of FilteredTermsEnum and MultiTermQuery. FilteredTermsEnum is confusing as it is initially positioned to the first term. It should instead work like an uninitialized TermsEnum for a field before the first call to next() or seek().
FilteredTermsEnums cannot implement seek() as eg. NRQ or Automaton are not able to support this. Seeking is also not needed for MTQ at all, so seek can just throw UOE.
This issue changes some of the internal behaviour of MTQ and FilteredTermsEnum to allow also seeking in NRQ and Automaton (see comments below)."
0,Formatable changes log  (CHANGES.txt is easy to edit but not so friendly to read by Lucene users). Background in http://www.nabble.com/formatable-changes-log-tt15078749.html
0,Add simple benchmarking tools for jcr2spi read performance. 
0,Rename EasySimilarity to SimilarityBase. 
0,"ClientConnectionRelease example is incorrect. http://svn.apache.org/repos/asf/httpcomponents/httpclient/tags/4.0.1/httpclient/src/examples/org/apache/http/examples/client/ClientConnectionRelease.java

is incorrect: 

1. if error happens in BufferedReader constructor (OutOfMemoryError, StackOverflowError), reader.close() is not called and connection is not released

2. if error happens in reader.readLine(), reader.close() is called, but httpget.abort() is not."
0,"Move SmartChineseAnalyzer & resources to own contrib project. SmartChineseAnalyzer depends on  a large dictionary that causes the analyzer jar to grow up to 3MB. The dictionary is quite big compared to all the other resouces / class files contained in that jar. 
Having a separate analyzer-cn contrib project enables footprint-sensitive users (e.g. using lucene on a mobile phone) to include analyzer.jar without getting into trouble with disk space.

Moving SmartChineseAnalyzer to a separate project could also include a small refactoring as Robert mentioned in [LUCENE-1722|https://issues.apache.org/jira/browse/LUCENE-1722] several classes should be package protected, members and classes could be final, commented syserr and logging code should be removed etc.

I set this issue target to 2.9 - if we can not make it until then feel free to move it to 3.0
"
0,"Query scorers should not use MultiFields. Lucene does all searching/filtering per-segment, today, but there are a number of tests that directly invoke Scorer.scorer or Filter.getDocIdSet on a composite reader."
0,Remove RepositoryService exists(). RepositoryService exists() is not used anywhere. I suggest to remove it. 
0,"Document Package level javadocs need improving. The document package package level javadocs could use some improving, such as:
1. Info on what a Document is, as well as Field and Fieldable
2. Examples of FieldSelectors and how to implement
3. Samples of using DateTools and NumberTools"
0,"[PATCH] Decouple locking implementation from Directory implementation. This is a spinoff of http://issues.apache.org/jira/browse/LUCENE-305.

I've opened this new issue to capture that it's wider scope than
LUCENE-305.

This is a patch originally created by Jeff Patterson (see above link)
and then modified as described here:

  http://issues.apache.org/jira/browse/LUCENE-305#action_12418493

with some small additional changes:

  * For each FSDirectory.getDirectory(), I made a corresponding
    version that also accepts a LockFactory instance.  So, you can
    construct an FSDirectory with your own LockFactory.

  * Cascaded defaulting for FSDirectory's LockFactory implementation:
    if you pass in a LockFactory instance, it's used; else if
    setDisableLocks was called, we use NoLockFactory; else, if the
    system property ""org.apache.lucene.store.FSDirectoryLockFactoryClass""
    is defined, we use that; finally, we'll use the original locking
    implementation (SimpleFSLockFactory).

The gist is that all locking code has been moved out of *Directory and
into subclasses of a new abstract LockFactory class.  You can now set
the LockFactory of a Directory to change how it does locking.  For
example, you can create an FSDirectory but set its locking to
SingleInstanceLockFactory (if you know all writing/reading will take
place a single JVM).

The changes pass all unit tests (on Ubuntu Linux Sun Java 1.5 and
Windows XP Sun Java 1.4), and I added another TestCase to test the
LockFactory code.

Note that LockFactory defaults are not changed: FSDirectory defaults
to SimpleFSLockFactory and RAMDirectory defaults to
SingleInstanceLockFactory.

Next step (separate issue) is to create a LockFactory that uses the OS
native locks (through java.nio).
"
0,"Leftover legacy enum in IndexReader. In IndexReader we still have some leftover ""handmade"" enum from pre-Java5 times. Unfortunately the Generics/Java5 Policeman did not notice it.

This patch is just code cleanup, no baclkwards breaks, as code using this enum would not see any difference (because only superclass changes).

I will commit this asap."
0,PropertyTypeRegistry should also yield if property is multi-valued. Currently only the PropertyType is available for a certain property name. In some cases it is also required to know if the property is single- or multi-valued.
0,"Allow similarity to encode norms other than a single byte. LUCENE-3628 cut over norms to docvalues. This removes the long standing limitation that norms are a single byte. Yet, we still need to expose this functionality to Similarity to write / encode norms in a different format. "
0,"DefaultHttpRequestRetryHandler is not handling PUT as an idempotent method for retries, though RFC2616 section 9.1.2 clearly defines it to be one.. See attached patch file for a fix:

Fix treats PUT requests as idempotent, marking them to be retried when their enclosed HttpEntity is either null or repeatable.

"
0,"Try harder to prevent SIGSEGV on cloned MMapIndexInputs. We are unmapping mmapped byte buffers which is disallowed by the JDK, because it has the risk of SIGSEGV when you access the mapped byte buffer after unmapping.

We currently prevent this for the main IndexInput by setting its buffer to null, so we NPE if somebody tries to access the underlying buffer. I recently fixed also the stupid curBuf (LUCENE-3200) by setting to null.

The big problem are cloned IndexInputs which are generally not closed. Those still contain references to the unmapped ByteBuffer, which lead to SIGSEGV easily. The patch from Mike in LUCENE-3439 prevents most of this in Lucene 3.5, but its still not 100% safe (as it uses non-volatiles).

This patch will fix the remaining issues by also setting the buffers of clones to null when the original is closed. The trick is to record weak references of all clones created and close them together with the original. This uses a ConcurrentHashMap<WeakReference<MMapIndexInput>,?> as store with the logic borrowed from WeakHashMap to cleanup the GCed references (using ReferenceQueue).

If we respin 3.5, we should maybe also get this in."
0,"revise Scorer visitor API. Currently there is an (expert) API in scorer to allow you to take a scorer, and visit its children etc (e.g. booleanquery).

I think we should improve this, so its general to all queries.

The current issues are:
* the current api is hardcoded for booleanquery's Occurs, but we should support other types of children (e.g. disjunctionmax)
* it can be difficult to use the API, because of the amount of generics and the visitor callback API. 
* the current API enforces a DFS traversal when you might prefer BFS instead.
"
0,"Each TransactionContext creates new thread. The rollback threads are not stopped when the transaction commits, but only when the timeout occurs. This has the effect that lots of threads are created and sleeping when many transactions are committed in a short time frame. The rollback thread should be signaled when the transaction is committed or even better a Timer should be used with a single thread for all transaction contexts."
0,Refactor RewriteMethods out of MultiTermQuery. Policeman work :-) - as usual
0,"Move the XASession interface to jackrabbit-api. As discussed in JCR-953, it would probably make sense to move the org.apache.jackrabbit.core.XASession from jackrabbit-core to jackrabbit-api as org.apache.jackrabbit.api.XASession."
0,"CookieIdentityComparator and CookiePathComparator could/should implement Serializable. CookieIdentityComparator and CookiePathComparator could/should implement Serializable

As Findbugs suggests:

""Comparator doesn't implement Serializable

This class implements the Comparator interface. You should consider whether or not it should also implement the Serializable interface. If a comparator is used to construct an ordered collection such as a TreeMap, then the TreeMap will be serializable only if the comparator is also serializable. As most comparators have little or no state, making them serializable is generally easy and good defensive programming. ""

Neither class has any state, so implementing Serializable would be trivial.

"
0,Avoid item state reads during Session.logout(). Local item states are discarded during Session.logout(). Currently the CachingHierarchyManager is still registered as a item state listener at that time and will cause numerous ItemStateManager.hasItemState() calls. This is unnecessary and just adds overhead to the logout call. In addition it will also contribute to a potential lock contention on the SharedItemStateManager.
0,"Change all FilteredTermsEnum impls into TermsEnum decorators. Currently, FilteredTermsEnum has two ctors:
* FilteredTermsEnum(IndexReader reader, String field)
* FilteredTermsEnum(TermsEnum tenum)

But most of our concrete implementations (e.g. TermsRangeEnum) use the IndexReader+field ctor

In my opinion we should remove this ctor, and switch over all FilteredTermsEnum implementations to just take a TermsEnum.

Advantages:
* This simplifies FilteredTermsEnum and its subclasses, where they are more decorator-like (perhaps in the future we could compose them)
* Removes silly checks such as if (tenum == null) in every next()
* Allows for consumers to pass in enumerators however they want: e.g. its their responsibility if they want to use MultiFields or not, it shouldnt be buried in FilteredTermsEnum.

I created a quick patch (all core+contrib+solr tests pass), but i think this opens up more possibilities for refactoring improvements that haven't yet been done in the patch: we should explore these too.
"
0,"Create new method optimize(int maxNumSegments) in IndexWriter. Spinning this out from the discussion in LUCENE-847.

I think having a way to ""slightly optimize"" your index would be useful
for many applications.

The current optimize() call is very expensive for large indices
because it always optimizes fully down to 1 segment.  If we add a new
method which instead is allowed to stop optimizing once it has <=
maxNumSegments segments in the index, this would allow applications to
eg optimize down to say <= 10 segments after doing a bunch of updates.
This should be a nice compromise of gaining good speedups of searching
while not spending the full (and typically very high) cost of
optimizing down to a single segment.

Since LUCENE-847 is now formalizing an API for decoupling merge policy
from IndexWriter, if we want to add this new optimize method we need
to take it into account in LUCENE-847.
"
0,"Remove jdk 1.4 restriction for jcr-tests. This restriction only exist because these tests form the TCK for JSR-283 which needed to support JDK 1.4. If maintenance on the JSR-283 TCK is needed, it can happen in a previous branch (2.3?).

"
0,Create HTML excerpt provider. Jackrabbit should have a built in HTML excerpt because web applications usually prefer HTML rather than XML.
0,Missing dependencies in two generated maven pom files . There are some missing dependencies in generated maven pom.xml files (benchmark and highlighter)
0,"Make version recovery extensible. Currently JCR-2551 (Recovery from a lost version history) is implemented within the protected method RepositoryImpl$WorkspaceInfo.doInitialize(). However, it is implemented in a way that can't be re-used in a subclass (RepositoryChecker is not a public class).

I will create a new method doVersionRecovery() that contains the code for JCR-2551."
0,FieldCache should support longs and doubles. Would be nice if FieldCache supported longs and doubles
0,"switch MultiTermQuery to ""constant score auto"" rewrite by default. Right now it defaults to scoring BooleanQuery, and that's inconsistent w/ QueryParser which does constant score auto.

The new multi-term queries already set this default, so the only core queries this will impact are PrefixQuery and WildcardQuery.  FuzzyQuery, which has its own rewrite to BooleanQuery, will keep doing so."
0,"Add support for Ideographic Space to the queryparser - also know as fullwith space and wide-space. The Ideographic Space is a space character that is as wide as a normal CJK character cell.
It is also known as wide-space or fullwith space.This type of space is used in CJK languages.

This patch adds support for the wide space, making the queryparser component more friendly
to queries that contain CJK text.

Reference:
'http://en.wikipedia.org/wiki/Space_(punctuation)' - see Table of spaces, char U+3000.

I also added a new testcase that fails before the patch.
After the patch is applied all junits pass."
0,"Scorer.explain is deprecated but abstract, should have impl that throws UnsupportedOperationException. Suggest having Scorer implement explain to throw UnsupportedOperationException

right now, i have to implement this method (because its abstract), and javac yells at me for overriding a deprecated method

if the following implementation is in Scorer, i can remove my ""empty"" implementations of explain from my Scorers
{code}
  /** Returns an explanation of the score for a document.
   * <br>When this method is used, the {@link #next()}, {@link #skipTo(int)} and
   * {@link #score(HitCollector)} methods should not be used.
   * @param doc The document number for the explanation.
   *
   * @deprecated Please use {@link IndexSearcher#explain}
   * or {@link Weight#explain} instead.
   */
  public Explanation explain(int doc) throws IOException {
    throw new UnsupportedOperationException();
  }
{code}

best i figure, this shouldn't break back compat (people already have to recompile anyway) (2.9 definitely not binary compatible with 2.4)
"
0,"GData Server - TestCase Deadlock  StorageModifier. Solfed the racecondition deadlock while closing the StorageController.
This occured the first time hossman tried to run the test cases. 

Concurrent Modification Exception while iteration over a collection in a sepereate thread -- ModifiedEntryFilter replaced list with array.

@Hossman if you can get to it, could you try the testcases again.

@all If you guys do have time you could run the testcases on different environment, that would help to resolve bugs in the test cases and the server.

simon"
0,"TCK: PropertyDefTest and NodeDefTest do not respect the value of the testroot configuration property. In PropertyDefTest and NodeDefTest, the test setup doesn't respect the value of the testroot configuration property.

Proposal: use the testroot configuration property.

Patch for PropertyDefTest:
--- nodetype/PropertyDefTest.java       (revision 422074)
+++ nodetype/PropertyDefTest.java       (working copy)
@@ -94,7 +94,7 @@
  
         session = helper.getReadOnlySession();
         manager = session.getWorkspace().getNodeTypeManager();
-        rootNode = session.getRootNode();
+        rootNode = testRootNode;
     }

Patch for NodeDefTest:
--- nodetype/NodeDefTest.java   (revision 422074)
+++ nodetype/NodeDefTest.java   (working copy)
@@ -68,7 +68,7 @@
  
         session = helper.getReadOnlySession();
         manager = session.getWorkspace().getNodeTypeManager();
-        rootNode = session.getRootNode();
+        rootNode = testRootNode;
     }
"
0,[PATCH] Comment corrections in MMapDirectory.java. These comments ended up on the wrong lines after the last changes
0,"MultithreadedConnectionManager and IdleConnectionTimeoutThread improvements. Changes to MultithreadedConnectionManager and IdleConnectionTimeoutThread following the suggestions of Balazs SZCS.


-------- Forwarded Message --------
From: SZCS Balazs <Balazs.Szuecs@wave-solutions.com>
Reply-To: HttpClient User Discussion
<httpclient-user@jakarta.apache.org>
To: 'HttpClient User Discussion' <httpclient-user@jakarta.apache.org>
Subject: RE: MultithreadedConnectionManager pooling strategy
Date: Mon, 15 May 2006 15:26:08 +0200

Hello,

I made two changes to the HttpClient code:

1) in the class ConnectionPool in the method getFreeConnection( ... ) I
changed freeConnections.removeFirst() to freeConnections.removeLast(). Now
the container for free connections behaves as a stack rather than a queue.

2) additionally I changed the IdleConnectionTimeoutThread, in the run()
method I added connectionManager.deleteClosedConnections() so that the pool
size is maintained correctly.

What do you think?
Balazs"
0,"Add ShingleFilter option to output unigrams if no shingles can be generated. Currently if ShingleFilter.outputUnigrams==false and the underlying token stream is only one token long, then ShingleFilter.next() won't return any tokens. This patch provides a new option, outputUnigramIfNoNgrams; if this option is set and the underlying stream is only one token long, then ShingleFilter will return that token, regardless of the setting of outputUnigrams.

My use case here is speeding up phrase queries. The technique is as follows:

First, doing index-time analysis using ShingleFilter (using outputUnigrams==true), thereby expanding things as follows:

""please divide this sentence into shingles"" ->
 ""please"", ""please divide""
 ""divide"", ""divide this""
 ""this"", ""this sentence""
 ""sentence"", ""sentence into""
 ""into"", ""into shingles""
 ""shingles""

Second, do query-time analysis using ShingleFilter (using outputUnigrams==false and outputUnigramIfNoNgrams==true). If the user enters a phrase query, it will get tokenized in the following manner:

""please divide this sentence into shingles"" ->
 ""please divide""
 ""divide this""
 ""this sentence""
 ""sentence into""
 ""into shingles""

By doing phrase queries with bigrams like this, I can gain a very considerable speedup. Without the outputUnigramIfNoNgrams option, then a single word query would tokenize like this:

""please"" ->
   [no tokens]

But thanks to outputUnigramIfNoNgrams, single words will now tokenize like this:

""please"" ->
  ""please""

****

The patch also adds a little to the pre-outputUnigramIfNoNgrams option tests.

****

I'm not sure if the patch in this state is useful to anyone else, but I thought I should throw it up here and try to find out.
"
0,"unit tests should use private directories. This only affects our unit tests...

I run ""ant test"" and ""ant test-tag"" concurrently, but some tests have false failures (eg TestPayloads) because they use a fixed test directory in the filesystem for testing.

I've added a simple method to _TestUtil to get a temp dir, and switched over those tests that I've hit false failures on."
0,"Buffered output to socket. --Posted by Slavik Markovich:

Hi all,

This is probably a known issue (but I haven't found the answer for it yet).
I'm using httpclient to post data to a remote server but as far as I can see
(using ethereal) the client is writing every line to the wire without buffering.
After examining the code, I can see that the HttpConnection class is using the
output stream received from the socket directly.
Is there a reason for the direct writing?
This is a problem for me 'cause the remote server sets a very low timeout and
returns a bad request response after receiving the request line (without any
other header line or request body).

Can I easily add a buffered behavior to the http client?

10x"
0,"OperandEvaluator should be able to handle Nodes as well, not just Rows. OperandEvaluator is used to evaluate Operands values against given Rows, and in an effort to improve the sorting part of SQL2 (JCR-2959), I need it to handle plain Nodes as well.

This is a small change, as the OperandEvaluator already extracts the Node info from the Row, so there is no obvious reason no to expose the Node operations directly."
0,"LineDocSource should assign stable IDs; docdate field should be NumericField. Some small enhancements when indexing docs from a line doc source:

  * Assign docid by line number instead of by number-of-docs-indexed;
    this makes the resulting ID stable when using multiple threads

  * The docdate field is now indexed as a String (possible created
    through DateTools).  I added two numeric fields: one that indexes
    .getTime() (= long msec) and another that indexes seconds since
    the day started.  This gives us two numeric fields to play
    with...
"
0,Update Lucene to 3.0. Lucene 3.0 was released on 2009/11/25. They migrated to Java 1.5 as Jackrabbit is doing with 2.0. Also they added some new optimizations. It would be nice if Jackrabbit could switch to the new lucene version too.
0,Cookie.java: 'bookean' typo.  
0,Include all CHANGES.txt under contrib. We already include to root CHANGES.txt but fail to include the ones under contrib.
0,ValueSourceQuery hits synchronization bottleneck in IndexReader.isDeleted. I plan to fix it the same way we did in LUCENE-1316 for MatchAllDocsQuery (use TermDocs(null)).
0,HttpClient 'ParamBeans' for easier configuration. As I did for a 'core' here I would like to contribute for 'client' part as few 'ParamBeans' for easier external configuration... Any comment or improvement is very welcome...
0,replace Vector with ArrayList in Queries. Replace Vector with ArrayList in Queries.  This can make a difference in heavily concurrent scenarios when Query objects are examined or compared (e.g. used as cache keys).
0,"jcr-server: make auth-header configurable for JCR-Server. In WEB-INF/web.xml, there is a section (commented-out by default) that reads:
        <init-param>
            <param-name>authenticate-header</param-name>
            <param-value>Basic realm=""Jackrabbit Webdav Server""</param-value>
            <description>
                Defines the value of the 'WWW-Authenticate' header.
            </description>
        </init-param>

This parameter is ignored (not loaded) by the code - the default string is always used instead
Note: this was more of a problem before JCR-286 had been fixed
"
0,"client cache currently allows incomplete responses to be passed on to the client. Per the HTTP/1.1 spec:

""A cache that receives an incomplete response (for example, with fewer bytes of data than specified in a Content-Length header) MAY store the response. However, the cache MUST treat this as a partial response. Partial responses MAY be combined as described in section 13.5.4; the result might be a full response or might still be partial. A cache MUST NOT return a partial response to a client without explicitly marking it as such, using the 206 (Partial Content) status code. A cache MUST NOT return a partial response using a status code of 200 (OK).""

(http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.8)

For example, if a 200 response shows up with 128 bytes in the body but a Content-Length header of 256, the cache MUST NOT pass this through unchanged.
"
0,"bad java practices which affect performance (result of code inspection). IntelliJ IDEA found the following issues in the Lucense source code and tests:

1) explicit for loops where calls to System.arraycopy() should have been
2) calls to Boolean constructor (in stead of the appropriate static method/field)
3) instantiation of unnecessary Integer instances for toString, instead of calling the static one
4) String concatenation using + inside a call to StringBuffer.append(), in stead of chaining the append calls

all minor issues. patch is forthcoming.
"
0,"Allow CFS be empty. since we changed CFS semantics slightly closing a CFS directory on an error can lead to an exception. Yet, an empty CFS is still a valid CFS so for consistency we should allow CFS to be empty.
here is an example:

{noformat}
1 tests failed.
REGRESSION:  org.apache.lucene.index.TestIndexWriterOnDiskFull.testAddDocumentOnDiskFull

Error Message:
CFS has no entries

Stack Trace:
java.lang.IllegalStateException: CFS has no entries
       at org.apache.lucene.store.CompoundFileWriter.close(CompoundFileWriter.java:139)
       at org.apache.lucene.store.CompoundFileDirectory.close(CompoundFileDirectory.java:181)
       at org.apache.lucene.store.DefaultCompoundFileDirectory.close(DefaultCompoundFileDirectory.java:58)
       at org.apache.lucene.index.SegmentMerger.createCompoundFile(SegmentMerger.java:139)
       at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4252)
       at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3863)
       at org.apache.lucene.index.SerialMergeScheduler.merge(SerialMergeScheduler.java:37)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:2715)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:2710)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:2706)
       at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3513)
       at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:2064)
       at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:2031)
       at org.apache.lucene.index.TestIndexWriterOnDiskFull.addDoc(TestIndexWriterOnDiskFull.java:539)
       at org.apache.lucene.index.TestIndexWriterOnDiskFull.testAddDocumentOnDiskFull(TestIndexWriterOnDiskFull.java:74)
       at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1277)
       at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1195)
{noformat}"
0,"Drop the Dumpable interface. I belive the o.a.j.core.util.Dumpable interface was originally used for diagnostic purposes, but AFAIUI we don't use it anywhere anymore. I'd like to drop the interface and refactor the dump() methods in various Jackrabbit classes to more detailed toString() methods that would be more useful to debuggers and other general-purpose diagnostic tools."
0,"Choose a specific Directory implementation running the CheckIndex main. It should be possible to choose a specific Directory implementation to use during the CheckIndex process when we run it from its main.
What about an additional main parameter?
In fact, I'm experiencing some problems with MMapDirectory working with a big segment, and after some failed attempts playing with maxChunkSize, I decided to switch to another FSDirectory implementation but I needed to do that on my own main.
Should we also consider to use a FileSwitchDirectory?
I'm willing to contribute, could you please let me know your thoughts about it?"
0,JSR 283 NodeType Management. 
0,"GData Server MileStone 1 Revision. Some Improvements to the GData Server.
CRUD actions for Entries implemented / tested 
StorageComponent storing entries / feeds / users
Dynamic Feed elements like links added.
Decoupled all server components (storage / ReqeustHandler etc) using lookup service

Added some JavaDoc "
0,"[PATCH] better exception messages when generating schema. When a statement fails to execute generating the schema, patch outputs the statement that failed."
0,spi2davex NodeInfoImpl should use HashSet instead of ArrayList for childInfos. The subsequent contains call is prohibitively expensive since it returns in an equals call for all existing child infos. 
0,"Include OCM in the main Jackrabbit build when using Java 5. Currently the OCM component are separate from the rest of Jackrabbit build due to the fact that they need Java 5 to compile. I'd like to add a java5 profile to the main Jackrabbit build that contains the OCM components and is automatically activated when building with Java 5 or higher.

This would simplify build instructions and allow us to remove the extra Jackrabbit-ocm Hudson build that we currently use to build the OCM component."
0,"Quick Start guide for HttpClient 4.0. cannot determine the required jar files to compile samples/ own files

example org.apache.http.examples.client.ClientFormLogin

used

set CLASSPATH=P:\j\samples\httpClient\examples;j:\j\j5\h;P:\j\e\mysql.jar;P:\j\e\commons-logging-api-1.1.1.jar;P:\j\e\commons-logging-1.1.1.jar;P:\j\e\4\httpmime-4.0-beta2.jar;P:\j\e\4\httpclient-4.0-beta2.jar;P:\j\e\4\h\e\4\httpcore-4.0-beta3.jar;P:\j\e\4\httpcore-nio-4.0-beta3.jar

but still needs org.apache.http.*; ... where are these jars? a quick start guide / link to all jars for one version would help

- only known workaround - go back to old version 3.1"
0,"No need for NodeReferences in jcr2spi. I happened to come across the org.apache.jackrabbit.jcr2spi.state.NodeReferences interface, and realized that with the current SPI definitions there's really no need for that abstraction. The PropertyId array returned by NodeInfo.getReferences() is quite good enough for jcr2spi without any NodeReferences wrapping around it."
0,"FastVectorHighlighter: add a FragmentBuilder to return entire field contents. In Highlightrer, there is a Nullfragmenter. There is a requirement its counterpart in FastVectorhighlighter."
0,"remove RoutedRequest from API. Remove RoutedRequest from the Client API. It can be moved to impl, or dropped altogether.
HttpClient could accept separate request and target arguments instead of RoutedRequest.
No routes should be passed in the API. "
0,"TestIndexWriter failes for SimpleTextCodec. I just ran into this failure since SimpleText obviously takes a lot of disk space though.

{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestIndexWriter
    [junit] Testcase: testCommitOnCloseDiskUsage(org.apache.lucene.index.TestIndexWriter):	FAILED
    [junit] writer used too much space while adding documents: mid=608162 start=5293 end=634214
    [junit] junit.framework.AssertionFailedError: writer used too much space while adding documents: mid=608162 start=5293 end=634214
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)
    [junit] 	at org.apache.lucene.index.TestIndexWriter.testCommitOnCloseDiskUsage(TestIndexWriter.java:1047)
    [junit] 
    [junit] 
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 3.281 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testCommitOnCloseDiskUsage -Dtests.seed=-7526585723238322940:-1609544650150801239
    [junit] NOTE: test params are: codec=SimpleText, locale=th_TH, timezone=UCT
    [junit] ------------- ---------------- ---------------
    [junit] Test org.apache.lucene.index.TestIndexWriter FAILED
{noformat}

I did not look into SimpleText but I guess we need either change the threshold for this test or exclude SimpleText from it.

any ideas?"
0,"TCK: PredicatesTest does not respect testroot configuration property. Test does not respect testroot configuration property.

Proposal: use testroot configuration property in constructing the test queries.

--- PredicatesTest.java (revision 422074)
+++ PredicatesTest.java (working copy)
@@ -78,7 +78,7 @@
      * @throws RepositoryException
      */
     public void testEquality() throws RepositoryException {
-        String stmt = ""/"" + jcrRoot + ""/*[@"" + jcrPrimaryType + ""='"" + nodeTypeName + ""']"";
+        String stmt = ""/"" + jcrRoot + ""/"" + testPath + ""/*[@"" + jcrPrimaryType + ""='"" + nodeTypeName + ""']"";  
         try {
             qm.createQuery(stmt, Query.XPATH);
@@ -93,7 +93,7 @@
      * @throws RepositoryException
      */
     public void testCombinedOr() throws RepositoryException {
-        String stmt = ""/"" + jcrRoot + ""/*[@"" + jcrPrimaryType + ""='"" + nodeTypeName + ""' or @"" + jcrPrimaryType + ""='"" + ntBase + ""']"";
+        String stmt = ""/"" + jcrRoot  + ""/"" + testPath + ""/*[@"" + jcrPrimaryType + ""='"" + nodeTypeName + ""' or @"" + jcrPrimaryType + ""='"" + ntBase + ""']"";
  
         try {
             qm.createQuery(stmt, Query.XPATH);
@@ -108,7 +108,7 @@
      * @throws RepositoryException
      */
     public void testOr() throws RepositoryException {
-        String stmt = ""/"" + jcrRoot + ""/*[@"" + jcrPrimaryType + "" or @"" + jcrMixinTypes + ""]"";
+        String stmt = ""/"" + jcrRoot  + ""/"" + testPath + ""/*[@"" + jcrPrimaryType + "" or @"" + jcrMixinTypes + ""]"";
  
         try {
             qm.createQuery(stmt, Query.XPATH);
@@ -123,7 +123,7 @@
      * @throws RepositoryException
      */
     public void testAnd() throws RepositoryException {
-        String stmt = ""/"" + jcrRoot + ""/*[@"" + jcrPrimaryType + "" and @"" + jcrMixinTypes + ""]"";
+        String stmt = ""/"" + jcrRoot  + ""/"" + testPath + ""/*[@"" + jcrPrimaryType + "" and @"" + jcrMixinTypes + ""]"";
  
         try {
             qm.createQuery(stmt, Query.XPATH);
@@ -138,7 +138,7 @@
      * @throws RepositoryException
      */
     public void testCombinedAnd() throws RepositoryException {
-        String stmt = ""/"" + jcrRoot + ""/*[@"" + jcrPrimaryType + ""='"" + nodeTypeName + ""' and @"" + jcrPrimaryType + ""='"" + ntBase + ""']"";
+        String stmt = ""/"" + jcrRoot  + ""/"" + testPath + ""/*[@"" + jcrPrimaryType + ""='"" + nodeTypeName + ""' and @"" + jcrPrimaryType + ""='"" + ntBase + ""']"";
  
         try {
             qm.createQuery(stmt, Query.XPATH);

"
0,"CMS merge throttling is not aggressive enough. I hit this crab while working on the NRT benchmarker (in luceneutil).

CMS today forcefully idles any incoming threads, when there are too many merges pending.

This is the last line of defense that it has, since it also juggles thread priorities (and forcefully idles the biggest merges) to try to reduce the outstanding merge count.

But when it cannot keep up it has no choice but to stall those threads responsible for making new segments.

However, the logic is in the wrong place now -- the stalling happens after pulling the next merge from IW.  This is poor because it means if you have N indexing threads, you allow max + N outstanding merges.

I have a simple fix, which is to just move the stall logic to before we pull the next merge from IW."
0,"add TCK test for Info map of NODE_MOVED event on node reordering. add the TCK test for this problem, and mark this as known test failure for now"
0,"Incorrect license headers in multiple components. As noticed by Thomas, we have a number of files with missing or incorrect license headers both in trunk and in the 1.5 branch.

The following lists all troublesome files in the 1.5.1 release candidate:

 !????? ./jackrabbit-api/src/main/java/org/apache/jackrabbit/api/security/user/User.java
 !????? ./jackrabbit-core/src/main/java/org/apache/jackrabbit/core/security/authorization/principalbased/ACLEditor.java
 !????? ./jackrabbit-core/src/main/java/org/apache/jackrabbit/core/security/authorization/principalbased/ACLProvider.java
 !????? ./jackrabbit-core/src/test/java/org/apache/jackrabbit/api/jsr283/retention/AbstractRetentionTest.java
 !????? ./jackrabbit-core/src/test/java/org/apache/jackrabbit/core/security/authorization/combined/TestAll.java
 !????? ./jackrabbit-core/src/test/java/org/apache/jackrabbit/core/security/authorization/principalbased/EvaluationTest.java
 !????? ./jackrabbit-core/src/test/java/org/apache/jackrabbit/core/security/authorization/principalbased/TestAll.java
 !????? ./jackrabbit-core/src/test/java/org/apache/jackrabbit/core/security/principal/TestAll.java
 !????? ./jackrabbit-core/src/test/java/org/apache/jackrabbit/core/security/user/AdministratorTest.java
 !????? ./jackrabbit-ocm/src/main/java/org/apache/jackrabbit/ocm/manager/objectconverter/impl/AbstractLazyLoader.java
 !????? ./jackrabbit-ocm/src/main/java/org/apache/jackrabbit/ocm/manager/objectconverter/impl/OcmProxy.java
 !????? ./jackrabbit-ocm/src/main/java/org/apache/jackrabbit/ocm/manager/objectconverter/impl/OcmProxyUtils.java
 !????? ./jackrabbit-ocm/src/test/java/org/apache/jackrabbit/ocm/testmodel/MultiValueWithObjectCollection.java
 !????? ./jackrabbit-ocm/src/test/java/org/apache/jackrabbit/ocm/testmodel/SimpleAnnotedAbstractClass.java
 !????? ./jackrabbit-ocm/src/test/java/org/apache/jackrabbit/ocm/testmodel/SimpleAnnotedClass.java
 !????? ./jackrabbit-ocm/src/test/java/org/apache/jackrabbit/ocm/testmodel/SimpleInterface.java
 !????? ./jackrabbit-ocm/src/test/java/org/apache/jackrabbit/ocm/testmodel/UnmappedInterface.java
 !????? ./jackrabbit-ocm/src/test/java/org/apache/jackrabbit/ocm/testmodel/version/Author.java
 !????? ./jackrabbit-ocm/src/test/java/org/apache/jackrabbit/ocm/testmodel/version/PressRelease.java
"
0,"Deprecate/remove unused FileSystem features such as RandomAccessOutputStream. Currently the FileSystem interface includes a method getRandomAccessOutputStream.
It looks like this method and the class RandomAccessOutputStream is no longer used.
If this is the case, I suggest we remove the method everywhere and deprecate the class."
0,"JavaCC 4.0 fails to generate QueryParser.java. When generating the Java source for QueryParser via the ant task 'javacc-QueryParser' against Subversion trunk (updated Jan. 25, 2006), JavaCC 4.0 gives the following error:

javacc-QueryParser:
   [javacc] Java Compiler Compiler Version 4.0 (Parser Generator)
   [javacc] (type ""javacc"" with no arguments for help)
   [javacc] Reading from file [...]/src/java/org/apache/lucene/queryParser/QueryParser.jj . . .
   [javacc] org.javacc.parser.ParseException: Encountered ""<<"" at line 754, column 3.
   [javacc] Was expecting one of:
   [javacc]     <STRING_LITERAL> ...
   [javacc]     ""<"" ...
   [javacc]     
   [javacc] Detected 1 errors and 0 warnings.

BUILD FAILED
"
0,Replace xerces for serialization by JAXP. The org.apache.jackrabbit.rmi.xml.ImportContentHandler class currently uses Xerces to implement the SAX DocumentHandler and serialize XML into a byte[]. This dependency should be dropped and JAXP be used instead for this functionality.
0,"Pass resultFetchSize/limit hint to SortedLuceneQueryHits. The SortedLuceneQueryHits currently uses a default value of 100 (taken from lucene) for initially retrieved and sorted results. For larger result sets this is not optimal because it will cause re-execution of the underlying query with values 200, 400, 800, 1600, 3200, 6400, etc. Instead the query hits should get the limit that is set on the query or the resultFetchSize configured for the SearchIndex."
0,[patch] remove bogus test. code checks an int to see if it's bigger than MAX_VALUE which is impossible -- removed.
0,Return bind variable names on RepositoryService.checkQueryStatement(). To properly support JSR 283 bind variables the SPI layer needs to return the names of the bind variables. Otherwise the jcr2spi implementation cannot check for unknown names.
0,"Refactor DBMS support for JNDI datasources. Our shop currently uses Oracle for most projects, most commonly in an application server (Tomcat, WebSphere, etc.), and use configured J2EE datasources. Unfortunately, many of the classes that fix quirks on specific DBMS force you to configure a JDBC connection (look at org.apache.jackrabbit.core.fs.db.OracleFileSystem for instance), which is a ""bad idea"" on an application server -- the application server should be managing resources like DB connections, etc.  If you want to use an DbFileSystem based on an Oracle database, you can't use a datasource from a JNDI lookup.  This in effect makes Jackrabbit unusable in clustered enterprise environments.

It would be much better to refactor the current database support to separate the method that an implementation obtains its connection from its functionality."
0,"Introduce similarity function. The query handler should support a similarity function that allows one to find nodes that are similar to a given existing one.

Example:

//*[rep:similar(""/foo/bar"")]

Finds nodes that are similar to node /foo/bar."
0,"Introduce Timer idle time. Currently the Timer stops the internal thread as soon as there are no more tasks scheduled. With a usage pattern that repeatedly schedules tasks and immediately cancels them, one thread per scheduled task is created. There should be some idle time before the background thread is stopped."
0,allow AbstractFileSystemTest.getFileSystem to throw an Exception. 
0,Rename FilterIndexReader to FilterAtomicReader. This class has to be renamed to be consistent with the new naming.
0,Performance improvement in OpenBitSetDISI.inPlaceAnd(). 
0,"all references to incubator need to be replaced with new locations. The following files under 1.0 branch refer to incubator in one way or another.  Some of them may be benign.

./contrib/bdb-persistence/project.properties
./contrib/bdb-persistence/project.xml
./contrib/bdb-persistence/README.txt
./contrib/classloader/project.properties
./contrib/classloader/project.xml
./contrib/examples/project.xml
./contrib/extension-framework/project.properties
./contrib/extension-framework/project.xml
./contrib/jcr-commands/jmeter-chain/project.properties
./contrib/jcr-commands/project.properties
./contrib/jcr-commands/xdocs/navigation.xml
./contrib/jcr-ext/project.xml
./contrib/jcrtaglib/project.properties
./contrib/orm-persistence/project.properties
./contrib/orm-persistence/project.xml
./jackrabbit/applications/test/cnd-reader-test-input.cnd
./jackrabbit/project.properties
./jackrabbit/project.xml
./jackrabbit/README.txt
./jackrabbit/src/site/fml/faq.fml
./jackrabbit/src/site/xdoc/doc/arch/overview/jcrlevels.xml
./jackrabbit/src/site/xdoc/doc/building.xml
./jackrabbit/src/site/xdoc/doc/config.xml
./jackrabbit/src/site/xdoc/downloads.xml
./jackrabbit/src/site/xdoc/index.xml
./jackrabbit/src/site/xdoc/tasks.xml
./jackrabbit/src/test/java/org/apache/jackrabbit/core/nodetype/compact/CompactNodeTypeDefTest.java
./jca/project.properties
./jca/project.xml
./textfilters/project.properties
./textfilters/project.xml

I'd edit them myself, but I need to sleep... maybe tomorrow if nobody beats me to it.
"
0,"remove deprecated classes from spatial. spatial has not been released, so we can remove the deprecated classes"
0,"build.xml's tar task should use longfile=""gnu"". The default (used now) is the same, but we get all those nasty false warnings filling the screen."
0,"Further steps towards flexible indexing. I attached a very rough checkpoint of my current patch, to get early
feedback.  All tests pass, though back compat tests don't pass due to
changes to package-private APIs plus certain bugs in tests that
happened to work (eg call TermPostions.nextPosition() too many times,
which the new API asserts against).

[Aside: I think, when we commit changes to package-private APIs such
that back-compat tests don't pass, we could go back, make a branch on
the back-compat tag, commit changes to the tests to use the new
package private APIs on that branch, then fix nightly build to use the
tip of that branch?o]

There's still plenty to do before this is committable! This is a
rather large change:

  * Switches to a new more efficient terms dict format.  This still
    uses tii/tis files, but the tii only stores term & long offset
    (not a TermInfo).  At seek points, tis encodes term & freq/prox
    offsets absolutely instead of with deltas delta.  Also, tis/tii
    are structured by field, so we don't have to record field number
    in every term.
.
    On first 1 M docs of Wikipedia, tii file is 36% smaller (0.99 MB
    -> 0.64 MB) and tis file is 9% smaller (75.5 MB -> 68.5 MB).
.
    RAM usage when loading terms dict index is significantly less
    since we only load an array of offsets and an array of String (no
    more TermInfo array).  It should be faster to init too.
.
    This part is basically done.

  * Introduces modular reader codec that strongly decouples terms dict
    from docs/positions readers.  EG there is no more TermInfo used
    when reading the new format.
.
    There's nice symmetry now between reading & writing in the codec
    chain -- the current docs/prox format is captured in:
{code}
FormatPostingsTermsDictWriter/Reader
FormatPostingsDocsWriter/Reader (.frq file) and
FormatPostingsPositionsWriter/Reader (.prx file).
{code}
    This part is basically done.

  * Introduces a new ""flex"" API for iterating through the fields,
    terms, docs and positions:
{code}
FieldProducer -> TermsEnum -> DocsEnum -> PostingsEnum
{code}
    This replaces TermEnum/Docs/Positions.  SegmentReader emulates the
    old API on top of the new API to keep back-compat.
    
Next steps:

  * Plug in new codecs (pulsing, pfor) to exercise the modularity /
    fix any hidden assumptions.

  * Expose new API out of IndexReader, deprecate old API but emulate
    old API on top of new one, switch all core/contrib users to the
    new API.

  * Maybe switch to AttributeSources as the base class for TermsEnum,
    DocsEnum, PostingsEnum -- this would give readers API flexibility
    (not just index-file-format flexibility).  EG if someone wanted
    to store payload at the term-doc level instead of
    term-doc-position level, you could just add a new attribute.

  * Test performance & iterate.
"
0,"ItemInfoBuilder should not include PropertyInfos in ChildInfos. When building a NodeInfo instance using the ItemInfoBuilder, getChildInfos() returns entries for the PopertyInfos. This is wrong: getChildInfos should only include entries for the child nodes. "
0,"Make IndexReader.open() always return MSR to simplify (re-)opens.. As per discussion in mailing list, I'm making DirectoryIndexReader.open() always return MSR, even for single-segment indexes.
While theoretically valid in the past (if you make sure to keep your index constantly optimized) this feature is made practically obsolete by per-segment collection.

The patch somewhat de-hairies (re-)open logic for MSR/SR.
SR no longer needs an ability to pose as toplevel directory-owning IR.
All related logic is moved from DIR to MSR.
DIR becomes almost empty, and copying two or three remaining fields over to MSR/SR, I remove it.
Lots of tests fail, as they rely on SR returned from IR.open(), I fix by introducing SR.getOnlySegmentReader static package-private method.
Some previous bugs are uncovered, one is fixed in LUCENE-1645, another (partially fixed in LUCENE-1648) is fixed in this patch. "
0,"Deprecate SimilarityDelegator and Similarity.lengthNorm. SimilarityDelegator is a back compat trap (see LUCENE-2828).

Apps should just [statically] subclass Sim or DefaultSim; if they really need ""runtime subclassing"" then they can make their own app-level delegator.

Also, Sim.computeNorm subsumes lengthNorm, so we should deprecate lengthNorm in favor of computeNorm."
0,Possible slowdown of indexing/merging on 3.x vs trunk. Opening an issue to pursue the possible slowdown Marc Sturlese uncovered.
0,switch appendingcodec to use appending blocktree. currently it still uses block terms + fixed gap index
0,"lucene benchmark has some unnecessary files. lucene/contrib/benchmark/.rsync-filter is only in the source pack (and in SVN), I was not aware of this file, though it was added long ago in https://issues.apache.org/jira/browse/LUCENE-848?focusedCommentId=12491404&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-12491404
Not a blocker for this RC, just interesting to note.

maybe this is related to LUCENE-3155 too, in that we could consider this one for automatic exclusion (like DS_Store), but we should fix it if its committed in SVN too.
"
0,"[patch] better support gcj compilation. There are two methods in IndexReader.java called 'delete'. That is a reserved
keyword in C++ and these methods cause trouble for gcj which implements a clever
workaround in renaming them delete$ but the OS X dynamic linker doesn't pick-up
on it.
The attached patch renames delete(int) to deleteDocument(int) and delete(Term)
to deleteDocuments(Term) and deprecates the delete methods (as requested by Doug
Cutting)."
0,"JCRTest.java (First Steps example code) creates a StringValue with ""new"". The JCRTest.java file described in the First Steps document (http://incubator.apache.org/jackrabbit/firststeps.html) on the jackrabbit incubator website contains a line that attempts to create a StringValue using new, rather than using the ValueFactory interface. This causes the code to fail to compile - perhaps an initiative test, but could be off-putting...

Simple fix is to swap the line:

 n.setProperty(""testprop"", new StringValue(""Hello, World.""));

to 

n.setProperty(""testprop"", session.getValueFactory().createValue(""Hello, World.""));

"
0,"Automaton Query/Filter (scalable regex). Attached is a patch for an AutomatonQuery/Filter (name can change if its not suitable).

Whereas the out-of-box contrib RegexQuery is nice, I have some very large indexes (100M+ unique tokens) where queries are quite slow, 2 minutes, etc. Additionally all of the existing RegexQuery implementations in Lucene are really slow if there is no constant prefix. This implementation does not depend upon constant prefix, and runs the same query in 640ms.

Some use cases I envision:
 1. lexicography/etc on large text corpora
 2. looking for things such as urls where the prefix is not constant (http:// or ftp://)

The Filter uses the BRICS package (http://www.brics.dk/automaton/) to convert regular expressions into a DFA. Then, the filter ""enumerates"" terms in a special way, by using the underlying state machine. Here is my short description from the comments:

     The algorithm here is pretty basic. Enumerate terms but instead of a binary accept/reject do:
      
     1. Look at the portion that is OK (did not enter a reject state in the DFA)
     2. Generate the next possible String and seek to that.

the Query simply wraps the filter with ConstantScoreQuery.

I did not include the automaton.jar inside the patch but it can be downloaded from http://www.brics.dk/automaton/ and is BSD-licensed."
0,"FastVectorHighlighter truncates words at beginning and end of fragments. FastVectorHighlighter does not take word boundaries into consideration when building fragments, so that in most cases the first and last word of a fragment are truncated.  This makes the highlights less legible than they should be.  I will attach a patch to BaseFragmentBuilder that resolves this by expanding the start and end boundaries of the fragment to the first whitespace character on either side of the fragment, or the beginning or end of the source text, whichever comes first.  This significantly improves legibility, at the cost of returning a slightly larger number of characters than specified for the fragment size."
0,FastVectorHighlighter: enable FragListBuilder and FragmentsBuilder to be set per-field override. 
0,"Backport FilteredQuery/IndexSearcher changes to 3.x: Remove filter logic from IndexSearcher and delegate to FilteredQuery. Spinoff from LUCENE-1536: We simplified the code in IndexSearcher to no longer do the filtering there, instead wrap all Query with FilteredQuery, if a non-null filter is given. The conjunction code would then only exist in FilteredQuery which makes it easier to maintain. Currently both implementations differ in 3.x, in trunk we used the more optimized IndexSearcher variant with addition of a simplified in-order conjunction code.

This issue will backport those changes (without random access bits)."
0,"WebdavResponseImpl should cache TransformerFactory. JackrabbitResponeImpl.sendXmlResponse creates an instance of TransformerFactory on each invocation. We see, that this TransformerFactory initialization consumes significant amount of time, because of complex logic inside:

{code}
    at java.lang.String.intern(Native Method)
    at java.util.jar.Attributes$Name.<init>(Attributes.java:449)
    at java.util.jar.Attributes.putValue(Attributes.java:151)
    at java.util.jar.Attributes.read(Attributes.java:404)
    at java.util.jar.Manifest.read(Manifest.java:234)
    at sun.security.util.SignatureFileVerifier.processImpl(SignatureFileVerifier.java:188)
    at sun.security.util.SignatureFileVerifier.process(SignatureFileVerifier.java:176)
    at java.util.jar.JarVerifier.processEntry(JarVerifier.java:277)
    at java.util.jar.JarVerifier.update(JarVerifier.java:188)
    at java.util.jar.JarFile.initializeVerifier(JarFile.java:321)
    at java.util.jar.JarFile.getInputStream(JarFile.java:386)
    at sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:144)
    at java.net.URL.openStream(URL.java:1009)
    at java.lang.ClassLoader.getResourceAsStream(ClassLoader.java:1170)
    at javax.xml.transform.SecuritySupport$4.run(SecuritySupport.java:94)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.xml.transform.SecuritySupport.getResourceAsStream(SecuritySupport.java:87)
    at javax.xml.transform.FactoryFinder.findJarServiceProvider(FactoryFinder.java:250)
    at javax.xml.transform.FactoryFinder.find(FactoryFinder.java:223)
    at javax.xml.transform.TransformerFactory.newInstance(TransformerFactory.java:102)
    at org.apache.jackrabbit.webdav.WebdavResponseImpl.sendXmlResponse(WebdavResponseImpl.java:163)
{code}

TransformerFactory can be cached in static field:

private static final TransofmerFactory transformerFactory = TransformerFactory.newInstance()."
0,"Patch for ShingleFilter.enablePositions (or PositionFilter). Make it possible for *all* words and shingles to be placed at the same position, that is for _all_ shingles (and unigrams if included) to be treated as synonyms of each other.

Today the shingles generated are synonyms only to the first term in the shingle.
For example the query ""abcd efgh ijkl"" results in:
   (""abcd"" ""abcd efgh"" ""abcd efgh ijkl"") (""efgh"" efgh ijkl"") (""ijkl"")

where ""abcd efgh"" and ""abcd efgh ijkl"" are synonyms of ""abcd"", and ""efgh ijkl"" is a synonym of ""efgh"".

There exists no way today to alter which token a particular shingle is a synonym for.
This patch takes the first step in making it possible to make all shingles (and unigrams if included) synonyms of each other.

See http://comments.gmane.org/gmane.comp.jakarta.lucene.user/34746 for mailing list thread."
0,"Use hash codes instead of sequence numbers for string indexes. We use index numbers instead of namespace URIs or other strings in many places. The two-way mapping between namespace URIs and index numbers is by default stored in the repository-global ns_idx.properties file, and the index numbers are allocated using a linear sequence. The problem with this approach is that two repositories will easily end up with different string index mappings, which makes it practically impossible to make low-level copies of workspace content across repositories.

The ultimate solution for this problem would be to store the namespace URIs closer to the stored content, ideally as an implementation detail of a persistence manager.

An easier short-term solution would be to decrease the chances of two repositories having different string index mappings. A simple (and backwards-compatible) way to do this is to use the hash code of a namespace URI as the basis of allocating a new index number. Hash collisions are fairly unlikely, and can be handled by incrementing the intial hash code until the collision is avoided. In the common case of no collisions (with a uniform hash function the chance of a collision is less than 1% even with tousands of registered namespaces) this solution allows workspaces to be copied between repositories without worrying about the namespace index mappings."
0,"use isBinary cached variable instead of instanceof in Field. Field class can hold three types of values, 
See: AbstractField.java  protected Object fieldsData = null; 

currently, mainly RTTI (instanceof) is used to determine the type of the value stored in particular instance of the Field, but for binary value we have mixed RTTI and cached variable ""boolean isBinary"" 

This patch makes consistent use of cached variable isBinary.

Benefit: consistent usage of method to determine run-time type for binary case  (reduces chance to get out of sync on cached variable). It should be slightly faster as well.

Thinking aloud: 
Would it not make sense to maintain type with some integer/byte""poor man's enum"" (Interface with a couple of constants)
code:java{
public static final interface Type{
public static final byte BOOLEAN = 0;
public static final byte STRING = 1;
public static final byte READER = 2;
....
}
}

and use that instead of isBinary + instanceof? "
0,"Implement anonymous login with credentials. Jackrabbit currently implements anonymous login by detecting a null credentials argument on login. This is actually not compliant to the specification.

<spec>
If credentials is null, it is assumed that authentication is handled by a mechanism external to the repository itself (for example, through the JAAS framework) and that the repository implementation exists within a context (for example, an application server) that allows it to handle authorization of the request for access to the specified workspace.
</spec>

Jackrabbit should rather support anonymous login with a defined credential, either some subclass of SimpleCredentials or a predefined / known userId that has read-only access."
0,"Add a waitForMerges() method to IndexWriter. It would be very useful to have a waitForMerges() method on the IndexWriter.

Right now, the only way i can see to achieve this is to call IndexWriter.close()

ideally, there would be a method on the IndexWriter to wait for merges without actually closing the index.
This would make it so that background merges (or optimize) can be waited for without closing the IndexWriter, and then reopening a new IndexWriter

the close() reopen IndexWriter method can be problematic if the close() fails as the write lock won't be released
this could then result in the following sequence:
* close() - fails
* force unlock the write lock (per close() documentation)
* new IndexWriter() (acquires write lock)
* finalize() on old IndexWriter releases the write lock
* Index is now not locked, and another IndexWriter pointing to the same directory could be opened

If you don't force unlock the write lock, opening a new IndexWriter will fail until garbage collection calls finalize() the old IndexWriter

If the waitForMerges() method is available, i would likely never need to close() the IndexWriter until right before the process being shutdown, so this issue would not occur (worst case scenario, the waitForMerges() fails)


"
0,"optimize spanfirstquery, spanpositionrangequery. SpanFirstQuery and SpanPositionRangeQuery (SpanFirst is just a special case of this), are currently inefficient.

Take this worst case example: SpanFirstQuery(""the"").
Currently the code reads all the positions for the term ""the"".

But when enumerating spans, once we have passed the allowable range we should move on to the next document (skipTo)
 "
0,"WindowsDirectory. We can use Windows' overlapped IO to do pread() and avoid the performance problems of SimpleFS/NIOFSDir.
"
0,"Optimize bundle serialization. There are a number of ways we could use to make bundle serialization more optimized. Thomas has already done some work on this in the Jackrabbit 3 sandbox, and I'd like to apply some of the optimizations also to the trunk."
0,"GetReferencesNodeTest test assumptions. Bad test assumptions in GetReferencesNodeTest:

1) In setUp(): there is a primary node type including mixin:versionable. Proposed fix: just create the node, try to add mixin:versionable, check the node type after save.

2) The repository supports non-protected reference properties. Proposed fix: check with AbstractJCRTest's ensureCanSetProperty method, and let NotExecutableException be thrown.
"
0,"JackrabbitParser and tika 0.7 parser. Hi,

I was trying to implement a custom parser and found the following problem.
Since tika 0.7 it is possible to implement your custom parser and specify it into a service provider configuration file (META-INF/services/org.apache.tika.parser.Parser). In this way there would be no need to maintain a custom tika-config.xml file if you'd like to implement a custom parser.

The problem that I had was in the JackrabbitParser because I wasn't able to instantiate the AutoDetectParser with the default constructor is will be instantiated using the default TikaConfig constructor.
Basically from tika 0.7, the TikaConfig.getTikaConfig() is instantiating the TikaConfig using the default constructor instead of accessing the tika-config.xml file from withing the package, and reads the service provider configuration files and populate the parsers map.

What I'm proposing is to change the JackrabbitParser to instantiate the AutoDetectParser using the default constructor, in this way the using tika version >= 0.7 we could easily implement our own parsers and there won't be a reason to maintain the tika-config.xml, also a sort of ""backward"" compatibility would be maintained because using the AutoDetectParser default constructor the TikaConfig is instantiated using TikaConfig.getTikaConfig() wich for tika versions < 0.7 calls the TikaConfig(InputStream) constructor whcih reads the configuration directly from the package.

Basically the JackrabbitParser should look like this:

    public JackrabbitParser() {
            	parser = new AutoDetectParser();
    }
 
Thanks,
Dan"
0,"Use Iterable<? extends UrlEncodedFormEntity> instead of List<? extends UrlEncodedFormEntity> in URLEncodedUtils.format and UrlEncodedFormEntity. UrlEncodedFormEntity requires a List<? extends UrlEncodedFormEntity> to pass it to URLEncodedUtils.format. It would be nice to use Iterable<? extends UrlEncodedFormEntity> to be able to use other collections, e.g. a Set<? extends UrlEncodedFormEntity>"
0,EventFilterImpl should implement toString. This would simplify logging and debugging.
0,"FileDataStore Garbage Collector and empty directories. When the org.apache.jackrabbit.core.data.GarbageCollector is called for a FileDataStore the file objects are correctly deleted.
But the (sub)directories aren't removed.
In time this will result in a huge tree of unused empty directories

I've created a small chance in method FileDataStore.deleteOlderRecursive()
It will remove a directory when it hasn't any files entries. Please note that currently the file objects are stored three levels deep, so it
will take three gc calls remove all directories. Which I think is no problem because the currently implementation is lightweighted.

>>>>> CURRENT FileDataStore.java

    private int deleteOlderRecursive(File file, long min) {
        int count = 0;
        if (file.isFile() && file.exists() && file.canWrite()) {
            if (file.lastModified() < min) {
                DataIdentifier id = new DataIdentifier(file.getName());
                if (!inUse.containsKey(id)) {
                    file.delete();
                    count++;
                }
            }
        } else if (file.isDirectory()) {
            File[] list = file.listFiles();
            for (int i = 0; i < list.length; i++) {
                count += deleteOlderRecursive(list[i], min);
            }
        }
        return count;
    }

>>>>>>> NEW

    private int deleteOlderRecursive(File file, long min) {
        int count = 0;
        if (file.isFile() && file.exists() && file.canWrite()) {
            if (file.lastModified() < min) {
                DataIdentifier id = new DataIdentifier(file.getName());
                if (!inUse.containsKey(id)) {
                    file.delete();
                    count++;
                }
            }
        } else if (file.isDirectory()) {
            File[] list = file.listFiles();
            if (list.length==0) {
              file.delete();
            } else {
              for (int i = 0; i < list.length; i++) {
                count += deleteOlderRecursive(list[i], min);
              }
            }
        }
        return count;
    }
"
0,"Class DisjunctionSumScorer does not need to be public.. See title, patch follows."
0,"Similarity.lengthNorm and positionIncrement=0. Calculation of lengthNorm factor should in some cases take into account the number of tokens with positionIncrement=0. This should be made optional, to support two different scenarios:

* when analyzers insert artificially constructed tokens into TokenStream (e.g. ASCII-fied versions of accented terms, stemmed terms), and it's unlikely that users submit queries containing both versions of tokens: in this case lengthNorm calculation should ignore the tokens with positionIncrement=0.

* when analyzers insert synonyms, and it's likely that users may submit queries that contain multiple synonymous terms: in this case the lengthNorm should be calculated as it is now, i.e. it should take into account all terms no matter what is their positionIncrement.

The default should be backward-compatible, i.e. it should count all tokens.

(See also the discussion here: http://markmail.org/message/vfvmzrzhr6pya22h )"
0,"Implement caching mechanism for ItemInfo batches. Currently all ItemInfos returned by RepositoryService#getItemInfos are placed into the hierarchy right away. For big batch sizes this is prohibitively expensive. The overhead is so great (*), that it quickly outweighs the overhead of network round trips. Moreover, SPI implementations usually choose the batch in a way determined by the backing persistence store and not by the requirements of the consuming application on the JCR side. That is, many of the items in the batch might never be actually needed. 

I suggest to implement a cache for ItemInfo batches. Conceptually such a cache would live inside jcr2spi right above the SPI API. The actual implementation would be provided by SPI implementations. This approach allows for fine tuning cache/batch sizes to a given persistence store and network environment. This would also better separate different concerns: the purpose of the existing item cache is to optimize for the requirement of the consumer of the JCR API ('the application'). The new ItemInfo cache is to optimize for the specific network environment and backing persistence store. 

(*) Numbers follow "
0,"Handle Returning Null consistantly. Consider returning empty arrays instead of null consistantly.  eg:
getResponseBody().  There may be good reason for both null and empty array
depending on the circumstannces."
0,"Introduce 'SecurityConfig' for better extensability.. the current repository configuration parser parses the security confguration (inluding appName, AccessManagerConfig and LoginModuleconfig) internally and the passes those 3 values to the repository config. i suggest to add a new 'SecurityConfig' object that encapsulates those 3 values and is parsed in a seperate method, in order to allow for better extensability. this also reduces the size of the alredy bloated repository config constructor."
0,"Small performance enhancement for StandardAnalyzer. The class StandardAnalyzer has an inner class, SavedStreams, which is used internally for maintaining some state. This class doesn't use the implicit reference to the enclosing class, so it can be made static and reduce some memory requirements. A patch will be attached shortly."
0,"Add IndexReader.flush(commitUserData). IndexWriter offers a commit(String commitUserData) method.
IndexReader can commit as well using the flush/close methods and so
needs an analogous method that accepts commitUserData."
0,add IndexCommit.getTimestamp method. Convenience method for getDirectory().fileModified(getSegmentsFileName()).
0,"improve BaseTokenStreamTestCase random string generation. Most analysis tests use mocktokenizer (which splits on whitespace), but
its rare that we generate a string with 'many tokens'. So I think we should
try to generate more realistic test strings."
0,"TestCachingSpanFilter sometimes fails. if I run 
{noformat} 
ant test -Dtestcase=TestCachingSpanFilter -Dtestmethod=testEnforceDeletions -Dtests.seed=5015158121350221714:-3342860915127740146 -Dtests.iter=100
{noformat} 

I get two failures on my machine against current trunk
{noformat} 

junit-sequential:
    [junit] Testsuite: org.apache.lucene.search.TestCachingSpanFilter
    [junit] Testcase: testEnforceDeletions(org.apache.lucene.search.TestCachingSpanFilter):	FAILED
    [junit] expected:<2> but was:<3>
    [junit] junit.framework.AssertionFailedError: expected:<2> but was:<3>
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)
    [junit] 	at org.apache.lucene.search.TestCachingSpanFilter.testEnforceDeletions(TestCachingSpanFilter.java:101)
    [junit] 
    [junit] 
    [junit] Testcase: testEnforceDeletions(org.apache.lucene.search.TestCachingSpanFilter):	FAILED
    [junit] expected:<2> but was:<3>
    [junit] junit.framework.AssertionFailedError: expected:<2> but was:<3>
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)
    [junit] 	at org.apache.lucene.search.TestCachingSpanFilter.testEnforceDeletions(TestCachingSpanFilter.java:101)
    [junit] 
    [junit] 
    [junit] Tests run: 100, Failures: 2, Errors: 0, Time elapsed: 2.297 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestCachingSpanFilter -Dtestmethod=testEnforceDeletions -Dtests.seed=5015158121350221714:-3342860915127740146
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestCachingSpanFilter -Dtestmethod=testEnforceDeletions -Dtests.seed=5015158121350221714:-3342860915127740146
    [junit] NOTE: test params are: codec=MockVariableIntBlock(baseBlockSize=43), locale=fr, timezone=Africa/Bangui
    [junit] ------------- ---------------- ---------------
    [junit] Test org.apache.lucene.search.TestCachingSpanFilter FAILED
{noformat}

not sure what it is but it seems likely to be a WeakRef / GC issue in the cache. "
0,"Default retry count three even if documentation says it's five. The exception handling documentation (http://hc.apache.org/httpclient-3.x/exception-handling.html) says ""HttpClient will automatically retry up to 5 times those methods..."", but in DefaultHttpMethodRetryHandler  e.g. in trunk (http://svn.apache.org/viewvc/httpcomponents/oac.hc3x/trunk/src/java/org/apache/commons/httpclient/DefaultHttpMethodRetryHandler.java?revision=608014&view=markup) you can see that the retry count is three:

    public DefaultHttpMethodRetryHandler(int retryCount, boolean requestSentRetryEnabled) {
        super();
        this.retryCount = retryCount;
        this.requestSentRetryEnabled = requestSentRetryEnabled;
    }
    
    /**
     * Creates a new DefaultHttpMethodRetryHandler that retries up to 3 times
     * but does not retry methods that have successfully sent their requests.
     */
    public DefaultHttpMethodRetryHandler() {
        this(3, false);
    }"
0,"CollationKeyFilter: convert tokens into CollationKeys encoded using IndexableBinaryStringTools. Converts each token into its CollationKey using the provided collator, and then encodes the CollationKey with IndexableBinaryStringTools, to allow it to be stored as an index term.

This will allow for efficient range searches and Sorts over fields that need collation for proper ordering.
"
0,JSR 283: Access Control. container issue for JSR 283 access control functionality
0,"TCK: RestoreTest.testRestoreLabel. According to tobi the jackrabbit implementation of 'Node.restoreByLabel' is an interpretation of the
specification regarding the restore behaviour of versionable child nodes. while that interpetration might
be legal unless the specification is violated, i would argue that the TCK should not test the interpretation.

therefore i suggest to modify

org.apache.jackrabbit.test.api.version.RestoreTest.testRestoreLabel

by skipping line 334 - 345 in order to limit the test case to the behaviour that is defined by the specification.

regards
angela

ps: the mentioned test is also executed within the scope of WorkspaceRestoreTest because the latter  extends RestoreTest.... that's misleading."
0,"Improve architecture of FieldSortedHitQueue. Per the discussion (quite some time ago) on issue LUCENE-806, I'd like to propose an architecture change to the way FieldSortedHitQueue works, and in particular the way it creates SortComparatorSources. I think (I hope) that anyone who looks at the FSHQ code will agree that the class does a lot and much of it's repetitive stuff that really has no business being in that class.

I am about to attach a patch which, in and of itself, doesn't really achieve much that's concrete but does tidy things up a great deal and makes it easier to plug in different behaviours. I then have a subsequent patch which provides a fairly simple and flexible example of how you might replace an implementation, in this case the field-local-String-comparator version from LUCENE-806.

The downside to this patch is that it involved changing the signature of SortComparatorSource.newComparator to take a Locale. There would be ways around this (letting FieldSortedHitQueue take in either a SortComparatorSource or some new, improved interface which takes a Locale (and possibly extends SortComparatorSource). I'm open to this but personally I think that the Locale version makes sense and would suggest that the code would be nicer by breaking the API (and hence targeting this to, presumably, 3.0 at a minimum).

This code does not include specific tests (I will add these, if people like the general idea I'm proposing here) but all current tests pass with this change.

Patch to follow."
0,"Contrib JCR-Server: enable PROPPATCH for simple-davresource. implement as suggested:

- jcr-properties are exposed as webdav properties
- PROPPATCH is forwarded to javax.jcr.Property.setValue() and Item.remove()"
0,"Improve readability of StandardTermsDictWriter. One variable is named indexWriter, but it is a termsIndexWriter. Also some layout."
0,"Documentation bug.  The 2.4.1 query parser syntax wiki page says it is for 1.9. This page:

http://lucene.apache.org/java/2_4_1/queryparsersyntax.html

says this:
.bq
This page provides the Query Parser syntax in Lucene 1.9. If you are using a different version of Lucene, please consult the copy of docs/queryparsersyntax.html that was distributed with the version you are using. 

This is misleading on a doc page for 2.4.1"
0,Move hasVectors() & hasProx() responsibility out of SegmentInfo to FieldInfos . Spin-off from LUCENE-2881 which had this change already but due to some random failures related to this change I remove this part of the patch to make it more isolated and easier to test. 
0,"merge LuceneTestCase and LuceneTestCaseJ4. We added Junit4 support, but as a separate test class.

So unfortunately, we have two separate base classes to maintain: LuceneTestCase and LuceneTestCaseJ4.
This creates a mess and is difficult to manage.

Instead, I propose a single base test class that works both junit3 and junit4 style.

I modified our LuceneTestCaseJ4 in the following way:
* the methods to run are not limited to the ones annotated with @Test, but also any void no-arg methods that start with ""test"", like junit3. this means you dont have to sprinkle @Test everywhere.
* of course, @Ignore works as expected everywhere.
* LuceneTestCaseJ4 extends TestCase so you dont have to import static Assert.* to get all the asserts.

for most tests, no changes are required. but a few very minor things had to be changed:
* setUp() and tearDown() must be public, not protected.
* useless ctors must be removed, such as TestFoo(String name) { super(name); }
* LocalizedTestCase is gone, instead of
{code}
public class TestQueryParser extends LocalizedTestCase {
{code}
it is now
{code}
@RunWith(LuceneTestCase.LocalizedTestCaseRunner.class)
public class TestQueryParser extends LuceneTestCase {
{code}
* Same with MultiCodecTestCase: (LuceneTestCase.MultiCodecTestCaseRunner.class}

I only did the core tests in the patch as a start, and i just made an empty LuceneTestCase extends LuceneTestCaseJ4.

I'd like to do contrib and solr and rename this LuceneTestCaseJ4 to only a single class: LuceneTestCase.
"
0,"Tests not executable for already present mixins. org.apache.jackrabbit.test.api.NodeRemoveMixinTest.testCheckedIn() and 
org.apache.jackrabbit.test.api.NodeAddMixinTest.testCheckedIn() 

fail when the mixin being added is already present on the node. The tests should check for this and trow a NotExecutableException."
0,Replace SegmentReader.Ref with AtomicInteger. I think the patch should be applied to backcompat tag in its entirety.
0,Adding a custom location header extractor method for RedirectStrategy.. Sometimes Web Servers respond to http requests with non-standard location response headers during a server side redirect. (302)  ADding a convenience method to over come this.
0,"divorce defaultsimilarityprovider from defaultsimilarity. In LUCENE-2236 as a start, we made DefaultSimilarity which implements the factory interface (SimilarityProvider), and also extends Similarity.

Its factory interface just returns itself always by default.

Doron mentioned it would be cleaner to split the two, and I thought it would be good to revisit it later.

Today as I was looking at SOLR-2338, it became pretty clear that we should do this, it makes things a lot cleaner. I think currently its confusing to users to see the two apis mixed if they are trying to subclass.
"
0,"NRTCachingDirectory, to buffer small segments in a RAMDir. I created this simply Directory impl, whose goal is reduce IO
contention in a frequent reopen NRT use case.

The idea is, when reopening quickly, but not indexing that much
content, you wind up with many small files created with time, that can
possibly stress the IO system eg if merges, searching are also
fighting for IO.

So, NRTCachingDirectory puts these newly created files into a RAMDir,
and only when they are merged into a too-large segment, does it then
write-through to the real (delegate) directory.

This lets you spend some RAM to reduce I0.
"
0,"nightly builds depend on clover. as reported by Michael Pelz Sherman on java-dev@lucene and solr-user@lucene the nightly builds coming out of hudson current depend on clover...

  [root@crm.test.bbhmedia.net tmp]# strings lucene-core-nightly.jar | grep -i clover|more
org/apache/lucene/LucenePackage$__CLOVER_0_0.class
org/apache/lucene/analysis/Analyzer$__CLOVER_1_0.class
...

the old nightly.sh dealt with this by running ant nightly twice, first without clover to get the jars and then with clover to get the report.  it loks like maybe this logic never made it into the hudson setup.

someone with hudson admin access/knowledge will need to look into this."
0,"PostgreSQL support in clustering module. There is no ddl file for PostgreSQL in clustering module, so I'm attaching here the one we are using in our project. Hope it helps."
0,"RFE: Make Credentials Serializable. I've been working on upgrading the HtmlUnit library to use HttpClient 4, and I've realized that we could eliminate some hackish internal code if Credentials instances were Serializable. I don't really see a downside, and this would be a huge convenience for us.

The change would involve making the org.apache.http.auth.Credentials interface extend Serializable, and having org.apache.http.auth.BasicUserPrincipal and org.apache.http.auth.NTUserPrincipal implement Serializable (plus serialVersionUIDs where appropriate, I guess)."
0,"Journal: Use buffered input / output streams. The journal should use buffered input / output streams wherever possible. Currently there are some places where bytes are directly written to the journal file, which degrades performance."
0,"spellchecker cleanup. Some cleanup, attached here so it can be tracked if necessary: javadoc improvements; don't print exceptions to stderr but re-throw them; new constructor for a new test case. I will commit this soon."
0,InstantiatedIndex supports non-optimized IndexReaders. InstantiatedIndex does not currently support non-optimized IndexReaders.  
0,"improve termquery ""pk lookup"" performance. For things that are like primary keys and don't exist in some segments (worst case is primary/unique key that only exists in 1)
we do wasted seeks.

While LUCENE-2694 tries to solve some of this issue with TermState, I'm concerned we could every backport that to 3.1 for example.

This is a simpler solution here just to solve this one problem in termquery... we could just revert it in trunk when we resolve LUCENE-2694,
but I don't think we should leave things as they are in 3.x
"
0,Performance tuning. 
0,Improved javadocs for PriorityQueue#lessThan. It kills me that I have to inspect the code every time I implement a PriorityQueue. :)
0,Reduce calls to RepositoryService.getRepositoryDescriptors(). Descriptors do not change and should not be requested for each session.
0,Enable DocValues by default for every Codec. Currently DocValues are enable with a wrapper Codec so each codec which needs DocValues must be wrapped by DocValuesCodec. The DocValues writer and reader should be moved to Codec to be enabled by default.
0,"Supporting deleteDocuments in IndexWriter (Code and Performance Results Provided). Today, applications have to open/close an IndexWriter and open/close an
IndexReader directly or indirectly (via IndexModifier) in order to handle a
mix of inserts and deletes. This performs well when inserts and deletes
come in fairly large batches. However, the performance can degrade
dramatically when inserts and deletes are interleaved in small batches.
This is because the ramDirectory is flushed to disk whenever an IndexWriter
is closed, causing a lot of small segments to be created on disk, which
eventually need to be merged.

We would like to propose a small API change to eliminate this problem. We
are aware that this kind change has come up in discusions before. See
http://www.gossamer-threads.com/lists/lucene/java-dev/23049?search_string=indexwriter%20delete;#23049
. The difference this time is that we have implemented the change and
tested its performance, as described below.

API Changes
-----------
We propose adding a ""deleteDocuments(Term term)"" method to IndexWriter.
Using this method, inserts and deletes can be interleaved using the same
IndexWriter.

Note that, with this change it would be very easy to add another method to
IndexWriter for updating documents, allowing applications to avoid a
separate delete and insert to update a document.

Also note that this change can co-exist with the existing APIs for deleting
documents using an IndexReader. But if our proposal is accepted, we think
those APIs should probably be deprecated.

Coding Changes
--------------
Coding changes are localized to IndexWriter. Internally, the new
deleteDocuments() method works by buffering the terms to be deleted.
Deletes are deferred until the ramDirectory is flushed to disk, either
because it becomes full or because the IndexWriter is closed. Using Java
synchronization, care is taken to ensure that an interleaved sequence of
inserts and deletes for the same document are properly serialized.

We have attached a modified version of IndexWriter in Release 1.9.1 with
these changes. Only a few hundred lines of coding changes are needed. All
changes are commented by ""CHANGE"". We have also attached a modified version
of an example from Chapter 2.2 of Lucene in Action.

Performance Results
-------------------
To test the performance our proposed changes, we ran some experiments using
the TREC WT 10G dataset. The experiments were run on a dual 2.4 Ghz Intel
Xeon server running Linux. The disk storage was configured as RAID0 array
with 5 drives. Before indexes were built, the input documents were parsed
to remove the HTML from them (i.e., only the text was indexed). This was
done to minimize the impact of parsing on performance. A simple
WhitespaceAnalyzer was used during index build.

We experimented with three workloads:
  - Insert only. 1.6M documents were inserted and the final
    index size was 2.3GB.
  - Insert/delete (big batches). The same documents were
    inserted, but 25% were deleted. 1000 documents were
    deleted for every 4000 inserted.
  - Insert/delete (small batches). In this case, 5 documents
    were deleted for every 20 inserted.

                                current       current          new
Workload                      IndexWriter  IndexModifier   IndexWriter
-----------------------------------------------------------------------
Insert only                     116 min       119 min        116 min
Insert/delete (big batches)       --          135 min        125 min
Insert/delete (small batches)     --          338 min        134 min

As the experiments show, with the proposed changes, the performance
improved by 60% when inserts and deletes were interleaved in small batches.


Regards,
Ning


Ning Li
Search Technologies
IBM Almaden Research Center
650 Harry Road
San Jose, CA 95120"
0,Speedup of CharArraySet#copy if a CharArraySet instance is passed to copy.. the copy method should use the entries array itself to copy the set internally instead of iterating over all values. this would speedup copying even small set 
0,"Further parallelizaton of ParallelMultiSearcher. When calling {{search(Query, Filter, int)}} on a ParallelMultiSearcher, the {{createWeights}} function of MultiSearcher is called, and sequentially calls {{docFreqs()}} on every sub-searcher. This can take a significant amount of time when there are lots of remote sub-searchers.

"
0,Add join query to Lucene. Solr has (psuedo) join query for a while now. I think this should also be available in Lucene.  
0,move nrtcachingdir to core in 4.0. in 4.0 with the IOContext changes this implementation is clean and I think we should move it to core and use it in our tests etc.
0,Group.addMemeber() might add a REFERENCE instead of a WEAKREFERENCE. ...and this causes that the member can't be removed afterwards.
0,"Database persistence managers: log database and driver name and version. Database related problems can be solved more easily when we know what database and driver version is used. Sometimes multiple database drivers are installed in an app server environment, and the user may not even know it. 

Currently the driver class name is logged. I suggest to log the driver and database name and version as well."
0,"full text search tests use incorrect character for escaping phrases. The query test cases use single quotes to escape phrases. The grammar in 6.6.5.2 however requires double quotes.
"
0,"Default configuration not suitable for demo web application. The default configuration is not suitable for the demo application. There are no text extractors configured, which makes the populate and search demos useless.

Proposed solution: create a new repository.xml in jackrabbit-webapp with text extractors configured.

I know we should actually try to reduce the number of repository.xml files, but having one dedicated to jackrabbit-webapp seems reasonable, while we should try to achieve the same for the jackrabbit-core module."
0,"Add contrib libs to classpath for javadoc. I don't know Ant well enough to just do this easily, so I've labeled a wish - would be nice to get rid of all the errors/warnings that not finding these classes generates when building javadoc."
0,add missing name constants for mix:title. 
0,"thread pool implementation of parallel queries. This component is a replacement for ParallelMultiQuery that runs a thread pool
with queue instead of starting threads for every query execution (so its
performance is better)."
0,"add LuceneTestCase.newSearcher(). Most tests in the search package don't care about what kind of searcher they use.

we should randomly use MultiSearcher or ParallelMultiSearcher sometimes in tests."
0,"[PATCH] Indexing on Hadoop distributed file system. In my current project we needed a way to create very large Lucene indexes on Hadoop distributed file system. When we tried to do it directly on DFS using Nutch FsDirectory class - we immediately found that indexing fails because DfsIndexOutput.seek() method throws UnsupportedOperationException. The reason for this behavior is clear - DFS does not support random updates and so seek() method can't be supported (at least not easily).
 
Well, if we can't support random updates - the question is: do we really need them? Search in the Lucene code revealed 2 places which call IndexOutput.seek() method: one is in TermInfosWriter and another one in CompoundFileWriter. As we weren't planning to use CompoundFileWriter - the only place that concerned us was in TermInfosWriter.
 
TermInfosWriter uses IndexOutput.seek() in its close() method to write total number of terms in the file back into the beginning of the file. It was very simple to change file format a little bit and write number of terms into last 8 bytes of the file instead of writing them into beginning of file. The only other place that should be fixed in order for this to work is in SegmentTermEnum constructor - to read this piece of information at position = file length - 8.
 
With this format hack - we were able to use FsDirectory to write index directly to DFS without any problems. Well - we still don't index directly to DFS for performance reasons, but at least we can build small local indexes and merge them into the main index on DFS without copying big main index back and forth. 

"
0,Improve performance of simple path queries. Queries with simple path constraints can be quite slow because of the way they are implemented. The current implementation basically does a hierarchical join with the context nodes and the set of nodes with the name of the next location step. When the specified path is quite selective the implementation should   rather resolve the path expression using the item state manager (similar to how regular paths are resolved in the JCR API).
0,"There is no way to specify a different auth scheme priority for host and proxy. Using HttpClient 3.0 rc2, you cannot authenticate to a site using Basic
Autentication and to a proxy server using NTLM authentication.

When you indicate a prefference to use NTLM over Basic authentication the
authentication will fail when it tries to authenticate NTML to the Proxy and to
the Site. If you indicate Basic, then NTLM authentication order the Basic
authentication will fail when used for the Proxy (since basic authentication
can't send the domain name it fails).

The email thread from the discussion group is pasted below for refference.

==============================================================
==============================================================

Hi all,
I am trying to authenticate to a server via a proxy which also requires
authentication. It seems that I can get either the proxy authentication to work
OR the site authentication to work, but not both.

Both seem to work independently when I set the credentials (or proxy
credentials) using NTCredentials (e.g. if I connect to the site from a network
not using a proxy I can get it to work, and I can authenticate to the proxy only
to get a 401 authentication failed from the server when using the proxy).

I read in the Authentication tutorial that you can't authenticate using NTLM to
both the proxy and site, so I'm trying various combinations of authentication,
but I can't find any documentation that specifically covers this case and I feel
like I'm just taking stabs in the dark right now.

If anyone can point me in the direction of the light at the end of the tunnel
I'd really appreciate it.

Thanks,
David

----------------

On Wed, Jun 29, 2005 at 09:53:07AM -0700, David Parks wrote:
> Hi all,
> I am trying to authenticate to a server via a proxy which also requires
authentication. It seems that I can get either the proxy authentication to work
OR the site authentication to work, but not both.
> 
> Both seem to work independently when I set the credentials (or proxy
credentials) using NTCredentials (e.g. if I connect to the site from a network
not using a proxy I can get it to work, and I can authenticate to the proxy only
to get a 401 authentication failed from the server when using the proxy).
> 
> I read in the Authentication tutorial that you can't authenticate using NTLM
to both the proxy and site, so I'm trying various combinations of
authentication, but I can't find any documentation that specifically covers this
case and I feel like I'm just taking stabs in the dark right now.

David,

You _really_ can't use NTLM to authenticate with the proxy and the
target host at the same, due to the nature of this authentication
scheme. Really. That was not a joke.

Please consider using one of the following combinations instead:

(1) BASIC proxy + NTLM host if both the clent and the proxy are within a
trusted network segment

(2) NTLM proxy + SSL + BASIC host

Both combinations should provide an adequate (or better in the latter case)
security

Hope this helps

Oleg

> 
> If anyone can point me in the direction of the light at the end of the tunnel
I'd really appreciate it.
> 
> Thanks,
> David
> 
> 

-------------------

Thanks for the reply Oleg. This is what I figured, but I cannot see how to use
different authentication schemes for the Proxy vs. the Site authentication
challenge.

I tried adding the code suggested in the Authentication tutorial:

        List authPrefs = new ArrayList(2);
        authPrefs.add(AuthPolicy.DIGEST);
        authPrefs.add(AuthPolicy.BASIC);
        authPrefs.add(AuthPolicy.NTLM);
         This will exclude the NTLM authentication scheme
        httpclient.getParams().setParameter(AuthPolicy.AUTH_SCHEME_PRIORITY,
authPrefs);

I got a message stating that it was attempting BASIC authentication for the
Proxy and that it failed (probably because the domain doesn't get passed I
guess). So my thought is that I need NTLM for the proxy authentication and Basic
will work for the site authentication.

The question I am then working on is how to direct the HttpClient to select that
order of authentication methods. If I let it take NTLM as the preffered
authentication method then it will try to authenticate both challenges with NTLM.

I sure there is just some little detail I'm missing here somewhere, it's just
hard to find it.

Thanks a lot!
David

------------------

David,

I see the problem. This will require a patch and a new parameter.
Luckily the preference API introduced in HttpClient 3.0 allows up to add
parameters quite easily. Please file a feature request with Bugzilla
ASAP and I'll do my best to hack up a patch before I leave for holidays
(that is Friday, July 1st)

Oleg


--------------

Hi Oleg, thanks, I'll put that request in today.
This helps a lot, at least I know I'm on the right path now.

I am attempting to devise a workaround for this by handling the authentication
manually (setDoAuthentication(false)).

When I see a 401 error I am processing a basic authentication with the site
credentials, when I see a 407 error I want to process an NTLM authentication
with the proxy credentials.

To that end I have the following code that runs after
httpclient.execute(getmethod) executes. The code below works perfectly for the
basic authentication (when the proxy is not in the picture).

In looking up the Handshake of the NTLM authentication I see that I have a
problem with the code below since the handshake includes 2 challenge and
authorization steps before the authentication succeeds. I'm not clear how I
could manually authenticate the NTLM response. I would expect the NTLMScheme
class to contain a Type 1 and Type 3 authenticate() method for processing both
challenge responses. Is there another way of processing the NTLM authentication
after receiving the initial authentication challenge from the server?

        //Check for Proxy or Site authentication
        if(getmethod.getStatusCode() == 401){
            //Authenticate to the site using Basic authentication.
            BasicScheme basicscheme = new BasicScheme();
            String basic_auth_string = basicscheme.authenticate(new
NTCredentials(""cwftp"", ""664A754c"", """", """"), getmethod);
            Header basic_auth_header = new Header(""Authorization"",
basic_auth_string);
            getmethod.addRequestHeader(basic_auth_header);
            try{
                httpclient.executeMethod(getmethod);
            }catch(Exception e){
                logger.log(Level.SEVERE, ""ack!!!!"", e);
            }
            return getmethod;
       }else if(getmethod.getStatusCode() == 407){
            //Authenticate to the site using Basic authentication
            NTLMScheme ntlmscheme = new NTLMScheme();
            String basic_auth_string = ntlmscheme.authenticate(new
NTCredentials(""00mercbac"", ""!@SAMmerc2004"", ""simproxy"", ""CFC""), getmethod);
            Header basic_auth_header = new Header(""Authorization"",
basic_auth_string);
            getmethod.addRequestHeader(basic_auth_header);
            try{
                httpclient.executeMethod(getmethod);
            }catch(Exception e){
                logger.log(Level.SEVERE, ""ack!!!!"", e);
            }
            return getmethod;
       }


Thanks,
David"
0,"JSP page compilation errors when depoyed using oc4j. An error in the Welcome.jsp was produced as follows:

cannot find symbol symbol : method log(java.lang.String,java.lang.Throwable)

In a response from the user group it was determined that there should be no expectation in Jackrabbit that the JspPage implementation will inherit from the GeneralServlet base class.
"
0,"Bring Hunspell for Lucene into analysis module. Some time ago I along with Robert and Uwe, wrote an Stemmer which uses the Hunspell algorithm.  It has the benefit of supporting dictionaries for a wide array of languages.   

It seems to still be being used but has fallen out of date.  I think it would benefit from being inside the analysis module where additional features such as decompounding support, could be added."
0,"concurrent read-only access to a session. Even though the JCR specification does not make a statement about Sessions shared across a number of threads I think it would be great for many applications if we could state that sharing a read-only session is supported by Jackrabbit.
On many occasions in the mailing lists we stated that there should not be an issue with sharing a read-only session, however I think it has never been thoroughly tested or even specified as a ""design goal"".

If we can come to an agreement that this is desirable I think it would be great to start including testcases to validate that behaviour and update the documentation respectively."
0,IOContext should be part of the SegmentReader cache key . Once IOContext (LUCENE-2793) is landed the IOContext should be part of the key used to cache that reader in the pool
0,"fix LowerCaseFilter for unicode 4.0. lowercase suppl. characters correctly. 

this only fixes the filter, the LowerCaseTokenizer is part of a more complex issue (CharTokenizer)
"
0,"Add Google Analytics to Jackrabbit web site. I'd like to add Google Analytics to our web site to better track how the site is used and how much traffic we are generating.

Currently the best stats we have are at http://people.apache.org/~vgritsenko/stats/projects/jackrabbit.html, which is nice but not nearly as good as we could have."
0,Add customizable filtering to GQL. Currently GQL is not very flexible because it does not have any hooks that  allows you to modify the query that gets generated from the GQL syntax. As a first step I'd like to introduce a filtering mechanism that can be used to post process the result set and exclude certain rows. This is useful when you cannot express an application constraint in GQL.
0,"add spanquery support for all multitermqueries. I set fix version: 4.0, but possibly we could do this for 3.x too

Currently, we have a special SpanRegexQuery in contrib, and issues like LUCENE-522 open for SpanFuzzyQuery.
The SpanRegexQuery in contrib is a little messy additionally.

For any arbitrary MultiTermQueries to work as a SpanQuery, there are only 3 requirements:
# The un-rewritten query must extend SpanQuery so it can be included in Span clauses
# The rewritten query should be SpanOrQuery instead of BooleanQuery
# The rewritten term clauses should be SpanTermQueries.

Instead of having logic like this for each query, i suggest adding two rewrite methods:
* ScoringSpanBoolean rewrite
* TopTermsSpanBoolean rewrite

as a start i wrote these up, and added a SpanMultiTermQueryWrapper that can be used to wrap any multitermquery this way.
there are a few kinks, but I think the MTQ policeman can probably help get through them.
"
0,"Updates to connectionStaleCheckingEnabled docs.. Comments from Itai Brickner:

In the Threading section of the UserGuide
(
http://jakarta.apache.org/commons/httpclient/threading.html
)

There is no mentioning of the
'setConnectionStaleCheckingEnabled'
I also felt that it wasn't clear from the APIDOC
(http://jakarta.apache.org/commons/httpclient/apidocs/org/apache/commons/httpclient/MultiThreadedHttpConnectionManager.html)
that staleCheckingEnabled will cause a stale
connection to be reconnected by the
MultiThreadedHttpConnectionManager

thanks,

Itai"
0,"Lucene requires ant 1.6?. The latest version in CVS as of April 3rd only builds with ant 1.6.   If this is intentional, BUILD.txt should 
be updated.

Here's the error I get with ant 1.5:

BUILD FAILED
file:/Users/skybrian/remote-cvs/jakarta-lucene/build.xml:11: Unexpected element ""tstamp"""
0,Add support for benchmarking Collectors. As the title says.
0,"multipart feedback. never got a reply on this from 10/20/02 mailing to email address in ""author"" tag, so posting here.
-----------------------
Matt and Jeff,

Excuse me for writing directly to the addresses found in the code as authors.  Please feel free to forward this to any list that is more appropriate.

Thank you very much for your efforts in making an HTTP client for Java.  It will find great use.  Below are some aspects of the org.apache.commons.httpclient.methods.multipart package that I'd like you to consider.  

First, consider making the encoding a parameter.  Currently, the content disposition and other general-purpose headers are written with a String.getBytes () call, which will use the default encoding on whatever client is being used by the customer.  Does the RFC and for HTTP post specify an encoding for these lines?  Perhaps the header information and disposition information should be a standard UTF-8 encoding, and an additional parameter could specify encoding for anything supplied by the users of the library, most notably the StringPart.

Second, consider that the content length header may not be important for many contexts.  When you receive a post on the server side, can you depend on the content length header?  Some browsers do not supplied this header, and even if all of them did, would you be wise to believe it on the server side?  In fact, common libraries for handling post, most notably http://www.servlets.com/cos/index.html , ignore any content length header that is supplied by the client.  On the server side, content length is calculated from the actual bytes that are received.

Why do I mention this?  Because it appears that a trade-off has been made in this alpha code such that the content length calculation was more important than polymorphism in Part.java:

    /* The following 2 methods don't need to be final, but they DO need
     * to be overridden as a pair, and the only way to make sure of that
     * is to make sure they AREN'T overridden. 
     */

    final public void send(OutputStream out) throws IOException {
        sendStart(out);
        sendHeader(out);
        sendEndOfHeader(out);
        sendData(out);
        sendEnd(out);
    }
    
    final public long length() throws IOException {
        return lengthOfStart() + 
               lengthOfHeader() + 
               lengthOfEndOfHeader() +
               lengthOfData() + 
               lengthOfEnd();
    }

The method send() seems like an important method to be able to override.  For example, consider a situation where the post content is zipped on-the-fly.  The content length is not known when writing the headers.  Further, it would be handy to override certain methods like send() in order to manipulate the output stream. 

Basically, since your library will be very general purpose and used widely, the more you can do for easy polymorphism, the more your customers will appreciate your library. Is there a way to make the content length calculation and header writing more flexible, so that it may be avoided when it is not known a priori?

Third, consider making the content type a parameter for a FilePart.  In the example above, the content type for the zipped file should be ""application/x-zip-compressed"" rather than ""application/octet-stream"".

Again, I was very happy to find your excellent work on this library. Thank you for your contributions to apache jakarta.

larry hamel"
0,"DirectoryIndexReader finalize() holding TermInfosReader longer than necessary. DirectoryIndexReader has a finalize method, which causes the JDK to keep a reference to the object until it can be finalized.  SegmentReader and MultiSegmentReader are subclasses that contain references to, potentially, hundreds of megabytes of cached data in a TermInfosReader.

Some options would be removing finalize() from DirectoryIndexReader (it releases a write lock at the moment) or possibly nulling out references in various close() and doClose() methods throughout the class hierarchy so that the finalizable object doesn't references the Term arrays.

Original mailing list message:
http://mail-archives.apache.org/mod_mbox/lucene-java-user/200906.mbox/%3C7A5CB4A7BBCE0C40B81C5145C326C31301A62971@NUMEVP06.na.imtn.com%3E"
0,"Allow whitespaces in base64 encoded binary fields of XML import files. When importing files using Session.importXML(), the Binary property values are Base64 encoded.  However you cannot put whitespaces in them, and XML files with binaries in them become very long lines.  The files are more manageable if whilespaces could be put in them, as is common to do in base base64 encoded files."
0,"Synchronization bottleneck in FieldSortedHitQueue with many concurrent readers. The below is from a post by (my colleague) Paul Smith to the java-users list:

---

Hi ho peoples.

We have an application that is internationalized, and stores data from many languages (each project has it's own index, mostly aligned with a single language, maybe 2).

Anyway, I've noticed during some thread dumps diagnosing some performance issues, that there appears to be a _potential_ synchronization bottleneck using Locale-based sorting of Strings.  I don't think this problem is the root cause of our performance problem, but I thought I'd mention it here.  Here's the stack dump of a thread waiting:

""http-1001-Processor245"" daemon prio=1 tid=0x31434da0 nid=0x3744 waiting for monitor entry [0x2cd44000..0x2cd45f30]
        at java.text.RuleBasedCollator.compare(RuleBasedCollator.java)
        - waiting to lock <0x6b1e8c68> (a java.text.RuleBasedCollator)
        at org.apache.lucene.search.FieldSortedHitQueue$4.compare(FieldSortedHitQueue.java:320)
        at org.apache.lucene.search.FieldSortedHitQueue.lessThan(FieldSortedHitQueue.java:114)
        at org.apache.lucene.util.PriorityQueue.upHeap(PriorityQueue.java:120)
        at org.apache.lucene.util.PriorityQueue.put(PriorityQueue.java:47)
        at org.apache.lucene.util.PriorityQueue.insert(PriorityQueue.java:58)
        at org.apache.lucene.search.FieldSortedHitQueue.insert(FieldSortedHitQueue.java:90)
        at org.apache.lucene.search.FieldSortedHitQueue.insert(FieldSortedHitQueue.java:97)
        at org.apache.lucene.search.TopFieldDocCollector.collect(TopFieldDocCollector.java:47)
        at org.apache.lucene.search.BooleanScorer2.score(BooleanScorer2.java:291)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:132)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:110)
        at com.aconex.index.search.FastLocaleSortIndexSearcher.search(FastLocaleSortIndexSearcher.java:90)
.....

In our case we had 12 threads waiting like this, while one thread had the lock on the RuleBasedCollator.  Turns out RuleBasedCollator's.compare(...) method is synchronized.  I wonder if a ThreadLocal based collator would be better here... ?  There doesn't appear to be a reason for other threads searching the same index to wait on this sort.  Be just as easy to use their own.  (Is RuleBasedCollator a ""heavy"" object memory wise?  Wouldn't have thought so, per thread)

Thoughts?

---

I've investigated this somewhat, and agree that this is a potential problem with a series of possible workarounds. Further discussion (including proof-of-concept patch) to follow."
0,"Analyzer for preventing overload of search service by queries with common terms in large indexes. An analyzer used primarily at query time to wrap another analyzer and provide a layer of protection
which prevents very common words from being passed into queries. For very large indexes the cost
of reading TermDocs for a very common word can be  high. This analyzer was created after experience with
a 38 million doc index which had a term in around 50% of docs and was causing TermQueries for 
this term to take 2 seconds.

Use the various ""addStopWords"" methods in this class to automate the identification and addition of 
stop words found in an already existing index."
0,"Improve performance of CharTermAttribute(Impl) and also fully implement Appendable. The Appendable.append(CharSequence) method in CharTermAttributes is good for general use. But like StringBuilder has for some common use cases specialized methods, this does the same and adds separate append methods for String, StringBuilder and CharTermAttribute itsself. This methods enable the compiler to directly link the specialized methods and don't use the instanceof checks. The unspecialized method only does the instanceof checks for longer CharSequences (>8 chars), else it simply iterates.

This patch also fixes the required special ""null"" handling. append() methods are required by Appendable to append ""null"", if the argument is null. I dont like this, but its required. Maybe we should document, that we dont dont support it. Otherwise, JDK's formatter fails with formatting null."
0,"Implement ""point in time"" searching without relying on filesystem semantics. This was touched on in recent discussion on dev list:

  http://www.gossamer-threads.com/lists/lucene/java-dev/41700#41700

and then more recently on the user list:

  http://www.gossamer-threads.com/lists/lucene/java-user/42088

Lucene's ""point in time"" searching currently relies on how the
underlying storage handles deletion files that are held open for
reading.

This is highly variable across filesystems.  For example, UNIX-like
filesystems usually do ""close on last delete"", and Windows filesystem
typically refuses to delete a file open for reading (so Lucene retries
later).  But NFS just removes the file out from under the reader, and
for that reason ""point in time"" searching doesn't work on NFS
(see LUCENE-673 ).

With the lockless commits changes (LUCENE-701 ), it's quite simple to
re-implement ""point in time searching"" so as to not rely on filesystem
semantics: we can just keep more than the last segments_N file (as
well as all files they reference).

This is also in keeping with the design goal of ""rely on as little as
possible from the filesystem"".  EG with lockless we no longer re-use
filenames (don't rely on filesystem cache being coherent) and we no
longer use file renaming (because on Windows it can fails).  This
would be another step of not relying on semantics of ""deleting open
files"".  The less we require from filesystem the more portable Lucene
will be!

Where it gets interesting is what ""policy"" we would then use for
removing segments_N files.  The policy now is ""remove all but the last
one"".  I think we would keep this policy as the default.  Then you
could imagine other policies:

  * Keep past N day's worth

  * Keep the last N

  * Keep only those in active use by a reader somewhere (note: tricky
    how to reliably figure this out when readers have crashed, etc.)

  * Keep those ""marked"" as rollback points by some transaction, or
    marked explicitly as a ""snaphshot"".

  * Or, roll your own: the ""policy"" would be an interface or abstract
    class and you could make your own implementation.

I think for this issue we could just create the framework
(interface/abstract class for ""policy"" and invoke it from
IndexFileDeleter) and then implement the current policy (delete all
but most recent segments_N) as the default policy.

In separate issue(s) we could then create the above more interesting
policies.

I think there are some important advantages to doing this:

  * ""Point in time"" searching would work on NFS (it doesn't now
    because NFS doesn't do ""delete on last close""; see LUCENE-673 )
    and any other Directory implementations that don't work
    currently.

  * Transactional semantics become a possibility: you can set a
    snapshot, do a bunch of stuff to your index, and then rollback to
    the snapshot at a later time.

  * If a reader crashes or machine gets rebooted, etc, it could choose
    to re-open the snapshot it had previously been using, whereas now
    the reader must always switch to the last commit point.

  * Searchers could search the same snapshot for follow-on actions.
    Meaning, user does search, then next page, drill down (Solr),
    drill up, etc.  These are each separate trips to the server and if
    searcher has been re-opened, user can get inconsistent results (=
    lost trust).  But with, one series of search interactions could
    explicitly stay on the snapshot it had started with.

"
0,Include the README file in the generated jar files. The Incubator would prefer if we had the incubation notice included in the binary jar files we release. It should be a simple Maven configuration change to get the README.txt file included in the binary jars.
0,"When A URL is redirected, there is no easy way to encode the new url before HC tries to execute it/. When you implement your custom RedirectHandler, there is no easy way to encode the new URL being redirected to.

A public method to access the location string prior to the URI generation would be useful."
0,"Can't quickly create StopFilter. Due to the use of CharArraySet by StopFilter, one can no longer efficiently pre-create a Set for use by future StopFilter instances."
0,"Correctly handle concurrent calls to addIndexes, optimize, commit. Spinoff from here:

    http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200807.mbox/%3Cc7b302c50807111018j58b6d08djd56b5889f6b3780d@mail.gmail.com%3E"
0,"caching module should use HttpParams-style configuration. The constructor for CachingHttpClient currently accepts combinations of:
* HttpCache
* HttpClient
* integer for max object size in bytes

As I started looking at being able to configure this for behaving as a non-shared cache, I realized that we actually want to be replacing that last int with an HttpParams argument, and tracking all the various options in that style. I have a patch with this update which I will upload shortly.
"
0,"HttpMethodBase.getResponseBodyAsString(long limit). Currently HttpMethodBase.getResponseBodyAsString() prints warning in log, and suggests using getResponseStream(). However getResponseBodyAsString() is extremely useful (as it is easy to use). So my wish is to have method

getResponseBodyAsString(long limit)

that should throw HttpException if response size exceeds specified limit.

Same things with getResponseBody(long limit) .

Original methods should be deprecated because of danger, explained in javadoc."
0,"Add signature and major/minor version to the journal files used for clustering. Journal files used for clustering should contain a signature, and a major/minor version that helps identifying them."
0,"If IndexWriter hits OutOfMemoryError it should not commit. While progress has been made making IndexWriter robust to OOME, I
think there is still a real risk that an OOME at a bad time could put
IndexWriter into a bad state such that if close() is called and
somehow it succeeds without hitting another OOME, it risks
introducing messing up the index.

I'd like to detect if OOME has been hit in any of the methods that
alter IW's state, and if so, do not commit changes to the index.  If
close is called after hitting OOME, I think writer should instead
abort.

Attached patch just adds try/catch clauses to catch OOME, note that
it was hit, and re-throw it.  Then, sync() refuses to commit a new
segments_N if OOME was hit, and close instead calls abort when OOME
was hit.  All tests pass.  I plan to commit in a day or two."
0,"Remove old hooks for the implementation of hard links. Early drafts of the JCR specification specified that repositories should support ""hard links"", which woud lead to the situation, that items might have multiple parent nodes. In the meantime hard links have been removed from the spec and are unlikely to be re-added in future revisions.

Nevertheless, Jackrabbit still contains some references to supporting this mechanism (e.g. the NodeState.parentUUIDs field), which should be removed."
0,"better payload testing with mockanalyzer. MockAnalyzer currently always indexes some fixed-length payloads.

Instead it should decide for each field randomly (and remember it for that field):
* if the field should index no payloads at all
* field should index fixed length payloads
* field should index variable length payloads.
"
0,"Upgrade to Logback 1.0. Logback 1.0 was just released (see http://mailman.qos.ch/pipermail/announce/2011/000093.html). There are no big new features or other major changes, but the bump to 1.0 is still a good point for us to upgrade to get all the latest bug fixes and other improvements.

At the same time we should upgrade our SLF4J depedency to the latest 1.6.4 version."
0,"Tests need to clean up after themselves. I havent run 'ant clean' for a while.

The randomly generated temporarily file names just piled up from running the tests many times... so ant clean is still running after quite a long time.

We should take the logic in the base solr test cases, and push it into LuceneTestCase, so a test cleans up all its temporary stuff.
"
0,Avoid using MultiTermDocs. Similar to MatchAllQuery also RangeQuery and WildcardQuery will result in use of MultiTermDocs. Those queries should also use the MultiScorer. See also issue JCR-791.
0,"remove field param from computeNorm, scorePayload ; remove UOE'd lengthNorm, switch SweetSpot to per-field . In LUCENE-2236 we switched sim to per field (SimilarityProvider returns a per-field similarity).

But we didn't completely cleanup there... I think we should now do this:
* SweetSpotSimilarity loses all its hashmaps. Instead, just configure one per field and return it in your SimilarityProvider. this means for example, all its TF factors can now be configured per-field too, not just the length normalization factors.
* computeNorm and scorePayload lose their field parameter, as its redundant and confusing.
* the UOE'd obselete lengthNorm is removed. I also updated javadocs that were pointing to it (this is bad!).

"
0,"Jcr-Server: useful output upon GET to root- and workspace-resources. in contrast to the resources representing JCR Node or Properties, the 'root' resource and the resources representing the
workspaces present in the repository don't provide any output when accessed in a browser.

-> add some very simple listing for those 2 resource types."
0,"Update dependency versions for commons-collections, slf4j and derby. Some of the dependencies used by the 2.0-beta1 could be upgraded:
commons-collections from 3.1 to 3.2.1
slf4j from 1.5.3 to 1.5.8
derby from 10.2.1.6 to 10.5.3.0

Not sure about derby but the other two seems to be just drop in replacements for their older verisons."
0,"Logging per test case. Thanks to the switch to Logback a while ago (JCR-2584) we can start taking advantage of some nice new features like the one described in [1]. With this trick we'll be able to split the currently pretty large jcr.log test log file we have to separate log files per each test case. This will make it much easier to review the logs written during any particular test.

[1] http://www.nalinmakar.com/2010/07/28/logging-tests-to-separate-files/"
0,"Resource association not compliant to JTA spec. According to JTA specifcation, section 3.4.4 (Transaction Association), a resource's association may be ended (state T0 in the spec's table) in the suspended state (T2), i.e. without having been resumed (T1) again. The code in XASessionImpl.end(), however, assumes that the resource must be associated in order to end its association. This causes an exception in JBoss 4.0.5.GA:

09:37:15,525 WARN  [TransactionImpl] XAException: tx=TransactionImpl:XidImpl[FormatId=257, GlobalId=kneipix.dev.day.com/14, BranchQual=, localId=14] errorCode=XAER_PROTO
javax.transaction.xa.XAException
        at org.apache.jackrabbit.core.XASessionImpl.end(XASessionImpl.java:279)
        at org.apache.jackrabbit.jca.TransactionBoundXAResource.end(TransactionBoundXAResource.java:46)
        at org.jboss.tm.TransactionImpl$Resource.endResource(TransactionImpl.java:2143)
        at org.jboss.tm.TransactionImpl$Resource.endResource(TransactionImpl.java:2118)
        at org.jboss.tm.TransactionImpl.endResources(TransactionImpl.java:1462)
        at org.jboss.tm.TransactionImpl.beforePrepare(TransactionImpl.java:1116)
        at org.jboss.tm.TransactionImpl.commit(TransactionImpl.java:324)
        at org.jboss.tm.TxManager.commit(TxManager.java:240)
        at org.jboss.aspects.tx.TxPolicy.endTransaction(TxPolicy.java:175)
"
0,"speedup recycling of per-doc RAM. Robert found one source of slowness when indexing tiny docs, where we use List.toArray to recycle the byte[] buffers used by per-doc doc store state (stored field, term vectors).  This was added in LUCENE-2283, so not yet released."
0,Wrong javadoc on LowerCaseTokenizer.normalize. The javadoc on LowerCaseTokenizer.normalize seems to be copy/paste from LetterTokenizer.isTokenChar.
0,"Don't create compound file for large segments by default. Spinoff from LUCENE-2762.

CFS is useful for keeping the open file count down.  But, it costs
some added time during indexing to build, and also ties up temporary
disk space, causing eg a large spike on the final merge of an
optimize.

Since MergePolicy dictates which segments should be CFS, we can
change it to only build CFS for ""smallish"" merges.

I think we should also set a maxMergeMB by default so that very large
merges aren't done.
"
0,"Misleading method names in SetValueBinaryTest. Some of the method names in SetValueBinaryTest say ""Boolean"" when they should say ""Binary"" (copy&paste error from SetValueBooleanTest?).

"
0,"connection wrapper prevents GC of TSCCM. Even if a connection is released back to the ThreadSafeClientConnManager, a hard reference to the connection wrapper will prevent GC of the TSCCM.
Make sure the connection wrapper is properly detached on release. Then update TestTSCCMWithServer.testConnectionManagerGC() accordingly. 
"
0,add support for RFC 3253 to the simple server. http://www.ietf.org/rfc/rfc3253.txt
0,"Tika-based type detection in jcr-server. As discussed on dev@, I'd like to make the jackrabbit-jcr-server component use Apache Tika for automatic media type detection."
0,"Provide more options for OCM CRUD API Writers to enhance the functionality. I am working on an Extension to Object Content Manager and from that angle require a few methods and variable from the base classes to be exposed with protected access.  I have modifier only the getters and these should not cause any issues to the current functionality.  Request a review and addition to the trunk.  

1. added a clone implementation to FilterImpl
2.  Exposed : 
public CollectionConverter getCollectionConverter(Session session, CollectionDescriptor collectionDescriptor) from ObjectConverter"
0,"Simplify configuration API of contrib Query Parser. The current configuration API is very complicated and inherit the concept used by Attribute API to store token information in token streams. However, the requirements for both (QP config and token stream) are not the same, so they shouldn't be using the same thing.

I propose to simplify QP config and make it less scary for people intending to use contrib QP. The task is not difficult, it will just require a lot of code change and figure out the best way to do it. That's why it's a good candidate for a GSoC project.

I would like to hear good proposals about how to make the API more friendly and less scaring :)"
0,"Restore top level disjunction performance. This patch restores the performance of top level disjunctions. 
The introduction of BooleanScorer2 had impacted this as reported
on java-user on 21 Nov 2006 by Stanislav Jordanov.
"
0,"in advance(), don't try to skip if there is evidence it will fail. There are TODO's about this in the code everywhere, and this was part of
Mike speeding up ExactPhraseScorer.

I think the codec should do this."
0,"Path should implement Serializable. QName already implements Serializable, for ease of use Path should also support Serializable."
0,"yank SegmentReader.norm out of SegmentReader.java. While working on flex scoring branch and LUCENE-3012, I noticed it was difficult to navigate 
the norms handling in SegmentReader's code.

I think we should yank this inner class out into a separate file as a start."
0,"ServerTestBase, LocalTestServer should be redistributed. ServerTestBase and LocalTestServer should be redistributed, so that we can all benefit from these wonderful classes without having to copy / paste them.

For instance, I am currently creating a REST client for some web service, and I would totally benefit from your classes to check borderline cases (404, 500, etc..)

the simplest possible way to achieve this would be to add a maven-jar-plugin entry to the POM that executes the test-jar goal.

regards,
Sami Dalouche

"
0,"[PATCH] Trivial Javadoc fix for RepositoryConfig. Yes, this is really trivial, but i keep coming to this class, trying to figure out it works, and the javadoc parms are scrambled.

patch 'fixes' this."
0,"Change BooleanFilter to have only a single clauses ArrayList (so toString() works fine, clauses() method could be added) so it behaves more lik BooleanQuery. This is unrelated to the other BF changes, but should be done"
0,"SimpleHttpConnectionManager is used incorrectly by tutorial code. Using pretty well standard (from the tutorial) code causes the 
SimpleHttpConnectionManager to print its ""being used incorrectly"" warning if 
the connection times out (or other I/O exception occurs).

I will attach a simple test I made to demonstrate this."
0,"DocumentsWriter.applyDeletes should not create TermDocs or IndexSearcher if not needed. DocumentsWriter.applyDeletes(IndexReader, int) always creates TermDocs and IndexSearcher, even if there were no deletes by Term or by Query. The attached patch wraps those creations w/ checks on whether there were any deletes by these two. Additionally, the searcher wasn't closed in a finally block, so I fixed that as well.

I'll attach a patch shortly."
0,"Replace commons-logging dependency with SLF4J. The poi dependency in jackrabbit-text-extractors brings in a transitive dependency to commons-logging. Since we use SLF4J for all logging, we should exclude the commons-logging dependency and replace it with jcl104-over-slf4j."
0,"Source distribution packaging targets should make a tarball from ""svn export"". Instead of picking and choosing which stuff to include from a local working copy, Lucene's dist-src/package-tgz-src target and Solr's package-src target should simply perform ""svn export"" with the same revision and URL as the local working copy."
0,"random analyzer tests. we have been finding+fixing lots of bugs by randomizing lucene tests.
in r966878 I added a variant of random unicode string that gives you a random string within the same unicode block (for other purposes)

I think we should use this to test the analyzers better, for example we should pound tons of random greek strings against the greek analyzer and at least make sure there aren't exceptions.
"
0,"no classes with default visibility. There should be no classes with default (package) visibility. They cause problems when classes using them are extended. All classes should either be public, or nested with protected visibility where they are used. Nesting with private visibility may be acceptable in certain cases, for example in final classes.
"
0,"Correct copy-paste victim Comment. Correct the doc-comment of FieldsProducer (being a copy-paste victim of FieldsConsumer).
""consumes"" replaced with ""produces"".

One word change to avoid confusion: safe to commit.
"
0,FAQ document. Collect the frequently discussed issues from the mailing list and the wiki into an FAQ document.
0,"Make termInfosIndexDivisor configurable. Workspaces with large indexes may consume considerable heap memory. Lucene implements multi level skip lists for terms in the index. The first level of the skip list is kept in memory. This is usually not an issue, but when terms consist of long Strings the memory consumption increases drastically. Jackrabbit not just tokenizes string properties, but it also creates a single term, based on the complete string property value (needed for jcr:like function). These long terms are the reason for the increased memory consumption."
0,"Use NIO positional read to avoid synchronization in FSIndexInput. As suggested by Doug, we could use NIO pread to avoid synchronization on the underlying file.
This could mitigate any MT performance drop caused by reducing the number of files in the index format."
0,"Clean up spi-commons pom.xml. The pom.xml contains lines that were copied from the jackrabbit-core but are not actually needed. A log4j.properties is also missing in test resources.

See attached patch."
0,"remove RoutedRequest from ClientRequestDirector interface. Remove the RoutedRequest from ClientRequestDirector.execute, pass the request and route/target separately."
0,"Create UAX29URLEmailAnalyzer: a standard analyzer that recognizes URLs and emails. This Analyzer should contain the same components as StandardAnalyzer, except for the tokenizer, which should be UAX29URLEmailTokenizer instead of StandardTokenizer."
0,"addIndexes(Directory...) should not trigger merge on flush(). IndexWriter.addIndexes(Directory..) calls flush() w/ triggerMerge=true. This beats the purpose of the changes done to addIndexes to not merge any segments and leave it as the application's choice. The change is very simple - pass false instead of true. I don't plan to post a patch, however opened an issue in case some want to comment about it."
0,"WebApp: Ease first access for new users looking for a WebDAV server. suggestion posted by mike oliver in the user list:

> I know that JackRabbit isn't the same as Jakarta Slide and not expecting it to be, but one thing we did right on 
> that project was create a runnable war file that doesn't require any learning curve to get started.  Install the war file, 
> create the network place and login as the root:root user and start creating content folders and documents. 
> If JackRabbit did that, then I think more people would try it and use it and then spend the time to learn how to make 
> it all it can be."
0,"Parallelize Tests. The Lucene tests can be parallelized to make for a faster testing system.  

This task from ANT can be used: http://ant.apache.org/manual/CoreTasks/parallel.html

Previous discussion: http://www.gossamer-threads.com/lists/lucene/java-dev/69669

Notes from Mike M.:
{quote}
I'd love to see a clean solution here (the tests are embarrassingly
parallelizable, and we all have machines with good concurrency these
days)... I have a rather hacked up solution now, that uses
""-Dtestpackage=XXX"" to split the tests up.

Ideally I would be able to say ""use N threads"" and it'd do the right
thing... like the -j flag to make.
{quote}"
0,"Lucli: Command to change the Analyzer. Currently, Lucli is hardcoded to use StandardAnalyzer. The provided patch introduces a command ""analyzer"" to specify a different Analyzer class. 
If something fails, StandardAnalyzer is the fall-back."
0,"Move some *TermsEnum.java from oal.search to oal.index. I think FilteredTermsEnum, SingleTermsEnum should move?

I left TermRangeTermsEnum and FuzzyTermsEnum and PrefixTermsEnum since they seemed search specific."
0,"replacing an extended mixin with it's supertype is problematic. node.addMixin() / node.removeMixin() have some checks to avoid redundant mixin settings on a node and not only when the node is saved.

eg: have 2 mixins: mix:A and mix:AA where mix:AA > mix:A and a node (N with mix:AA) on it.

then, N.addMixin(mix:A)  has no effect, since it's regarded as redundant.  so you have to remove mix:AA first and then add mix:A.
there is the first problem when applying mixin types programmatically, just be sure to remove them first before adding new ones.

the 2nd problem occurs when mix:A has a mandatory property. then somehow when downgrading from mix:AA to mix:A, some information is lost, and a save call results in

Unable to save node 'N': javax.jcr.nodetype.ConstraintViolationException: /test/A: mandatory property {}prop does not exist.
you need to ""touch"" the property, otherwise it will not work.

so only this works:

N.removeMixin(""mix:AA"");
N.addMixin(""mix:A"");
N.setProperty(""prop"", N.getProperty(""prop"").getValue());
session.save();



"
0,"NodeTypeRegistry could auto-subtype from nt:base. this is basically a copy of JCR-433, which was fixed but somehow sneaked in again:

when tying to register a (primary) nodetype that does not extend from nt:base the following error is
thrown:

""all primary node types except nt:base itself must be (directly or indirectly) derived from nt:base""

since the registry is able to detect this error, it would be easy to auto-subtype all nodetypes from nt:base. 
imo it's pointless to explicitly add the nt:base to every superclass set. as an analogy, you don't need to 
'extend from java.lang.Object' explicitly - the compiler does that automatically for your."
0,"exception consistency in o.a.l.store. just some minor improvements:
* always use EOFException when its eof
* always include the inputstream too so we know filename etc
* use FileNotFoundException consistently in CFS when a sub-file is not found
"
0,"cache should not generate stale responses to requests explicitly requesting first-hand or fresh ones. The current implementation will serve a stale response in the case that it has a stale cache entry but revalidation with the origin fails. However, the RFC says we SHOULD NOT do this if the client explicitly requested a first-hand or fresh response (via no-cache, max-age, max-stale, or min-fresh).
"
0,"Speedup Startup. jackrabbit startup gets slower the more items are in the repository.

possible reasons:
- versioning
- search index"
0,"JSR 283: Node Type Attribute Subtyping Rules . JCR 2.0 clarifies node type attribute subtyping rules whereas JCR 1.0 didn't mandate any specific behavior.

""3.7.6.7 Node Type Attribute Subtyping Rules"" states  (assume T' being a subtype of T):

- if T has orderable child nodes then T' must have orderable child nodes
- if T specifies a primary item then T' inherits that setting and must not override it

"
0,Enable passing a config into PKIndexSplitter. I need to be able to pass the IndexWriterConfig into the IW used by PKIndexSplitter.
0,Make PKIndexSplitter and MultiPassIndexSplitter work per segment. Spinoff from LUCENE-3624: DocValuesw merger throws exception on IW.addIndexes(SlowMultiReaderWrapper) as string-index like docvalues cannot provide asSortedSource.
0,"read/write .del as d-gaps when the deleted bit vector is sufficiently sparse. .del file of a segment maintains info on deleted documents in that segment. The file exists only for segments having deleted docs, so it does not exists for newly created segments (e.g. resulted from merge). Each time closing an index reader that deleted any document, the .del file is rewritten. In fact, since the lock-less commits change a new (generation of) .del file is created in each such occasion.

For small indexes there is no real problem with current situation. But for very large indexes, each time such an index reader is closed, creating such new bit-vector seems like unnecessary overhead in cases that the bit vector is sparse (just a few docs were deleted). For instance, for an index with a segment of 1M docs, the sequence: {open reader; delete 1 doc from that segment; close reader;} would write a file of ~128KB. Repeat this sequence 8 times: 8 new files of total size of 1MB are written to disk.

Whether this is a bottleneck or not depends on the application deletes pattern, but for the case that deleted docs are sparse, writing just the d-gaps would save space and time. 

I have this (simple) change to BitVector running and currently trying some performance tests to, yet, convince myself on the worthiness of this.

"
0,"Move common node type functionality to jackrabbit-spi-commons. now, that jackrabbit-core has a dependency to jackrabbit-spi-commons it would be possible to have the common functionality shared by core and jcr2spi in the spi-commons project.

the node type package offers quite some potential for that.
"
0,"short circuit FuzzyQuery.rewrite when input token length is small compared to minSimilarity. I found this (unreplied to) email floating around in my Lucene folder from during the holidays...

{noformat}
From: Timo Nentwig
To: java-dev
Subject: Fuzzy makes no sense for short tokens
Date: Mon, 31 Dec 2007 16:01:11 +0100
Message-Id: <200712311601.12255.lucene@nitwit.de>

Hi!

it generally makes no sense to search fuzzy for short tokens because changing
even only a single character of course already results in a high edit
distance. So it actually only makes sense in this case:

           if( token.length() > 1f / (1f - minSimilarity) )

E.g. changing one character in a 3-letter token (foo) results in an edit
distance of 0.6. And if minSimilarity (which is by default: 0.5 :-) is higher
we can save all the expensive rewrite() logic.
{noformat}

I don't know much about FuzzyQueries, but this reasoning seems sound ... FuzzyQuery.rewrite should be able to completely skip all TermEnumeration in the event that the input token is shorter then some simple math on the minSimilarity.  (i'm not smart enough to be certain that the math above is right however ... it's been a while since i looked at Levenstein distances ... tests needed)
"
0,"add an interface for plugable dns clients. Currently Httpclient implicitly uses InetAddress.getByName() for DNS resolution.
This has some drawbacks. One is that the DNS cache of Java per default caches entries forever.

So I'd like to be able to replace InetAddress.getByName() with another DNS client implementation.

"
0,"LockInfo.logginOut(SessionImpl): javadoc does not correspond to executed code. /**
     * {@inheritDoc}
     * <p/>
     * When the owning session is logging out, we have to perform some
     * operations depending on the lock type.
     * (1) If the lock was session-scoped, we unlock the node.
     * (2) If the lock was open-scoped, we remove the lock token
     *     from the session.
     */
    public void loggingOut(SessionImpl session) {
        if (live) {
            if (sessionScoped) {
                lockMgr.unlock(this);
            } else {
                if (session.equals(lockHolder)) {
                    lockHolder = null;
                }
            }
        }
    } 


if (2) is true, the lockToken is not removed from the session (at least not within the method).

regards
angela"
0,"BooleanScorer.nextDoc should also delegate to sub-scorer's bulk scoring method. BooleanScorer uses the bulk score methods of its sub scorers, asking them to score each chunk of 2048 docs.

However, its .nextDoc fails to do this, instead manually walking through the sub's docs (calling .nextDoc()), which is slower (though this'd be tiny in practice).

As far as I can tell it should delegate to the bulk scorer just like it does in its bulk scorer method."
0,"Ant contrib test can fail if there is a space in path to lucene project. A couple contrib ant tests get the path to test files through a URL object, and so the path is URL encoded. Normally fine, but if you have a space in your path (/svn stuff/lucene/contrib/ant) then it will have %20 for the space and (at least on my Ubuntu system) the test will fail with filenotfound. This patch simply replaces all %20 with "" "". Not sure if we want/need to take it any further."
0,"Lucene's nightly Hudson builds don't have svn version in MANIFEST.MF. Solr had the same issue but apparently made a configuration change to the Hudson configuration to get it working:

    https://issues.apache.org/jira/browse/SOLR-684

Also I opened this INFRA issue:

    https://issues.apache.org/jira/browse/INFRA-1721

which says the svnversion exe is located in this path:

    /opt/subversion-current/bin

In that INRA issue, /etc/init.d/tomcat was also fixed in theory so that svnversion would be on the path the next time Hudson is restarted.  Still, in case that doesn't work, or it changes in the future, it seems a good idea to make the same change that Solr made to Lucene's Hudson configuration.

Hoss can you detail what you needed to do for Solr?  Or maybe just do it also for Lucene ;)  Thanks!"
0,"Java 1.4 compile error in Eclipse. As reported by Ate Douma in JCR-804, the revision 520841 introduced code that causes the Eclipse compiler to fail in Java 1.4 compliance mode. However, the same code compiles with the Sun JDK 1.4.

The problem is a enclosing class reference that an anonymous innner class instantiated in the constructor of a named inner class contains. Apparently (and understandably), in Eclipse 1.4 complier the instance references are not yet available when evaluating the super() arguments in the constructor."
0,"[GSoC 2011] Fluent API to HttpClient. Develop fluent API / facade to HttpClient based on code currently maintained by Apache Stanbol and Apache Sling projects. 

For details see 

http://markmail.org/message/mmyljtgjp3za6kyz

or contact Apache HttpComponents committers at dev@hc.apache.org"
0,"Change AtomicReaderContext.leaves() to return itsself as only leave to simplify code and remove an otherwise unneeded ReaderUtil method. The documentation of IndexReaderContext.leaves() states that it returns (for convenience) all leave nodes, if the context is top-level (directly got from IndexReader), otherwise returns null. This is not correct for AtomicReaderContext, where it returns null always.

To make it consistent, the convenience method should simply return itsself as only leave for atomic contexts. This makes the utility method ReaderUtil.leaves() obsolete and simplifies code."
0,"Minimize use of fields in lucene index. Currently every property name creates a field in the lucene index, bloating the size of the index because of the norm files created for each field.

When values are indexed as is (not tokenized for fulltext indexing), then the property name may be part of the term text. That way lucene must only maintain one field for all property names. With this approach the search terms are always a combination of property name and literal value. e.g. instead of using TermQuery(new Term(""prop"", ""foo"")) the query must be TermQuery(new TermQuery(""common-field"", ""prop:foo"")). this works for general comparison / value comparison operators and also for the like function. the contains function uses the fulltext index which uses a different field anyway.

Using the property name as part of the indexed term text, requires a custom SortComparator which is aware of the property name.

This change will not be backward compatible with earlier indexes created by jackrabbit."
0,"optimize fuzzytermsenum per-segment. we can make fuzzyquery about 3% faster by not creating DFA(s) for each segment.

creating the DFAs is still somewhat heavy: i can address this here too, but this is easy."
0,"Timeout for Session and/or Lock. I think there needs to be a mechanism where we can set the timeout for a particular jcr Session.  Or at the most, there should be a provision to set a timeout for a lock on a node.

Hope this is implemented soon.

Thanks."
0,"Better MimeType Handling. After saving a Excel File through WebDAV the mimetype will be changed.
The mimetype for a Win2000 Exel File is application/vnd.ms-excel. This will be changed to application/msexcel.
Also problems makes the new office 07 format (docx,xlsx,pptx). They will also be changed to application/octet-stream (default mimetype).
We have a lot of file types that we store in jackrabbit that are not in the properties file (MSInfoPath-, OutlookMsg-, MsAccess-Files, ...)
I think it will be better to let the mimetype property untouched if a mimetype is present so we must not put all the possible mimetypes in the property file.

BR
claus"
0,"CharArraySet cannot be made generic, because it violates the Set<char[]> interface. I tried to make CharArraySet using generics (extends AbstractSet<char[]>) but this is not possible, as it e.g. returns sometimes String instances in the Iterator instead of []. Also its addAll method accepts both String and char[]. I think this class is a complete mis-design and violates almost everything (sorry).

What to do? Make it Set<?> or just place a big @SuppressWarnings(""unchecked""> in front of it?

Because of this problem also a lot of Set declarations inside StopAnalyzer cannot be made generic as you never know whats inside."
0,"add back Document.getValues(). I'm porting some code to trunk's new Doc/Field apis, and i keep running into this pattern:
{noformat}
String[] values = doc.getValues(""field"");
{noformat}

But with the new apis, this becomes a little too verbose:

{noformat}
IndexableField[] fields = doc.getFields(""field"");
String[] values = new String[fields.length];
for (int i = 0; i < values.length; i++) {
  values[i] = fields[i].stringValue();
}
{noformat}

I think we should probably add back the sugar api (with the same name) ?
"
0,"Make CachingTokenFilter faster. The LinkedList used by CachingTokenFilter is accessed using the get() method. Direct access on a LinkedList is slow and an Iterator should be used instead. For more than a handful of tokens, the difference in speed grows exponentially."
0,"querystring still not set in Url*Method constructors. The queryString is still not set in various Url*Method's constructors. It does 
get set in setUrl. The simplest fix is to call setUrl in these constructors."
0,"improved snowball testing. Snowball project has test vocabulary files for each language in their svn repository, along with expected output.

We should use these tests to ensure all languages are working correctly, and it might be helpful in the future for identifying back breaks/changes if we ever want to upgrade snowball, etc.
"
0,"Allow redirects between hosts and ports. Redirects to different hosts, ports and protocols are currently prevented. 
Historicly, HttpMethodBase.checkValidRedirects() is used to prevent these types
of redirects due how state information was being managed in the connection.  

Much has changed since then.  We should relax the check and allow for redirects
 between hosts and ports. 

Redirects across protocols should not be considered at this time as there are
other issues related to security that is best left up to the user of HttpClient."
0,"Update license terms. Copyright 1999-2003 The Apache Software Foundation.

   Licensed under the Apache License, Version 2.0 (the ""License"");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an ""AS IS"" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License."
0,"Releasing a connection is unconfirmed. When a connection is attempted to be released using
HttpConnection.releaseConnection(), it is unclear whether this is actually done.
The implementation for the method is as follows in 3.0-beta1:

    /**
     * Release the connection.
     */
    public void releaseConnection() {
        LOG.trace(""enter HttpConnection.releaseConnection()"");
        if (locked) {
            LOG.debug(""Connection is locked.  Call to releaseConnection() ignore
        } else if (httpConnectionManager != null) {
            LOG.debug(""Releasing connection back to connection manager."");
            httpConnectionManager.releaseConnection(this);
        } else {
            LOG.warn(""HttpConnectionManager is null.  Connection cannot be relea
        }
    }

Silently ignoring a request (to release the connection, in this case) is hardly
ever the right thing to do, in my opinion. Instead, I suggest the method
indicates whether the connection was actually closed or not.

I see at least 2 alternatives:

1) throw an exception to indicate the connection could not be released;
2) return a flag indicating whether the connection could actually be released."
0,[PATCH] Add Column and line numbers to repository.xml parse exception messages. Code caught SAXExceptions when the an xml parsing exception occurred parsing the repository.xml. But catching SAXParseException (a subclass) allows access to column and line numbers of the problem. So also catch this exception and add this to the exception message to make it easier to fix errors.
0,"GData's TestGdataIndexer.testDestroy() intermittently hits spin loop & causes build timeout. Several nightly builds (at least #136, #143 and #144) have failed due
to timeout at 45 minutes while running the TestGdataIndexer.testDestroy()
test case.

I tracked it down to this line:

      // wait active for the commit
      while(this.indexer.writer != null){}

Intermittently, that while loop will spin forever.  I can only get the
failure to happen on Linux: it doesn't happen on Mac OS X (haven't
tried windows).  The nightly build runs on Solaris 10, so it also
happens there.

It turns out, this is due to the fact that ""writer"" is not declared as
""volatile"".  This is because one thread is closing the indexer, which
sets writer to null, but another thread is running the while loop.
If this.indexer.writer was set to null before that while loop starts,
the test will run through fine; else, it won't.

I plan to fix this by adding this method to GDataIndexer class:

    // Used only for testing
    protected synchronized IndexWriter getWriter() {
      return this.writer;
    }

and changing unit test to call that method."
0,"Improve lifecycle management of JCA connector. the shutdown mechanism doesn't work correctly. It shutdowns the repository when the RA (resource adapter) is garbage collected. It causes redeployment to fail because sometimes the new RA is redeployed before the old one is garbage collected.

Implementing the JCA 1.5 interface to manage the lifecycle would be useful."
0,workspace-wide default for lock timeout. There should be a way to define a workspace-wide default for JCR lock timeouts (in case the code creating the lock did not specify one).
0,"Queries with too many asterisks causing 100% CPU usage. If a search query has many adjacent asterisks (e.g. fo**************obar), I can get my webapp caught in a loop that does not seem to end in a reasonable amount of time and may in fact be infinite. For just a few asterisks the query eventually does return some results, but as I add more it takes a longer and longer amount of time. After about six or seven asterisks the query never seems to finish. Even if I abort the search, the thread handling the troublesome query continues running in the background and pinning a CPU.

I found the problem in src/java/org/apache/lucene/search/WildcardTermEnum.java on Lucene 3.0.1 and it looks like 3.0.2 ought to be affected as well. I'm not sure about trunk, though. I have a patch that fixes the problem for me in 3.0.1."
0,"Remove Priority-Queue size trap in MultiTermQuery.TopTermsBooleanQueryRewrite. These APIs are new in 3.x, so we can do this with no backwards-compatibility issue:

Before 3.1, FuzzyQuery had its own internal rewrite method.
We exposed this in 3.x as TopTermsBooleanQueryRewrite, and then as subclasses for Scoring and Boost-only variants.

The problem I have is that the PQ has a default (large) size of Integer.MAX_VALUE... of course its later limited by
the value of BooleanQuery's maxClauseCount, but I think this is a trap.

Instead its better to simply remove these defaults and force the user to provide a default (reasonable) size.
"
0,"Refactoring of the Persistence Managers. currently the persistence managers reside in:
 org.apache.jackrabbit.core.state
 org.apache.jackrabbit.core.state.db
 org.apache.jackrabbit.core.state.mem
 org.apache.jackrabbit.core.state.obj
 org.apache.jackrabbit.core.state.xml
 (org.apache.jackrabbit.core.state.util)

there are also a lot of other classes that deal with states (eg:
SharedItemStateManager) in the state package that do not relate to
pms.

i would like to move all persistencemanagers and pm related stuff to:

 org.apache.jackrabbit.core.persistence

I'd keep the current classes as deprecated subclasses within
jackrabbit-core.jar until Jackrabbit 2.0. There may (?) be people who
are extending the existing classes, so I'd avoid breaking binary
compatibility there even though we've never promised to actually honor
compatiblity within o.a.j.core."
0,JSR 283: New Event Types. 
0,Reorganize test suites. I'd like to better organize the test setup in jackrabbit-core. The current test repository is located in applications/test and managed with custom ant tasks and explicit do_init/do_test surefire configuration. It would be better to have the test repository located in target/repository (with template content coming from src/test/repository) and managed using the Maven 2 integration test lifecycle phases.
0,"[PATCH] Field.toString could be more helpful. org.apache.lucene.document.Field.toString defaults to using Object.toString
for some sensible fields. e.g. !isStored && isIndexed && !isTokenized
fields. This makes debugging slightly more difficult than is really needed.

Please find pasted below possible alternative:

 /** Prints a Field for human consumption. */
  public final String toString() {
  	StringBuffer result = new StringBuffer();
  	if (isStored) {
  		if (isIndexed) {
  			if (isTokenized) {
  				result.append(""Text"");
  			} else {
  				result.append(""Keyword"");
  			}
  		} else {
			// XXX warn on tokenized not indexed?
  			result.append(""Unindexed"");
  		}
  	} else {
  		if (isIndexed) {
  			if (isTokenized) {
  				result.append(""Unstored"");
  			} else {
  				result.append(""UnstoredUntokenized"");
  			}
  		} else {
			result.append(""Nonsense_UnstoredUnindexed"");
  		}
  	}
  	
  	result.append('<');
  	result.append(name);
  	result.append(':');
  	if (readerValue != null) {
  		result.append(readerValue.toString());
  	} else {
  		result.append(stringValue);
  	}
  	result.append('>');
  	return result.toString();
  }


NB Im working against CVS HEAD"
0,"Avoid INFINITE RECURSION when Object Model has cycles.. The default ObjectConverterImpl is restricted to acyclic graphs in the object model.

Many Java object models are NOT acyclic.   For instance, I am on your Friends list.   Yoar are on my Friends list.     Java encourages such structures.   Almost any large object model in Java will have hidden cycles.

Saving an Object Model that contains cycles using Graffito causes an infinite recursion.

Clearly, it is important to maintain a 1-to-1 correspondence between Nodes and Objects to prevent this.   In the absence of Multiple Parent Nodes, it will be necessary to use REFERENCE or UNDEFINED Items in place of the 2nd (or greater) Node representing a given Object.   My preference si that the default ObjectConverterImpl should support REFERENCE.,    Failing this, use of UNDEFINED also solves this problem and would  acceptable (as default).  Whether or not REFERENCE is used, both insertion and retrieval must provide a reasonable result.   A custom ojbect converter should be available to switch UNDEFINED to REFERENCE, or vice versa.

Also, it is probably best to keep the targeted, well-defined Nodes close to the Root Node.    This implies that the default ObjectConverterImpl should implement a Breadth-First, rather than a Depth-First, traversal of the Object Model on both insertion and retrieval.   Again, if the default is Depth-First, a custom object converter should be available that implements Breadth-First.

Admittedly, support for (2 representations) X (2 traversals) implies a drastic refactoring and/or rewriting of the ObjectConverterImpl class."
0,"InstantiatedIndex - faster but memory consuming index. Represented as a coupled graph of class instances, this all-in-memory index store implementation delivers search results up to a 100 times faster than the file-centric RAMDirectory at the cost of greater RAM consumption.

Performance seems to be a little bit better than log2n (binary search). No real data on that, just my eyes.

Populated with a single document InstantiatedIndex is almost, but not quite, as fast as MemoryIndex.    

At 20,000 document 10-50 characters long InstantiatedIndex outperforms RAMDirectory some 30x,
15x at 100 documents of 2000 charachters length,
and is linear to RAMDirectory at 10,000 documents of 2000 characters length.

Mileage may vary depending on term saturation.


"
0,"need the ability to also sort SpellCheck results by freq, instead of just by Edit Distance+freq. This issue was first noticed and reported in this Solr thread; http://lucene.472066.n3.nabble.com/spellcheck-issues-td489776.html#a489788

Basically, there are situations where it would be useful to sort by freq first, instead of the current ""sort by edit distance, and then subsort by freq if edit distance is equal""

The author of the thread suggested ""What I think would work even better than allowing a custom compareTo function would be to incorporate the frequency directly into the distance function.  This would allow for greater control over the trade-off between frequency and edit distance""

However, custom compareTo functions are not always be possible (ie if a certain version of Lucene must be used, because it was release with Solr) and incorporating freq directly into the distance function may be overkill (ie depending on the implementation)

it is suggested that we have a simple modification of the existing compareTo function in Lucene to allow users to specify if they want the existing sort method or if they want to sort by freq.

"
0,"HttpClient drops connection to the proxy when an invalid 'connection: close' directive is encountered in 'connection established' response. One of our customer is using our application to connect to our servlet using 
https.  We are using httpClient for http protocol handling.  The customer has a 
IBM proxy (see log file).  The connect failed with a null pointer exception.

The log seem to indicate that the proxy server is returning 200 for ""CONNECT"", 
but the proxy also sends a ""Connection:close"" header.  The httpClient closed 
the connection and then tried to create the SSL socket.  If the proxy server is 
incorrect in sending 200 with ""Connection:close"", then httpClient should throw 
exception for invalid state (IllegalStateException ?).

I will attach the log file."
0,Migrate to Lucene 2.3. 
0,"Improve excerpt fragments. Improve the excerpt fragments:

- If a fragment starts at the very beginning of a text, the first Word is cut off
- If a fragment does not start with the beginning of a sentence a '...' should be prepended
- If matching terms in a fragment are within range of 75 characters the fragment is extended too far and may produce a larger fragment than requested"
0,"TCK: NodeTest#testAddNodeItemExistsException fails if validation deferred until save. The test expects addNode to fail if a same-name sibling already exists.  JSR-170 allows this validation to be deferred until save.

Proposal: call save in the ""try"" block.

--- NodeTest.java       (revision 422074)
+++ NodeTest.java       (working copy)
@@ -380,6 +391,7 @@
         try {
             // try to add a node with same name again
             defaultTestNode.addNode(nodeName3, testNodeType);
+            defaultRootNode.save();
             fail(""Adding a node to a location where same name siblings are not allowed, but a node with same name"" +
                     "" already exists should throw ItemExistsException "");
         } catch (ItemExistsException e) {
"
0,"Add JMX support to register a JCR RMI Server into Jboss.  I added two classes and one descriptor file to the jcr-rmi project. These files provide support to make the generated jar deployable into a Jboss server. 

 The deployment descriptor contains two parameters, the address of the local repository instance, and the target address where the rmi server should be registered. 

e.g.

<server>
 <mbean code=""org.apache.jackrabbit.rmi.server.jmx.JCRServer""
     name=""Jackrabbit.services:RMIServer = JCR RMI Server"">
    <attribute name=""Local"">java:jcr/local</attribute>
    <attribute name=""Target"">jnp://localhost:1099/jcrServer</attribute>	
<depends>jboss.jca:service=ManagedConnectionFactory,name=jcr/local</depends>					
  </mbean>
</server>	

this configuration registers an RMI server at /jcrServer that wraps the local repository at java:jcr/local.

br,
Edgar"
0,"Default blob size for mysql ddl too small. the default datatype for:
NODE.NODE_DATA
PROP.PROP_DATA
REFS.REFS_DATA 
in the mysql ddl is ""BLOB"" which is pretty small to the default size in other dbs.

When playing with a (not very large) jackrabbit repo using mysql for persistence I easily got data truncation errors on both NODE.NODE_DATA and PROP.PROP_DATA columns. The same issue has been reported in the past by other users.
Although anyone could easily create a custom ddl with larger fields it should be nice to increase the blob size in the mysql ddl embedded in jackrabbit, in order to avoid this kind of problems for new users (you usually learn this the hard way, when the number of nodes in your repository starts to grow and jackrabbit start throwing errors :/).
Changing BLOB to MEDIUMBLOB will make the default size for mysql more similar to the one in other dbs, without critically increasing the used space...

"
0,Make ItemInfoBuilder name space aware. Currently there is no way to to have NodeInfoBuilder/PropertyInfoBuilder build NodeInfos/ItemInfos on a name space other than the default one. I suggest to add appropriate methods to NodeInfoBuilder/PropertyInfoBuilder to set the name space.
0,"Add the Data Store to the Jackrabbit API. Currently, the garbage collection is not part of the Jackrabbit API. However, the data store garbage collection must be used once in a while if the data store is enabled. I propose to add the required interfaces to the Jackrabbit API. This will also allow to call garbage collection using RMI."
0,"Some improvements to Benchmark. I've noticed that WriteLineDocTask declares it does not support multi-threading, but taking a closer look I think this is really for no good reason. Most of the work is done by reading from the ContentSource and constructing the document. If those two are mult-threaded (and I think all ContentSources are), then we can synchronize only around writing the actual document to the line file.

While investigating that, I've noticed some 1.5 TODOs and some other minor improvements that can be made. If you've wanted to make some minor improvements to benchmark, let me know :). I intend to include only minor and trivial ones."
0,"Packed ints: move .getArray into Reader API. This is a simple code cleanup... it's messy that a consumer of
PackedInts.Reader must check whether the impl is Direct8/16/32/64 in
order to get an array; it's better to move up the .getArray into the
Reader interface and then make the DirectN impls package private.
"
0,"Extract a generic framework for running randomized tests.. {color:red}The work on this issue is temporarily at github{color} (lots of experiments and tweaking):
https://github.com/carrotsearch/randomizedtesting
Or directly: git clone git://github.com/carrotsearch/randomizedtesting.git
{color}
----

RandomizedRunner is a JUnit runner, so it is capable of running @Test-annotated test cases. It
respects regular lifecycle hooks such as @Before, @After, @BeforeClass or @AfterClass, but it
also adds the following:

Randomized, but repeatable execution and infrastructure for dealing with randomness:

- uses pseudo-randomness (so that a given run can be repeated if given the same starting seed)
  for many things called ""random"" below,
- randomly shuffles test methods to ensure they don't depend on each other,
- randomly shuffles hooks (within a given class) to ensure they don't depend on each other,
- base class RandomizedTest provides a number of methods for generating random numbers, strings
  and picking random objects from collections (again, this is fully repeatable given the
  initial seed if there are no race conditions),
- the runner provides infrastructure to augment stack traces with information about the initial
  seeds used for running the test, so that it can be repeated (or it can be determined that
  the test is not repeatable -- this indicates a problem with the test case itself).

Thread control:

- any threads created as part of a test case are assigned the same initial random seed 
  (repeatability),
- tracks and attempts to terminate any Threads that are created and not terminated inside 
  a test case (not cleaning up causes a test failure),
- tracks and attempts to terminate test cases that run for too long (default timeout: 60 seconds,
  adjustable using global property or annotations),

Improved validation and lifecycle support:

- RandomizedRunner uses relaxed contracts of hook methods' accessibility (hook methods _can_ be
  private). This helps in avoiding problems with method shadowing (static hooks) or overrides
  that require tedious super.() chaining). Private hooks are always executed and don't affect
  subclasses in any way, period.
- @Listeners annotation on a test class allows it to hook into the execution progress and listen
  to events.
- @Validators annotation allows a test class to provide custom validation strategies 
  (project-specific). For example a base class can request specific test case naming strategy
  (or reject JUnit3-like methods, for instance).
- RandomizedRunner does not ""chain"" or ""suppress"" exceptions happening during execution of 
  a test case (including hooks). All exceptions are reported as soon as they happened and multiple
  failure reports can occur. Most environments we know of then display these failures sequentially
  allowing a clearer understanding of what actually happened first.
"
0,"Add support for custom ExecutorServices in ParallelMultiSearcher. Right now, the ParallelMultiSearcher uses a cachedThreadPool, which is limitless and a poor choice for a web application, given the threaded nature of the requests (say a webapp with tomcat-default 200 threads and 100 indexes could be looking at 2000 searching threads pretty easily).  Support for adding a custom ExecutorService is pretty trivial.  Patch forthcoming."
0,"Reduce memory usage of DocIds. Implementations of DocIds are used to cache parent child relations of nodes in the index. Usually there are a lot of duplicate objects because a DocId instance is used to identify the parent of a node in the index. That is, sibling nodes will all have DocIds with the same value. Currently a new DocId instance is created for each node. Caching the most recently used DocIds and reuse them might help to reduce the memory usage. Furthermore there are DocIds that could be represented with a short instead of an int when possible."
0,"gcj ant target doesn't work on windows. In order to fix it I made two changes, both really simple.

First I added to org/apache/lucene/store/GCJIndexInput.cc some code to use windows memory-mapped I/O instead than unix mmap().

Then I had to rearrange the link order in the Makefile in order to avoid unresolved symbol errors. Also to build repeatedly I had to instruct make to ignore the return code for the mkdir command as on windows it fails if the directory already exists.

I'm attaching two patches corresponding to the changes; please note that with the patches applied, the gcj target still works on linux. Both patches apply cleanly to the current svn head."
0,"Improve Benchmark. Benchmark can be improved by incorporating recent suggestions posted
on java-dev. M. McCandless' Python scripts that execute multiple
rounds of tests can either be incorporated into the codebase or
converted to Java."
0,"Contrib/Jcr-Server: Improve package structure. + org
  + apache
    + jackrabbit
       + webdav
         + <dav-specific packages as currently present>
         + spi_jcr (formerly spi)
         + spi_simple (formerly dav-package below server/simple        
         + client (webdav-client lib)

       + server
         + jcr (jcr-server-classes formerly below server)
         + simple
   
       + client
         + jcr (client-side jcr impl. without dav-dependency)
    
         
"
0,Caching client has a class for common headers that was not being used consistently in the code. The HttpCachingClient has a class called HeaderConstants that contains all the cache interesting headers that are used in the code base.  This class of string constants was not being used consistently in the code base.  The attached patch cleans this up.
0,"TermAttribute.termLength() optimization. 
   public int termLength() {
     initTermBuffer(); // This patch removes this method call 
     return termLength;
   }

I see no reason to initTermBuffer() in termLength()... all tests pass, but I could be wrong?

"
0,"Query#mergeBooleanQueries argument should be of type BooleanQuery[] instead of Query[]. The method #mergeBooleanQueries accepts Query[] and casts elements to BooleanQuery without checking. This will guarantee a ClassCastException if it is not a boolean query. We should enforce this by changing the signature. This won't really break back compat. as it only works with instances of BooleanQuery.

"
0,"Participation of a workspace in a cluster should be configurable. Currently, when clustering is enabled, every workspace participates automatically. This should be configurable in the workspace, by introducing an attribute such as ""clustered=[true|false]""."
0,Add test case support for shard searching. New test case that helps stress test the APIs to support sharding....
0,"Creation of JavaDoc fails on jackrabbit-jcr-server. The creation of JavaDoc fails on project jackrabbit-jcr-server if the command ""mvn javadoc:javadoc"" is called on the latest checkout of jackrabbit. See attached log..."
0,"nuke/clean up AtomicReader.hasNorms. implementations already have to return fieldInfos() [which can tell you this], and normValues() [which can also tell you this].

So if we want to keep it, I think it should just have a final implementation and not be required for FilterReaders, etc.

Or we can just nuke it... do we really need 3 ways to do the same thing?"
0,Remove Deprecated Benchmarking Utilities from contrib/benchmark. The old Benchmark utilities in contrib/benchmark have been deprecated and should be removed in 2.9 of Lucene.
0,"repositoryConfig should use setter for its internal components. From the mailing list (not archived at the moment):
--- Jukka's reply ---
I refactored the config classes last year but didn't change the way
the config instances are being used by Jackrabbit. In general I think
that a IoC approach (use setters to configure the Jackrabbit
components) would be better than passing config objects around and
letting the components to instantiate any subcomponents based on the
configuration. This is why I didn't really want to make the config
constructors public, otherwise we'd easily up with backwards
compatibility issues if we were to change the way configuration is
handled.
---

"
0,"TCK: Node types selected by SetValueValueFormatExceptionTest may lead to incorrect test failure. During setup, the test selects two node types, one with a BOOLEAN property and one with a DATE property, and creates nodes of these types as children of the target node.  Having the TCK select these node types creates two problems:  First, JSR-170 does not require an implementation to provide deterministic, stable ordering of node types returned by NodeTypeManager.  Consequently, the node types selected during test setup may vary from run to run, making test configuration difficult or impossible.  Second, if a repository imposes implementation-specific type constraints not discoverable through JSR-170, the TCK may select a node type inappropriate as a child of the test node.

Proposal: introduce configuration properties which, if set, override the property definition and node type selected by NodeTypeUtil.
"
0,"Handling of binary properties (streams) in QValue interface. The current SPI requires QValue to return new streams upon each call to getStream(). As far as I can tell, this essentially requires a QValue implementation to preserve the whole content of a stream, be it in memory or on disk.

In particular (and unless I'm missing something), when importing large content into a repository, this causes the whole data stream to be written twice. We really should try to avoid that.
"
0,"Documentation for tii and tis files seems to be out of sync with code. The documentation on the .tii file in fileformats.xml seems to be out of sync
with the actual code in TermInfosReader.java.

Specifically, the docs for the TermInfosIndex file seems to leave out several
fields that are read from the file in the readIndex() method (well, specifically
they're read in by the SegmentTermEnum constructor, but you get the idea)."
0,"SharedFieldCache$StringIndex memory leak causing OOM's . SharedFieldCache$StringIndex is not working properly. It is meant to cache the docnumbers in lucene along with the term to sort on. The issue is twofold. I have a solution for the second one, the first one is not really solvable from jr pov, because lucene index readers are already heavily caching Terms. 

Explanation of the problem:

For *each* unique property where is sorted on, a new lucene ScoreDocComparator is created (see SharedFieldComparator newComparator). This new comparator creates *per* lucene indexreader  SharedFieldCache.StringIndex which is stored in a WeakHashMap with as key, the indexreader . As this indexreader  almost *never* can be garbage collected (only if it is merged and thus unused after), the SharedFieldCache.StringIndex are there to be the rest of the jvm life (which is sometime short, as can be seen from the simple unittest attached).  Obviously, this results pretty fast in OOM.

1) issue one:  The cached terms[] in SharedFieldCache.StringIndex can become huge when you sort on a common property (date) which is present in a lot of nodes. It you sort on large properties, like 'title' this SharedFieldCache.StringIndex  will quickly use hundreds of Mb for a couple of hundred of thousand of nodes with a title. This issue is already a lucene issue, as lucene already caches the terms. OTOH, I really doubt whether we should index long string values as UNTOKENIZED in lucene at all. A half working solution might be a two-step solution, where the first sort is on the first 10 chars, and only if the comparator returns 0, take the entire string to sort on

2) issue two:  The cached terms[] in SharedFieldCache.StringIndex is frequently sparse, consuming an incredible amount of memory for string arrays containing mainly null values. For example (see attached unit test):

- add 1.000.000 nodes
- do a query and sort on a non existing property
- you'll loose 1.000.000 * 4 bytes ~ 4 Mb of memory
- sort on another non existing prop : another 4 Mb is lost
- do it 100 times --> 400 Mb is lost, and can't be reclaimed

I'll attach a solution which works really fine for me, still having the almost unavoidable memory absorption, but makes it much smaller. The solution is, that if < 10% of the String array is filled, i consider the array already sparse, and move to a HashMap solution. Performance does not decrease much (and in case of large sparsity increases because less memory consumption --> less gc, etc). 

Perhaps it does not seem to be a common issue (certainly the unit test) but our production environments memory snapshots indicate most memory being held by the SharedFieldCache$StringIndex (and the lucene Terms, which is harder to avoid)

I'd like to see this in the 1.5.1 if others are ok with it


"
0,"Consider making HostConfiguration immutable. HostConfiguration class should be immutable. This should also allow methods of this class to be non-synchronized.

Oleg"
0,"Proxy tunneling/auth with CONNECT for non-HTTP protocols. HttpClient would be even more useful if it supported connections tunneled 
through proxies and proxy authentication for non-HTTP protocols. E.g. Binary 
protocols such as SSH or JXTA-TCP could be tunneled through a web proxy if 
HttpClient provided access to the underlying Socket after the negotiations 
(auth, CONNECT) with the web proxy were complete."
0,"Simple Google style query. In the Sling project there's a need for a simple query language. See SLING-573.

I've created a parser that translates the simple query into an XPath query statement and executes it on a JCR workspace.

I'll commit it to the jackrabbit-jcr-commons module."
0,"[patch] Support for digest auth MD5-sess. I was attempting to access a device that requires Digest authentication using
MD5-sess, which does not seem to be supported."
0,Add memory based bundle store. 
0,"consistency check should get node ids in chunks, not rely on total count. The PM consistency checker should use the paging feature to fetch nodeIds in chunks, and also not rely on the total number of ids for logging purposes."
0,Remove DocumentWriter. DocumentWriter has been replaced by DocumentsWriter (from LUCENE-843) so we need to remove it & fix the unit tests that directly use it...
0,"Document Maven nightly builds, artifact generation, and using Maven to build Lucene/Solr. There should be documentation we can point people to when they ask how to use Maven with Lucene and Solr."
0,"Support for transactions when using JCR over RMI.. At this time, the sessions obtained from o.a.j.rmi.client.LocalAdapterFactory do not implement the methods for the XASession.  Therefor the RMI access layer does not support a transactional session."
0,"Add FileBody constructor with explicit filename. FileBody does not allow the filename field in the Content-Disposition header to be overriden, the filename taken from the File object - I have software that creates temporary files and needs to assign an implicit logical filename."
0,Best effort merge if concurrent modifications include changes to mixin types. currently the NodeStateMerger#merge method immediately aborts if the mixin types of the 2 nodes are not the same.
0,Optimize first execution queries for DescendantSelfAxisWeight/ChildAxisQuery. The first execution of a query involving DescendantSelfAxisWeight/ChildAxisQuery is slow. Consecutive queries are faster because the hierarchy is cached
0,"Use TRACE logging instead of DEBUG for the absolute nitty-gritties. [This is basically a copy of the Spring improvement request SPR-2873: http://opensource.atlassian.com/projects/spring/browse/SPR-2873 )

Given a developer situation: Much of the DEBUG information in the log of HttpClient is very un-interesting as long as it works. Some of these lines are however of much bigger importance than others (thus turning off DEBUG globally for HttpClient isn't good either).

TRACE and DEBUG are the two developer-centric logging levels of log4j and commons logging (the rest are ""production levels""). Since log4j-1.2.12, TRACE have existed. Clogging have always had trace, but before release 1.1 mapped Log.trace to log4j's DEBUG, but 1.1 (released May 9. 2006) now maps to log4j's TRACE.

I think that HttpClient's logging would benefit a lot by using TRACE level extensively, in that developers could turn all of httpclient's logging down to DEBUG, but still see ""major developer events"" like connections being opened, the request being sent, and e.g. the response's status line, size of headers and body, keep-alive vs. closing of connection.

Candidates for TRACE level include:
  * httpclient.wire.*
  * org.apache.commons.httpclient.params.DefaultHttpParams
  * org.apache.commons.httpclient.HttpMethodBase
  * .. and probably a bunch of others that doesn't bring the developer in the standard ""good flow mode"" any highly interesting information. 

Please note that I do NOT view these lines as worthless. It is however in _normal_ developer circumstances not valuable information, and it would ease development if it was possible to turn these ultra-verbose loglines off easily. When things just aren't working out, and your exciting REST-based query doesn't work out, or your charset encodings just doesn't give what you're expecting, you'd turn on TRACE to really get down to the hard core. You'd find the problem, fix it, and set it to DEBUG again.

In addition, the lines that were left on the DEBUG level should obviously be as informative as possible, and thus maybe somewhat more verbose than now, trying to ""aggregate"" some pieces of information that now are output over several DEBUG lines..

I do realize that I could achive a lot of this with a rather extensive log configuration, that also had to include raw text filters, but I do believe that this affects more developers than me!

PS: it wouldn't hurt either if all of httpclient's log-lines came from a common root, e.g. ""HttpClient"", or ""org.apache.commons.httpclient"", instead of having several roots. This would however be a somewhat ""backward incompatible"" change, since it now has (at least?) two roots."
0,"Extend contrib Highlighter to properly support PhraseQuery, SpanQuery,  ConstantScoreRangeQuery. This patch adds a new Scorer class (SpanQueryScorer) to the Highlighter package that scores just like QueryScorer, but scores a 0 for Terms that did not cause the Query hit. This gives 'actual' hit highlighting for the range of SpanQuerys, PhraseQuery, and  ConstantScoreRangeQuery. New Query types are easy to add. There is also a new Fragmenter that attempts to fragment without breaking up Spans.

See http://issues.apache.org/jira/browse/LUCENE-403 for some background.

There is a dependency on MemoryIndex."
0,Add method to set uuid in NodeInfoBuilder. 
0, Reduce number of compiler warning by adding @Override and generics where appropriate . Same as JCR-2482 for the webdav library and the modules using it.
0,"Port to Java5. For my needs I've updated Lucene so that it uses Java 5 constructs. I know Java 5 migration had been planned for 2.1 someday in the past, but don't know when it is planned now. This patch against the trunk includes :

- most obvious generics usage (there are tons of usages of sets, ... Those which are commonly used have been generified)
- PriorityQueue generification
- replacement of indexed for loops with for each constructs
- removal of unnececessary unboxing

The code is to my opinion much more readable with those features (you actually *know* what is stored in collections reading the code, without the need to lookup for field definitions everytime) and it simplifies many algorithms.

Note that this patch also includes an interface for the Query class. This has been done for my company's needs for building custom Query classes which add some behaviour to the base Lucene queries. It prevents multiple unnnecessary casts. I know this introduction is not wanted by the team, but it really makes our developments easier to maintain. If you don't want to use this, replace all /Queriable/ calls with standard /Query/.

"
0,"Request for other RMI binding options in RepositoryStartupServlet. The current deployment options for RepositoryStartupServlet bind a local repository to JNDI and/or register a remote ServerRepository via RMI using an RMIServerSocketFactory and LocateRegistry. 

The LocateRegistry mechanism does not appear to work by default in a Weblogic environment.

I would like to request the option of binding a ServerRepository in registerRMI() to JNDI using the servlets current context instead of attempting LocateRegistry.createRegistry() and then LocateRegistry.getRegistry(). This JNDI binding option could use a different name so as to not interfere with the JNDI binding attempted in registerJNDI().

For example, if requested in the web.xml config, registerRMI() could bind the following using it's reference to a ServerRepository:

      Context ctx = new InitialContext();
      ctx.bind(repositoryName + ""Remote"", remote);

This allows for easy remote access using weblogic's native T3 protocol using the following on the client with:

      Context ctx = new InitialContext(); // with say -Djava.naming.provider.url and -Djava.naming.factory.initial set 
      Object ref = ctx.lookup(repositoryName + ""Remote"");
      LocalAdapterFactory laf = new ClientAdapterFactory();
      Repository remote = laf.getRepository((RemoteRepository) ref);

From the initial tests I have done this appears to work well inside the Weblogic container. 

- Paul."
0,extract test content loading from JackrabbitRepositoryStub. discussed here: http://markmail.org/message/vl5ldnfbocccccxw
0,"Make lazy loading proxy callback Serializable. Hello (probably Christophe :) ),

It would be nice to have the CGLib callbacks Serializable, because if they're not, the proxies are not either, even if the target is.

I've seen that you've made BeanLazyLoader Serializable. Wouldn't be better to have AbstractLazyLoader Serializable, so CollectionLazyLoader is too?

What I've done is :
*) declaring AbstractLazyLoader as Serializable
*) declaring non-Serializable resources such as Session as volatile
*) throwing an IllegalStateException in BeanLazyLoader.fetch and CollectionLazyLoader.fetch if the Session is null. This case can only happen if the proxy has been serialized without the volatile Session, and it doesn't make sense to lazy load the target then.

BTW, I realized I didn't clean up the beanClassDescriptor in BeanLazyLoader.cleanUp, so I corrected this.

I'll attach the modified classes.

Sincerely,

Stphane Landelle"
0,"2.1 Locking documentation in ""Apache Lucene - Index File Formats"" section ""6.2 Lock File"" out dated. I am in the process to migrate from Lucene 2.0 to Lucene 2.1.

From reading the Changes document I understand that the write locks are now written into the index folder instead of the java.io.tmpdir. 

In the ""Apache Lucene - Index File Formats"" document in section ""6.2 Lock File"" I read that there is a write lock used to indicate that another process is writing into the index and that this file is stored in the java.io.tempdir.

This is confusing to me.  I had the impression all lock files go into the index folder now.  And using the the java.io.tempdir is only local and does not support access to shared index folders.

Do I miss something here or is the documentation not updated?
"
0,"refactoring of Similarity.sloppyFreq() and Similarity.scorePayload. Currently these are top-level, but they only affect the SloppyDocScorer.
So it makes more sense to put these into the SloppyDocScorer api, this gives you additional flexibility
(e.g. combining payloads with CSF or whatever the hell you want to do), and is cleaner.

Furthermore, there are the following confusing existing issues:
* scorePayload should take bytesref
* PayloadTermScorer passes a *null* byte[] array to the sim if there are no payloads. I don't think it should do this, and its inconsistent with PayloadNearQuery, which does not do this. Its an undocumented conditional you need to have in the scoring algorithm which we should remove.
* there is an unused constant for scorepayload (NO_DOC_ID_PROVIDED), which is a documented, but never used anywhere. I think we should remove this conditional too, because its not possible to have a payload without a docid, and we shouldn't be passing fake document ids (-1) to our scoring APIs anyway.
"
0,"Multivalued property sorted by last/random value. Sorting on multivalued property may produce incorrect result because sorting is performed only by last value of multivalued property.
Steps to reproduce:
1. Create multivalued field in repository. Example from nodetypes file:
<propertyDefinition name=""MyProperty"" requiredType=""String"" autoCreated=""false"" mandatory=""false""
   onParentVersion=""COPY"" protected=""false"" multiple=""false"">
2. Create few records so that all records except one would contain single value for MyProperty and one record would contain 
first value which is greater then of any other record and the second value is somewhere in the middle. Here is an example:
1st record: ""aaaa""
2nd record: ""cccc""
3rd record: ""dddd"", ""bbbb""
3. Run some query which sorts Example of XPath query:
//*[...here are some criteria...] order by @MyProperty ascending
The query would return documents in such order:
""aaaa""
""dddd"", ""bbbb""
""cccc""
which is not expected order (expected same order as they were entered - as ""aaaa"" < ""cccc"", ""cccc"" < ""dddd"")

After some digging I found out that it happens because method 
org.apache.jackrabbit.core.query.lucene.SharedFieldCache.getValueIndex
(called from org.apache.jackrabbit.core.query.lucene.SharedFieldSortComparator.SimpleScoreDocComparator constructor)
returns only last Comparable of the document. Here is overwrites previous value:
retArray[termDocs.doc()] = getValue(value, type);

I tried to concatenate comparables (just to check if it would work for my case):
	if(retArray[termDocs.doc()] == null) {
		retArray[termDocs.doc()] = getValue(value, type);
	} else {
		retArray[termDocs.doc()] =
				retArray[termDocs.doc()] + "" "" + getValue(value, type);
	}
But it didn't worked well either - TermEnum returns terms not in the same order as JackRabbit returns values of multivalued field
(as an example [""qwer"", ""asdf""] may become [""asdf"", ""qwer""] ). So, simple concatenation doesn't help.
"
0,"Setup nightly builds for Jackrabbit. Once the Jackrabbit zone is created (see INFRA-1008), setup nightly Jackrabbit builds as discussed in http://thread.gmane.org/gmane.comp.apache.jackrabbit.devel/9190/.
"
0,Create default repository in target. One of the JCR API tests gets a default repository from the RepositoryFactory. This default repository should use a home directory under target.
0,More verbose message on reference constraint violation. 
0,Avoid docFreq calls for non-fulltext queries. Looking up the document frequency for a query terms costs I/O and is only useful for fulltext queries (-> the document frequency has an influence on the relevance of a result node). Simple constraints like: @foo = 'bar' may return a constant value for docFreq.
0,"Move jackrabbit/trunk/contrib to jackrabbit/sandbox. As discussed on the mailing list (see http://www.nabble.com/Moving-contrib-outside-trunk-and-rename-to-sandbox-tf4635301.html), we should do the following:

    svn move https://svn.apache.org/repos/asf/jackrabbit/trunk/contrib https://svn.apache.org/repos/asf/jackrabbit/sandbox

I will do this in a few days unless anyone objects."
0,"Make QValueFactoryImpl extensible. The class is currently final and other modules therefore copied code. This kind of duplication it hard to maintain and should be avoided.

If QValueFactoryImpl would be designed to be extensible then other classes could reuse much of the code."
0,"Core: Misleading method naming with Workspace and Session (move283, copy283, clone283). there seems to be leftovers from previous drafts of the jsr 283 within jackrabbit core, namely:

- WorkspaceImpl#clone283
- WorkspaceImpl#copy283
- WorkspaceImpl#move283
- SessionImpl#move283

giving the impression that with jsr 283 those method would return 'String'. this however is not the case.
therefore i suggest to remove those methods again."
0,"Redesign virtual host API. HttpClient is ignoring an explicity set host.  e.g. if you set the host like
client.getHostConfiguration().setHost(""127.0.0.1"") then execute a method looking
up say http://google.com then the program will connect to google.com rather than
the localhost.

The fix that works for me:
diff -Naur
../../t2/commons-httpclient/src/java/org/apache/commons/httpclient/HttpClient.java
src/java/org/apache/commons/httpclient/HttpClient.java
---
../../t2/commons-httpclient/src/java/org/apache/commons/httpclient/HttpClient.java
2005-12-22 01:06:54.000000000 +1300
+++ src/java/org/apache/commons/httpclient/HttpClient.java	2005-12-22
19:13:30.000000000 +1300
@@ -383,7 +383,9 @@
         if (hostconfig == defaulthostconfig || uri.isAbsoluteURI()) {
             // make a deep copy of the host defaults
             hostconfig = new HostConfiguration(hostconfig);
-            if (uri.isAbsoluteURI()) {
+	    // if the host is explicity set already (e.g. to the IP of the virtual host
+	    // on which we are executing a method), just leave it
+            if (uri.isAbsoluteURI()  && hostconfig.getHost()==null) {
                 hostconfig.setHost(uri);
             }

Note: Why do we care that the host is specified?  Why not just use the uri
authority?  In my case I have a virtual host running on several servers/IPs and
I need to make sure the request goes through to a specific IP and the response
that comes back is for the virtual host I am testing."
0,"Provide query support  for WEAKREFERENCE reverse lookup. the current implementation of Node.getWeakReferences() and getWeakReferences(String) uses a fulltext query in order to find weak references to a particular node.

this requires the PlainTextExtractor to be enabled in the Search config, e.g. :

    <param name=""textFilterClasses"" value=""org.apache.jackrabbit.extractor.PlainTextExtractor""/>

providing 'native' WEAKREFERNCE reverse lookup in Jackrabbit's QOM implementation would be certainly more efficient.

"
0,"[API Doc] Compile new preference architecture and HTTP parameterization guide. Document the new preference architecture based on the hierarchy of HttpParams
collections, as well as available parameters and options"
0,"Replace NodeReferencesId with NodeId. The NodeReferencesId class simply wraps a NodeId and forwards all essential method calls to it.

The main (only?) benefit of having NodeReferencesId as a separate class is the ability to distinguish between the overloaded exists() and load() method signatures on PersistenceManager. The downside is the need to instantiate all the NodeReferencesId wrapper objects whenever accessing the references to a node.

I propose to rename the overloaded methods to hasReferencesTo(NodeId) and getReferencesTo(NodeId) and to replace the NodeReferencesId with just the target NodeId wherever used.
"
0,"Mandatory authentication prevents webdav client connections. As seen on the mailing list:

java -jar target/jackrabbit-standalone-2.3-SNAPSHOT.jar --cli http://localhost:8080
Exception in thread ""main"" javax.jcr.RepositoryException: Unable to access a repository with the following settings:
   org.apache.jackrabbit.repository.uri: http://localhost:8080
The following RepositoryFactory classes were consulted:
   org.apache.jackrabbit.jcr2dav.Jcr2davRepositoryFactory: declined
   org.apache.jackrabbit.jcr2spi.Jcr2spiRepositoryFactory: declined
   org.apache.jackrabbit.commons.JndiRepositoryFactory: declined
   org.apache.jackrabbit.core.RepositoryFactoryImpl: declined
   org.apache.jackrabbit.rmi.repository.RmiRepositoryFactory: failed
       because of RepositoryException: Failed to read the resource at URL http://localhost:8080
       because of IOException: Server returned HTTP response code: 401 for URL: http://localhost:8080
Perhaps the repository you are trying to access is not available at the moment.
       at org.apache.jackrabbit.commons.JcrUtils.getRepository(JcrUtils.java:216)
       at org.apache.jackrabbit.commons.JcrUtils.getRepository(JcrUtils.java:256)
       at org.apache.jackrabbit.standalone.Main.run(Main.java:127)
       at org.apache.jackrabbit.standalone.Main.main(Main.java:61)"
0,Make all classes that have a close() methods instanceof Closeable (Java 1.5). This should be simple.
0,"Move contribs/modules away from QueryParser dependency. Some contribs and modules depend on the core QueryParser just for simplicity in their tests.  We should apply the same process as I did to the core tests, and move them away from using the QueryParser where possible."
0,"Some small fixes to contrib/benchmark. I've fixed a few small issues I've hit in contrib/benchmark.

First, this alg was only doing work on the first round.  All
subsequent rounds immediately finished:

{code}
analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
doc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker
work.dir = /lucene/work
docs.file=work/reuters.lines.txt
doc.maker.forever=false
directory=FSDirectory
doc.add.log.step=3000

{ ""Rounds""
  ResetSystemErase
  CreateIndex
  { ""AddDocs"" AddDoc > : *
  CloseIndex
  NewRound
} : 3
{code}

I think this is because we are failing to reset ""exhausted"" to false
in PerfTask.doLogic(), so I added that.  Plus I had to re-open the
file in LineDocMaker.

Second, I made a small optimization to not call updateExhausted unless
any of the child tasks are TaskSequence or ResetInputsTask (which I
compute up-front).

Finally, we were not allowing flushing by RAM and doc count, so I
fixed the logic in Create/OpenIndexTask to set both RAMBufferSizeMB
and MaxBufferedDocs.
"
0,have jackrabbit-core produce a test jar. I'm writing a custom FileSystem implementation and it would be nice to be able to reuse AbstractFileSystemTest.
0,"Provide access to cluster records. Cluster records are read/written inside o.a.j.core.cluster.ClusterNode in private methods. In order to support tools such as a journal walker that would display human readable descriptions of cluster records, these inner workings should be made public. "
0,fix for Document.getBoost() documentation. The attached patch fixes the javadoc to make clear that getBoost() will never return a useful value in most cases. I will commit this unless someone has a better wording or a real fix.
0,"New Analysis  Contributions. With the advent of the new TeeTokenFilter and SinkTokenizer, there now exists some interesting new things that can be done in the analysis phase of indexing.  See LUCENE-1058.

This patch provides some new implementations of SinkTokenizer that may be useful."
0,"[PATCH] tests use 12 for month which is invalid. tests create calendar with 12 as a month, which is invalid. December is 11, so use Calendar.DECEMBER instead. - patch fixes this."
0,"MultiReader should not use PQ for its Term/sEnum if it has only 1 reader. Related to LUCENE-2130....

Even though we've switched to segment-based searching, there are still times when the Term/sEnum is used against the top-level reader.  I think Solr does this, and from LUCENE-2130, certain rewrite modes of MTQ will do this as well.

Currently, on an optimized index, MTQ is still using a PQ to present the terms, which is silly because this just adds a sizable amount of overhead.  In such cases we should simply delecate to the single segment.

Note that the single segment can have deletions, and we should still delegate.  Ie, the index need not be optimized, just have a single segment."
0,Move Function grouping collectors from Solr to grouping module. Move the Function*Collectors from Solr (inside Grouping source file) to grouping module.
0,Provide a method for writing name space declarations in CompactNodeTypeDefWriter. Currently CompactNodeTypeDefWriter includes (when configured to do so) only name space declarations from name spaces actually used in the node type definitions written. In some situations it is necessary to write additional name space declarations. I thus propose to add a method writeNamespaceDelclaration.
0,"IndexWriter.addIndexes can make any incoming segment into CFS if it isn't already. Today, IW.addIndexes(Directory) does not modify the CFS-mode of the incoming segments. However, if IndexWriter's MP wants to create CFS (in general), there's no reason why not turn the incoming non-CFS segments into CFS. We anyway copy them, and if MP is not against CFS, we should create a CFS out of them.

Will need to use CFW, not sure it's ready for that w/ current API (I'll need to check), but luckily we're allowed to change it (@lucene.internal).

This should be done, IMO, even if the incoming segment is large (i.e., passes MP.noCFSRatio) b/c like I wrote above, we anyway copy it. However, if you think otherwise, speak up :).

I'll take a look at this in the next few days."
0,"BaseTokenStreamTestCase should test analyzers on real-ish content. We already have LineFileDocs, that pulls content generated from europarl or wikipedia... I think sometimes BTSTC should test the analyzers on that as well."
0,"Similarity#score deprecated method - javadoc reference + SimilarityDelegator. Old method  

  public float scorePayload(String fieldName, byte [] payload, int offset, int length)

has been deprecated by - 

  public float scorePayload(int docId, String fieldName, int start, int end, byte [] payload, int offset, int length)


References in PayLoadNearQuery (javadoc) changed. 

Also - SimilarityDelegator overrides the new method as opposed to the (deprecated) old one. "
0,"Enable bzip compression in benchmark. bzip compression can aid the benchmark package by not requiring extracting bzip files (such as enwiki) in order to index them. The plan is to add a config parameter bzip.compression=true/false and in the relevant tasks either decompress the input file or compress the output file using the bzip streams.
It will add a dependency on ant.jar which contains two classes similar to GZIPOutputStream and GZIPInputStream which compress/decompress files using the bzip algorithm.

bzip is known to be superior in its compression performance to the gzip algorithm (~20% better compression), although it does the compression/decompression a bit slower.

I wil post a patch which adds this parameter and implement it in LineDocMaker, EnwikiDocMaker and WriteLineDoc task. Maybe even add the capability to DocMaker or some of the super classes, so it can be inherited by all sub-classes."
0,"IndexWriter.getReader() allocates file handles. I am not sure if this is a ""bug"" or really just me not reading the Javadocs right...

The IR returned by IW.getReader() leaks file handles if you do not close() it, leading to starvation of the available file handles/process. If it was clear from the docs that this was a *new* reader and not some reference owned by the writer then this would probably be ok. But as I read the docs the reader is internally managed by the IW, which at first shot lead me to believe that I shouldn't close it.

So perhaps the docs should be amended to clearly state that this is a caller-owns reader that *must* be closed? Attaching a simple app that illustrates the problem."
0,"Use AtomicReaderContext also for CustomScoreProvider. When moving to AtomicReaderContext, one place was not changed to use it: CustomScoreQuery's CustomScoreProvider. It should also take AtomicReaderContext instead of IndexReader, as this may help users to effectively implement custom scoring there absolute DocIds are needed."
0,"FuzzyLikeThisQuery should set MaxNonCompetitiveBoost for faster speed. FuzzyLikeThisQuery uses FuzzyTermsEnum directly, and maintains 
a priority queue for its purposes.

Just like TopTermsRewrite method, it should set the 
MaxNonCompetitiveBoost attribute, so that FuzzyTermsEnum can
run faster. Its already tracking the minScore, just not updating
the attribute.

This would be especially nice as it appears to have nice defaults
already (pq size of 50)
"
0,release is not signed. there are no signatures & checksums available for your latest release
0,"Should USE_EXPECT_CONTINUE be false by default?. It seems the point of USE_EXPECT_CONTINUE is to improve performance when posting large data. 
http://www.w3.org/Protocols/rfc2616/rfc2616-sec8.html says:

<< The purpose of the 100 (Continue) status (see section 10.1.1) is to allow a client that is sending a request message with a request body to determine if the origin server is willing to accept the request (based on the request headers) before the client sends the request body. In some cases, it might either be inappropriate or highly inefficient for the client to send the body if the server will reject the message without looking at the body. >>

There's nothing wrong with HttpClient performing well by default, however, every other HTTP client library I've used does not behave like this (PHP curl, Perl LWP). The default is always to do one request, including the body. Maybe dumb, but simple.

It seems to me HttpClient's default behavior should the simplest, most compatible with all HTTP-speaking services out there. ""100 Continue"" is somewhat advanced, and may not be correctly implemented by all services. (That's of course how I found out about it -- my server doesn't implement it.)

If USE_EXPECT_CONTINUE is used only for performance reasons, it seems like it would be simpler (and therefore maybe more ""correct"") to have it ""off"" by default. And only enable it when needed, when there is a good reason to.

Just my thoughts. And a wish. Thanks! 


"
0,JSR 283 Query. 
0," MalformedCookieException: distinguish cookie syntax errors from cross-domain errors. MalformedCookieException is used for both cookies with syntax errors,
and for cookies which are invalid for the particular context - e.g.
cross-domain cookies.

I think it would be helpful to be able to distinguish these without
needing to examine the message text."
0,Added the functionality to Map and Manage Type Enum. OCM API does not come with a mapper that can map Type Enum.  I have added this functionality.  Attached patch has test cases that tests the feature for Simple fields and Collection fields (For both anotations and digester based implementations)
0,"Contribution: Efficient Sorting of DateField/DateTools Encoded Timestamp Long Values. Hello Tim,

As promised, the sort functionality for ""long"" values is included in the
attached files.

patchTestSort.txt contains the diff info. for my modifications to the
TestSort.java class

org.apache.lucene.search.ZIP contains the three new class files for
efficient sorting of ""long"" field values and of encoded timestamp
field values as ""long"" values.

Let me know if you have any questions.

Regards,
Rus"
0,"Allow customizing/subclassing of DirectoryReader. DirectoryReader is final and has only static factory methods. It is not possible to subclass it in any way.

The problem is mainly Solr, as Solr accesses directory(), IndexCommits,... and therefore cannot work on abstract IndexReader anymore. This should be changed, by e.g. handling reopening in the IRFactory, also versions, commits,... Currently its not possible to implement any other IRFactory that returns something else.

On the other hand, it should be possible to ""wrap"" a DirectoryReader / CompositeReader to handle filtering of collection based information (subreaders, reopening hooks,...). This can be done by making DirectoryReader abstract and let DirectoryReader.open return a internal hidden class ""StandardDirectoryReader"". This is similar to the relatinship between IndexReader and hidden DirectoryReader in the past.

DirectoryReader will have final implementations of most methods like getting document stored fields, global docFreq and other statistics, but allows hooking into doOpenIfChanged. Also it should not be limited to SegmentReaders as childs - any AtomicReader is fine. This allows users to create e.g. a Directory-based ParallelReader (see LUCENE-3736) that supports reopen and (partially commits)."
0,"Add some more constants for newer Java versions to Constants.class, remove outdated ones.. Preparation for LUCENE-3235:
This adds constants to quickly detect Java6 and Java7 to Constants.java. It also deprecated and removes the outdated historical Java versions."
0,"wrong exception from NativeFSLockFactory (LIA2 test case). As part of integrating Lucene In Action 2 test cases (LUCENE-2661), I found one of the test cases fail

the test is pretty simple, and passes on 3.0. The exception you get instead (LockReleaseFailedException) is 
pretty confusing and I think we should fix it.
"
0,"Add PlainTextExtractor to default configuration of TransientRepository. The current default configuration of TransientRepository does not include the PlainTextExtractor, which means resources of type text/plain are not fulltext indexed. The PlainTextExtractor should be added to the configuration."
0,"Add log.step support per task. Following LUCENE-1774, this will add support for log.step per task name, rather than a single log.step setting for all tasks. The .alg file will support:
* log.step - for all tasks.
* log.step.<Task Class Name> - for a specific task. For example, log.step.AddDoc, or log.step.DeleteDoc

I will post the patch soon"
0,Improved Kuromoji search mode segmentation/decompounding. Kuromoji has a segmentation mode for search that uses a heuristic to promote additional segmentation of long candidate tokens to get a decompounding effect.  This heuristic has been improved.  Patch is coming up.
0,"PostMethod Java doc refers to wrong section of RFC1945. ""The HTTP POST method is defined in section 9.5 of RFC1945"" should read ""The
HTTP POST method is defined in section 8.3 of RFC1945""

Change 9.5 to 8.3."
0,JSR 283: Introduce Event.getDate(). JSR 283 adds a method to an Event that returns the date when the change happened that caused the event.
0,"remove contrib/misc and contrib/wordnet's dependencies on analyzers module. These contribs don't actually analyze any text.

After this patch, only the contrib/demo relies upon the analyzers module... we can separately try to figure that one out (I don't think any of these lucene contribs needs to reach back into modules/)

"
0,"building trunk  fails with javacc plugin version 2.2. 

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[23,38] cannot find symbol
symbol: class JCRSQLParserVisitor
class DefaultParserVisitor implements JCRSQLParserVisitor {

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[25,24] cannot find symbol
symbol  : class SimpleNode
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[29,24] cannot find symbol
symbol  : class ASTQuery
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[33,24] cannot find symbol
symbol  : class ASTSelectList
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[37,24] cannot find symbol
symbol  : class ASTFromClause
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[41,24] cannot find symbol
symbol  : class ASTWhereClause
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTPredicate.java:[21,34] cannot find symbol
symbol: class SimpleNode
public class ASTPredicate extends SimpleNode {

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[49,24] cannot find symbol
symbol  : class ASTOrExpression
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[53,24] cannot find symbol
symbol  : class ASTAndExpression
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[57,24] cannot find symbol
symbol  : class ASTNotExpression
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[61,24] cannot find symbol
symbol  : class ASTBracketExpression
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTLiteral.java:[19,32] cannot find symbol
symbol: class SimpleNode
public class ASTLiteral extends SimpleNode {

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTIdentifier.java:[21,35] cannot find symbol
symbol: class SimpleNode
public class ASTIdentifier extends SimpleNode {

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[73,24] cannot find symbol
symbol  : class ASTOrderByClause
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTContainsExpression.java:[21,43] cannot find symbol
symbol: class SimpleNode
public class ASTContainsExpression extends SimpleNode {

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[81,24] cannot find symbol
symbol  : class ASTOrderSpec
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[85,24] cannot find symbol
symbol  : class ASTAscendingOrderSpec
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[89,24] cannot find symbol
symbol  : class ASTDescendingOrderSpec
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[93,24] cannot find symbol
symbol  : class ASTLowerFunction
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[97,24] cannot find symbol
symbol  : class ASTUpperFunction
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[101,24] cannot find symbol
symbol  : class ASTExcerptFunction
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTPredicate.java:[37,22] cannot find symbol
symbol  : class JCRSQLParser
location: class org.apache.jackrabbit.core.query.sql.ASTPredicate

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTPredicate.java:[82,26] cannot find symbol
symbol  : class JCRSQLParserVisitor
location: class org.apache.jackrabbit.core.query.sql.ASTPredicate

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTLiteral.java:[30,22] cannot find symbol
symbol  : class JCRSQLParser
location: class org.apache.jackrabbit.core.query.sql.ASTLiteral

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTLiteral.java:[54,28] cannot find symbol
symbol  : class JCRSQLParserVisitor
location: class org.apache.jackrabbit.core.query.sql.ASTLiteral

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTIdentifier.java:[29,23] cannot find symbol
symbol  : class JCRSQLParser
location: class org.apache.jackrabbit.core.query.sql.ASTIdentifier

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTIdentifier.java:[42,26] cannot find symbol
symbol  : class JCRSQLParserVisitor
location: class org.apache.jackrabbit.core.query.sql.ASTIdentifier

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTContainsExpression.java:[31,33] cannot find symbol
symbol  : class JCRSQLParser
location: class org.apache.jackrabbit.core.query.sql.ASTContainsExpression

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTContainsExpression.java:[54,28] cannot find symbol
symbol  : class JCRSQLParserVisitor
location: class org.apache.jackrabbit.core.query.sql.ASTContainsExpression

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/lucene/LuceneQueryBuilder.java:[44,56] cannot find symbol
symbol  : class QueryParser
location: package org.apache.jackrabbit.core.query.lucene.fulltext

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/lucene/LuceneQueryBuilder.java:[45,56] cannot find symbol
symbol  : class ParseException
location: package org.apache.jackrabbit.core.query.lucene.fulltext

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[55,42] cannot find symbol
symbol: class XPathVisitor
public class XPathQueryBuilder implements XPathVisitor, XPathTreeConstants {

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[55,56] cannot find symbol
symbol: class XPathTreeConstants
public class XPathQueryBuilder implements XPathVisitor, XPathTreeConstants {

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[23,35] cannot find symbol
symbol: class Node
public class SimpleNode implements Node {

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[24,14] cannot find symbol
symbol  : class Node
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[25,14] cannot find symbol
symbol  : class Node
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[27,14] cannot find symbol
symbol  : class XPath
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[33,22] cannot find symbol
symbol  : class XPath
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[39,33] cannot find symbol
symbol  : class XPath
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[39,18] cannot find symbol
symbol  : class Node
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[49,29] cannot find symbol
symbol  : class Node
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[53,11] cannot find symbol
symbol  : class Node
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[57,28] cannot find symbol
symbol  : class Node
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[68,11] cannot find symbol
symbol  : class Node
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[79,28] cannot find symbol
symbol  : class XPathVisitor
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[86,33] cannot find symbol
symbol  : class XPathVisitor
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[132,29] cannot find symbol
symbol  : class Token
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/lucene/JQOM2LuceneQueryBuilder.java:[53,56] cannot find symbol
symbol  : class QueryParser
location: package org.apache.jackrabbit.core.query.lucene.fulltext

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/lucene/fulltext/FastCharStream.java:[30,45] cannot find symbol
symbol: class CharStream
public final class FastCharStream implements CharStream {

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[60,43] cannot find symbol
symbol: class JCRSQLParserVisitor
public class JCRSQLQueryBuilder implements JCRSQLParserVisitor {

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[81,18] cannot find symbol
symbol  : class ASTQuery
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[121,31] cannot find symbol
symbol  : class ASTQuery
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[201,24] cannot find symbol
symbol  : class SimpleNode
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[206,24] cannot find symbol
symbol  : class ASTQuery
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[279,24] cannot find symbol
symbol  : class ASTSelectList
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[297,24] cannot find symbol
symbol  : class ASTFromClause
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[311,24] cannot find symbol
symbol  : class ASTWhereClause
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[447,24] cannot find symbol
symbol  : class ASTOrExpression
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[459,24] cannot find symbol
symbol  : class ASTAndExpression
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[469,24] cannot find symbol
symbol  : class ASTNotExpression
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[479,24] cannot find symbol
symbol  : class ASTBracketExpression
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[496,24] cannot find symbol
symbol  : class ASTOrderByClause
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[505,24] cannot find symbol
symbol  : class ASTOrderSpec
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[526,24] cannot find symbol
symbol  : class ASTAscendingOrderSpec
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[531,24] cannot find symbol
symbol  : class ASTDescendingOrderSpec
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[556,24] cannot find symbol
symbol  : class ASTLowerFunction
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[566,24] cannot find symbol
symbol  : class ASTUpperFunction
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[576,24] cannot find symbol
symbol  : class ASTExcerptFunction
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTPredicate.java:[87,15] cannot find symbol
symbol  : variable super
location: class org.apache.jackrabbit.core.query.sql.ASTPredicate

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTLiteral.java:[59,15] cannot find symbol
symbol  : variable super
location: class org.apache.jackrabbit.core.query.sql.ASTLiteral

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTIdentifier.java:[47,15] cannot find symbol
symbol  : variable super
location: class org.apache.jackrabbit.core.query.sql.ASTIdentifier

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/lucene/LuceneQueryBuilder.java:[376,12] cannot find symbol
symbol  : class QueryParser
location: class org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/lucene/LuceneQueryBuilder.java:[376,37] cannot find symbol
symbol  : class QueryParser
location: class org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/lucene/LuceneQueryBuilder.java:[378,31] cannot find symbol
symbol  : variable QueryParser
location: class org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/lucene/LuceneQueryBuilder.java:[440,17] cannot find symbol
symbol  : class ParseException
location: class org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[267,12] cannot find symbol
symbol  : class XPath
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[269,26] cannot find symbol
symbol  : class XPath
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[271,33] cannot find symbol
symbol  : class XPath
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[283,17] cannot find symbol
symbol  : class ParseException
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[356,17] cannot find symbol
symbol  : variable JJTXPATH2
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[359,17] cannot find symbol
symbol  : variable JJTROOT
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[360,17] cannot find symbol
symbol  : variable JJTROOTDESCENDANTS
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[368,17] cannot find symbol
symbol  : variable JJTSTEPEXPR
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[415,17] cannot find symbol
symbol  : variable JJTNAMETEST
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[429,17] cannot find symbol
symbol  : variable JJTELEMENTNAMEORWILDCARD
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[432,41] cannot find symbol
symbol  : variable JJTANYNAME
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[437,17] cannot find symbol
symbol  : variable JJTTEXTTEST
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[443,17] cannot find symbol
symbol  : variable JJTTYPENAME
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[456,17] cannot find symbol
symbol  : variable JJTOREXPR
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[463,17] cannot find symbol
symbol  : variable JJTANDEXPR
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[470,17] cannot find symbol
symbol  : variable JJTCOMPARISONEXPR
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[473,17] cannot find symbol
symbol  : variable JJTSTRINGLITERAL
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[474,17] cannot find symbol
symbol  : variable JJTDECIMALLITERAL
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[475,17] cannot find symbol
symbol  : variable JJTDOUBLELITERAL
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[476,17] cannot find symbol
symbol  : variable JJTINTEGERLITERAL
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[480,40] cannot find symbol
symbol  : variable JJTINTEGERLITERAL
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[490,17] cannot find symbol
symbol  : variable JJTUNARYMINUS
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[497,17] cannot find symbol
symbol  : variable JJTFUNCTIONCALL
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[500,17] cannot find symbol
symbol  : variable JJTORDERBYCLAUSE
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[505,17] cannot find symbol
symbol  : variable JJTORDERMODIFIER
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder
"
0,"IndexWriter does not do the right thing when a Thread is interrupt()'d. Spinoff from here:

    http://www.nabble.com/Deadlock-with-concurrent-merges-and-IndexWriter--Lucene-2.4--to22714290.html

When a Thread is interrupt()'d while inside Lucene, there is a risk currently that it will cause a spinloop and starve BG merges from completing.

Instead, when possible, we should allow interruption.  But unfortunately for back-compat, we will need to wrap the exception in an unchecked version.  In 3.0 we can change that to simply throw InterruptedException."
0,"allow SPI implementation to compute default values for autocreated properties. Currently, when creating nodes in transient space, JCR2SPI uses hard-wired logic trying to populate system generated properties such as jcr:created, jcr;uuid and so on.

This is problematic as

- it doesn't scale -- it fails for autocreated properties not known to JCR2SPI, and

- the syntax for the defaults may be dependant on the back end, such as legal syntax for (UU)IDs.

Proposal:

- extend QValueFactory with something like

  QValue computeDefaultValue(QPropertyDefinition)

- use that in JCR2SPI, getting rid of the currently hard-wired logic.
"
0,"Tests failing when run with tests.iter > 1. TestMultiLevelSkipList and TestsFieldReader are falling if run with -Dtests.iter > 1 - not all values are reset though
I will attach a patch in a second."
0,"Add a TypeTokenFilter. It would be convenient to have a TypeTokenFilter that filters tokens by its type, either with an exclude or include list. This might be a stupid thing to provide for people who use Lucene directly, but it would be very useful to later expose it to Solr and other Lucene-backed search solutions."
0,HttpMethodBase Javadoc patches. Clarify that HTTP 1.1 is the default. Attaching.
0,"DisjunctionMaxQuery - Type safety  . DisjunctionMaxQuery code has containers that are not type-safe . The comments indicate type-safety though. 

Better to express in the API and the internals the explicit type as opposed to type-less containers. 

Patch attached. 

Comments / backward compatibility concerns welcome.  "
0,Add support for boolean values to QValue. I suggest to add support for reading and writing boolean values to QValue and QValueFactory. I find it strange that there is such support for the other data types but booleans must be constructed via strings. 
0,"BasicAuthenticatonExample.java in CVS. From the example in CVS Revision 1.1.2.1 the code below would lead you to 
belive that setCredentials uses (""HOST"", ""REALM"", credientials). It actually 
should be (""REALM"", ""HOST"", credientials). It looks like it was correct in the 
Revision 1.1 and was changed with Revision 1.1.2.1

// pass our credentials to HttpClient, they will only be used for
// authenticating to servers with realm ""realm"", to authenticate agains
// an arbitrary realm change this to null.
client.getState().setCredentials(
            ""www.verisign.com"",
            ""realm"",
            new UsernamePasswordCredentials(""username"", ""password"")
);"
0,"JCR2SPI: add JNDI support. adding jndi support to jcr2spi was one of the improvements that came up during the f2f.
julian volunteered to take a look at it."
0,"fix or deprecate TermsEnum.skipTo. This method is a trap: it looks legitimate but it has hideously poor performance (simple linear scan implemented in the TermsEnum base class since none of the concrete impls override it with a more efficient implementation).

The least we should do for 2.9 is deprecate the method with  a strong warning about its performance.

See here for background: http://www.lucidimagination.com/search/document/77dc4f8e893d3cf3/possible_terminfosreader_speedup

And, here for historical context: 

http://www.lucidimagination.com/search/document/88f1b95b404ebf16/remove_termenum_skipto_term_target"
0,"MultiThreadedConnectionManager Accounting Problems. getConnectionsInPool() is certainly a more intutive name. 
At the same, as you already mentioned, certainly there need to be a connection killer method: 
MultiThreadedHttpConnectionManager.destroyIdleConnections(long idleTime) 

Also, I would recommend one method which could spit out connection statistics at any time for the 
given Connection Manager. This will be great method for testing purpose as well. 
MultiThreadedHttpConnectionManager.displayCurrentStatistics(); 
----------------------------------
Curent Connection Statistics
----------------------------------
Total connectinos in Pool = 10
Open connectinos          = 3
Close connections         = 5
Stale connections         = 2 
And, if are even more adventurous we could extend our report to: 
Average wait time for connection = 1356 ms
Maximum wait time for connectino = 1892 ms"
0,"Prevent logins during repository shutdown. Related to the last comment in JCR-445, should we prevent new sessions from being created during repository shutdown? It is an odd chance to run into a problem like that, but it seems like the issue could be easily solved by making getWorkspaceInfo() synchronized and adding sanityCheck() calls to the createSession() methods."
0,"Move to jackrabbit.apache.org. Jackrabbit will be moving to

   http://jackrabbit.apache.org/

There are a number of infrastructure tasks that will need to be done by Roy.

There will also be a need to change our documentation and site to point to the new URL
and mailing list addresses, which can be done by anyone.

The existing mailing lists will be moved

    jackrabbit-dev at incubator  -->  dev at jackrabbit.apache.org
    jackrabbit-commits at incubator  -->  commits at jackrabbit.apache.org

and I will add

   users at jackrabbit.apache.org
"
0,"Add myqsql ddl for clustering (DatabaseJournal). the default ddl for clustering does't work with mysql, so it would be nice to include a mysql specific ddl also for clustering."
0,"Wrong method signatures in AbstractHttpClient. The method signatures for removeRequestInterceptorByClass and removeResponseInterceptorByClass in AbstractHttpClient are wrong. Must be

public void removeRequestInterceptorByClass(Class<? extends
HttpRequestInterceptor> clazz);

and

public void removeResponseInterceptorByClass(Class<? extends
HttpRequestInterceptor> clazz);"
0,"Use an enumeration for QOM join types. Like JCR-2094 but for join types. The join type constants in the PFD version of QueryObjectModelConstants are broken, and a type-safe enumeration would in any case be a good alternative to the string constants."
0,"Scorer.skipTo(current) remains on current for some scorers. Background in http://www.nabble.com/scorer.skipTo%28%29-contr-tf3880986.html

It appears that several scorers do not strictly follow the spec of Scorer.skipTo(n), and skip to current location remain in current location whereas the spec says: ""beyond current"". 

We should (probably) either relax the spec or fix the implementations."
0,"entities and connection handling incomplete. 1. entities can not be explicitly disconnected from the underlying stream
2. entities do not tell whether they have an underlying stream

patch follows

cheers,
  Roland"
0,"Move JCRWebdavServerServlet to jcr-server and make it abstract. In line with isse JCR-417, I suggest to partially move the JCRWebdavServerServlet from the jcr-webapp project to the jcr-server project. By partially I mean, that the new (moved) servlet will be abstract and the getRepository() method will be abstract. The jcr-webapp project will still contain a JCRWebdavServerServlet (for backwards compatibility maintaing the same name) which just extends the new servlet and implements the getRepository() method using the RepositoryAccess servlet.

This allows for the reuse of the jcr-server project including the abstract JCRWebdavServerServlet in other environments.
"
0,"Update to Lucene 2.4.1. Lucene 2.4 contains a couple of performance improvements.

See: http://lucene.apache.org/java/2_4_0/changes/Changes.html"
0,Move NodeTypeStorage to spi-commons and provide default implementation. This facilitates the implementation of NodeTypes methods of RepositoryService.
0,"Improve reading of cached UUID for given document number. CachingIndexReader.document(int n, FieldSelector fieldSelector) creates a new
Field from the cached UUID. The lucene Field implementation always does a
String.intern() on the field name, which is quite slow. We should probably have
our own implementation for that specific use case where we know that the name
is already interned. e.g. UUIDField implements Fieldable."
0,SPI sandbox: use tests-jars introduced with JCR-1629 and JCR-1683. ... and remove the duplicated tests.
0,"Enable flexible scoring. This is a first step (nowhere near committable!), implementing the
design iterated to in the recent ""Baby steps towards making Lucene's
scoring more flexible"" java-dev thread.

The idea is (if you turn it on for your Field; it's off by default) to
store full stats in the index, into a new _X.sts file, per doc (X
field) in the index.

And then have FieldSimilarityProvider impls that compute doc's boost
bytes (norms) from these stats.

The patch is able to index the stats, merge them when segments are
merged, and provides an iterator-only API.  It also has starting point
for per-field Sims that use the stats iterator API to compute boost
bytes.  But it's not at all tied into actual searching!  There's still
tons left to do, eg, how does one configure via Field/FieldType which
stats one wants indexed.

All tests pass, and I added one new TestStats unit test.

The stats I record now are:

  - field's boost

  - field's unique term count (a b c a a b --> 3)

  - field's total term count (a b c a a b --> 6)

  - total term count per-term (sum of total term count for all docs
    that have this term)

Still need at least the total term count for each field.
"
0,"Allow other string distance measures in spellchecker. Updated spelling code to allow for other string distance measures to be used.

Created StringDistance interface.
Modified existing Levenshtein distance measure to implement interface (and renamed class).
Verified that change to Levenshtein distance didn't impact runtime performance.
Implemented Jaro/Winkler distance metric
Modified SpellChecker to take distacne measure as in constructor or in set method and to use interface when calling.
"
0,"New Gump projects for HttpComponents 4.0. Create new Gump definitions for the 4.0 code base, both core and client.
There are other Maven-based projects in Gump to learn from, for example Apollo and Excalibur.

"
0,"allow an alg file to specify the default codec. I already committed this one by accident so I better open an issue!

I added this:

  default.codec = Pulsing

so that your alg file can specify the codec to be used when writing new segments in an index."
0,"Cleanup suggester API. Currently the suggester api and especially TermFreqIterator don't play that nice with BytesRef and other paradigms we use in lucene, further the java iterator pattern isn't that useful when it gets to work with TermsEnum, BytesRef etc. We should try to clean up this api step by step moving over to BytesRef including the Lookup class and its interface..."
0,"review TSCCM for spurious wakeups. Review the code of the TSCCM/ConnPoolByRoute for places where spurious wakeups may happen.
Verify that this case is dealt with correctly. Unit test by giving invalid wakeup signals?"
0,"[PATCH] fixes for gcj target.. I've modified the Makefile so that it compiles with GCJ-4.0.

This involved fixing the CORE_OBJ macro to match the generated jar file as well
as excluding FieldCacheImpl from being used from its .java source (GCJ has
problems with anonymous inner classes, I guess).

Also, I changed the behaviour of FieldInfos.fieldInfo(int). It depended on
catching IndexOutOfBoundsException exception. I've modified it to test the
bounds first, returning -1 in that case. This helps with gcj since we build with
-fno-bounds-check.

I compiled with;

GCJ=gcj-4.0 GCJH=gcjh-4.0 GPLUSPLUS=g++-4.0 ant clean gcj

patch to follow."
0,"Improve payload error handling/reporting. If you try to load a payload more than once you get the exception:  IOException(""Payload cannot be loaded more than once for the same term position."");

You also get this exception if their is no payload to load, and its a bit confusing, as the message doesn't relate to the actual problem."
0,"add db connection autoConnect for BundleDbPersistenceManager.. Since bundled db pm doesn't inherited from database pm, it can't reconnect once database is bounced. it would be nice to add this feature. "
0,"allow automatontermsenum to work on full byte range. AutomatonTermsEnum is really agnostic to whats in your byte[], only that its in binary order.
so if you wanted to use this on some non-utf8 terms, thats just fine.

the patch just does some code cleanup and removes ""utf8"" references, etc.
additionally i changed the pkg-private, lucene-internal byte-oriented ctor, to public, lucene.experimental.
"
0,"OracleFileSystem uses getClass().getResourceAsStream to load schema file. org.apache.jackrabbit.core.fs.db.OracleFileSystem loads the schema via getClass().getResourceAsStream(...).
This makes it impossible to extend the class without either copying the schema ddl file, or overwriting checkSchema(...),
as the schema file is not accessible.

The solution is to use OracleFilesystem.class.getResourceAsStream(...).

See JCR-595 which fixed this already for org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager."
0,"Typo in repository.xml. There's another typo in repository.xml. This is basically the same as JCR-1460, but for the SearchIndex element at the bottom of the repository.xml."
0,"Transaction-safe versioning. I've been working on a partial fix to JCR-630. Instead of implementing fully transactional versioning (i.e. a checkin will disappear when a transactin is rolled back), I'm ensuring that all versioning operations within a transaction will leave the version store in a consistent state even if the transaction otherwise fails at any point."
0,"Refactor segmentInfos from IndexReader into its subclasses. References to segmentInfos in IndexReader cause different kinds of problems
for subclasses of IndexReader, like e. g. MultiReader.

Only subclasses of IndexReader that own the index directory, namely 
SegmentReader and MultiSegmentReader, should have a SegmentInfos object
and be able to access it.

Further information:
http://www.gossamer-threads.com/lists/lucene/java-dev/51808
http://www.gossamer-threads.com/lists/lucene/java-user/52460

A part of the refactoring work was already done in LUCENE-781"
0,"Jcr-Server: Improve implementation of DavResource#getProperty(DavPropertyName). this issue has already been described in JCR-397

problem: even if only a subset of properties has been requested by the client dav resource initializes the complete set of properties."
0,"Utility class to tranform JCR-SQL2 to/from JCR-JQOM. The JCR2 doc specify that both contain the same thing and can be translated from one to another
in a straightforward manner. The jackrabbit-jcr-commons module should offer a utility class to transform
from one language to another in a generic way, 

for exemple :
- String toSQL2(QueryObjectModel qom)
- QueryObjectModel toJQOM(QueryObjectModelFactory factory, String query)"
0,"Some contrib packages are missing a package.html. Dunno if we will get to this one this release, but a few contribs don't have a package.html (or a good overview that would work as a replacement) - I don't think this is hugely important, but I think it is important - you should be able to easily and quickly read a quick overview for each contrib I think.

So far I have identified collation and spatial."
0,Move tests that require indexing configuration. Tests that require an indexing configuration should be moved to the indexing-test workspace. This allows us to remove the indexing configuration from the default workspace and speed up the JCR API tests.
0,"ReorderReferenceableSNSTest failure. I have checked out the Jackrabbit 1.4 branch to a new directory, and called:

mvn clean install

The error is:

Building Jackrabbit JCR to SPI
  task-segment: [clean, install]
---------------------------------
...
testRevertReorder(org.apache.jackrabbit.jcr2spi.ReorderReferenceableSNSTest)
junit.framework.AssertionFailedError: Reorder added a child node.
       at junit.framework.Assert.fail(Assert.java:47)
       at org.apache.jackrabbit.jcr2spi.ReorderTest.testOrder(ReorderTest.java:90)
       at org.apache.jackrabbit.jcr2spi.ReorderTest.testRevertReorder(ReorderTest.java:122)
"
0,"backport suggest module to branch 3.x. It would be nice to develop a plan to expose the autosuggest functionality to Lucene users in 3.x

There are some complications, such as seeing if we can backport the FST-based functionality,
which might require a good bit of work. But I think this would be well-worth it.
"
0,"Access control for repository level API operations. it is a open issue (i guess since jackrabbit 1.0) that the repository level write operations lack any kind of permission check.
this issues has been raised during specification of jsr 283 [1] but didn't made it into the specification (left to implementation).

in jackrabbit 2.0 this affects the following parts of the API

- namespace registration
- node type registration
- workspace creation/removal

based on a issue reported by david (""currently an anonymous user can write the namespace registry which is probably
undesirable [...]""), we could at least add some minimal restrictions. In addition i would like to take up this discussion
for jsr 333.

[1] https://jsr-283.dev.java.net/issues/show_bug.cgi?id=486"
0,"Skip sync delay when changes are found. The cluster synchronization on a slave does always wait for some time (as specified in the sync delay) before fetching changes. If a lot of changes are being written to the master, a slave will considerably fall behind the master in term of revisions, which may endanger the integrity of the cluster if the master will crash. I therefore suggest that a slave should rather immediately contact the master again after some changes have been found, until it sees no more changes."
0,Typos in jcr-server io package javadocs. 
0,"Cleanup some unused and unnecessary code. Several classes in trunk have some unused and unnecessary code. This includes unused fields, unused automatic variables, unused imports and unnecessary assignments. Attached it a patch to clean these up."
0,"URI class constructors need revision, optimization. 1. Currently there's not way to pass an escaped string as a parameter to URI
class. As a result the url parameter in HttpMethodBase#HttpMethodBase(String)
constructor gets converted into an array of char just to be converted back to
string in URI contructor called in that method. 

2. The overall design of URI class contructors does not appear very coherent (at
least to me)"
0,JSR 283: Simple versioning. 
0,"when tests fail, sometimes the testmethod in 'reproduce with' is null. an example is the recent fail: https://builds.apache.org/job/Lucene-3.x/680/

it would be better to not populate -Dtestmethod with anything here..."
0,"core analyzers should not produce tokens > N (100?) characters in length. Discussion that led to this:

  http://www.gossamer-threads.com/lists/lucene/java-dev/56103

I believe nearly any time a token > 100 characters in length is
produced, it's a bug in the analysis that the user is not aware of.

These long tokens cause all sorts of problems, downstream, so it's
best to catch them early at the source.

We can accomplish this by tacking on a LengthFilter onto the chains
for StandardAnalyzer, SimpleAnalyzer, WhitespaceAnalyzer, etc.

Should we do this in 2.3?  I realize this is technically a break in
backwards compatibility, however, I think it must be incredibly rare
that this break would in fact break something real in the application?"
0,"Typo on query parser syntax web page.. On the web page http://lucene.apache.org/java/docs/queryparsersyntax.html#N10126 the text says:

""To search for documents that must contain ""jakarta"" and may contain ""lucene"" use the query:""

The example says:

+jakarta apache

The problem:
The example uses apache where the text says lucene."
0,"clean up obselete information on the website. When searching for information on 'lucene indexing speed' I get back some really out of date stuff:
1. on the features page it proudly proclaims 20MB/minute, on some really old hardware. I think we should
change this to 95GB/hour: http://blog.mikemccandless.com/2010/09/lucenes-indexing-is-fast.html
2. there are ancient benchmarks results from versioned data we link to the website. We list versioned
websites for ancient versions going back to 1.4.3. Also i noticed when just casually googling for
API documentation I tend to get results going to these ancient versions. I think we should remove
stuff for all versions prior to 2.9"
0,"SQL2 parser: Support CAST. Some CAST(...) data conversions are not yet implemented, for example String to Decimal."
0,"Disallow the use of SecureProtocolSocketFactory with ProxyClient. ProxyClient cannot work correctly if SecureProtocolSocketFactory socket factory
is being used to establish connection with the target server"
0,"wire logger skips empty line. When logging with 
org.apache.commons.logging.simplelog.log.httpclient.wire=debug, HttpConnection 
skips one line of server output in logs -- CRLF line between headers and body."
0,"Add relative path parameter to rep:excerpt(). This allows one to create an excerpt not just for the node associated with a result node, but also for a node relative to the result node."
0,"Deprecate non-pooled bundle DB persistence managers. In JCR-1456 and Jackrabbit 2.0 we introduced database connection pooling, but decided to keep the existing database bundle persistence managers intact to avoid potential regressions. We haven't seen such problems even though pooled bundle persistence has been the default since the 2.0 release, so I think it would be safe to deprecate all the non-pooled bundle DB PMs.

And in order to remove duplicate code (that has already complicated some changes within o.a.j.persistence), I'd also take the extra step of  making the o.a.j.p.bundle.* classes extend respective the o.a.j.p.pool.* classes. This would automatically allow also old non-pooled configurations to benefit from connection pooling."
0,"Performance improvement for merging stored, compressed fields. Hello everyone,

currently the merging of stored, compressed fields is not optimal for the following reason: every time a stored, compressed field is being merged, the FieldsReader uncompresses the data, hence the FieldsWriter has to compress it again when it writes the merged fields data (.fdt) file. The uncompress/compress step is unneccessary and slows down the merge performance significantly.

This patch improves the merge performance by avoiding the uncompress/compress step. In the following I give an overview of the changes I made:
   * Added a new FieldSelectorResult constant named ""LOAD_FOR_MERGE"" to org.apache.lucene.document.FieldSelectorResult
   * SegmentMerger now uses an FieldSelector to get stored fields from the FieldsReader. This FieldSelector's accept() method returns the FieldSelectorResult ""LOAD_FOR_MERGE"" for every field.
   * Added a new inner class to FieldsReader named ""FieldForMerge"", which extends  org.apache.lucene.document.AbstractField. This class holds the field properties and its data. If a field has the FieldSelectorResult ""LOAD_FOR_MERGE"", then the FieldsReader creates an instance of ""FieldForMerge"" and does not uncompress the field's data.
   * FieldsWriter checks if the field it is about to write is an instanceof FieldsReader.FieldForMerge. If true, then it does not compress the field data.


To test the performance I index about 350,000 text files and store the raw text in a stored, compressed field in the lucene index. I use a merge factor of 10. The final index has a size of 366MB. After building the index, I optimize it to measure the pure merge performance.

Here are the performance results:

old version:
   * Time for Indexing:  36.7 minutes
   * Time for Optimizing: 4.6 minutes

patched version:
   * Time for Indexing:  20.8 minutes
   * Time for Optimizing: 0.5 minutes

The results show that the index build time improved by about 43%, and the optimizing step is more than 8x faster. 

A diff of the final indexes (old and patched version) shows, that they are identical. Furthermore, all junit testcases succeeded with the patched version. 

Regards,
  Michael Busch"
0,"Add LuSql project to ""Apache Lucene - Contributions"" wiki page. Add [LuSql|http://lab.cisti-icist.nrc-cnrc.gc.ca/cistilabswiki/index.php/LuSql] to the Apache Lucene - Contributions page [http://lucene.apache.org/java/2_9_0/contributions.html]
I am the author of LuSql. I can supply any text needed. 

Perhaps a new heading is needed to capture Database/JDBC oriented Lucene tools (there are others out there)?"
0,"Add tool to upgrade all segments of an index to last recent supported index format without optimizing. Currently if you want to upgrade an old index to the format of your current Lucene version, you have to optimize your index or use addIndexes(IndexReader...) [see LUCENE-2893] to copy to a new directory. The optimize() approach fails if your index is already optimized.

I propose to add a custom MergePolicy to upgrade all segments to the last format. This MergePolicy could simply also ignore all segments already up-to-date. All segments in prior formats would be merged to a new segment using another MergePolicy's optimize strategy.

This issue is different from LUCENE-2893, as it would only support upgrading indexes from previous Lucene versions in-place using the official path. Its a tool for the end user, not a developer tool.

This addition should also go to Lucene 3.x, as we need to make users with pre-3.0 indexes go the step through 3.x, else they would not be able to open their index with 4.0. With this tool in 3.x the users could safely upgrade their index without relying on optimize to work on already-optimized indexes."
0,"Redefine HttpClient vs HttpMultiClient interface for 2.0. In particular the HttpClient/HttpMultiClient issue must be resolved. 
HttpultiClient functionality should be prefered, but HttpClient is the most
suitable name.  Consider impact to other projects.  Is java1.1 compatability
really an issue anymore?"
0,"Set source and output encoding in POMs. Modification to POM files to explicitly set the source and report encoding. I've set everything to UTF-8, but this may not be appropriate. However, the encoding properties should be set to ensure that source files are compiled correctly, resources are filtered appropriately and that the reports are using a consistent encoding.

Related info
http://docs.codehaus.org/display/MAVENUSER/POM+Element+for+Source+File+Encoding
http://docs.codehaus.org/display/MAVEN/Reporting+Encoding+Configuration"
0,move wordnet based synonym code out of contrib/memory and into contrib/wordnet (or somewhere else). see LUCENE-387 ... some synonym related code has been living in contrib/memory for a very long time ... it should be refactored out.
0,"Add fixed size DocValues int variants & expose Arrays where possible. currently we only have variable bit packed ints implementation. for flexible scoring or loading field caches it is desirable to have fixed int implementations for 8, 16, 32 and 64 bit. "
0,"typos on FAQ. I found out the following typos on the FAQ (http://lucene.sourceforge.net/cgi-
bin/faq/faqmanager.cgi) of lucene:
in 8. Will Lucene work with my Java application ?
- felxible
- applciations"
0,"Use ConcurrentHashMap instead of HashMap wherever thread-safe access is needed. Consider using ConcurrentHashMap instead of HashMap for any Maps that are used by multiple threads.

For example SchemeRegistry and AuthSchemeRegistry."
0,"Make SessionProvider pluggable in JCRWebdavServerServlet. Although there's a SessionProvider interface in o.a.j.server, the SessionProviderImpl implementation class is hard-coded into JCRWebdavServerServlet."
0,"Duplicate log of HTTP header. The HTTP header line:

""HTTP/1.1 200 OK[\r][\n]"" 

is duplicated in the wire logs. Seems to be because the line is logged at:

HttpParser [line: 131] - readLine(InputStream, String)

and at:

HttpMethodBase [line: 1980] - readStatusLine(HttpState, HttpConnection)

It looks like the latter log should be removed?"
0,"add getFinalOffset() to TokenStream. If you add multiple Fieldable instances for the same field name to a document, and you then index those fields with TermVectors storing offsets, it's very likely the offsets for all but the first field instance will be wrong.

This is because IndexWriter under the hood adds a cumulative base to the offsets of each field instance, where that base is 1 + the endOffset of the last token it saw when analyzing that field.

But this logic is overly simplistic.  For example, if the WhitespaceAnalyzer is being used, and the text being analyzed ended in 3 whitespace characters, then that information is lost and then next field's offsets are then all 3 too small.  Similarly, if a StopFilter appears in the chain, and the last N tokens were stop words, then the base will be 1 + the endOffset of the last non-stopword token.

To fix this, I'd like to add a new getFinalOffset() to TokenStream.  I'm thinking by default it returns -1, which means ""I don't know so you figure it out"", meaning we fallback to the faulty logic we have today.

This has come up several times on the user's list."
0,"HttpClient Manual Simplified Chinese. During Nov 2010, The Chinese user Nanlei translated the Apache HttpClient manual into Chinese and contribute it to HttpClient project freely. In the future, the Chinese translation of HttpCore manual will be finished and contribute to HttpCore project freely too. "
0,"Improve the use of isDeleted in the indexing code. A spin off from here: http://www.nabble.com/Some-thoughts-around-the-use-of-reader.isDeleted-and-hasDeletions-td23931216.html.
Two changes:
# Optimize SegmentMerger work when a reader has no deletions.
# IndexReader.document() will no longer check if the document is deleted.

Will post a patch shortly"
0,"TextFilters get called three times within checkin() method. If you want to add a PDF document to a repository using a PdfTextFilter, and you do the following steps:

session.save()
node.checkin();

The method PdfTextFilter.doFilter() gets called 4 times!!!

session's save method calls doFilter one time. This is normal

But checkin method calls doFilter three times. Is this normal? I do not see the sense.

------------------

		
Marcel Reutegger 	
<marcel.reutegger@gmx.net> to jackrabbit-dev
	 More options	  11:43 am (13 minutes ago)
Hi Martin,

this is unfortunate and should be improved. the reason why this happens
is the following:
the search index implementation always indexes a node as a whole to
improve query performance. that means even if a single property changes
the parent node with all its properties is re-indexed.

unfortunately the checkin method sets properties in three separate
'transactions', causing the search to re-index the according node three
times.

usually this is not an issue, because the index implementation keeps a
buffer for pending index work. that is, if you change the same property
several times and save after each setProperty() call, it won't actually
get re-indexed several times. but text filters behave differently here,
because they extract the text even though the text will never be used.

eventually this will improve without any change to the search index
implementation, because as soon as versioning participates properly in
transactions there will only be one call to index a node on checkin().

as a quick fix we could improve the text filter classes to only parse
the binary when the returned reader is acutally used."
0,"not all applicable URIs are invalidated on PUT/POST/DELETEs that pass through client cache. ""Some HTTP methods MUST cause a cache to invalidate an entity. This is either the entity referred to by the Request-URI, or by the Location or Content-Location headers (if present). These methods are: PUT, DELETE, POST.""

http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.10

The current caching implementation only invalidates the Request URI, and not those present in the Location or Content-Location headers on the request.

I have a patch that fixes this which I will upload momentarily.
"
0,"Remove segments with all documents deleted in commit/flush/close of IndexWriter instead of waiting until a merge occurs.. I do not know if this is a bug in 2.9.0, but it seems that segments with all documents deleted are not automatically removed:

{noformat}
4 of 14: name=_dlo docCount=5
  compound=true
  hasProx=true
  numFiles=2
  size (MB)=0.059
  diagnostics = {java.version=1.5.0_21, lucene.version=2.9.0 817268P - 2009-09-21 10:25:09, os=SunOS,
     os.arch=amd64, java.vendor=Sun Microsystems Inc., os.version=5.10, source=flush}
  has deletions [delFileName=_dlo_1.del]
  test: open reader.........OK [5 deleted docs]
  test: fields..............OK [136 fields]
  test: field norms.........OK [136 fields]
  test: terms, freq, prox...OK [1698 terms; 4236 terms/docs pairs; 0 tokens]
  test: stored fields.......OK [0 total field count; avg ? fields per doc]
  test: term vectors........OK [0 total vector count; avg ? term/freq vector fields per doc]
{noformat}

Shouldn't such segments not be removed automatically during the next commit/close of IndexWriter?

*Mike McCandless:*
Lucene doesn't actually short-circuit this case, ie, if every single doc in a given segment has been deleted, it will still merge it [away] like normal, rather than simply dropping it immediately from the index, which I agree would be a simple optimization. Can you open a new issue? I would think IW can drop such a segment immediately (ie not wait for a merge or optimize) on flushing new deletes.
"
0,"Remove deprecated charset support from Greek and Russian analyzers. This removes the deprecated support for custom charsets.

One thing I found is that once these charsets are removed, RussianLowerCaseFilter is the same as LowerCaseFilter.
So I marked it deprecated to be removed in 3.1
"
0,"Deprecate NamespaceListener and AbstractNamespaceResolver. The NamespaceListener interface is no longer used with the JSR 283 style namespace handling that avoids lots of the synchronization that was previously to keep the local namespace mappings up to date.

Also, the only (remaining) purpose of the AbstractNamespaceResolver class is to add support for managing NamespaceListeners. Since that functionality is nowhere used anymore, we can make all subclasses use the NamespaceResolver interface directly.

Since NamespaceListener and AbstractNamespaceResolver are public in jackrabbit-spi-commons, I will for now only mark them as deprecated. We can get rid of them in Jackrabbit 2.0."
0,"remove unused code in SmartChineseAnalyzer hmm pkg. there is some unused code in the hmm package.

I would like to remove it before I supply a fix for LUCENE-1817.

only after this can we refactor any of this analyzer, otherwise we risk breaking custom dictionary support."
0,More javadocs for Weight. From Doug's reply of 21 Feb 2005 in bug 31841
0,"JCR2SPI: Move test execution to SPI2JCR. proposed patches see  issue JCR-1629

this allows to
- remove dependency to jackrabbit-spi2jcr and jackrabbit-core from jcr2spi
- remove duplicated tests from sandbox/spi"
0,"Auto method retrial broken. Folks,
While working on the exception handling guide for the 3.0-alpha2 release I
stumbled upon a problem with HttpTimeoutException and its subclasses. In 3.0a1
HttpTimeoutException subclasses HttpRecoverableException which causes HTTP
methods failed due to a connect or socket timeout to be automatically retried. 

[INFO] HttpMethodDirector - -Recoverable exception caught when processing request
[INFO] HttpMethodDirector - -Recoverable exception caught when processing request
[INFO] HttpMethodDirector - -Recoverable exception caught when processing request
[INFO] HttpMethodDirector - -Recoverable exception caught when processing request
[WARN] HttpMethodDirector - -Recoverable exception caught but
MethodRetryHandler.retryMethod() returned false, rethrowing exception
org.apache.commons.httpclient.IOTimeoutException: Read timed out
	at
org.apache.commons.httpclient.HttpConnection$WrappedInputStream.handleException(HttpConnection.java:1350)
	at
org.apache.commons.httpclient.HttpConnection$WrappedInputStream.read(HttpConnection.java:1360)
	at java.io.FilterInputStream.read(FilterInputStream.java:66)
	at java.io.PushbackInputStream.read(PushbackInputStream.java:120)
	at org.apache.commons.httpclient.HttpParser.readRawLine(HttpParser.java:76)
	at org.apache.commons.httpclient.HttpParser.readLine(HttpParser.java:104)
	at org.apache.commons.httpclient.HttpConnection.readLine(HttpConnection.java:1054)
	at
org.apache.commons.httpclient.HttpMethodBase.readStatusLine(HttpMethodBase.java:1785)
	at
org.apache.commons.httpclient.HttpMethodBase.readResponse(HttpMethodBase.java:1546)
	at org.apache.commons.httpclient.HttpMethodBase.execute(HttpMethodBase.java:977)
	at
org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:383)
	at
org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:164)
	at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:437)
	at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:324)
	at Test.main(Test.java:13)
Caused by: java.net.SocketTimeoutException: Read timed out
	at java.net.SocketInputStream.socketRead0(Native Method)
	at java.net.SocketInputStream.read(SocketInputStream.java:129)
	at java.net.SocketInputStream.read(SocketInputStream.java:182)
	at
org.apache.commons.httpclient.HttpConnection$WrappedInputStream.read(HttpConnection.java:1358)
	... 13 more
Exception in thread ""main"" 

This probably is not what we want. Besides, for non-idempotent methods this may
simply be fatal and result in all sorts of unpleasant side-effects.

One possibilty that I personally favour is to make HttpTimeoutException class
extend IOException instead of HttpRecoverableException. There are others. The
question is whether timeout exceptions should be considered recoverable from the
conseptual standpoint. What do you think?

Oleg"
0,"Use the new Jackrabbit parent POM. Now that we have an official release of the new org.apache.jackrabbit:parent:2 POM, we should start using that as the parent of all Jackrabbit builds."
0,"move PerFieldCodecWrapper into codecs package. PerFieldCodecWrapper is a codec, but its 'hardwired' as lucene's only codec currently (except for PreFlex/3.x case)

it lets you choose a format for the postings lists per-field.

I think we should move this to the codecs package as a start... just a rote refactor."
0,Improve multihome support. MultihomePlainSocketFactory is basically broken and should be deprecated. Multihome logic needs to be moved to the DefaultClientConnectionOperator
0,"Dependency URL broken for commons-logging. On http://jakarta.apache.org/commons/httpclient/dependencies.html there is a 
typo in the href to the logging dependency, this should be

http://jakarta.apache.org/commons/logging/"
0,Remaining contrib testcases should use Version based ctors instead of deprecated ones. Many testcases in contrib use deprecated ctors for WhitespaceTokenizer / Analyzer etc.
0,"Add convenient constructor to PerFieldAnalyzerWrapper for Dependency Injection. It would be good if PerFieldAnalyzerWrapper had a constructor which took in an analyzer map, rather than having to repeatedly call addAnalyzer -- this would make it much easier/cleaner to use this class in e.g. Spring XML configurations.

Relatively trivial change, patch to be attached."
0,"Deprecate RepositoryService.getPropertyInfo method. I would like to deprecate and ultimately remove the RepositoryService.getPropertyInfo method and extend RepositoryService.getItemInfos to take over that functionality. getItemInfos would thus change to

    /**
     * Method used to 'batch-read' from the persistent storage. It returns the
     * <code>ItemInfo</code> for the given <code>ItemId</code> as the first
     * element in the <code>Iterator</code>. In addition the iterator may contain
     * arbitrary <code>ItemInfo</code>s.
     */
    public Iterator<? extends ItemInfo> getItemInfos(SessionInfo sessionInfo, ItemId itemId) throws ItemNotFoundException, RepositoryException;

"
0,"improve how IndexWriter uses RAM to buffer added documents. I'm working on a new class (MultiDocumentWriter) that writes more than
one document directly into a single Lucene segment, more efficiently
than the current approach.

This only affects the creation of an initial segment from added
documents.  I haven't changed anything after that, eg how segments are
merged.

The basic ideas are:

  * Write stored fields and term vectors directly to disk (don't
    use up RAM for these).

  * Gather posting lists & term infos in RAM, but periodically do
    in-RAM merges.  Once RAM is full, flush buffers to disk (and
    merge them later when it's time to make a real segment).

  * Recycle objects/buffers to reduce time/stress in GC.

  * Other various optimizations.

Some of these changes are similar to how KinoSearch builds a segment.
But, I haven't made any changes to Lucene's file format nor added
requirements for a global fields schema.

So far the only externally visible change is a new method
""setRAMBufferSize"" in IndexWriter (and setMaxBufferedDocs is
deprecated) so that it flushes according to RAM usage and not a fixed
number documents added.
"
0,"Check for boundary conditions in FieldInfos. In FieldInfos there are three methods in which we don't check for
boundary conditions but catch e. g. an IndexOutOfBoundsException
or a NPE. I think this isn't good code style and is probably not
even faster than checking explicitly.

""Exceptions should not be used to alter the flow of a program as 
part of normal execution.""

Also this can be irritating when you're trying to debug an 
IndexOutOfBoundsException that is thrown somewhere else in your
program and you place a breakpoint on that exception.

The three methods are:

  public int fieldNumber(String fieldName) {
    try {
      FieldInfo fi = fieldInfo(fieldName);
      if (fi != null)
        return fi.number;
    }
    catch (IndexOutOfBoundsException ioobe) {
      return -1;
    }
    return -1;
  }
  

  public String fieldName(int fieldNumber) {
    try {
      return fieldInfo(fieldNumber).name;
    }
    catch (NullPointerException npe) {
      return """";
    }
  }
  
  
  public FieldInfo fieldInfo(int fieldNumber) {
    try {
      return (FieldInfo) byNumber.get(fieldNumber);
    }
    catch (IndexOutOfBoundsException ioobe) {
      return null;
    }
  }"
0,"Add SimpleText codec. Inspired by Sahin Buyrukbilen's question here:

  http://www.lucidimagination.com/search/document/b68846e383824653/how_to_export_lucene_index_to_a_simple_text_file#b68846e383824653

I made a simple read/write codec that stores all postings data into a
single text file (_X.pst), looking like this:

{noformat}
field contents
  term file
    doc 0
      pos 5
  term is
    doc 0
      pos 1
  term second
    doc 0
      pos 3
  term test
    doc 0
      pos 4
  term the
    doc 0
      pos 2
  term this
    doc 0
      pos 0
END
{noformat}

The codec is fully funtional -- all Lucene & Solr tests pass with
-Dtests.codec=SimpleText -- but, its performance is obviously poor.

However, it should be useful for debugging, transparency,
understanding just what Lucene stores in its index, etc.  And it's a
quick way to gain some understanding on how a codec works...
"
0,"UUIDDocId cache does not work properly because of weakReferences in combination with new instance for combined indexreader . Queries that use ChildAxisQuery or DescendantSelfAxisQuery make use of getParent() functions to know wether the parents are correct and if the result is allowed. The getParent() is called recursively for every hit, and can become very expensive. Hence, in DocId.UUIDDocId, the parents are cached. 

Currently,  docId.UUIDDocId's are cached by having a WeakRefence to the CombinedIndexReader, but, this CombinedIndexReader is recreated all the time, implying that a gc() is allowed to remove the 'expensive' cache.

A much better solution is to not have a weakReference to the CombinedIndexReader, but to a reference of each indexreader segment. This means, that in getParent(int n) in SearchIndex the return 

return id.getDocumentNumber(this) needs to be replaced by return id.getDocumentNumber(subReaders[i]); and something similar in CachingMultiReader. 

That is all. Obviously, when a node/property is added/removed/changed, some parts of the cached DocId.UUIDDocId will be invalid, but mainly small indexes are updated frequently, which obviously are less expensive to recompute."
0,"OpenBitSet.prevSetBit(). Find a previous set bit in an OpenBitSet.
Useful for parent testing in nested document query execution LUCENE-2454 ."
0,"Change log level in UserManagerImpl#getAuthorizable(NodeImpl) and UserImporter#handlePropInfo. This is current implementation:

Authorizable getAuthorizable(NodeImpl n) throws RepositoryException {
        Authorizable authorz = null;
        if (n != null) {
            String path = n.getPath();
            if (n.isNodeType(NT_REP_USER) && Text.isDescendant(usersPath, path)) {
                authorz = createUser(n);
            } else if (n.isNodeType(NT_REP_GROUP) && Text.isDescendant(groupsPath, path)) {
                authorz = createGroup(n);
            } else {
                /* else some other node type or outside of the valid user/group
                   hierarchy  -> return null. */
                log.debug(""Unexpected user nodetype "" + n.getPrimaryNodeType().getName());
            }
        } /* else no matching node -> return null */
        return authorz;
    }


It seems that 'else' branch can be improved, at least by increasing log level. But I think, that best way is to throw exception.
Current message can also be misleading, in case when user type is correct but check Text.isDescendant fails.

Above method is called from within UserImporter#handlePropInfo

...
Authorizable a = userManager.getAuthorizable(parent);
if (a == null) {
     log.debug(""Cannot handle protected PropInfo "" + protectedPropInfo + "". Node "" + parent + "" doesn't represent a valid Authorizable."");
     return false;
} 
....

Here again log level is debug. Because at this point we have return statement, property 'principalName' is not set, and if we try to save session following exception will be thrown:

javax.jcr.nodetype.ConstraintViolationException: /home/public/users/b/bb2: mandatory property {internal}password does not exist
     at org.apache.jackrabbit.core.ItemSaveOperation.validateTransientItems(ItemSaveOperation.java:537)
     at org.apache.jackrabbit.core.ItemSaveOperation.perform(ItemSaveOperation.java:216)
     at org.apache.jackrabbit.core.session.SessionState.perform(SessionState.java:200)
     at org.apache.jackrabbit.core.ItemImpl.perform(ItemImpl.java:91)
     at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:329)
    ...
 

So if the log level is not set to 'debug' it is not obvious why mentioned property is missing. Use case and root cause is that 'path' (/home/public/users/b/bb2)  is not descendant of 'usersPath' (/home/users).

Regards,
Miroslav"
0,"CookiePolicy.registerCookieSpec(CookiePolicy.DEFAULT, <Some CookieSpec>); does not work as documented. I use HtmlUnit to access some quote information. Cookies shall be ignored,so I
set the default policy to CookiePolicy.IGNORE_COOKIES. Nevertheless I get error
messages after starting my program. 

The code reads:
---------------------------------------
CookiePolicy.registerCookieSpec(CookiePolicy.DEFAULT, IgnoreCookiesSpec.class);
final WebClient webClient = new WebClient();
final URL url = new URL(""http://de.finance.yahoo.com/q?s=CB3569.SG"");
final HtmlPage page = (HtmlPage) webClient.getPage(url);
----------------------------------------

The error messages are:
----------------------------------------
WARNUNG: Cookie rejected: ""$Version=0; PRF=&t=CB3569.SG; $Domain=finance.yahoo.c
om; $Path=/"". Domain attribute ""finance.yahoo.com"" violates RFC 2109: domain mus
t start with a dot
30.11.2005 10:28:20 org.apache.commons.httpclient.HttpMethodBase processResponse
Headers
WARNUNG: Cookie rejected: ""$Version=0; B=7fkoh9l1oqs4h&b=3&s=hb; $Domain=.yahoo.
com; $Path=/"". Domain attribute "".yahoo.com"" violates RFC 2109: host minus domai
n may not contain any dots
----------------------------------------"
0,"Reduce exposure of nightly build documentation. From LUCENE-1157  -

 ..the nightly build documentation is too prominent. A search for ""indexwriter api"" on Google or Yahoo! returns nightly documentation before released documentation.

(https://issues.apache.org/jira/browse/LUCENE-1157?focusedCommentId=12565820#action_12565820)
"
0,"Create ScoreNode on demand in SortedLuceneQueryHits. ScoreNodes are current created for the full result fetch. Instead, the ScoreNodes should be created on demand when requested in nextScoreNode()."
0,"JCR2SPI: add configurable cache for Item instances (ItemManager). Currently the ItemManager implementation uses a simple map with weak keys (ItemState) and weak values (Item) as cache.
Marcel recently suggested to replace this with a more sophisticated cache mechanism that can be configured."
0,"upgrade icu to 4.8. we should upgrade from 4.6 to 4.8.

some internal methods became public, also a package-private reflection hack can be removed."
0,"Optimize refresh operations . With the current implementation (recursive) refresh operations cause a full traversal of the sub-tree rooted at the item causing the refresh. This is potentially expensive. 

Instead of invalidating each item in the respective sub-tree I propose to mark the root of the sub-tree as invalidated. Such a mark would include a time stamp. Also individual items would be time stamped with their resolution time. When an item is accessed, it would check if its resolution time stamp is older than the latest invalidation time stamp. If so, it checks whether the invalidation applies to it at all (by traversing up the path) and if so it would re-resolve itself. In any case its resolution time stamp will be updated.

This approach would make invalidation much cheaper without putting much additional load to simple item access. Moreover most of the additional load (traversing up the path) only applies when an invalidation is pending."
0,"DerbyPersistenceManager only usable for embedded databases. DerbyPersistenceManager always shuts down the database on exit, which makes it unusable for standalone databases."
0,"the spi2dav sandbox project should be put into a common release cycle. Remoting JSR170 calls requires obviously both server and client sides.  The server is available for download, as JCR WebDAV Server in both http://www.apache.org/dyn/closer.cgi/jackrabbit/binaries/jackrabbit-jcr-server-1.4.1.jar and http://www.apache.org/dyn/closer.cgi/jackrabbit/binaries/jackrabbit-webapp-1.4.war.  However, the client is just casually mentioned as ""can be found in the Jackrabbit sandbox.""  This issue is to request that the SPI2DAV client code (especially a ResponseFactory that returns a JCR2SPI Repository implementation) be available for download as well.

Furthermore, please make the RepositoryFactory implements javax.naming.spi.ObjectFactory so that only configuration (vs. Java coding) is needed in order to use it.  This is how org.apache.jackrabbit.rmi.client.ClientRepositoryFactory works."
0,"Workspace{Copy|Move}VersionableTest assumptions on versioning. These test cases assume that an ancestor of a versioned node can be made versioned. This may not be true for all JCR compliant stores.

There should be a way to skip the test when it can not be executed.

One obvious approach would be to throw a NotExecutableException when the attempt to enable versioning on the parent fails. However this has the drawback that it can mask configuration errors.

Thoughts?
"
0,It's not possible to register event listeners that filters on mixin supertypes. The current implementation of blocks() in EventFilter does not check if the given EventState has a mixin that is derived from one of the given node types.
0,"Allow multiple producers to feed/consume journal. Some clustered application based on jackrabbit might want to append custom records to the central journal in order to synchronize all nodes. Therefore, journal should provide support for multiple consumers/producers."
0,"Cloned SegmentReaders fail to share FieldCache entries. I just hit this on LUCENE-1516, which returns a cloned readOnly
readers from IndexWriter.

The problem is, when cloning, we create a new [thin] cloned
SegmentReader for each segment.  FieldCache keys directly off this
object, so if you clone the reader and do a search that requires the
FieldCache (eg, sorting) then that first search is always very slow
because every single segment is reloading the FieldCache.

This is of course a complete showstopper for LUCENE-1516.

With LUCENE-831 we'll switch to a new FieldCache API; we should ensure
this bug is not present there.  We should also fix the bug in the
current FieldCache API since for 2.9, users may hit this.
"
0,"Add fake charfilter to BaseTokenStreamTestCase to find offsets bugs. Recently lots of issues have been fixed about broken offsets, but it would be nice to improve the
test coverage and test that they work across the board (especially with charfilters).

in BaseTokenStreamTestCase.checkRandomData, we can sometimes pass the analyzer a reader wrapped
in a ""MockCharFilter"" (the one in the patch sometimes doubles characters). If the analyzer does
not call correctOffsets or does incorrect ""offset math"" (LUCENE-3642, etc) then eventually
this will create offsets and the test will fail.

Other than tests bugs, this found 2 real bugs: ICUTokenizer did not call correctOffset() in its end(),
and ThaiWordFilter did incorrect offset math."
0,"Text extractor classes are obsolete in web. Text extractor classes are obsolete in http://jackrabbit.apache.org/doc/components/index-filters.html

""org.apache.jackrabbit.core.query"" are actually ""org.apache.jackrabbit.extractor""

Plain text extractor continue being ""org.apache.jackrabbit.core.query.lucene.TextPlainTextFilter""? "
0,"Fix buggy stemmers and Remove duplicate analysis functionality. would like to remove stemmers in the following packages, and instead in their analyzers use a SnowballStemFilter instead.

* analyzers/fr
* analyzers/nl
* analyzers/ru

below are excerpts from this code where they proudly proclaim they use the snowball algorithm.
I think we should delete all of this custom stemming code in favor of the actual snowball package.


{noformat}
/**
 * A stemmer for French words. 
 * <p>
 * The algorithm is based on the work of
 * Dr Martin Porter on his snowball project<br>
 * refer to http://snowball.sourceforge.net/french/stemmer.html<br>
 * (French stemming algorithm) for details
 * </p>
 */

public class FrenchStemmer {

/**
 * A stemmer for Dutch words. 
 * <p>
 * The algorithm is an implementation of
 * the <a href=""http://snowball.tartarus.org/algorithms/dutch/stemmer.html"">dutch stemming</a>
 * algorithm in Martin Porter's snowball project.
 * </p>
 */
public class DutchStemmer {

/**
 * Russian stemming algorithm implementation (see http://snowball.sourceforge.net for detailed description).
 */
class RussianStemmer
{noformat}

"
0,Backport FSTs to 3.x. 
0,Convenience method to Or multiple values with a single filter. Added a convenience method to add Or Filter for the same property with multiple values. This is to simulate an IN Clause in JackRabbit. 
0,Similarity javadocs for scoring function to relate more tightly to scoring models in effect. See discussion in the related issue.
0,"[PATCH] FuzzyTermEnum optimization and refactor. I took a look at it to see if it could be improved.  I saw speed improvements of
20% - 40% by making a couple changes.  

The patch is here: http://www.hagerfamily.com/patches/FuzzyTermEnumOptimizePatch.txt

The Patch is based on the HEAD of the CVS tree as of Oct 22, 2004.

What Changed?

Since the word was discarded if the edit distance for the word was
above a certain threshold, I updated the distance algorithm to abort
if at any time during the calculation it is determined that the best
possible outcome of the edit distance algorithm is above this
threshold.  The source code has a great explanation.

I also reduced the amount of floating point math, reduced the amount
of potential space the array takes in its first dimension, removed the
potential divide by 0 error when one term is an empty string, and
fixed a bug where an IllegalArgumentException was thrown if the class
was somehow initialized wrong, instead of looking at the arguments.

The behavior is almost identical.  The exception is that similarity is
set to 0.0 when it is guaranteed to be below the minimum similarity.

Results

I saw the biggest improvement from longer words, which makes a sense.
My long word was ""bridgetown"" and I saw a 60% improvement on this.
The biggest improvement are for words that are farthest away from the
median length of the words in the index.  Short words (1-3 characters)
saw a 30% improvement.  Medium words saw a 10% improvement (5-7
characters).  These improvements are with the prefix set to 0."
0,"cache entry resource management should be extracted from CachingHttpClient. As we have built in support for stream-based management of cached response bodies, the CachingHttpClient class has its fingers in too many pies and is involved in resource management but not storage of the actual HttpCacheEntries.

I have a patch forthcoming. :)
"
0,"FileDataStore performance improvements. As seen in JCR-2695, the FileDataStore is slow on some file system. 

Some file operations such as File.exists() or File.isDirectory() can be replaced with try / catch, or by inspecting the return value of a previous method (File.renameTo)."
0,"AbstractHttpProcessor hard-wired against a single context. I can't use the AbstractHttpProcessor as it is for asynchronously
processing different requests, because it is hard-wired to use a
single context which can not be changed. Async requires different
contexts for requests. Patch follows.

cheers,
  Roland"
0,"Several final classes have non-overriding protected members. Protected member access in final classes, except where a protected method overrides a superclass's protected method, makes little sense.  The attached patch converts final classes' protected access on fields to private, removes two final classes' unused protected constructors, and converts one final class's protected final method to private."
0,support offsets in MemoryPostings. Really we should add this for Sep & Pulsing too... but this is one more
0,"Rename Field.Index.UN_TOKENIZED/TOKENIZED/NO_NORMS. There is confusion about these current Field options and I think we
should rename them, deprecating the old names in 2.4/2.9 and removing
them in 3.0.  How about this:

{code}
TOKENIZED --> ANALYZED
UN_TOKENIZED --> NOT_ANALYZED
NO_NORMS --> NOT_ANALYZED_NO_NORMS
{code}

Should we also add ANALYZED_NO_NORMS?

Spinoff from here:

    http://mail-archives.apache.org/mod_mbox/lucene-java-user/200808.mbox/%3C48a3076a.2679420a.1c53.ffffa5c4%40mx.google.com%3E
    "
0,"Remove deprecated methods in BooleanQuery. Remove deprecated methods setUseScorer14 and getUseScorer14 in BooleanQuery, and adapt javadocs."
0,"auto close idle connections. This has been mentioned several times on the mailing list (most recently here:
http://nagoya.apache.org/eyebrowse/ReadMsg?listName=commons-httpclient-dev@jakarta.apache.org&msgNo=5191
)
It is desirable for the http client to close it's connection after some
configurable idle time. Failing to do so causes the server (and every TCP
resource in between) to keep the socket open and possibly run out of resources
under load.

The HTTP 1.1 RFC has this to say under section 8.1.4:
Servers will usually have some time-out value beyond which they will
   no longer maintain an inactive connection. Proxy servers might make
   this a higher value since it is likely that the client will be making
   more connections through the same server. The use of persistent
   connections places no requirements on the length (or existence) of
   this time-out for either the client or the server.

   When a client or server wishes to time-out it SHOULD issue a graceful
   close on the transport connection. Clients and servers SHOULD both
   constantly watch for the other side of the transport close, and
   respond to it as appropriate. If a client or server does not detect
   the other side's close promptly it could cause unnecessary resource
   drain on the network.

The first sentence of the 2nd paragraph is interesting: how is the client
supposed to do a ""graceful close""? Does it simply mean closing the socket?
One possiblity may be to issue a HTTP/OPTIONS * request with a Connection:close
header."
0,"Occasional JCA test failures. As discussed in JCR-2870, the JCA packaging tests are occasionally failing with assertion failures in TransientRepository.startRepository(). We haven't seen this before JCR-2870, but it doesn't look like that change could possibly trigger this failure except perhaps by subtly affecting garbage collection. Thus I'm opening this new issue to track this as a separate problem.

Based on my analysis so far it looks likely that this problem has something to do with the ReferenceMap used by TransientRepository to track open sessions. The fact that the problem occurs only occasionally and on just some systems supports the assumption that this is related to some non-deterministic process like garbage collection."
0,"Remove JDOM dependency. Proposed by Sylvain Wallez on the dev mailing list.

Replace the JDOM code in the config, nodetype, and xml persistence manager code with equivalent standard DOM code. This change introduces some extra lines of code, but would remove an external dependency and avoid unnecessary deployment problems."
0,"Make BooleanWeight and DisjunctionMaxWeight protected. Currently, BooleanWeight is private, yet it has 2 protected members (similarity, weights) which are unaccessible from custom code

i have some use cases where it would be very useful to crawl a BooleanWeight to get at the sub Weight objects

however, since BooleanWeight is private, i have no way of doing this

If BooleanWeight is protected, then i can subclass BooleanQuery to hook in and wrap BooleanWeight with a subclass to facilitate this walking of the Weight objects

Would also want DisjunctionMaxWeight to be protected, along with its ""weights"" member

Would be even better if these Weights were made public with accessors to their sub ""weights"" objects (then no subclassing would be necessary on my part)

this should be really trivial and would be great if it can get into 2.9

more generally, it would be nice if all Weight classes were public with nice accessors to relevant ""sub weights""/etc so custom code can get its hooks in where and when desired"
0,expose PM for versioning manager so that the consistency check can be run from test cases. We need to be able to run the PM consistency checks for the versioning store as well.
0,"Add org.apache.lucene.store.FSDirectory.getDirectory(). On the Apache Lucene.Net side, we have done some clean up with the upcoming 2.9.1 such that we are now depreciating improperly use of parameter type for some public APIs.  When we release 3.0, those depreciated code will be removed.

One area where we had difficulty with required us to add a new method like so: Lucene.Net.Store.FSDirectory.GetDirectory().  This method does the same thing as Lucene.Net.Store.FSDirectory.GetFile().  This was necessary because we switched over from using System.IO.FileInfo to System.IO.DirectoryInfo.  Why?  In the .NET world, a file and a directory are two different things.

Why did we have to add Lucene.Net.Store.FSDirectory.GetDirectory()?  Because we can't change the return type of Lucene.Net.Store.FSDirectory.GetFile() and still remain backward compatible (API wise) to be depreciated with the next release.

Why ask for Java Lucene to add org.apache.lucene.store.FSDirectory.getDirectory()?  To keep the APIs 1-to-1 in par with Java Lucene and Lucene.Net."
0,"Expert API to specify indexing chain. It would be nice to add an expert API to specify an indexing chain, so that
we can make use of Mike's nice LUCENE-1301 feature.

This patch simply adds a package-protected expert API to IndexWriter and 
DocumentsWriter. It adds a inner, abstract class to DocumentsWriter called 
IndexingChain, and a default implementation that is the currently used one.

This might not be the final solution, but a nice way to play with different
modules in the indexing chain.

Could you take a look at the patch, Mike? "
0,"Remove or deprecate contrib/similarity. Classes under contrib/similarity seem to be duplicates of classes under contrib/queries.
I'd like to remove *.java from contrib/similarity without bothering with deprecation, since the same functionality exists in contrib/queries.
Anyone minds?
"
0,"JsonWriter: missing handling of new JCR 2.0 property types. the json writer sends extra type information in case it cannot be determined unambiguously from the json string.
this needs to be adjusted for the new property types added for JCR 2.0"
0,"Initial size of ConcurrentCache depends on number of segments (available processors). This causes a build failure on my machine. Tests run into an OOME because the initial memory footprint of a ConcurrentCache on my machine is 8k. Many of the tests keep references to some kind of repository objects (node, session, x-manager), which means ConcurrentCache instances  cannot be garbage collected immediately after a test run.

I think the overall initial size of the cache should be independent of the number of segments. See proposed patch."
0,"Remove ""synchonized"" from FuzzyTermEnum#similarity(final String target). The similarity method in FuzzyTermEnum is synchronized which is stupid because of:
- TermEnums are the iterator pattern and so are single-thread per definition
- The method is private, so nobody could ever create a fake FuzzyTermEnum just to have this method and use it multithreaded.
- The method is not static and has no static fields - so instances do not affect each other

The root of this comes from LUCENE-296, but was never reviewd and simply committed. The argument for making it synchronized is wrong."
0,"getDocValues should provide a MultiReader DocValues abstraction. When scoring a ValueSourceQuery, the scoring code calls ValueSource.getValues(reader) on *each* leaf level subreader -- so DocValue instances are backed by the individual FieldCache entries of the subreaders -- but if Client code were to inadvertently  called getValues() on a MultiReader (or DirectoryReader) they would wind up using the ""outer"" FieldCache.

Since getValues(IndexReader) returns DocValues, we have an advantage here that we don't have with FieldCache API (which is required to provide direct array access). getValues(IndexReader) could be implimented so that *IF* some a caller inadvertently passes in a reader with non-null subReaders, getValues could generate a DocValues instance for each of the subReaders, and then wrap them in a composite ""MultiDocValues"".


"
0,"LoginModuleConfig should allow to specify principalProvider-name in addition to the class. Gilles Metz reported this issue regarding login module configuration with Day's CRX based on Jackrabbit, which in
previous versions allowed to specify multiple prinicpal providers of the same class but with different configurations.
With JR 2.0 and 2.1 this is not supported as the pp class name is used as key in the registry and does not allow
to specify a separate key/name.





"
0,"ResourceConfig: read additional parameters for IOHandler and PropertyHandler that are covered by public setters. the reason for this is that currently default node types used in DefaultHandler cannot be changed using the resource config, which is
a bit cumbersome and leads to useless copies of DefaultIOManager."
0,"benchmark pkg: allow TrecContentSource not to change the docname. TrecContentSource currently appends 'iteration number' to the docname field.
Example: if the original docname is DOC0001 then it will be indexed as DOC0001_0

this presents a problem for relevance testing, because when judging results, the expected docname will never be present.
This patch adds an option to disable this behavior, defaulting to the existing behavior (which is to append the iteration number).
"
0,Move privilege reader/writer to spi-commons and use qualified names. the current privilege reader in jcr-commons uses a PrivilegeDefinition that is based on pure string rather than qualified names. suggest to move that to the spi-commons and use org.apache.jackrabbit.spi.Name for the privilege names.
0,"IP address of the server of a HttpConnection. AFAIK it's not possible to get the IP address of the server of a HttpConnection.

I propose to add a getServerAddress() method to the HttpConnection class that returns the IP address of the server, if the connection has been opened.
And either returns null or throws an Exception if the IP address is not available, i.e. the connection is not open.

Below is a workaround for getting the IP address in current versions.

-----------------------
package org.apache.commons.httpclient;

import java.io.IOException;
import java.net.InetAddress;

public class InetAddressFetcher {
	private HttpConnection hc;

	public InetAddressFetcher(HttpConnection hc) {
		this.hc = hc;
	}

	public InetAddress getInetAddress() throws IOException {
		if (!hc.isOpen()) {
			hc.open();
		}
		return hc.getSocket().getInetAddress();
	}
}"
0,"Add offsets to postings (D&PEnum). I think should explore making start/end offsets a first-class attr in the
postings APIs, and fixing the indexer to index them into postings.

This will make term vector access cleaner (we now have to jump through
hoops w/ non-first-class offset attr).  It can also enable efficient
highlighting without term vectors / reanalyzing, if the app indexes
offsets into the postings.
"
0,"FieldCacheTermsFilter. This is a companion to FieldCacheRangeFilter except it operates on a set of terms rather than a range. It works best when the set is comparatively large or the terms are comparatively common.

"
0,"Add javadoc notes about ICUCollationKeyFilter's advantages over CollationKeyFilter. contrib/collation's ICUCollationKeyFilter, which uses ICU4J collation, is faster than CollationKeyFilter, the JVM-provided java.text.Collator implementation in the same package.  The javadocs of these classes should be modified to add a note to this effect.

My curiosity was piqued by [Robert Muir's comment|https://issues.apache.org/jira/browse/LUCENE-1581?focusedCommentId=12720300#action_12720300] on LUCENE-1581, in which he states that ICUCollationKeyFilter is up to 30x faster than CollationKeyFilter.

I timed the operation of these two classes, with Sun JVM versions 1.4.2/32-bit, 1.5.0/32- and 64-bit, and 1.6.0/64-bit, using 90k word lists of 4 languages (taken from the corresponding Debian wordlist packages and truncated to the first 90k words after a fixed random shuffling), using Collators at the default strength, on a Windows Vista 64-bit machine.  I used an analysis pipeline consisting of WhitespaceTokenizer chained to the collation key filter, so to isolate the time taken by the collation key filters, I also timed WhitespaceTokenizer operating alone for each combination.  The rightmost column represents the performance advantage of the ICU4J implemtation (ICU) over the java.text.Collator implementation (JVM), after discounting the WhitespaceTokenizer time (WST): (JVM-ICU) / (ICU-WST). The best times out of 5 runs for each combination, in milliseconds, are as follows:

||Sun JVM||Language||java.text||ICU4J||WhitespaceTokenizer||ICU4J Improvement||
|1.4.2_17 (32 bit)|English|522|212|13|156%|
|1.4.2_17 (32 bit)|French|716|243|14|207%|
|1.4.2_17 (32 bit)|German|669|264|16|163%|
|1.4.2_17 (32 bit)|Ukranian|931|474|25|102%|
|1.5.0_15 (32 bit)|English|604|176|16|268%|
|1.5.0_15 (32 bit)|French|817|209|17|317%|
|1.5.0_15 (32 bit)|German|799|225|20|280%|
|1.5.0_15 (32 bit)|Ukranian|1029|436|26|145%|
|1.5.0_15 (64 bit)|English|431|89|10|433%|
|1.5.0_15 (64 bit)|French|562|112|11|446%|
|1.5.0_15 (64 bit)|German|567|116|13|438%|
|1.5.0_15 (64 bit)|Ukranian|734|281|21|174%|
|1.6.0_13 (64 bit)|English|162|81|9|113%|
|1.6.0_13 (64 bit)|French|192|92|10|122%|
|1.6.0_13 (64 bit)|German|204|99|14|124%|
|1.6.0_13 (64 bit)|Ukranian|273|202|21|39%|
"
0,"Change access levels in SearchIndex and NodeIndexer for better inherance. I want to change NodeIndexer#addBinaryValue logic in JR 1.5.6, therefore i needed to:
* create a custom class extending NodeIndexer for changing binary field indexation
* create a custom class extending SearchIndex for using this custom NodeIndexer

I was obliged to:
* override SearchIndex#createTextExtractor in order to store created TextExtractor because of private attribute
* put both classes into package org.apache.jackrabbit.core.query.lucene because NodeIndexer#createDoc(...) is protected

In trunk TextExtractor has now a getter but there are still some private attributes.
And NodeIndexer#createDoc(...) is still protected and there are some private methods."
0,"Use Java 5 enums. Replace the use of o.a.l.util.Parameter with Java 5 enums, deprecating Parameter.

Replace other custom enum patterns with Java 5 enums."
0,Make CFS appendable  . Currently CFS is created once all files are written during a flush / merge. Once on disk the files are copied into the CFS format which is basically a unnecessary for some of the files. We can at any time write at least one file directly into the CFS which can save a reasonable amount of IO. For instance stored fields could be written directly during indexing and during a Codec Flush one of the written files can be appended directly. This optimization is a nice sideeffect for lucene indexing itself but more important for DocValues and LUCENE-3216 we could transparently pack per field files into a single file only for docvalues without changing any code once LUCENE-3216 is resolved.
0,"Set up a release goal in Maven. Create a single Maven goal for building the Jackrabbit release packages. The goal should be based on the standard Maven dist goal, but include also the jackrabbit-commons and other modules we want to include in the release."
0,"SnowballAnalyzer has a link to net.sf (a package that is empty and needs to be removed).. need to remove net.sf and points to org.tartarus.snowball.ext. Doesn't work as a link though, so I'll also remove the @link to lose the javadoc error and broken link."
0,"Inadequate HTTP proxy server support in HttpClient.. 1. The HttpClient class does not save the StatusLine from the hidden
ConnectMethod object used to connect via an HTTP proxy server, thus any proxy
failures are only picked up as 'anonymous exceptions', this is useless for
gracefull recovery and rapid debugging.

2. The current class structure is too fragile to neatly support HTTP Proxy (and
authenication) chains so it would be a good idea to look at this at the same
time, preferable with support for a Proxy chain redirect when an non/dead HTTP
Proxy server is found."
0,RAMDirectory implements Serializable. RAMDirectory is for some reason not serializable.
0,"spellchecker: make hard-coded values configurable. the class org.apache.lucene.search.spell.SpellChecker uses the following hard-coded values in its method
indexDictionary:
        writer.setMergeFactor(300);
        writer.setMaxBufferedDocs(150);
this poses problems when the spellcheck index is created on systems with certain limits, i.e. in unix
environments where the ulimit settings are restricted for the user (http://www.gossamer-threads.com/lists/lucene/java-dev/47428#47428).

there are several ways to circumvent this:
1. add another indexDictionary method with additional parameters:
    public void indexDictionary (Dictionary dict, int mergeFactor, int maxBufferedDocs) throws IOException
    
2. add setter methods for mergeFactor and maxBufferedDocs 
    (see code in http://www.gossamer-threads.com/lists/lucene/java-dev/47428#47428 )

3. Make SpellChecker subclassing easier as suggested by Chris Hostetter 
   (see reply  http://www.gossamer-threads.com/lists/lucene/java-dev/47463#47463)

thanx,
karin
"
0,"[PATCH] ant-task ""javadocs-all"" fails with OutOfMemoryError. Hi all,

the current nightly build's ""ant dist"" fails with an OutOfMemoryError at ant task javadocs-all (see below).
Apparently javadoc needs more memory.

A similar case has been reported in HADOOP-5561 (add a maxmemory statement to the javadoc task), and I propose the same change for Lucene as well.

Cheers,
Christian


javadocs-all:
  [javadoc] Generating Javadoc
  [javadoc] Javadoc execution
  [javadoc] Loading source files for package org.apache.lucene...
  [javadoc] Loading source files for package org.apache.lucene.analysis...
(...)
  [javadoc] Loading source files for package org.apache.lucene.queryParser.standard.config...
  [javadoc] Loading source files for package org.apache.lucene.queryParser.standard.nodes...
  [javadoc] Loading source files for package org.apache.lucene.queryParser.standard.parser...
  [javadoc] Loading source files for package org.apache.lucene.queryParser.standard.processors...
  [javadoc] Constructing Javadoc information...
  [javadoc] Standard Doclet version 1.6.0_15
  [javadoc] Building tree for all the packages and classes...

 (takes a long time here until OOME)

  [javadoc] java.lang.OutOfMemoryError: Java heap space
  [javadoc] java.lang.OutOfMemoryError: Java heap space
  [javadoc] 	at java.lang.Throwable.getStackTraceElement(Native Method)
  [javadoc] 	at java.lang.Throwable.getOurStackTrace(Throwable.java:591)
  [javadoc] 	at java.lang.Throwable.printStackTrace(Throwable.java:462)
  [javadoc] 	at java.lang.Throwable.printStackTrace(Throwable.java:451)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.AbstractBuilder.build(AbstractBuilder.java:103)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.AbstractMemberBuilder.build(AbstractMemberBuilder.java:56)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.buildMemberSummary(ClassBuilder.java:279)
  [javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  [javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
  [javadoc] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
  [javadoc] 	at java.lang.reflect.Method.invoke(Method.java:597)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.invokeMethod(ClassBuilder.java:101)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.AbstractBuilder.build(AbstractBuilder.java:90)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.buildClassDoc(ClassBuilder.java:124)
  [javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  [javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
  [javadoc] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
  [javadoc] 	at java.lang.reflect.Method.invoke(Method.java:597)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.invokeMethod(ClassBuilder.java:101)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.AbstractBuilder.build(AbstractBuilder.java:90)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.build(ClassBuilder.java:108)
  [javadoc] 	at com.sun.tools.doclets.formats.html.HtmlDoclet.generateClassFiles(HtmlDoclet.java:155)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.AbstractDoclet.generateClassFiles(AbstractDoclet.java:164)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.AbstractDoclet.startGeneration(AbstractDoclet.java:106)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.AbstractDoclet.start(AbstractDoclet.java:64)
  [javadoc] 	at com.sun.tools.doclets.formats.html.HtmlDoclet.start(HtmlDoclet.java:42)
  [javadoc] 	at com.sun.tools.doclets.standard.Standard.start(Standard.java:23)
  [javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  [javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
  [javadoc] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
  [javadoc] 	at java.lang.reflect.Method.invoke(Method.java:597)
  [javadoc] 	at com.sun.tools.javadoc.DocletInvoker.invoke(DocletInvoker.java:269)
  [javadoc] java.lang.OutOfMemoryError: Java heap space
  [javadoc] 	at java.util.Arrays.copyOfRange(Arrays.java:3209)
  [javadoc] 	at java.lang.String.<init>(String.java:215)
  [javadoc] 	at com.sun.tools.javac.util.Convert.utf2string(Convert.java:131)
  [javadoc] 	at com.sun.tools.javac.util.Name.toString(Name.java:164)
  [javadoc] 	at com.sun.tools.javadoc.ClassDocImpl.getClassName(ClassDocImpl.java:341)
  [javadoc] 	at com.sun.tools.javadoc.TypeMaker.getTypeName(TypeMaker.java:100)
  [javadoc] 	at com.sun.tools.javadoc.ParameterizedTypeImpl.parameterizedTypeToString(ParameterizedTypeImpl.java:117)
  [javadoc] 	at com.sun.tools.javadoc.TypeMaker.getTypeString(TypeMaker.java:121)
  [javadoc] 	at com.sun.tools.javadoc.ExecutableMemberDocImpl.makeSignature(ExecutableMemberDocImpl.java:217)
  [javadoc] 	at com.sun.tools.javadoc.ExecutableMemberDocImpl.signature(ExecutableMemberDocImpl.java:198)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.util.VisibleMemberMap.getMemberKey(VisibleMemberMap.java:485)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.util.VisibleMemberMap.access$1000(VisibleMemberMap.java:28)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.util.VisibleMemberMap$ClassMembers.isOverridden(VisibleMemberMap.java:442)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.util.VisibleMemberMap$ClassMembers.addMembers(VisibleMemberMap.java:316)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.util.VisibleMemberMap$ClassMembers.build(VisibleMemberMap.java:278)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.util.VisibleMemberMap$ClassMembers.access$100(VisibleMemberMap.java:230)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.util.VisibleMemberMap.<init>(VisibleMemberMap.java:93)
  [javadoc] 	at com.sun.tools.doclets.formats.html.ConstructorWriterImpl.<init>(ConstructorWriterImpl.java:38)
  [javadoc] 	at com.sun.tools.doclets.formats.html.WriterFactoryImpl.getConstructorWriter(WriterFactoryImpl.java:129)
  [javadoc] 	at com.sun.tools.doclets.formats.html.WriterFactoryImpl.getMemberSummaryWriter(WriterFactoryImpl.java:141)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.MemberSummaryBuilder.init(MemberSummaryBuilder.java:104)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.MemberSummaryBuilder.getInstance(MemberSummaryBuilder.java:64)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.BuilderFactory.getMemberSummaryBuilder(BuilderFactory.java:191)
  [javadoc] 	at com.sun.tools.doclets.formats.html.ClassWriterImpl.navSummaryLinks(ClassWriterImpl.java:474)
  [javadoc] 	at com.sun.tools.doclets.formats.html.ClassWriterImpl.printSummaryDetailLinks(ClassWriterImpl.java:456)
  [javadoc] 	at com.sun.tools.doclets.formats.html.HtmlDocletWriter.navLinks(HtmlDocletWriter.java:462)
  [javadoc] 	at com.sun.tools.doclets.formats.html.ClassWriterImpl.writeFooter(ClassWriterImpl.java:146)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.buildClassFooter(ClassBuilder.java:330)
  [javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  [javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
  [javadoc] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
  [javadoc] 	at java.lang.reflect.Method.invoke(Method.java:597)
  [javadoc] java.lang.OutOfMemoryError: Java heap space
  [javadoc] 	at java.util.LinkedHashMap.createEntry(LinkedHashMap.java:424)
  [javadoc] 	at java.util.LinkedHashMap.addEntry(LinkedHashMap.java:406)
  [javadoc] 	at java.util.HashMap.put(HashMap.java:385)
  [javadoc] 	at sun.util.resources.OpenListResourceBundle.loadLookup(OpenListResourceBundle.java:118)
  [javadoc] 	at sun.util.resources.OpenListResourceBundle.loadLookupTablesIfNecessary(OpenListResourceBundle.java:97)
  [javadoc] 	at sun.util.resources.OpenListResourceBundle.handleGetObject(OpenListResourceBundle.java:58)
  [javadoc] 	at sun.util.resources.TimeZoneNamesBundle.handleGetObject(TimeZoneNamesBundle.java:59)
  [javadoc] 	at java.util.ResourceBundle.getObject(ResourceBundle.java:378)
  [javadoc] 	at java.util.ResourceBundle.getObject(ResourceBundle.java:381)
  [javadoc] 	at java.util.ResourceBundle.getStringArray(ResourceBundle.java:361)
  [javadoc] 	at sun.util.TimeZoneNameUtility.retrieveDisplayNames(TimeZoneNameUtility.java:100)
  [javadoc] 	at sun.util.TimeZoneNameUtility.retrieveDisplayNames(TimeZoneNameUtility.java:81)
  [javadoc] 	at java.util.TimeZone.getDisplayNames(TimeZone.java:399)
  [javadoc] 	at java.util.TimeZone.getDisplayName(TimeZone.java:350)
  [javadoc] 	at java.util.Date.toString(Date.java:1025)
  [javadoc] 	at com.sun.tools.doclets.formats.html.markup.HtmlDocWriter.today(HtmlDocWriter.java:337)
  [javadoc] 	at com.sun.tools.doclets.formats.html.HtmlDocletWriter.printHtmlHeader(HtmlDocletWriter.java:281)
  [javadoc] 	at com.sun.tools.doclets.formats.html.ClassWriterImpl.writeHeader(ClassWriterImpl.java:122)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.buildClassHeader(ClassBuilder.java:164)
  [javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  [javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
  [javadoc] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
  [javadoc] 	at java.lang.reflect.Method.invoke(Method.java:597)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.invokeMethod(ClassBuilder.java:101)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.AbstractBuilder.build(AbstractBuilder.java:90)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.buildClassDoc(ClassBuilder.java:124)
  [javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  [javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
  [javadoc] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
  [javadoc] 	at java.lang.reflect.Method.invoke(Method.java:597)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.invokeMethod(ClassBuilder.java:101)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.AbstractBuilder.build(AbstractBuilder.java:90)
  [javadoc] java.lang.OutOfMemoryError: Java heap space
  [javadoc] java.lang.OutOfMemoryError: Java heap space
  [javadoc] java.lang.OutOfMemoryError: Java heap space
  [javadoc] java.lang.OutOfMemoryError: Java heap space
  [javadoc] java.lang.OutOfMemoryError: Java heap space
"
0,"URI.java readObject()/writeObject() must be private. In the class org.apache.commons.httpclient.URI, the readObject/writeObject methods are currently protected - they need to be private, or Java will not invoke them."
0,[PATCH] more verbose exception messages (BatchedItemOperations). added context to exception messages in BatchedItemOperations to aid debugging
0,"GData-Server - Website sandbox part. Added GData-Server to the sandbox part of the website -- xdocs/sandbox/

Build of website is fine."
0,"REFERENCE properties produce duplicate strings in memory. When reference property is loaded from PM, Serializer.deserialize(NodeReferences, InputStream) is called, which calls PropertyId.valueOf(String), which in turn calls NameFactoryImpl.create(String) which finally splits a full property name to namespace and local name. Namespace is internalized, but local name is not (comments say that this is done to avoid perm space overfilling).
So, in the end, a new String instance is created for local name. This leads to considerable memory waste when repository has a lot of nodes with REFERENCE properties.
It seems that local name part could be internalized here too because in the most repositories it's not allowed to create properties with arbitrary names, so the danger of perm space exhaust does not seem to be an argument.

As for ways to resolve this, maybe a new NameFactory implementation could be created which would be used for properties only (and, possibly, mainly in the PropertyId.valueOf(String)) which would extend an existing NameFactoryImpl overriding its create(String) method.

What do you think about all this?"
0,"HttpURLConnection wrapper. Initial comments from Vincent Massol for this feature:

I am moving Jakarta Cactus from using the JDK HttpURLConnection to
Commons HttpClient. However, I have some public interface that return
HttpURLConnection and I cannot break that contract with Cactus users.

I propose to write a HttpURLConnection wrapper for HttpMethod (I have
actually already written it but I am currently testing it on Cactus and
will make a proper donation once I am sure it works - i.e all the Cactus
tests pass as before ... ).

I attach a preview of it for those interested.

What do you think of including it in HttpClient distribution ?

Thanks
-Vincent"
0,"AbstractJCRTest fails on level 1 repositories. If a test case indicates that it's not read-only, org.apache.jackrabbit.test.AbstractJCRTest tries to cleanup the test root in the setUp method. This will cause the test case to fail, because a Level 1 repository will throw an UnsupportedOperationException here.

Proposal: before trying the cleanup, check for L2 functionality and throw a NotExecutableException otherwise:

            if (! isSupported(Repository.LEVEL_2_SUPPORTED)) {
              cleanUp();
              String msg = ""Test case requires level 2 functionality"";
              throw new NotExecutableException(msg);
            }
"
0,"Change defaults in IndexWriter to maximize ""out of the box"" performance. This is follow-through from LUCENE-845, LUCENE-847 and LUCENE-870;
I'll commit this once those three are committed.

Out of the box performance of IndexWriter is maximized when flushing
by RAM instead of a fixed document count (the default today) because
documents can vary greatly in size.

Likewise, merging performance should be faster when merging by net
segment size since, to minimize the net IO cost of merging segments
over time, you want to merge segments of equal byte size.

Finally, ConcurrentMergeScheduler improves indexing speed
substantially (25% in a simple initial test in LUCENE-870) because it
runs the merges in the backround and doesn't block
add/update/deleteDocument calls.  Most machines have concurrency
between CPU and IO and so it makes sense to default to this
MergeScheduler.

Note that these changes will break users of ParallelReader because the
parallel indices will no longer have matching docIDs.  Such users need
to switch IndexWriter back to flushing by doc count, and switch the
MergePolicy back to LogDocMergePolicy.  It's likely also necessary to
switch the MergeScheduler back to SerialMergeScheduler to ensure
deterministic docID assignment.

I think the combination of these three default changes, plus other
performance improvements for indexing (LUCENE-966, LUCENE-843,
LUCENE-963, LUCENE-969, LUCENE-871, etc.) should make for some sizable
performance gains Lucene 2.3!"
0,jcr:path in QueryResult is only tested with SQL. The TCK should also include a test case for XPath query syntax.
0,"Unclosed sessions in test cases. Some tests may throw exceptions in the setUp() method and leave the session open that was opened in the super class setUp() method. For jackrabbit-core, this is not really a problem, because the memory footprint of a session is quite small, but in jcr2spi the memory footprint is considerable higher, which may lead to out of memory errors when running the tests."
0,"Proxy authentication does not handle multiple multiple authentication schemes. My proxy server returns the following header lines in the response:

    Proxy-Authenticate: NTLM
    Proxy-Authenticate: Basic realm=""10.105.20.201""

i.e., it returns two Proxy-Authenticate header lines. Unfortunately this does 
not work. In line 253 of class Authenticator (method: authenticate(HttpMethod, 
HttpState, Header, String)) I see this comment:

    // FIXME: Note that this won't work if there is more than one realm within 
the challenge

so it looks like this is something that isn't yet implemented. In the log, I 
can see that the Authenticator attempts to parse the realm, but it looks like
this is not being done correctly:

   411 DEBUG Attempting to authenticate challenge: Proxy-Authenticate: NTLM, 
Basic realm=""10.105.20.201""

   411 DEBUG Parsed realm ""ealm=""10.105.20.201"" from challenge ""NTLM, Basic 
realm=""10.105.20.201"""".
   421 WARN  Exception thrown authenticating
java.lang.UnsupportedOperationException: Authentication type ""NTLM,"" is not 
recognized.
    at org.apache.commons.httpclient.Authenticator.authenticate
(Authenticator.java:274)
    at org.apache.commons.httpclient.Authenticator.authenticateProxy
(Authenticator.java:178)
    at 
org.apache.commons.httpclient.HttpMethodBase.processAuthenticationResponse
(HttpMethodBase.java:580)
    at org.apache.commons.httpclient.HttpMethodBase.execute
(HttpMethodBase.java:668)
    at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:355)
    at com.cmg.httptest.Main.main(Main.java:34)

It looks wrong to me that the realm name seems to be parsed as: 
ealm=""10.105.20.201

I understand that Authenticator does not know what NTLM is but I would like it 
to use Basic authentication in this case.

If there are more authentication methods possible, how can I specify which one 
I want to use?

Jesper de Jong"
0,"Terms dict should block-encode terms. With PrefixCodedTermsReader/Writer we now encode each term standalone,
ie its bytes, metadata, details for postings (frq/prox file pointers),
etc.

But, this is costly when something wants to visit many terms but pull
metadata for only few (eg respelling, certain MTQs).  This is
particularly costly for sep codec because it has more metadata to
store, per term.

So instead I think we should block-encode all terms between indexed
term, so that the metadata is stored ""column stride"" instead.  This
makes it faster to enum just terms.
"
0,"Create OSGi Bundles from jackrabbit-webdav and jackrabbit-jcr-server libraries. Propose to generate bundles from the jackrabbit-webdav (exporting everything) and jackrabbit-jcr-server (exporting nothing) libraries. In addition a new class is added to the jackrabbit-jcr-server library, which in case of deployment in an OSGi framework will register a Servlet with the OSGi HttpService to expose the JcrRemotingServlet."
0,"tm-extractors.jar blocks usage of newer poi versions. The used tm-extractors-0.4.jar (http://repo1.maven.org/maven2/org/textmining/tm-extractors/) includes various classes from poi as well as poi-2.5.1 is referenced as dependency in the pom.xml.
It's seems not possible to use a newer version of poi (e.g. poi-3.0.1-FINAL) together with tm-extractors (and so jackrabbit).

A solution could be using functions of newer poi versions (I'm not sure if they are only in scratchpad yet) or use a fixed version of tm-extractors.jar which doesn't inlude the poi classes."
0,"Nightly build archives do not contain Java source code.. Under the Lucene News section of the Overview page, this item's link:

26 January 2006 - Nightly builds available
http://cvs.apache.org/dist/lucene/java/nightly/

goes to a directory with several 1.9M files, none of which have the src/java tree in them."
0,Deprecated classes point to wrong replacements (due to various package renamings). classes that have been replaced by equivalents in jackrabbit-spi-commons point to wrong replacements in the @deprecated javadoc tag.
0,"TCK: NodeReadMethodsTest.testGetName fails with NPE if  'testroot' has no child node. The 'testGetName' does not assert, that  the childnode field has been populated during setup.
if  for whatever reason  the test data don't provide a single childnode below the test root, this test will fail with nullpointer
exception.

i would like to suggest to use the same assertion as in testGetPath and throw a NotExecutableException in case of
missing child node.

patch attached.

"
0,"bad project.xml. project.xml in the ""api"" module actually contains an error (nested comment) and can't be read by maven."
0,"Open IndexWriter API to allow custom MergeScheduler implementation. IndexWriter's getNextMerge() and merge(OneMerge) are package-private, which makes it impossible for someone to implement his own MergeScheduler. We should open up these API, as well as any other that can be useful for custom MS implementations."
0,"Log path of missing node when re-indexing fails. If one tries to re-index a corrupt workspace, then the UUID of the missing nodes is logged. If possible the log should also contain the path to the missing node."
0,"better diagnostics when version storage is broken. In InternalVersionManagerBase, the code doesn't do a null-check on the predecessors property, assuming it'll always be present. When this is not the case due to a repository problem, we'll see a NPE.

Proposal: add code that generates a more meaningful error message; making it easier to debug in production."
0,Upgrade to Derby 10.2. Apache Derby 10.2 was released recently. The release contains a number of improvements (including performance) and requires no special upgrade procedures. I suggest we upgrade to Derby 10.2 along with Lucene 2.0 (JCR-352) and Maven 2 (JCR-332).
0,"Jackrabbit performance test suite. I'd like to set up a multi-version performance test suite inside jackrabbit-core/src/test/performance, similar to the compatibility test suite we added in JCR-2631. This performance test suite would produce comparable performance numbers for a number of simple benchmark tests across different Jackrabbit versions, including the latest snapshot.

"
0,Use FieldSelector in Sorted/LuceneQueryHits when reading UUID. LuceneQueryHits currently reads the complete lucene document. This also prevents usage of an underlying UUID cache.
0,"Broke lock tests in jcr2spi after the JCR 2.0 upgrade. My changes during the JCR 2.0 upgrade have broken the following tests in spi2jcr:

    org.apache.jackrabbit.test.api.lock.LockTest
    org.apache.jackrabbit.jcr2spi.lock.SessionScopedLockTest

I'll mark them as known issues for now to get the Hudson build back up again, but will continue looking at what I did to cause this breakage."
0,"Problem with formerly escaped JCR node names when upgrading to Jackrabbit 2.2.9. The following unit test fails:

{code}
import static org.junit.Assert.*;

import org.apache.jackrabbit.util.Text;
import org.junit.Test;

public class TestEscaping
{
   @Test
   public void testEscaping() throws Exception
   {
      // expect this as an escaped string (e.g. formerly escaped with jackrabbit 1.6)
      String escaped = ""nam%27e"";
      String unescaped = Text.unescapeIllegalJcrChars(escaped);
      assertEquals(escaped, Text.escapeIllegalJcrChars(unescaped));
   }
}
{code}

This is a major problem when upgrading from 1.6.x to 2.2.9. The node names that were escaped in jackrabbit 1.6 are not longer escaped and that breaks the backward compatibility. I think the problem comes in with JCR-2198. "
0,"Tests fail with NoClassDefFoundError: org/w3c/dom/ranges/DocumentRange. The nekohtml dependency in jackrabbit-text-extractors brings in a transitive xerces 2.4.0 dependency without the extra XML API classes required by Xerces. This causes test failures in jackrabbit-core and jackrabbit-jca because the Xerces dependency included in the test classpath overrides the default XML parser. Then, when the test cases try to parse XML documents, the missing XML API dependency causes a NoClassDefFoundError."
0,"TCK: AbstractImportXmlTest incorrectly assumes mix:referenceable can be added to created node. isMixRefRespected() assumes that if the NodeTypeManager contains mix:referenceable, addMixin can be called to add mix:referenceable to a created node.  This assumption is incorrect for at least two reasons.  First, the created node might already be mix:referenceable, either because its primary node type is a subtype of mix:referenceable or because the implementation added mix:referenceable as a mixin type in creating the node.  Second, a repository may restrict the nodes to which mix:referenceable can be added.  In the extreme case, the repository may not allow mix:referenceable to be added to any node using addMixin, in which case the only referenceable nodes would be those which are mix:referenceable by virtue of primary type or the implementation's adding mix:referenceable as a mixin type at node creation.

Proposal: test canAddMixin before calling addMixin.
"
0,"TestConstantScoreRangeQuery does not compile with ecj. TestConstantScoreRangeQuery has an assertEquals(String, Float, Float)
but most of the calls to assertEquals are (String, int, int).

ecj complains with the following error:
The method assertEquals(String, float, float) is ambiguous for the type TestConstantScoreRangeQuery

The simple solution is to supply an assertEquals(String, int, int) which calls Assert.assertEquals(String, int, int)

Patch to follow.
"
0,"stackable parameters. Implement ""stackable parameters"" to allow for a parameter hierarchy without linking params instances."
0,"The repository-1.5.dtd is not well formed. The repository-1.5.dtd file at http://jackrabbit.apache.org/dtd/repository-1.5.dtd
is not well formed at the time of this writing 200/05/23 19:30GMT

1. It looks like a #REQUIRED is missing at line 173
2. Detected this while trying the 5minutes with ocm tutorial

Hope this helps,
 S."
0,"Send all variants' ETags on ""variant miss"". From section 13.6 of RFC 2616:

If an entity tag was assigned to a cached representation, the forwarded request SHOULD be conditional and include the entity tags in an If-None-Match header field from all its cache entries for the resource. This conveys to the server the set of entities currently held by the cache, so that if any one of these entities matches the requested entity, the server can use the ETag header field in its 304 (Not Modified) response to tell the cache which entry is appropriate. If the entity-tag of the new response matches that of an existing entry, the new response SHOULD be used to update the header fields of the existing entry, and the result MUST be returned to the client.

Presently, we simply forward the request to the request without the conditionals.  This improvement would consist of adding the conditionals to the request, and properly handling the response.  An example of such would be the following:

 - request resource with ""Accept-Encoding: gzip"", response has ""Etag: etag1"", ""Vary: Accept-Encoding""
 - request resource with ""Accept-Encoding: deflate"", request is forwarded with ""If-None-Match: etag1"" added, response is 200, with ""ETag: etag2""
 - request resource with ""Accept-Encoding: gzip, deflate"", request is forwarded with ""If-None-Match: etag1, etag2"" added, response is 304, with ""ETag: etag1"" indicating we should use the first response for this request"
0,"allow ResourceType dav property to have multiple values. attached is a patch that allows the ResourceType dav property to have multiple values (useful for dav protocol extensions such as caldav). 

it is not a perfect patch, in that subclasses of ResourceType do not know about each others' resource types, but it is a decent start. one way to address this issue might be to have subclasses register extended resource types and their corresponding xml representations with ResourceType, removing the need for them to override resourceTypeToXml() and isValidResourceType().
"
0,"MemcachedHttpCacheStorage should throw IOExceptions instead of Runtime Exceptions. The MemcachedHttpCacheStorage class implements HttpCacheStorage which defines that methods will throw IOExceptions, but the underlying net.spy.memcached.MemcachedClientIF throws runtime exceptions. These exceptions are not caught in the code where IOExceptions are expected causing these exception bubble up to the calling code. It seems like the MemcachedHttpCacheStorage class should treat at least some of these runtime exceptions as IOExceptions so that normal code execution paths can be followed.  

I'm proposing that MemcachedHttpCacheStorage treat a OperationTimeoutException from the memcached client as an IOException. This would allow the existing CachingHttpClient code to catch and log the exception as a warning, instead of bubbling the exception up the calling code.
"
0,"Change SortField types to an Enum. When updating my SOLR-2533 patch, one issue was that the int value I had given my new type had been used by another change in the mean time.  Since we don't use these fields in a bitset kind of way, we can convert them to an enum."
0,"add svn ignores for eclipse artifacts. Be nice to ignore the files eclipse puts into the project root as we do the .idea file for intellij.

The two files are

.project
.classpath

I'm gonna lie and say there's a patch available for this because an svn diff patch with propery changes can't be applied with patch anyway."
0,"configurable User-Agent string. User configurable item to set the user agent without haveing to set it on a per
HttpMethod basis."
0,Data store garbage collection: log deleted files and total size. The data store garbage collection should list the names and the total size of all deleted files.
0,"Workspace.xml can't be loaded if it has a BOM. I wondered if there was a specific reason why workspace.xml files are loaded using a FileReader instead of an InputStream in RepositoryConfig?

If the workspace.xml file has a BOM (which could happen if someone edited the file manually with some misbehaving editor), then it can't be loaded (""Content not allowed in prolog"") - here's a little patch that fixes this.

I left the output part untouched (i.e still using a Writer) - which makes it a little inconsistent - maybe someone with a better knowledge of the JR FileSystem api could fix this.

"
0,WriteLineDocTask should write gzip/bzip2/txt according to the extension of specified output file name. Since the readers behave this way it would be nice and handy if also this line writer would.
0,MatchAllDocsQuery to return all documents. It would be nice to have a type of query just return all documents from an index.
0,"Implement StandardTokenizer with the UAX#29 Standard. It would be really nice for StandardTokenizer to adhere straight to the standard as much as we can with jflex. Then its name would actually make sense.

Such a transition would involve renaming the old StandardTokenizer to EuropeanTokenizer, as its javadoc claims:

bq. This should be a good tokenizer for most European-language documents

The new StandardTokenizer could then say

bq. This should be a good tokenizer for most languages.

All the english/euro-centric stuff like the acronym/company/apostrophe stuff can stay with that EuropeanTokenizer, and it could be used by the european analyzers.
"
0,"Move UnInvertedField into Lucene core. Solr's UnInvertedField lets you quickly lookup all terms ords for a
given doc/field.

Like, FieldCache, it inverts the index to produce this, and creates a
RAM-resident data structure holding the bits; but, unlike FieldCache,
it can handle multiple values per doc, and, it does not hold the term
bytes in RAM.  Rather, it holds only term ords, and then uses
TermsEnum to resolve ord -> term.

This is great eg for faceting, where you want to use int ords for all
of your counting, and then only at the end you need to resolve the
""top N"" ords to their text.

I think this is a useful core functionality, and we should move most
of it into Lucene's core.  It's a good complement to FieldCache.  For
this first baby step, I just move it into core and refactor Solr's
usage of it.

After this, as separate issues, I think there are some things we could
explore/improve:

  * The first-pass that allocates lots of tiny byte[] looks like it
    could be inefficient.  Maybe we could use the byte slices from the
    indexer for this...

  * We can improve the RAM efficiency of the TermIndex: if the codec
    supports ords, and we are operating on one segment, we should just
    use it.  If not, we can use a more RAM-efficient data structure,
    eg an FST mapping to the ord.

  * We may be able to improve on the main byte[] representation by
    using packed ints instead of delta-vInt?

  * Eventually we should fold this ability into docvalues, ie we'd
    write the byte[] image at indexing time, and then loading would be
    fast, instead of uninverting
"
0,"stop writing shared doc stores across segments. Shared doc stores enables the files for stored fields and term vectors to be shared across multiple segments.  We've had this optimization since 2.1 I think.

It works best against a new index, where you open an IW, add lots of docs, and then close it.  In that case all of the written segments will reference slices a single shared doc store segment.

This was a good optimization because it means we never need to merge these files.  But, when you open another IW on that index, it writes a new set of doc stores, and then whenever merges take place across doc stores, they must now be merged.

However, since we switched to shared doc stores, there have been two optimizations for merging the stores.  First, we now bulk-copy the bytes in these files if the field name/number assignment is ""congruent"".  Second, we now force congruent field name/number mapping in IndexWriter.  This means this optimization is much less potent than it used to be.

Furthermore, the optimization adds *a lot* of hair to IndexWriter/DocumentsWriter; this has been the source of sneaky bugs over time, and causes odd behavior like a merge possibly forcing a flush when it starts.  Finally, with DWPT (LUCENE-2324), which gets us truly concurrent flushing, we can no longer share doc stores.

So, I think we should turn off the write-side of shared doc stores to pave the path for DWPT to land on trunk and simplify IW/DW.  We still must support reading them (until 5.0), but the read side is far less hairy."
0,"SerializationTest and AbstractImportXmlTest leak temporary files. Both test classes leak temporary files when setUp() fails.
"
0,"[PATCH] Remove Stutter in ItemValidator. ItemValidator duplicates code for no reason. Remove the duplication

        if (permissions > Permission.NONE) {
            Path path = item.getPrimaryPath();
            if (!accessMgr.isGranted(item.getPrimaryPath(), permissions)) {
                return false;
            }

to

        if (permissions > Permission.NONE) {
            Path path = item.getPrimaryPath();
            if (!accessMgr.isGranted(path, permissions)) {
                return false;
            }

"
0,"""System Properties"" doc lists ""lockDir"" instead of ""lockdir"". The ""System Properties"" documentation page states that the lock file directory
can be set with the system property ""org.apache.lucene.lockDir"".  However, as
implemented in org.apache.lucene.store.FSDirectory, line 56, the property name
is actually ""org.apache.lucene.lockdir"" (lower case ""d"" in ""lockdir""). 
Recommend changing documentation to match code."
0,"Get javadoc for the similarities package in shape. 1. Create a package.html in the similarities package.
2. Update the javadoc of the search package (package.html mentions Similarity)?
3. Compile the javadoc to see if there are any warnings."
0,"introduce HttpRoutePlanner interface. Define an interface to determine a route for a given target host.
Create default implementation replacing DefaultHttpClient.determineRoute(...);
Implementations will need access to params and/or request.

The interface fits into HttpConn, but DHC.dR(...) uses client parameters.
Either move parameters to HttpConn, or keep default implementation in HttpClient.

Alternative implementations could evaluate Java system properties related to proxy settings.

"
0,"jcr:like() does not scale well on large value ranges. There are two major issues with the current WildcardQuery implementation:

1) A wildcard expression is restricted to match at most 1024 terms, otherwise a TooManyClauses exception is thrown. Similar to the RangeQuery issue: JCR-111
2) The enumeration over the terms that match the wildcard pattern is slow"
0,"RepositoryImpl.activeSessions should use Session instead of SessionImpl. Turn Map<SessionImpl, SessionImpl> activeSessions into Map<Session, Session> activeSessions as there is not clear need for the use of SessionImpl."
0,"Allow Directory.copy() to accept a collection of file names to be copied. Par example, I want to copy files pertaining to a certain commit, and not everything there is in a Directory."
0,"Refactor the Mapper & DescriptotReader classes. I would like to refactor the mappers and the descriptor readers  in order to : 
* Create an abstract mapper impl because both Mapper classes have a lot of code in common (AnnotedObjectMapper & DigesterMapperImpl). Only the readers are different. The Mappers can make exactly the same process. 
* The Mapper classes should not have the responsibility to create the jcr node types. This can be done outside the mapper and it should be an optional operation. There are certainly some use cases where node type creation is not necessary. Right now, the annotated object mapper creates jcr node types. "
0,"IndexReader currently has javadoc errors. Current trunk has some javadoc errors in IndexReader and some more in contrib.
"
0,"benchmark for collation. Steven Rowe attached a contrib/benchmark-based benchmark for collation (both jdk and icu) under LUCENE-2084, along with some instructions to run it... 

I think it would be a nice if we could turn this into a committable patch and add it to benchmark.
"
0,"Use a System.arraycopy more than a for. In org.apache.lucene.index.DocumentWriter. The patch will explain by itself. I didn't make any performance test, but I think it is obvious that it will be faster.
All tests passed."
0,"""JCR levels"" link on http://jackrabbit.apache.org/doc/index.html broken. The ""JCR levels"" link on the Jackrabbit home page is broken.
"
0,"Share the Term -> TermInfo cache across threads. Right now each thread creates its own (thread private) SimpleLRUCache,
holding up to 1024 terms.

This is rather wasteful, since if there are a high number of threads
that come through Lucene, you're multiplying the RAM usage.  You're
also cutting way back on likelihood of a cache hit (except the known
multiple times we lookup a term within-query, which uses one thread).
In NRT search we open new SegmentReaders (on tiny segments) often
which each thread must then spend CPU/RAM creating & populating.

Now that we are on 1.5 we can use java.util.concurrent.*, eg
ConcurrentHashMap.  One simple approach could be a double-barrel LRU
cache, using 2 maps (primary, secondary).  You check the cache by
first checking primary; if that's a miss, you check secondary and if
you get a hit you promote it to primary.  Once primary is full you
clear secondary and swap them.

Or... any other suggested approach?
"
0,Move to a newer vesion of Commons Collections. It would be useful if the developer wants to use jackrabbit in an application that uses a newer version of this library.
0,"Add katakana stem filter to better deal with certain katakana spelling variants. Many Japanese katakana words end in a long sound that is sometimes optional.

For example,  and  are both perfectly valid for ""party"".  Similarly we have  and  that are variants of ""center"" as well as  and  for ""server"".

I'm proposing that we add a katakana stemmer that removes this long sound if the terms are longer than a configurable length.  It's also possible to add the variant as a synonym, but I think stemming is preferred from a ranking point of view.

"
0,"When 3.1 is released, update backwards tests in 3.x branch. When we have released the official artifacts of Lucene 3.1 (the final ones!!!), we need to do the following:

- svn rm backwards/src/test
- svn cp https://svn.apache.org/repos/asf/lucene/dev/branches/lucene_solr_3_1/lucene/src/test backwards/src/test
- Copy the lucene-core-3.1.0.jar from the last release tarball to lucene/backwards/lib and delete old one.
- Check that everything is correct: The backwards folder should contain a src/ folder that now contains ""test"". The files should be the ones from the branch.
- Run ""ant test-backwards""

Uwe will take care of this!"
0,"QueryScorer and SpanRegexQuery are incompatible.. Since the resolution of #LUCENE-1685, users are not supposed to rewrite their queries before submitting them to QueryScorer:

bq.------------------------------------------------------------------------
bq.r800796 | markrmiller | 2009-08-04 06:56:11 -0700 (Tue, 04 Aug 2009) | 1 line
bq.
bq.LUCENE-1685: The position aware SpanScorer has become the default scorer for Highlighting. The SpanScorer implementation has replaced QueryScorer and the old term highlighting QueryScorer has been renamed to QueryTermScorer. Multi-term queries are also now expanded by default. If you were previously rewritting the query for multi-term query highlighting, you should no longer do that (unless you switch to using QueryTermScorer). The SpanScorer API (now QueryScorer) has also been improved to more closely match the API of the previous QueryScorer implementation.
bq.------------------------------------------------------------------------

This is a great convenience for the most part, but it's causing me difficulties with SpanRegexQuerys, as the WeightedSpanTermExtractor uses Query.extractTerms() to collect the fields used in the query, but SpanRegexQuery does not implement this method, so highlighting any query with a SpanRegexQuery throws an UnsupportedOpertationException.  If this issue is circumvented, there is still the issue of SpanRegexQuery throwing an exception when someone calls its getSpans() method.

I can provide the patch that I am currently using, but I'm not sure that my solution is optimal.  It adds two methods to SpanQuery: extractFields(Set<String> fields) which is equivalent to fields.add(getField()) except when MaskedFieldQuerys get involved, and mustBeRewrittenToGetSpans() which returns true for SpanQuery, false for SpanTermQuery, and is overridden in each composite SpanQuery to return a value depending on its components.  In this way SpanRegexQuery (and any other custom SpanQuerys) do not need to be adjusted.

Currently the collection of fields and non-weighted terms are done in a single step.  In the proposed patch the WeightedSpanTerm extraction from a SpanQuery proceeds in two steps.  First, if the QueryScorer's field is null, then the fields are collected from the SpanQuery using the extractFields() method.  Second the terms are collected using extractTerms(), rewriting the query for each field if mustBeRewrittenToGetSpans() returns true."
0,"Prepare CharArraySet for Unicode 4.0. CharArraySet does lowercaseing if created with the correspondent flag. This causes that  String / char[] with uncode 4 chars which are in the set can not be retrieved in ""ignorecase"" mode.
"
0,"extend LevenshteinAutomata to support transpositions as primitive edits. This would be a nice improvement for spell correction: currently a transposition counts as 2 edits,
which means users of DirectSpellChecker must use larger values of n (e.g. 2 instead of 1) and 
larger priority queue sizes, plus some sort of re-ranking with another distance measure for good results.

Instead if we can integrate ""chapter 7"" of http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.16.652 
then you can just build an alternative DFA where a transposition is only a single edit 
(http://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance)

According to the benchmarks in the original paper, the performance for LevT looks to be very similar to Lev.

Support for this is now in moman (https://bitbucket.org/jpbarrette/moman/) thanks to Jean-Philippe 
Barrette-LaPierre.
"
0,SerializationTest leaks sessions. The class TreeComparator extends from AbstractJCRTest and opens a session in its constructor because it calls the setUp() method. The tearDown() method is never called.
0,"Item states cached in UpdatableItemStateManager not discarded on logout. The SessionItemStateManager disposes only the TransientItemStateManager but not the UpdatableItemStateManager. The latter doesn't release the resources and the cached items keep listening the overlayed state events until the ReferenceMap do its magic. According to my tests in certain circumstances it slows down jackrabbit very much. 

In the charts you can see the time it takes to jackrabbit to save each child node. The first test uses a single session and the second creates a new session for each added node.

The patch I attach makes both tests take the same time. Opinions?

05-08-16-one session.GIF
/ login
  / loop
    / add node
    / save
/ logout


05-08-16-one session per save.GIF
/ loop
  / login
  / add node
  / save
  / logout

"
0,"Append-only index updates. Currently index updates modify some existing files. This is troublesome in scenarios like a backup or when an index will be shared in a cluster (though this is not yet the case).

Requirements are:

- index segments need a custom (lucene) IndexDeletionPolicy to keep index commits for a given time.
- index segments are not only referenced by their name, but also with their generation
- the segments file must now also record the generation of a segment. the file itself must be generational itself.
- purging of outdated index segment commits
"
0,"Add LocalLucene. Local Lucene (Geo-search) has been donated to the Lucene project, per https://issues.apache.org/jira/browse/INCUBATOR-77.  This issue is to handle the Lucene portion of integration.

See http://lucene.markmail.org/message/orzro22sqdj3wows?q=LocalLucene
"
0,"Add MergePolicy to IndexWriterConfig. Now that IndexWriterConfig is in place, I'd like to move MergePolicy to it as well. The change is not straightforward and so I've kept it for a separate issue. MergePolicy requires in its ctor an IndexWriter, however none can be passed to it before an IndexWriter actually exists. And today IW may create an MP just for it to be overridden by the application one line afterwards. I don't want to make iw member of MP non-final, or settable by extending classes, however it needs to remain protected so they can access it directly. So the proposed changes are:

* Add a SetOnce object (to o.a.l.util), or Immutable, which can only be set once (hence its name). It'll have the signature SetOnce<T> w/ *synchronized set<T>* and *T get()*. T will be declared volatile, so that get() won't be synchronized.
* MP will define a *protected final SetOnce<IndexWriter> writer* instead of the current writer. *NOTE: this is a bw break*. any suggestions are welcomed.
* MP will offer a public default ctor, together with a set(IndexWriter).
* IndexWriter will set itself on MP using set(this). Note that if set will be called more than once, it will throw an exception (AlreadySetException - or does someone have a better suggestion, preferably an already existing Java exception?).

That's the core idea. I'd like to post a patch soon, so I'd appreciate your review and proposals."
0,"DbDataStore: delete temporary files using finalize(). Currently, reading from the DbDataStore creates a temporary file by default. If the application doesn't fully read or close the input stream, the file is not deleted. The best solution is to use finally { in.close() } in the application, but this is easily forgotten.

I suggest to delete the temp file using finalize(). There is a small performance penalty when creating the temporary object, but compared to I/O it is very small. Note that FileInputStream and FileOutputStream also use finalize()."
0,"JSR 283 Repository Descriptors. - new methods returning Value objects (jcr2spi)
- check descriptor-report in jcr-server and corresponding handling on the client side
- etc.etc."
0,"Move NoDeletionPolicy from benchmark to core. As the subject says, but I'll also make it a singleton + add some unit tests, as well as some documentation. I'll post a patch hopefully today."
0,"Allow to control how payloads are merged. Lucene handles backwards-compatibility of its data structures by
converting them from the old into the new formats during segment
merging. 

Payloads are simply byte arrays in which users can store arbitrary
data. Applications that use payloads might want to convert the format
of their payloads in a similar fashion. Otherwise it's not easily
possible to ever change the encoding of a payload without reindexing.

So I propose to introduce a PayloadMerger class that the SegmentMerger
invokes to merge the payloads from multiple segments. Users can then
implement their own PayloadMerger to convert payloads from an old into
a new format.

In the future we need this kind of flexibility also for column-stride
fields (LUCENE-1231) and flexible indexing codecs.

In addition to that it would be nice if users could store version
information in the segments file. E.g. they could store ""in segment _2
the term a:b uses payloads of format x.y"".
"
0,"Expose IndexFileNames as public, and make use of its methods in the code. IndexFileNames is useful for applications that extend Lucene, an in particular those who extend Directory or IndexWriter. It provides useful constants and methods to query whether a certain file is a core Lucene file or not. In addition, IndexFileNames should be used by Lucene's code to generate segment file names, or query whether a certain file matches a certain extension.

I'll post the patch shortly."
0,"Norm codec strategy in Similarity. The static span and resolution of the 8 bit norms codec might not fit with all applications. 

My use case requires that 100f-250f is discretized in 60 bags instead of the default.. 10?
"
0,"More Fine grained Permission Flags. It would be fine to have one more Permission Flag on node add.
At the moment there are 3 flags. We need to know if a node will be updated or created.
This is not possible with the current implementation because on node add the permission flag 
AccessManager.WRITE will be used. This is a Problem in a  WebDav Scenario with Microsoft-Word because if i open a Node and 
try to save it i need write permissions on the parent node. this is ok. If a user trys to save the file with a other name
he can because the same PermissionFlag will be used.
Maybe there is a other solution for this problem ?
BR,
claus"
0,"Deprecating InstantiatedIndexWriter. http://markmail.org/message/j6ip266fpzuaibf7

I suppose that should have been suggested before 2.9 rather than  
after...

There are at least three reasons to why I want to do this:

The code is based on the behaviour or the Directory IndexWriter as of  
2.3 and I have not been touching it since then. If there will be  
changes in the future one will have to keep IIW in sync, something  
that's easy to forget.
There is no locking which will cause concurrent modification  
exceptions when accessing the index via searcher/reader while  
committing.
It use the old token stream API so it has to be upgraded in case it  
should stay.

The java- and package level docs have since it was committed been  
suggesting that one should consider using II as if it was immutable  
due to the locklessness. My suggestion is that we make it immutable  
for real.

Since II is ment for small corpora there is very little time lost by  
using the constructor that builts the index from an IndexReader. I.e.  
rather than using InstantiatedIndexWriter one would have to use a  
Directory and an IndexWriter and then pass an IndexReader to a new  
InstantiatedIndex.

Any objections?"
0,Avoid using BitSets in ChildAxisQuery to minimize memory usage. When doing ChildAxisQueries on large indexes the internal BitSet instance (hits) may consume a lot of memory because the BitSet is always as large as IndexReader.maxDoc(). In our case we had a query consisting of 7 ChildAxisQueries which combined to a total of 14MB. Since we have multiple users executing this query simultaneously this caused an out of memory error.
0,"NullPointerException when temporary directory not readable. We have customers reporting errors such as:

Caused by: java.lang.NullPointerException
	at org.apache.lucene.store.FSDirectory.create(FSDirectory.java:200)
	at org.apache.lucene.store.FSDirectory.getDirectory(FSDirectory.java:144)
	at org.apache.lucene.store.FSDirectory.getDirectory(FSDirectory.java:117)
	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:205)
	at com.atlassian.jira.util.LuceneUtils.getIndexWriter(LuceneUtils.java:46)
	at com.atlassian.jira.issue.index.DefaultIndexManager.getIndexWriter(DefaultIndexManager.java:568)
	at com.atlassian.jira.issue.index.DefaultIndexManager.indexIssuesAndComments(DefaultIndexManager.java:287)
	... 59 more


This occurs when the lock directory is unreadable (eg. because Tomcat sets java.io.tmpdir to temp/ and the permissions here are broken). Attached is "
0,"Improvements to user management. Container issue for various improvements needed for the user management implementation in jr-core.

Known improvements are:

- extensibility
- current structuring of users/groups in the JCR content doesn't allow for easy finding user/group by ID.
- groupID should be unescaped before being returned by getID"
0,"Include generated website in the distribution. A user should be able to build the non-api docs as well.

So it would be nice, to include xdocs in the source packages as well ..."
0,"Add configurable hook for password validation. it's a common use case that applications would like to enforce additional logic associated with 
changing a user password. currently this can only be achieved by using a derived user implementation.
by extending the functionality added with JCR-3118 it was fairly trivial to provide a hook for those
custom password validation checks, writing password expiration date etc.etc. 
"
0,"JCR-Server Code depends on Log4J directly. The code is written against the Log4J APIs, which forces all users of Jackarabbit to pick up log4J dependency and to juggle with JDK logging and Log4J configuration if other components of the project uses JDK 1.4 logging.
If the code is move to depend on Apache commons-logging this issue will be resolved. Also this should be a minor fix. "
0,read IOHandlers from the config.xml. I would like to be able to change the order of the IOHandlers and add some.
0,"spi2dav : EventJournal not  implemented. i didn't look at the details just realized that all EventJournalTest of the TCK fail in the setup
jcr2spi - spi2dav(ex) - jcr-server.
i assume that this is due to missing implementation (the corresponding SPI method throws UnsupportedRepositoryOperationException)."
0,"CooperativeFileLock improvements. The CooperativeFileLock doesn't have any test cases. Also, there are a few ways to improve it:

- Avoid the exception ""lock file modified in the future""
- Stop the thread immediately after releasing the lock
- Detect locked repositories a bit earlier

"
0,"codec postings api (finishDoc) is inconsistent. finishDoc says:

{noformat}
  /** Called when we are done adding positions & payloads
   *  for each doc.  Not called  when the field omits term
   *  freq and positions. */
   public abstract void finishDoc() throws IOException;
{noformat}

But this is confusing (because a field can omit just positions, is it called then?!),
and wrong (because merging calls it always, even if freq+positions is omitted).

I think we should fix the javadoc and fix FreqProxTermsWriter to always call finish()
"
0,need getResponseContentLength in HttpMethod. We need a way to find out the response content length in HttpMethod
0,"Flex on non-flex emulation of TermsEnum incorrectly seeks/nexts beyond current field. Spinoff of LUCENE-2111, where Uwe found this issue with the flex on non-flex emulation."
0,"add shortcut method to CndImporter which makes it easier to rereigster node types. CndImporter has a two argument registerNodeTypes method which is a nice shortcut, but in order to rereigster node types, you have to use the non-shortcut method. Attached patch adds a three-argument method which provide a shortcut for doing reregistration."
0,"misleading lack of javadoc in StringRequestEntity. When using httpclient2, we were doing the following:

	// Add the Content-type header.  This sets the charset to UTF-8.
	method.setRequestHeader( ""Content-type"", ""text/xml; charset=UTF-8"" );
	// The given string is converted internally by the post method into
	// a UTF-8 encoded byte array.
	method.setRequestBody( xmlstring );

The comments show that this was the way we used to obtain a UTF-8 encoded XML
document (if this was wrong, that may be the origin of the problem?).


When upgrading to httpclient3 and killing deprecated code, this was converted to:

	// Add the Content-type header.  This sets the charset to UTF-8.
	method.setRequestHeader( ""Content-type"", ""text/xml; charset=UTF-8"" );
	// The given string is converted internally by the post method into
	// a UTF-8 encoded byte array.
        method.setRequestEntity( new StringRequestEntity( xmlstring ) );

which went without problem during the tests on my machine and on test production
machine.. because platforms charset were UTF-8, which is not the case for
production machines :(

I think the javadoc of the used StringRequestEntity constructor should strongly
state that it uses String#getBytes for the content, which uses the platform
charset. Also, I didn't notice any ""upgrade to 3.x"" documentation which would
have helped me :/"
0,"AttributeSource holds strong reference to class instances and prevents unloading e.g. in Solr if webapplication reload and custom attributes in separate classloaders are used (e.g. in the Solr plugins classloader). When working on the dynmaic proxy classes using cglib/javaassist i recognized a problem in the caching code inside AttributeSource:
- AttributeSource has a static (!) cache map that holds implementation classes for attributes to be faster on creating new attributes (reflection cost)
- AttributeSource has a static (!) cache map that holds a list of all interfaces implemented by a specific AttributeImpl

Also:
- VirtualMethod in 3.1 hold a map of implementation distances keyed by subclasses of the deprecated API

Both have the problem that this strong reference is inside Lucene's classloader and so persists as long as lucene lives. The classes referenced can never be unloaded therefore, which would be fine if all live in the same classloader. As soon as the Attribute or implementation class or the subclass of the deprecated API are loaded by a different classloder (e.g. Lucene lives in bootclasspath of tomcat, but lucene-consumer with custom attributes lives in a webapp), they can never be unloaded, because a reference exists.

Libs like CGLIB or JavaAssist or JDK's reflect.Proxy have a similar cache for generated class files. They also manage this by a WeakHashMap. The cache will always work perfect and no class will be evicted without reason, as classes are only unloaded when the classloader goes and this will only happen on request (e.g. by Tomcat)."
0,"JcrRemotingServlet should interpolate system properties in the home init-param. For deployment scenarios where the same Jackrabbit WAR file is deployed multiple times on the same server with the same current working directory, it is useful to have the home init-param support system property interpolation."
0,"Lazy field loading breaks backward compat. Document.getField() and Document.getFields() have changed in a non backward compatible manner.
Simple code like the following no longer compiles:
 Field x = mydoc.getField(""x"");"
0,"Data Store: enable and fix tests. Currently the unit test TestTwoGetStreams fails in the trunk (it worked in older versions). This should be fixed. 

Also, the data store is disabled by default, so this test doesn't run by default. The data store should be enabled for testing."
0,"TermVectorMapper.setDocumentNumber(). Passes down the index of the document whose term vector is currently beeing mapped, once for each top level call to a term vector reader.  

See http://www.nabble.com/Allowing-IOExceptions-in-TermVectorMapper--tf4687704.html#a13397341"
0,"DavMethods.POST should be public, not private. DavMethods.POST is declared as a private constant, when it should be public. attached is a patch to fix this."
0,"[PATCH] cleanup unwanted stream closing that isn't used. Due to refactoring, a stream is being closed that is never used. Isn't harmful, just is cruft."
0,Improve Performance of DescendantSelfAxisQuery. In DescendantSelfAxisQuery.DescendantSelfAxisScorer.isValid(int) contextHits is populated with docs that are found on the way down the axis. The current algorithm unfortunately doesn't add any new docs at all because it only adds docs already present in contextHits. This leads to more calls to HierarchyResolver.getParent(int) than necessary.
0,"Weight is not serializable for some of the queries. In order to work with ParallelMultiSearcher, Query weights need to be serializable.  The interface Weight extends java.io.Serializable, but it appears that some of the newer queries unnecessarily store global state in their weights, thus causing serialization errors."
0,"Add getIndexCommit method to IndexReader. Spinoff from this thread:

  http://markmail.org/message/bojgqfgyxkkv4fyb

I think it makes sense ask an IndexReader for the commit point it has
open.  This enables the use case described in the above thread, which
is to create a deletion policy that is able to query all open readers
for what commit points they are using, and prevent deletion of them.

"
0,"populate.jsp uses Java 1.5 method. The method is URLConnection.setReadTimeout()
"
0,"Typo in repository.xml. The extractorPoolSize parameter name has a trailing white space, which makes jackrabbit ignore it."
0,"GCJ makefile hardcodes compiler commands. src/gcj/Makefile hardcodes the command names for gcj, gcjh, and g++. This makes it difficult to 
compile with a particular version of GCJ if multiple are installed with suffixes (eg, gcj-4.0)

Steps to reproduce:
1. Configure, compile, and install GCC/GCJ with something like --program-suffix=-4.0
2. cd ~/src/lucene && ant gcj

Expected results:
Somehow be able to specify my compiler.

Actual results:
Can't find 'gcj' executable, or worse runs wrong version. :)

Suggested fix: as is common with variable names like CC to force a C compiler, allow the builder to 
override the compiler commands used by setting optional environment variables GCJ etc.
Patch to be attached.

Additional info:
Building Lucene from SVN 2005-04-19."
0,"allow specifying -Dbootclasspath for javac/javadocs. So that you can compile/javadoc against the actual target JRE libraries
even if you have a newer compiler."
0,"Enable access to the freq information in a Query's sub-scorers. The ability to gather more details than just the score, of how a given
doc matches the current query, has come up a number of times on the
user's lists.  (most recently in the thread ""Query Match Count"" by
Ryan McV on java-user).

EG if you have a simple TermQuery ""foo"", on each hit you'd like to
know how many times ""foo"" occurred in that doc; or a BooleanQuery +foo
+bar, being able to separately see the freq of foo and bar for the
current hit.

Lucene doesn't make this possible today, which is a shame because
Lucene in fact does compute exactly this information; it's just not
accessible from the Collector.
"
0,Backport JCR-1111: Access to version history results in reading all versions of versionable node. Backport issue JCR-1111 (Accesss to version history results in reading all versions of versionable node) to 1.3 branch for 1.3.4 (separate issue to avoid re-opening JCR-1111 which was already released with 1.4).
0,"AttributeSource.addAttribute should only accept interfaces, the missing test leads to problems with Token.TOKEN_ATTRIBUTE_FACTORY. This is a blocker, because you can call addAttribute(Token.class) without getting an error message.

I will commit the fix and restart the vote for 3.0. This also applies to 2.9, but there is no Token Attribute Factory. But I will merge to 2.9, too, if a 2.9.2 comes."
0,"Use only the standard Maven repository for dependencies. The JCR API jars are now available in the standard Maven repository, see http://jira.codehaus.org/browse/MAVENUPLOAD-1050. We could thus remove the dependency on the Day repository, as requested in http://jira.codehaus.org/browse/MEV-453.
"
0,"remove DocsAndPositionsEnum.getPayloadLength. This was an accidental leftover; now that getPayload returns a BytesRef, this method is not needed."
0,"Maybe rename Field.omitTf, and strengthen the javadocs. Spinoff from here:

  http://www.nabble.com/search-problem-when-indexed-using-Field.setOmitTf()-td22456141.html

Maybe rename omitTf to something like omitTermPositions, and make it clear what queries will silently fail to work as a result."
0,"Various places do map lookups in loop instead of using entrySet iterator. Various places loop over a keyset iterator and do a map look up each time thru the loop, I plan to convert these places to use an entryset iterator to avoid this."
0,CharacterCache - references deleted . CharacterCache is deprecated by Character.valueOf(c) . Hence the latter is chosen over the former. 
0,"Support multiple proxies. HttpClient supports one proxy currently.
Our requirement is to suppport more than one proxy. We may need to connect more than one proxies before connects to target resource. 
I found that HttpMethodDirector creates tunnelled socket and there is no easy way to plugin our custom HttpMethodDirector class with HttpClient other than extending HttpClient to override ""public int executeMethod(HostConfiguration hostconfig, final HttpMethod method, final HttpState state"" method.


"
0,"Optimize BlockTermsReader.seek. When we seek, we first consult the terms index to find the right block
of 32 (default) terms that may hold the target term.  Then, we scan
that block looking for an exact match.

The scanning just uses next() and then compares the full term, but
this is actually rather wasteful.  First off, since all terms in the
block share a common prefix, we should compare the target against that
common prefix once, and then only compare the new suffix of each
term.  Second, since the term suffixes have already been read up front
into a byte[], we should do a no-copy comparison (vs today, where we
first read a copy into the local BytesRef and then compare).

With this opto, I removed the ability for BlockTermsWriter/Reader to
support arbitrary term sort order -- it's now hardwired to
BytesRef.utf8SortedAsUnicode.
"
0,"spi2dav: Drop Q*DefinitionImpl implementations and use spi-commons Q*DefinitionBuilder. spi2dav provides separate implementations of the Q*Definition interfaces that apart from the construction just duplicate the code
present in spi-commons. Instead the Q*DefinitionBuilder helpers could be used to generate the definition instances."
0,"add one setter for start and end offset to OffsetAttribute. add OffsetAttribute. setOffset(startOffset, endOffset);

trivial change, no JUnit needed

Changed CharTokenizer to use it"
0,"Maven artifacts for Lucene 4 are not stored in the correct path. Hello,

I would like to use the maven artifacts for Lucene 4.0 produced by the Hudson build machine. The artifacts are correctly produced (http://hudson.zones.apache.org/hudson/view/Lucene/job/Lucene-trunk/lastSuccessfulBuild/artifact/maven_artifacts/lucene/).
However, the artifacts which should be stored under the path ""org/apache/lucene/"" are currently stored under ""lucene"" which prevents a project using maven to correctly download the Lucene 4.0 artifacts.

Thanks again for your help.  "
0,"Move FunctionQuery, ValueSources and DocValues to Queries module. Having resolved the FunctionQuery sorting issue and moved the MutableValue classes, we can now move FunctionQuery, ValueSources and DocValues to a Queries module."
0,"sort missing string fields last. A SortComparatorSource for string fields that orders documents with the sort
field missing after documents with the field.  This is the reverse of the
default Lucene implementation.

The concept and first-pass implementation was done by Chris Hostetter."
0,"Observation logs error when a node is moved in place. An error message is written to the log when the following sequence of operations is executed:

- create node 'parent'
- create node 'child' as a child of 'parent'
- save
- create node 'tmp'
- move 'child' under 'tmp'
- remove 'parent'
- move 'tmp' to former path of 'parent'

The log will say: EventStateCollection: Unable to calculate old path of moved node

This is because the zombie path of 'child' is equal to the new path after the move. The EventStateCollection detects a new parentId assigned to 'child' and expects a new path that is different from the zombie path. The above case however shows that there is a use case where the paths are equal and events should be generated."
0,"Czech Stemmer. Currently, the CzechAnalyzer is merely stopwords, and there isn't a czech stemmer in snowball.

This patch implements the light stemming algorithm described in: http://portal.acm.org/citation.cfm?id=1598600

In their measurements, it improves MAP 42%

The analyzer does not use this stemmer if LUCENE_VERSION <= 3.0, for back compat.
"
0,"Contributed utility for determing content type from file type extension. /*
 * ====================================================================
 *
 * The Apache Software License, Version 1.1
 *
 * Copyright (c) 2002-2003 The Apache Software Foundation.  All rights
 * reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 *
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 *
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in
 *    the documentation and/or other materials provided with the
 *    distribution.
 *
 * 3. The end-user documentation included with the redistribution, if
 *    any, must include the following acknowlegement:
 *       ""This product includes software developed by the
 *        Apache Software Foundation (http://www.apache.org/).""
 *    Alternately, this acknowlegement may appear in the software itself,
 *    if and wherever such third-party acknowlegements normally appear.
 *
 * 4. The names ""The Jakarta Project"", ""Commons"", and ""Apache Software
 *    Foundation"" must not be used to endorse or promote products derived
 *    from this software without prior written permission. For written
 *    permission, please contact apache@apache.org.
 *
 * 5. Products derived from this software may not be called ""Apache""
 *    nor may ""Apache"" appear in their names without prior written
 *    permission of the Apache Group.
 *
 * THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESSED OR IMPLIED
 * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
 * DISCLAIMED.  IN NO EVENT SHALL THE APACHE SOFTWARE FOUNDATION OR
 * ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
 * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
 * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF
 * USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
 * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
 * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT
 * OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 * ====================================================================
 *
 * This software consists of voluntary contributions made by many
 * individuals on behalf of the Apache Software Foundation.  For more
 * information on the Apache Software Foundation, please see
 * <http://www.apache.org/>.
 *
 * [Additional notices, if required by prior licensing conditions]
 *
 */

package org.apache.commons.httpclient.contrib.utils;

import java.io.File;
import java.io.IOException;

/**
 * This class provides mappings from file name extensions to content types.
 *
 * @author <a href=""mailto:emdevlin@charter.net"">Eric Devlin</a>
 * 
 * DISCLAIMER: HttpClient developers DO NOT actively support this component.
 * The component is provided as a reference material, which may be inappropriate
 * to be used without additional customization.
 */

public class ContentType {

	/** Mime Type mappings 'liberated' from Tomcat4.1.18/conf/web.xml*/
	public static final String[][] MIME_TYPE_MAPPINGS =	
		{	{ ""abs"",		""audio/x-mpeg"" },
			{ ""ai"",			""application/postscript"" },
			{ ""aif"",		""audio/x-aiff"" },
			{ ""aifc"",		""audio/x-aiff"" },
			{ ""aiff"",		""audio/x-aiff"" },
			{ ""aim"",		""application/x-aim"" },
			{ ""art"",		""image/x-jg"" },
			{ ""asf"",		""video/x-ms-asf"" },
			{ ""asx"",		""video/x-ms-asf"" },
			{ ""au"",			""audio/basic"" },
			{ ""avi"",		""video/x-msvideo"" },
			{ ""avx"",		""video/x-rad-screenplay"" },
			{ ""bcpio"",		""application/x-bcpio"" },
			{ ""bin"",		""application/octet-stream"" },
			{ ""bmp"",		""image/bmp"" },
			{ ""body"",		""text/html"" },
			{ ""cdf"",		""application/x-cdf"" },
			{ ""cer"",		""application/x-x509-ca-cert"" },
			{ ""class"",		""application/java"" },
			{ ""cpio"",		""application/x-cpio"" },
			{ ""csh"",		""application/x-csh"" },
			{ ""css"",		""text/css"" },
			{ ""dib"",		""image/bmp"" },
			{ ""doc"",		""application/msword"" },
			{ ""dtd"",		""text/plain"" },
			{ ""dv"",			""video/x-dv"" },
			{ ""dvi"",		""application/x-dvi"" },
			{ ""eps"",		""application/postscript"" },
			{ ""etx"",		""text/x-setext"" },
			{ ""exe"",		""application/octet-stream"" },
			{ ""gif"",		""image/gif"" },
			{ ""gtar"",		""application/x-gtar"" },
			{ ""gz"",			""application/x-gzip"" },
			{ ""hdf"",		""application/x-hdf"" },
			{ ""hqx"",		""application/mac-binhex40"" },
			{ ""htc"",		""text/x-component"" },
			{ ""htm"",		""text/html"" },
			{ ""html"",		""text/html"" },
			{ ""hqx"",		""application/mac-binhex40"" },
			{ ""ief"",		""image/ief"" },
			{ ""jad"",		""text/vnd.sun.j2me.app-
descriptor"" },
			{ ""jar"",		""application/java-archive"" },
			{ ""java"",		""text/plain"" },
			{ ""jnlp"",		""application/x-java-jnlp-
file"" },
			{ ""jpe"",		""image/jpeg"" },
			{ ""jpeg"",		""image/jpeg"" },
			{ ""jpg"",		""image/jpeg"" },
			{ ""js"",			""text/javascript"" },
			{ ""jsf"",		""text/plain"" },
			{ ""jspf"",		""text/plain"" },
			{ ""kar"",		""audio/x-midi"" },
			{ ""latex"",		""application/x-latex"" },
			{ ""m3u"",		""audio/x-mpegurl"" },
			{ ""mac"",		""image/x-macpaint"" },
			{ ""man"",		""application/x-troff-man"" },
			{ ""me"",			""application/x-troff-me"" },
			{ ""mid"",		""audio/x-midi"" },
			{ ""midi"",		""audio/x-midi"" },
			{ ""mif"",		""application/x-mif"" },
			{ ""mov"",		""video/quicktime"" },
			{ ""movie"",		""video/x-sgi-movie"" },
			{ ""mp1"",		""audio/x-mpeg"" },
			{ ""mp2"",		""audio/x-mpeg"" },
			{ ""mp3"",		""audio/x-mpeg"" },
			{ ""mpa"",		""audio/x-mpeg"" },
			{ ""mpe"",		""video/mpeg"" },
			{ ""mpeg"",		""video/mpeg"" },
			{ ""mpega"",		""audio/x-mpeg"" },
			{ ""mpg"",		""video/mpeg"" },
			{ ""mpv2"",		""video/mpeg2"" },
			{ ""ms"",			""application/x-wais-source"" },
			{ ""nc"",			""application/x-netcdf"" },
			{ ""oda"",		""application/oda"" },
			{ ""pbm"",		""image/x-portable-bitmap"" },
			{ ""pct"",		""image/pict"" },
			{ ""pdf"",		""application/pdf"" },
			{ ""pgm"",		""image/x-portable-graymap"" },
			{ ""pic"",		""image/pict"" },
			{ ""pict"",		""image/pict"" },
			{ ""pls"",		""audio/x-scpls"" },
			{ ""png"",		""image/png"" },
			{ ""pnm"",		""image/x-portable-anymap"" },
			{ ""pnt"",		""image/x-macpaint"" },
			{ ""ppm"",		""image/x-portable-pixmap"" },
			{ ""ps"",			""application/postscript"" },
			{ ""psd"",		""image/x-photoshop"" },
			{ ""qt"",			""video/quicktime"" },
			{ ""qti"",		""image/x-quicktime"" },
			{ ""qtif"",		""image/x-quicktime"" },
			{ ""ras"",		""image/x-cmu-raster"" },
			{ ""rgb"",		""image/x-rgb"" },
			{ ""rm"",			""application/vnd.rn-
realmedia"" },
			{ ""roff"",		""application/x-troff"" },
			{ ""rtf"",		""application/rtf"" },
			{ ""rtx"",		""text/richtext"" },
			{ ""sh"",			""application/x-sh"" },
			{ ""shar"",		""application/x-shar"" },
			{ ""smf"",		""audio/x-midi"" },
			{ ""snd"",		""audio/basic"" },
			{ ""src"",		""application/x-wais-source"" },
			{ ""sv4cpio"",	""application/x-sv4cpio"" },
			{ ""sv4crc"",		""application/x-sv4crc"" },
			{ ""swf"",		""application/x-shockwave-
flash"" },
			{ ""t"",			""application/x-troff"" },
			{ ""tar"",		""application/x-tar"" },
			{ ""tcl"",		""application/x-tcl"" },
			{ ""tex"",		""application/x-tex"" },
			{ ""texi"",		""application/x-texinfo"" },
			{ ""texinfo"",	""application/x-texinfo"" },
			{ ""tif"",		""image/tiff"" },
			{ ""tiff"",		""image/tiff"" },
			{ ""tr"",			""application/x-troff"" },
			{ ""tsv"",		""text/tab-separated-values"" },
			{ ""txt"",		""text/plain"" },
			{ ""ulw"",		""audio/basic"" },
			{ ""ustar"",		""application/x-ustar"" },
			{ ""xbm"",		""image/x-xbitmap"" },
			{ ""xml"",		""text/xml"" },
			{ ""xpm"",		""image/x-xpixmap"" },
			{ ""xsl"",		""text/xml"" },
			{ ""xwd"",		""image/x-xwindowdump"" },
			{ ""wav"",		""audio/x-wav"" },
			{ ""svg"",		""image/svg+xml"" },
			{ ""svgz"",		""image/svg+xml"" },
			{ ""wbmp"",		""image/vnd.wap.wbmp"" },
			{ ""wml"",		""text/vnd.wap.wml"" },
			{ ""wmlc"",		""application/vnd.wap.wmlc"" },
			{ ""wmls"",		""text/vnd.wap.wmlscript"" },
			{ ""wmlscriptc"",	""application/vnd.wap.wmlscriptc"" },
			{ ""wrl"",		""x-world/x-vrml"" },
			{ ""Z"",			""application/x-compress"" },
			{ ""z"",			""application/x-compress"" },
			{ ""zip"",		""application/zip"" } };

    /**
     * Get the content type based on the extension of the file name<br>
     *
     * @param fileName for which the content type is to be determined.
     *
     * @return the content type for the file or null if no mapping was
     * possible.
     */
	public static String get( String fileName  ) {
		String contentType = null;

		if ( fileName != null ) {
			int extensionIndex = fileName.lastIndexOf( '.' );
			if ( extensionIndex != -1 ) {
				if ( extensionIndex + 1 < fileName.length() ) {
					String extension = fileName.substring( 
extensionIndex + 1 );
					for( int i = 0; i < 
MIME_TYPE_MAPPINGS.length; i++ ) {
						if ( extension.equals( 
MIME_TYPE_MAPPINGS[i][0] ) ) {
							contentType = 
MIME_TYPE_MAPPINGS[i][1];
							break;
						}
					}
				}
			}
		}

		return contentType;
	}

    /**
     * Get the content type based on the extension of the file name<br>
     *
     * @param file for which the content type is to be determined.
     *
     * @return the content type for the file or null if no mapping was
     * possible.
     *
     * @throws IOException if the construction of the canonical path for 
	 * the file fails.
     */
	public static String get( File file ) 
		throws IOException
	{
		String contentType = null;

		if ( file != null ) {

			contentType = get( file.getCanonicalPath() );
		}

		return contentType;
	}
}"
0,"RFC4918 feature: absolute paths in ""Destination"" and ""If"" headers. RFC4918 allows absolute paths (instead of absolute URIs) in the ""Destination"" and ""If"" headers (<http://greenbytes.de/tech/webdav/rfc4918.html#rfc.section.14.8>). This makes it simpler to deal with situations where reverse proxies are involved (because those usually are not aware of WebDAV request headers and do not rewrite them)."
0,"[PATCH] Clear ThreadLocal instances in close(). As already found out in LUCENE-436, there seems to be a garbage collection problem with ThreadLocals at certain constellations, resulting in an OutOfMemoryError.
The resolution there was to remove the reference to the ThreadLocal value when calling the close() method of the affected classes (see FieldsReader and TermInfosReader).
For Java < 5.0, this can effectively be done by calling threadLocal.set(null); for Java >= 5.0, we would call threadLocal.remove()

Analogously, this should be done in *any* class which creates ThreadLocal values

Right now, two classes of the core API make use of ThreadLocals, but do not properly remove their references to the ThreadLocal value
1. org.apache.lucene.index.SegmentReader
2. org.apache.lucene.analysis.Analyzer

For SegmentReader, I have attached a simple patch.
For Analyzer, there currently is no patch because Analyzer does not provide a close() method (future to-do?)

"
0,Contrib-Spatial should use DocSet API rather then deprecated BitSet API. Contrib-Spatial should be rewritten to use the new DocIdSet Filter API with OpenBitSets instead of j.u.BitSets. FilteredDocIdSet can be used to replace (I)SerialChainFilter.
0,"Refactoring config handling. As discussed on the mailing list:

   * Move all XML handling to a separate ConfigurationParser class
   * Clean up and document the *Config classes
   * Get rid of the messy AbstractConfig class"
0,"ProtocolSocketFactory equals and hashCode don't support subclassing. In the implemenation of equals and hashCode for the classes
org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory
org.apache.commons.httpclient.protocol.SSLProtocolSocketFactory

The implementation of equals and hashCode attempts to make all instances of the classes equal.  However, the manner in which the methods are coded makes it necessary for any subclass to implement equals and hashCode themselves.  A minor change to the methods in these classes will make possible to subclass these factories without re-implementing the equals and hashCode.  The method equals should be written as

        return ((obj != null) && obj.getClass().equals(getClass()));

rather than

        return ((obj != null) && obj.getClass().equals(DefaultProtocolSocketFactory.class));

And similarly, the hashCode method should be

        return getClass().hashCode();

rather than

        return DefaultProtocolSocketFactory.class.hashCode();"
0,"Support for OpenOffice text extraction. Hi, here is the patch.

>hi nicolas,
>
>thanks for your offer to contribute your openoffice textfilter, that's greatly appreciated!
>
>i suggest you post a jira 'Improvement' or ""New Feature' issue and attach your code as an svn patch. somebody will take care 
>of it (i assume marcel), i.e. review your contribution and provide feedback/further instructions.

>cheers
>stefan

>On 2/2/06, Nicolas Jouanin <nicolas.jouanin@gmail.com> wrote:
>>
>>
>>
>> Hi Stefan,
>>
>>
>>
>> I work with Martin Perez, main developper of jLibrary.
>>
>> Therefore, I developed a class which extracts metadata and text 
>> content from any openoffice file (that was not the hardest job). This 
>> class is already used into jLibrary.
>>
>> As Martin suggested me, I used this class to create a new TextFilter 
>> subclass into textfilters contrib project. I downloaded textfilters 
>> project from svn, created my class into the project tree and tested it 
>> with a test class , just like it was done with the other extractors.
>>
>> I can send you the code if you want to review it, or just tell me how 
>> I can commit it.
>>
>>
>>
>> Regards,
>>
>>
>>
>> Nicolas.
"
0,"FST apis out of sync between trunk/3.x. Looks like the offender is LUCENE-3030 :)

Not sure if everything is generally useful but it does change the public API (e.g. you can specify FreezeTail to the super-scary Builder ctor among other things).

Maybe we should sync up for 3.x? "
0,Some unit tests are not well configured. Some unit tests used for the annotation support are not well defined. There are inherited from DigesterTestBase instead of AnnotationTestBase
0,Add parser callback to GQL. The parsing of GQL is currently hidden in the implementation. It would be nice to have a mechanism that allows client code to get callbacks whenever a field/value pair is parsed.
0,Speed up repeated TokenStream init.  by caching isMethodOverridden results
0,"Bad transitive dependencies in commons-httpclient. As reported in HTTPCLIENT-605, the commons-httpclient 3.0 dependency introduces junit as a transitive ""compile"" scope dependency. The library also uses commons-logging, which sidesteps Jackrabbit's use of slf4j for logging.

To avoid these issues we should locally override the junit dependency in commons-httpclient and replace the commons-logging dependency with jcl104-over-slf4j."
0,"getPayloadSpans on org.apache.lucene.search.spans.SpanQuery should be abstract. I just spent a long time tracking down a bug resulting from upgrading to Lucene 2.4.1 on a project that implements some SpanQuerys of its own and was written against 2.3.  Since the project's SpanQuerys didn't implement getPayloadSpans, the call to that method went to SpanQuery.getPayloadSpans which returned null and caused a NullPointerException in the Lucene code, far away from the actual source of the problem.  

It would be much better for this kind of thing to show up at compile time, I think.

Thanks!"
0,"remove MoreLikeThis's default analyzer. MoreLikeThis has the following:

{code}
public static final Analyzer DEFAULT_ANALYZER = new StandardAnalyzer(Version.LUCENE_CURRENT);
{code}"
0,"Revert subsequent token-node updates (tentatively introduced). i would like to revert this improvement has been tentatively introduced based on the following
thread on the dev list: http://www.mail-archive.com/dev@jackrabbit.apache.org/msg24437.html
as i am still concerned about undesired effects. in addition i still feel that this somehow
violates the basic contract."
0,Create enwiki indexable data as line-per-article rather than file-per-article. Create a line per article rather than a file. Consume with indexLineFile task.
0,JSR 283: NodeType Management. 
0,"Improve how ConcurrentMergeScheduler handles too-many-merges case. CMS now lets you set ""maxMergeThreads"" to control max # simultaneous
merges.

However, when CMS hits that max, it still allows further merges to
run, by running them in the foreground thread.  So if you set this max
to 1, and use 1 thread to add docs, you can get 2 merges running at
once (which I think is broken).

I think, instead, CMS should pause the foreground thread, waiting
until the number of merge threads drops below the limit.  Then, kick
off the backlog merge in a thread and return control back to primary
thread.
"
0,"CacheManager resizeAll is slow. CacheManager.resizeAll calls log.debug with a complex constructed log message.
The message is immediately discarded except when using the debug log level.

To improve performance, log.isDebugEnabled() should be used."
0,"Change QueryParser to use ConstantScoreRangeQuery in preference to RangeQuery by default. Change to QueryParser to default to using new ConstantScoreRangeQuery in preference to RangeQuery
for range queries. This implementation is generally preferable because it 
a) Runs faster 
b) Does not have the scarcity of range terms unduly influence score 
c) avoids any ""TooManyBooleanClauses"" exception.

However, if applications really need to use the old-fashioned RangeQuery and the above
points are not required then the  ""useOldRangeQuery"" property can be used to revert to old behaviour.

The patch includes extra Junit tests for this flag and all other Junit tests pass"
0,"increase default maxFieldLength?. To my understanding, Lucene 2.3 will easily index large documents. So shouldn't we get rid of the 10,000 default limit for the field length? 10,000 isn't that much and as Lucene doesn't have any error logging by default, this is a common problem for users that is difficult to debug if you don't know where to look.

A better new default might be Integer.MAX_VALUE.
"
0,"enable DefaultSimilarity.setDiscountOverlaps by default. I think we should enable setDiscountOverlaps in DefaultSimilarity by default.

If you are using synonyms or commongrams or a number of other 0-posInc-term-injecting methods, these currently screw up your length normalization.
These terms have a position increment of zero, so they shouldnt count towards the length of the document.

I've done relevance tests with persian showing the difference is significant, and i think its a big trap to anyone using synonyms, etc: your relevance can actually get worse if you don't flip this boolean flag."
0,"Wrapup flexible indexing. Spinoff from LUCENE-1458.

The flex branch is in fairly good shape -- all tests pass, initial search performance testing looks good, it survived several visits from the Unicode policeman ;)

But it still has a number of nocommits, could use some more scrutiny especially on the ""emulate old API on flex index"" and vice/versa code paths, and still needs some more performance testing.  I'll do these under this issue, and we should open separate issues for other self contained fixes.

The end is in sight!"
0,"Create a new sub-class of SpanQuery to enable use of a RangeQuery within a SpanQuery. Our users express queries using a syntax which enables them to embed various query types within SpanQuery instances.  One feature they've been asking for is the ability to embed a numeric range query so they could, for example, find documents matching ""[2.0 2.75]MHz"".  The attached patch adds the capability and I hope others will find it useful.
"
0,"Support for passing an SSLContext to the SSLSocketFactory of HttpClient. Would it be possible to use an existing instance of SSLContext to initialise an SSLSocketFactory? This would allow using SSLContexts configured with more options, such as CRLs.

(This follows the thread of the httpclient-commons-dev list: http://marc.info/?l=httpclient-commons-dev&m=121737017814116&w=2 )."
0,Remove SessionImpl dependency from QueryObjectModelFactoryImpl. The QueryObjectModelFactoryImpl should be independent of the jackrabbit-core.
0,"setAuthPreemptive restricted to BASIC AuthScheme. Pre-emptive authentication is hardcoded to be restricted to the BASIC
authentication scheme.  To fully support custom authentication schemes,
pre-emptive authentication should be made configurable, either globally, or on a
per-scheme basis.  A potential compromise may be to require AuthSchemes to
report whether they support pre-emptive capability if we wish to explicitly
exclude certain schemes from pre-emptive authentication.

(reported against 3.0 RC 1)"
0,"speed up core tests. Our core tests have gotten slower and slower, if you don't have a really fast computer its probably frustrating.

I think we should:
1. still have random parameters, but make the 'obscene' settings like SimpleText rarer... we can always make them happen more on NIGHTLY
2. tests that make a lot of documents can conditionalize on NIGHTLY so that they are still doing a reasonable test on ordinary runs e.g. numdocs = (NIGHTLY ? 10000 : 1000) * multiplier
3. refactor some of the slow huge classes with lots of tests like TestIW/TestIR, at least pull out really slow methods like TestIR.testDiskFull into its own class. this gives better parallelization.
"
0,"AccessControlManager#getApplicablePolicy should check for colliding rep:policy node. while AccessControlManager#getApplicablePolicy returns an empty iterator if the target node cannot get the accesscontrollable-mixin set, it does not test if there is a colliding child node that would prevent the policy to be applied calling AccessControlManager#setPolicy. consequently, the setPolicy call fails with ItemExistsException. A simple test upfront could prevent this unexpected failure."
0,"HTTPClient 4.1 auto slash removal. I've put the same comment as in the following issue.

https://issues.apache.org/jira/browse/HTTPCLIENT-929?focusedCommentId=13001748#comment-13001748

I am using httpclient 4.1. I had a problem with this fix. In DefaultRequestDirector.rewriteRequestURI method, for non-proxied URI and when it is a absolute URI, it will call the URIUtils.rewriteURI, which then take the ""RawPath"" from an uri and normalize it. So when I pass an uri, for example, http://www.whatever.com/1//3, it will automatically remove the extra slash and become http://www.whatever.com/1/3. I've got a REStful service to accept the uri (/{param1}/{param2}/{param3}) and it takes when there is an empty value past in. Now because of the auto slash removal, the ""3"" value shift left for a position and match to the {param2}. I wouldn't say the above solution is wrong, but I guess it should not change what value that user pass in."
0,"Handle date values in the far future or prevent these from being persisted. Setting a date property with a value in the far future (e.g., the year 20009) and saving the session causes the index component to throw an exception (see the DateField#timeToString method). Furthermore, when the repository is restarted, the properties' value cannot be retrieved anymore because of a ValueFormatException caused by an empty value. Restarting the repository with an empty search index does not work because indexing fails. I haven't looked into the effect on queries.
"
0,"Allow MergePolicy to select non-contiguous merges. I started work on this but with LUCENE-1044 I won't make much progress
on it for a while, so I want to checkpoint my current state/patch.

For backwards compatibility we must leave the default MergePolicy as
selecting contiguous merges.  This is necessary because some
applications rely on ""temporal monotonicity"" of doc IDs, which means
even though merges can re-number documents, the renumbering will
always reflect the order in which the documents were added to the
index.

Still, for those apps that do not rely on this, we should offer a
MergePolicy that is free to select the best merges regardless of
whether they are continuguous.  This requires fixing IndexWriter to
accept such a merge, and, fixing LogMergePolicy to optionally allow
it the freedom to do so.
"
0,"Jcr-Server: DavResource#getDavSession() missing. Instead of having DavResource#getDavSession() this method is defined individually by most of the
derived interfaces."
0,"contrib/benchmark tests fail find data dirs. This was exposed by LUCENE-940 - a test was added that uses the Reuters collection. Then tests succeed when ran from contrib/benchmark (e.g. by IDE) but fail when running as part of ""ant test-contrib"" because the test expects to find the Reuters data under trunk/work. 
"
0,"Provide limit on phrase analysis in FastVectorHighlighter. With larger documents, FVH can spend a lot of time trying to find the best-scoring snippet as it examines every possible phrase formed from matching terms in the document.  If one is willing to accept
less-than-perfect scoring by limiting the number of phrases that are examined, substantial speedups are possible.  This is analogous to the Highlighter limit on the number of characters to analyze.

The patch includes an artifical test case that shows > 1000x speedup.  In a more normal test environment, with English documents and random queries, I am seeing speedups of around 3-10x when setting phraseLimit=1, which has the effect of selecting the first possible snippet in the document.  Most of our sites operate in this way (just show the first snippet), so this would be a big win for us.

With phraseLimit = -1, you get the existing FVH behavior. At larger values of phraseLimit, you may not get substantial speedup in the normal case, but you do get the benefit of protection against blow-up in pathological cases.
"
0,"AppendRecord writes single bytes to disk. The AppendRecord initially buffers writes in memory and starts
to write it to a temp file as soon as it occupies more than
64k heap. After switching to the temp file, data is written
unbuffered."
0,"Windows specific implementation of the Digest auth scheme. Microsoft Windows 2003 implementation of digest auth scheme is essentially a
superset of RFC 2617 with Windows specific aspects:
http://www.microsoft.com/technet/prodtechnol/windowsserver2003/library/TechRef/717b450c-f4a0-4cc9-86f4-cc0633aae5f9.mspx

Provide a super class of DigestScheme with Windows 2003 specific extensions,
which can be plugged in instead of the standard Digest impl

For details see PR #34909"
0,"Separate NOTICEs and LICENSEs for binary and source packages. Based on recent discussions on sling-dev@ (see [1]) and on legal-discuss@  (see [2]), I'd like to rearrange our NOTICE and LICENSE files so that the root level files refer only to bits included in source releases and that the (in some cases different) files to be included in the binary artifacts would be placed in src/main/resources/META-INF.

See also JCR-1630 for related work.

[1] http://markmail.org/message/2enw6ktxhc4ixmrk
[2] http://markmail.org/message/bttmkavpicxxg7gl
"
0,"Include diagnostics per-segment when writing a new segment. It would be very helpful if each segment in an index included
diagnostic information, such as the current version of Lucene.

EG, in LUCENE-1474 this would be very helpful to see if certain
segments were written under 2.4.0.

We can start with just the current version.

We could also consider making this extensible, so you could provide
your own arbitrary diagnostics, but SegmentInfo/s is not public so I
think such an API would be ""one-way"" in that you'd have to use
CheckIndex to check on it later.  Or we could wait on such extensibility
until we provide some consistent way to access per-segment details
in the index.
"
0,"Add a isDeleted method to IndexCommit. I wish to add a IndexCommit.isDeleted() method.

The use-case is that Solr will now support configurable IndexDeletionPolicy (SOLR-617). For the new replication (SOLR-561) to work, we need access to a list of IndexCommit instances which haven't been deleted yet. I can wrap the user specified IndexDeletionPolicy but since the IndexCommit does not have a isDeleted method, I may store a reference to an IndexCommit on which delete() has been called by the deletion policy. I can wrap the IndexCommit objects too just for having a isDeleted() method so a workaround exists. Not a big pain but if it can be managed on the lucene side easily, I'll appreciate it. It would save me from writing some delegate code."
0,"Decide if we should remove lines numbers from latest Changes. As Lucene dev has grown, a new issue has arisen - many times, new changes invalidate old changes. A proper changes file should just list the changes from the last version, not document the dev life of the issues. Keeping changes in proper order now requires a lot of renumbering sometimes. The numbers have no real meaning and could be added to more rich versions (such as the html version) automatically if desired.

I think an * makes a good replacement myself. The issues already have ids that are stable, rather than the current, decorational numbers which are subject to change over a dev cycle.

I think we should replace the numbers with an asterix for the 2.9 section and going forward (ie 4. becomes *).

If we don't get consensus very quickly, this issue won't block."
0,"add infrastructure for longer running nightly test cases. I'm spinning this out of LUCENE-2762...

The patch there adds initial infrastructure for tests to pull documents from a line file, and adds a longish running test case using that line file to test NRT.

I'd like to see some tests run on more substantial indices based on real data... so this is just a start."
0,"Analysis for Irish. Adds analysis for Irish.

The stemmer is generated from a snowball stemmer. I've sent it to Martin Porter, who says it will be added during the week."
0,Webdav: Drop xerces dependency. jukka is the final assignee :) thanks for taking over.
0,"improve documentation of SPI Batch addProperty. Clarify that Batch.addProperty should succeed even though the property already exists.


(See mailing list thread starting with: http://mail-archives.apache.org/mod_mbox/jackrabbit-dev/200801.mbox/%3c47A1E1C1.2050107@gmx.de%3e)"
0,"Enable maven-source-plugin. Currently the maven-source-plugin is enabled by default in jackrabbit-jcr-rmi, but it would be good to enable it globally for all Jackrabbit components."
0,"Flexible query parser does not support open ranges and range queries with mixed inclusive and exclusive ranges. Flexible query parser does not support open ranges and range queries with mixed inclusive and exclusive ranges.

These two problems were found while developing LUCENE-1768."
0,"Simultaneous updates by multiple sessions might not appear in the journal. In a clustering environment, simultaneous updates by multiple sessions in the same cluster node might not appear in the journal, because only record at a time can be handled by the cluster's workspace-specific callback method. When such a situtation arises, the following warnings can be found in the log:

*WARN * ClusterNode: No record created.
*WARN * ClusterNode: No record prepared.
"
0,"Customizable Cookie Policy. Even if the server is not complying with the cookie specification, sometimes 
you still need to talk to it.

It would be nice that when setting the Cookie Policy for the HttpMethod, you 
could specify a custom policy that implements the CookieSpecBase but may 
handle certain problems more leniently.  This would be instead of specifying 
one of the three hard-coded policies."
0,Add getTotalSize() to QueryResults. As discussed in http://www.nabble.com/Total-size-of-a-query-result-and-setLimit%28%29-tf4280909.html#a12185543 a getTotalSize() method should be added to QueryResults.
0,"jackrabbit JCA pom.xml. do not see a way to add attachments, so here it is below inline.
Note, need to move the src/rar/META-INF/ra.xml to src/main/rar/META-INF/ra.xml (which is the default location with maven rar packager).
==========================================
<?xml version=""1.0"" encoding=""UTF-8""?>

<!--
   Licensed to the Apache Software Foundation (ASF) under one or more
   contributor license agreements.  See the NOTICE file distributed with
   this work for additional information regarding copyright ownership.
   The ASF licenses this file to You under the Apache License, Version 2.0
   (the ""License""); you may not use this file except in compliance with
   the License.  You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an ""AS IS"" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
  -->

<project xmlns=""http://maven.apache.org/POM/4.0.0""
         xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0
                             http://maven.apache.org/maven-v4_0_0.xsd "">
  <modelVersion>4.0.0</modelVersion>

<!-- ====================================================================== -->
<!-- P R O J E C T  D E S C R I P T I O N                                   -->
<!-- ====================================================================== -->
  <groupId>org.apache.jackrabbit</groupId>
  <artifactId>jackrabbit-jca</artifactId>
  <packaging>rar</packaging>
  <name>Jackrabbit JCA</name>
  <version>1.1-SNAPSHOT</version>
  <!--
    Keep the description on a single line. Otherwise Maven might generate
    a corrupted MANIFEST.MF (see http://jira.codehaus.org/browse/MJAR-4)
   -->
  <description>
A resource adapter for Jackrabbit as specified by JCA 1.0.
</description>
  <url>http://jackrabbit.apache.org/</url>
  <prerequisites>
    <maven>2.0</maven>
  </prerequisites>
  <issueManagement>
    <system>Jira</system>
    <url>http://issues.apache.org/jira/browse/JCR</url>
  </issueManagement>
  <inceptionYear>2005</inceptionYear>




<!-- ====================================================================== -->
<!-- M A I L I N G   L I S T S                                              -->
<!-- ====================================================================== -->
  <mailingLists>
    <mailingList>
      <name>Jackrabbit Announce List</name>
      <subscribe>announce-subscribe@jackrabbit.apache.org</subscribe>
      <unsubscribe>announce-unsubscribe@jackrabbit.apache.org</unsubscribe>
      <archive>http://mail-archives.apache.org/mod_mbox/jackrabbit-announce/</archive>
    </mailingList>
    <mailingList>
      <name>Jackrabbit Users List</name>
      <subscribe>users-subscribe@jackrabbit.apache.org</subscribe>
      <unsubscribe>users-unsubscribe@jackrabbit.apache.org</unsubscribe>
      <post>users at jackrabbit.apache.org</post>
      <archive>http://mail-archives.apache.org/mod_mbox/jackrabbit-users/</archive>
      <otherArchives>
        <otherArchive>
          http://dir.gmane.org/gmane.comp.apache.jackrabbit.user
        </otherArchive>
        <otherArchive>
          http://www.mail-archive.com/users@jackrabbit.apache.org/
        </otherArchive>
      </otherArchives>
    </mailingList>
    <mailingList>
      <name>Jackrabbit Development List</name>
      <subscribe>dev-subscribe@jackrabbit.apache.org</subscribe>
      <unsubscribe>dev-unsubscribe@jackrabbit.apache.org</unsubscribe>
      <post>dev at jackrabbit.apache.org</post>
      <archive>http://mail-archives.apache.org/mod_mbox/jackrabbit-dev/</archive>
      <otherArchives>
        <otherArchive>
          http://dir.gmane.org/gmane.comp.apache.jackrabbit.devel
        </otherArchive>
        <otherArchive>
          http://www.mail-archive.com/dev@jackrabbit.apache.org/
        </otherArchive>
        <otherArchive>
          http://www.mail-archive.com/jackrabbit-dev@incubator.apache.org/
        </otherArchive>
      </otherArchives>
    </mailingList>
    <mailingList>
      <name>Jackrabbit Source Control List</name>
      <subscribe>commits-subscribe@jackrabbit.apache.org</subscribe>
      <unsubscribe>commits-unsubscribe@jackrabbit.apache.org</unsubscribe>
      <archive>http://mail-archives.apache.org/mod_mbox/jackrabbit-commits/</archive>
    </mailingList>
  </mailingLists>


  <licenses>
    <license>
      <name>The Apache Software License, Version 2.0</name>
      <url>http://www.apache.org/licenses/LICENSE-2.0</url>
      <distribution>repo</distribution>
    </license>
  </licenses>
  <scm>
    <connection>scm:svn:http://svn.apache.org/repos/asf/jackrabbit/trunk/jca</connection>
    <developerConnection>scm:svn:https://svn.apache.org/repos/asf/jackrabbit/trunk/jca</developerConnection>
    <url>http://svn.apache.org/viewvc/jackrabbit/trunk/jca</url>
  </scm>
  <organization>
    <name>The Apache Software Foundation</name>
    <url>http://www.apache.org/</url>
  </organization>
  <build>
    <resources>
      <resource>
        <directory>src/java</directory>
      </resource>
    </resources>
<!-- 
   <testResources>
      <testResource>
        <directory>applications/test</directory>
        <includes>
          <include>*.properties</include>
          <include>*.xml</include>
        </includes>
      </testResource>
      <testResource>
        <directory>src/test/java</directory>
        <includes>
          <include>**/*.xml</include>
          <include>**/*.txt</include>
        </includes>
      </testResource>
    </testResources>
-->
    <plugins>
      <plugin>
        <artifactId>maven-compiler-plugin</artifactId>
        <configuration>
          <target>1.4</target>
          <source>1.4</source>
        </configuration>
      </plugin>
<!-- 
      <plugin>
        <artifactId>maven-surefire-plugin</artifactId>
        <configuration>
          <excludes>
            <exclude>**/init/*</exclude>
          </excludes>
          <includes>
            <include>**/*TestAll.java</include>
          </includes>
          <forkMode>once</forkMode>
          <argLine>-Xmx128m -enableassertions</argLine>
          <systemProperties>
            <property>
              <name>derby.system.durability</name>
              <value>test</value>
            </property>
            <property>
              <name>known.issues</name>
              <value>org.apache.jackrabbit.core.xml.DocumentViewTest#testMultiValue org.apache.jackrabbit.value.BinaryValueTest#testBinaryValueEquals</value>
            </property>
          </systemProperties>          
        </configuration>
      </plugin>
-->
    </plugins>
  </build>

  <dependencies>
  
  	<dependency>
	    <groupId>org.apache.jackrabbit</groupId>
	    <artifactId>jackrabbit-core</artifactId>
	    <version>1.1-SNAPSHOT</version>
	</dependency>
    <dependency>
      <groupId>concurrent</groupId>
      <artifactId>concurrent</artifactId>
      <version>1.3.4</version>
    </dependency>
    <dependency>
      <groupId>commons-collections</groupId>
      <artifactId>commons-collections</artifactId>
      <version>3.1</version>
    </dependency>
    <dependency>
      <groupId>org.apache.geronimo.specs</groupId>
      <artifactId>geronimo-jta_1.0.1B_spec</artifactId>
      <version>1.0.1</version>
    </dependency>
    <dependency>
      <groupId>javax.jcr</groupId>
      <artifactId>jcr</artifactId>
      <version>1.0</version>
    </dependency>
    <dependency>
      <groupId>log4j</groupId>
      <artifactId>log4j</artifactId>
      <version>1.2.8</version>
    </dependency>
    <dependency>
      <groupId>org.slf4j</groupId>
      <artifactId>slf4j-log4j12</artifactId>
      <version>1.0</version>
    </dependency>
    <dependency>
      <groupId>lucene</groupId>
      <artifactId>lucene</artifactId>
      <version>1.4.3</version>
    </dependency>
    <dependency>
      <groupId>org.apache.derby</groupId>
      <artifactId>derby</artifactId>
      <version>10.1.1.0</version>
      <optional>true</optional>
    </dependency>
    <dependency>
      <groupId>junit</groupId>
      <artifactId>junit</artifactId>
      <version>3.8.1</version>
      <scope>test</scope>
    </dependency>
  </dependencies>

  <distributionManagement>
    <repository>
      <id>apache.releases</id>
      <name>Apache Repository for PMC approved releases</name>
      <url>scp://people.apache.org/www/www.apache.org/dist/maven-repository/</url>
    </repository>
    <snapshotRepository>
      <id>apache.snapshots</id>
      <name>Apache Development Repository</name>
      <url>scp://people.apache.org/www/cvs.apache.org/maven-snapshot-repository</url>
    </snapshotRepository>
    <site>
      <id>website</id>
      <url>scp://people.apache.org/www/jackrabbit.apache.org/</url>
    </site>
  </distributionManagement>

</project>"
0,"Remove deprecated classes in jackrabbit-core. similar to JCR-2109 i think i would be favorable to get rid of stuff that has been marked deprecated for 1.x releases.
"
0,SimpleText is too slow. If you are unlucky enough to get SimpleText codec on the TestBasics (span query) test then it runs very slowly...
0,"Make TopDocs constructor public. TopDocs constructor is package visible. This prevents instantiating it from outside this package. For example, I wrote a HitColletor that couldn't extend directly from TopDocCollector. I need to create a new TopDocs instance, however since the c'tor is package visible, I can't do that.
For now, I completely duplicated the code, but I hope you'll fix it soon."
0,create test case to verify we support > 2.1B terms. I created a test case for this... I'm leaving it as @Ignore because it takes more than four hours on a faaast machine (beast) to run.  I think we should run this before each release.
0,PersistenceManager sanity check. Library that provides a framework for testing the repository consistency and repairing it if necessary.
0,"[PATCH] FilteredTermEnum code cleanup. FilteredTermEnum's constructor takes two parameters but doesn't use them. This 
patch changes that and thus makes the code easier readable. Maybe the old 
constructor should be kept (as deprecated)? I'm not sure, this version seems 
cleaner to me."
0,Build instructions for contributions. Add READMEs in order to instruct how the contribution projects can ne built
0,"cache module does not recognize equivalent URIs. http://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.2.3

""When comparing two URIs to decide if they match or not, a client SHOULD use a case-sensitive octet-by-octet comparison of the entire URIs, with these exceptions:
  * A port that is empty or not given is equivalent to the default port for that URI-reference;
  * Comparisons of host names MUST be case-insensitive;
  * Comparisons of scheme names MUST be case-insensitive;
  * An empty abs_path is equivalent to an abs_path of ""/"".
Characters other than those in the ""reserved"" and ""unsafe"" sets (see RFC 2396 [42]) are equivalent to their """"%"" HEX HEX"" encoding.

For example, the following three URIs are equivalent:
      http://abc.com:80/~smith/home.html
      http://ABC.com/%7Esmith/home.html
      http://ABC.com:/%7esmith/home.html""

The current implementation does not canonicalize the URIs it uses for cache keys, and so is missing potential cache hits. More importantly, though, required invalidations due to PUT/POST/DELETE to a URI (as well as those mentioned in Location or Content-Location headers) may not occur properly due to this bug."
0,"DatabasePersistenceManager: don't log exceptions for each statement when a connection needs to be reestablished. This is just a ""cosmetic"" fix: when reestablishConnection() is called in DatabasePersistenceManager all the statements are closed but if an error occurs two exceptions are logged for each statement.
Since reestablishConnection() is already called when an exception has been caught and its only purpose is to cleanup an existing connection and to reopen a new one is pretty common that the connection is already not valid and that each statement close will throw an exception.

For example if the connection has been broken due to a network problem DatabasePersistenceManager  will log *40* exceptions (2 for each statement) before trying to establish a connection, and that's pretty annoying (expecially if you use a mail appender for log4j....)
"
0,"uploading large streams through rmi. when I try to upload a file of 35 Meg, I get an out of memory error.

This is caused because the whole file is read into memory instead of buffering"
0,"A tokenfilter to decompose compound words. A tokenfilter to decompose compound words you find in many germanic languages (like German, Swedish, ...) into single tokens.

An example: Donaudampfschiff would be decomposed to Donau, dampf, schiff so that you can find the word even when you only enter ""Schiff"".

I use the hyphenation code from the Apache XML project FOP (http://xmlgraphics.apache.org/fop/) to do the first step of decomposition. Currently I use the FOP jars directly. I only use a handful of classes from the FOP project.

My question now:
Would it be OK to copy this classes over to the Lucene project (renaming the packages of course) or should I stick with the dependency to the FOP jars? The FOP code uses the ASF V2 license as well.

What do you think?"
0,"[PATCH] to remove synchronized code from TermVectorsReader. Otis,

here the latest and last patch to get rid of all synchronized code from
TermVectorsReader. It should include at least 3 files, TermVectorsReader.diff,
SegmentReader.diff and the new junit test case TestMultiThreadTermVectors.java.
The patch was generated against the current CVS version of TermVectorsReader and
SegmentReader. All lucene related junit tests pass fine.

best regards
Bernhard"
0,"Set default precisionStep for NumericField and NumericRangeFilter. This is a spinoff from LUCENE-1701.

A user using Numeric* should not need to understand what's
""under the hood"" in order to do their indexing & searching.

They should be able to simply:
{code}
doc.add(new NumericField(""price"", 15.50);
{code}

And have a decent default precisionStep selected for them.

Actually, if we add ctors to NumericField for each of the supported
types (so the above code works), we can set the default per-type.  I
think we should do that?

4 for int and 6 for long was proposed as good defaults.

The default need not be ""perfect"", as advanced users can always
optimize their precisionStep, and for users experiencing slow
RangeQuery performance, NumericRangeQuery with any of the defaults we
are discussing will be much faster.
"
0,"IndexWriter.close(false) does not actually stop background merge threads. Right now when you close(false), IndexWriter marks any running merges
as aborted but then does not wait for these merges to finish.  This
can cause problems because those threads still hold files open, so,
someone might think they can call close(false) and then (say) delete
all files from that directory, which would fail on Windows.

Instead, close(false) should notify each running merge that it has
been aborted, and not return until all running merges are done.  Then,
SegmentMerger should periodically check whether it has been aborted
and stop if so.
"
0,"Move QueryParsers from contrib/queryparser to queryparser module. Each of the QueryParsers will be ported across.

Those which use the flexible parsing framework will be placed under the package flexible.  The StandardQueryParser will be renamed to FlexibleQueryParser and surround.QueryParser will be renamed to SurroundQueryParser."
0,"Move common validation checks to a single place. check for effective locks, nodes being checked-in, protection of item definitions etc. are abundant throughout jackrabbit-core. now that in addition retention and holds will be added and need to be checked as well, i suggest to move those checks to a common utility class and pass item and a list of checks to be performed.

batcheditemoperations already provides a similar pattern for the operations on item states.
i therefore suggest to move the flags to its base (ItemValidator) and add the utility methods needed."
0,Only load root node definition when required. The root node definition is currently loaded whenever a session logs in.
0,"make it possible to use searchermanager with distributed stats. LUCENE-3555 added explicit stats methods to indexsearcher, but you must
subclass to override this (e.g. populate with distributed stats).

Its also impossible to then do this with SearcherManager.

One idea is make this a factory method (or similar) on IndexSearcher instead,
so you don't need to subclass it to override.

Then you can initialize this in a SearcherWarmer, except there is currently
a lot of hair in what this warming should be. This is a prime example where
Searcher has different meaning from Reader, we should clean this up.

Otherwise, lets make NRT/SearcherManager subclassable in such a way that 
you can return a custom indexsearcher."
0,Use out-of-process text extraction. The upcoming Tika 0.9 release will contain a highly useful out-of-process text extraction feature (TIKA-416) that we should use also in Jackrabbit.
0,"ConjunctionScorer tune-up. I just recently ran a load test on the latest code from lucene , which is using a new BooleanScore and noticed the ConjunctionScorer was crunching through objects , especially while sorting as part of the skipTo call. It turns a linked list into an array, sorts the array, then converts the array back to a linked list for further processing by the scoring engines below.

'm not sure if anyone else is experiencing this as I have a very large index (> 4 million items) and I am issuing some heavily nested queries

Anyway, I decide to change the link list into an array and use a first and last marker to ""simulate"" a linked list.

This scaled much better during my load test as the java gargbage collector was less - umm - virulent "
0,"FuzzyQuery should not be final. I am trying to extend the FuzzyQuery to further filter the TermEnum. (I am indexing stem forms and original forms, but I only want to match original forms with a fuzzy term, otherwise I get to much noise). However, FuzzyQuery is a final class and I cannot extend it.  

As discussed in the mailing list (http://www.gossamer-threads.com/lists/lucene/java-dev/38756), we want to make the private variables and inner classes protected.

I am attaching a patch for FuzzyQuery.java that implements this. I ran all unit tests and they passed without errors.

Andreas."
0,"Fix charset problems in XML loading in HyphenationCompoundWordTokenFilter (also Solr's loader from schema). As said in LUCENE-2731, the handling of XML in HyphenationCompoundWordTokenFilter is broken and breaks XML 1.0 (5th edition) spec totally. You should never supply a Reader to any XML api, unless you have internal character data (e.g. created programmatically). Also you should supply a system id, as resolving external entities does not work. The loader from files is much more broken, it always open the file as a Reader and then passes it to InputSource. Instead it should point filename directly to InputSource.

This issue will fix it in trunk and use InputSource in Solr, but will still supply the Reader possibility in previous versions (deprecated)."
0,"move HttpRoute and related classes to separate package. The route-related stuff in o.a.h.conn is detached from the rest of the connection management API.
Move HttpRoute, RouteTracker, HttpRouteDirector, HttpRoutePlanner to o.a.h.conn.route or ...routing.
Implementation classes have a dependency on Scheme and SchemeRegistry in o.a.h.conn,
but that does not introduce a recursive dependency between packages.
"
0,Separate initial index creation from MultiIndex construction. If there is no index present the MultiIndex constructor will create an initial index by traversing the workspace item states. This makes it difficult for an outside class to detect the situation where no index is present.
0,"StopFilter should have option to incr positionIncrement after stop word. I've seen this come up on the mailing list a few times in the last month, so i'm filing a known bug/improvement arround it...

StopFilter should have an option that if set, records how many stop words are ""skipped"" in a row, and then sets that value as the positionIncrement on the ""next"" token that StopFilter does return."
0,"Consolidate CustomScoreQuery, ValueSourceQuery and BoostedQuery . Lucene's CustomScoreQuery and Solr's BoostedQuery do essentially the same thing: they boost the scores of Documents by the value from a ValueSource.  BoostedQuery does this in a direct fashion, by accepting a ValueSource. CustomScoreQuery on the other hand, accepts a series of ValueSourceQuerys.  ValueSourceQuery seems to do exactly the same thing as FunctionQuery.

With Lucene's ValueSource being deprecated / removed, we need to resolve these dependencies and simplify the code.

Therefore I recommend we do the following things:

- Move CustomScoreQuery (and CustomScoreProvider) to the new Queries module and change it over to use FunctionQuerys instead of ValueSourceQuerys.  
- Deprecate Solr's BoostedQuery in favour of the new CustomScoreQuery.  CSQ provides a lot of support for customizing the scoring process.
- Move and consolidate all tests of CSQ and BoostedQuery, to the Queries module and have them test CSQ instead."
0,MemoryFileSystem is different from other FileSystems. JCR-1175 uncovered inconsistencies in how the deleteFolder() and list() methods are implemented. The MemoryFileSystem class acts differently from the LocalFileSystem and DatabaseFileSystem classes. MemoryFileSystem and the related incorrect test cases should be fixed.
0,"Optimize usage of norms. There is a very significant potential for optimizing the size of the search index.

We have seen a case where there were multiple segments with about the same number of nodes (roughly 10 million), but the size on disk was very different.
One segment was 19 GB while all others where around 3 GB. The major difference was the number of fields indexed. The large segment had significantly more fields, which resulted in a large norms file.

We should go through our implementation and see where norms are really necessary and disable tracking of norms wherever possible."
0,"Make the Lucene jar an OSGi bundle. In order to use Lucene in an OSGi environment, some additional headers are needed in the manifest of the jar. As Lucene has no dependency, it is pretty straight forward and it ill be easy to maintain I think."
0,"Consolidate CND related classes from SPI and Core. currently SPI Commons and Core, both have a CND Reader/Writer. they should be consolidated; i.e. core should use the SPI one."
0,"[PATCH] Highlighter: Delegate output escaping to Formatter. Patch for jakarta-lucene-sandbox/contributions/highlighter
CVS version 3rd February 2005

This patch allows the highlighter Formatter to control escaping of the non
highlighted text as well as the highlighting of the matching text.

The example formatters highlight the matching text using XML/HTML tags. This
works fine if the plain text does not contain any characters that need to be
escaped for HTML output (i.e. <, &, and ""), however this cannot be guaranteed.
As the formatter controls the method of highlighting (in the examples this is
HTML, but it could be any other form of markup) it should also be responsible
for escaping the rest of the output.

This patch adds a method, encodeText(String), to the Formatter interface. This
is a breaking change. This method is called from the Highlighter with the text
that is not passed to the formatter's highlightTerm method. 
The SimpleHTMLFormatter has a public static method for performing simple HTML
escaping called htmlEncode. 
The SimpleHTMLFormatter, GradientFormatter, and SpanGradientFormatter have been
updated to implement the encodeText method and call the htmlEncode method to
escape the output.

For existing formatter to maintain exactly the same behaviour as before applying
this patch they would need to implement the encodeText method to return the
argument value without modification, e.g.:

public String encodeText(String originalText)
{
  return originalText;
}"
0,"Session.importXml should close the input stream (as to JSR 283/JCR 2.0). http://markmail.org/thread/crwx27dkt2cnjjy7

This is available for all that follow:
  Node.setProperty(String, InputStream)
  Property.setValue(InputStream)
  ValueFactory.createValue(InputStream)
  ValueFactory.createBinary(InputStream)
  Session.importXML(String, InputStream, int)
  Workspace.importXML(String, InputStream, int)
"
0,"Wire log is incomplete if HttpParser detects an error. If HttpParser detects an error in any of the headers, it throws a ProtocolException

Although the failing header is included in the Exception detail, the headers leading up to the failure are not logged, which makes it hard to debug (and is quite confusing, as the PE does not appear to be related to the data that has been received).

This is because the wire-logging is done in the caller (HttpMethodDirector) which only logs the header if the parse succeeds.

Perhaps the Wire logging should be done at the point where the HttpParser reads the line."
0,"website: 404 for several documentation pages. There are several 404 Not Found pages linked from hc.apache.org.
Please fix them as it's important documentation for HttpClient.

Specifically:

linked from http://hc.apache.org/user-docs.html

not found:
http://hc.apache.org/httpcomponents-client/primer.html
http://hc.apache.org/httpcomponents-client-4.0.1/httpclient/apidocs/index.html
http://hc.apache.org/httpcomponents-client-4.0.1/httpmime/apidocs/index.html"
0,jcr2spi: use Soft refs for hierarchy. stefan suggested to use soft refs for the hierarchy.
0,"Make QueryAutoStopWordAnalyzer immutable and reusable. Currently QueryAutoStopWordAnalyzer allows its list of stop words to be changed after instantiation through its addStopWords() methods.  This stops the Analyzer from being reusable since it must instantiate its StopFilters every time.

Having these methods means that although the Analyzer can be instantiated once and reused between IndexReaders, the actual analysis stack is not reusable (which is probably the more expensive part).

So lets change the Analyzer so that its stop words are set at instantiation time, facilitating reuse."
0,"Maven build failure in textfilter contrib project. I've tried to build the textfilters contrib but get the following error:

Attempting to download jackrabbit-0.16.4.1-dev.jar.
WARNING: Failed to download jackrabbit-0.16.4.1-dev.jar.
The build cannot continue because of the following unsatisfied dependency:
jackrabbit-0.16.4.1-dev.jar

I tried changing the project.xml to look for jackrabbit-1.0-dev.jar instead, but it didn't work, not sure what maven expects here, but should be an easy fix for somebody in the know."
0,"make SchemeRegistry friendlier for DI frameworks. Scheme's in SchemeRegistry are registered via 'register' method, but there is no way to pass it a set of schemes so those can be registered in one step. This way it can be externally configured and 'spring/guice friendly'... something like this is sufficient...

public void setSchemes (final Set <Scheme> schemes) {
    for (final Scheme scheme : schemes) 
        register(scheme);    
}
"
0,"Enhanced JCR remoting (extending webdav SPI impl, basic remoting servlet). "
0,"SSL contrib files do not use standard javax.net.ssl package provided from JDK 1.4.2. Hi all,

While trying to use ssl on AIX, i found that some of the files contributed in 
src/contrib/org/apache/commons/httpclient/contrib/ssl were making hard 
references to com.sun.net.ssl package. Since JDK 1.4.2, one shall use the 
javax.net.ssl package instead.

I have then:
1/ fixed the source files appropriately
2/ updated the build.xml to also build a commons-http-client-contrib.jar 

I will attached to this bug report the resulting unified diff to include in svn"
0,Use inheritance rather than delegation for SPI ValueFactoryImpl. The ValueFactoryImpl now has a protected constructor and the SPI variant of the value factory can use it.
0,"Similarity.Stats class for term & collection statistics. In order to support ranking methods besides TF-IDF, we need to make the statistics they need available. These statistics could be computed in computeWeight (soon to become computeStats) and stored in a separate object for easy access. Since this object will be used solely by subclasses of Similarity, it should be implented as a static inner class, i.e. Similarity.Stats.

There are two ways this could be implemented:
- as a single Similarity.Stats class, reused by all ranking algorithms. In this case, this class would have a member field for all statistics;
- as a hierarchy of Stats classes, one for each ranking algorithm. Each subclass would define only the statistics needed for the ranking algorithm.

In the second case, the Stats class in DefaultSimilarity would have a single field, idf, while the one in e.g. BM25Similarity would have idf and average field/document length."
0,"change sort order to binary order. Since flexible indexing, terms are now represented as byte[], but for backwards compatibility reasons, they are not sorted as byte[], but instead as if they were char[].

I think its time to look at sorting terms as byte[]... this would yield the following improvements:
* terms are more opaque by default, they are byte[] and sort as byte[]. I think this would make lucene friendlier to customizations.
* numerics and collation are then free to use their own encoding (full byte) rather than avoiding the use of certain bits to remain compatible with char[] sort order.
* automaton gets simpler because as in LUCENE-2265, it uses byte[] too, and has special hacks because terms are sorted as char[]
"
0,"optimization in sending request. By doing network sniffering, I noticed that httpclient send the request line 
and headers as separate packages. All other httpclient, including IE, Netscape, 
Java's URLConnection all send it as one package. This can be easily fixed using 
some buffering in HttpMethodBase."
0,"Remove deprecated TermAttribute from tokenattributes and legacy support in indexer. The title says it:
- Remove interface TermAttribute
- Remove empty fake implementation TermAttributeImpl extends CharTermAttributeImpl
- Remove methods from CharTermAttributeImpl (and indirect from Token)
- Remove sophisticated backwards Layer in TermsHash*
- Remove IAE from NumericTokenStream, if TA is available in AS
- Fix rest of core tests (TestToken)"
0,"MoreLikeThis - allow to exclude terms that appear in too many documents (patch included). The MoreLikeThis class allows to generate a likeness query based on a given document. So far, it is impossible to suppress words from the likeness query, that appear in almost all documents, making it necessary to use extensive lists of stop words.

Therefore I suggest to allow excluding words for which a certain absolute document count or a certain percentage of documents is exceeded. Depending on the corpus of text, words that appear in more than 50 or even 70% of documents can usually be considered insignificant for classifying a document.      "
0,"back-compat tests (""ant test-tag"") should test JAR drop-in-ability. 
We now test back-compat with ""ant test-tag"", which is very useful for
catching breaks in back compat before committing.

However, that currently checks out ""src/test"" sources and then
compiles them against the trunk JAR, and runs the tests.  Whereas our
back compat policy:

  http://wiki.apache.org/lucene-java/BackwardsCompatibility

states that no recompilation is required on upgrading to a new JAR.
Ie you should be able to drop in the new JAR in place of your old one
and things should work fine.

So... we should fix ""ant test-tag"" to:

  * Do full checkout of core sources & tests from the back-compat-tag

  * Compile the JAR from the back-compat sources

  * Compile the tests against that back-compat JAR

  * Swap in the trunk JAR

  * Run the tests

"
0,"Remove sanityCheck() from ItemImpl.getSession(). The following code causes an InvalidItemStateException to be thrown for no good reason:

    Property property = ...;
    property.setValue((Value) null);
    property.getSession();

There are cases (I'm looking at one right now) where it's good to be able to access the session of an Item even if it has already been invalidated.

The simple fix is to remove the sanityCheck() call from ItemImpl.getSession(). I'll do that unless someone has a good reason why the sanity check should be kept."
0,"Upgrade to Tika 0.6 and PDFBox 1.0.0. Tika version 0.6 uses POI 3.6 that's notably smaller (-10MB!) than previous versions. There are also a number of other improvements in Tika 0.6 since the 0.5 release.

While doing the upgrade we should also force the PDFBox version to 1.0.0 from the 0.8.0-incubating version that Tika 0.6 uses. PDFBox 1.0.0 has some nice performance gains (around 30% faster) to text extraction along with other improvements."
0,Upgrade to Tika 1.0. Tika 1.0 was released today and has many improvements over the earlier 0.10 release. We should upgrade.
0,"TextFilterService uses Sun specific classes. The TextFilterService uses the Sun specific and actually undocumented class sun.misc.Service class to lookup TextFilter implementations. This approach will not work on all JVM implementations.

The Service should rather use javax.imageio.spi.ServiceRegistry, which is part of the standard J2SE API."
0,"[patch] javadoc and comment updates for BooleanClause.. Javadoc and comment updates for BooleanClause, one minor code simplification."
0,"Internal Timeout Handling in the TransactionContext is not XA Spec. conform. The problem here is that in a 2 phase transaction the xa spec does not  
permit a RB* return code in response to xa_commit().  The xa spec says  
the following about RB* return codes in the xa_commit() section:        
                                                                        
""The resource manager did not commit the work done on behalf of the     
transaction branch.  Upon return, the resource manager has rolled back  
the branch?s work and has released all held resources.  These values may
be returned only if TMONEPHASE is set in flags""                         
                                                                        
Essentially, the only two return codes from xa_commit that J2EE Containers can     
handle sensibly are XA_OK (normal case) and XA_RMFAIL.  RMFAIL will     
cause the containers to retry to commit the  transaction.  Any other return code will result in a heuristic          
transaction outcome (non-atomic).  

In a xa environment the TMONEPHASE is not set on the flags and so XA_RBTIMEOUT is 
not a permitted return code. A Container  transaction service cannot do anything to ensure an atomic     
outcome if an XAResource fails to honour its promise to be able to commit it made when it answer XA_OK in response to xa_prepare(). 

The internal timeout handling will rollback the Jackrabbit XAResource if the time exceeds between prepare and commit.
and in the commit Method will always throw a XA_RBTIMEOUT.

We should not handle the timeout internal because this should make the container in a 2 Phase transaction."
0,"UserManagement: Missing assertion that Principal name isn't """". Creating users/groups with a Principal having an empty name should not be allowed."
0,"Variant spelling ""Trasaction"" and ""Transaction"" now in codebase. Incorrectly spelled ""Trasaction"" now in codebase.  The correctly spelled ""Transaction"" appears a far greater number of times.

Isolated to package: org.apache.jackrabbit.jca

Incorrectly  spelled ""Trasaction"" found in:
(3x) JCAManagedConnection.java 
(7x) JCAManagedConnectionFActory.java

Correctly spelled ""Transaction"" found in:
(20x) JCAManagedConnection.java 
(2x) JCAManagedConnectionFActory.java 
(1x) JCAResourceAdapter.java 
(12x) TransactionBoundXAResource.java 
"
0,"TransientRepository: application doesn't exit quickly. When using the TransientRepository, the repository should be closed when the last session logs out. This works, but in some cases there is a very long (60 seconds) delay between closing the last session and closing the repository.

Test case:

    public static void main(String[] args) throws Exception {
        Repository repository = new TransientRepository();
        Session session = repository.login(new SimpleCredentials("""", new char[0]));
        session.getRootNode().setProperty(""a"", ""0"");
        session.save(); // very quick logout without this line
        session.logout();
        System.out.println(""Logout..."");
        final long time = System.currentTimeMillis();
        Runtime.getRuntime().addShutdownHook(new Thread() {
            public void run() {
                System.out.println(""End after: "" + (System.currentTimeMillis() - time));
            }
        });
    }

"
0,"results.jsp in luceneweb.war uses unknown parse-Method. results.jsp in luceneweb.war demo throws JasperException:

org.apache.jasper.JasperException: Unable to compile class for JSP

An error occurred at line: 60 in the jsp file: /results.jsp
Generated servlet error:
The method parse(String) in the type QueryParser is not applicable for the arguments (String, String, Analyzer)

I think, the code in line 81 of results.jsp should maybe look like the following ?

QueryParser qp = new QueryParser(""contents"", analyzer);
query = qp.parse(queryString);"
0,"add a tokenfilter for icu transforms. I pulled the ICUTransformFilter out of LUCENE-1488 and create an issue for it here.

This is a tokenfilter that applies an ICU Transliterator, which is a context-sensitive way
to transform text. 

These are typically rule-based and you can use ones included with ICU (such as Traditional-Simplified)
or you can make your own from your own set of rules.

User's Guide: http://userguide.icu-project.org/transforms/general
Rule Tutorial: http://userguide.icu-project.org/transforms/general/rules
"
0,"Detect the test thread by reference, not by name.. Get rid of this:
{code}
      if (doFail && (Thread.currentThread().getName().equals(""main"") 
          || Thread.currentThread().getName().equals(""Main Thread""))) {
{code}"
0,"TCK vs available property types. The TCK tests allow configuration of node type / property names to tests specific property types, but they do not take into account that a given repository may not support a specific property type (this is similar to issue JCR-801 about multiple workspace support).

JSR-170 is a bit fuzzy here: it requires all types, but does not require that every type actually exists on a settable node type. In practice, a repository may support reference properties on the builtin nodetypes for version storage, but nowhere else.

Thus, there should be a way to configure the tests so that specific property type tests are left out. Again, there are a few possibilities to do that:

1) reserve a special property name for the case where the test should be skipped (""PROPERTY_TYPE_NOT_SUPPORT""), or

2) add new config flags.

The latter arguably is the cleaner approach, the former avoids introducing new configuration parameters. Thus, I'm leaning to 2). Feedback appreciated.

"
0,"CacheManager (Memory Management in Jackrabbit). Jackrabbit can run out of memory because the the combined size of the various caches is not managed. The biggest problem (for me) is the combined size of the o.a.j.core.state.MLRUItemStateCache caches. Each session seems to create a few (?) of those caches, and each one is limited to 4 MB by default.

I have implemented a dynamic (cache-) memory management service that distributes a fixed amount of memory dynamically to all those caches.

Here is the patch"
0,PackedInts does not support structures above 256MB. The PackedInts Packed32 and Packed64 fails when the internal structure exceeds 256MB. This is due to a missing cast that results in the bit position calculation being limited by Integer.MAX_VALUE (256MB * 8 = 2GB).
0,"RAMInputStream and RAMOutputStream without further buffering. From java-dev, Doug's reply of 12 Sep 2005 
on Delaying buffer allocation in BufferedIndexInput: 
 
Paul Elschot wrote: 
... 
> I noticed that RAMIndexInput extends BufferedIndexInput. 
> It has all data in buffers already, so why is there another 
> layer of buffering? 
 
No good reason: it's historical. 
 
To avoid this either: (a) the BufferedIndexInput API would need to be  
modified to permit subclasses to supply the buffer; or (b)  
RAMInputStream could subclass IndexInput directly, using its own  
buffers.  The latter would probably be simpler. 
 
End of quote. 
 
I made version (b) of RAMInputStream. 
Using this RAMInputStream, TestTermVectorsReader failed as the only 
failing test."
0,"Strengthen CheckIndex a bit. A few small improvements to CheckIndex to detect possible ""docs out of order"" cases."
0,Jcr-Server Module: Remove Dependency from Jackrabbit-Core. 
0,"LogByteSizeMergePolicy over-merges with autoCommit=false and documents with term vectors and/or stored fields. Mark Miller noticed this slowdown (see details in LUCENE-994) in his
app.

This happens because in SegmentInfo.sizeInBytes(), we just run through
all files associated with that segment, summing up their byte sizes.

But in the case of shared doc stores (which happens when
autoCommit=false), this is not quite correct because those files are
shared across multiple segments.

I plan to fix sizeInBytes() to not include the size of the doc stores
when they are shared.
"
0,"ProxyHost/HttpHost: Checks for null when javadoc document null ok. The constructor javadocs for ProxyHost and HttpHost all state that null is an allowed value - but there's an check in the HttpHost constructor for this which throws IllegalArgumentException.

(Actually allowing null as documented would also allow for a spring wiring remaining the same when using a proxy or not - steering the values from a propertyfile.)"
0,"add a test for PorterStemFilter. There are no tests for PorterStemFilter, yet svn history reveals some (very minor) cleanups, etc.
The only thing executing its code in tests is a test or two in SmartChinese tests.

This patch runs the StemFilter against Martin Porter's test data set for this stemmer, checking for expected output.

The zip file is 100KB added to src/test, if this is too large I can change it to download the data instead.
"
0,BTreeManager needs more flexible mechanism for ignoring (internal) properties. The current BTreeManager implementation has some hard coded logic to ignore jcr:primaryType properties. There should be a mechanism to parametrize BTreeManager with a set of properties to ignore.
0,"add target jvm in maven properties for compilation. actually the compatibility level for sources/binaries is not defined in project.properties, so if you compile jackrabbit with a 1.5 jdk it will not run on older vm.

It would be nice to add the following properties to assure that the generated jar will work on different jvms:
maven.compile.target=1.4
maven.compile.source=1.4

(or 1.3 if you are targetting also java 1.3)"
0,Test case for RTFTextExtractor. There should be a test case for the RTFTextExtractor.
0,IndexReader.listCommits should return a List and not an abstract Collection. Spinoff from here: http://www.mail-archive.com/dev@lucene.apache.org/msg07509.html
0,"Unit tests for persistence managers. Currently we only test our persistence managers indirectly via JCR-level test cases. The downside of this approach is that we can only test one persistence manager implementation at a time, and need separate build profiles to switch from one implementation to another. To ensure better coverage and consistent behaviour across all our persistence managers I implemented a simple unit test that works directly against the PersistenceManager interface."
0,"add checks to MockTokenizer to enforce proper consumption. we can enforce things like consumer properly iterates through tokenstream lifeycle
via MockTokenizer. this could catch bugs in consumers that don't call reset(), etc."
0,"Implement search facility for users and groups. Implement a search facility for users and groups supporting:
- search for users and/or groups with a certain property (value) either directly on the user/group node or on any of its sub nodes
- full text search on user and/or group nodes and its sub nodes
- inclusion/exclusion based on group membership: i.e. restricting search to members of a group or to groups with a certain member
- ordering 
- paging"
0,"HttpConnection.isOpen() logging is not accurate. isOpen() does not differentiate between stale and closed.  If the connection is closed isStale() will return 
true.  The logs will then indicate that the connection was stale, even though it was really just closed.  
close() is also called a second time unnecessarily.

This should be fixed for 3.0, and perhaps even 2.0.1.

<http://nagoya.apache.org/eyebrowse/ReadMsg?listName=commons-httpclient-
dev@jakarta.apache.org&msgNo=7205>"
0,"Optimization for FieldDocSortedHitQueue. When updating core for generics,  I found the following as a optimization of FieldDocSortedHitQueue:

All FieldDoc values are Compareables (also the score or docid, if they
appear as SortField in a MultiSearcher or ParallelMultiSearcher). The code
of lessThan seems very ineffective, as it has a big switch statement on the
SortField type, then casts the value to the underlying numeric type Object,
calls Number.xxxValue() & co for it and then compares manually. As
j.l.Number is itself Comparable, I see no reason to do this. Just call
compareTo on the Comparable interface and we are happy. The big deal is that
it prevents casting and the two method calls xxxValue(), as Number.compareTo
works more efficient internally.

The only special cases are String sort, where the Locale may be used and the
score sorting which is backwards. But these are two if statements instead of
the whole switch.

I had not tested it now for performance, but in my opinion it should be
faster for MultiSearchers. All tests still pass (because they should).
"
0,"PathHierarchyTokenizer adaptation for urls: splits reversed. {{PathHierarchyTokenizer}} should be usable to split urls the a ""reversed"" way (useful for faceted search against urls):
{{www.site.com}} -> {{www.site.com, site.com, com}}

Moreover, it should be able to skip a given number of first (or last, if reversed) tokens:
{{/usr/share/doc/somesoftware/INTERESTING/PART}}
Should give with 4 tokens skipped:
{{INTERESTING}}
{{INTERESTING/PART}}"
0,"Fix FuzzyQuery's defaults, so its fast.. We worked a lot on FuzzyQuery, but you need to be a rocket scientist to ensure good results.

The main problem is that the default distance is 0.5f, which doesn't take into account the length of the string.
To add insult to injury, the default number of expansions is 1024 (traditionally from BooleanQuery maxClauseCount)

I propose:
* The syntax of FuzzyQuery is enhanced, so that you can specify raw edits too: such as foobar~2 (all terms within 2 levenshtein edits of foobar). Previously if you specified any amount >=1, you got IllegalArgumentException, so this won't break anyone. You can still use foobar~0.5, and it works just as before
* The default for minimumSimilarity then becomes LevenshteinAutomata.MAXIMUM_SUPPORTED_DISTANCE, which is 2. This way if you just do foobar~, its always fast.
* The size of the priority queue is reduced by default from 1024 to a much more reasonable value: 50. This is what FuzzyLikeThis uses.

I think its best to just change the defaults for this query, since it was so aweful before. We can add notes in migrate.txt that if you care about using the old values, then you should provide them explicitly, and you will get the same results!
"
0,"SPI: Add RepositoryService.getQNodeTypeDefinition. Finding of the F2F (2/3 July)

similar to recent modifications to the retrieval of name spaces the node type management should be changed in order to only retrieve the complete set of node types on demand. Otherwise single node type definitions should be retrieved as required.
To achieve this we agreed to add RepositoryService.getQNodeTypeDefinition"
0,"A new Greek Analyzer for Lucene. I would like to contribute a greek analyzer for lucene. It is based on the
existing Russian analyzer and features:

- most common greek character sets, such as Unicode, ISO-8859-7 and Windows-1253
- a collection of common greek stop words
- conversion of characters with diacritics (accent, diaeresis) in the lower case
filter, as well as handling of special characters, such as small final sigma

For the character sets I used RFC 1947 (Greek Character Encoding for Electronic
Mail Messages) as a reference. I have incorporated this analyzer in Luke as well
as used it successfully in a recent project of my company (EBS Ltd.).

I hope you will find it a useful addition to the project."
0,"Add reflection API to AttributeSource/AttributeImpl. AttributeSource/TokenStream inspection in Solr needs to have some insight into the contents of AttributeImpls. As LUCENE-2302 has some problems with toString() [which is not structured and conflicts with CharSequence's definition for CharTermAttribute], I propose an simple API that get a default implementation in AttributeImpl (just like toString() current):

- Iterator<Map.Entry<String,?>> AttributeImpl.contentsIterator() returns an iterator (for most attributes its a singleton) of a key-value pair, e.g. ""term""->""foobar"",""startOffset""->Integer.valueOf(0),...
- AttributeSource gets the same method, it just concat the iterators of each getAttributeImplsIterator() AttributeImpl

No backwards problems occur, as the default toString() method will work like before (it just gets iterator and lists), but we simply remove the documentation for the format. (Char)TermAttribute gets a special impl fo toString() according to CharSequence and a corresponding iterator.

I also want to remove the abstract hashCode() and equals() methods from AttributeImpl, as they are not needed and just create work for the implementor."
0,"Initialize hierarchy cache on startup. In some cases it may be desirable to initialize the hierarchy cache in the search index on startup. Currently this initialization is done in the background. For larger workspaces, this puts considerable load on the repository and may slow down access and queries. There should be a configuration parameter that forces the repository to initialize the hierarchy cache on startup and only return the repository instance when the initialization is completed.  The default value would be the current behaviour (using background thread)."
0,"Add N-Gram String Matching for Spell Checking. N-Gram version of edit distance based on paper by Grzegorz Kondrak, ""N-gram similarity and distance"". Proceedings of the Twelfth International Conference on String Processing and Information Retrieval (SPIRE 2005), pp. 115-126,  Buenos Aires, Argentina, November 2005. 
http://www.cs.ualberta.ca/~kondrak/papers/spire05.pdf
"
0,BytesRefHash#get() should expect a BytesRef instances for consistency. BytesRefHash#get should use a provided BytesRef instances instead of the internally used scratch. This is how all other APIs currently work and we should be consistent.
0,"Default merge policy should take deletions into account. LUCENE-1634 added a calibrateSizeByDeletes; we had a TODO to default this to true as of 3.0 but we missed it.  I'll fix it now for 3.1 and 4.0.  While this is technically a change in back-compat (for 3.x), I think it's fine to make an exception here; this should be a big win for indices that have high doc turnover with time."
0,"unicode escapes in files generated by JJTree. Maven build fails on windows machines if sources are located in a directory starting with a 'u'. This is because files created by JJTree (javacc) put filename and path in a comment of the generated files, like this:

/*@bgen(jjtree) Generated By:JJTree: Do not edit this line. D:\usr\projects\workspace\jackrabbit\target\generated-src\main\org\apache\jackrabbit\core\query\sql\JCRSQL.jj */

The \u in interpreted as an escape characted and so you get a 
""BUILD FAILED ... Invalid escape character""
"
0,Allow name to be set in PropertyInfoBuilder and NodeInfoBuilder. Currently the property name for new Properties and Nodes can only be set when their builder is created. I suggest to add methods for setting the names by themselves. 
0,"Signature changes in AttributeSource for better Generics support of AddAttribute/getAttribute. The last update of Attribute API using AttributeImpl as implementation oif Attributes changed the API a little bit. This change leads to the fact, that in Java 1.5 using generics we are no longer able to add Attributes without casting. addAttribute and getAttribute should return the Attribute interface because the implementation of the attribute is not interesting to the caller. By that in 1.5 using generics, one could add a TermAttribute without casting using:
{code}
TermAttribute termAtt = addAttribute(TermAttribute.class);
{code}
The signature to do this is:
{code}
public <T extends Attribute> T addAttribute(Class<T>)
{code}

The attached patch applies the mentioned change to the signature (without generic, only returning Attribute). No other code changes are needed, as current code always casts the result to the requested interface. I also added the 1.5 method signature for all these methods to the javadocs.

All tests pass."
0,"Documentation improvements for 1.9 release. I've poked arround the 1.9-rc1 builds and noticed a few simple documentation things that could be cleaned up, a patch will follow that...

1) Adds some additional info to the README.txt
2) Updates the version info in queryparsersyntax.xml and fileformats.html, and advises people 
     with older versions how to find the correct documentation for their version
3) Builds javadocs for all of the contrib modules (the list was incomplete)"
0,"Expose DocValues via Fields. DocValues Reader are currently exposed / accessed directly via IndexReader. To integrate the new feature in a more ""native"" way we should expose the DocValues via Fields on a perSegment level and on MultiFields in the multi reader case. DocValues should be side by side with Fields.terms  enabling access to Source, SortedSource and ValuesEnum something like that:

{code}
public abstract class Fields {
...

  public DocValues values();

}

public abstract class DocValues {
  /** on disk enum based API */
  public abstract ValuesEnum getEnum() throws IOException;
  /** in memory Random Access API - with enum support - first call loads values in ram*/
  public abstract Source getSource() throws IOException;
  /** sorted in memory Random Access API - optional operation */
  public SortedSource getSortedSource(Comparator<BytesRef> comparator) throws IOException, UnsupportedOperationException;
  /** unloads previously loaded source only but keeps the doc values open */
  public abstract unload();
  /** closes the doc values */
  public abstract close();
}
{code}

"
0,"Add System.getProperty(""tempDir"") as final static to LuceneTestCase(J4). Almost every test calls System.getProperty(""tempDir"") and some of them check the return value for null. In other cases the test simply fails from within eclipse.

We should add this to LuceneTestCase(J4) as a static final constant. For enabling tests run in eclipse, we can add a fallback to ""."", if the Sysprop is not defined."
0,JSR 283: Retention & Hold Management. 
0,dists include analyzer contrib in src dist but not binary dist. dists include analyzer contrib in src dist but not binary dist
0,SQL Azure support: clustered indexes. We tried to install JackRabbit in the Windows Azure cloud using SQL Azure. One of the limitations of SQL Azure is that it needs clustered indexes to work but the current implementation of the JackRabbit creates the indexes not clustered.
0,"deprecate Document.fields(), add getFields(). A simple API improvement that I'm going to commit if nobody objects."
0,"Various small improvements to contrib/benchmark. I've worked out a few small improvements to contrib/benchmark:

  * Refactored the common code in Open/CreateIndexTask that sets the
    configuration for the IndexWriter.  This also fixes a bug in
    OpenIndexTasks that prevented you from disabling flushing by RAM.

  * Added a new config property for LineDocMaker:

      doc.reuse.fields=true|false

    which turns on/off reusing of Field/Document by LineDocMaker.
    This lets us measure performance impact of sharing Field/Document
    vs not, and also turn it off when necessary (eg if you have your
    own consumer that uses private threads).

  * Added merge.scheduler & merge.policy config options.

  * Added param for OptimizeTask, which expects an int and calls
    optimize(maxNumSegments) with that param.

  * Added param for CloseIndex(true|false) -- if you pass false that
    means close the index, aborting any running merges
"
0,"Add unit test showing how to do a ""live backup"" of an index. The question of how to backup an index comes up every so often on the
lists.  Backing up and index is also clearly an important fundamental
admin task that many applications need to do for fault tolerance.

In the past you were forced to stop & block all changes to your index,
perform the backup, and then resume changes.  But many applications
cannot afford a potentially long pause in their indexing.

With the addition of DeletionPolicy (LUCENE-710), it's now possible to
do a ""live backup"", which means backup your index in the background
without pausing ongoing changes to the index.  This
SnapshotDeletionPolicy just has to mark the chosen commit point as not
deletable, until the backup finishes.
"
0,"[patch] Fix overly specific casting in core. several places in core, casts are made to overly concrete classes when, interfaces are only needed. Doing so ties the algorithms to specific implementations, unnecessarily. patch fixes these."
0,"Rename RangeQuery -> TermRangeQuery. Since we now have NumericRangeQuery (LUCENE-1701) we should rename RangeQuery to TextRangeQuery to make it clear that TextRangeQuery (TermRangeQuery?  StringRangeQuery) is based entirely on text comparison.

And, existing users on upgrading to 2.9 and using RangeQuery for [slow] numeric searching would realize they now have a good option for numeric range searching."
0,"FIXME in src/test/org/apache/lucene/IndexTest.java. Index: src/test/org/apache/lucene/IndexTest.java
===============================================================
====
--- src/test/org/apache/lucene/IndexTest.java   (revision 155945)
+++ src/test/org/apache/lucene/IndexTest.java   (working copy)
@@ -27,8 +27,7 @@   
public static void main(String[] args) {
     try {
       Date start = new Date();
-      // FIXME: OG: what's with this hard-coded dirs??
-      IndexWriter writer = new IndexWriter(""F:\\test"", new SimpleAnalyzer(),
+      IndexWriter writer = new IndexWriter(File.createTempFile(""luceneTest"",""idx""), new 
SimpleAnalyzer(),
                                           true);
        writer.setMergeFactor(20);"
0,Remove HitCollector. Remove the rest of HitCollectors
0,"more lenient behavior of Node#addMixin if mixin is already present . Change implementation of addMixin() so that it doesn't fail when the mixin is already present.

See also:

jackrabbit core change: <http://svn.apache.org/viewvc?view=rev&revision=570149>

JSR-283 issue: <https://jsr-283.dev.java.net/issues/show_bug.cgi?id=353>

(this affects both the TCK and JCR2SPI, so I didn't specify a component)"
0,"Re-index fails on corrupt bundle. The re-indexing process should be more resilient, log an error and simply continue with the next node. It doesn't seem useful to refuse repository startup in this case."
0,"repository is locked by WorkspaceJanitor when another workspace is reindexing. when the searchindex is corrupt or missing, it is rebuilt lazily after initialization of a workspace. usually by the first login on that workspace. during this time, the workspaceinfo is locked. unfortunately, the workspace janitor locks the repository and checks all workspace infos, if they can be disposed. in this case, no other access to the repository can be performed until the searchindex is initialized (which can take some time).

T1 -> WS1.login -> WS initializing
Janitor -> lock repo -> scan -> try lock WS1
T2 -> WS2.login -> must wait for T1
"
0,"TestExcetions never run. In one of the testcases of HttpClient, TestExcetions, it reads:

    // ------------------------------------------------------------------- Main
    public static void main(String args[]) {
        String[] testCaseName = { TestChallengeParser.class.getName() };
        junit.textui.TestRunner.main(testCaseName);
    }

    // ------------------------------------------------------- TestCase Methods

    public static Test suite() {
        return new TestSuite(TestChallengeParser.class);
    }

Where ""TestChallengeParser"" should be ""TestExcetions""."
0,"Jcr-server: Report#init limits the Report interface to DeltaV compliant resources. Although the REPORT features is defined by RFC 3253 and required for DeltaV compliant resources it is not limited to deltaV. See RFC 3744 (WebDAV Access Control Protocol) for additional usage of the REPORT functionality.

In order to keep the Report interface open for later usage by resources providing support for RFC 3744,
the init method signature could be modified as follows:

Report#init(DavResource, ReportInfo) instead of
Report#init(DeltaVResource, ReportInfo)"
0,"SPI: change param order with RepositoryService.createBatch. all methods on RepositoryService that require a SessionInfo list the info as first parameter, except for 

RepositoryService.createBatch(ItemId, SessionInfo) 

unless someone objects i would refacter the method signature for consistency reasons.

new:

RepositoryService.createBatch(SessionInfo, ItemId)"
0,"LayeredSchemeSocketFactory.createLayeredSocket() should have access to HttpParams. We use a custom implementation of LayeredSchemeSocketFactory that manages a keystore location through HttpParams. That allows us to use different keystores on a per connection basis.

When a proxy is used LayeredSchemeSocketFactory.createLayeredSocket() is invoked which does not have a parameter that passes the HttpParams along. In consequence certificate authentication fails in our implementation. Is there a reason why all other factory methods in the super class have an HttpParams parameter except for LayeredSchemeSocketFactory.createLayeredSocket()?

The downstream bug is here:

369805: certificate authentication with custom keystore fails behind proxy
https://bugs.eclipse.org/bugs/show_bug.cgi?id=369805

Any input would be greatly appreciated."
0,"move preflexrw to lucene3x package. Currently there are a lot of things made public in lucene3x codec, but all marked internal/experimental/deprecated.

A lot of this is just so our test codec (preflexrw) can subclass it. I think we should just move it to the same
package, then it call all be package-private."
0,"Completeness/Freshness of NamespaceRegistry and NodeTypeRegistry. We need to define the requirements on completeness and freshness of RepositoryService.getRegisteredNamespaces().

Right now the optimistic assumption seems to be that an SPI provider is able to report all namespaces that can occur in a repository beforehand. Even if it can do that (and I know of potential targets for SPI that simply can't), this seems to be quite a waste of time if these namespace prefixes aren't actually used later on.

Furthermore, in SPI namespace prefixes aren't really relevant, except to enable the transient layer to return ""meaningful"" prefixes instead of automatically generated ones.

Therefore my propoal would be to:

1) Clarify that the Map returned from getRegisteredNamespaces() isn't required to be complete,

2) Enhance JCR2SPI to auto-generate prefixes when it encounters namespaces not in the registry.

I expect this to also affect RepositoryService.(un)registerNamespace(...), but let's discuss the underlying issue first...

"
0,Improvement in comment of QValue.getLength() . The comment of QValue.getLength() should document -1 return values.
0,"Incorporate GeoHash in contrib/spatial. Based on comments from Yonik and Ryan in SOLR-773 
GeoHash provides the ability to store latitude / longitude values in a single field consistent hash field.
Which elements the need to maintain 2 field caches for latitude / longitude fields, reducing the size of an index
and the amount of memory needed for a spatial search."
0,"remove random juggling in tests, add -Dtests.seed. Since we added newIndexWriterConfig/newDirectory, etc, a lot of tests are juggling randoms around.

Instead this patch:
* changes it so LuceneTestCase[J4] manage the random.
* allow you to set -Dtests.seed=23432432432 to reproduce a test, rather than editing the code
* removes random arguments from newIndexWriterConfig, newDirectory.

I want to do this before looking at doing things like newField so we can vary term vectors, etc.

I also fixed the solr contrib builds so they arent hiding the exceptions i noted in SOLR-2002."
0,"GData Server - Milestone 3 Patch, Bugfixes, Documentation. For Milestone 3 added Features:

- Update Delete Concurrency
- Version control
- Second storage impl. based on Db4o. (Distributed Storage)
- moved all configuration in one single config file.
- removed dependencies in testcases.
- added schema validation for and all  xml files in the project (Configuration etc.)
- added JavaDoc
- much better Performance after reusing some resources
- added recovering component to lucene based storage to recover entries after a server crash or OOM Error (very simple)

- solved test case fail on hyperthread / multi core machines (@ hossman: give it a go)

@Yonik && Doug could you get that stuff in the svn please

regards simon

"
0,"Expose explicit 2-phase commit in IndexWriter. Currently when IndexWriter commits, it does so with a two-phase
commit, internally: first it prepares all the new index files, syncs
them; then it writes a new segments_N file and syncs that, and only if
that is successful does it remove any now un-referenced index files.

However, these two phases are done privately, internal to the commit()
method.

But when Lucene is involved in a transaction with external resources
(eg a database), it's very useful to explicitly break out the prepare
phase from the commit phase.

Spinoff from this thread:

  http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200804.mbox/%3C16627610.post@talk.nabble.com%3E

"
0,"Add missing license headers. There are a few source files and a number of other files with missing or incorrect license headers within the Jackrabbit source tree. See the  discusssion at http://thread.gmane.org/gmane.comp.apache.jackrabbit.devel/8698 for details. The missing license headers need to be added. 

There are also W3C licensed files. This needs to be mentioned in the NOTICE file."
0,"Generify FST shortestPaths() to take a comparator. Not sure we should do this, it costs 5-10% performance for WFSTSuggester.
But maybe we can optimize something here, or maybe its just no big deal to us.

Because in general, this could be pretty powerful, e.g. if you needed to store 
some custom stuff in the suggester, you could use pairoutputs, or whatever.

And the possibility we might need shortestPaths for other cool things... at the
least I just wanted to have the patch up here.

I haven't tested this on pairoutputs... but i've tested it with e.g. FloatOutputs
and other things and it works fine.

I tried to minimize the generics violations, there is only 1 (cannot create generic array).
"
0,"TCK: NodeOrderableChildNodesTest tests node order even if node type doesn't support child node ordering. NodeOrderableChildNodesTest# testOrderBeforeUnsupportedRepositoryOperationException

This test calls prepareTest, which requires getNodes() to return the child nodes in the order added, even if the node type doesn't support child node ordering.  JSR-170 (Section 4.4.2) imposes no such requirement.

Proposal: do not check child node order in this test case.
"
0,"Access to version history results in reading all versions of versionable node. InternalVersionHistoryImpl loads all versions at once during initialization. Because of that all versioning operations (incl. checkin, label, restore) are significantly slower when node has many versions.

"
0,"Add a constructor to org.apache.http.conn.ssl.SSLSocketFactory to allow for directly wrapping a javax.net.ssl.SSLSocketFactory socketfactory. Our application use Java Webstart for deployment.  Amoung other things, Webstart gives us the ability to access the system's (in our case, Windows) certificate system.  For instance, one of our client is using certificate based authentication to their webserver.  This is done through a hardware device they attach to their system.  Window's already has a way to interface with this device, and Webstart has a way to interface with the Windows API.

I don't think we can get by with using any SocketFactory that we create.  (We would have to check with Oracle to be sure.)  I think we need to use the one that is set as the default in HttpsURLConnection.

What I am suggesting is that another constructor be added to allow for just wrapping this one.  I was not planning on putting a dependancy on HttpsURLConnection, but rather just add the ability to wrap any javax.net.ssl.SSLSocketFactory.

This will not be a big change to the API.  I will get a patch ready soon."
0,"GetMethod.getResponseBodyAsStream() .available() could return content-length. It would be nice if the InputStream returned from
GetMethod.getResponseBodyAsStream() could override the available()
method to return the content-length of the requested URL.  This would
make things like ProgressMonitorInputStream useful for monitoring the
progress of a download.  Here is a code snippet:


/**
 * supply a hard-coded value for available() method.
 */
class FixedInputStream extends FilterInputStream {
  private int contentLength;

  public FixedInputStream(InputStream is,
              int contentLength) {
    super(is);
    this.contentLength = contentLength;
  }

  public int available() throws IOException {
    return contentLength;
  }
} 



Also, somewhat related to this request, could
GetMethod.getResponseContentLength() must be made public?  Is there a
good reason for it to be protected?  I had to extend GetMethod and
implement a public getResponseContentLength() in order to feed that
value to my FixedInputStream.

Thanks for your time."
0,"MultiThreadedHttpConnectionManager never reclaims unused connectons. There is no limit on the number of connections that will get created by the
MultiThreadedHttpConnectionManager.  Unused connections are never destroyed."
0,"Make it posible not to include TF information in index. Term Frequency is typically not needed  for all fields, some CPU (reading one VInt less and one X>>>1...) and IO can be spared by making pure boolen fields possible in Lucene. This topic has already been discussed and accepted as a part of Flexible Indexing... This issue tries to push things a bit faster forward as I have some concrete customer demands.

benefits can be expected for fields that are typical candidates for Filters, enumerations, user rights, IDs or very short ""texts"", phone  numbers, zip codes, names...

Status: just passed standard test (compatibility), commited for early review, I have not tried new feature, missing some asserts and one two unit tests

Complexity: simpler than expected

can be used via omitTf() (who used omitNorms() will know where to find it :)  "
0,"Rename OriginalQueryParserHelper. We should rename the new QueryParser so it's clearer that it's
Lucene's default QueryParser, going forward, and not just a temporary
""bridge"" to a future new QueryParser.

How about we rename oal.queryParser.original -->
oal.queryParser.standard (can't use ""default"": it's a Java keyword)?
Then, leave the OriginalQueryParserHelper under that package, but
simply rename it to QueryParser?

This way if we create other sub-packages in the future, eg
ComplexPhraseQueryParser, they too can have a QueryParser class under
them, to make it clear that's the ""top"" class you use to parse
queries.
"
0,"HttpClientUtils  - Helper methods to release resources of HttpClient / HttpResponse after use . Found myself writing this boiler plate code in various httpclient related projects , to release resources , and wanted to provide a simpler way to release resources similar to IOUtils.closeQuietly as opposed to maintaining the same in my code. 

New class: 

o.a.http.client.utils.HttpClientUtils added: 
<pre>
  public static void closeQuietly(final HttpResponse response); 
  public static void closeQuietly(final HttpClient httpClient); 
</pre>

with 2 methods ( as above ) in the same, to help release resources: 
"
0,"make frozenbuffereddeletes more efficient for terms. when looking at LUCENE-3340, I thought its also ridiculous how much ram we use for delete by term.

so we can save a lot of memory, especially object overhead by being a little more efficient."
0,"substitute for URLUtils.java. I would like to contribute a substitute for existing URLUtils.java... UrlEncodedUtils.java offer utility methods for dealing with 'urlencoded' data. Main difference with existing class is that parameters are Map <String, List <String>> instead of NameValue pairs and lack of third party dependencies...  It's partially covered with tests which I will further extend to cover all methods of this utility class."
0,"[Patch] Adding history, tab completion, status info command and masked password input for jcr-commands. I have created this patch which improves the usability of the interactive jcr command line client. It uses jline (http://jline.sourceforge.net) for the input, which gives history, tab completion and masked password input. Tab completion completes on available commands and on the jcr children of the current node for command arguments.

The login command now asks for the password if none is given, this uses the advanced password masking feature of jline to avoid the echoing of the password while typing (which is not possible using standard java System.in).

I also changed the Maven 2 pom to version 1.3-SNAPSHOT to compile with the current jackrabbit trunk and to include a manifest file with the correct starter class."
0,Basic support for fn:name(). Add basic support for fn:name() in XPath queries. Jackrabbit should at least support the the fn:name() function within an equals expression.
0,"Better name and path factory exception messages. I've ran across a few cases where the name and path factories throw an exception about an invalid path or name, but fail to include the actual path or name in the exception message. It would be very helpful to have that extra bit of information included."
0,"Create a Size Estimator model for Lucene and Solr. It is often handy to be able to estimate the amount of memory and disk space that both Lucene and Solr use, given certain assumptions.  I intend to check in an Excel spreadsheet that allows people to estimate memory and disk usage for trunk.  I propose to put it under dev-tools, as I don't think it should be official documentation just yet and like the IDE stuff, we'll see how well it gets maintained."
0,"Make Jackrabbit compile on Java 7. Compiling on Java 7 fails with the following error:

    jackrabbit-core/src/main/java/org/apache/jackrabbit/core/util/db/DataSourceWrapper.java:[30,7] error:
    DataSourceWrapper is not abstract and does not override abstract method getParentLogger() in CommonDataSource

We should fix that."
0,JSR 283: Binary interfaces . The Binary interface replaces the deprecated methods for getting/setting the InputStream of a given JCR value and the method to create binary value (ValueFactory).
0,"New method to add an array of parameters to PostMethod. When posting a form a web page may have many parameters to post to the 
webserver.  Currently in PostMethod, if you wanted to add multiple parameters, 
you would have to call addParameter(name, value) for each one.

A new convinence method should be added to allow for simpler client code by 
taking an array of NameValuePair objects and adding all parameters in a single 
function call.

void addParameters(NameValuePair[] parameters) 

Also, the comments for PostMethod functions that deal with parameters 
state ""Override method of HttpMethodBase ..."" which is incorrect.  More 
informative comments should be added to this public API."
0,"Upgrade to Lucene 2.2. Lucene 2.1 contains a number of useful enhancements, which could be benefical to jackrabbit:

- less locking on index updates -> less IO calls
- introduces FieldSelector -> allows jackrabbit to only load required fields"
0,"Use Apache Tika for text extraction. Once Apache Tika is released with a resolution to TIKA-175 (making Tika available to Java 1.4 projects), we should replace our direct parser library dependencies with Tika parsers. Ideally we'd just use the Tika AutoDetectParser that'll automatically detect the type of a binary and parse it accordingly, solving JCR-728.

I guess we should keep some level of backwards compatibility with existing textFilterClasses=""..."" configurations, perhaps by keeping the existing TextExtractor classes as wrappers around respective Tika parsers."
0,"don't require an analyzer, if all fields are NOT_ANALYZED. This seems wierd, if you analyze only NOT_ANALYZED fields, you must have an analyzer (null will not work)
because documentsinverter wants it for things like offsetGap"
0,"Use correct version number in repository descriptor. The repository descriptor 'jcr.repository.version' always shows 1.0-dev.

The value should reflect the current jackrabbit version."
0,AbstractReadableRepositoryService should pass credentials to createSessionInfo. Currently AbstractReadableRepositoryService.obtain() calls checkCredentials() followed by createSessionInfo() where it only passes the userId rather than the credentials. In many cases an implementation will need the credentials to create the SessionInfo. 
0,"FastVectorHighlighter - expose FieldFragList.fragInfo for user-customizable FragmentsBuilder. Needed to build a custom highlightable snippet - snippet should start with the sentence containing the first match, then continue for 250 characters.

So created a custom FragmentsBuilder extending SimpleFragmentsBuilder and overriding the createFragments(IndexReader reader, int docId, String fieldName, FieldFragList fieldFragList) method - unit test containing the code is attached to the JIRA.

To get this to work, needed to expose (make public) the FieldFragList.fragInfo member variable. This is currently package private, so only FragmentsBuilder implementations within the lucene-highlighter o.a.l.s.vectorhighlight package (such as SimpleFragmentsBuilder) can access it. Since I am just using the lucene-highlighter.jar as an external dependency to my application, the simplest way to access FieldFragList.fragInfo in my class was to make it public.
"
0,"TCK: GetPersistentQueryPathTest and SaveTest require nt:query. GetPersistentQueryPathTest and SaveTest require implementation to support nt:query node type.  This is an optional node type.

Proposal: throw NotExecutableException if nt:query not in node type registry
"
0,"Setting different MAX_HOST_CONNECTION values per host using a single MultiThreadedHttpConnectionManager. Right now, it's not possible to use the
MultiThreadedHttpConnectionManager.setMaxConnectionsPerHost(int) method in a per
HostConfiguration basis. The value applies to every HostConfiguration the
current connection manager is managing.

I would be quite useful to allow the connection manager to set different values
depending on the HostConfiguration."
0,Update Jackrabbit API JavaDoc on http://jackrabbit.apache.org. Update the JavaDoc on http://jackrabbit.apache.org to the latest release version of jackrabbit.
0,"pom.xml in sandbox/spi has wrong scm url. the pom in sandbox/spi has the wrong scm url.
You can see the pom here:
http://svn.apache.org/repos/asf/jackrabbit/sandbox/spi/pom.xml

It probably should be:
<scm>
    <connection>scm:svn:http://svn.apache.org/repos/asf/jackrabbit/sandbox/spi </connection>
    <developerConnection>scm:svn:https://svn.apache.org/repos/asf/jackrabbit/sandbox/spi </developerConnection>
    <url>http://svn.apache.org/viewvc/jackrabbit/sandbox/spi </url>
</scm>
"
0,"Implement a backup tool. Issue for tracking the progress of the Google Summer of Code project assigned to Nicolas Toper.  The original project requirements are:

""Implement a tool for backing up and restoring content in an Apache Jackrabbit content repository. In addition to the basic content hierarchies, the tool should be able to efficiently manage binary content, node version histories, custom node types, and namespace mappings. Incremental or selective backups would be a nice addition, but not strictly necessary."""
0,Make Jackrabbit repository DTD easier to extend. It would be nice if a downstream project could easily extend the Jackrabbit repository configuration format by adding new configuration elements under the <Repository> root. Currently that can only be done by customizing a copy of the entire DTD. It would be better if the relevant parts of the DTD could simply be included via an external parameter entity into a downstream DTD.
0,Sessions are not logged out in case of exceptions. Some test cases do not logout sessions if an exception occurs.
0,"[PATCH] don't delete all files in index directory on index creation. Many people use Lucene to index a part of their file system. The chance that  
you some day mix up index directory and document directory isn't that bad.  
Currently Lucene will delete *all* files in the index directory when the  
create paramater passed to IndexWriter is true, thus deleting your documents 
if you mixed up the parameters. I'll attach a patch that fixes  
this. Any objections?"
0,"TCK: NamespaceRegistryTest#testUnregisterNamespaceExceptions doesn't fail if expected exception isn't thrown. In two places, the test doesn't fail if an expected exception isn't thrown.

Proposal: test should fail if expected exception isn't thrown.

--- NamespaceRegistryTest.java  (revision 422074)
+++ NamespaceRegistryTest.java  (working copy)
@@ -150,6 +154,7 @@
         for (int t = 0; t < SYSTEM_PREFIXES.length; t++) {
             try {
                 nsp.unregisterNamespace(SYSTEM_PREFIXES[t]);
+                fail(""Trying to unregister "" + SYSTEM_PREFIXES[t] + "" must fail"");
             } catch (NamespaceException e) {
                 // expected behaviour
             }
@@ -159,6 +164,7 @@
         // must throw a NamespaceException.
         try {
             nsp.unregisterNamespace(""ThisNamespaceIsNotCurrentlyRegistered"");
+            fail(""Trying to unregister an unused prefix must fail"");
         } catch (NamespaceException e) {
             // expected behaviour
         }
"
0,"CachingHttpClient returns a 503 response when the backend HttpClient produces an IOException. The CachingHttpClient returns an HTTP 503 response when the backend HttpClient throws an IOException.

It happens for instance when the backend is DefaultHttpClient (AbstractHttpClient), issuing a request to a server not listening on the target port.
Well, it sounds tricky, but it makes the HttpClient not having a consistant behaviour in an implementation using both caching and regular clients.

If a 503 should really be returned in that case, I suggest the AbstractHttpClient to return it and the CachingHttpClient to just propagate any exception thrown by the backend.
"
0,"BooleanScorer should not limit number of prohibited clauses. Today it's limited to 32, because it uses a separate bit in the mask
for each clause.

But I don't understand why it does this; I think all prohibited
clauses can share a single boolean/bit?  Any match on a prohibited
clause sets this bit and the doc is not collected; we don't need each
prohibited clause to have a dedicated bit?

We also use the mask for required clauses, but this code is now
commented out (we always use BS2 if there are any required clauses);
if we re-enable this code (and I think we should, at least in certain
cases: I suspect it'd be faster than BS2 in many cases), I think we
can cutover to an int count instead of bit masks, and then have no
limit on the required clauses sent to BooleanScorer also.

Separately I cleaned a few things up about BooleanScorer: all of the
embedded scorer methods (nextDoc, docID, advance, score) now throw
UOE; pre-allocate the buckets instead of doing it lazily
per-sub-collect.
"
0,"JSR 283: VersionManager and new versioning methods. JSR 283 introduces a new interface ""VersionManager"" that exposes all versioning functionality that was formerly present on Node and Workspace. 

In addition the following new methods:

- checkpoint(String absPath) Version 
- merge(String absPath, String srcWorkspace, boolean bestEffort, boolean isShallow) NodeIterator 
- merge(Node activityNode) NodeIterator 

See Issue #JCR-1592 for Activity and Configuration feature."
0,"Index File Format - Example for frequency file .frq is wrong. Reported by Johan Stuyts - http://www.nabble.com/Possible-documentation-error--p7012445.html - 

Frequency file example says: 

     For example, the TermFreqs for a term which occurs once in document seven and three times in document eleven would be the following sequence of VInts: 
         15, 22, 3 

It should be: 

     For example, the TermFreqs for a term which occurs once in document seven and three times in document eleven would be the following sequence of VInts: 
         15, 8, 3 


"
0,"textfilters module patch: Support for text extraction for HTML,XML and RTF files. This patch adds text extraction support form XML, RTF and HTML files.

The unique dependency is htmlparser library for handling HTML text extraction."
0,"upgrade icu to 4.6. version 4.6 supports unicode 6, new collators (search collators) etc."
0,"don't try to cache a composite reader's MultiBits deletedDocs. MultiFields.getDeletedDocs now builds up a MultiBits instance (so that one can check if a top-level docID is deleted), but it now stuffs it into a private cache on IndexReader.

This is invalid when the composite reader is read/write, and can result in a MultiReader falsely claiming a doc was not deleted."
0,"Add FixedBitSet.and(other/DISI), andNot(other/DISI). For the parent issue, and() and andNot() on DISIs and other FixedBitSets are missing. This issue will add those methods.

The DISI methods (also the already existing or(DISI)) method will check for OpenBitSetIterator and do an inplace operation using the bits as optimization."
0,"Caching does not work when using RMI. Filters and caching uses transient maps so that caching does not work if you are using RMI and a remote searcher 

I want to add a new RemoteCachededFilter that will make sure that the caching is done on the remote searcher side 
 "
0,"Reduce temporary memory usage of hierarchy cache initialization. Initializing the hierarchy cache temporarily uses memory, which is linear to the size of the index segment. This process should be split into multiple phases to limit the memory usage to a fixed amount.

The temporary memory usage for an index segment current is about 170 bytes per node."
0,"singletermsenum. singletermsenum for flex (like the existing singletermenum, it is a filteredtermSenum that only matches one term, to preserve multitermquery semantics)"
0,"Reintroduce NamespaceStorage. hi jukka

i open this issue as a reminder of our recent discussion in basel:

we decided that you will 
- reintroduce the NamespaceStorage you recently removed from jcr2spi
- reintroduce a namespace cache in jcr2spi (but using a simple map instead of NamespaceCache object)

in addition we agreed that we want to share the NamespaceRegistryImpl between jcr2spi and jackrabbit-core
and you volenteered to provide a patch for that.

thanks in advance
angela"
0,"Make Lucene - Java 1.9.1 Available in Maven2 repository in iBibilio.org. Please upload 1.9.1 release to iBiblio so that Maven users can easily use the latest release.  Currently 1.4.3 is the most recently available version: http://www.ibiblio.org/maven2/lucene/lucene/

Please read the following FAQ for more information: http://maven.apache.org/project-faq.html"
0,"contrib/xml-query-parser, BoostingTermQuery support. I'm not 100% on this patch. 

BooleanTermQuery is a part of the spans family, but I generally use that class as a replacement for TermQuery.  Thus in the DTD I have stated that it can be a part of the root queries as well as a part of a span. 

However, SpanFooQueries xml elements are named <SpanFoo/> rather than <SpanFooQuery/>, I have however chosen to call it <BoostingTermQuery/>. It would be possible to set it up so it would be parsed as <SpanBoostingTerm/> when inside of a <SpanSomething>, but I just find that confusing.
"
0,"A Linux-specific Directory impl that bypasses the buffer cache. I've been testing how we could prevent Lucene's merges from evicting
pages from the OS's buffer cache.  I tried fadvise/madvise (via JNI)
but (frustratingly), I could not get them to work (details at
http://chbits.blogspot.com/2010/06/lucene-and-fadvisemadvise.html).

The only thing that worked was to use Linux's O_DIRECT flag, which
forces all IO to bypass the buffer cache entirely... so I created a
Linux-specific Directory impl to do this.
"
0,"Change Term to use bytes. in LUCENE-2426, the sort order was changed to codepoint order.

unfortunately, Term is still using string internally, and more importantly its compareTo() uses the wrong order [utf-16].
So MultiTermQuery, etc (especially its priority queues) are currently wrong.

By changing Term to use bytes, we can also support terms encoded as bytes such as numerics, instead of using
strange string encodings.
"
0,"[PATCH] queryParser.setOperator(int) should be made typesafe. There are AND and DEFAULT_OPERATOR_AND in QueryParser, so calling 
setOperator(QueryParser.AND) looks okay and compiles, but it's not correct. 
I'll attach a patch that uses a typesafe enum to avoid this problem. As 
there's also a getOperator method I had to change the name of the new method 
to get/setDefaultOperator. I don't like that, but it seems to be the only way 
to avoid compile errors for people who switch to a new version of Lucene. 
 
Okay to commit?"
0,"Move ocm documentation to jackrabbit-site. The OCM documentation from jackrabbit-ocm/xdocs should be moved to jackrabbit-site.

Also all old references to Graffito should be replaced with Jackrabbit."
0,"Similarity javadocs look ugly if created with java7's javadoc. The captions used to illustrate the formulas are tables here:
in jdk 5/6 the table is centered nicely.

But with java7's javadocs (I think due to some css styles changes?),
the table is not centered but instead stretched.

I think we just need to center this table with a different technique?

Have a look at http://people.apache.org/~rmuir/java7-style-javadocs/org/apache/lucene/search/Similarity.html to see what I mean.

NOTE: these javadocs are under TFIDFSimilarity.java in trunk."
0,"[RFE] Allow streaming of POST methods via chunked transfer encoding.. This is an RFE with a possible implementation attached. The implementation does
not modify any existing code.

We're using HTTP POST to send a large amount of data with an unknown size. We
don't want to buffer the entire request, so we implemented a streaming POST
method. The implementation has 3 classes: StreamedPostMethod,
BufferedChunkedOutputStream and OutputStreamWriter. The bulk of the code is in
the BufferedChunkedOutputStream, which may be a good target for replacing
ChunkedOutputStream from the main distribution.

BufferedChunkedOutputStream has the following charactersitics:
1) It has an internal 2K buffer. Without the buffer, chunk sizes would be too
small in many cases (e.g. ObjectOutputStream likes to call write(byte[]) with 4
byte long arguments). 2K was chosen to minimize the chunk overhead to less than 1%.
2) If the entire entity body fits within the 2K buffer, it does not use
chunking. This implies that the headers are only sent out when the first chunk
(or the entire body) has to be written, but no sooner.
3) The chunk size is not limited to 2K: if write(byte[]) is called with a large
argument, the internal buffer and the new request are sent out as a single chunk.
4) Because of (2) it's tightly coupled to StreamedPostMethod.reallyWriteHeaders.
5) StreamedPostMethod calls BufferedChunkedOutputStream.finish() to write the
last buffer and ending chunk.

Because of 4 and 5, we didn't want to touch ChunkedOutputStream. Interestingly,
EntityEnclosingMethod is already tightly coupled to ChunkedOutputStream because
it has to call writeClosingChunk. There is probably some room for refactoring here.

The package is just a suggestion; feel free to move the files as appropirate.
This code was written against 2.0rc2. We're hoping it will get included in time
for the 2.1 release.

To use the code, you must implement OutputStreamWriter and pass it to
StreamedPostMethod's constructor. Execute the method as usual.

Caveats: StreamedPostMethod does not implement Expect/continue logic. We had no
way to test this. It is also strictly for POST. In general, the same methodology
is applicable to PUT, etc. It should be fairly simple to generalize.

Legal: Goldman, Sachs & Co. is making this code available under the Apache License."
0,"make it easier to access default stopwords for language analyzers. DM Smith made the following comment: (sometimes it is hard to dig out the stop set from the analyzers)

Looking around, some of these analyzers have very different ways of storing the default list.
One idea is to consider generalizing something like what Simon did with LUCENE-1965, LUCENE-1962,
and having all stopwords lists stored as .txt files in resources folder.

{code}
  /**
   * Returns an unmodifiable instance of the default stop-words set.
   * @return an unmodifiable instance of the default stop-words set.
   */
  public static Set<String> getDefaultStopSet()
{code}
"
0,FieldDoc.toString only returns super.toString. The FieldDoc.toString method very carefully builds a StringBuffer sb containing the information for the FieldDoc instance and then just returns super.toString() instead of sb.toString()
0,"jcr-commons: add cnd writer functionality. currently jcr-commons only provides an cnd-reader while the writer functionality is only present in spi-commons.
for JCR-2948 a implementation independent cnd-writer would be useful and i would therefore suggest to
add this to jcr-commons based on the code present in spi-commons and let the implementation in spi-commons
extend from the general functionality."
0,"CompactNodeTypeDefReader could auto-provide default namespace mappings if omitted. the default namespaces for 'jcr', 'nt', 'mix', '' should  be automatically registered during parsing if needed."
0,"Polish Analyzer. Andrzej Bialecki has written a Polish stemmer and provided stemming tables for it under Apache License.

You can read more about it here: http://www.getopt.org/stempel/

In reality, the stemmer is general code and we could use it for more languages too perhaps."
0,"FileDataStore: only open a stream when really necessary. Currently, PropertyImpl.getValue() opens a FileInputStream if the FileDataStore is used.
If the application doesn't use the value, this stream is never closed.

PropertyImpl.getValue():
  return internalGetValue().toJCRValue(session);
InternalValue.toJCRValue(..):
  case PropertyType.BINARY:
    return new BinaryValue(((BLOBFileValue) val).getStream());
BLOBInDataStore.getStream():
  return getDataRecord().getStream();
FileDataRecord.getStream():
  return new FileInputStream(file);

One solution is to return a 'lazy' file input stream that only opens the file when reading from the stream (and closing the file when the last byte was read). Maybe there is already a class (in Apache Commons maybe?) that can do that.
"
0,Promote ItemInfo builder classes from GetItemsTest to top level classes. org.apache.jackrabbit.jcr2spi.GetItem test contains builders for ItemInfo and NodeInfo instances. These should be generalized and promoted to spi-commons. 
0,"Improvement to MultiValueCollectionConverterImpl to Map collections with element class Object.class. Currently MultiValueCollectionConverterImpl  does not support elements of type Object.class.  The type of the contained class has to be specified either through the mapping file or through the Bean annotation.  Even with that flexibility Object.class is specifically excluded (For good reasons.).  

My view is that by definition MultiValueCollectionConverterImpl   should make a best effort to convert and that best effort should include using Undefined UndefinedTypeConverterImpl to convert an object when all the other conversion strategies run out.  To this resolve I have patched the OCM source.  I have test cases also.  I will upload the patch files right after."
0,"FieldCacheImpl's getCacheEntries() is buggy as it uses WeakHashMap incorrectly and leads to ConcurrentModExceptions. The way how WeakHashMap works internally leads to the fact that it is not allowed to iterate over a WHM.keySet() and then get() the value. As each get() operation inspects the ReferenceQueue of the weak keys, they may suddenly disappear. If you use the entrySet() iterator you get key and value and no need to call get(), contains(),... that inspects the ReferenceQueue."
0,"IndexSplitter that divides by primary key term. Index splitter that divides by primary key term.  The contrib MultiPassIndexSplitter we have divides by docid, however to guarantee external constraints it's sometimes necessary to split by a primary key term id.  I think this implementation is a fairly trivial change."
0,"Allow heuristic freshness caching. I noticed that the CachingHttpClient behaves strangely when it receives responses with only the public cache-control directive, e.g.:

HTTP/1.0 200 OK
Server: My test server
Cache-control: public
Content-Length: 1

1


Using a debugger, I could see that the response is cached. But when the response is queried from the cache, it is not considered as ""fresh"".
According to the HTTP RFC, such responses ""may"" be cached (I understand it as a ""should"" in our case)... but there's no reason to put responses in the cache if we don't use them later one.

The ""freshness of the response is analysed after the response is queried from the cache, thanks to:
CachedResponseSuitabilityChecker#canCachedResponseBeUsed()
... calling CacheEntry#isResponseFresh()
... returning true if the response date (getCurrentAgeSecs()) is lower than its use-by date (getFreshnessLifetimeSecs())

The issue is that getFreshnessLifetimeSecs() returns 0 when there is no max-age directive.

This could be fixed by replacing the code of CacheEntry#isResponseFresh() by:
    public boolean isResponseFresh() {
        final long freshnessLifetime = getFreshnessLifetimeSecs();
        if (freshnessLifetime == 0) {
            return true;
        }
        return (getCurrentAgeSecs() < getFreshnessLifetimeSecs());
    }

But i'm not 100% confident about not producing some bad side-effects..."
0,"Incorrect transitive snapshot dependencies. Using ${version} in dependency declarations causes troubles since for snapshot dependencies the version variable apparently gets replaced by the exact timestamp of a deployed snapshot instead of the ""x.y-SNAPSHOT"" string. Typically the timestamps of different artifacts are not the same, causing broken dependencies."
0,"Replace spatial contrib module with LSP's spatial-lucene module. I propose that Lucene's spatial contrib module be replaced with the spatial-lucene module within Lucene Spatial Playground (LSP).  LSP has been in development for approximately 1 year by David Smiley, Ryan McKinley, and Chris Male and we feel it is ready.  LSP is here: http://code.google.com/p/lucene-spatial-playground/  and the spatial-lucene module is intuitively in svn/trunk/spatial-lucene/.

I'll add more comments to prevent the issue description from being too long."
0,"Unique ID for org.apache.jackrabbit.value.BinaryValue. BinaryValue should have a method get the unique identifier (if one is available). That way an application may not have to read the stream if that value is already processed.

When the DataStore is used, a unique identifier is available, so probably this feature is quite simple to implement.

See also http://www.nabble.com/Workspace.copy()-Question-...-td20435164.html (but please don't reply to this thread from now on - instead add comments to this issue).

Another feature is getFileName() to get the file name if it is stored in the file system. This method may need a security mechanism, for example getFileName(Session s) so that the system can check it. In any case the file should not be modified, but maybe knowing the file name is already too dangerous in some cases."
0,"Add matchVersion to StandardAnalyzer. I think we should add a matchVersion arg to StandardAnalyzer.  This
allows us to fix bugs (for new users) while keeping precise back
compat (for users who upgrade).

We've discussed this on java-dev, but I'd like to now make it concrete
(patch attached).  I think it actually works very well, and is a
simple tool to help us carry out our back-compat policy.

I coded up an example with StandardAnalyzer:

  * The ctor now takes a required arg (Version matchVersion).  You
    pass Version.LUCENE_CURRENT to always get lates & greatest, or eg
    Version.LUCENE_24 to match 2.4's bugs/settings/behavior.

  * StandardAalyzer conditionalizes the ""replace invalid acronym"" and
    ""enable position increment in StopFilter"" based on matchVersion.

  * It also prevents creating zillions of ctors, over time, as we need
    to change settings in the class.  EG StandardAnalyzer now has 2
    settings that are version dependent, and there's at least another
    2 issues open on fixing some more of its bugs.

The migration is also very clean: we'd only add this to classes on an
""as needed"" basis.  On the first release that adds the arg, the
default remains back compatible with the prior release.  Then, going
forward, we are free to fix issues on that class and conditionalize by
matchVersion.

The javadoc at the top of StandardAnalyzer clearly calls out what
version specific behavior is done:

{code}
 * <p>You must specify the required {@link Version}
 * compatibility when creating StandardAnalyzer:
 * <ul>
 *   <li> As of 2.9, StopFilter preserves position
 *        increments by default
 *   <li> As of 2.9, Tokens incorrectly idenfied as acronyms
 *        are corrected (see <a href=""https://issues.apache.org/jira/browse/LUCENE-1068"">LUCENE-1608</a>
 * </ul>
 *
{code}
"
0,"Decouple indexer from Document/Field impls. I think we should define minimal iterator interfaces,
IndexableDocument/Field, that indexer requires to index documents.

Indexer would consume only these bare minimum interfaces, not the
concrete Document/Field/FieldType classes from oal.document package.

Then, the Document/Field/FieldType hierarchy is one concrete impl of
these interfaces. Apps are free to make their own impls as well.
Maybe eventually we make another impl that enforces a global schema,
eg factored out of Solr's impl.

I think this frees design pressure on our Document/Field/FieldType
hierarchy, ie, these classes are free to become concrete
fully-featured ""user-space"" classes with all sorts of friendly sugar
APIs for adding/removing fields, getting/setting values, types, etc.,
but they don't need substantial extensibility/hierarchy. Ie, the
extensibility point shifts to IndexableDocument/Field interface.

I think this means we can collapse the three classes we now have for a
Field (Fieldable/AbstracField/Field) down to a single concrete class
(well, except for LUCENE-2308 where we want to break out dedicated
classes for different field types...).
"
0,"javacc on Win32 (cygwin) creates wrong line endings - fix them with 'ant replace'. ""ant javacc"" in Windows/Cygwin generates files with wrong line endings (\r  or \r\n instead of *Nix's \n). 
I managed to get rid of those using    perl -p -e 's/(\r\n|\n|\r)/\n/g'
Some useful info on line ending issues is in http://en.wikipedia.org/wiki/Newline

After wasting some time to get rid of those, I modified javacc-QueryParser build.xml task to take care of that.
So now QueryParser files created with ""ant javacc"" are fixed (if required) to have \n as line ends.

Should probably do that also for the other javacc targets: javacc-HTMLParser and javacc-StandardAnalyzer(?)
"
0,"First Steps document is outdated. As reported by Manoj Prasad on the development mailing list, the code and configuration shown on the First Steps document [1] is no longer up to date with the latest Jackrabbit sources. The differences are:

   * the Versioning element needs to be added to the repository configuration file
   * the output values printed by the examples have changes (log messages, new node types, etc.)
   * multiple values (especially jcr:mixinTypes properties) are not handled correctly

The document should be updated.

[1] http://incubator.apache.org/jackrabbit/firststeps.html"
0,PrivilegeHandlerTest fails on Windows. The test fails on Windows because there are differences in line breaks between expected and actual results.
0,"upgrade icu libraries to 4.4.2. modules/analysis uses 4.4
solr/contrib/extraction uses 4.2.1

I think we should keep them the same version, for consistency, and go to 4.4.2 since it has bugfixes."
0,"Allow parsing custom elements in workspace config. In RepositoryConfigurationParser, most *Config elements can be extended in a derived class, e.g.

    public LoginModuleConfig parseLoginModuleConfig(Element security)

Unfortunately, parseWorkspaceConfig expects an InputSource. One should add a

    protected WorkspaceConfig parseWorkspaceConfig(Element root)

to allow returning a WorkspaceConfig derived class, without having to copy the entire implementation."
0,"add IndexWriter.removeUnferencedFiles, so apps can more immediately delete index files when readers are closed. This has come up several times on the user's list.

On Windows, which prevents deletion of still-open files, IndexWriter cannot remove files that are in-use by open IndexReaders.  This is fine, and IndexWriter periodically retries the delete, but it doesn't retry very often (only on open, on flushing a new segment, and on committing a merge).  So it lacks immediacy.

With this expert method, apps that want faster deletion can call this method."
0,"Payload Queries. Now that payloads have been implemented, it will be good to make them searchable via one or more Query mechanisms.  See http://wiki.apache.org/lucene-java/Payload_Planning for some background information and https://issues.apache.org/jira/browse/LUCENE-755 for the issue that started it all.  "
0,"add ElisionsFilter to ItalianAnalyzer. we set this up for french by default, but we don't for italian.
we should enable it with the standard italian contractions (e.g. definite articles).

the various stemmers for these languages assume this is already being taken care of
and don't do anything about it... in general things like snowball assume really dumb
tokenization, that you will split on the word-internal ', and they add these to stoplists."
0,"Large fetch sizes have potentially deleterious effects on VM memory requirements when using Oracle. Since Release 10g, Oracle JDBC drivers use the fetch size to allocate buffers for caching row data.
cf. http://www.oracle.com/technetwork/database/enterprise-edition/memory.pdf

r1060431 hard-codes the fetch size for all ResultSet-returning statements to 10,000. This value has significant, potentially deleterious, effects on the heap space required for even moderately-sized repositories. For example, the BUNDLE table (from 'oracle.ddl') has two columns -- NODE_ID raw(16) and BUNDLE_DATA blob -- which require 16 b and 4 kb of buffer space, respectively. This requires a buffer of more than 40 mb [(16+4096) * 10000 = 41120000].

If the issue described in JCR-2832 is truly specific to PostgreSQL, I think its resolution should be moved to a PostgreSQL-specific ConnectionHelper subclass. Failing that, there should be a way to override this hard-coded value in OracleConnectionHelper."
0,"internal hashing improvements. Internal power-of-two closed hashtable traversal in DocumentsWriter and CharArraySet could be better.

Here is the current method of resolving collisions:
    if (text2 != null && !equals(text, len, text2)) {
      final int inc = code*1347|1;
      do {
        code += inc;
        pos = code & mask;
        text2 = entries[pos];
      } while (text2 != null && !equals(text, len, text2));

The problem is that two different hashCodes with the same lower bits will keep picking the same slots (the upper bits will be ignored).
This is because multiplication (*1347) only really shifts bits to the left... so given that the two codes already matched on the right, they will both pick the same increment, and this will keep them on the same path through the table (even though it's being added to numbers that differ on the left).  To resolve this, some bits need to be moved to the right when calculating the increment.

"
0,"Extend the client's redirect handling interface to allow control of the content of the redirect. The existing RedirectHandler interface provides the ability influence which situations cause redirects, but gives you no control over the content of the redirect itself.  For example, if you want the client follow the redirect of a POST request with a POST request to the new location, you can't do it.  DefaultRequestDirector decides what method will be used on the redirect request and as of the most recent patch, it's always either a HEAD or a GET.

One option for resolving this might be extending the RedirectHandler interface to be a factory for creating the redirect request object.  The the DefaultRequestDirector could then be changed to ask the RedirectHandler to create the appropriate request for the situation.

Thanks,
Ben"
0,"Can't put non-index files (e.g. CVS, SVN directories) in a Lucene index directory. Lucene won't tolerate foreign files in its index directories.  This makes it impossible to keep an index in a CVS or Subversion repository.

For instance, this exception appears when creating a RAMDirectory from a java.io.File that contains a subdirectory called "".svn"".

java.io.FileNotFoundException: /home/local/ejj/ic/.caches/.search/.index/.svn
(Is a directory)
        at java.io.RandomAccessFile.open(Native Method)
        at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
        at
org.apache.lucene.store.FSIndexInput$Descriptor.<init>(FSDirectory.java:425)
        at org.apache.lucene.store.FSIndexInput.<init>(FSDirectory.java:434)
        at org.apache.lucene.store.FSDirectory.openInput(FSDirectory.java:324)
        at org.apache.lucene.store.RAMDirectory.<init>(RAMDirectory.java:61)
        at org.apache.lucene.store.RAMDirectory.<init>(RAMDirectory.java:86)"
0,"Clone proxStream lazily in SegmentTermPositions. In SegmentTermPositions the proxStream should be cloned lazily, i. e. at the first time nextPosition() is called. Then the initialization costs of TermPositions are not higher anymore compared to TermDocs and thus there is no reason anymore for Scorers to use TermDocs instead of TermPositions. In fact, all Scorers should use TermPositions, because custom subclasses of existing scorers might want to access payloads, which is only possible via TermPositions. We could further merge SegmentTermDocs and SegmentTermPositions into one class and deprecate the interface TermDocs.

I'm going to attach a patch once the payloads feature (LUCENE-755) is committed."
0,"Deprecate Analyzer.tokenStream. The addition of reusableTokenStream to the core analyzers unfortunately broke back compat of external subclasses:

    http://www.nabble.com/Extending-StandardAnalyzer-considered-harmful-td23863822.html

On upgrading, such subclasses would silently not be used anymore, since Lucene's indexing invokes reusableTokenStream.

I think we should should at least deprecate Analyzer.tokenStream, today, so that users see deprecation warnings if their classes override this method.  But going forward when we want to change the API of core classes that are extended, I think we have to  introduce entirely new classes, to keep back compatibility."
0,Add RAR META-INF/ra.xml descriptor to be used with JCA1.5. I added ra.xml that lets jackrabbit jca to be used with JCA1.5 like in JBoss
0,"Use Apache Codec 1.4. 1.4 fixes many bugs and added some nice features: http://commons.apache.org/codec/changes-report.html

It took me a whiel to find out, that this was the main reason my tests failed (MethodNotFoundException).
"
0,"Optimize TermInfosWriter.add. I found one more optimization, in how terms are written in
TermInfosWriter.  Previously, each term required a new Term() and a
new String().  Looking at the cpu time (using YourKit), I could see
this was adding a non-trivial cost to flush() when indexing Wikipedia.

I changed TermInfosWriter.add to accept char[] directly, instead.

I ran a quick test building first 200K docs of Wikipedia.  With this
fix it took 231.31 sec (best of 3) and without the fix it took 236.05
sec (best of 3) = ~2% speedup.
"
0,ORM PersistenceManagers don't compile. ORM PMs are out of synch with the latest changes of the api. 
0,"need members of MultipartRequestEntity to be ""protected"" instead of ""private"" to make it extendable for multipart/related. As explained in the mailing-list[1], I'd like to have some of 
MultipartRequestEntity move from ""private"" visibility to ""protected"" visibility,
to be able to extend as MultipartRelatedRequestEntity. Namely, the attribute
""parts"" and the method ""getMultipartBoundary"" would be needed.

Thank you.

[1]
http://mail-archives.apache.org/mod_mbox/jakarta-httpclient-dev/200510.mbox/%3c87irw18ndm.fsf@meuh.mnc.ch%3e"
0,Improve password hashing. 
0,"Add CharArrayMap to lucene and make CharAraySet an proxy on the keySet() of it. This patch adds a CharArrayMap<V> to Lucene's analysis package as compagnon of CharArraySet. It supports fast retrieval of char[] keys like CharArraySet does. This is important for some stemmers and other places in Lucene.

Stemers generally use CharArrayMap<String>, which has then get(char[]) returning String. Strings are compact and can be easily copied into termBuffer. A Map<String,String> would be slow as the termBuffer would be first converted to String, then looked up. The return value as String is perfectly legal, as it can be copied easily into termBuffer.

This class borrows lots of code from Solr's pendant, but has additional features and more consistent API according to CharArraySet. The key is always <?>, because as of CharArraySet, anything that has a toString() representation can be used as key (of course with overhead). It also defines a unmodifiable map and correct iterators (returning the native char[]).

CharArraySet was made consistent and now returns for matchVersion>=3.1 also an iterator on char[]. CharArraySet's code was almost completely copied to CharArrayMap and removed in the Set. CharArraySet is now a simple proxy on the keySet().

In future we can think of making CharArraySet/CharArrayMap/CharArrayCollection an interface so the whole API would be more consistent to the Java collections API. But this would be a backwards break. But it would be possible to use better impl instead of hashing (like prefix trees)."
0,"GData html render preview. GData output is usually ATOM / RSS e.g plain xml. This feature enables users or admins to preview the server output as html transformed by user defined xsl stylesheet. Stylesheet is configurable per service.

That's just a cool feature for developing and for users wanna use the server for simple blog or feed server.

regards simon"
0,"[PATCH] Allow RepositoryAccessServlet to get the Repository from a ServletContext attribute. The attached patch adds a repository.context.attribute.name init parameter to the RepositoryAccessServlet:

        <init-param>
          <param-name>repository.context.attribute.name</param-name>
          <param-value>javax.jcr.Repository</param-value>
          <description>
            If this is set, the RepositoryAccessServlet expects a Repository in the ServletContext 
            attribute having this name. This allows servlets of this module to be used with repositories
            intialized by the jackrabbit-jcr-servlet module utilities.
          </description>
        </init-param>"
0,"[PATCH] fix various small issues with the ""getting started"" demo pages. 
This patch contains numerous small fixes for the ""getting started""
pages on the Lucene Java web site.  Here are the rough fixes:

  * To results.jsp:

    - changed StopAnalyzer -> StandardAnalyzer

    - changed references of ""url"" to ""path"" (field ""url"" is never set
      and was therefore always null)

    - remove prefix of ""../webapps"" from path so clicking through works

  * Fixed typos, grammar and other cosmetic things.

  * Modernized some things that have changed with time (names of JAR
    files, which languages have analyzers, etc.)

  * Added outbound links to Javadocs, Wiki, Lucene static web site,
    external sites, when appropriate.

  * Removed exact version of Tomcat for the demo web app (I think all
    recent versions of Tomcat will work as described)

  * Other small changes...

Net/net I think this is an improved version of what's available on the
site today."
0,"If tests fail, don't report about unclosed resources. LuceneTestCase ensures in afterClass() if you closed all your directories, which in turn will check if you have closed any open files.

This is good, as a test will fail if we have resource leaks.

But if a test truly fails, this is just confusing, because its usually not going to make it to the part of its code where it would call .close()

So, if any tests fail, I think we should omit this check in afterClass()"
0,"upgrade contrib/ant's tidy.jar. contrib/ant uses a Tidy.jar that also includes classes in org.w3c.dom, org.xml.sax, etc.

This is no problem if you are an ant user, but if you are an IDE user you need to carefully configure the order of your classpath or things will not compile, as these will override the ones in the Solr libs, for example.

The solution is to upgrade the tidy.jar to the newest one that only includes org.w3c.tidy and doesn't cause these problems."
0,"JavaCC grammar generation to ${maven.build.dir}/generated-src. Currently the JavaCC grammars in src/grammar/{xpath,sql} are processed into Java source files in src/java/org/apache/jacrabbit/core/query/{xpath,sql} where we also have normal version controlled source files. This leads to the need to maintain special svn:ignore properties and also the more general issue of mixing manually written and automatically generated source files. Because of this the ""maven clean"" command does not (at the moment) truly restore your source tree to a ""fresh checkout"" state.

I'm proposing (as a wish, you are free to disagree) that the JavaCC grammars be generated into Java files within the Maven build directory. The attached patch modifies the javacc maven goals to generate files into ${maven.build.dir}/generated-src. The modified prepare-filesystem goal also adds the generated source path ${maven.build.dir}/generated-src/main/java into the maven compile set so that the generated sources are included in the normal builds.

PS. There are a couple of JavaCC generated files that have been intentionally modified for Jackrabbit. The ant:delete commands at the end of the jacrabbit:generate-*-parser goals specifically remove these generated files. It would however be nicer if custom modifications would not be needed."
0,"move class Scheme and friends to a separate package. We currently have a recursive dependency between packages o.a.h.conn and o.a.h.conn.routing, because routing depends on Scheme/SchemeRegistry. While these classes are used throughout the connection management code, they are not really part of the connection management itself and would therefore fit nicely into a separate package.

preliminary list of classes and interfaces to move:
- Scheme
- SchemeRegistry
- SocketFactory
- LayeredSocketFactory
- PlainSocketFactory

suggested package name: o.a.h.conn.scheme

Package o.a.h.conn.ssl can stay where it is, it only changes its dependency from conn to the new package.

cheers,
  Roland
"
0,"Benchmarks Enhancements (precision/recall, TREC, Wikipedia). Would be great if the benchmark contrib had a way of providing precision/recall benchmark information ala TREC.  I don't know what the copyright issues are for the TREC queries/data (I think the queries are available, but not sure about the data), so not sure if the is even feasible, but I could imagine we could at least incorporate support for it for those who have access to the data.  It has been a long time since I have participated in TREC, so perhaps someone more familiar w/ the latest can fill in the blanks here.

Another option is to ask for volunteers to create queries and make judgments for the Reuters data, but that is a bit more complex and probably not necessary.  Even so, an Apache licensed set of benchmarks may be useful for the community as a whole.  Hmmm.... 

Wikipedia might be another option instead of Reuters to setup as a download for benchmarking, as it is quite large and I believe the licensing terms are quite amenable.  Having a larger collection would be good for stressing Lucene more and would give many users a demonstration of how Lucene handles large collections.

At any rate, this kind of information could be useful for people looking at different indexing schemes, formats, payloads and different query strategies.

"
0,"Wrong trailing index calculation in PatternReplaceCharFilter. Reimplementation of PatternReplaceCharFilter to pass randomized tests (used to throw exceptions previously). Simplified code, dropped boundary characters, full input buffered for pattern matching."
0,"Build failed in the flexscoring branch because of Javadoc warnings. Ant build log:
  [javadoc] Standard Doclet version 1.6.0_24
  [javadoc] Building tree for all the packages and classes...
  [javadoc] /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/src/java/org/apache/lucene/search/Similarity.java:93: warning - Tag @link: can't find tf(float) in org.apache.lucene.search.Similarity
  [javadoc] /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/src/java/org/apache/lucene/search/TFIDFSimilarity.java:588: warning - @param argument ""term"" is not a parameter name.
  [javadoc] /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/src/java/org/apache/lucene/search/TFIDFSimilarity.java:588: warning - @param argument ""docFreq"" is not a parameter name.
  [javadoc] /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/src/java/org/apache/lucene/search/TFIDFSimilarity.java:618: warning - @param argument ""terms"" is not a parameter name.
  [javadoc] Generating /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/org/apache/lucene/store/instantiated//package-summary.html...
  [javadoc] Copying file /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/doc-files/classdiagram.png to directory /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/org/apache/lucene/store/instantiated/doc-files...
  [javadoc] Copying file /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/doc-files/HitCollectionBench.jpg to directory /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/org/apache/lucene/store/instantiated/doc-files...
  [javadoc] Copying file /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/doc-files/classdiagram.uxf to directory /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/org/apache/lucene/store/instantiated/doc-files...
  [javadoc] Generating /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/serialized-form.html...
  [javadoc] Copying file /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/prettify/stylesheet+prettify.css to file /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/stylesheet+prettify.css...
  [javadoc] Building index for all the packages and classes...
  [javadoc] Building index for all classes...
  [javadoc] Generating /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/help-doc.html...
  [javadoc] 4 warnings
"
0,"TCK: SetPropertyValueTest#testCompactValueArrayWithNulls does not respect nodename1 and nodetype configuration properties. Test doesn't respect value of nodename1 and nodetype configuration properties.

Proposal: create property under testnode instead of testrootnode.

--- SetPropertyValueTest.java   (revision 422074)
+++ SetPropertyValueTest.java   (working copy)
@@ -374,11 +374,11 @@
      * the value array by removing all null values
      */
     public void testCompactValueArrayWithNulls() throws Exception {
-        testRootNode.setProperty(propertyName2, vArrayWithNulls);
+        testNode.setProperty(propertyName2, vArrayWithNulls);
         superuser.save();
         assertEquals(""Node.setProperty(String, valueArrayWithNulls[]) did not compact the value array by removing the null values"",
                 2,
-                testRootNode.getProperty(propertyName2).getValues().length);
+                testNode.getProperty(propertyName2).getValues().length);
     }
"
0,"ASCIIFoldingFilter: expose folding logic + small improvements to ISOLatin1AccentFilter. This patch adds a couple of non-ascii chars to ISOLatin1AccentFilter (namely: left & right single quotation marks, en dash, em dash) which we very frequently encounter in our projects. I know that this class is now deprecated; this improvement is for legacy code that hasn't migrated yet.

It also enables easy access to the ascii folding technique use in ASCIIFoldingFilter for potential re-use in non-Lucene-related code."
0,"Upgrade to Maven 2. If you are interested in migrating to maven2 (or adding optional maven 2 build scripts) this is a full maven 2 pom.xml for the main jackrabbit jar.

All the xpath/javacc stuff, previously done in maven.xml, was pretty painfull to reproduce in maven2... the attached pom exactly reproduces the m1 build by using the maven2 javacc plugin + a couple of antrun executions.
Test configuration is not yet complete, I think it will be a lot better to reproduce the previous behaviour (init tests run first) without any customization (maybe using a single junit test suite with setUp tasks). Also custom packaging goals added to maven.xml (that can be esily done in m2 by using the assembly plugin) are not yet reproduced too.

If there is interest, I can also provide poms for the contribution projects (that will be easy, the only complex pom is the main one).
"
0,"Remove ExtendedFieldCache by rolling functionality into FieldCache. It is silly that we have ExtendedFieldCache.  It is a workaround to our supposed back compatibility problem.  This patch will merge the ExtendedFieldCache interface into FieldCache, thereby breaking back compatibility, but creating a much simpler API for FieldCache."
0,"Override method MultipartEntity.addPart so that applications may use FormBodyPart. FormBodyPart is similar to Part in HttpClient 3.x in that it couples the form name with the value.  Some applications may find this useful, but cannot really utilize these objects since there is only MultipartEntity.addPart(String name,ContentBody) and FormBodyPart does not have a getContent method:

  entity.addPart(part.getName(), part.getContent()); // Almost but there is no getContent method

How about overriding addPart to take a FormBodyPart object:

  entity.addPart(part);"
0,"potential memory leak when using ThreadSafeClientConnManager. When using ThreadSafeClientConnManager and developing with Jetty using auto-redeploy feature eventually I run into a PermGen out of memory exception.  I investigated with YourKit 8.0.6 and found a class loader circular reference in RefQueueWorker.  Not really sure what I was doing I made the refQueueHandler non-final and nulled it in the shutdown method of RedQueueWorker.  I don't seem to have the problem any longer with circular class loader references.

Here is a diff from 4.0-beta2


--- httpclient/src/main/java/org/apache/http/impl/conn/tsccm/RefQueueWorker.jav(revision 763223)
+++ httpclient/src/main/java/org/apache/http/impl/conn/tsccm/RefQueueWorker.jav(working copy)
@@ -50,7 +50,7 @@
     protected final ReferenceQueue<?> refQueue;
 
     /** The handler for the references found. */
-    protected final RefQueueHandler refHandler;
+    protected RefQueueHandler refHandler;
 
 
     /**
@@ -112,6 +112,8 @@
             this.workerThread = null; // indicate shutdown
             wt.interrupt();
         }
+
+        refHandler = null;
     }
 
 
"
0,"WebDAV BIND support. I'm tempted to work on implementing the WebDAV BIND protocol, as currently defined in http://greenbytes.de/tech/webdav/draft-ietf-webdav-bind-20.html.

This issue can be used to collect design proposals and track progress.

1) DAV:resource-id live property

This can be implemented in terms of the JCR UUID. However, we need to turn this one into a URI for WebDAV. If the JCR UUID happens to *really* use UUID syntax, we *could* use urn:uuid. Otherwise, it would probably useful to mint an HTTP URI, served by the WebDAV servlet. (note that the latter has the disadvantage that moving a node to a different server will affect its resource-id, in case that other server allows importing UUIDs).

2) REBIND and UNBIND methods

Same as MOVE and DELETE, with the excpetion of marshalling.

3) DAV:parent-set property

Either trivial (when node isn't shared), or needs to use the JCR 2.0 shared set functionality.

4) BIND method

Either trivial (when shareable nodes aren't supported), or needs to use the JCR 2.0 shared set functionality.

5) Cycle detection in depth:infinity requests

TBD :-)
"
0,"Ensure queries are not blocked during large updates. The index currently guarantees that long running queries do not block updates. In addition a query *may* run during an update, but there is not guarantee because it depends on the availability of an index reader being available when the update starts. The index reader is invalidated at the end of the update, which will force the creation of a new index reader when the next query is executed.

Consider the following scenario:

1) update index -> transaction id T1
2) potential index reader is invalidated
3) execute query -> creates index reader R1, which includes changes up to T1
4) update index -> transaction id T2
5) index reader R1 is invalidated
6) update index (large transaction) -> transaction id T3
7) while previous update is running execute query -> thread is blocked because no reader is available


The improvement should detect the large transaction and prepare an index reader for potential queries during the update. That is, 6) should be split into:

6a) detect large transaction and prepare index reader R2, which includes changes up to T2
6b) update index -> transaction id T3

While the update is running a query will use index reader R2."
0,Add the new DataSource element to the repository DTD. The connection pooling feature from JCR-1456 introduced a new DataSource configuration element to Jackrabbit. It should be added to the repository config DTD.
0,ItemSaveOperation should not swallow stacktrace. When a StaleItemStateException is thrown the stacktrace is swallowed. This makes it much harder to figure out what went wrong from the logs.
0,"customize handling of 302 redirects. I tried this with both the beta2 2.0 release, and the nightly build.  The
following code snippet describes what I am trying to do:

httpClient.getHostConfiguration().setHost(sHost, 80, ""http"");
HttpMethod method=null;
if (sMethod.indexOf(""POST"")!=-1) {
     method=new PostMethod(sURLInfo);
} else {
     method=new GetMethod(sURLInfo);
}
method.setFollowRedirects(true);
httpClient.executeMethod(method);

After this code executes, the ""getFollowRedirects"" method still returns false,
and any redirects which are sent by the webserver are not followed.  As a
temporary workaround, since I want all redirects followed, I commented out the
following code in the HttpMethodBase class in the ""processRedirectResponse"" method:

/*if (!getFollowRedirects()) {
     LOG.info(""Redirect requested but followRedirects is ""
     + ""disabled"");
     return false;
}*/

If this bug has already been reported, I apologize...I searched for and found
nothing related to this issue."
0,possible SynonymFilter bug: hudson fail. See https://builds.apache.org/job/Lucene-trunk/1867/consoleText (no seed)
0,"Logging documentation refers to nonexisting level ERROR in java.util.logging section. Page ""Logging Practices""  Section ""java.util.logging Examples"" 
(http://hc.apache.org/httpcomponents-client/logging.html) 
says:
""org.apache.http.level = FINEST
org.apache.http.wire.level = ERROR""

However, there is no such thing as java.util.logging.Level.ERROR

Did you mean SEVERE as in
Logger.getLogger(""org.apache.http.wire"").setLevel(Level.SEVERE);

Thanks"
0,"JCA project tests assume Windows paths. The org.apache.jackrabbit.jca package in the top-level jca directory has unit tests that assume a Windows environment. It should be fixed to work in any environment. The best solution may be to use a test repository configuration file in the current directory.

The following is the start of the test case failures that I got running on MacOS X.

Testsuite: org.apache.jackrabbit.jca.test.ConnectionFactoryTest
Tests run: 3, Failures: 0, Errors: 3, Time elapsed: 0.778 sec

Testcase: testAllocation(org.apache.jackrabbit.jca.test.ConnectionFactoryTest): Caused an ERROR
org.apache.jackrabbit.core.config.ConfigurationException: Configuration file could not be read.: /Users/mark/Documents/school/whisper/jackrabbit/jackrabbit-trunk/jca/c:\dev\jcr\repository.xml (N
o such file or directory): /Users/mark/Documents/school/whisper/jackrabbit/jackrabbit-trunk/jca/c:\dev\jcr\repository.xml (No such file or directory)
org.apache.jackrabbit.core.config.ConfigurationException: Configuration file could not be read.: /Users/mark/Documents/school/whisper/jackrabbit/jackrabbit-trunk/jca/c:\dev\jcr\repository.xml (N
o such file or directory): /Users/mark/Documents/school/whisper/jackrabbit/jackrabbit-trunk/jca/c:\dev\jcr\repository.xml (No such file or directory)
        at org.apache.jackrabbit.jca.JCAManagedConnectionFactory.createRepository(JCAManagedConnectionFactory.java:278)
        at org.apache.jackrabbit.jca.JCAManagedConnectionFactory.createConnectionFactory(JCAManagedConnectionFactory.java:116)
        at org.apache.jackrabbit.jca.JCAManagedConnectionFactory.createConnectionFactory(JCAManagedConnectionFactory.java:108)
        at org.apache.jackrabbit.jca.test.ConnectionFactoryTest.testAllocation(ConnectionFactoryTest.java:43)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
"
0,"optimizer for n-gram PhraseQuery. If 2-gram is used and the length of query string is 4, for example q=""ABCD"", QueryParser generates (when autoGeneratePhraseQueries is true) PhraseQuery(""AB BC CD"") with slop 0. But it can be optimized PhraseQuery(""AB CD"") with appropriate positions.

The idea came from the Japanese paper ""N.M-gram: Implementation of Inverted Index Using N-gram with Hash Values"" by Mikio Hirabayashi, et al. (The main theme of the paper is different from the idea that I'm using here, though)"
0,jcr:like on node name. Until now it is only possible to do an exact match on the node name. It would be useful to also use jcr:like on the node name.
0,"CartesianTierPlotter fieldPrefix should be configurable. CartesianTierPlotter field prefix is currrently hardcoded to ""_localTier"" -- this should be configurable"
0,"repository.xml DTD doesn't allow <DataStore> element. The repository.xml DTD at http://jackrabbit.apache.org/dtd/repository-1.4.dtd conflicts with the instructions in the wiki page at http://wiki.apache.org/jackrabbit/DataStore

Adding the <DataStore> element as specified in the wiki page violates the DTD.

"
0,"fix getting started / demo docs. Opening a new issue for this since there are a number of problems...:

  * We should get the versions right, eg when we explain how to do a src checkout it should point to the path for that release

  * Source checkout / build JARs instructions must be updated for the merger

  * Analyzers JAR must be on the classpath too

  * Demo sources are no longer shipped in a binary release

  * Fixup from LUCENE-2923 (remove web app, new command-line ops for IndexFiles, etc.)"
0,refactor consistency checks in BundleDBPersistenceManager into a standalone class that could be re-used for other PMs. see subject
0,"CustomScoreQuery (function query) is broken (due to per-segment searching). Spinoff from here:

  http://lucene.markmail.org/message/psw2m3adzibaixbq

With the cutover to per-segment searching, CustomScoreQuery is not really usable anymore, because the per-doc custom scoring method (customScore) receives a per-segment docID, yet there is no way to figure out which segment you are currently searching.

I think to fix this we must also notify the subclass whenever a new segment is switched to.  I think if we copy Collector.setNextReader, that would be sufficient.  It would by default do nothing in CustomScoreQuery, but a subclass could override."
0,"SessionImpl#getSubject() should return an unmodifiable subject. for security reasons the subject exposed by SessionImpl#getSubject() should be unmodifiable or at least changes made
to it should not be modify the subject hold by the session.

currently i see the following options to get there:
a: set readonly flag on the subject associated with the session
b: getSubject() returns a new instance of Subject having the same characteristics as the subject associated with the session
c: getSubject() returns a new but readonly Subject instance

my preferred solution was c as
- it doesn't change the characteristics of the subject
- the unmodifiable status is transparent to the caller since modifying the subject fails without forcing the api consumer
  to read the javadoc to know why changing the subject is not reflected on the session itself (that would be a drawback of b)."
0,ConsistencyCheck uses too much memory. A consistency check loads all lucene documents into memory. On a large repository this may lead to an OutOfMemoryError. The consistency check should rather read the lucene documents on demand and discard them afterwards.
0,"NetscapeDraftSpec is too strict about cookie expires date format. The Netscape Draft specification (http://curl.haxx.se/rfc/cookie_spec.html) specifies clearly that the date format for Set-Cookie expires is ""Wdy, DD-Mon-YYYY HH:MM:SS GMT"". But on the other hand, in the examples section of the same document, the only example header that contains ""Expires"" is the following:

Set-Cookie: CUSTOMER=WILE_E_COYOTE; path=/; expires=Wednesday, 09-Nov-99 23:12:40 GMT

Note that the weekday is fully spelled out and that the year is written as two digits only. I would say that the specification therefore makes the 2 or 4 digit year optional. I think NetscapeDraftSpec should reflect this. An example of a product that uses the 2 digit version is jetty 6 and 7. When using httpclient 4 talking to a jetty server, any Set-Cookie headers for persistent cookies will be interpreted as a 4 digit year in the date and the cookie will immediately be disregarded as expired by some 2,000 years or so. Httpclient 3 on the other hand had no problem understanding the persistent cookies from jetty. I filed a bug report https://bugs.eclipse.org/bugs/show_bug.cgi?id=304698 on jetty to change their date format, but on the other hand I also think httpclient 4 is too strict about the date format when even the original specification uses two alternatives.

Workaround is easy by setting CookieSpecPNames.DATE_PATTERNS, but I really think that projects like jetty and httpclient should be compatible by default. Also, since the date format used by jetty is parsable but misinterpreted and disregarded by httpclient makes it especially hard to detect the first time on encounters the problem."
0,"Changes.html not explicitly included in release. None of the release related ant targets explicitly call cahnges-to-html ... this seems like an oversight.  (currently it's only called as part of the nightly target)

"
0,Remove unnecessary memory barriers in DWPT. Currently DWPT still uses AtomicLong to count the bytesUsed. Each write access issues an implicite memory barrier which is totally unnecessary since we doing everything single threaded on that level. This might be very minor but we shouldn't issue unnecessary memory barriers causing processors to lock their instruction pipeline for no reason.
0,Wrong link in javadoc of QNodeTypeDefinition. The javadoc of QNodeTypeDefinition links to javax.jcr.nodetype.NodeDefinition instead of javax.jcr.nodetype.NodeType
0,"document LengthFilter wrt Unicode 4.0. LengthFilter calculates its min/max length from TermAttribute.termLength()
This is not characters, but instead UTF-16 code units.

In my opinion this should not be changed, merely documented.
If we changed it, it would have an adverse performance impact because we would have to actually calculate Character.codePointCount() on the text.

If you feel strongly otherwise, fixing it to count codepoints would be a trivial patch, but I'd rather not hurt performance.
I admit I don't fully understand all the use cases for this filter.
"
0,"CompoundFileReader's openInput produces streams that may do an extra buffer copy. Spinoff of LUCENE-888.

The class for reading from a compound file (CompoundFileReader) has a
primary stream which is a BufferedIndexInput when that stream is from
an FSDirectory (which is the norm).  That is one layer of buffering.

Then, when its openInput is called, a CSIndexInput is created which
also subclasses from BufferedIndexInput.  That's a second layer of
buffering.

When a consumer actually uses that CSIndexInput to read, and a call to
readByte or readBytes runs out of what's in the first buffer, it will
go to refill its buffer.  But that refill calls the first
BufferedIndexInput which in turn may refill its buffer (a double
copy) by reading the underlying stream.

Not sure how to fix it yet but we should change things to not do the
extra buffer copy.
"
0,"Add InputStream buffering.. Currently HttpClient does not buffer the InputStream received from the socket. 
Perhaps doing so would improve performance.

Reported by Tony Bigbee."
0,"Support easy pre-authenticated login. Some applications authenticate users themselves and just need to access the repository on behalf of these pre-authenticated users.

Examples of such pre-authentications include SSO solutions or web applications using a web-based authentication protocol not easily implementable in a JAAS LoginModule, for example OpenID or similar.

In such situations a password may not be provided in SimpleCredentials and thus regular login with user name and password is not possible.

Therefore I propose the enhancement of the AbstractLoginModule to allow for setting a specific attribute in the SimpleCredentials attribute map. If this attribute is set, authentication and login succeeds and a session for the user named in the SimpleCredentials is created.

As a starter we might just check for the presence of the attribute."
0,"improve test coverage of multi-segment indices. Simple patch that adds a test-only helper class, RandomIndexWriter, that lets you add docs, but it will randomly do things like use a different merge policy/scheduler, flush by doc count instead of RAM, flush randomly (so we get multi-segment indices) but also randomly optimize in the end (so we also sometimes test single segment indices)."
0,"AbstractHttpClient.addRequestInterceptor should document in what order the interceptors run. http://hc.apache.org/httpcomponents-client/httpclient/apidocs/org/apache/http/impl/client/AbstractHttpClient.html#addRequestInterceptor(org.apache.http.HttpRequestInterceptor) has no documentation. It should at least say what order new interceptors run in. Presumably they run in order by index, but does the lowest or highest index run first?

This class or DefaultHttpClient should also say what interceptors are added by default. That is, what would I be getting rid of by calling clearResponseInterceptors()?"
0,"Support contains queries with wildcard prefix. The current implementation only allows wildcards in the middle or at the end of a search term. 

The following queries work right now:
//*[jcr:contains(., 'foo*')
//*[jcr:contains(., 'fo?o')

But the following does not:
//*[jcr:contains(., '*bar')

There was already a thread in the mailing list on this topic:
http://thread.gmane.org/gmane.comp.apache.jackrabbit.devel/3304"
0,Allow for wildcard restriction in resource-based ACEs. 
0,"Optimize queries that check for the existence of a property. //*[@mytext] is transformed into the org.apache.jackrabbit.core.query.lucene.MatchAllQuery, that through the MatchAllWeight uses the MatchAllScorer.  The calculateDocFilter() in MatchAllScorer  does not scale and becomes slow for growing number of nodes. 

Solution: lucene documents will get a new Field:

public static final String PROPERTIES_SET = ""_:PROPERTIES_SET"".intern();

that holds the available properties of this document. 

NOTE: Lucene indices build without this performance improvement should still work and fall back to the original implementation"
0,"Disable Users. add ""disable user"" functionality to prevent an existing user from login into the repository."
0,Remaining reallocation should use ArrayUtil.getNextSize(). See recent discussion on ArrayUtils.getNextSize().
0,"Make collecting group membership information lazy. JCR-2710 added a more scalable content model for storing group membership information. To further leverage the new model it would be preferable when group membership collecting where lazy. (i.e. Group#getDeclaredMembers() and Group#getMembers() should not construct the list of all members up front). 
"
0,"Do MultiTermQuery boolean rewrites per segment. MultiTermQuery currently rewrites FuzzyQuery (using TopTermsBooleanQueryRewrite), the auto constant rewrite method and the ScoringBQ rewrite methods using a MultiFields wrapper on the top-level reader. This is inefficient.

This patch changes the rewrite modes to do the rewrites per segment and uses some additional datastructures (hashed sets/maps) to exclude duplicate terms. All tests currently pass, but FuzzyQuery's tests should not, because it depends for the minimum score handling, that the terms are collected in order..

Robert will fix FuzzyQuery in this issue, too. This patch is just a start."
0,"Add top-down version of BlockJoinQuery. Today, BlockJoinQuery can join from child docIDs up to parent docIDs.
EG this works well for product (parent) + many SKUs (child) search.

But the reverse, which BJQ cannot do, is also useful in some cases.
EG say you index songs (child) within albums (parent), but you want to
search and present by song not album while involving some fields from
the album in the query.  In this case you want to wrap a parent query
(against album), joining down to the child document space.
"
0,IndexReader.open should take Codecs. Need to make this public... it's private now.
0,"CLONE -Handling of multiple residual prop defs in EffectiveNodeTypeImpl. org.apache.jackrabbit.jcr2spi.nodetype.EffectiveNodeTypeImpl currently rejects multiple residual property definitions, if they do not differ in getMultiple(). In fact, it should accept all combinations, so differing values for getOnParentVersionAction and other aspects should be accepted as well.

See JSR 170, 6.7.8:

""For purposes of the above, the notion of two definitions having the same name does not apply to two residual definitions. Two (or more) residual property or child node definitions with differing subattributes must be permitted to co-exist in the same effective node type. They are interpreted as disjunctive (ORed) options."""
0,"DatabaseJournal needs connection reestablishment logic. The DB based file system and persistence manager implementations have logic for connection reestablishment in case the DB server bounces while the repository is running, but the DB based journal implementation doesn't."
0,"Generalize directory copy operation. The copy operation in RAMDirectory(Directory) constructor can be used more generally to copy one directory to another. Why bound it only to RAMDirectory?. For example, I build index in RAMDirectory but I need it to persist in FSDirectory. I created a patch to solve it."
0,"Allow usage of HyphenationCompoundWordTokenFilter without dictionary. We should allow to use the HyphenationCompoundWordTokenFilter without a dictionary. This produces a lot of ""nonword"" tokens but might be useful sometimes."
0,"WebDAV: drop dependency on commons-collections. the webdav library brings in a dependency on commons-collections solely for one reference to LinkedMap. Since none of the additional features of this class are used, and as I understand it Jackrabbit requires JDK 1.4+, this can be replaced with LinkedHashMap.

jcr-commons still brings in the commons-collections dependency, but I believe it can be safely excluded by users that don't need to use the predicates (Which is true of the webdav client)"
0,"cache revalidation of variants does not update original variant entry. When the cache stories multiple variant entries due to Vary headers in responses, the cache correctly sends a conditional request containing the etags of any existing variants on a ""variant miss"" (incoming request does not match the request variants already cached). In addition, when it receives a 304 response, it correctly returns the indicated variant to the request that causes the variant miss. However, it does not update the pre-existing variant cache entry as recommended by RFC 2616.

For example:

request 1, User-Agent: agent1 results in a 200 OK with Etag: etag1 and Vary: User-Agent.
request 2, User-Agent: agent2 causes an If-None-Match to the origin; if it returns 304 Not Modified with Etag: etag1
request 3, User-Agent: agent1 results in a 200 OK but gets the (outdated) entry that resulted from request 1

in other words, the origin response from request 2 does not update the variant for ""agent1"".

This does not cause incorrect behavior (this is a SHOULD) but does miss out on some caching opportunities here.
"
0,"[PATCH] setIndexInterval() in IndexWriter. Following a discussion with Doug (see
http://article.gmane.org/gmane.comp.jakarta.lucene.devel/5804) here is a patch
that add a setIndexInterval() in IndexWriter.

This patch adds also a getDirectory method to IndexWriter and modifies 
SegmentMerger, IndexWriter and TermInfosWriter.

This patch passes all tests.

Any comments/criticisms welcome.

Julien"
0,"Unreachable catch block for NameException in ValueConstraint.java. Unreachable catch block for NameException. Only more specific exceptions are thrown and handled by previous catch block(s). ValueConstraint.java	line 855
"
0,"cache should use both Last-Modified and ETag for validations when available. This is a protocol recommendation:

""[HTTP/1.1 clients], if both an entity tag and a Last-Modified value have been provided by the origin server, SHOULD use both validators in cache-conditional requests. This allows both HTTP/1.0 and HTTP/1.1 caches to respond appropriately.""

http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.3.4

The current implementation only uses the ETag when conditionally validating an entry, so HTTP/1.0 caches can't currently reply to us with a 304 (Not Modified), even if that would be appropriate.
"
0,"[PATCH] to store binary fields with compression. hi all,

as promised here is the enhancement for the binary field patch with optional
compression. The attachment includes all necessary diffs based on the latest
version from CVS. There is also a small junit test case to test the core
functionality for binary field compression. The base implementation for binary
fields where this patch relies on, can be found in patch #29370. The existing
unit tests pass fine.

For testing binary fields and compression, I'm creating an index from 2700 plain
text files (avg. 6kb per file) and store all file content within that index
without using compression. The test was created using the IndexFiles class from
the demo distribution. Setting up the index and storing all content without
compression took about 60 secs and the final index size was 21 MB. Running the
same test, switching compression on, the time to index increase to 75 secs, but
the final index size shrinks to 13 MB. This is less than the plain text files
them self need in the file system (15 MB)

Hopefully this patch helps people dealing with huge index and want to store more
than just 300 bytes per document to display a well formed summary.

regards
Bernhard"
0,"SegmentInfos shouldn't blindly increment version on commit. SegmentInfos currently increments version on the assumption that there are always changes.

But, both DirReader and IW are more careful about tracking whether there are changes.  DirReader has hasChanges and IW has changeCount.  I think these classes should notify the SIS when there are in fact changes; this will fix the case Simon hit on fixing LUCENE-2082 when the NRT reader thought there were changes, but in fact there weren't because IW simply committed the exact SIS it already had.
"
0,Make it possible to configure Lucene Analyzer for SearchIndex. Jackrabbit does not support the specification of a Lucene analyzer class (org.apache.lucene.analysis.Analyzer) to be used by a SearchIndex (or other Lucene based indices) declared in an XML configuration file. Custom analyzer is useful for indexing language specific content.
0,"jackrabbit-jcr-tests should still be based on Java 1.4. The JCR 2.0 TCK needs to be runnable on Java 1.4, so even though we've upgraded to Java 5 as the base platform for Jackrabbit 2.0, the jackrabbit-jcr-tests component needs to still be based on Java 1.4."
0,"ItemImpl#validateTransientItems: Incomplete validation of mandatory child item. ItemImpl#validateTransientItems iterates over all mandatory child node/property definitions in order to assert that those items have
been created. However, it only checks if an item with the name defined by the mandatory item definition is present and not if that
existing item really has the mandatory definition.

the example i had:
- mandatory single-value property.
- there is the possibility to add residual props
- added a residual property with the name of the mandatory prop but with multiple values

-> changes are saved without exception.
-> the node doesn't have a property with the mandatory definition.

((without having tried it out, i think the same would be possible with child nodes))

suggested fix:
if there is a child item with the mandatory-item-name -> make sure it's definition is mandatory (or the expected one...)
patch will follow.
"
0,"java.util.logging configuration examples does not work as intended. java.util.logging configuration examples do not work as intended. Those can be found here: http://jakarta.apache.org/httpcomponents/httpclient-3.x/logging.html

Steps to reproduce:
1. Create a simple project using HttpClient (see listing below) and JDK 1.6 (I suppose it is JDK 1.4 or higher, but I did not test anything other than 1.6). Without log4j in the classpath and without any commons-logging system properties set, java.util.logging is automatically selected by commons-logging.
2. Create logging.properties file as shown in any of the java.util.logging examples
3. Run a program, passing -Djava.util.logging.config.file=logging.properties argument to the JVM

Expected results:
Quite a few log messages should be sent to System.err

Actual results:
Unless there is an I/O error encountered, no log messages are sent to System.err

The problem, as far as I can see, is caused by the default logging level of java.util.logging.ConsoleHandler, which is set to INFO. In order for any log messages to go through, the log hadler log level needs to be lower than logged messages log level. Adding the following line to all java.util.logging examples should fix the problem:

java.util.logging.ConsoleHandler.level = ALL

--- Get.java -----------------------------------------

import java.io.IOException;
import java.io.InputStreamReader;
import java.io.Reader;

import org.apache.commons.httpclient.HostConfiguration;
import org.apache.commons.httpclient.HttpClient;
import org.apache.commons.httpclient.HttpException;
import org.apache.commons.httpclient.HttpMethodBase;
import org.apache.commons.httpclient.methods.GetMethod;


public class Get {

	/**
	 * @param args
	 */
	public static void main(String[] args) {
		
 		HttpClient client = new HttpClient();
		HttpMethodBase get = new GetMethod(""http://www.apache.org"");
		
		try {
			int code = client.executeMethod(get);
			System.out.println(""Status code: "" + code);
			
			String csn = get.getResponseCharSet();
			System.out.println(""Charset is: "" + csn);
			
			long len = get.getResponseContentLength();
			System.out.println(""Length is: "" + len);
			len = len < 0 ? 200 : len;
			
			StringBuilder buf = new StringBuilder((int)len);
			Reader r = new InputStreamReader(get.getResponseBodyAsStream(), csn);
			for (int c = r.read(); c >= 0; c = r.read()) {
				buf.append((char)c);
			}
			
			System.out.println(""Body:"");
			System.out.println(buf.toString());			
			
		} catch (HttpException e) {
			e.printStackTrace();
		} catch (IOException e) {
			e.printStackTrace();
		} finally {
			get.releaseConnection();
		}				
	}
}

--- logging.properties ----- From examples ----------------------

.level=INFO

handlers=java.util.logging.ConsoleHandler
java.util.logging.ConsoleHandler.formatter = java.util.logging.SimpleFormatter

httpclient.wire.header.level=FINEST
org.apache.commons.httpclient.level=FINEST

--------------------------------------------------------------------------------
"
0,"CheckPermissionTest-testCheckPermission() doesn't allow config of node type to be created. CheckPermissionTest-testCheckPermission() doesn't allow configuration of node type to be created. Proposal to re-use the testNodeType config property.
"
0,spi2davex: reduce memory footprint of Node/PropertyInfoImpl. the in-memory footprint of o.a.jackrabbit.spi2davex.NodeinfoImpl & PropertyInfoImp is quite big. 
0,"Define and implement Logging policy. When to use info vs debug vs warning?  
When to log exceptions?
enter() and exit() messages on a per method basis?
Always log debug when swallowing an exception?"
0,separate java code from .jj file. It would make development easier to move most of the java code out from the .jj file and into a real java file.
0,augment logging information around CachingEntryCollector. add more logging information for the purpose of debugging CachingEntryCollector bottlenecks
0,"Header adding in org.apache.http.client.protocol.RequestAcceptEncoding should be conditional. org.apache.http.client.protocol.RequestAcceptEncoding adds a header in any case. Any chance to do it conditional (like in RequestClientConnControl)? The code would be something like
if (!request.containsHeader(""Accept-Encoding"")) {
    request.addHeader(""Accept-Encoding"", ""gzip,deflate"");
}

In our app this header may be added before request intercepting, so would be great if this fact is checked.
"
0,"jcr mapping layer (OCM) should expose lock owner. jcr mapping layer 's  persistencemanager.java  does not expose an API for returning lockowner. Ideally   , the following method 

public String lock(final String absPath, final boolean isDeep, final boolean isSessionScoped) 

should return a hashmap/String array containing locktoken as well as lockowner. 

I tried having lockowner as a field in my java object and mapping it to jcr:lockOwner , so that I can just use getLockOwner() . But the problem is this property gets introduced in the node only if  the node is locked. So, when I try to insert a node , before I can even lock it , the insertion fails since there is no property like jcr:lockOwner   till then . 

So, I feel there is need for the above API. It is ok to have it exposed via separate call in order to maintain backward compatability
"
0,"Litmus prophighunicode test failure on JRE 1.5. The WebDAV Litmus test suite contains a test case for writing and reading the Unicode character &#x10000; which can't be represented as a single 16-bit char in Java. Instead the character is stored as a surrogate pair of two 16-bit chars. Unfortunately the Xalan XML serializer used by Sun JRE 1.5 incorrectly encodes these as two separate characters in UTF-8, which leads to the following Litmus test failure:

-> running `props':
[...]
17. prophighunicode....... pass
18. propget............... FAIL (PROPFIND on `/default/litmus/prop2': XML parse error at line 1: not well-formed (invalid token))
"
0,"Minor performance improvement to IdleConnectionHandler. The attached patch does the following changes to IdleConnectionHandler
 - as it iterator over a map of connections, using a LinkedHashMap is a faster
 - rather than using an iterator over the keyset and subsequently getting the values, an iterator over the entry set is used instead for efficiency (at least according to FindBugs)

Note that the patch contains other changes to make variables final where possible. This was done automatically by Eclipse, and can be removed if desired. However I see no harm in them, other than they affect more of the code than intended by the patch."
0,"Preserve whitespace in <code> sections in the Changes.html generated from CHANGES.txt by changes2html.pl. The Trunk section of CHANGES.txt sports use of a new feature: <code> sections, for the two mentions of LUCENE-1575.

This looks fine in the text rendering, but looks crappy in the HTML version, since changes2html.pl escapes HTML metacharacters to appear as-is in the HTML rendering, but the newlines in the code are converted to a single space. 

I think this should be fixed by modifying changes2html.pl to convert <code> and </code> into (unescaped) <code><pre> and </pre></code>, respectively, since just passing through <code> and </code>, without </?pre>, while changing the font to monospaced (nice), still collapses whitespace (not nice). 

See the java-dev thread that spawned this issue here: http://www.nabble.com/CHANGES.txt-td23102627.html"
0,"Grouped total count. When grouping currently you can get two counts:
* Total hit count. Which counts all documents that matched the query.
* Total grouped hit count. Which counts all documents that have been grouped in the top N groups.

Since the end user gets groups in his search result instead of plain documents with grouping. The total number of groups as total count makes more sense in many situations. "
0,"Remove per-document multiply in FilteredQuery. Spinoff of LUCENE-1536.

In LUCENE-1536, Uwe suggested using FilteredQuery under-the-hood to implement filtered search.

But this query is inefficient, it does a per-document multiplication (wrapped.score() * boost()).

Instead, it should just pass the boost down in its weight, like BooleanQuery does to avoid this per-document multiply."
0,"[PATCH] Improved javadoc for maxClauseCount. As discussed on lucene-dev before, queries with lots of terms can use 
up a lot of unused buffer space for their TermDocs, because most terms 
have few documents."
0,extensibility patch for DavResourceImpl. attached is a very simple patch that allows subclasses of DavResourceImpl to access the Node represented by the dav resource.
0,Enable DataStore in default configuration. Currently the default configuration causes binary properties to be stored in the derby database. This is very inefficient. The standalone server (and the web application it contains) should run with a reasonable default configuration. 
0,"SnowballAnalyzer lacks a constructor that takes a Set of Stop Words. As discussed on the java-user list, the SnowballAnalyzer has been updated to use a Set of stop words. However, there is no constructor which accepts a Set, there's only the original String[] one

This is an issue, because most of the common sources of stop words (eg StopAnalyzer) have deprecated their String[] stop word lists, and moved over to Sets (eg StopAnalyzer.ENGLISH_STOP_WORDS_SET). So, for now, you either have to use a deprecated field on StopAnalyzer, or manually turn the Set into an array so you can pass it to the SnowballAnalyzer

I would suggest that a constructor is added to SnowballAnalyzer which accepts a Set. Not sure if the old String[] one should be deprecated or not.

A sample patch against 2.9.1 to add the constructor is:


--- SnowballAnalyzer.java.orig  2009-12-15 11:14:08.000000000 +0000
+++ SnowballAnalyzer.java       2009-12-14 12:58:37.000000000 +0000
@@ -67,6 +67,12 @@
     stopSet = StopFilter.makeStopSet(stopWords);
   }
 
+  /** Builds the named analyzer with the given stop words. */
+  public SnowballAnalyzer(Version matchVersion, String name, Set stopWordsSet) {
+    this(matchVersion, name);
+    stopSet = stopWordsSet;
+  }
+
"

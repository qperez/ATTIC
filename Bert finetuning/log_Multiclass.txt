Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Token indices sequence length is longer than the specified maximum sequence length for this model (1137 > 512). Running this sequence through the model will result in indexing errors
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Token indices sequence length is longer than the specified maximum sequence length for this model (2671 > 512). Running this sequence through the model will result in indexing errors
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Token indices sequence length is longer than the specified maximum sequence length for this model (2671 > 512). Running this sequence through the model will result in indexing errors
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
starting creation of datasets for cross validation
creating oversampled dataset of : (5591, 2)
starting creation of datasets for cross validation
creating dataset of : (5591, 2)
starting creation of datasets for cross validation
creating undersampled dataset of : (5591, 2)
starting fine tuning
cuda:0
STARTING CROSS VALIDATION FOR NotSampled DATASET

STARTING WITH FOLD NB 0

Epoch [1/5], Step [214/2140], Train Loss: 1.9594, Valid Loss: 1.8450
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_0/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_0/metrics/metrics.pth
Epoch [1/5], Step [428/2140], Train Loss: 1.6260, Valid Loss: 1.4558
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_0/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_0/metrics/metrics.pth
Epoch [2/5], Step [642/2140], Train Loss: 1.2130, Valid Loss: 1.1317
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_0/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_0/metrics/metrics.pth
Epoch [2/5], Step [856/2140], Train Loss: 1.0452, Valid Loss: 1.0305
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_0/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_0/metrics/metrics.pth
Epoch [3/5], Step [1070/2140], Train Loss: 0.7071, Valid Loss: 1.0666
Epoch [3/5], Step [1284/2140], Train Loss: 0.7846, Valid Loss: 1.1021
Epoch [4/5], Step [1498/2140], Train Loss: 0.4997, Valid Loss: 1.0152
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_0/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_0/metrics/metrics.pth
Epoch [4/5], Step [1712/2140], Train Loss: 0.5061, Valid Loss: 1.0651
Epoch [5/5], Step [1926/2140], Train Loss: 0.3032, Valid Loss: 1.1701
Epoch [5/5], Step [2140/2140], Train Loss: 0.3362, Valid Loss: 1.1878
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_0/metrics/metrics.pth
Finished Training!
Model loaded from <== Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_0/metrics/metrics.pth
Model loaded from <== Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_0/model/model.pth
Pred saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_0/evaluate/evaluate.pth
Classification Report :
              precision    recall  f1-score   support

           0     0.4848    0.6667    0.5614        24
           1     0.8075    0.8696    0.8374       299
           2     0.7280    0.6642    0.6947       137
           3     0.6167    0.6810    0.6472       163
           4     0.7292    0.7292    0.7292        48
           5     0.0000    0.0000    0.0000         3
           6     0.0000    0.0000    0.0000         6
           7     0.6970    0.8679    0.7731        53
           8     0.0000    0.0000    0.0000         1
           9     0.0000    0.0000    0.0000         9
          10     0.5667    0.5152    0.5397        33
          12     0.5714    0.1600    0.2500        25
          13     0.0000    0.0000    0.0000         4

   micro avg     0.7152    0.7205    0.7178       805
   macro avg     0.4001    0.3964    0.3871       805
weighted avg     0.6935    0.7205    0.7013       805

STARTING WITH FOLD NB 1

Epoch [1/5], Step [214/2140], Train Loss: 1.8919, Valid Loss: 1.6047
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_1/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_1/metrics/metrics.pth
Epoch [1/5], Step [428/2140], Train Loss: 1.5559, Valid Loss: 1.2895
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_1/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_1/metrics/metrics.pth
Epoch [2/5], Step [642/2140], Train Loss: 1.1707, Valid Loss: 1.0723
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_1/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_1/metrics/metrics.pth
Epoch [2/5], Step [856/2140], Train Loss: 1.0338, Valid Loss: 0.9851
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_1/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_1/metrics/metrics.pth
Epoch [3/5], Step [1070/2140], Train Loss: 0.7347, Valid Loss: 0.9881
Epoch [3/5], Step [1284/2140], Train Loss: 0.7583, Valid Loss: 0.9132
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_1/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_1/metrics/metrics.pth
Epoch [4/5], Step [1498/2140], Train Loss: 0.5153, Valid Loss: 0.9905
Epoch [4/5], Step [1712/2140], Train Loss: 0.4743, Valid Loss: 1.0964
Epoch [5/5], Step [1926/2140], Train Loss: 0.2953, Valid Loss: 1.1087
Epoch [5/5], Step [2140/2140], Train Loss: 0.3518, Valid Loss: 1.1376
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_1/metrics/metrics.pth
Finished Training!
Model loaded from <== Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_1/metrics/metrics.pth
Model loaded from <== Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_1/model/model.pth
Pred saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_1/evaluate/evaluate.pth
Classification Report :
              precision    recall  f1-score   support

           0     0.6500    0.5417    0.5909        24
           1     0.8037    0.8629    0.8323       299
           2     0.7818    0.6277    0.6964       137
           3     0.5646    0.7239    0.6344       163
           4     0.7273    0.8333    0.7767        48
           5     0.0000    0.0000    0.0000         3
           6     0.0000    0.0000    0.0000         6
           7     0.8125    0.7358    0.7723        53
           8     0.0000    0.0000    0.0000         1
           9     0.0000    0.0000    0.0000         9
          10     0.5000    0.6061    0.5479        33
          12     0.7500    0.1200    0.2069        25
          13     0.0000    0.0000    0.0000         4

   micro avg     0.7150    0.7168    0.7159       805
   macro avg     0.4300    0.3886    0.3891       805
weighted avg     0.7059    0.7168    0.6998       805

STARTING WITH FOLD NB 2

Epoch [1/5], Step [214/2140], Train Loss: 1.8057, Valid Loss: 1.4578
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_2/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_2/metrics/metrics.pth
Epoch [1/5], Step [428/2140], Train Loss: 1.4046, Valid Loss: 1.1702
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_2/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_2/metrics/metrics.pth
Epoch [2/5], Step [642/2140], Train Loss: 1.0486, Valid Loss: 1.0065
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_2/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_2/metrics/metrics.pth
Epoch [2/5], Step [856/2140], Train Loss: 0.9279, Valid Loss: 0.9034
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_2/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_2/metrics/metrics.pth
Epoch [3/5], Step [1070/2140], Train Loss: 0.6578, Valid Loss: 0.9191
Epoch [3/5], Step [1284/2140], Train Loss: 0.6675, Valid Loss: 0.9444
Epoch [4/5], Step [1498/2140], Train Loss: 0.4111, Valid Loss: 1.0279
Epoch [4/5], Step [1712/2140], Train Loss: 0.4482, Valid Loss: 1.0762
Epoch [5/5], Step [1926/2140], Train Loss: 0.2525, Valid Loss: 1.0805
Epoch [5/5], Step [2140/2140], Train Loss: 0.3038, Valid Loss: 1.1671
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_2/metrics/metrics.pth
Finished Training!
Model loaded from <== Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_2/metrics/metrics.pth
Model loaded from <== Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_2/model/model.pth
Pred saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_2/evaluate/evaluate.pth
Classification Report :
              precision    recall  f1-score   support

           0     0.5833    0.2917    0.3889        24
           1     0.7576    0.9197    0.8308       299
           2     0.6875    0.7226    0.7046       137
           3     0.6806    0.6012    0.6384       163
           4     0.6452    0.8333    0.7273        48
           5     0.0000    0.0000    0.0000         3
           6     0.0000    0.0000    0.0000         6
           7     0.8000    0.7547    0.7767        53
           8     0.0000    0.0000    0.0000         1
           9     0.0000    0.0000    0.0000         9
          10     0.5714    0.4848    0.5246        33
          12     0.0000    0.0000    0.0000        25
          13     0.0000    0.0000    0.0000         4

   micro avg     0.7161    0.7143    0.7152       805
   macro avg     0.3635    0.3545    0.3532       805
weighted avg     0.6681    0.7143    0.6854       805
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Token indices sequence length is longer than the specified maximum sequence length for this model (2671 > 512). Running this sequence through the model will result in indexing errors
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Token indices sequence length is longer than the specified maximum sequence length for this model (2671 > 512). Running this sequence through the model will result in indexing errors
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Token indices sequence length is longer than the specified maximum sequence length for this model (2671 > 512). Running this sequence through the model will result in indexing errors
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

STARTING WITH FOLD NB 3

Epoch [1/5], Step [214/2140], Train Loss: 1.7778, Valid Loss: 1.5284
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_3/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_3/metrics/metrics.pth
Epoch [1/5], Step [428/2140], Train Loss: 1.3442, Valid Loss: 1.2724
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_3/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_3/metrics/metrics.pth
Epoch [2/5], Step [642/2140], Train Loss: 1.0368, Valid Loss: 1.0739
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_3/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_3/metrics/metrics.pth
Epoch [2/5], Step [856/2140], Train Loss: 0.9011, Valid Loss: 1.0236
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_3/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_3/metrics/metrics.pth
Epoch [3/5], Step [1070/2140], Train Loss: 0.6138, Valid Loss: 1.0602
Epoch [3/5], Step [1284/2140], Train Loss: 0.6573, Valid Loss: 1.1030
Epoch [4/5], Step [1498/2140], Train Loss: 0.4005, Valid Loss: 1.1465
Epoch [4/5], Step [1712/2140], Train Loss: 0.4253, Valid Loss: 1.1242
Epoch [5/5], Step [1926/2140], Train Loss: 0.2457, Valid Loss: 1.2262
Epoch [5/5], Step [2140/2140], Train Loss: 0.2750, Valid Loss: 1.1857
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_3/metrics/metrics.pth
Finished Training!
Model loaded from <== Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_3/metrics/metrics.pth
Model loaded from <== Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_3/model/model.pth
Pred saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_3/evaluate/evaluate.pth
Classification Report :
              precision    recall  f1-score   support

           0     0.6000    0.3750    0.4615        24
           1     0.7804    0.8796    0.8270       299
           2     0.6443    0.7007    0.6713       137
           3     0.7355    0.5460    0.6268       163
           4     0.4778    0.8958    0.6232        48
           5     0.0000    0.0000    0.0000         3
           6     0.0000    0.0000    0.0000         6
           7     0.7302    0.8679    0.7931        53
           8     0.0000    0.0000    0.0000         1
           9     0.0000    0.0000    0.0000         9
          10     0.5484    0.5152    0.5312        33
          12     0.0000    0.0000    0.0000        25
          13     0.0000    0.0000    0.0000         4

   micro avg     0.6985    0.6994    0.6989       805
   macro avg     0.3474    0.3677    0.3488       805
weighted avg     0.6654    0.6994    0.6733       805

STARTING WITH FOLD NB 4

Epoch [1/5], Step [214/2140], Train Loss: 1.8526, Valid Loss: 1.5856
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_4/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_4/metrics/metrics.pth
Epoch [1/5], Step [428/2140], Train Loss: 1.4304, Valid Loss: 1.2304
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_4/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_4/metrics/metrics.pth
Epoch [2/5], Step [642/2140], Train Loss: 1.0382, Valid Loss: 1.0638
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_4/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_4/metrics/metrics.pth
Epoch [2/5], Step [856/2140], Train Loss: 0.9363, Valid Loss: 0.9999
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_4/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_4/metrics/metrics.pth
Epoch [3/5], Step [1070/2140], Train Loss: 0.6463, Valid Loss: 1.0107
Epoch [3/5], Step [1284/2140], Train Loss: 0.6702, Valid Loss: 1.0124
Epoch [4/5], Step [1498/2140], Train Loss: 0.4189, Valid Loss: 1.0660
Epoch [4/5], Step [1712/2140], Train Loss: 0.4233, Valid Loss: 1.0217
Epoch [5/5], Step [1926/2140], Train Loss: 0.2554, Valid Loss: 1.1623
Epoch [5/5], Step [2140/2140], Train Loss: 0.2939, Valid Loss: 1.1258
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_4/metrics/metrics.pth
Finished Training!
Model loaded from <== Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_4/metrics/metrics.pth
Model loaded from <== Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_4/model/model.pth
Pred saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_4/evaluate/evaluate.pth
Classification Report :
              precision    recall  f1-score   support

           0     0.9000    0.3750    0.5294        24
           1     0.7846    0.8528    0.8173       299
           2     0.7132    0.7080    0.7106       137
           3     0.6625    0.6503    0.6563       163
           4     0.6562    0.8750    0.7500        48
           5     0.0000    0.0000    0.0000         3
           6     0.0000    0.0000    0.0000         6
           7     0.5823    0.8679    0.6970        53
           8     0.0000    0.0000    0.0000         1
           9     0.0000    0.0000    0.0000         9
          10     0.6207    0.5455    0.5806        33
          12     0.0000    0.0000    0.0000        25
          13     0.0000    0.0000    0.0000         4

   micro avg     0.7136    0.7118    0.7127       805
   macro avg     0.3784    0.3750    0.3647       805
weighted avg     0.6767    0.7118    0.6876       805

STARTING WITH FOLD NB 5

Epoch [1/5], Step [214/2140], Train Loss: 1.8585, Valid Loss: 1.6667
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_5/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_5/metrics/metrics.pth
Epoch [1/5], Step [428/2140], Train Loss: 1.4785, Valid Loss: 1.3119
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_5/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_5/metrics/metrics.pth
Epoch [2/5], Step [642/2140], Train Loss: 1.0946, Valid Loss: 1.1103
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_5/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_5/metrics/metrics.pth
Epoch [2/5], Step [856/2140], Train Loss: 0.9549, Valid Loss: 1.0505
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_5/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_5/metrics/metrics.pth
Epoch [3/5], Step [1070/2140], Train Loss: 0.6725, Valid Loss: 1.0877
Epoch [3/5], Step [1284/2140], Train Loss: 0.6654, Valid Loss: 1.0834
Epoch [4/5], Step [1498/2140], Train Loss: 0.4408, Valid Loss: 1.1402
Epoch [4/5], Step [1712/2140], Train Loss: 0.4464, Valid Loss: 1.1579
Epoch [5/5], Step [1926/2140], Train Loss: 0.2703, Valid Loss: 1.2138
Epoch [5/5], Step [2140/2140], Train Loss: 0.2976, Valid Loss: 1.2700
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_5/metrics/metrics.pth
Finished Training!
Model loaded from <== Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_5/metrics/metrics.pth
Model loaded from <== Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_5/model/model.pth
Pred saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_5/evaluate/evaluate.pth
Classification Report :
              precision    recall  f1-score   support

           0     0.7500    0.2500    0.3750        24
           1     0.7957    0.8729    0.8325       299
           2     0.6906    0.7007    0.6957       137
           3     0.5952    0.6135    0.6042       163
           4     0.5753    0.8750    0.6942        48
           5     0.0000    0.0000    0.0000         3
           6     0.0000    0.0000    0.0000         6
           7     0.7377    0.8491    0.7895        53
           8     0.0000    0.0000    0.0000         1
           9     0.0000    0.0000    0.0000         9
          10     0.5600    0.4242    0.4828        33
          12     0.0000    0.0000    0.0000        25
          13     0.0000    0.0000    0.0000         4

   micro avg     0.7032    0.7006    0.7019       805
   macro avg     0.3619    0.3527    0.3441       805
weighted avg     0.6618    0.7006    0.6743       805
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Token indices sequence length is longer than the specified maximum sequence length for this model (2671 > 512). Running this sequence through the model will result in indexing errors
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Token indices sequence length is longer than the specified maximum sequence length for this model (2671 > 512). Running this sequence through the model will result in indexing errors
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Token indices sequence length is longer than the specified maximum sequence length for this model (2671 > 512). Running this sequence through the model will result in indexing errors
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

STARTING WITH FOLD NB 6

Epoch [1/5], Step [214/2140], Train Loss: 1.8397, Valid Loss: 1.4475
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_6/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_6/metrics/metrics.pth
Epoch [1/5], Step [428/2140], Train Loss: 1.3881, Valid Loss: 1.1410
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_6/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_6/metrics/metrics.pth
Epoch [2/5], Step [642/2140], Train Loss: 1.0643, Valid Loss: 1.0336
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_6/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_6/metrics/metrics.pth
Epoch [2/5], Step [856/2140], Train Loss: 0.9189, Valid Loss: 0.9669
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_6/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_6/metrics/metrics.pth
Epoch [3/5], Step [1070/2140], Train Loss: 0.6629, Valid Loss: 0.9642
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_6/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_6/metrics/metrics.pth
Epoch [3/5], Step [1284/2140], Train Loss: 0.6736, Valid Loss: 1.1338
Epoch [4/5], Step [1498/2140], Train Loss: 0.4073, Valid Loss: 1.1903
Epoch [4/5], Step [1712/2140], Train Loss: 0.4452, Valid Loss: 1.0893
Epoch [5/5], Step [1926/2140], Train Loss: 0.2590, Valid Loss: 1.0649
Epoch [5/5], Step [2140/2140], Train Loss: 0.2935, Valid Loss: 1.1110
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_6/metrics/metrics.pth
Finished Training!
Model loaded from <== Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_6/metrics/metrics.pth
Model loaded from <== Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_6/model/model.pth
Pred saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_6/evaluate/evaluate.pth
Classification Report :
              precision    recall  f1-score   support

           0     0.5769    0.6250    0.6000        24
           1     0.8161    0.8462    0.8309       299
           2     0.6258    0.7445    0.6800       137
           3     0.6884    0.5828    0.6312       163
           4     0.5972    0.8958    0.7167        48
           5     0.0000    0.0000    0.0000         3
           6     0.0000    0.0000    0.0000         6
           7     0.7636    0.7925    0.7778        53
           8     0.0000    0.0000    0.0000         1
           9     0.0000    0.0000    0.0000         9
          10     0.4889    0.6667    0.5641        33
          12     0.0000    0.0000    0.0000        25
          13     0.0000    0.0000    0.0000         4

   micro avg     0.7070    0.7106    0.7088       805
   macro avg     0.3505    0.3964    0.3693       805
weighted avg     0.6722    0.7106    0.6871       805

STARTING WITH FOLD NB 7

Epoch [1/5], Step [214/2140], Train Loss: 1.9077, Valid Loss: 1.6203
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_7/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_7/metrics/metrics.pth
Epoch [1/5], Step [428/2140], Train Loss: 1.4829, Valid Loss: 1.1810
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_7/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_7/metrics/metrics.pth
Epoch [2/5], Step [642/2140], Train Loss: 1.1238, Valid Loss: 1.0022
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_7/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_7/metrics/metrics.pth
Epoch [2/5], Step [856/2140], Train Loss: 0.9496, Valid Loss: 1.0386
Epoch [3/5], Step [1070/2140], Train Loss: 0.6970, Valid Loss: 1.0657
Epoch [3/5], Step [1284/2140], Train Loss: 0.7036, Valid Loss: 1.0148
Epoch [4/5], Step [1498/2140], Train Loss: 0.4464, Valid Loss: 0.9648
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_7/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_7/metrics/metrics.pth
Epoch [4/5], Step [1712/2140], Train Loss: 0.4498, Valid Loss: 1.0685
Epoch [5/5], Step [1926/2140], Train Loss: 0.2737, Valid Loss: 1.1531
Epoch [5/5], Step [2140/2140], Train Loss: 0.2956, Valid Loss: 1.1640
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_7/metrics/metrics.pth
Finished Training!
Model loaded from <== Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_7/metrics/metrics.pth
Model loaded from <== Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_7/model/model.pth
Pred saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_7/evaluate/evaluate.pth
Classification Report :
              precision    recall  f1-score   support

           0     0.6667    0.5833    0.6222        24
           1     0.8115    0.8495    0.8301       299
           2     0.7132    0.6715    0.6917       137
           3     0.6114    0.6564    0.6331       163
           4     0.7222    0.8125    0.7647        48
           5     0.0000    0.0000    0.0000         3
           6     0.0000    0.0000    0.0000         6
           7     0.6765    0.8679    0.7603        53
           8     0.0000    0.0000    0.0000         1
           9     0.0000    0.0000    0.0000         9
          10     0.5667    0.5152    0.5397        33
          12     0.6250    0.2000    0.3030        25
          13     0.0000    0.0000    0.0000         4

   micro avg     0.7193    0.7130    0.7162       805
   macro avg     0.4149    0.3966    0.3958       805
weighted avg     0.6967    0.7130    0.7000       805

STARTING WITH FOLD NB 8

Epoch [1/5], Step [214/2140], Train Loss: 1.8356, Valid Loss: 1.4851
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_8/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_8/metrics/metrics.pth
Epoch [1/5], Step [428/2140], Train Loss: 1.3873, Valid Loss: 1.1094
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_8/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_8/metrics/metrics.pth
Epoch [2/5], Step [642/2140], Train Loss: 1.0524, Valid Loss: 0.9848
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_8/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_8/metrics/metrics.pth
Epoch [2/5], Step [856/2140], Train Loss: 0.9452, Valid Loss: 0.9289
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_8/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_8/metrics/metrics.pth
Epoch [3/5], Step [1070/2140], Train Loss: 0.6582, Valid Loss: 1.0017
Epoch [3/5], Step [1284/2140], Train Loss: 0.6850, Valid Loss: 0.8849
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_8/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_8/metrics/metrics.pth
Epoch [4/5], Step [1498/2140], Train Loss: 0.4477, Valid Loss: 0.9236
Epoch [4/5], Step [1712/2140], Train Loss: 0.4381, Valid Loss: 1.0595
Epoch [5/5], Step [1926/2140], Train Loss: 0.2913, Valid Loss: 1.1548
Epoch [5/5], Step [2140/2140], Train Loss: 0.3323, Valid Loss: 1.0766
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_8/metrics/metrics.pth
Finished Training!
Model loaded from <== Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_8/metrics/metrics.pth
Model loaded from <== Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_8/model/model.pth
Pred saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_8/evaluate/evaluate.pth
Classification Report :
              precision    recall  f1-score   support

           0     0.5000    0.6250    0.5556        24
           1     0.8432    0.8094    0.8259       299
           2     0.6185    0.7810    0.6903       137
           3     0.6667    0.5644    0.6113       163
           4     0.7556    0.7083    0.7312        48
           5     0.0000    0.0000    0.0000         3
           6     0.0000    0.0000    0.0000         6
           7     0.6098    0.9434    0.7407        53
           8     0.0000    0.0000    0.0000         1
           9     0.0000    0.0000    0.0000         9
          10     0.6061    0.6061    0.6061        33
          12     0.4444    0.3200    0.3721        25
          13     0.0000    0.0000    0.0000         4

   micro avg     0.7047    0.7056    0.7052       805
   macro avg     0.3880    0.4121    0.3949       805
weighted avg     0.6922    0.7056    0.6934       805
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Token indices sequence length is longer than the specified maximum sequence length for this model (2671 > 512). Running this sequence through the model will result in indexing errors
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Token indices sequence length is longer than the specified maximum sequence length for this model (697 > 512). Running this sequence through the model will result in indexing errors
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

STARTING WITH FOLD NB 9

Epoch [1/5], Step [214/2140], Train Loss: 1.8800, Valid Loss: 1.7441
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_9/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_9/metrics/metrics.pth
Epoch [1/5], Step [428/2140], Train Loss: 1.4253, Valid Loss: 1.3289
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_9/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_9/metrics/metrics.pth
Epoch [2/5], Step [642/2140], Train Loss: 1.0891, Valid Loss: 1.1877
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_9/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_9/metrics/metrics.pth
Epoch [2/5], Step [856/2140], Train Loss: 0.9361, Valid Loss: 1.1219
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_9/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_9/metrics/metrics.pth
Epoch [3/5], Step [1070/2140], Train Loss: 0.6936, Valid Loss: 1.2488
Epoch [3/5], Step [1284/2140], Train Loss: 0.6650, Valid Loss: 1.1175
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_9/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_9/metrics/metrics.pth
Epoch [4/5], Step [1498/2140], Train Loss: 0.4608, Valid Loss: 1.1354
Epoch [4/5], Step [1712/2140], Train Loss: 0.4296, Valid Loss: 1.1785
Epoch [5/5], Step [1926/2140], Train Loss: 0.2762, Valid Loss: 1.1854
Epoch [5/5], Step [2140/2140], Train Loss: 0.2954, Valid Loss: 1.2872
Model saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_9/metrics/metrics.pth
Finished Training!
Model loaded from <== Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_9/metrics/metrics.pth
Model loaded from <== Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_9/model/model.pth
Pred saved to ==> Temp_Data_Files_Multiclass/NotSampled/Dataset_KFold_9/evaluate/evaluate.pth
Classification Report :
              precision    recall  f1-score   support

           0     0.6111    0.4583    0.5238        24
           1     0.8386    0.7993    0.8185       299
           2     0.6213    0.7664    0.6863       137
           3     0.6644    0.6074    0.6346       163
           4     0.7551    0.7708    0.7629        48
           5     0.0000    0.0000    0.0000         3
           6     0.0000    0.0000    0.0000         6
           7     0.5783    0.9057    0.7059        53
           8     0.0000    0.0000    0.0000         1
           9     0.0000    0.0000    0.0000         9
          10     0.4375    0.6364    0.5185        33
          12     1.0000    0.0400    0.0769        25
          13     0.0000    0.0000    0.0000         4

   micro avg     0.6995    0.6969    0.6982       805
   macro avg     0.4236    0.3834    0.3636       805
weighted avg     0.7021    0.6969    0.6805       805

STARTING CROSS VALIDATION FOR UnderSampled DATASET

STARTING WITH FOLD NB 0

Epoch [1/5], Step [2/25], Train Loss: 2.8719, Valid Loss: 2.6516
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_0/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_0/metrics/metrics.pth
Epoch [1/5], Step [4/25], Train Loss: 2.7334, Valid Loss: 2.6450
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_0/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_0/metrics/metrics.pth
Epoch [2/5], Step [6/25], Train Loss: 2.6133, Valid Loss: 2.6591
Epoch [2/5], Step [8/25], Train Loss: 2.6491, Valid Loss: 2.6499
Epoch [2/5], Step [10/25], Train Loss: 2.6568, Valid Loss: 2.6457
Epoch [3/5], Step [12/25], Train Loss: 2.5173, Valid Loss: 2.6466
Epoch [3/5], Step [14/25], Train Loss: 2.5885, Valid Loss: 2.6482
Epoch [4/5], Step [16/25], Train Loss: 2.6937, Valid Loss: 2.6477
Epoch [4/5], Step [18/25], Train Loss: 2.5933, Valid Loss: 2.6553
Epoch [4/5], Step [20/25], Train Loss: 2.4357, Valid Loss: 2.6573
Epoch [5/5], Step [22/25], Train Loss: 2.4543, Valid Loss: 2.6583
Epoch [5/5], Step [24/25], Train Loss: 2.4772, Valid Loss: 2.6516
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_0/metrics/metrics.pth
Finished Training!
Model loaded from <== Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_0/metrics/metrics.pth
Model loaded from <== Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_0/model/model.pth
Pred saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_0/evaluate/evaluate.pth
Classification Report :
              precision    recall  f1-score   support

           0     0.0000    0.0000    0.0000        24
           1     0.0000    0.0000    0.0000       299
           2     0.0000    0.0000    0.0000       137
           3     0.2013    0.5706    0.2976       163
           4     0.0000    0.0000    0.0000        48
           5     0.0000    0.0000    0.0000         3
           6     0.0000    0.0000    0.0000         6
           7     0.0000    0.0000    0.0000        53
           8     0.0000    0.0000    0.0000         1
           9     0.0000    0.0000    0.0000         9
          10     0.0000    0.0000    0.0000        33
          12     0.0000    0.0000    0.0000        25
          13     0.0028    0.2500    0.0055         4

   micro avg     0.1123    0.1168    0.1145       805
   macro avg     0.0157    0.0631    0.0233       805
weighted avg     0.0408    0.1168    0.0603       805

STARTING WITH FOLD NB 1

Epoch [1/5], Step [2/25], Train Loss: 2.5374, Valid Loss: 2.7420
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_1/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_1/metrics/metrics.pth
Epoch [1/5], Step [4/25], Train Loss: 2.7086, Valid Loss: 2.7480
Epoch [2/5], Step [6/25], Train Loss: 2.7538, Valid Loss: 2.7585
Epoch [2/5], Step [8/25], Train Loss: 2.5920, Valid Loss: 2.7774
Epoch [2/5], Step [10/25], Train Loss: 2.5948, Valid Loss: 2.7538
Epoch [3/5], Step [12/25], Train Loss: 2.6208, Valid Loss: 2.7182
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_1/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_1/metrics/metrics.pth
Epoch [3/5], Step [14/25], Train Loss: 2.6215, Valid Loss: 2.6778
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_1/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_1/metrics/metrics.pth
Epoch [4/5], Step [16/25], Train Loss: 2.6038, Valid Loss: 2.6891
Epoch [4/5], Step [18/25], Train Loss: 2.4767, Valid Loss: 2.7171
Epoch [4/5], Step [20/25], Train Loss: 2.5481, Valid Loss: 2.7310
Epoch [5/5], Step [22/25], Train Loss: 2.5076, Valid Loss: 2.7458
Epoch [5/5], Step [24/25], Train Loss: 2.5030, Valid Loss: 2.7502
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_1/metrics/metrics.pth
Finished Training!
Model loaded from <== Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_1/metrics/metrics.pth
Model loaded from <== Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_1/model/model.pth
Pred saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_1/evaluate/evaluate.pth
Classification Report :
              precision    recall  f1-score   support

           0     0.0000    0.0000    0.0000        24
           1     0.6333    0.0635    0.1155       299
           2     0.0000    0.0000    0.0000       137
           3     0.1861    0.3620    0.2458       163
           4     0.2500    0.0208    0.0385        48
           5     0.0000    0.0000    0.0000         3
           6     0.0131    0.6667    0.0257         6
           7     0.0706    0.2264    0.1076        53
           8     0.0000    0.0000    0.0000         1
           9     0.0000    0.0000    0.0000         9
          10     0.0000    0.0000    0.0000        33
          12     0.0000    0.0000    0.0000        25
          13     0.0000    0.0000    0.0000         4

   micro avg     0.1134    0.1180    0.1156       805
   macro avg     0.0887    0.1030    0.0410       805
weighted avg     0.2926    0.1180    0.1022       805
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Token indices sequence length is longer than the specified maximum sequence length for this model (3407 > 512). Running this sequence through the model will result in indexing errors
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Token indices sequence length is longer than the specified maximum sequence length for this model (929 > 512). Running this sequence through the model will result in indexing errors
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Token indices sequence length is longer than the specified maximum sequence length for this model (751 > 512). Running this sequence through the model will result in indexing errors
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Token indices sequence length is longer than the specified maximum sequence length for this model (654 > 512). Running this sequence through the model will result in indexing errors

STARTING WITH FOLD NB 2

Epoch [1/5], Step [2/25], Train Loss: 2.4788, Valid Loss: 2.5565
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_2/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_2/metrics/metrics.pth
Epoch [1/5], Step [4/25], Train Loss: 2.7562, Valid Loss: 2.6160
Epoch [2/5], Step [6/25], Train Loss: 2.7339, Valid Loss: 2.7082
Epoch [2/5], Step [8/25], Train Loss: 2.5780, Valid Loss: 2.7044
Epoch [2/5], Step [10/25], Train Loss: 2.6402, Valid Loss: 2.7117
Epoch [3/5], Step [12/25], Train Loss: 2.5706, Valid Loss: 2.7440
Epoch [3/5], Step [14/25], Train Loss: 2.5829, Valid Loss: 2.7652
Epoch [4/5], Step [16/25], Train Loss: 2.3939, Valid Loss: 2.6645
Epoch [4/5], Step [18/25], Train Loss: 2.6117, Valid Loss: 2.6352
Epoch [4/5], Step [20/25], Train Loss: 2.5011, Valid Loss: 2.6165
Epoch [5/5], Step [22/25], Train Loss: 2.5729, Valid Loss: 2.6006
Epoch [5/5], Step [24/25], Train Loss: 2.4218, Valid Loss: 2.5989
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_2/metrics/metrics.pth
Finished Training!
Model loaded from <== Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_2/metrics/metrics.pth
Model loaded from <== Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_2/model/model.pth
Pred saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_2/evaluate/evaluate.pth
Classification Report :
              precision    recall  f1-score   support

           0     0.0000    0.0000    0.0000        24
           1     0.6429    0.0903    0.1584       299
           2     0.3333    0.0219    0.0411       137
           3     0.1720    0.3313    0.2264       163
           4     0.0739    0.6667    0.1331        48
           5     0.0000    0.0000    0.0000         3
           6     0.0000    0.0000    0.0000         6
           7     0.0789    0.0566    0.0659        53
           8     0.0000    0.0000    0.0000         1
           9     0.0000    0.0000    0.0000         9
          10     0.0000    0.0000    0.0000        33
          12     0.0000    0.0000    0.0000        25
          13     0.0000    0.0000    0.0000         4

   micro avg     0.1418    0.1478    0.1448       805
   macro avg     0.1001    0.0898    0.0481       805
weighted avg     0.3399    0.1478    0.1239       805

STARTING WITH FOLD NB 3

Epoch [1/5], Step [2/25], Train Loss: 2.6021, Valid Loss: 2.6569
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_3/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_3/metrics/metrics.pth
Epoch [1/5], Step [4/25], Train Loss: 2.7044, Valid Loss: 2.6547
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_3/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_3/metrics/metrics.pth
Epoch [2/5], Step [6/25], Train Loss: 2.7656, Valid Loss: 2.5998
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_3/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_3/metrics/metrics.pth
Epoch [2/5], Step [8/25], Train Loss: 2.5167, Valid Loss: 2.5842
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_3/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_3/metrics/metrics.pth
Epoch [2/5], Step [10/25], Train Loss: 2.5550, Valid Loss: 2.5891
Epoch [3/5], Step [12/25], Train Loss: 2.4766, Valid Loss: 2.5943
Epoch [3/5], Step [14/25], Train Loss: 2.5361, Valid Loss: 2.6272
Epoch [4/5], Step [16/25], Train Loss: 2.5510, Valid Loss: 2.6916
Epoch [4/5], Step [18/25], Train Loss: 2.4804, Valid Loss: 2.7351
Epoch [4/5], Step [20/25], Train Loss: 2.4913, Valid Loss: 2.7366
Epoch [5/5], Step [22/25], Train Loss: 2.4085, Valid Loss: 2.7230
Epoch [5/5], Step [24/25], Train Loss: 2.4488, Valid Loss: 2.7008
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_3/metrics/metrics.pth
Finished Training!
Model loaded from <== Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_3/metrics/metrics.pth
Model loaded from <== Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_3/model/model.pth
Pred saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_3/evaluate/evaluate.pth
Classification Report :
              precision    recall  f1-score   support

           0     0.0286    0.2083    0.0503        24
           1     0.0000    0.0000    0.0000       299
           2     0.0000    0.0000    0.0000       137
           3     0.2034    0.2209    0.2118       163
           4     0.0743    0.4583    0.1279        48
           5     0.0000    0.0000    0.0000         3
           6     0.0000    0.0000    0.0000         6
           7     0.3333    0.0566    0.0968        53
           8     0.0000    0.0000    0.0000         1
           9     0.0500    0.1111    0.0690         9
          10     0.0000    0.0000    0.0000        33
          12     0.0000    0.0000    0.0000        25
          13     0.0000    0.0000    0.0000         4

   micro avg     0.0799    0.0832    0.0815       805
   macro avg     0.0530    0.0812    0.0427       805
weighted avg     0.0690    0.0832    0.0591       805

STARTING WITH FOLD NB 4

Epoch [1/5], Step [1/10], Train Loss: 2.6144, Valid Loss: 2.6873
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_4/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_4/metrics/metrics.pth
Epoch [1/5], Step [2/10], Train Loss: 3.0822, Valid Loss: 2.6934
Epoch [2/5], Step [3/10], Train Loss: 2.7244, Valid Loss: 2.7055
Epoch [2/5], Step [4/10], Train Loss: 2.5150, Valid Loss: 2.7259
Epoch [3/5], Step [5/10], Train Loss: 2.6356, Valid Loss: 2.7261
Epoch [3/5], Step [6/10], Train Loss: 2.5251, Valid Loss: 2.7307
Epoch [4/5], Step [7/10], Train Loss: 2.5121, Valid Loss: 2.7230
Epoch [4/5], Step [8/10], Train Loss: 2.5375, Valid Loss: 2.7185
Epoch [5/5], Step [9/10], Train Loss: 2.4665, Valid Loss: 2.7192
Epoch [5/5], Step [10/10], Train Loss: 2.5294, Valid Loss: 2.7166
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_4/metrics/metrics.pth
Finished Training!
Model loaded from <== Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_4/metrics/metrics.pth
Model loaded from <== Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_4/model/model.pth
Pred saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_4/evaluate/evaluate.pth
Classification Report :
              precision    recall  f1-score   support

           0     0.0000    0.0000    0.0000        24
           1     0.2222    0.0067    0.0130       299
           2     0.0000    0.0000    0.0000       137
           3     0.0000    0.0000    0.0000       163
           4     0.0000    0.0000    0.0000        48
           5     0.0000    0.0000    0.0000         3
           6     0.0000    0.0000    0.0000         6
           7     0.0710    0.2075    0.1058        53
           8     0.0000    0.0000    0.0000         1
           9     0.0000    0.0000    0.0000         9
          10     0.0000    0.0000    0.0000        33
          12     0.0000    0.0000    0.0000        25
          13     0.0040    0.2500    0.0079         4

   micro avg     0.0255    0.0174    0.0207       805
   macro avg     0.0229    0.0357    0.0097       805
weighted avg     0.0872    0.0174    0.0118       805

STARTING WITH FOLD NB 5

Epoch [1/5], Step [2/25], Train Loss: 2.6190, Valid Loss: 2.5301
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_5/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_5/metrics/metrics.pth
Epoch [1/5], Step [4/25], Train Loss: 2.7896, Valid Loss: 2.5566
Epoch [2/5], Step [6/25], Train Loss: 2.7527, Valid Loss: 2.5830
Epoch [2/5], Step [8/25], Train Loss: 2.5292, Valid Loss: 2.5824
Epoch [2/5], Step [10/25], Train Loss: 2.7139, Valid Loss: 2.5815
Epoch [3/5], Step [12/25], Train Loss: 2.6343, Valid Loss: 2.5789
Epoch [3/5], Step [14/25], Train Loss: 2.5259, Valid Loss: 2.5791
Epoch [4/5], Step [16/25], Train Loss: 2.4490, Valid Loss: 2.5842Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Token indices sequence length is longer than the specified maximum sequence length for this model (643 > 512). Running this sequence through the model will result in indexing errors
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Token indices sequence length is longer than the specified maximum sequence length for this model (721 > 512). Running this sequence through the model will result in indexing errors
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Token indices sequence length is longer than the specified maximum sequence length for this model (599 > 512). Running this sequence through the model will result in indexing errors

Epoch [4/5], Step [18/25], Train Loss: 2.4488, Valid Loss: 2.5917
Epoch [4/5], Step [20/25], Train Loss: 2.5377, Valid Loss: 2.5990
Epoch [5/5], Step [22/25], Train Loss: 2.5522, Valid Loss: 2.6030
Epoch [5/5], Step [24/25], Train Loss: 2.4510, Valid Loss: 2.6059
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_5/metrics/metrics.pth
Finished Training!
Model loaded from <== Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_5/metrics/metrics.pth
Model loaded from <== Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_5/model/model.pth
Pred saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_5/evaluate/evaluate.pth
Classification Report :
              precision    recall  f1-score   support

           0     0.0000    0.0000    0.0000        24
           1     0.0000    0.0000    0.0000       299
           2     0.0000    0.0000    0.0000       137
           3     0.0000    0.0000    0.0000       163
           4     0.0676    0.7083    0.1234        48
           5     0.0000    0.0000    0.0000         3
           6     0.0127    0.6667    0.0249         6
           7     0.1053    0.0377    0.0556        53
           8     0.0000    0.0000    0.0000         1
           9     0.0000    0.0000    0.0000         9
          10     0.0000    0.0000    0.0000        33
          12     0.0000    0.0000    0.0000        25
          13     0.0000    0.0000    0.0000         4

   micro avg     0.0477    0.0497    0.0487       805
   macro avg     0.0143    0.1087    0.0157       805
weighted avg     0.0111    0.0497    0.0112       805

STARTING WITH FOLD NB 6

Epoch [1/5], Step [1/15], Train Loss: 2.6994, Valid Loss: 2.6514
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_6/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_6/metrics/metrics.pth
Epoch [1/5], Step [2/15], Train Loss: 2.7005, Valid Loss: 2.7004
Epoch [1/5], Step [3/15], Train Loss: 2.7161, Valid Loss: 2.6746
Epoch [2/5], Step [4/15], Train Loss: 2.5314, Valid Loss: 2.6359
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_6/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_6/metrics/metrics.pth
Epoch [2/5], Step [5/15], Train Loss: 2.5148, Valid Loss: 2.5840
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_6/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_6/metrics/metrics.pth
Epoch [2/5], Step [6/15], Train Loss: 2.6118, Valid Loss: 2.5511
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_6/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_6/metrics/metrics.pth
Epoch [3/5], Step [7/15], Train Loss: 2.4798, Valid Loss: 2.5517
Epoch [3/5], Step [8/15], Train Loss: 2.4975, Valid Loss: 2.5523
Epoch [3/5], Step [9/15], Train Loss: 2.5666, Valid Loss: 2.5466
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_6/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_6/metrics/metrics.pth
Epoch [4/5], Step [10/15], Train Loss: 2.4772, Valid Loss: 2.5418
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_6/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_6/metrics/metrics.pth
Epoch [4/5], Step [11/15], Train Loss: 2.4434, Valid Loss: 2.5379
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_6/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_6/metrics/metrics.pth
Epoch [4/5], Step [12/15], Train Loss: 2.4649, Valid Loss: 2.5356
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_6/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_6/metrics/metrics.pth
Epoch [5/5], Step [13/15], Train Loss: 2.3731, Valid Loss: 2.5349
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_6/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_6/metrics/metrics.pth
Epoch [5/5], Step [14/15], Train Loss: 2.4494, Valid Loss: 2.5441
Epoch [5/5], Step [15/15], Train Loss: 2.3692, Valid Loss: 2.5549
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_6/metrics/metrics.pth
Finished Training!
Model loaded from <== Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_6/metrics/metrics.pth
Model loaded from <== Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_6/model/model.pth
Pred saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_6/evaluate/evaluate.pth
Classification Report :
              precision    recall  f1-score   support

           0     0.0288    0.3750    0.0534        24
           1     0.4231    0.1104    0.1751       299
           2     0.0000    0.0000    0.0000       137
           3     0.2089    0.2025    0.2056       163
           4     0.0600    0.1250    0.0811        48
           5     0.0000    0.0000    0.0000         3
           6     0.0000    0.0000    0.0000         6
           7     0.0000    0.0000    0.0000        53
           8     0.0000    0.0000    0.0000         1
           9     0.0000    0.0000    0.0000         9
          10     0.0000    0.0000    0.0000        33
          12     0.0000    0.0000    0.0000        25
          13     0.0078    0.2500    0.0152         4

   micro avg     0.0977    0.1019    0.0998       805
   macro avg     0.0560    0.0818    0.0408       805
weighted avg     0.2039    0.1019    0.1132       805

STARTING WITH FOLD NB 7

Epoch [1/5], Step [2/25], Train Loss: 2.6988, Valid Loss: 2.5593
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_7/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_7/metrics/metrics.pth
Epoch [1/5], Step [4/25], Train Loss: 2.8260, Valid Loss: 2.5813
Epoch [2/5], Step [6/25], Train Loss: 2.5681, Valid Loss: 2.5809
Epoch [2/5], Step [8/25], Train Loss: 2.6837, Valid Loss: 2.5981
Epoch [2/5], Step [10/25], Train Loss: 2.6251, Valid Loss: 2.5986
Epoch [3/5], Step [12/25], Train Loss: 2.5529, Valid Loss: 2.5871
Epoch [3/5], Step [14/25], Train Loss: 2.4984, Valid Loss: 2.5818
Epoch [4/5], Step [16/25], Train Loss: 2.5200, Valid Loss: 2.5998
Epoch [4/5], Step [18/25], Train Loss: 2.3217, Valid Loss: 2.6267
Epoch [4/5], Step [20/25], Train Loss: 2.5136, Valid Loss: 2.6454
Epoch [5/5], Step [22/25], Train Loss: 2.3298, Valid Loss: 2.6544
Epoch [5/5], Step [24/25], Train Loss: 2.4837, Valid Loss: 2.6444
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_7/metrics/metrics.pth
Finished Training!
Model loaded from <== Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_7/metrics/metrics.pth
Model loaded from <== Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_7/model/model.pth
Pred saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_7/evaluate/evaluate.pth
Classification Report :
              precision    recall  f1-score   support

           0     0.0000    0.0000    0.0000        24
           1     0.0000    0.0000    0.0000       299
           2     0.1631    0.9562    0.2787       137
           3     0.0000    0.0000    0.0000       163
           4     0.0000    0.0000    0.0000        48
           5     0.0000    0.0000    0.0000         3
           6     0.0000    0.0000    0.0000         6
           7     0.0000    0.0000    0.0000        53
           8     0.0000    0.0000    0.0000         1
           9     0.0000    0.0000    0.0000         9
          10     0.0000    0.0000    0.0000        33
          12     0.0000    0.0000    0.0000        25
          13     0.0000    0.0000    0.0000         4

   micro avg     0.1621    0.1627    0.1624       805
   macro avg     0.0125    0.0736    0.0214       805
weighted avg     0.0278    0.1627    0.0474       805

STARTING WITH FOLD NB 8

Epoch [1/5], Step [2/25], Train Loss: 2.6160, Valid Loss: 2.5875
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_8/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_8/metrics/metrics.pthSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Token indices sequence length is longer than the specified maximum sequence length for this model (3076 > 512). Running this sequence through the model will result in indexing errors
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Token indices sequence length is longer than the specified maximum sequence length for this model (1137 > 512). Running this sequence through the model will result in indexing errors
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Token indices sequence length is longer than the specified maximum sequence length for this model (2671 > 512). Running this sequence through the model will result in indexing errors

Epoch [1/5], Step [4/25], Train Loss: 2.7928, Valid Loss: 2.6166
Epoch [2/5], Step [6/25], Train Loss: 2.6286, Valid Loss: 2.6202
Epoch [2/5], Step [8/25], Train Loss: 2.6116, Valid Loss: 2.6366
Epoch [2/5], Step [10/25], Train Loss: 2.5774, Valid Loss: 2.7087
Epoch [3/5], Step [12/25], Train Loss: 2.5685, Valid Loss: 2.7105
Epoch [3/5], Step [14/25], Train Loss: 2.4982, Valid Loss: 2.7367
Epoch [4/5], Step [16/25], Train Loss: 2.4460, Valid Loss: 2.7659
Epoch [4/5], Step [18/25], Train Loss: 2.3246, Valid Loss: 2.7825
Epoch [4/5], Step [20/25], Train Loss: 2.3660, Valid Loss: 2.7629
Epoch [5/5], Step [22/25], Train Loss: 2.3580, Valid Loss: 2.7222
Epoch [5/5], Step [24/25], Train Loss: 2.4519, Valid Loss: 2.6734
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_8/metrics/metrics.pth
Finished Training!
Model loaded from <== Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_8/metrics/metrics.pth
Model loaded from <== Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_8/model/model.pth
Pred saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_8/evaluate/evaluate.pth
Classification Report :
              precision    recall  f1-score   support

           0     0.0000    0.0000    0.0000        24
           1     0.3636    0.0134    0.0258       299
           2     0.1748    0.5474    0.2650       137
           3     0.0000    0.0000    0.0000       163
           4     0.0413    0.3333    0.0736        48
           5     0.0000    0.0000    0.0000         3
           6     0.0000    0.0000    0.0000         6
           7     0.0000    0.0000    0.0000        53
           8     0.0000    0.0000    0.0000         1
           9     0.0000    0.0000    0.0000         9
          10     0.0000    0.0000    0.0000        33
          12     0.0000    0.0000    0.0000        25
          13     0.0000    0.0000    0.0000         4

   micro avg     0.1132    0.1180    0.1156       805
   macro avg     0.0446    0.0688    0.0280       805
weighted avg     0.1673    0.1180    0.0591       805

STARTING WITH FOLD NB 9

Epoch [1/5], Step [2/25], Train Loss: 2.7110, Valid Loss: 2.8311
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_9/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_9/metrics/metrics.pth
Epoch [1/5], Step [4/25], Train Loss: 2.6569, Valid Loss: 2.7850
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_9/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_9/metrics/metrics.pth
Epoch [2/5], Step [6/25], Train Loss: 2.6572, Valid Loss: 2.7271
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_9/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_9/metrics/metrics.pth
Epoch [2/5], Step [8/25], Train Loss: 2.6183, Valid Loss: 2.7102
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_9/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_9/metrics/metrics.pth
Epoch [2/5], Step [10/25], Train Loss: 2.6142, Valid Loss: 2.6850
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_9/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_9/metrics/metrics.pth
Epoch [3/5], Step [12/25], Train Loss: 2.5441, Valid Loss: 2.6480
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_9/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_9/metrics/metrics.pth
Epoch [3/5], Step [14/25], Train Loss: 2.4782, Valid Loss: 2.6488
Epoch [4/5], Step [16/25], Train Loss: 2.4856, Valid Loss: 2.7018
Epoch [4/5], Step [18/25], Train Loss: 2.4854, Valid Loss: 2.7736
Epoch [4/5], Step [20/25], Train Loss: 2.5026, Valid Loss: 2.8057
Epoch [5/5], Step [22/25], Train Loss: 2.4180, Valid Loss: 2.8058
Epoch [5/5], Step [24/25], Train Loss: 2.5571, Valid Loss: 2.7706
Model saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_9/metrics/metrics.pth
Finished Training!
Model loaded from <== Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_9/metrics/metrics.pth
Model loaded from <== Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_9/model/model.pth
Pred saved to ==> Temp_Data_Files_Multiclass/UnderSampled/Dataset_KFold_9/evaluate/evaluate.pth
Classification Report :
              precision    recall  f1-score   support

           0     0.0000    0.0000    0.0000        24
           1     0.5000    0.0033    0.0066       299
           2     0.2254    0.1168    0.1538       137
           3     0.1333    0.0368    0.0577       163
           4     0.0000    0.0000    0.0000        48
           5     0.0083    0.3333    0.0163         3
           6     0.0000    0.0000    0.0000         6
           7     0.0645    0.1509    0.0904        53
           8     0.0000    0.0000    0.0000         1
           9     0.0000    0.0000    0.0000         9
          10     0.0000    0.0000    0.0000        33
          12     0.0000    0.0000    0.0000        25
          13     0.0112    0.2500    0.0215         4

   micro avg     0.0397    0.0410    0.0403       805
   macro avg     0.0725    0.0686    0.0266       805
weighted avg     0.2554    0.0410    0.0465       805

STARTING CROSS VALIDATION FOR OverSampled DATASET

STARTING WITH FOLD NB 0

Epoch [1/5], Step [1023/10230], Train Loss: 1.1778, Valid Loss: 1.3756
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_0/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_0/metrics/metrics.pth
Epoch [1/5], Step [2046/10230], Train Loss: 0.2768, Valid Loss: 1.1368
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_0/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_0/metrics/metrics.pth
Epoch [2/5], Step [3069/10230], Train Loss: 0.1362, Valid Loss: 1.1501
Epoch [2/5], Step [4092/10230], Train Loss: 0.1175, Valid Loss: 1.3148
Epoch [3/5], Step [5115/10230], Train Loss: 0.0683, Valid Loss: 1.2476
Epoch [3/5], Step [6138/10230], Train Loss: 0.0723, Valid Loss: 1.2907
Epoch [4/5], Step [7161/10230], Train Loss: 0.0381, Valid Loss: 1.4800
Epoch [4/5], Step [8184/10230], Train Loss: 0.0457, Valid Loss: 1.4179
Epoch [5/5], Step [9207/10230], Train Loss: 0.0335, Valid Loss: 1.5091
Epoch [5/5], Step [10230/10230], Train Loss: 0.0378, Valid Loss: 1.6001
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_0/metrics/metrics.pth
Finished Training!
Model loaded from <== Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_0/metrics/metrics.pth
Model loaded from <== Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_0/model/model.pth
Pred saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_0/evaluate/evaluate.pth
Classification Report :
              precision    recall  f1-score   support

           0     0.7500    0.6250    0.6818        24
           1     0.8289    0.7291    0.7758       299
           2     0.6306    0.7226    0.6735       137
           3     0.5850    0.5276    0.5548       163
           4     0.5769    0.9375    0.7143        48
           5     1.0000    0.3333    0.5000         3
           6     0.0000    0.0000    0.0000         6
           7     0.6184    0.8868    0.7287        53
           8     0.0000    0.0000    0.0000         1
           9     0.0909    0.1111    0.1000         9
          10     0.5676    0.6364    0.6000        33
          12     0.5000    0.1200    0.1935        25
          13     1.0000    0.5000    0.6667         4

   micro avg     0.6700    0.6683    0.6692       805
   macro avg     0.5499    0.4715    0.4761       805
weighted avg     0.6796    0.6683    0.6629       805

STARTING WITH FOLD NB 1

Epoch [1/5], Step [1034/10340], Train Loss: 1.1780, Valid Loss: 1.1162
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_1/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_1/metrics/metrics.pth
Epoch [1/5], Step [2068/10340], Train Loss: 0.2740, Valid Loss: 0.9669Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Token indices sequence length is longer than the specified maximum sequence length for this model (2671 > 512). Running this sequence through the model will result in indexing errors
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Token indices sequence length is longer than the specified maximum sequence length for this model (2671 > 512). Running this sequence through the model will result in indexing errors
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Token indices sequence length is longer than the specified maximum sequence length for this model (2671 > 512). Running this sequence through the model will result in indexing errors
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_1/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_1/metrics/metrics.pth
Epoch [2/5], Step [3102/10340], Train Loss: 0.1179, Valid Loss: 1.0628
Epoch [2/5], Step [4136/10340], Train Loss: 0.1167, Valid Loss: 1.1019
Epoch [3/5], Step [5170/10340], Train Loss: 0.0650, Valid Loss: 1.3886
Epoch [3/5], Step [6204/10340], Train Loss: 0.0660, Valid Loss: 1.3444
Epoch [4/5], Step [7238/10340], Train Loss: 0.0329, Valid Loss: 1.5295
Epoch [4/5], Step [8272/10340], Train Loss: 0.0435, Valid Loss: 1.5463
Epoch [5/5], Step [9306/10340], Train Loss: 0.0325, Valid Loss: 1.5059
Epoch [5/5], Step [10340/10340], Train Loss: 0.0465, Valid Loss: 1.3216
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_1/metrics/metrics.pth
Finished Training!
Model loaded from <== Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_1/metrics/metrics.pth
Model loaded from <== Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_1/model/model.pth
Pred saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_1/evaluate/evaluate.pth
Classification Report :
              precision    recall  f1-score   support

           0     0.6667    0.5833    0.6222        24
           1     0.7981    0.8328    0.8151       299
           2     0.7258    0.6569    0.6897       137
           3     0.5922    0.6503    0.6199       163
           4     0.7400    0.7708    0.7551        48
           5     0.5000    0.3333    0.4000         3
           6     0.0000    0.0000    0.0000         6
           7     0.7069    0.7736    0.7387        53
           8     0.0000    0.0000    0.0000         1
           9     0.0000    0.0000    0.0000         9
          10     0.5625    0.5455    0.5538        33
          12     0.6667    0.4000    0.5000        25
          13     0.6667    0.5000    0.5714         4

   micro avg     0.7082    0.7056    0.7069       805
   macro avg     0.5097    0.4651    0.4820       805
weighted avg     0.6993    0.7056    0.7004       805

STARTING WITH FOLD NB 2

Epoch [1/5], Step [1035/10355], Train Loss: 1.2545, Valid Loss: 1.1384
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_2/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_2/metrics/metrics.pth
Epoch [1/5], Step [2070/10355], Train Loss: 0.2803, Valid Loss: 1.0303
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_2/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_2/metrics/metrics.pth
Epoch [2/5], Step [3105/10355], Train Loss: 0.1247, Valid Loss: 1.1628
Epoch [2/5], Step [4140/10355], Train Loss: 0.1159, Valid Loss: 1.1944
Epoch [3/5], Step [5175/10355], Train Loss: 0.0604, Valid Loss: 1.1857
Epoch [3/5], Step [6210/10355], Train Loss: 0.0730, Valid Loss: 1.2575
Epoch [4/5], Step [7245/10355], Train Loss: 0.0356, Valid Loss: 1.4200
Epoch [4/5], Step [8280/10355], Train Loss: 0.0515, Valid Loss: 1.2843
Epoch [5/5], Step [9315/10355], Train Loss: 0.0304, Valid Loss: 1.5792
Epoch [5/5], Step [10350/10355], Train Loss: 0.0435, Valid Loss: 1.2206
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_2/metrics/metrics.pth
Finished Training!
Model loaded from <== Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_2/metrics/metrics.pth
Model loaded from <== Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_2/model/model.pth
Pred saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_2/evaluate/evaluate.pth
Classification Report :
              precision    recall  f1-score   support

           0     0.6400    0.6667    0.6531        24
           1     0.8172    0.7926    0.8048       299
           2     0.8214    0.5036    0.6244       137
           3     0.5284    0.7423    0.6173       163
           4     0.7647    0.8125    0.7879        48
           5     0.3333    0.3333    0.3333         3
           6     0.0000    0.0000    0.0000         6
           7     0.6716    0.8491    0.7500        53
           8     0.0000    0.0000    0.0000         1
           9     0.3333    0.1111    0.1667         9
          10     0.5862    0.5152    0.5484        33
          12     0.6111    0.4400    0.5116        25
          13     1.0000    0.5000    0.6667         4

   micro avg     0.6944    0.6944    0.6944       805
   macro avg     0.5467    0.4820    0.4972       805
weighted avg     0.7122    0.6944    0.6908       805

STARTING WITH FOLD NB 3

Epoch [1/5], Step [1037/10370], Train Loss: 1.1467, Valid Loss: 1.1649
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_3/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_3/metrics/metrics.pth
Epoch [1/5], Step [2074/10370], Train Loss: 0.2715, Valid Loss: 1.2540
Epoch [2/5], Step [3111/10370], Train Loss: 0.1209, Valid Loss: 1.2551
Epoch [2/5], Step [4148/10370], Train Loss: 0.1198, Valid Loss: 1.3220
Epoch [3/5], Step [5185/10370], Train Loss: 0.0610, Valid Loss: 1.4147
Epoch [3/5], Step [6222/10370], Train Loss: 0.0608, Valid Loss: 1.6180
Epoch [4/5], Step [7259/10370], Train Loss: 0.0322, Valid Loss: 1.6070
Epoch [4/5], Step [8296/10370], Train Loss: 0.0562, Valid Loss: 1.5711
Epoch [5/5], Step [9333/10370], Train Loss: 0.0306, Valid Loss: 1.6914
Epoch [5/5], Step [10370/10370], Train Loss: 0.0431, Valid Loss: 1.6009
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_3/metrics/metrics.pth
Finished Training!
Model loaded from <== Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_3/metrics/metrics.pth
Model loaded from <== Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_3/model/model.pth
Pred saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_3/evaluate/evaluate.pth
Classification Report :
              precision    recall  f1-score   support

           0     0.7083    0.7083    0.7083        24
           1     0.8063    0.7659    0.7856       299
           2     0.6923    0.6569    0.6742       137
           3     0.5347    0.4724    0.5016       163
           4     0.7907    0.7083    0.7473        48
           5     1.0000    0.3333    0.5000         3
           6     0.0000    0.0000    0.0000         6
           7     0.6351    0.8868    0.7402        53
           8     0.0000    0.0000    0.0000         1
           9     0.0000    0.0000    0.0000         9
          10     0.4773    0.6364    0.5455        33
          12     0.3429    0.4800    0.4000        25
          13     0.2500    0.2500    0.2500         4

   micro avg     0.6713    0.6571    0.6642       805
   macro avg     0.4798    0.4537    0.4502       805
weighted avg     0.6709    0.6571    0.6604       805

STARTING WITH FOLD NB 4

Epoch [1/5], Step [1028/10285], Train Loss: 1.1499, Valid Loss: 1.2118
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_4/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_4/metrics/metrics.pth
Epoch [1/5], Step [2056/10285], Train Loss: 0.2801, Valid Loss: 1.0980
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_4/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_4/metrics/metrics.pth
Epoch [2/5], Step [3084/10285], Train Loss: 0.1364, Valid Loss: 1.2691
Epoch [2/5], Step [4112/10285], Train Loss: 0.1288, Valid Loss: 1.2395
Epoch [3/5], Step [5140/10285], Train Loss: 0.0555, Valid Loss: 1.2355
Epoch [3/5], Step [6168/10285], Train Loss: 0.0672, Valid Loss: 1.3555
Epoch [4/5], Step [7196/10285], Train Loss: 0.0511, Valid Loss: 1.4744
Epoch [4/5], Step [8224/10285], Train Loss: 0.0435, Valid Loss: 1.4924
Epoch [5/5], Step [9252/10285], Train Loss: 0.0365, Valid Loss: 1.5697
Epoch [5/5], Step [10280/10285], Train Loss: 0.0415, Valid Loss: 1.4715
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_4/metrics/metrics.pth
Finished Training!
Model loaded from <== Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_4/metrics/metrics.pth
Model loaded from <== Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_4/model/model.pthSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Token indices sequence length is longer than the specified maximum sequence length for this model (2671 > 512). Running this sequence through the model will result in indexing errors
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Token indices sequence length is longer than the specified maximum sequence length for this model (2671 > 512). Running this sequence through the model will result in indexing errors
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Token indices sequence length is longer than the specified maximum sequence length for this model (2671 > 512). Running this sequence through the model will result in indexing errors
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Pred saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_4/evaluate/evaluate.pth
Classification Report :
              precision    recall  f1-score   support

           0     0.6800    0.7083    0.6939        24
           1     0.8545    0.7659    0.8078       299
           2     0.6059    0.7518    0.6710       137
           3     0.6475    0.5521    0.5960       163
           4     0.8182    0.7500    0.7826        48
           5     0.3333    0.3333    0.3333         3
           6     0.0000    0.0000    0.0000         6
           7     0.6806    0.9245    0.7840        53
           8     0.0000    0.0000    0.0000         1
           9     0.5000    0.1111    0.1818         9
          10     0.5588    0.5758    0.5672        33
          12     0.5000    0.4800    0.4898        25
          13     1.0000    0.2500    0.4000         4

   micro avg     0.7054    0.6932    0.6992       805
   macro avg     0.5522    0.4771    0.4852       805
weighted avg     0.7157    0.6932    0.6976       805

STARTING WITH FOLD NB 5

Epoch [1/5], Step [1028/10285], Train Loss: 1.2278, Valid Loss: 1.3233
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_5/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_5/metrics/metrics.pth
Epoch [1/5], Step [2056/10285], Train Loss: 0.2759, Valid Loss: 1.2502
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_5/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_5/metrics/metrics.pth
Epoch [2/5], Step [3084/10285], Train Loss: 0.1305, Valid Loss: 1.2630
Epoch [2/5], Step [4112/10285], Train Loss: 0.1127, Valid Loss: 1.3690
Epoch [3/5], Step [5140/10285], Train Loss: 0.0614, Valid Loss: 1.5160
Epoch [3/5], Step [6168/10285], Train Loss: 0.0686, Valid Loss: 1.5384
Epoch [4/5], Step [7196/10285], Train Loss: 0.0502, Valid Loss: 1.5352
Epoch [4/5], Step [8224/10285], Train Loss: 0.0470, Valid Loss: 1.6750
Epoch [5/5], Step [9252/10285], Train Loss: 0.0227, Valid Loss: 1.6274
Epoch [5/5], Step [10280/10285], Train Loss: 0.0457, Valid Loss: 1.5273
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_5/metrics/metrics.pth
Finished Training!
Model loaded from <== Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_5/metrics/metrics.pth
Model loaded from <== Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_5/model/model.pth
Pred saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_5/evaluate/evaluate.pth
Classification Report :
              precision    recall  f1-score   support

           0     0.6667    0.6667    0.6667        24
           1     0.8519    0.7692    0.8084       299
           2     0.5510    0.7883    0.6486       137
           3     0.6142    0.4785    0.5379       163
           4     0.7143    0.8333    0.7692        48
           5     0.0000    0.0000    0.0000         3
           6     0.0000    0.0000    0.0000         6
           7     0.6522    0.8491    0.7377        53
           8     0.0000    0.0000    0.0000         1
           9     0.3333    0.1111    0.1667         9
          10     0.5000    0.6364    0.5600        33
          12     0.5455    0.2400    0.3333        25
          13     1.0000    0.5000    0.6667         4

   micro avg     0.6812    0.6795    0.6803       805
   macro avg     0.4945    0.4517    0.4535       805
weighted avg     0.6861    0.6795    0.6724       805

STARTING WITH FOLD NB 6

Epoch [1/5], Step [1030/10305], Train Loss: 1.1786, Valid Loss: 1.2609
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_6/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_6/metrics/metrics.pth
Epoch [1/5], Step [2060/10305], Train Loss: 0.2775, Valid Loss: 1.1145
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_6/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_6/metrics/metrics.pth
Epoch [2/5], Step [3090/10305], Train Loss: 0.1160, Valid Loss: 1.1230
Epoch [2/5], Step [4120/10305], Train Loss: 0.1152, Valid Loss: 1.2084
Epoch [3/5], Step [5150/10305], Train Loss: 0.0703, Valid Loss: 1.3265
Epoch [3/5], Step [6180/10305], Train Loss: 0.0581, Valid Loss: 1.3579
Epoch [4/5], Step [7210/10305], Train Loss: 0.0436, Valid Loss: 1.4605
Epoch [4/5], Step [8240/10305], Train Loss: 0.0450, Valid Loss: 1.3408
Epoch [5/5], Step [9270/10305], Train Loss: 0.0275, Valid Loss: 1.5315
Epoch [5/5], Step [10300/10305], Train Loss: 0.0411, Valid Loss: 1.3337
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_6/metrics/metrics.pth
Finished Training!
Model loaded from <== Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_6/metrics/metrics.pth
Model loaded from <== Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_6/model/model.pth
Pred saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_6/evaluate/evaluate.pth
Classification Report :
              precision    recall  f1-score   support

           0     0.5926    0.6667    0.6275        24
           1     0.8476    0.7625    0.8028       299
           2     0.7395    0.6423    0.6875       137
           3     0.6788    0.5706    0.6200       163
           4     0.5857    0.8542    0.6949        48
           5     0.3333    0.3333    0.3333         3
           6     0.1667    0.1667    0.1667         6
           7     0.6575    0.9057    0.7619        53
           8     0.0000    0.0000    0.0000         1
           9     1.0000    0.1111    0.2000         9
          10     0.3710    0.6970    0.4842        33
          12     0.3636    0.4800    0.4138        25
          13     1.0000    0.5000    0.6667         4

   micro avg     0.6908    0.6882    0.6895       805
   macro avg     0.5643    0.5146    0.4969       805
weighted avg     0.7191    0.6882    0.6918       805

STARTING WITH FOLD NB 7

Epoch [1/5], Step [1023/10235], Train Loss: 1.2917, Valid Loss: 1.0783
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_7/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_7/metrics/metrics.pth
Epoch [1/5], Step [2046/10235], Train Loss: 0.3012, Valid Loss: 1.0555
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_7/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_7/metrics/metrics.pth
Epoch [2/5], Step [3069/10235], Train Loss: 0.1371, Valid Loss: 1.0310
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_7/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_7/metrics/metrics.pth
Epoch [2/5], Step [4092/10235], Train Loss: 0.1185, Valid Loss: 1.1451
Epoch [3/5], Step [5115/10235], Train Loss: 0.0773, Valid Loss: 1.2471
Epoch [3/5], Step [6138/10235], Train Loss: 0.0753, Valid Loss: 1.2791
Epoch [4/5], Step [7161/10235], Train Loss: 0.0498, Valid Loss: 1.4447
Epoch [4/5], Step [8184/10235], Train Loss: 0.0473, Valid Loss: 1.4973
Epoch [5/5], Step [9207/10235], Train Loss: 0.0397, Valid Loss: 1.5310
Epoch [5/5], Step [10230/10235], Train Loss: 0.0416, Valid Loss: 1.5273
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_7/metrics/metrics.pth
Finished Training!
Model loaded from <== Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_7/metrics/metrics.pth
Model loaded from <== Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_7/model/model.pth
Pred saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_7/evaluate/evaluate.pth
Classification Report :
              precision    recall  f1-score   support

           0     0.7222    0.5417    0.6190        24
           1     0.7890    0.8629    0.8243       299
           2     0.7438    0.6569    0.6977       137
           3     0.6392    0.6196    0.6293       163
           4     0.7321    0.8542    0.7885        48
           5     1.0000    0.3333    0.5000         3
           6     0.0000    0.0000    0.0000         6
           7     0.7167    0.8113    0.7611        53
           8     0.0000    0.0000    0.0000         1
           9     0.0000    0.0000    0.0000         9
          10     0.4878    0.6061    0.5405        33
          12     0.5882    0.4000    0.4762        25
          13     0.6667    0.5000    0.5714         4

   micro avg     0.7201    0.7193    0.7197       805
   macro avg     0.5451    0.4758    0.4929       805
weighted avg     0.7068    0.7193    0.7095       805
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Token indices sequence length is longer than the specified maximum sequence length for this model (2671 > 512). Running this sequence through the model will result in indexing errors
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Token indices sequence length is longer than the specified maximum sequence length for this model (2671 > 512). Running this sequence through the model will result in indexing errors
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

STARTING WITH FOLD NB 8

Epoch [1/5], Step [1042/10420], Train Loss: 1.1323, Valid Loss: 0.9444
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_8/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_8/metrics/metrics.pth
Epoch [1/5], Step [2084/10420], Train Loss: 0.2605, Valid Loss: 1.0591
Epoch [2/5], Step [3126/10420], Train Loss: 0.1167, Valid Loss: 1.1530
Epoch [2/5], Step [4168/10420], Train Loss: 0.1141, Valid Loss: 1.2010
Epoch [3/5], Step [5210/10420], Train Loss: 0.0514, Valid Loss: 1.4157
Epoch [3/5], Step [6252/10420], Train Loss: 0.0716, Valid Loss: 1.3032
Epoch [4/5], Step [7294/10420], Train Loss: 0.0427, Valid Loss: 1.3333
Epoch [4/5], Step [8336/10420], Train Loss: 0.0376, Valid Loss: 1.3247
Epoch [5/5], Step [9378/10420], Train Loss: 0.0277, Valid Loss: 1.5174
Epoch [5/5], Step [10420/10420], Train Loss: 0.0428, Valid Loss: 1.4263
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_8/metrics/metrics.pth
Finished Training!
Model loaded from <== Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_8/metrics/metrics.pth
Model loaded from <== Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_8/model/model.pth
Pred saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_8/evaluate/evaluate.pth
Classification Report :
              precision    recall  f1-score   support

           0     0.5862    0.7083    0.6415        24
           1     0.7961    0.8094    0.8027       299
           2     0.6978    0.7080    0.7029       137
           3     0.6903    0.4785    0.5652       163
           4     0.6949    0.8542    0.7664        48
           5     0.0000    0.0000    0.0000         3
           6     0.0000    0.0000    0.0000         6
           7     0.6429    0.8491    0.7317        53
           8     0.0000    0.0000    0.0000         1
           9     0.0000    0.0000    0.0000         9
          10     0.5122    0.6364    0.5676        33
          12     0.3929    0.4400    0.4151        25
          13     0.0000    0.0000    0.0000         4

   micro avg     0.6943    0.6857    0.6900       805
   macro avg     0.3856    0.4218    0.3995       805
weighted avg     0.6886    0.6857    0.6814       805

STARTING WITH FOLD NB 9

Epoch [1/5], Step [1058/10580], Train Loss: 1.3908, Valid Loss: 1.3329
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_9/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_9/metrics/metrics.pth
Epoch [1/5], Step [2116/10580], Train Loss: 0.3026, Valid Loss: 1.3047
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_9/model/model.pth
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_9/metrics/metrics.pth
Epoch [2/5], Step [3174/10580], Train Loss: 0.1326, Valid Loss: 1.3477
Epoch [2/5], Step [4232/10580], Train Loss: 0.1204, Valid Loss: 1.4931
Epoch [3/5], Step [5290/10580], Train Loss: 0.0631, Valid Loss: 1.5691
Epoch [3/5], Step [6348/10580], Train Loss: 0.0706, Valid Loss: 1.5682
Epoch [4/5], Step [7406/10580], Train Loss: 0.0466, Valid Loss: 1.6788
Epoch [4/5], Step [8464/10580], Train Loss: 0.0463, Valid Loss: 1.7547
Epoch [5/5], Step [9522/10580], Train Loss: 0.0417, Valid Loss: 1.7827
Epoch [5/5], Step [10580/10580], Train Loss: 0.0414, Valid Loss: 1.7699
Model saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_9/metrics/metrics.pth
Finished Training!
Model loaded from <== Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_9/metrics/metrics.pth
Model loaded from <== Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_9/model/model.pth
Pred saved to ==> Temp_Data_Files_Multiclass/OverSampled/Dataset_KFold_9/evaluate/evaluate.pth
Classification Report :
              precision    recall  f1-score   support

           0     0.5882    0.8333    0.6897        24
           1     0.8375    0.7926    0.8144       299
           2     0.7177    0.6496    0.6820       137
           3     0.6312    0.6196    0.6254       163
           4     0.6508    0.8542    0.7387        48
           5     1.0000    0.3333    0.5000         3
           6     0.0000    0.0000    0.0000         6
           7     0.6351    0.8868    0.7402        53
           8     0.0000    0.0000    0.0000         1
           9     0.2500    0.1111    0.1538         9
          10     0.4651    0.6061    0.5263        33
          12     0.6154    0.3200    0.4211        25
          13     1.0000    0.5000    0.6667         4

   micro avg     0.7035    0.7043    0.7039       805
   macro avg     0.5685    0.5005    0.5045       805
weighted avg     0.7089    0.7043    0.7001       805


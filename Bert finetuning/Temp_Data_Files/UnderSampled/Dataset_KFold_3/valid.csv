label,summmarydescription
1,"CachingHieraarchyManager may serve moved itemsThere is a problem with weak referenced item states and event notification in the
LocalItemStateManager.

consider the following:
- Session A traverses some nodes and fills-up the cache of the ChachingHierarchyManager
- This also fills the weak-ref cache in Session A LocalItemStateManager.
- Session B does some operations
- At some point, GC decides to remove the weakly refferenced ItemStates in Session As
  LocalItemStateManager
- Session B moves a node and saves the changes.
- The SharedItemStateManager notifies all listeners that a node was modified
- The LocalItemStateManager of Session A receives the event, but does not bubble it,
  because it does not have the item anymore in its cache
- The CachingHierarchyManager of Session A never receives the modification event and still
  servers the items at the old location.

Solution A:
reconnect missing states in the LocalItemStateManager when an event is received. this has
the drawback that a lot of state would be generated that are not needed.

Solution B:
add a new event 'nodeModified' that is only sent by the LocalItemStateManager if a
'stateModified' was received for which it does not have the item aymore. this has the
drawback that alot more events are generated.

Will implement solution B
"
0,"Repository requires access to external resourceWith the changes from JCR-626 jackrabbit requires access to the resource at the URL http://jackrabbit.apache.org/dtd/repository-1.2.dtd. If no connection to the internet is available jackrabbit will refuse to start. At least that's the case when I run the test cases in the above mentioned environment.

I'm not an XML expert, but shouldn't the doctype declaration use a public identifier? Otherwise the ConfigurationEntityResolver class doesn't make much sense.

The attached patch solve the issue for me, please comment. I've also targetted this issue for 1.2 because it seems rather serious to me if you can't start jackrabbit when you don't have an internet connection, or am I the only one with this issue?"
0,"Convert some tests to new TokenStream API, better support of cross-impl AttributeImpl.copyTo()This patch converts some remaining tests to the new TokenStream API and non-deprecated classes.
This patch also enhances AttributeImpl.copyTo() of Token and TokenWrapper to also support copying e.g. TermAttributeImpl into Token. The target impl must only support all interfaces but must not be of the same type. Token and TokenWrapper use optimized coping without casting to 6 interfaces where possible.
Maybe the special tokenizers in contrib (shingle matrix and so on using tokens to cache may be enhanced by that). Also Yonik's request for optimized copying of states between incompatible AttributeSources may be enhanced by that (possibly a new issue)."
0,"[PATCH] better exception messages when generating schemaWhen a statement fails to execute generating the schema, patch outputs the statement that failed."
0,"Move core QueryParsers to queryparser moduleMove the contents of lucene/src/java/org/apache/lucene/queryParser to the queryparser module.

To differentiate these parsers from the others, they are going to be placed a 'classic' package.  We'll rename QueryParser to ClassicQueryParser as well."
0,"expose shutdown method in o.a.j.jndi.BindableRepositorysee http://thread.gmane.org/gmane.comp.apache.jackrabbit.devel/3680
"
0,"A handy utility class for tracking deprecated overridden methodsThis issue provides a new handy utility class that keeps track of overridden deprecated methods in non-final sub classes. This class can be used in new deprecations.

See the javadocs for an example."
0,Un-deprecate QueryParser and remove documentation that says it will be replaced in 3.0This looks like the consensus move at first blush. We can (of course) re-evaluate if things change.
0,"GQLTest fails occasionallyThis is again the text extraction, which may hit a time out."
0,"Add signature and major/minor version to the journal files used for clusteringJournal files used for clustering should contain a signature, and a major/minor version that helps identifying them."
1,"NullPointerException during indexing in DocumentsWriter$ThreadState$FieldData.addPositionIn my case during indexing sometimes appear documents with unusually large ""words"" - text-encoded images in fact.
Attempt to add document that contains field with such token produces java.lang.IllegalArgumentException:
java.lang.IllegalArgumentException: term length 37944 exceeds max term length 16383
        at org.apache.lucene.index.DocumentsWriter$ThreadState$FieldData.addPosition(DocumentsWriter.java:1492)
        at org.apache.lucene.index.DocumentsWriter$ThreadState$FieldData.invertField(DocumentsWriter.java:1321)
        at org.apache.lucene.index.DocumentsWriter$ThreadState$FieldData.processField(DocumentsWriter.java:1247)
        at org.apache.lucene.index.DocumentsWriter$ThreadState.processDocument(DocumentsWriter.java:972)
        at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:2202)
        at org.apache.lucene.index.DocumentsWriter.addDocument(DocumentsWriter.java:2186)
        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1432)
        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1411)

This is expected, exception is caught and ignored. The problem is that after this IndexWriter becomes somewhat corrupted and subsequent attempts to add documents to the index fail as well, this time with NPE:
java.lang.NullPointerException
        at org.apache.lucene.index.DocumentsWriter$ThreadState$FieldData.addPosition(DocumentsWriter.java:1497)
        at org.apache.lucene.index.DocumentsWriter$ThreadState$FieldData.invertField(DocumentsWriter.java:1321)
        at org.apache.lucene.index.DocumentsWriter$ThreadState$FieldData.processField(DocumentsWriter.java:1247)
        at org.apache.lucene.index.DocumentsWriter$ThreadState.processDocument(DocumentsWriter.java:972)
        at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:2202)
        at org.apache.lucene.index.DocumentsWriter.addDocument(DocumentsWriter.java:2186)
        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1432)
        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1411)

This is 100% reproducible."
1,ItemManager.toString() causes StackOverflowError
1,"Contrib queries package Query implementations do not override equals()Query implementations should override equals() so that Query instances can be cached and that Filters can know if a Query has been used before.  See the discussion in this thread.

http://www.mail-archive.com/java-user@lucene.apache.org/msg13061.html

Following 3 contrib Query implementations do no override equals()

org.apache.lucene.search.BoostingQuery;
org.apache.lucene.search.FuzzyLikeThisQuery;
org.apache.lucene.search.similar.MoreLikeThisQuery;

Test cases below show the problem.

package com.teamware.office.lucene.search;

import static org.junit.Assert.*;

import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.index.Term;
import org.apache.lucene.search.BoostingQuery;
import org.apache.lucene.search.FuzzyLikeThisQuery;
import org.apache.lucene.search.TermQuery;
import org.apache.lucene.search.similar.MoreLikeThisQuery;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;
public class ContribQueriesEqualsTest
{
    /**
     * @throws java.lang.Exception
     */
    @Before
    public void setUp() throws Exception
    {
    }

    /**
     * @throws java.lang.Exception
     */
    @After
    public void tearDown() throws Exception
    {
    }
    
    /**
     *  Show that the BoostingQuery in the queries contrib package 
     *  does not implement equals() correctly.
     */
    @Test
    public void testBoostingQueryEquals()
    {
        TermQuery q1 = new TermQuery(new Term(""subject:"", ""java""));
        TermQuery q2 = new TermQuery(new Term(""subject:"", ""java""));
        assertEquals(""Two TermQueries with same attributes should be equal"", q1, q2);
        BoostingQuery bq1 = new BoostingQuery(q1, q2, 0.1f);
        BoostingQuery bq2 = new BoostingQuery(q1, q2, 0.1f);
        assertEquals(""BoostingQuery with same attributes is not equal"", bq1, bq2);
    }

    /**
     *  Show that the MoreLikeThisQuery in the queries contrib package 
     *  does not implement equals() correctly.
     */
    @Test
    public void testMoreLikeThisQueryEquals()
    {
        String moreLikeFields[] = new String[] {""subject"", ""body""};
        
        MoreLikeThisQuery mltq1 = new MoreLikeThisQuery(""java"", moreLikeFields, new StandardAnalyzer());
        MoreLikeThisQuery mltq2 = new MoreLikeThisQuery(""java"", moreLikeFields, new StandardAnalyzer());
        assertEquals(""MoreLikeThisQuery with same attributes is not equal"", mltq1, mltq2);
    }
    /**
     *  Show that the FuzzyLikeThisQuery in the queries contrib package 
     *  does not implement equals() correctly.
     */
    @Test
    public void testFuzzyLikeThisQueryEquals()
    {
        FuzzyLikeThisQuery fltq1 = new FuzzyLikeThisQuery(10, new StandardAnalyzer());
        fltq1.addTerms(""javi"", ""subject"", 0.5f, 2);
        FuzzyLikeThisQuery fltq2 = new FuzzyLikeThisQuery(10, new StandardAnalyzer());
        fltq2.addTerms(""javi"", ""subject"", 0.5f, 2);
        assertEquals(""FuzzyLikeThisQuery with same attributes is not equal"", fltq1, fltq2);
    }
}
"
0,ValueSourceQuery hits synchronization bottleneck in IndexReader.isDeletedI plan to fix it the same way we did in LUCENE-1316 for MatchAllDocsQuery (use TermDocs(null)).
0,Release the OCM componentThe contrib/jackrabbit-jcr-mapping/jcr-mapping should be promoted from contrib into a jackrabbit-jcr-ocm component.
0,"Add contrib libs to classpath for javadocI don't know Ant well enough to just do this easily, so I've labeled a wish - would be nice to get rid of all the errors/warnings that not finding these classes generates when building javadoc."
0,BytesRefHash#get() should expect a BytesRef instances for consistencyBytesRefHash#get should use a provided BytesRef instances instead of the internally used scratch. This is how all other APIs currently work and we should be consistent.
0,"Do not fulltext index jcr:uuid propertyThe UUID value of a referenceable nodes is currently add to the index four times:
- As a system field _:UUID
- As a property value for jcr:uuid
- As part of the node scope fulltext index
- As part of the jcr:uuid property scope fulltext index

In a repository with lots of referenceable nodes this bloates the index and does not add much value. Searching a node by its UUID is preferably done using the equal operator and not by using the jcr:contains() function.

I suggest to remove fulltext indexing of the jcr:uuid property. Existing indexes would still work after this change, new or updated nodes would simply not have the property jcr:uuid fulltext indexed anymore."
1,"Denying a primaryType does not work in XPathThe following query does not work:

//element(*, my:type)[jcr:contains(.,'foo') and @jcr:primaryType != 'nt:frozenNode')

The jcr:primaryType predicate does not respect the 'not equal' operation."
1,"unable to workspace import XML.tika detects xml as ""application/xml"" thus breaking the org.apache.jackrabbit.server.io.XmlHandler
which just checks for ""text/xml""."
0,"OOM in TestBeiderMorseFilter.testRandomThis has been OOM'ing a lot... we should see why, its likely a real bug.

ant test -Dtestcase=TestBeiderMorseFilter -Dtestmethod=testRandom -Dtests.seed=2e18f456e714be89:310bba5e8404100d:-3bd11277c22f4591 -Dtests.multiplier=3 -Dargs=""-Dfile.encoding=ISO8859-1"""
1,FastVectorHighlighter: out of alignment when the first value is empty in multiValued field
0,"Deprecate Analyzer.tokenStreamThe addition of reusableTokenStream to the core analyzers unfortunately broke back compat of external subclasses:

    http://www.nabble.com/Extending-StandardAnalyzer-considered-harmful-td23863822.html

On upgrading, such subclasses would silently not be used anymore, since Lucene's indexing invokes reusableTokenStream.

I think we should should at least deprecate Analyzer.tokenStream, today, so that users see deprecation warnings if their classes override this method.  But going forward when we want to change the API of core classes that are extended, I think we have to  introduce entirely new classes, to keep back compatibility."
0,"Create IndexWriterConfiguration and store all of IW configuration thereI would like to factor out of all IW configuration parameters into a single configuration class, which I propose to name IndexWriterConfiguration (or IndexWriterConfig). I want to store there almost everything besides the Directory, and to reduce all the ctors down to one: IndexWriter(Directory, IndexWriterConfiguration). What I was thinking of storing there are the following parameters:
* All of ctors parameters, except for Directory.
* The different setters where it makes sense. For example I still think infoStream should be set on IW directly.

I'm thinking that IWC should expose everything in a setter/getter methods, and defaults to whatever IW defaults today. Except for Analyzer which will need to be defined in the ctor of IWC and won't have a setter.

I am not sure why MaxFieldLength is required in all IW ctors, yet IW declares a DEFAULT (which is an int and not MaxFieldLength). Do we still think that 10000 should be the default? Why not default to UNLIMITED and otherwise let the application decide what LIMITED means for it? I would like to make MFL optional on IWC and default to something, and I hope that default will be UNLIMITED. We can document that on IWC, so that if anyone chooses to move to the new API, he should be aware of that ...

I plan to deprecate all the ctors and getters/setters and replace them by:
* One ctor as described above
* getIndexWriterConfiguration, or simply getConfig, which can then be queried for the setting of interest.
* About the setters, I think maybe we can just introduce a setConfig method which will override everything that is overridable today, except for Analyzer. So someone could do iw.getConfig().setSomething(); iw.setConfig(newConfig);
** The setters on IWC can return an IWC to allow chaining set calls ... so the above will turn into iw.setConfig(iw.getConfig().setSomething1().setSomething2()); 

BTW, this is needed for Parallel Indexing (see LUCENE-1879), but I think it will greatly simplify IW's API.

I'll start to work on a patch."
1,"Jcr-Server: registration of ReportTypes failsRegistration of ReportType(s) using 

ReportType.register(String localName, Namespace namespace, Class reportClass)  [ReportType]

fails due to wrong evaluation of interfaces implemented by the given class object.
"
0,better explain outputVery simple patch that slightly improves output of idf: show both docFreq and numDocs.
0,"jcr2spi: create ChangePolling thread on demandcurrently a new ChangePolling thread is created for every single session even if there is neither observation eventlistener nor cachebehaviour#observation.
i think we could create that on demand."
0,TestIndexwriterWithThreads#testCloseWithThreads hangs if a thread hit an exception before indexing its first documentin TestIndexwriterWithThreads#testCloseWithThreads we loop until all threads have indexed a single document but if one or more threads fail on before they index the first doc the test hangs forever. We should check if the thread is still alive unless it has indexed a document and fail if it already died.
1,"Infinite loop on basic authenticationClass org.apache.http.impl.client.DefaultRequestDirector has a bug whereby when Authentication fails if the log is not warnEnabled then you will receive a retry request and end up in an infinite loop retrying requests.. This occurred for me when SL4J was being picked up as the implementation but not properly configured.

In 4.1.2 the line number of the offending code is in the handleResponse method, line 1126, the return null statement requires moving outside of the if statement that checkes if the log is warn enabled."
0,"Remove/Uncommit SegmentingTokenizerBaseI added this class in LUCENE-3305 to support analyzers like Kuromoji,
but Kuromoji no longer needs it as of LUCENE-3767. So now nothing uses it.

I think we should uncommit before releasing, svn doesn't forget so
we can add this back if we want to refactor something like Thai or Smartcn
to use it."
0,SQL Azure support: clustered indexesWe tried to install JackRabbit in the Windows Azure cloud using SQL Azure. One of the limitations of SQL Azure is that it needs clustered indexes to work but the current implementation of the JackRabbit creates the indexes not clustered.
0,"Javadoc - Field constructor with Reader needs comment about retained referenceIf you don't dig into the Lucene internals, it isn't obvious the Field constructor http://lucene.apache.org/java/docs/api/org/apache/lucene/document/Field.html#Field%28java.lang.String,%20java.io.Reader%29 retains a reference to the reader for use later on. It would be useful to have a comment added to the Javadoc saying something like:

Note: A reference to java.io.Reader is retained by the field. Reader is read from when the Document which this field is added to is itself added to the index.

Without this, the caller is liable to do silly things like closing the stream after constructing the org.apache.lucene.document.Field."
0,"DocValues.type() -> DocValues.getType()This makes the method easier to find and more clear that it has no side effects... on a
few occasions I've looked for this getter and missed it because of the name.

"
0,"Add getPath method to Authorizable interfacecurrently the only way to retrieve the path of the item associated with an authorizable is to check if the
principal obtained through Authorizable#getPrincipal() is an ItemBasedPrincipal.

having a getPath method would provide a convenient shortcut and would in addition allow
to determine of there is really an item associated with a authorizable that is accessible for the editing
session (which is not necessarily the case for ItemBasedPrincipal#getPath"
1,"DatabaseJournal commits twice inside a transaction, causing an error with MySQLWhen committing a transaction in a clustered setup, multiple records may be appended to the DatabaseJournal. After having appended a record, commit() is called on the connection and auto-commit mode is again enabled. Apart from not being semantically correct, committing a connection that is already in auto-commit mode throws an error when using MySQL as backend."
0,"factor out a shared spellchecking moduleIn lucene's contrib we have spellchecking support (index-based spellchecker, directspellchecker, etc). 
we also have some things like pluggable comparators.

In solr we have auto-suggest support (with two implementations it looks like), some good utilities like HighFrequencyDictionary, etc.

I think spellchecking is really important... google has upped the ante to what users expect.
So I propose we combine all this stuff into a shared modules/spellchecker, which will make it easier
to refactor and improve the quality.
"
1,Repository does not release all resources on shutdownWhen Jackrabbit is shutdown some java.util.Timer threads are still running in the background even though no tasks are scheduled. This prevents the GC from collecting the classes when Jackrabbit is redeployed within a web application.
1,"LuceneQueryBuilder assumes readability of root-Node to be granted in any case.Have a User U. 
Have the User U denied to read ""/"".
Have the User U allowed to read ""/home/u"".

Any query of User U on this workspace fails with an AccessDeniedException.

The exception is caused by a call insided LuceneQueryBuilder on ln212:
NodeId id = ((NodeImpl) session.getRootNode()).getNodeId(); 

I couldn't find a specification that imposes the readability of root-node as a precondtion for query.
Therefore I consider this behavior as a bug."
1,"ProxyCredentials disclosed to remote hostI'm using httpclient (svn-trunk of today) to connect to a remote SSL-Host 
via a proxy. The proxy requires authorization (basic) and I want to use 
preemptive authorization. 
 
Since HTTPCLIENT-514 is fixed the preemptive authorization works, but my traces 
show that the proxy credentials are also transmitted to the remote host 
through the CONNECT-tunnel, thus disclosing sensitive information to the 
remote host. 
 
My code looks like this: 
 
HttpClient client = new HttpClient(); 
HttpMethod method = new GetMethod(""https://test""); 
 
client.getHostConfiguration().setProxy(""127.0.0.1"",3128); 
client.getState().setProxyCredentials( 
                new AuthScope(""127.0.0.1"", 3128), 
                new UsernamePasswordCredentials(""proxy"", ""test"")); 
client.getState().setAuthenticationPreemptive(true); 
client.executeMethod(method); 
 
The trace: 
 
2005/11/03 13:53:13:244 CET [DEBUG] HttpMethodDirector - Preemptively 
sending default basic credentials 
2005/11/03 13:53:13:261 CET [DEBUG] HttpMethodDirector - Authenticating 
with BASIC <any realm>@127.0.0.1:3128 
2005/11/03 13:53:13:262 CET [DEBUG] HttpMethodParams - Credential charset 
not configured, using HTTP element charset 
2005/11/03 13:53:13:266 CET [DEBUG] HttpMethodDirector - Authenticating 
with BASIC <any realm>@test:443 
2005/11/03 13:53:13:267 CET [WARN] HttpMethodDirector - Required 
credentials not available for BASIC <any realm>@test:443 
2005/11/03 13:53:13:267 CET [WARN] HttpMethodDirector - Preemptive 
authentication requested but no default credentials available 
2005/11/03 13:53:13:268 CET [DEBUG] HttpConnection - Open connection to 
127.0.0.1:3128 
2005/11/03 13:53:13:279 CET [DEBUG] HttpMethodDirector - Preemptively 
sending default basic credentials 
2005/11/03 13:53:13:280 CET [DEBUG] HttpMethodDirector - Authenticating 
with BASIC <any realm>@127.0.0.1:3128 
2005/11/03 13:53:13:280 CET [DEBUG] HttpMethodParams - Credential charset 
not configured, using HTTP element charset 
2005/11/03 13:53:13:283 CET [DEBUG] header - >> ""CONNECT test:443 HTTP/1.1"" 
2005/11/03 13:53:13:284 CET [DEBUG] HttpMethodBase - Adding Host request 
header 
2005/11/03 13:53:13:284 CET [DEBUG] header - >> ""Proxy-Authorization: 
Basic cHJveHk6dGVzdA==[\r][\n]"" 
2005/11/03 13:53:13:285 CET [DEBUG] header - >> ""User-Agent: Jakarta 
Commons-HttpClient/3.0-rc4[\r][\n]"" 
2005/11/03 13:53:13:285 CET [DEBUG] header - >> ""Host: test[\r][\n]""       
                                                                           
2005/11/03 13:53:13:286 CET [DEBUG] header - >> ""Proxy-Connection: 
Keep-Alive[\r][\n]"" 
2005/11/03 13:53:13:286 CET [DEBUG] header - >> ""[\r][\n]""                 
                                                                         
2005/11/03 13:53:13:311 CET [DEBUG] header - << ""HTTP/1.0 200 
Connection established[\r][\n]""                                            
2005/11/03 13:53:13:326 CET [DEBUG] ConnectMethod - CONNECT status code 200 
2005/11/03 13:53:13:327 CET [DEBUG] HttpConnection - Secure tunnel to 
test:443 
2005/11/03 13:53:13:418 CET [DEBUG] header - >> ""GET / HTTP/1.1[\r][\n]"" 
2005/11/03 13:53:13:420 CET [DEBUG] HttpMethodBase - Adding Host request 
header 
2005/11/03 13:53:13:423 CET [DEBUG] header - >> ""Proxy-Authorization: 
Basic cHJveHk6dGVzdA==[\r][\n]"" 
2005/11/03 13:53:13:424 CET [DEBUG] header - >> ""User-Agent: Jakarta 
Commons-HttpClient/3.0-rc4[\r][\n]"" 
2005/11/03 13:53:13:425 CET [DEBUG] header - >> ""Host: test[\r][\n]"" 
2005/11/03 13:53:13:425 CET [DEBUG] header - >> ""[\r][\n]"" 
2005/11/03 13:53:14:391 CET [DEBUG] header - << ""HTTP/1.1 200 OK[\r][\n]"" 
 
As you can see the proxy credentials are also transmitted through the 
SSL-tunnel to the remote host which is a security risk."
0,"Add UserManager.createGroup(String groupID) methodAs discussed on the dev list [1] I think it would be useful (and consistent inside the API) to have a UserManager.createGroup(String groupID) method.

The specification of the method would be:

    /**
     * Creates a Group for the given groupID must not be <code>null</code>.
     * <br>
     * Same as {@link #createGroup(Principal,String)} where the specified groupID
     * is the name of a simple <code>Principal</code> implementation and the
     * intermediate path is <code>null</code>.
     *
     * @param groupID The id of the new group, must not be <code>null</code>.
     * @return The new <code>Group</code>.
     * @throws AuthorizableExistsException in case the given groupID is already
     * in use or another {@link Authorizable} with the same
     * {@link Authorizable#getID() ID} exists.
     * @throws RepositoryException If another error occurs.
     */
    Group createGroup(String groupID) throws AuthorizableExistsException, RepositoryException;

[1] http://markmail.org/message/rjofzg4t3kiht7xv"
0,"Add SearcherLifetimeManager, so you can retrieve the same searcher you previously usedThe idea is similar to SOLR-2809 (adding searcher leases to Solr).

This utility class sits above whatever your source is for ""the
current"" searcher (eg NRTManager, SearcherManager, etc.), and records
(holds a reference to) each searcher in recent history.

The idea is to ensure that when a user does a follow-on action (clicks
next page, drills down/up), or when two or more searcher invocations
within a single user search need to happen against the same searcher
(eg in distributed search), you can retrieve the same searcher you
used ""last time"".

I think with the new searchAfter API (LUCENE-2215), doing follow-on
searches on the same searcher is more important, since the ""bottom""
(score/docID) held for that API can easily shift when a new searcher
is opened.

When you do a ""new"" search, you record the searcher you used with the
manager, and it returns to you a long token (currently just the
IR.getVersion()), which you can later use to retrieve the same
searcher.

Separately you must periodically call prune(), to prune the old
searchers, ideally from the same thread / at the same time that
you open a new searcher."
1,TieredMergePolicy expungeDeletes should not enforce maxMergedSegmentMB
0,"Add test for Node.restore() may throw InvalidStateExceptionAdd a unit test for JCR-1399 in the 1.3 branch.

A test for the original feature in the trunk/1.4 (JCR-1197) needs a separate issue. "
0,"jcr2spi: reloading of invalidated nodes doesn't benefit from batch-readissue reported by stefan:

upon reloading of an invalidated node, jcr2spi doesn't use batch-read but only retrieves the NodeInfo.

basically, WorkspaceItemStateFactory ll 92 needs to be changed to omit the extra handling of 
invalidated node entries and always use RepositoryService.getItemInfos when build the NodeState.

i will try it out as soon as possible.

"
0,"src builds fail because of no ""lib"" directoryI just downloaded http://mirrors.ibiblio.org/pub/mirrors/apache/lucene/java/lucene-2.0.0-src.tar.gz and noticed that you can't compile and run the tests from that src build because it doesn't inlcude the lib dir (and the build file won't attempt to make it if it doesn't exist) ...

hossman@coaster:~/tmp/l2$ tar -xzvf lucene-2.0.0-src.tar.gz
  ...
hossman@coaster:~/tmp/l2$ cd lucene-2.0.0/
hossman@coaster:~/tmp/l2/lucene-2.0.0$ ant test
  ...
test:
    [mkdir] Created dir: /home/hossman/tmp/l2/lucene-2.0.0/build/test

BUILD FAILED
/home/hossman/tmp/l2/lucene-2.0.0/common-build.xml:169: /home/hossman/tmp/l2/lucene-2.0.0/lib not found.

(it's refrenced in junit.classpath, but i'm not relaly sure why)

"
1,"NPE in classes of OJB-PMNPE occurs while accessing the Id of the parent of the root node which is null.

Patch follows"
0,"IndexWriter should detect when it's used after being closedSpinoff from this thread on java-user:

    http://www.gossamer-threads.com/lists/lucene/java-user/45986

If you call addDocument on IndexWriter after it's closed you'll hit a
hard-to-explain NullPointerException (because the RAMDirectory was
closed).  Before 2.1, apparently you won't hit any exception and the
IndexWrite will keep running but will have released it's write lock (I
think).

I plan to fix IndexWriter methods to throw an IllegalStateException if
it has been closed.
"
0,"Mandatory authentication prevents webdav client connectionsAs seen on the mailing list:

java -jar target/jackrabbit-standalone-2.3-SNAPSHOT.jar --cli http://localhost:8080
Exception in thread ""main"" javax.jcr.RepositoryException: Unable to access a repository with the following settings:
   org.apache.jackrabbit.repository.uri: http://localhost:8080
The following RepositoryFactory classes were consulted:
   org.apache.jackrabbit.jcr2dav.Jcr2davRepositoryFactory: declined
   org.apache.jackrabbit.jcr2spi.Jcr2spiRepositoryFactory: declined
   org.apache.jackrabbit.commons.JndiRepositoryFactory: declined
   org.apache.jackrabbit.core.RepositoryFactoryImpl: declined
   org.apache.jackrabbit.rmi.repository.RmiRepositoryFactory: failed
       because of RepositoryException: Failed to read the resource at URL http://localhost:8080
       because of IOException: Server returned HTTP response code: 401 for URL: http://localhost:8080
Perhaps the repository you are trying to access is not available at the moment.
       at org.apache.jackrabbit.commons.JcrUtils.getRepository(JcrUtils.java:216)
       at org.apache.jackrabbit.commons.JcrUtils.getRepository(JcrUtils.java:256)
       at org.apache.jackrabbit.standalone.Main.run(Main.java:127)
       at org.apache.jackrabbit.standalone.Main.main(Main.java:61)"
1,"TaxonomyWriter parents array creation is not thread safe, can cause NPEFollowing user list thread [TaxWriter leakage? | http://markmail.org/thread/jkkhemfzpnbdzoft] it appears that if two threads or more are asking for the parent array for the first time, a context switch after the first thread created the empty parents array but before it initialized it would cause the other array to use an uninitialized array, causing an NPE. Fix is simple: synchronize the method getParentArray()"
0,"Create Jcr-Client Moduletask copied from JCR-1877:

i think it would be wise to create a new module that mainly consists of a RepositoryFactory and combines jackrabbit-jcr2spi with the known (and also any other) spi implementations."
1,"BLOBFileValue() might be discarded to earlySituation:

if the internal value of a property of type binary is created by the constructor BLOBFileValue(InputStream in) and the content is not stored in an temp-file, then calling the methods 

a) #setProperty(InputStream in) on this node and then
b) #refresh(false) on the node of this property 

on the node of this property leads to an internal value of this property with an erased byte[].

Solution:

Only if the spoolFile is created the field 'temp' should be set to true.
If the InputStream is stored in the byte[] the field 'temp' should be set to false.

Patch:

Index: BLOBFileValue.java
===================================================================
retrieving revision 1.1
diff -u -r1.1 BLOBFileValue.java
--- BLOBFileValue.java	8 May 2006 13:57:49 -0000	1.1
+++ BLOBFileValue.java	8 May 2006 15:19:54 -0000
@@ -142,6 +142,7 @@
                     len += read;
                 }
             }
+            in.close();
         } finally {
             if (out != null) {
                 out.close();
@@ -151,8 +152,15 @@
         // init vars
         file = spoolFile;
         fsResource = null;
-        // this instance is backed by a temporarily allocated resource/buffer
-        temp = true;
+        if (file != null)
+        {
+            // this instance is backed by a temporarily allocated resource
+            temp = true;
+        }
+        else
+        {
+            temp = true;
+        }
     }
 
     /**


"
0,Better 'invalid format' exception messages for value classesThe valueOf() methods of the Value classes throw an exception without information on the desired type and without the String value that gave the error.
0,"StopFilter should not create a new CharArraySet if the given set is already an instance of CharArraySetWith LUCENE-2094 a new CharArraySet is created no matter what type of set is passed to StopFilter. This does not behave as  documented and could introduce serious performance problems. Yet, according to the javadoc, the instance of CharArraySet should be passed to CharArraySet.copy (which is very fast for CharArraySet instances) instead of ""copied"" via ""new CharArraySet()"""
1,"NPE when calling isCurrent() on a ParallellReaderAs demonstrated by the test case below, if you call isCurrent() on a ParallelReader it causes an NPE. Fix appears to be to add an isCurrent() to ParallelReader which calls it on the underlying indexes but I'm not sure what other problems may be lurking here. Do methods such as getVersion(), lastModified(), isOptimized() also have to be rewritten or is this a use case where ParallelReader will never mimic IndexReader perfectly? At the very least this behavior should be documented so others know what to expect.


    [junit] Testcase: testIsCurrent(org.apache.lucene.index.TestParallelReader):        Caused an ERROR
    [junit] null
    [junit] java.lang.NullPointerException
    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:502)
    [junit]     at org.apache.lucene.index.SegmentInfos.readCurrentVersion(SegmentInfos.java:336)
    [junit]     at org.apache.lucene.index.IndexReader.isCurrent(IndexReader.java:316)
    [junit]     at org.apache.lucene.index.TestParallelReader.testIsCurrent(TestParallelReader.java:146)



Index: src/test/org/apache/lucene/index/TestParallelReader.java
===================================================================
--- src/test/org/apache/lucene/index/TestParallelReader.java    (revision 518122)
+++ src/test/org/apache/lucene/index/TestParallelReader.java    (working copy)
@@ -135,6 +135,15 @@
       assertEquals(docParallel.get(""f4""), docSingle.get(""f4""));
     }
   }
+  
+  public void testIsCurrent() throws IOException {
+    Directory dir1 = getDir1();
+    Directory dir2 = getDir2();
+    ParallelReader pr = new ParallelReader();
+    pr.add(IndexReader.open(dir1));
+    pr.add(IndexReader.open(dir2));
+    assertTrue(pr.isCurrent());
+  }
 
   // Fiels 1-4 indexed together:
   private Searcher single() throws IOException {
"
1,"DescendantSelfAxisQuery may fail with IOException when session has limited accessThe DescendantSelfAxisQuery uses the current session to look up nodes by id. When the session does not have access to a node the exception is incorrectly re-thrown an IOException. Instead, any ItemNotFoundException should be caught and ignored. This is probably a regression caused by JCR-1365 introduced with Jackrabbit 1.5."
0,"Contribute Pluggable Permission and User Management to JackrabbitWorking with a Jackrabbit based appliction I had to extend its security handling.
The aim of this extension has been to allow for a eitable resource based authorization.
The solution ended up in beeing plugable and extendable.
As there have been some questions in the Jackrabbit Developper-list about custom implementation of security or the management of privileges in Jackrabbit, I like to suggest my implementation as contribution with attached patches. 

Below you can find some high-level explanation of the contained files and concepts

I hope the prove to be usable and enhance this great repository.
I welcome your feed-back and like to thank for your kind inspection

Regards
Christian Keller

The patch contains the following:
=========================

1) API [jackrabbit-core-changes.20071010.patch]
-------------------------------------------------------------------
API which allows to implement and configure a mechanisms for Authentication and Authorization. 
The API is ACL- and Principal-based.
ACL and Principals Management is independent of the JCR api, to allow implementations to use different back-end systems like a Directory Server.

2) Changes to current core [jackrabbit-core-changes.20071010.patch]
-----------------------------------------------------------------------------------------------
Some small changes have been necessary to core to enable configuration and access of Management, like session access to UserManager.

3) Implementation [jackrabbit-core-implementation.20071010.patch]
-----------------------------------------------------------------------------------------------
Additionally an implemenation is contained. It is not dependent on any back-end system, and may therefore be used as a default.


Description:
==========
The extensions hook into Jackrabbit bei implementations of the Intefaces: AccessManager and LoginModule. 
Additionally there are changes for configuration, set-up and access of the used Object.

The patch extends the API, in order to allow client inspections of Users and Permission. These are contained in the api.patch

See a short Introduction below:
=========================

The Security extensions of this Patch contain both, Authentication and Authorization extensions for which the follwoing two modells are introduced:

I) The Authorizable
----------------------------
These are User's and Groups of Users. Users can authenticate. 
Authentication in Jackrabbit is done by LoginModules which issue Principals as result of an Authentication.
The Users are the objects which can be represented by such an Principal
They are therfore are the base for the Authorization.

II) The ACL
----------------
The ACL is the Policy for Authorziation. 
The ACL grants or denies a Principal Privileges which are called Actions.

Additional ther is a Management for Principals:

The Principal is the link between User and permission.
A User may related to multiple Principals. As this dependes on the LoginModules verfiying the Idendity of the login-attemp.
The LoginModules may expose their Principals to the Repository via a Provider interface, to allow for usage in ACEs.

All Modells and their Managing Classes API's are abstracted from the fact, that they are used in a JC-Repository. Aka there is no reference to javax.jcr.Items, Sessions etc.
This should allow to implement both for external sources for both without imposing any JCR specific methods. Taken an LDAP as UserBase for example.

The managing classes are UserManger, PrincpalManager and ACLManager. 
They are set-up and maintained by a repsoitory singular SecurityManger. 
Session specific versions of this Managers are exposed via Session.

PrincipalManger and ACLManger are feed by one to multiple Providers. 
PrincipalProviders may exist per LoginModule, ACLProvider per Workspace.

Authentication:
--------------------
The User will be used by the LoginModule. It will be resolved based on the given Credentials. If the Credentials can be validated, the User will be used to resolve Principals according its Group-Membership. As a result the Session's Subject will be extended by this principals.

Authorization:
-------------------
The ACL will be use be an Implementation of the AccessManager-Interface
An ACLManger relates Items to ACLs and the ACL evaluates the Permission for the current Subject's Principals.

Default Implementation
===================
The Default Implementation uses the Repository itself to store its security data.
The Users are stored within a dedicated workspace. 
The ACL are attached to the Nodes they relate to.
The ACLs are inherited along the Item-Hierarchy.
The Principals are taken from the Authorables.

Configuration
===========
The LoginModules may declare their PrincipalProvider class via a property key with the name ""principal_provider.class""

The Workspace specific ACL Providers may be added via a configuration element in Worskspace.xml, called WorkspaceSecurity.
A Factory class can be configured there."
0,"Wrong method signatures in AbstractHttpClientThe method signatures for removeRequestInterceptorByClass and removeResponseInterceptorByClass in AbstractHttpClient are wrong. Must be

public void removeRequestInterceptorByClass(Class<? extends
HttpRequestInterceptor> clazz);

and

public void removeResponseInterceptorByClass(Class<? extends
HttpRequestInterceptor> clazz);"
1,"CookieSpec.formatCookie(Cookie) produces an incorrect cookie header valueConsider the following:
----------------------------------------------------------------------
Cookie cookie = new Cookie("".foo.com"", ""name"", ""value"");
cookie.setVersion(1);
cookie.setPath(""/"");
CookieSpec spec = CookiePolicy.getSpecByPolicy(CookiePolicy.RFC2109);
System.out.println(spec.formatCookie(cookie));                
----------------------------------------------------------------------

When calling CookieSpec.formatCookie(Cookie) the resulting output is:

   name=""value""

The Version attribute is not present as required by RFC2109, nor is the path or
domain information included.

It seems that in this case, only Cookie type 0 output is produced."
0,"Remove deprecated Scorer.explain(int) methodThis is the only remaining deprecation in core, but is not so easy to handle, because lot's of code in core still uses the explain() method in Scorer. So e.g. in PhraseQuery, the explain method has to be moved from Scorer to the Weight."
0,"HTTPClient doesn't send authentication header in threaded environmentUsing HTTPClient with multiple threads and basic authentication seems to create a race condition. The request headers sometimes don't contain the authorization entry, which results in a 401 (although the username and password credentials are correctly set). "
0,"jira notificationsI have set up Jackrabbit's jira so that notifications will
go to the dev mailing list.  If that gets annoying, I can easily
switch it to a different list or turn off notifications such that
people have to register as watchers.

Let me know what is best for you.

....Roy
"
1,"Requests are retried 3 times unconditionalyUsing the 20020811 tarball and jdk1.4.0, a get or post will retry as soon
as it finishes sending the request. I turned on logging and verified that
as soon as the last \r\n hits the wire, it starts on the next retry. For
example:

08-10 09:53:12 [main] httpclient.wire: >> [\r\n]
08-10 09:53:12 [main] httpclient.methods.PostMethod: enter
PostMethod.writeRequestBody(HttpState, HttpConnection)
08-10 09:53:12 [main] commons.httpclient.HttpConnection: enter
HttpConnection.write(byte[], int, int)
08-10 09:53:12 [main] commons.httpclient.HttpMethod: Attempt number 3 to write
request
08-10 09:53:12 [main] commons.httpclient.HttpMethod: enter
HttpMethodBase.writeRequest(HttpState, HttpConnection)
08-10 09:53:12 [main] commons.httpclient.HttpMethod: enter
HttpMethodBase.writeRequestLine(HttpState, HttpConnection)
08-10 09:53:12 [main] commons.httpclient.HttpMethod: enter
HttpMethodBase.generateRequestLine(HttpConnection, String, String, String,
String)
08-10 09:53:12 [main] commons.httpclient.HttpConnection: enter
HttpConnection.print(String)
08-10 09:53:12 [main] commons.httpclient.HttpConnection: enter
HttpConnection.write(byte[])
08-10 09:53:12 [main] httpclient.wire: >> ""POST /lookup.jsp HTTP/1.1"" [\r\n]

The top line is the end of the second post and the last line is the start
of the third post.

To make sure the server really wasn't sending something back, I wrote a
quick server that would listen for a request and send a 404 as soon as it
read a post or get line (but would keep reading and dumping info). In the
httpclient log, it still shoots off 3 requests before it receives the
response and the server got all three requests. (client and server are
running on the same machine)

So why is httpclient sending three requests without waiting for a
response?"
0,Replace SegmentReader.Ref with AtomicIntegerI think the patch should be applied to backcompat tag in its entirety.
0,"Update idea plugin versionWe are using a quite outdated version (2.0). The most recent idea plugin release is 2.2.

Index: jackrabbit-parent/pom.xml
===================================================================
--- jackrabbit-parent/pom.xml	(revision 802755)
+++ jackrabbit-parent/pom.xml	(working copy)
@@ -73,7 +73,7 @@
       <plugin>
         <!-- http://maven.apache.org/plugins/maven-idea-plugin/ -->
         <artifactId>maven-idea-plugin</artifactId>
-        <version>2.0</version>
+        <version>2.2</version>
         <configuration>
           <downloadSources>true</downloadSources>
           <jdkLevel>1.5</jdkLevel>
"
0,"Provide access to SSLSession in ManagedClientConnectionProvide access to the wrappedConnection in org.apache.http.impl.conn.AbstractClientConnAdapter via some interface in order to access the socket from within an HttpProcessor. Currently the org.apache.http.conn.OperatedClientConnection has a getSocket() method, but the connection implementation returned by

  context.getAttribute(ExecutionContext.HTTP_CONNECTION) 

(org.apache.http.impl.conn.tsccm.BasicPooledConnAdapter) does not provide access to the wrappedConnection."
0,"Build.xml - add log level definitionsThe default log level is debug, which produces quite a lot of output when testing.

The patch allows separate definition of wire and other log levels (assuming SimpleLog is used)"
1,JCARepositoryManager does not close InputStream used to obtain repository config from classpath
1,"CLONE -ManageableCollectionUtil doesn't support MapsManageableCollectionUtil has two getManageableCollection methods, which do not currently return a ManageableCollection which wraps Maps. 

ManagedHashMap already exists in the codebase which I assume was created for this purpose, so both getManageableCollection methods could be modified so that they do something like:

            if (object instanceof Map){
                return new ManagedHashMap((Map)object);
            }


An alternative solution might be to modify the JCR mapping to support explicitly defining the 'ManagedXXX' class."
0,"AbstractHttpClient.addRequestInterceptor should document in what order the interceptors runhttp://hc.apache.org/httpcomponents-client/httpclient/apidocs/org/apache/http/impl/client/AbstractHttpClient.html#addRequestInterceptor(org.apache.http.HttpRequestInterceptor) has no documentation. It should at least say what order new interceptors run in. Presumably they run in order by index, but does the lowest or highest index run first?

This class or DefaultHttpClient should also say what interceptors are added by default. That is, what would I be getting rid of by calling clearResponseInterceptors()?"
0,Get rid of (another) hard coded path
1,"Incorrect usage of AttributeSource.addAttribute/getAttribute leads to failures when onlyUseNewAPI=truewhen seting ""use only new API"" for TokenStream, i received the following exception:

{code}
   [junit] Caused by: java.lang.IllegalArgumentException: This AttributeSource does not have the attribute 'interface org.apache.lucene.analysis.tokenattributes.TermAttribute'.
    [junit] 	at org.apache.lucene.util.AttributeSource.getAttribute(AttributeSource.java:249)
    [junit] 	at org.apache.lucene.index.TermsHashPerField.start(TermsHashPerField.java:252)
    [junit] 	at org.apache.lucene.index.DocInverterPerField.processFields(DocInverterPerField.java:145)
    [junit] 	at org.apache.lucene.index.DocFieldProcessorPerThread.processDocument(DocFieldProcessorPerThread.java:244)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:772)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:755)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:2613)
{code}

However, i can't actually see the culprit that caused this exception

suggest that the IllegalArgumentException include ""getClass().getName()"" in order to be able to identify which TokenStream implementation actually caused this
"
1,"PostMethod#setParameter[HttpClient2.0-rc1]

-------- code fragment 1 -------------------------
PostMethod method = new PostMethod(uriString);
method.addParameter(""tel"", ""1111-1111"");
method.addParameter(""tel"", ""2222-2222"");
method.setParameter(""tel"", ""3333-3333"");

(post data sent)
tel=1111-1111&tel=2222-2222&tel=3333-3333

(post data i hope)
tel=3333-3333
-------------------------------------------------

---------------- code fragment 2 -----------------
PostMethod method = new PostMethod(uriString);
method.addParameter(""tel"", ""1111-1111"");
method.addParameter(""tel"", ""2222-2222"");
method.addParameter(""tel"", ""3333-3333"");

(post data sent)
tel=1111-1111&tel=2222-2222&tel=3333-3333
--------------------------------------------------

what difference between code 1 and code2 ?

sorry for my poor english."
0,"ByteArrayBody as an alternative to InputStreamBodyInputStreamBody can not determine the content length, which in turn causes requests to be sent with a content length of 0, even though the content is there. .NET Servers have trouble dealing with this.

ByteArrayBody provides an alternative that alliviates this limitation.

Source:
 
import java.io.IOException;
import java.io.OutputStream;

import org.apache.http.entity.mime.MIME;
import org.apache.http.entity.mime.content.AbstractContentBody;

/**
 * Body part that is built using a byte array containing a file.
 * 
 * @author Axel Fontaine
 */
public class ByteArrayBody extends AbstractContentBody {
    /**
     * The contents of the file contained in this part.
     */
    private byte[] data;

    /**
     * The name of the file contained in this part.
     */
    private String filename;
    
    /**
     * Creates a new ByteArrayBody.
     * 
     * @param data The contents of the file contained in this part.
     * @param mimeType The mime type of the file contained in this part.
     * @param filename The name of the file contained in this part.
     */
    public ByteArrayBody(final byte[] data, final String mimeType, final String filename) {
        super(mimeType);
        if (data == null) {
            throw new IllegalArgumentException(""byte[] may not be null"");
        }
        this.data = data;
        this.filename = filename;
    }

    /**
     * Creates a new ByteArrayBody.
     * 
     * @param data The contents of the file contained in this part.
     * @param filename The name of the file contained in this part.
     */
    public ByteArrayBody(final byte[] data, final String filename) {
        this(data, ""application/octet-stream"", filename);
    }

    @Override
    public String getFilename() {
        return filename;
    }

    @Override
    public void writeTo(OutputStream out) throws IOException {
        out.write(data);
    }

    @Override
    public String getCharset() {
        return null;
    }

    @Override
    public String getTransferEncoding() {
        return MIME.ENC_BINARY;
    }

    @Override
    public long getContentLength() {
        return data.length;
    }
}
"
0,extract test content loading from JackrabbitRepositoryStubdiscussed here: http://markmail.org/message/vl5ldnfbocccccxw
0,"Error when registering nodetype with same propdef as supertypeerror in check:

                                if (pd.getRequiredType() == epd.getRequiredType()
                                        && pd.isMultiple() == epd.isMultiple()) {
                                    // conflict
                                    String msg = ""The property definition for '""
                                            + name + ""' in node type '""
                                            + def.getDeclaringNodeType()
                                            + ""' conflicts with node type '""
                                            + existingDef.getDeclaringNodeType()
                                            + ""': ambiguous property definition"";
                                    log.debug(msg);
                                    throw new NodeTypeConflictException(msg);
                                }

if needs to be inverted."
0,"PathHierarchyTokenizer adaptation for urls: splits reversed{{PathHierarchyTokenizer}} should be usable to split urls the a ""reversed"" way (useful for faceted search against urls):
{{www.site.com}} -> {{www.site.com, site.com, com}}

Moreover, it should be able to skip a given number of first (or last, if reversed) tokens:
{{/usr/share/doc/somesoftware/INTERESTING/PART}}
Should give with 4 tokens skipped:
{{INTERESTING}}
{{INTERESTING/PART}}"
1,"Exception in DocumentsWriter.ThreadState.init leads to corruptionIf an exception is hit in the init method, DocumentsWriter incorrectly
increments numDocsInRAM when in fact the document is not added.

Spinoff of this thread:

  http://markmail.org/message/e76hgkgldxhakuaa

The root cause that led to the exception in init was actually due to
incorrect use of Lucene's APIs (one thread still modifying the
Document while IndexWriter.addDocument is adding it) but still we
should protect against any exceptions coming out of init.

"
1,"inconsistent repository after overlapping node add operationsIt seems I can reproduce a sequence of operations that cause the repository to be inconsistent.

The short version: 2 sessions add a same-named child node to the same parent folder (not allowing same-name-siblings). Session 1's save() succeeds. Session 2's save() fails, but succeeds on retry (!).

After the operation, the child node created by session 1 is still present, but the parent doesn't list it as child node anymore.

(will add test case)"
0,"[PATCH] Better ""lock obtain timed out"" error messageThe attached patch prints the complete path and name of the lock file. This 
should simplify debugging (it's actually a wish from the Wiki)."
0,"Upgrade to Logback 1.0Logback 1.0 was just released (see http://mailman.qos.ch/pipermail/announce/2011/000093.html). There are no big new features or other major changes, but the bump to 1.0 is still a good point for us to upgrade to get all the latest bug fixes and other improvements.

At the same time we should upgrade our SLF4J depedency to the latest 1.6.4 version."
0,Allow o.a.j.jca.JCARepositoryManager to load repository configuration from the classpath.The current implementation of o.a.j.jca.JCARepositoryManager is only able to load configuration files from the file system. It would be useful to allow the configuration to be loaded from the classpath also.
1,"[PATCH] Ordered spanquery with slop can failIn CVS of 7 April 2004. 
An ordered SpanQuery with slop 1 querying: w1 w2 w3 
in document: w1 w3 w2 w3 
fails. It should match as: w1 . w2 w3"
0,"multipart feedbacknever got a reply on this from 10/20/02 mailing to email address in ""author"" tag, so posting here.
-----------------------
Matt and Jeff,

Excuse me for writing directly to the addresses found in the code as authors.  Please feel free to forward this to any list that is more appropriate.

Thank you very much for your efforts in making an HTTP client for Java.  It will find great use.  Below are some aspects of the org.apache.commons.httpclient.methods.multipart package that I'd like you to consider.  

First, consider making the encoding a parameter.  Currently, the content disposition and other general-purpose headers are written with a String.getBytes () call, which will use the default encoding on whatever client is being used by the customer.  Does the RFC and for HTTP post specify an encoding for these lines?  Perhaps the header information and disposition information should be a standard UTF-8 encoding, and an additional parameter could specify encoding for anything supplied by the users of the library, most notably the StringPart.

Second, consider that the content length header may not be important for many contexts.  When you receive a post on the server side, can you depend on the content length header?  Some browsers do not supplied this header, and even if all of them did, would you be wise to believe it on the server side?  In fact, common libraries for handling post, most notably http://www.servlets.com/cos/index.html , ignore any content length header that is supplied by the client.  On the server side, content length is calculated from the actual bytes that are received.

Why do I mention this?  Because it appears that a trade-off has been made in this alpha code such that the content length calculation was more important than polymorphism in Part.java:

    /* The following 2 methods don't need to be final, but they DO need
     * to be overridden as a pair, and the only way to make sure of that
     * is to make sure they AREN'T overridden. 
     */

    final public void send(OutputStream out) throws IOException {
        sendStart(out);
        sendHeader(out);
        sendEndOfHeader(out);
        sendData(out);
        sendEnd(out);
    }
    
    final public long length() throws IOException {
        return lengthOfStart() + 
               lengthOfHeader() + 
               lengthOfEndOfHeader() +
               lengthOfData() + 
               lengthOfEnd();
    }

The method send() seems like an important method to be able to override.  For example, consider a situation where the post content is zipped on-the-fly.  The content length is not known when writing the headers.  Further, it would be handy to override certain methods like send() in order to manipulate the output stream. 

Basically, since your library will be very general purpose and used widely, the more you can do for easy polymorphism, the more your customers will appreciate your library. Is there a way to make the content length calculation and header writing more flexible, so that it may be avoided when it is not known a priori?

Third, consider making the content type a parameter for a FilePart.  In the example above, the content type for the zipped file should be ""application/x-zip-compressed"" rather than ""application/octet-stream"".

Again, I was very happy to find your excellent work on this library. Thank you for your contributions to apache jakarta.

larry hamel"
1,"NodeTypeRegistry.registerNodeType(NodeTypeDef) does not verify that the referenced namespaces are registeredcurrently it's possible to register a node type using a defintion that contains references to unregistered namespaces.

using such a  node type in content would lead to unpredictable results."
1,"jcr2spi:  Remove sanityCheck() from ItemImpl.getSession()same as JCR-911 for jcr2spi.

the check was responsible for the failure of ActivitiesTest#testActivitiesRelation"
0,Remove some unused code in Surround query parser
1,"StandardBenchmarker#makeDocument does not explicitly close opened filesStandardBenchmarker#makeDocument(File in, String[] tags, boolean stored, boolean tokenized, boolean tfv)

        BufferedReader reader = new BufferedReader(new FileReader(in));

Above reader is not closed until GC hits it. Can cause problems in cases where ulimit is set too low.

I did this:

        while ((line = reader.readLine()) != null)
        {
            body.append(line).append(' ');
        }
+        reader.close();"
0,"Track FieldInfo per segment instead of per-IW-sessionCurrently FieldInfo is tracked per IW session to guarantee consistent global field-naming / ordering. IW carries FI instances over from previous segments which also carries over field properties like isIndexed etc. While having consistent field ordering per IW session appears to be important due to bulk merging stored fields etc. carrying over other properties might become problematic with Lucene's Codec support.  Codecs that rely on consistent properties in FI will fail if FI properties are carried over.

The DocValuesCodec (DocValuesBranch) for instance writes files per segment and field (using the field id within the file name). Yet, if a segment has no DocValues indexed in a particular segment but a previous segment in the same IW session had DocValues, FieldInfo#docValues will be true  since those values are reused from previous segments. 

We already work around this ""limitation"" in SegmentInfo with properties like hasVectors or hasProx which is really something we should manage per Codec & Segment. Ideally FieldInfo would be managed per Segment and Codec such that its properties are valid per segment. It also seems to be necessary to bind FieldInfoS to SegmentInfo logically since its really just per segment metadata.  "
0,"Cookie rejectedHello,

I'm using HttpClient 1.0 rc2 to login in the SourceForge website and perform
some operations, but i'm getting the following error:

Page 1 from https://sourceforge.net/account/login.php
6 dc. 2003 23:45:27 org.apache.commons.httpclient.HttpMethodBase
processResponseHeaders
ATTENTION: Cookie rejected: ""username=l6qpwtK5hpE%3D"". Illegal domain attribute
"".sourceforge.net"". Domain of origin: ""sourceforge.net""
6 dc. 2003 23:45:27 org.apache.commons.httpclient.HttpMethodBase readResponseBody

The cookie returned by Sourceforge is rejected because the domain for the
request was sourceforge.net and the domain for the cookie was .sourceforge.net
I have tried to change the cookie policy to all available options, but none work. 
CookiePolicy.setDefaultPolicy(CookiePolicy.COMPATIBILITY);

What can i do?

Thanks,
Ludovic"
0,"Some tests fail due to common use of java.io.tmpdirSome tests use java.io.tmpdir, while others use tempDir (which is defined in common-build.xml).  Those that rely on java.io.tmpdir can fail when being run on the same machine as someone else who is running tests (this came up in testing the new nightly build scripts on lucene.zones.a.o)

Proposed fix is to map java.io.tmpdir in the ANT Junit task to be the same value as tempDir."
0,Optimize queries with relative path in order by clauseThis is a follow up to JCR-800 and adds a way to configure relative property paths for aggregates in the indexing configuration. Aggregated properties are handled much more efficiently when used in an order by. The implementation from JCR-800 is used as a fallback when no aggregate is configured. See attached patch for details.
0,"remove Query.getSimilarity()Spinoff of LUCENE-2854.

See LUCENE-2828 and LUCENE-2854 for reference.

In general, the SimilarityDelegator was problematic with regards to back-compat, and if queries
want to score differently, trying to runtime subclass Similarity only causes trouble.

The reason we could not fix this in LUCENE-2854 is because:
{noformat}
Michael McCandless added a comment - 08/Jan/11 01:53 PM
bq. Is it possible to remove this method Query.getSimilarity also? I don't understand why we need this method!

I would love to! But I think that's for another day...

I looked into this and got stuck with BoostingQuery, which rewrites to an anon 
subclass of BQ overriding its getSimilarity in turn override its coord method. 
Rather twisted... if we can do this differently I think we could remove Query.getSimilarity.
{noformat}

here is the method in question:

{noformat}
/** Expert: Returns the Similarity implementation to be used for this query.
 * Subclasses may override this method to specify their own Similarity
 * implementation, perhaps one that delegates through that of the Searcher.
 * By default the Searcher's Similarity implementation is returned.*/
public Similarity getSimilarity(IndexSearcher searcher) {
  return searcher.getSimilarity();
}
{noformat}
"
1,"NullPointerException in AbstractVersionManager.createVersionHistory()Running ConcurrentCheckinMixedTransactionTest with 200 threads results in NullPointerExceptions in AbstractVersionManager.

Exception in thread ""Thread-16"" java.lang.NullPointerException
	at org.apache.jackrabbit.core.version.AbstractVersionManager.createVersionHistory(AbstractVersionManager.java:309)
	at org.apache.jackrabbit.core.version.XAVersionManager.createVersionHistory(XAVersionManager.java:145)
	at org.apache.jackrabbit.core.ItemImpl.initVersionHistories(ItemImpl.java:785)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1221)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:897)
	at org.apache.jackrabbit.core.ConcurrentCheckinMixedTransactionTest$1$1.execute(ConcurrentCheckinMixedTransactionTest.java:66)
	at org.apache.jackrabbit.core.AbstractConcurrencyTest$Executor.run(AbstractConcurrencyTest.java:110)
	at java.lang.Thread.run(Thread.java:619)

I'm not sure why the node that is created by the current thread is not available. I assume that some other thread using XA transactions is committing changes while the current thread creates the node. The changes from the committing thread then overwrite the node that has been modified by the current thread. The write lock is somewhat bypassed in that case."
0,"Make QueryAutoStopWordAnalyzer immutable and reusableCurrently QueryAutoStopWordAnalyzer allows its list of stop words to be changed after instantiation through its addStopWords() methods.  This stops the Analyzer from being reusable since it must instantiate its StopFilters every time.

Having these methods means that although the Analyzer can be instantiated once and reused between IndexReaders, the actual analysis stack is not reusable (which is probably the more expensive part).

So lets change the Analyzer so that its stop words are set at instantiation time, facilitating reuse."
0,"Session.importXml should close the input stream (as to JSR 283/JCR 2.0)http://markmail.org/thread/crwx27dkt2cnjjy7

This is available for all that follow:
  Node.setProperty(String, InputStream)
  Property.setValue(InputStream)
  ValueFactory.createValue(InputStream)
  ValueFactory.createBinary(InputStream)
  Session.importXML(String, InputStream, int)
  Workspace.importXML(String, InputStream, int)
"
0,"Add log.step support per taskFollowing LUCENE-1774, this will add support for log.step per task name, rather than a single log.step setting for all tasks. The .alg file will support:
* log.step - for all tasks.
* log.step.<Task Class Name> - for a specific task. For example, log.step.AddDoc, or log.step.DeleteDoc

I will post the patch soon"
0,"FieldCache should include a BitSet for matching docsThe FieldCache returns an array representing the values for each doc.  However there is no way to know if the doc actually has a value.

This should be changed to return an object representing the values *and* a BitSet for all valid docs."
1,"QValueFactoryImpl$BinaryQValue must not return 'this' on getBinaryThis is basically the same as JCR-2238, but for the spi-commons module. BinaryQValue returns 'this' on getBinary(), which will lead to a file not found exception because Binary.dispose() will delete the the underlying temp file.

The issue is partially hidden by the presence of a bug in BinaryQValue.read(): the RandomAccessFile is not closed after reading, which might prevent deleting of the temp file."
0,"LuceneTestCase's uncaught exceptions handler should check for AssumptionViolatedExceptions and then not trigger test failureAs in single-threaded tests, {{LuceneTestCase}} should not trigger test failures for {{AssumptionViolatedException}}'s when they occur in multi-threaded tests."
1,"""Directory was previously created with a different LockFactory"" when open, close, delete a repository in a loopOpening a TransientRepository in a loop throws the exception ""Directory was previously created with a different LockFactory instance"".

Test case:

for (int i = 0; i < 3; i++) {
	FileUtils.deleteDirectory(new File(""repository""));
	Repository rep = new TransientRepository();
	Session session = rep.login(new SimpleCredentials("""", new char[0]));
	session.logout();
}

The problem seems to be that org.apache.lucene.store.FSDirectory.DIRECTORIES is not cleared (FSDirectory.close() is not called?).

Stack trace:

Exception in thread ""main"" javax.jcr.RepositoryException: Directory was previously created with a different LockFactory instance; please pass null as the lockFactory instance and use setLockFactory to change it: Directory was previously created with a different LockFactory instance; please pass null as the lockFactory instance and use setLockFactory to change it: Directory was previously created with a different LockFactory instance; please pass null as the lockFactory instance and use setLockFactory to change it
	at org.apache.jackrabbit.core.SearchManager.initializeQueryHandler(SearchManager.java:555)
	at org.apache.jackrabbit.core.SearchManager.<init>(SearchManager.java:239)
	at org.apache.jackrabbit.core.RepositoryImpl.getSystemSearchManager(RepositoryImpl.java:688)
	at org.apache.jackrabbit.core.RepositoryImpl.access$3(RepositoryImpl.java:681)
	at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.getSearchManager(RepositoryImpl.java:1780)
	at org.apache.jackrabbit.core.RepositoryImpl.initWorkspace(RepositoryImpl.java:667)
	at org.apache.jackrabbit.core.RepositoryImpl.initStartupWorkspaces(RepositoryImpl.java:480)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:321)
	at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:618)
	at org.apache.jackrabbit.core.TransientRepository$2.getRepository(TransientRepository.java:241)
	at org.apache.jackrabbit.core.TransientRepository.startRepository(TransientRepository.java:261)
Caused by: java.io.IOException: Directory was previously created with a different LockFactory instance; please pass null as the lockFactory instance and use setLockFactory to change it
	at org.apache.lucene.store.FSDirectory.getDirectory(FSDirectory.java:192)
	at org.apache.jackrabbit.core.query.lucene.directory.FSDirectoryManager.getDirectory(FSDirectoryManager.java:64)
	at org.apache.jackrabbit.core.query.lucene.MultiIndex.<init>(MultiIndex.java:227)
	at org.apache.jackrabbit.core.query.lucene.SearchIndex.doInit(SearchIndex.java:477)
	at org.apache.jackrabbit.core.query.AbstractQueryHandler.init(AbstractQueryHandler.java:59)
"
0,"Access to SO_TIMEOUT for open connectionsI'm trying to access a set of pages in order, for which I have a maximum delay
permissible.  The complete operation includes following all redirects and
fetching the complete page content.  What I need, which doesn't seem to be
doable right now (according to the common-users list) is to reset the SO_TIMEOUT
property of the socket before each read to the inputstream.  I'd need an access
to the HttpConnection, or a way to set the parameters for that object.

This is a simplified version of what I'm doing:
-----
HttpURL url = new HttpURL(urlString);
method.setURI(url);
method.setFollowRedirects(false);
method.getParams().setSoTimeout(remainingTime);
HostConfiguration hostConfig = new HostConfiguration();
hostConfig.setHost(url);
method.setHostConfiguration(hostConfig);
timeoutChecker.getRemainingTime());

int statusCode = client.executeMethod(hostConfig, method, state);
String pageContent;

if (isRedirect(statusCode)) {
    if (timeoutChecker.isTimeout()) {
        throw new TimeoutException(""Total execution time for fetch exceeded
timeout parameter"");
    } else {
        Header locationHeader = method.getResponseHeader(""location"");
        HttpURL nextLocation = new HttpURL(locationHeader.getValue().toCharArray());
        pageContent = fetchGet(nextLocation.getEscapedURI(), addressHolder,
timeoutChecker, state);
    }
} else if (isSuccess(statusCode)) {
    // at least 4K buffers, might be as big as the webpage
    int responseSize = Math.max(getResponseSize(method), DEFAULT_RESPONSE_SIZE);
    InputStream response = method.getResponseBodyAsStream();
    ByteArrayOutputStream outstream = new ByteArrayOutputStream(responseSize);
    byte[] buffer = new byte[responseSize];
    int len;
    do {
        // ***TODO need to reset the SO_TIMEOUT to the remaining time
        len = response.read(buffer);
        outstream.write(buffer, 0, len);
    while ((len > 0) && !timeoutChecker.isTimeout());
    outstream.close();
    pageContent = EncodingUtil.getString(outstream.toByteArray(),
method.getResponseCharSet());
    response.close();
} else {
    ...
}"
1,"cluster synchronization NPEwe have a 4 machines setup and encountered the following NPE in one of the nodes. After restarting tomcat, the problem seems to go away. But it would be nice to find out why.


java.lang.NullPointerException
        at org.apache.jackrabbit.core.query.lucene.NodeIndexer.createDoc(NodeInd
exer.java:146)
        at org.apache.jackrabbit.core.query.lucene.SearchIndex.createDocument(Se
archIndex.java:566)
        at org.apache.jackrabbit.core.query.lucene.SearchIndex$2.next(SearchInde
x.java:368)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.update(MultiIndex.
java:354)
        at org.apache.jackrabbit.core.query.lucene.SearchIndex.updateNodes(Searc
hIndex.java:356)
        at org.apache.jackrabbit.core.SearchManager.onEvent(SearchManager.java:4
23)
        at org.apache.jackrabbit.core.observation.EventConsumer.consumeEvents(Ev
entConsumer.java:231)
        at org.apache.jackrabbit.core.observation.ObservationDispatcher.dispatch
Events(ObservationDispatcher.java:201)
        at org.apache.jackrabbit.core.observation.EventStateCollection.dispatch(
EventStateCollection.java:424)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.externalUpdat
e(SharedItemStateManager.java:882)
        at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.externalUpdat
e(RepositoryImpl.java:1957)
        at org.apache.jackrabbit.core.cluster.ClusterNode.end(ClusterNode.java:8
34)
        at org.apache.jackrabbit.core.cluster.ClusterNode.consume(ClusterNode.ja
va:929)
        at org.apache.jackrabbit.core.journal.AbstractJournal.doSync(AbstractJou
rnal.java:191)
        at org.apache.jackrabbit.core.journal.AbstractJournal.sync(AbstractJourn
al.java:166)
        at org.apache.jackrabbit.core.cluster.ClusterNode.sync(ClusterNode.java:
283)
        at org.apache.jackrabbit.core.cluster.ClusterNode.start(ClusterNode.java
:229)
        at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:
308)
        at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:
584)
        at org.apache.jackrabbit.core.jndi.BindableRepository.createRepository(B
indableRepository.java:174)"
0,Rename EasySimilarity to SimilarityBase
0,More verbose message on reference constraint violation
1,"DefaultAccessManager#hasPrivileges(String,Set,Privilege[]) doesn't close compiled permissionsDefaultAccessManager#hasPrivileges(String,Set,Privilege[]) retrieves the compiled permissions for the specified set of principals
from the ac provider but omit the CompiledPermissions#close() call before returning."
0,SPI: prefer 'Iterator' instead of specialized subclassesin the F2F we agreed that the SPI should rather use 'Iterator' instead of specialized subclassed (or RangeIterator).
1,"Config incorrectly handles Windows absolute pathnamesI have no idea how no one ran into this so far, but I tried to execute an .alg file which used ReutersContentSource and referenced both docs.dir and work.dir as Windows absolute pathnames (e.g. d:\something). Surprisingly, the run reported an error of missing content under benchmark\work\something.

I've traced the problem back to Config, where get(String, String) includes the following code:
{code}
    if (sval.indexOf("":"") < 0) {
      return sval;
    }
    // first time this prop is extracted by round
    int k = sval.indexOf("":"");
    String colName = sval.substring(0, k);
    sval = sval.substring(k + 1);
    ...
{code}

It detects "":"" in the value and so it thinks it's a per-round property, thus stripping ""d:"" from the value ... fix is very simple:
{code}
    if (sval.indexOf("":"") < 0) {
      return sval;
    } else if (sval.indexOf("":\\"") >= 0) {
      // this previously messed up absolute path names on Windows. Assuming
      // there is no real value that starts with \\
      return sval;
    }
    // first time this prop is extracted by round
    int k = sval.indexOf("":"");
    String colName = sval.substring(0, k);
    sval = sval.substring(k + 1);
{code}

I'll post a patch w/ the above fix + test shortly."
1,"redirect not handled correctly if location header doesn't have a protocolHttp redirect is not handled correctly if the location header doesn't have a 
protocol, e.g.:

Location: web/tbghome.nsf/pages/index

a java.net.MalformedURLException is throw in this case. The correct behavior is 
to inherit the protocol from current URL.

The relevant code is in HttpMethodBase.execute()"
0,"Support synonym searchesJackrabbit should support synonym searches in the jcr:contains function like Google does.

Example:

//element(*, nt:resource)[jcr:contains(., '~food')]

-> finds all nt:resource nodes that contain the word food or synonyms for food."
0,"TCK: SetPropertyValueTest#testCompactValueArrayWithNulls does not respect nodename1 and nodetype configuration propertiesTest doesn't respect value of nodename1 and nodetype configuration properties.

Proposal: create property under testnode instead of testrootnode.

--- SetPropertyValueTest.java   (revision 422074)
+++ SetPropertyValueTest.java   (working copy)
@@ -374,11 +374,11 @@
      * the value array by removing all null values
      */
     public void testCompactValueArrayWithNulls() throws Exception {
-        testRootNode.setProperty(propertyName2, vArrayWithNulls);
+        testNode.setProperty(propertyName2, vArrayWithNulls);
         superuser.save();
         assertEquals(""Node.setProperty(String, valueArrayWithNulls[]) did not compact the value array by removing the null values"",
                 2,
-                testRootNode.getProperty(propertyName2).getValues().length);
+                testNode.getProperty(propertyName2).getValues().length);
     }
"
1,"IndexWriter.addIndexesNoOptimize ignores the compound file setting of the destination indexIndexWriter.addIndexesNoOptimize(Directory[]) ignores the compound file setting of the destination index. It is using the compound file flags of segments in the source indexes.
This sometimes causes undesired increase of the number of files in the destination index when non-compound file indexes are added until merge kicks in."
1,"KuromojiTokenizer fails with large docsjust shoving largeish random docs triggers asserts like:

{noformat}
    [junit] Caused by: java.lang.AssertionError: backPos=4100 vs lastBackTracePos=5120
    [junit] 	at org.apache.lucene.analysis.kuromoji.KuromojiTokenizer.backtrace(KuromojiTokenizer.java:907)
    [junit] 	at org.apache.lucene.analysis.kuromoji.KuromojiTokenizer.parse(KuromojiTokenizer.java:756)
    [junit] 	at org.apache.lucene.analysis.kuromoji.KuromojiTokenizer.incrementToken(KuromojiTokenizer.java:403)
    [junit] 	at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:404)
{noformat}

But, you get no seed...

I'll commit the test case and @Ignore it."
0,"pom.xml in sandbox/spi has wrong scm urlthe pom in sandbox/spi has the wrong scm url.
You can see the pom here:
http://svn.apache.org/repos/asf/jackrabbit/sandbox/spi/pom.xml

It probably should be:
<scm>
    <connection>scm:svn:http://svn.apache.org/repos/asf/jackrabbit/sandbox/spi </connection>
    <developerConnection>scm:svn:https://svn.apache.org/repos/asf/jackrabbit/sandbox/spi </developerConnection>
    <url>http://svn.apache.org/viewvc/jackrabbit/sandbox/spi </url>
</scm>
"
0,"Convert Lucene Core tests over to a simple MockQueryParserMost tests use Lucene Core's QueryParser for convenience.  We want to consolidate it into a QP module which we can't have as a dependency.  We should add a simple MockQueryParser which does String.split() on the query string, analyzers the terms and builds a BooleanQuery if necessary.  Any more complex Queries (such as phrases) should be done programmatically. "
1,"Benchmark's ExtractReuters creates its temp dir wrongly if provided out-dir param ends by slashSee LUCENE-929 for context.
As result, it might fail to create the temp dir at all."
0,MemoryFileSystem is different from other FileSystemsJCR-1175 uncovered inconsistencies in how the deleteFolder() and list() methods are implemented. The MemoryFileSystem class acts differently from the LocalFileSystem and DatabaseFileSystem classes. MemoryFileSystem and the related incorrect test cases should be fixed.
0,"LogMergePolicy.findMergesToExpungeDeletes need to get deletes from the SegmentReaderWith LUCENE-1516, deletes are carried over in the SegmentReaders
which means implementations of
MergePolicy.findMergesToExpungeDeletes (such as LogMergePolicy)
need to obtain deletion info from the SR (instead of from the
SegmentInfo which won't have the information)."
0,"Move document type definition out of repository.xmlHello!

Here at Cognifide, Przemo and I we got a bit confused while trying to solve JCR-202. There was a need to modify repository.xml configuration file and it's DTD, and we have found that there are different repository.xml files within trunk that differs this definition. I think that it is a good idea to extract this definition to a one separate file (and maybe .xsd instead of .dtd) and then link it in other files. It would be also nice to put this file somewhere on the Web and reference it via URL.

I am waiting for your comments.

Regards, Jan"
0,"Improve jcr decorator in jcr-extThe jcr decorator in jcr-ext does not cover all the necessary interfaces of the jcr api. It may happen that a client loses the decoration layer when accessing properties.

I've added decoration for several jcr interfaces to ensure that the decoration layer is never left.

The attached patch also removes the classes related to decorator chaining. I found it hard to understand the purpose of those classes and decided to remove them from the default implementation. If we want to keep those classes they should be less intrusive.

I've also noticed that there are class name clashes, specifically the package org.apache.jackrabbit.name contains classes that are also present in the jackrabbit and jackrabbit-commons jar file. I propose to move the respective classes in jcr-ext to a different package or remove them in favor of the jackrabbit-commons classes.

Let me know if I should commit the the patch.

Thanks"
1,"XPathQueryBuilder may not handle multiple jcr:deref correctlyIf you have the following tree (inspired from DerefTest) :
+ people
   + carl (worksfor -> company/microsoft)
   + frank (worksfor -> company/microsoft)
+ company
    + microsoft (eotm -> carl)

The following queries will be translated to :

testroot/people/frank/jcr:deref(@worksfor, '*')/jcr:deref(@eotm, '*')
+ Root node
+ Select properties: *
  + PathQueryNode
    + LocationStepQueryNode:  NodeTest={}testroot Descendants=false Index=NONE
    + LocationStepQueryNode:  NodeTest={}people Descendants=false Index=NONE
    + LocationStepQueryNode:  NodeTest={}frank Descendants=false Index=NONE
    + DerefQueryNode:  NodeTest=* Descendants=false Index=NONE
    + DerefQueryNode:  NodeTest=* Descendants=false Index=NONE
=> Matching carl node

testroot/people/frank/jcr:deref(@worksfor, '*')/jcr:deref(@eotm, '*')[@jcr:uuid]
+ Root node
+ Select properties: *
  + PathQueryNode
    + LocationStepQueryNode:  NodeTest={}testroot Descendants=false Index=NONE
    + LocationStepQueryNode:  NodeTest={}people Descendants=false Index=NONE
    + LocationStepQueryNode:  NodeTest={}frank Descendants=false Index=NONE
    + LocationStepQueryNode:  NodeTest=* Descendants=false Index=NONE
    + DerefQueryNode:  NodeTest=* Descendants=false Index=NONE
      + RelationQueryNode: Op: NOT NULL Prop=@{http://www.jcp.org/jcr/1.0}uuid
=> Not matching carl node

testroot/people/frank/jcr:deref(@worksfor, '*')[@jcr:uuid]/jcr:deref(@eotm, '*')[@jcr:uuid]
+ Root node
+ Select properties: *
  + PathQueryNode
    + LocationStepQueryNode:  NodeTest={}testroot Descendants=false Index=NONE
    + LocationStepQueryNode:  NodeTest={}people Descendants=false Index=NONE
    + LocationStepQueryNode:  NodeTest={}frank Descendants=false Index=NONE
    + DerefQueryNode:  NodeTest=* Descendants=false Index=NONE
      + RelationQueryNode: Op: NOT NULL Prop=@{http://www.jcp.org/jcr/1.0}uuid
    + DerefQueryNode:  NodeTest=* Descendants=false Index=NONE
      + RelationQueryNode: Op: NOT NULL Prop=@{http://www.jcp.org/jcr/1.0}uuid
=> Matching carl node

testroot/people/frank/jcr:deref(@worksfor, '*')[@jcr:uuid]/jcr:deref(@eotm, '*')
+ Root node
+ Select properties: *
  + PathQueryNode
    + LocationStepQueryNode:  NodeTest={}testroot Descendants=false Index=NONE
    + LocationStepQueryNode:  NodeTest={}people Descendants=false Index=NONE
    + LocationStepQueryNode:  NodeTest={}frank Descendants=false Index=NONE
    + DerefQueryNode:  NodeTest=* Descendants=false Index=NONE
      + RelationQueryNode: Op: NOT NULL Prop=@{http://www.jcp.org/jcr/1.0}uuid
    + DerefQueryNode:  NodeTest=* Descendants=false Index=NONE
=> Matching carl node

This is because XPathQueryBuilder calls NAryQueryNode#removeOperand(QueryNode)
in order to replace current LocationStepQueryNode with a DerefQueryNode.

NAryQueryNode#removeOperand(QueryNode) uses internally a List and thus
relies on Object#equals(Object) for retrieving the object to remove.

But the equals method is redefined for every QueryNode with a different semantic.

Then, the call to NAryQueryNode#removeOperand(QueryNode) will not remove the
wanted operand but the first operand returning true after calling equals in
ArrayList#remove(Object)."
0,"introduce QValue.getCalendar()Introduce QValue.getCalendar() in order to avoid unnecessary conversions from/to string format.
"
0,"Fix rawtypes warnings for Java 7 compilerJava 7 changed the warnings a little bit:
- Java 6 only knew ""unchecked"" warning type, applying for all types of generics violations, like missing generics (raw types)
- Java 7 still knows ""unchecked"" but only emits warning if the call is really unchecked. Declaration of variables/fields or constructing instances without type param now emits ""rawtypes"" warning.

The changes above causes the Java 7 compile now emit lots of ""rawtypes"" warnings, where Java 6 is silent. The easy fix is to suppres both warning types: @SuppressWarnings({""unchecked"",""rawtypes""}) for all those places. Changes are easy to do, will provide patch later!"
1,"Jcr-Server: Invalid value for HTTP auth headerAt present, DAV Explorer won't log in to the JCR WebDav servlet - it doesn't even ask for a username & password.  (Neither the Microsoft WinXP WebDAV & Novell's NetDrive were as fussy and were happy to log in)
Using Ethereal, I compared the traffic for a valid Slide WebDav login compared to a JCR WebDav login.

I've now found and fixed the problem on my local build, and I've now got DAV Explorer to work with JCR Webdav.  Here's a description of the bugfix:

In jackrabbit/contrib/jcr-server/server/src/java/org/apache/jackrabbit/server/AbstractWebdavServlet.java, there is a public static final String DEFAULT_AUTHENTICATE_HEADER.
This is currently set to ""Basic Realm=Jackrabbit Webdav Server"".

This is not a valid string for use in this context as it is in breach of RFC2617 for 2 reasons:
1) ""Realm"" should be ""realm""
2) ""Jackrabbit Webdav Server"" should be in quotes, i.e. ""\""Jackrabbit Webdav Server\""""

According to http://www.ietf.org/rfc/rfc2617.txt, a valid challenge would be:
   WWW-Authenticate: Basic realm=""WallyWorld""
Note that ""realm"" is not capitalised and ""WallyWorld"" has been enclosed in quotes (the ""WWW-Authenticate: "" string is held elsewhere in the Java code and is correct)


In other words, AbstractWebdavServlet.java line 82, which currently reads:
    public static final String DEFAULT_AUTHENTICATE_HEADER = ""Basic Realm=Jackrabbit Webdav Server"";
should be changed to read
    public static final String DEFAULT_AUTHENTICATE_HEADER = ""Basic realm=\""Jackrabbit Webdav Server\"""";

"
0,"BaseTokenStreamTestCase should test analyzers on real-ish contentWe already have LineFileDocs, that pulls content generated from europarl or wikipedia... I think sometimes BTSTC should test the analyzers on that as well."
0,"questionable default value for BufferedOutputStream size in HttpConnectionFrom the dev list

--

Hi Eric

Thanks for bringing this up. HttpClient 3.0 allows for parameterization
of SO_SNDBUF and SO_RCVBUF settings. For HttpClient 2.0 (as well as for
3.0 when falling back onto the system defaults), however, it would make
sense to set a cap on the size of the send and receive buffers.

Feel free to open a ticket for this issue with Bugzilla

Oleg


On Fri, 2004-07-02 at 18:39, Eric Bloch wrote:

>> Hi httpclient folks,
>> 
>> I've been looking at 2.0 source code and the default value for the 
>> BufferedOutputStream that is used in an HttpConnectionn is coming from 
>> socket.getSendBufferSize().  My hunch, is that, in general, this is 
>> bigger than you'd want.
>> 
>> Most HTTP ""sends"" are less than 1KByte ('cept for big POSTs).
>> The default value I get for socket.getSendBufferSize for this is 8192.
>> I would think a better default for this buffer would be 1K, no?
>> 
>> Also, fyi, if someone happens to dork the system send buffer size hi 
>> (say MB) and you are using the MultiThreadedConnectionManager in 2.0 
>> (dunno about 3.0), you will use up a lot of memory for each connection 
>> since the pool doesn't let idle connections (or their buffers) be gced. 
>>   I just got bit bad by that.
>> 
>> -Eric
>> 
>"
1,"If you pass Integer.MAX_VALUE as 2nd param to search(Query, int) you hit unexpected NegativeArraySizeExceptionNote that this is a nonsense value to pass in, since our PQ impl allocates the array up front.

It's because PQ takes 1+ this value (which wraps to -1), and attempts to allocate that.  We should bounds check it, and drop PQ size by one in this case.

Better, maybe: in IndexSearcher, if that n is ever > maxDoc(), set it to maxDoc().

This trips users up fairly often because they assume our PQ doesn't statically pre-allocate (a reasonable assumption...)."
0,"Bulgarian Analyzersomeone asked about bulgarian analysis on solr-user today... http://www.lucidimagination.com/search/document/e1e7a5636edb1db2/non_english_languages
I was surprised we did not have anything.

This analyzer implements the algorithm specified here, http://members.unine.ch/jacques.savoy/Papers/BUIR.pdf

In the measurements there, this improves MAP approx 34%
"
0,"setAuthPreemptive restricted to BASIC AuthSchemePre-emptive authentication is hardcoded to be restricted to the BASIC
authentication scheme.  To fully support custom authentication schemes,
pre-emptive authentication should be made configurable, either globally, or on a
per-scheme basis.  A potential compromise may be to require AuthSchemes to
report whether they support pre-emptive capability if we wish to explicitly
exclude certain schemes from pre-emptive authentication.

(reported against 3.0 RC 1)"
0,"Introduce a temprary cache for intermediate query resultsSometimes queries execute the same sub query multiple times.

e.g. //element(*, nt:resource)[@jcr:mimeType != 'text/plain' and @jcr:mimeType != 'text/html']

will result in two internal MatchAllQuery on jcr:mimeType. The query should re-use the previously calculated results when possible."
0,"document LengthFilter wrt Unicode 4.0LengthFilter calculates its min/max length from TermAttribute.termLength()
This is not characters, but instead UTF-16 code units.

In my opinion this should not be changed, merely documented.
If we changed it, it would have an adverse performance impact because we would have to actually calculate Character.codePointCount() on the text.

If you feel strongly otherwise, fixing it to count codepoints would be a trivial patch, but I'd rather not hurt performance.
I admit I don't fully understand all the use cases for this filter.
"
0,"Revise NIOFSDirectory and its usage due to NIO limitations on Thread.interruptI created this issue as a spin off from http://mail-archives.apache.org/mod_mbox/lucene-java-dev/201001.mbox/%3Cf18c9dde1001280051w4af2bc50u1cfd55f85e50914f@mail.gmail.com%3E

We should decide what to do with NIOFSDirectory, if we want to keep it as the default on none-windows platforms and how we want to document this.

"
1,"XPath query with negative numbers incorrectA XPath query that contains a negative number does not return correct results.

E.g. the query:

//*[@number = -10]

does not return nodes with number properties containing a value of -10 but will return nodes with number values equal to 10. Similarly the query returns wrong results for double values."
1,"No entry created for this pool.Followup to https://issues.apache.org/jira/browse/HTTPCLIENT-741, as reported by Sam Berlin:

java.lang.IllegalStateException: No entry created for this pool. HttpRoute[{}->http://74.160.66.42:14561]
    at org.apache.http.impl.conn.tsccm.RouteSpecificPool.freeEntry(RouteSpecificPool.java:137)
    at org.apache.http.impl.conn.tsccm.ConnPoolByRoute.freeEntry(ConnPoolByRoute.java:337)
    at org.apache.http.impl.conn.tsccm.ThreadSafeClientConnManager.releaseConnection(ThreadSafeClientConnManager.java:230)
    at org.apache.http.impl.client.DefaultClientRequestDirector.execute(DefaultClientRequestDirector.java:427)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:500)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:455)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:421)
    at com.limegroup.gnutella.http.DefaultHttpExecutor.performRequest(DefaultHttpExecutor.java:97)
    at com.limegroup.gnutella.http.DefaultHttpExecutor.access$000(DefaultHttpExecutor.java:26)
    at com.limegroup.gnutella.http.DefaultHttpExecutor$MultiRequestor.run(DefaultHttpExecutor.java:139)
    at org.limewire.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1006)
    at org.limewire.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:549)
    at java.lang.Thread.run(Thread.java:613)
---

DefaultHttpExecutor$MultiRequestor basically is just a Runnable / Cancellable [exposes a cancel() method] that can be cancelled from any thread. cancel just calls abort() on the current AbortableHttpRequest, but is called on a thread other than the one that's doing the client.execute(request).

The last one is the most common exception, and seems to happen with some regularity. The other two we've only seen once, so may just be a memory quirk (we've seen some crazy bugs, including recursive NPEs while constructing an NPE.)
"
0,create configuration on InputStreamRepositoryConfig should be possible to create based on InputStreams (in case of URLs) ; right now it's possible only using String and InputSource. Please update also the JCA connector. 
0,"WordListLoader.java should be able to read stopwords from a ReaderWordListLoader should be able to read the stopwords from a Reader.

This would (for example) allow stopword lists to be stored as a resource in the
jar file of a Lucene application.

Diff is attached."
0,"[PATCH] ant-task ""javadocs-all"" fails with OutOfMemoryErrorHi all,

the current nightly build's ""ant dist"" fails with an OutOfMemoryError at ant task javadocs-all (see below).
Apparently javadoc needs more memory.

A similar case has been reported in HADOOP-5561 (add a maxmemory statement to the javadoc task), and I propose the same change for Lucene as well.

Cheers,
Christian


javadocs-all:
  [javadoc] Generating Javadoc
  [javadoc] Javadoc execution
  [javadoc] Loading source files for package org.apache.lucene...
  [javadoc] Loading source files for package org.apache.lucene.analysis...
(...)
  [javadoc] Loading source files for package org.apache.lucene.queryParser.standard.config...
  [javadoc] Loading source files for package org.apache.lucene.queryParser.standard.nodes...
  [javadoc] Loading source files for package org.apache.lucene.queryParser.standard.parser...
  [javadoc] Loading source files for package org.apache.lucene.queryParser.standard.processors...
  [javadoc] Constructing Javadoc information...
  [javadoc] Standard Doclet version 1.6.0_15
  [javadoc] Building tree for all the packages and classes...

 (takes a long time here until OOME)

  [javadoc] java.lang.OutOfMemoryError: Java heap space
  [javadoc] java.lang.OutOfMemoryError: Java heap space
  [javadoc] 	at java.lang.Throwable.getStackTraceElement(Native Method)
  [javadoc] 	at java.lang.Throwable.getOurStackTrace(Throwable.java:591)
  [javadoc] 	at java.lang.Throwable.printStackTrace(Throwable.java:462)
  [javadoc] 	at java.lang.Throwable.printStackTrace(Throwable.java:451)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.AbstractBuilder.build(AbstractBuilder.java:103)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.AbstractMemberBuilder.build(AbstractMemberBuilder.java:56)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.buildMemberSummary(ClassBuilder.java:279)
  [javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  [javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
  [javadoc] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
  [javadoc] 	at java.lang.reflect.Method.invoke(Method.java:597)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.invokeMethod(ClassBuilder.java:101)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.AbstractBuilder.build(AbstractBuilder.java:90)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.buildClassDoc(ClassBuilder.java:124)
  [javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  [javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
  [javadoc] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
  [javadoc] 	at java.lang.reflect.Method.invoke(Method.java:597)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.invokeMethod(ClassBuilder.java:101)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.AbstractBuilder.build(AbstractBuilder.java:90)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.build(ClassBuilder.java:108)
  [javadoc] 	at com.sun.tools.doclets.formats.html.HtmlDoclet.generateClassFiles(HtmlDoclet.java:155)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.AbstractDoclet.generateClassFiles(AbstractDoclet.java:164)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.AbstractDoclet.startGeneration(AbstractDoclet.java:106)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.AbstractDoclet.start(AbstractDoclet.java:64)
  [javadoc] 	at com.sun.tools.doclets.formats.html.HtmlDoclet.start(HtmlDoclet.java:42)
  [javadoc] 	at com.sun.tools.doclets.standard.Standard.start(Standard.java:23)
  [javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  [javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
  [javadoc] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
  [javadoc] 	at java.lang.reflect.Method.invoke(Method.java:597)
  [javadoc] 	at com.sun.tools.javadoc.DocletInvoker.invoke(DocletInvoker.java:269)
  [javadoc] java.lang.OutOfMemoryError: Java heap space
  [javadoc] 	at java.util.Arrays.copyOfRange(Arrays.java:3209)
  [javadoc] 	at java.lang.String.<init>(String.java:215)
  [javadoc] 	at com.sun.tools.javac.util.Convert.utf2string(Convert.java:131)
  [javadoc] 	at com.sun.tools.javac.util.Name.toString(Name.java:164)
  [javadoc] 	at com.sun.tools.javadoc.ClassDocImpl.getClassName(ClassDocImpl.java:341)
  [javadoc] 	at com.sun.tools.javadoc.TypeMaker.getTypeName(TypeMaker.java:100)
  [javadoc] 	at com.sun.tools.javadoc.ParameterizedTypeImpl.parameterizedTypeToString(ParameterizedTypeImpl.java:117)
  [javadoc] 	at com.sun.tools.javadoc.TypeMaker.getTypeString(TypeMaker.java:121)
  [javadoc] 	at com.sun.tools.javadoc.ExecutableMemberDocImpl.makeSignature(ExecutableMemberDocImpl.java:217)
  [javadoc] 	at com.sun.tools.javadoc.ExecutableMemberDocImpl.signature(ExecutableMemberDocImpl.java:198)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.util.VisibleMemberMap.getMemberKey(VisibleMemberMap.java:485)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.util.VisibleMemberMap.access$1000(VisibleMemberMap.java:28)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.util.VisibleMemberMap$ClassMembers.isOverridden(VisibleMemberMap.java:442)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.util.VisibleMemberMap$ClassMembers.addMembers(VisibleMemberMap.java:316)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.util.VisibleMemberMap$ClassMembers.build(VisibleMemberMap.java:278)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.util.VisibleMemberMap$ClassMembers.access$100(VisibleMemberMap.java:230)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.util.VisibleMemberMap.<init>(VisibleMemberMap.java:93)
  [javadoc] 	at com.sun.tools.doclets.formats.html.ConstructorWriterImpl.<init>(ConstructorWriterImpl.java:38)
  [javadoc] 	at com.sun.tools.doclets.formats.html.WriterFactoryImpl.getConstructorWriter(WriterFactoryImpl.java:129)
  [javadoc] 	at com.sun.tools.doclets.formats.html.WriterFactoryImpl.getMemberSummaryWriter(WriterFactoryImpl.java:141)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.MemberSummaryBuilder.init(MemberSummaryBuilder.java:104)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.MemberSummaryBuilder.getInstance(MemberSummaryBuilder.java:64)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.BuilderFactory.getMemberSummaryBuilder(BuilderFactory.java:191)
  [javadoc] 	at com.sun.tools.doclets.formats.html.ClassWriterImpl.navSummaryLinks(ClassWriterImpl.java:474)
  [javadoc] 	at com.sun.tools.doclets.formats.html.ClassWriterImpl.printSummaryDetailLinks(ClassWriterImpl.java:456)
  [javadoc] 	at com.sun.tools.doclets.formats.html.HtmlDocletWriter.navLinks(HtmlDocletWriter.java:462)
  [javadoc] 	at com.sun.tools.doclets.formats.html.ClassWriterImpl.writeFooter(ClassWriterImpl.java:146)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.buildClassFooter(ClassBuilder.java:330)
  [javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  [javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
  [javadoc] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
  [javadoc] 	at java.lang.reflect.Method.invoke(Method.java:597)
  [javadoc] java.lang.OutOfMemoryError: Java heap space
  [javadoc] 	at java.util.LinkedHashMap.createEntry(LinkedHashMap.java:424)
  [javadoc] 	at java.util.LinkedHashMap.addEntry(LinkedHashMap.java:406)
  [javadoc] 	at java.util.HashMap.put(HashMap.java:385)
  [javadoc] 	at sun.util.resources.OpenListResourceBundle.loadLookup(OpenListResourceBundle.java:118)
  [javadoc] 	at sun.util.resources.OpenListResourceBundle.loadLookupTablesIfNecessary(OpenListResourceBundle.java:97)
  [javadoc] 	at sun.util.resources.OpenListResourceBundle.handleGetObject(OpenListResourceBundle.java:58)
  [javadoc] 	at sun.util.resources.TimeZoneNamesBundle.handleGetObject(TimeZoneNamesBundle.java:59)
  [javadoc] 	at java.util.ResourceBundle.getObject(ResourceBundle.java:378)
  [javadoc] 	at java.util.ResourceBundle.getObject(ResourceBundle.java:381)
  [javadoc] 	at java.util.ResourceBundle.getStringArray(ResourceBundle.java:361)
  [javadoc] 	at sun.util.TimeZoneNameUtility.retrieveDisplayNames(TimeZoneNameUtility.java:100)
  [javadoc] 	at sun.util.TimeZoneNameUtility.retrieveDisplayNames(TimeZoneNameUtility.java:81)
  [javadoc] 	at java.util.TimeZone.getDisplayNames(TimeZone.java:399)
  [javadoc] 	at java.util.TimeZone.getDisplayName(TimeZone.java:350)
  [javadoc] 	at java.util.Date.toString(Date.java:1025)
  [javadoc] 	at com.sun.tools.doclets.formats.html.markup.HtmlDocWriter.today(HtmlDocWriter.java:337)
  [javadoc] 	at com.sun.tools.doclets.formats.html.HtmlDocletWriter.printHtmlHeader(HtmlDocletWriter.java:281)
  [javadoc] 	at com.sun.tools.doclets.formats.html.ClassWriterImpl.writeHeader(ClassWriterImpl.java:122)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.buildClassHeader(ClassBuilder.java:164)
  [javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  [javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
  [javadoc] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
  [javadoc] 	at java.lang.reflect.Method.invoke(Method.java:597)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.invokeMethod(ClassBuilder.java:101)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.AbstractBuilder.build(AbstractBuilder.java:90)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.buildClassDoc(ClassBuilder.java:124)
  [javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  [javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
  [javadoc] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
  [javadoc] 	at java.lang.reflect.Method.invoke(Method.java:597)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.invokeMethod(ClassBuilder.java:101)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.AbstractBuilder.build(AbstractBuilder.java:90)
  [javadoc] java.lang.OutOfMemoryError: Java heap space
  [javadoc] java.lang.OutOfMemoryError: Java heap space
  [javadoc] java.lang.OutOfMemoryError: Java heap space
  [javadoc] java.lang.OutOfMemoryError: Java heap space
  [javadoc] java.lang.OutOfMemoryError: Java heap space
"
0,"discrepancy in getTermFreqVector-methods getTermFreqVector(int, TermVectorMapper) never calls the mapper if there is no term vector, consitent with all the other getTermFreqVector methods that returns null. 

getTermFreqVector(int, String, TermVectorMapper) throws an IOException when a field does not contain the term vector.

My suggestion:

{code}
Index: src/java/org/apache/lucene/index/SegmentReader.java
===================================================================
--- src/java/org/apache/lucene/index/SegmentReader.java (revision 590149)
+++ src/java/org/apache/lucene/index/SegmentReader.java (working copy)
@@ -648,7 +648,7 @@
     ensureOpen();
     FieldInfo fi = fieldInfos.fieldInfo(field);
     if (fi == null || !fi.storeTermVector || termVectorsReaderOrig == null)
-      throw new IOException(""field does not contain term vectors"");
+      return; 
{code}"
0,"move PerFieldCodecWrapper into codecs packagePerFieldCodecWrapper is a codec, but its 'hardwired' as lucene's only codec currently (except for PreFlex/3.x case)

it lets you choose a format for the postings lists per-field.

I think we should move this to the codecs package as a start... just a rote refactor."
0,"TCK: GetPersistentQueryPathTest and SaveTest require nt:queryGetPersistentQueryPathTest and SaveTest require implementation to support nt:query node type.  This is an optional node type.

Proposal: throw NotExecutableException if nt:query not in node type registry
"
1,"JCRUrlConnection relies on nt:file/nt:resourceThe JCRUrlConnection class implementing the jcr: URL handler for the JCR class loader relies on the fact that the intended primary type of the jcr:content child node of an nt:file node is of type nt:resource. When writing files with the Jackrabbit WebDAV server this is not the case as the jcr:content child node is of type nt:unstructured.

As a result the JCRUrlConnection.connect method fails with an ItemNotFoundException in the Util.getProperty(Item)  method because the primary item of the nt:unstructured node type is not defined."
0,"Fix wrong clover analysis because of backwards-tests, upgrade clover to 2.6.3 or betterThis is a followup for [http://www.lucidimagination.com/search/document/6248d6eafbe10ef4/build_failed_in_hudson_lucene_trunk_902]

The problem with clover running on hudson is, that it does not instrument all tests ran. The autodetection of clover 1.x is not able to find out which files are the correct tests and only instruments the backwards test. Because of this, the current coverage report is only from the backwards tests running against the current Lucene JAR.

You can see this, if you install clover and start the tests. During test-core no clover data is added to the db, only when backwards-tests begin, new files are created in the clover db folder.

Clover 2.x supports a new ant task, <testsources> that can be used to specify the files, that are the tests. It works here locally with clover 2.4.3 and produces a really nice coverage report, also linking with test files work, it tells which tests failed and so on.

I will attach a patch, that changes common-build.xml to the new clover version (other initialization resource) and tells clover where to find the tests (using the test folder include/exclude properties).

One problem with the current patch: It does *not* instrument the backwards branch, so you see only coverage of the core/contrib tests. Getting the coverage also from the backwards tests is not easy possible because of two things:
- the tag test dir is not easy to find out and add to <testsources> element (there may be only one of them)
- the test names in BW branch are identical to the trunk tests. This completely corrupts the linkage between tests and code in the coverage report.

In principle the best would be to generate a second coverage report for the backwards branch with a separate clover DB. The attached patch does not instrument the bw branch, it only does trunk tests."
0,"Copy/Paste-Typo in toString() for SpanQueryFilter   public String toString() {
-    return ""QueryWrapperFilter("" + query + "")"";
+    return ""SpanQueryFilter("" + query + "")"";
   }

says it all."
0,Provide a method for writing name space declarations in CompactNodeTypeDefWriterCurrently CompactNodeTypeDefWriter includes (when configured to do so) only name space declarations from name spaces actually used in the node type definitions written. In some situations it is necessary to write additional name space declarations. I thus propose to add a method writeNamespaceDelclaration.
0,Improve logging in LazyItemIterator#prefetchNext When LazyItemIterator#prefetchNext fails an item it should spell out the name of that item in  the respective log message. 
0,"Check for boundary conditions in FieldInfosIn FieldInfos there are three methods in which we don't check for
boundary conditions but catch e. g. an IndexOutOfBoundsException
or a NPE. I think this isn't good code style and is probably not
even faster than checking explicitly.

""Exceptions should not be used to alter the flow of a program as 
part of normal execution.""

Also this can be irritating when you're trying to debug an 
IndexOutOfBoundsException that is thrown somewhere else in your
program and you place a breakpoint on that exception.

The three methods are:

  public int fieldNumber(String fieldName) {
    try {
      FieldInfo fi = fieldInfo(fieldName);
      if (fi != null)
        return fi.number;
    }
    catch (IndexOutOfBoundsException ioobe) {
      return -1;
    }
    return -1;
  }
  

  public String fieldName(int fieldNumber) {
    try {
      return fieldInfo(fieldNumber).name;
    }
    catch (NullPointerException npe) {
      return """";
    }
  }
  
  
  public FieldInfo fieldInfo(int fieldNumber) {
    try {
      return (FieldInfo) byNumber.get(fieldNumber);
    }
    catch (IndexOutOfBoundsException ioobe) {
      return null;
    }
  }"
0,"FastVectorHighlighter truncates words at beginning and end of fragmentsFastVectorHighlighter does not take word boundaries into consideration when building fragments, so that in most cases the first and last word of a fragment are truncated.  This makes the highlights less legible than they should be.  I will attach a patch to BaseFragmentBuilder that resolves this by expanding the start and end boundaries of the fragment to the first whitespace character on either side of the fragment, or the beginning or end of the source text, whichever comes first.  This significantly improves legibility, at the cost of returning a slightly larger number of characters than specified for the fragment size."
1,SPI: Helper does not properly retrieve org.apache.jackrabbit.spi.workspacename param.consequently the Helper always obtains SessionInfo with null workspace name.
0,"AbstractHttpProcessor hard-wired against a single contextI can't use the AbstractHttpProcessor as it is for asynchronously
processing different requests, because it is hard-wired to use a
single context which can not be changed. Async requires different
contexts for requests. Patch follows.

cheers,
  Roland"
0,"Allow easy extension of RAMDirectoryRAMDirectory uses RAMFiles to store the data. RAMFile offers a newBuffer() method for extensions to override and allocate buffers differently, from e.g. a pool or something. However, RAMDirectory always allocates RAMFile and doesn't allow allocating a RAMFile extension, which makes RAMFile.newBuffer() unusable.

I think we can simply introduce a newRAMFile() method on RAMDirectory and make the RAMFiles map protected, and it will allow really extending RAMDir.

I will post a patch later."
0,"Participation of a workspace in a cluster should be configurableCurrently, when clustering is enabled, every workspace participates automatically. This should be configurable in the workspace, by introducing an attribute such as ""clustered=[true|false]""."
0,have jackrabbit-core produce a test jarI'm writing a custom FileSystem implementation and it would be nice to be able to reuse AbstractFileSystemTest.
0,"IndexFormatTooOld/NewExc should try to include fileName + directory when possible(Spinoff from http://markmail.org/thread/t6s7nn3ve765nojc )

When we throw a too old/new exc we should try to include the full path to the offending file, if possible."
1,"StaleItemStateException during distributed transactionWe use the Jackrabbit JCA Component within a Weblogic 10.3 Application Server with distributed transactions between an Oracle Database an the Jackrabbit JCA.

Updating a node property multiple times in a transaction results in a XAException. Root cause seems to be a StaleItemStateException (see Stack-Trace).
Googling revealed, that a similar bug was fixed for Jackrabbit 1.5.3. Looking through the code showed, that the proposed fix in JCR-1554 seems not to be applied on Jackrabbit 2.0 (tag and trunk).

I tried to apply the proposed fix on the trunk code base, but this seemed not to help.

Stack-Trace:
javax.ejb.TransactionRolledbackLocalException: Error committing transaction:; nested exception is: javax.transaction.xa.XAException                                                                                             
        at weblogic.ejb.container.internal.EJBRuntimeUtils.throwTransactionRolledbackLocal(EJBRuntimeUtils.java:238)                                                                                                            
        at weblogic.ejb.container.internal.EJBRuntimeUtils.throwEJBException(EJBRuntimeUtils.java:133)                                                                                                                          
        at weblogic.ejb.container.internal.BaseLocalObject.postInvoke1(BaseLocalObject.java:623)                                                                                                                                
        at weblogic.ejb.container.internal.BaseLocalObject.postInvokeTxRetry(BaseLocalObject.java:424)                                                                                                                          
        at ch.ejpd.sireneit.facade.ejb.ablage.DokumentFacadeBean_7xdnsq_DokumentFacadeImpl.updateStructuredDokument(DokumentFacadeBean_7xdnsq_DokumentFacadeImpl.java:340)                                                      
        at ch.ejpd.sireneit.access.rest.ablage.DokumentResource.update(DokumentResource.java:453)                                                                                                                               
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)                                                                                                                                                          
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)                                                                                                                                        
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)                                                                                                                                
        at java.lang.reflect.Method.invoke(Method.java:597)                                                                                                                                                                     
        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:175)                                                
        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:67)                                                                                         
        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:208)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:109)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:109)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:109)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:75)                                                                                                                             
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:67)                                                                                                                 
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:724)                                                                                                                
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:689)                                                                                                                 
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:680)                                                                                                                 
        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:324)                                                                                                                                     
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:425)                                                                                                                             
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:604)                                                                                                                             
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)                                                                                                                                                         
        at weblogic.servlet.internal.StubSecurityHelper$ServletServiceAction.run(StubSecurityHelper.java:227)                                                                                                                   
        at weblogic.servlet.internal.StubSecurityHelper.invokeServlet(StubSecurityHelper.java:125)                                                                                                                              
        at weblogic.servlet.internal.ServletStubImpl.execute(ServletStubImpl.java:292)                                                                                                                                          
        at weblogic.servlet.internal.TailFilter.doFilter(TailFilter.java:26)                                                                                                                                                    
        at weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:42)                                                                                                                                          
        at ch.ejpd.lib.webclient.jfa.JfaTokenServletFilter.doFilter(JfaTokenServletFilter.java:108)                                                                                                                             
        at weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:42)                                                                                                                                          
        at weblogic.servlet.internal.WebAppServletContext$ServletInvocationAction.run(WebAppServletContext.java:3496)                                                                                                           
        at weblogic.security.acl.internal.AuthenticatedSubject.doAs(AuthenticatedSubject.java:321)                                                                                                                              
        at weblogic.security.service.SecurityManager.runAs(Unknown Source)                                                                                                                                                      
        at weblogic.servlet.internal.WebAppServletContext.securedExecute(WebAppServletContext.java:2180)                                                                                                                        
        at weblogic.servlet.internal.WebAppServletContext.execute(WebAppServletContext.java:2086)                                                                                                                               
        at weblogic.servlet.internal.ServletRequestImpl.run(ServletRequestImpl.java:1406)                                                                                                                                       
        at weblogic.work.ExecuteThread.execute(ExecuteThread.java:201)                                                                                                                                                          
        at weblogic.work.ExecuteThread.run(ExecuteThread.java:173)                                                                                                                                                              
javax.transaction.xa.XAException                                                                                                                                                                                                
        at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:171)                                                                                                                                   
        at org.apache.jackrabbit.core.XASessionImpl.commit(XASessionImpl.java:346)                                                                                                                                              
        at org.apache.jackrabbit.jca.TransactionBoundXAResource.commit(TransactionBoundXAResource.java:39)                                                                                                                      
        at weblogic.connector.security.layer.AdapterLayer.commit(AdapterLayer.java:252)                                                                                                                                         
        at weblogic.connector.transaction.outbound.XAWrapper.commit(XAWrapper.java:113)                                                                                                                                         
        at weblogic.transaction.internal.XAServerResourceInfo.commit(XAServerResourceInfo.java:1334)                                                                                                                            
        at weblogic.transaction.internal.XAServerResourceInfo.commit(XAServerResourceInfo.java:578)                                                                                                                             
        at weblogic.transaction.internal.ServerSCInfo.startCommit(ServerSCInfo.java:547)                                                                                                                                        
        at weblogic.transaction.internal.ServerTransactionImpl.localCommit(ServerTransactionImpl.java:2006)                                                                                                                     
        at weblogic.transaction.internal.ServerTransactionImpl.globalRetryCommit(ServerTransactionImpl.java:2723)                                                                                                               
        at weblogic.transaction.internal.ServerTransactionImpl.globalCommit(ServerTransactionImpl.java:2645)                                                                                                                    
        at weblogic.transaction.internal.ServerTransactionImpl.internalCommit(ServerTransactionImpl.java:282)                                                                                                                   
        at weblogic.transaction.internal.ServerTransactionImpl.commit(ServerTransactionImpl.java:230)                                                                                                                           
        at weblogic.ejb.container.internal.BaseLocalObject.postInvoke1(BaseLocalObject.java:591)                                                                                                                                
        at weblogic.ejb.container.internal.BaseLocalObject.postInvokeTxRetry(BaseLocalObject.java:424)                                                                                                                          
        at ch.ejpd.sireneit.facade.ejb.ablage.DokumentFacadeBean_7xdnsq_DokumentFacadeImpl.updateStructuredDokument(DokumentFacadeBean_7xdnsq_DokumentFacadeImpl.java:340)                                                      
        at ch.ejpd.sireneit.access.rest.ablage.DokumentResource.update(DokumentResource.java:453)                                                                                                                               
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)                                                                                                                                                          
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)                                                                                                                                        
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)                                                                                                                                
        at java.lang.reflect.Method.invoke(Method.java:597)                                                                                                                                                                     
        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:175)                                                
        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:67)                                                                                         
        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:208)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:109)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:109)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:109)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:75)                                                                                                                             
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:67)                                                                                                                 
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:724)                                                                                                                
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:689)                                                                                                                 
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:680)                                                                                                                 
        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:324)                                                                                                                                     
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:425)                                                                                                                             
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:604)                                                                                                                             
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)                                                                                                                                                         
        at weblogic.servlet.internal.StubSecurityHelper$ServletServiceAction.run(StubSecurityHelper.java:227)                                                                                                                   
        at weblogic.servlet.internal.StubSecurityHelper.invokeServlet(StubSecurityHelper.java:125)                                                                                                                              
        at weblogic.servlet.internal.ServletStubImpl.execute(ServletStubImpl.java:292)                                                                                                                                          
        at weblogic.servlet.internal.TailFilter.doFilter(TailFilter.java:26)                                                                                                                                                    
        at weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:42)                                                                                                                                          
        at ch.ejpd.lib.webclient.jfa.JfaTokenServletFilter.doFilter(JfaTokenServletFilter.java:108)                                                                                                                             
        at weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:42)                                                                                                                                          
        at weblogic.servlet.internal.WebAppServletContext$ServletInvocationAction.run(WebAppServletContext.java:3496)                                                                                                           
        at weblogic.security.acl.internal.AuthenticatedSubject.doAs(AuthenticatedSubject.java:321)                                                                                                                              
        at weblogic.security.service.SecurityManager.runAs(Unknown Source)                                                                                                                                                      
        at weblogic.servlet.internal.WebAppServletContext.securedExecute(WebAppServletContext.java:2180)                                                                                                                        
        at weblogic.servlet.internal.WebAppServletContext.execute(WebAppServletContext.java:2086)                                                                                                                               
        at weblogic.servlet.internal.ServletRequestImpl.run(ServletRequestImpl.java:1406)                                                                                                                                       
        at weblogic.work.ExecuteThread.execute(ExecuteThread.java:201)                                                                                                                                                          
        at weblogic.work.ExecuteThread.run(ExecuteThread.java:173)                                                                                                                                                              
org.apache.jackrabbit.core.TransactionException: Unable to prepare transaction.                                                                                                                                                 
        at org.apache.jackrabbit.core.state.XAItemStateManager.prepare(XAItemStateManager.java:169)                                                                                                                             
        at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:154)                                                                                                                                   
        at org.apache.jackrabbit.core.XASessionImpl.commit(XASessionImpl.java:346)                                                                                                                                              
        at org.apache.jackrabbit.jca.TransactionBoundXAResource.commit(TransactionBoundXAResource.java:39)                                                                                                                      
        at weblogic.connector.security.layer.AdapterLayer.commit(AdapterLayer.java:252)                                                                                                                                         
        at weblogic.connector.transaction.outbound.XAWrapper.commit(XAWrapper.java:113)                                                                                                                                         
        at weblogic.transaction.internal.XAServerResourceInfo.commit(XAServerResourceInfo.java:1334)                                                                                                                            
        at weblogic.transaction.internal.XAServerResourceInfo.commit(XAServerResourceInfo.java:578)                                                                                                                             
        at weblogic.transaction.internal.ServerSCInfo.startCommit(ServerSCInfo.java:547)                                                                                                                                        
        at weblogic.transaction.internal.ServerTransactionImpl.localCommit(ServerTransactionImpl.java:2006)                                                                                                                     
        at weblogic.transaction.internal.ServerTransactionImpl.globalRetryCommit(ServerTransactionImpl.java:2723)                                                                                                               
        at weblogic.transaction.internal.ServerTransactionImpl.globalCommit(ServerTransactionImpl.java:2645)                                                                                                                    
        at weblogic.transaction.internal.ServerTransactionImpl.internalCommit(ServerTransactionImpl.java:282)                                                                                                                   
        at weblogic.transaction.internal.ServerTransactionImpl.commit(ServerTransactionImpl.java:230)                                                                                                                           
        at weblogic.ejb.container.internal.BaseLocalObject.postInvoke1(BaseLocalObject.java:591)                                                                                                                                
        at weblogic.ejb.container.internal.BaseLocalObject.postInvokeTxRetry(BaseLocalObject.java:424)                                                                                                                          
        at ch.ejpd.sireneit.facade.ejb.ablage.DokumentFacadeBean_7xdnsq_DokumentFacadeImpl.updateStructuredDokument(DokumentFacadeBean_7xdnsq_DokumentFacadeImpl.java:340)                                                      
        at ch.ejpd.sireneit.access.rest.ablage.DokumentResource.update(DokumentResource.java:453)                                                                                                                               
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)                                                                                                                                                          
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)                                                                                                                                        
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)                                                                                                                                
        at java.lang.reflect.Method.invoke(Method.java:597)                                                                                                                                                                     
        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:175)                                                
        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:67)                                                                                         
        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:208)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:109)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:109)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:109)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:75)                                                                                                                             
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:67)                                                                                                                 
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:724)                                                                                                                
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:689)                                                                                                                 
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:680)                                                                                                                 
        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:324)                                                                                                                                     
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:425)                                                                                                                             
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:604)                                                                                                                             
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)                                                                                                                                                         
        at weblogic.servlet.internal.StubSecurityHelper$ServletServiceAction.run(StubSecurityHelper.java:227)                                                                                                                   
        at weblogic.servlet.internal.StubSecurityHelper.invokeServlet(StubSecurityHelper.java:125)                                                                                                                              
        at weblogic.servlet.internal.ServletStubImpl.execute(ServletStubImpl.java:292)                                                                                                                                          
        at weblogic.servlet.internal.TailFilter.doFilter(TailFilter.java:26)                                                                                                                                                    
        at weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:42)                                                                                                                                          
        at ch.ejpd.lib.webclient.jfa.JfaTokenServletFilter.doFilter(JfaTokenServletFilter.java:108)                                                                                                                             
        at weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:42)                                                                                                                                          
        at weblogic.servlet.internal.WebAppServletContext$ServletInvocationAction.run(WebAppServletContext.java:3496)                                                                                                           
        at weblogic.security.acl.internal.AuthenticatedSubject.doAs(AuthenticatedSubject.java:321)                                                                                                                              
        at weblogic.security.service.SecurityManager.runAs(Unknown Source)                                                                                                                                                      
        at weblogic.servlet.internal.WebAppServletContext.securedExecute(WebAppServletContext.java:2180)                                                                                                                        
        at weblogic.servlet.internal.WebAppServletContext.execute(WebAppServletContext.java:2086)                                                                                                                               
        at weblogic.servlet.internal.ServletRequestImpl.run(ServletRequestImpl.java:1406)                                                                                                                                       
        at weblogic.work.ExecuteThread.execute(ExecuteThread.java:201)                                                                                                                                                          
        at weblogic.work.ExecuteThread.run(ExecuteThread.java:173)                                                                                                                                                              
org.apache.jackrabbit.core.state.StaleItemStateException: e1863ec3-4eb7-483b-b1db-7586c089bc64/{}To has been modified externally                                                                                                
        at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:653)                                                                                                                
        at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:1110)                                                                                                                
        at org.apache.jackrabbit.core.state.XAItemStateManager.prepare(XAItemStateManager.java:163)                                                                                                                             
        at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:154)                                                                                                                                   
        at org.apache.jackrabbit.core.XASessionImpl.commit(XASessionImpl.java:346)                                                                                                                                              
        at org.apache.jackrabbit.jca.TransactionBoundXAResource.commit(TransactionBoundXAResource.java:39)                                                                                                                      
        at weblogic.connector.security.layer.AdapterLayer.commit(AdapterLayer.java:252)                                                                                                                                         
        at weblogic.connector.transaction.outbound.XAWrapper.commit(XAWrapper.java:113)                                                                                                                                         
        at weblogic.transaction.internal.XAServerResourceInfo.commit(XAServerResourceInfo.java:1334)                                                                                                                            
        at weblogic.transaction.internal.XAServerResourceInfo.commit(XAServerResourceInfo.java:578)                                                                                                                             
        at weblogic.transaction.internal.ServerSCInfo.startCommit(ServerSCInfo.java:547)                                                                                                                                        
        at weblogic.transaction.internal.ServerTransactionImpl.localCommit(ServerTransactionImpl.java:2006)                                                                                                                     
        at weblogic.transaction.internal.ServerTransactionImpl.globalRetryCommit(ServerTransactionImpl.java:2723)                                                                                                               
        at weblogic.transaction.internal.ServerTransactionImpl.globalCommit(ServerTransactionImpl.java:2645)                                                                                                                    
        at weblogic.transaction.internal.ServerTransactionImpl.internalCommit(ServerTransactionImpl.java:282)                                                                                                                   
        at weblogic.transaction.internal.ServerTransactionImpl.commit(ServerTransactionImpl.java:230)                                                                                                                           
        at weblogic.ejb.container.internal.BaseLocalObject.postInvoke1(BaseLocalObject.java:591)                                                                                                                                
        at weblogic.ejb.container.internal.BaseLocalObject.postInvokeTxRetry(BaseLocalObject.java:424)                                                                                                                          
        at ch.ejpd.sireneit.facade.ejb.ablage.DokumentFacadeBean_7xdnsq_DokumentFacadeImpl.updateStructuredDokument(DokumentFacadeBean_7xdnsq_DokumentFacadeImpl.java:340)                                                      
        at ch.ejpd.sireneit.access.rest.ablage.DokumentResource.update(DokumentResource.java:453)                                                                                                                               
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)                                                                                                                                                          
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)                                                                                                                                        
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)                                                                                                                                
        at java.lang.reflect.Method.invoke(Method.java:597)                                                                                                                                                                     
        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:175)                                                
        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:67)                                                                                         
        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:208)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:109)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:109)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:109)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:75)                                                                                                                             
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:67)                                                                                                                 
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:724)                                                                                                                
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:689)                                                                                                                 
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:680)                                                                                                                 
        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:324)                                                                                                                                     
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:425)                                                                                                                             
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:604)                                                                                                                             
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)                                                                                                                                                         
        at weblogic.servlet.internal.StubSecurityHelper$ServletServiceAction.run(StubSecurityHelper.java:227)                                                                                                                   
        at weblogic.servlet.internal.StubSecurityHelper.invokeServlet(StubSecurityHelper.java:125)                                                                                                                              
        at weblogic.servlet.internal.ServletStubImpl.execute(ServletStubImpl.java:292)                                                                                                                                          
        at weblogic.servlet.internal.TailFilter.doFilter(TailFilter.java:26)                                                                                                                                                    
        at weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:42)                                                                                                                                          
        at ch.ejpd.lib.webclient.jfa.JfaTokenServletFilter.doFilter(JfaTokenServletFilter.java:108)                                                                                                                             
        at weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:42)                                                                                                                                          
        at weblogic.servlet.internal.WebAppServletContext$ServletInvocationAction.run(WebAppServletContext.java:3496)                                                                                                           
        at weblogic.security.acl.internal.AuthenticatedSubject.doAs(AuthenticatedSubject.java:321)                                                                                                                              
        at weblogic.security.service.SecurityManager.runAs(Unknown Source)                                                                                                                                                      
        at weblogic.servlet.internal.WebAppServletContext.securedExecute(WebAppServletContext.java:2180)                                                                                                                        
        at weblogic.servlet.internal.WebAppServletContext.execute(WebAppServletContext.java:2086)                                                                                                                               
        at weblogic.servlet.internal.ServletRequestImpl.run(ServletRequestImpl.java:1406)                                                                                                                                       
        at weblogic.work.ExecuteThread.execute(ExecuteThread.java:201)                                                                                                                                                          
        at weblogic.work.ExecuteThread.run(ExecuteThread.java:173)                                                                                                                                                              
>                                                                                                                                                                                                                               "
1,Node.canAddMixin(String)after the spec this method must return false if the node is locked.
0,"Run 'test-tag' in nightly buildChanges in this trivial patch:
- ant target 'nightly' now also depends on 'test-tag'
- adds property 'compatibility.tag' to common-build.xml that should always point to the last tagged release; its unit tests will be downloaded unless -Dtag="""" is used to override
- 'download-tag' does not fail if the svn checkout wasn't successful; instead 'test-tag' checks if the specified tag is checked-out and available, if not it fails "
1,"TrecDocMaker skips over documents when ""Date"" is missing from documentsTrecDocMaker skips over Trec documents if they do not have a ""Date"" line. When such a document is encountered, the code may skip over several documents until the next tag that is searched for is found.
The result is, instead of reading ~25M documents from the GOV2 collection, the code reads only ~23M (don't remember the actual numbers).

The fix adds a terminatingTag to read() such that the code looks for prefix, but only until terminatingTag is found. Appropriate changes were made in getNextDocData().

Patch to follow"
1,"IndexWriter.addIndexes* can deadlock in rare casesIn somewhat rare cases it's possible for addIndexes to deadlock
because it is a synchronized method.

Normally the merges that are necessary for addIndexes are done
serially (with the primary thread) because they involve segments from
an external directory.  However, if mergeFactor of these merges
complete then a merge becomes necessary for the merged segments, which
are not external, and so it can run in the background.  If too many BG
threads need to run (currently > 4) then the ""pause primary thread""
approach adopted in LUCENE-1164 will deadlock, because the addIndexes
method is holding a lock on IndexWriter.

This was appearing as a intermittant deadlock in the
TestIndexWriterMerging test case.

This issue is not present in 2.3 (it was caused by LUCENE-1164).

The solution is to shrink the scope of synchronization: don't
synchronize on the whole method & wrap synchronized(this) in the right
places inside the methods."
0,"NRTManager shouldn't expose its private SearcherManagerSpinoff from LUCENE-3769.

To actually obtain an IndexSearcher from NRTManager, it's a 2-step process now.

You must .getSearcherManager(), then .acquire() from the returned SearcherManager.

This is very trappy... because if the app incorrectly calls maybeReopen on that private SearcherManager (instead of NRTManager.maybeReopen) then it can unexpectedly cause threads to block forever, waiting for the necessary gen to become visible.  This will be hard to debug... I don't like creating trappy APIs.

Hopefully once LUCENE-3761 is in, we can fix NRTManager to no longer expose its private SM, instead subclassing ReferenceManaager.

Or alternatively, or in addition, maybe we factor out a new interface (SearcherProvider or something...) that only has acquire and release methods, and both NRTManager and ReferenceManager/SM impl that, and we keep NRTManager's SM private."
1,"Error creating DB2 table for JournalingHi guys,

First of all, congratulations for the fantastic job.

I'm deploying a Jackrattbit-JCA resource adapter to a clustered Websphere environment and using a DB2 for storing data and realized some missing code to add DB2 support.

Here is:

JACKRABBIT-CORE, @ org.apache.jackrabbit.core.util.db, method guessValidationQuery (the last one), I adjusted as follows to add a DB2 validation query:

----------------------------------------------------------------------------------------------------------
    private String guessValidationQuery(String url) {
        if (url.contains(""derby"")) {
            return ""values(1)"";
        } else if (url.contains(""mysql"")) {
            return ""select 1"";
        } else if (url.contains(""sqlserver"") || url.contains(""jtds"")) {
            return ""select 1"";
        } else if (url.contains(""oracle"")) {
            return ""select 'validationQuery' from dual"";
        } else if (url.contains(""h2"")) {
            return ""select 1"";
        } else if (url.contains(""db2"")) {
        	return ""values(1)"";
        }
        log.warn(""Failed to guess validation query for URL "" + url);
        return null;
----------------------------------------------------------------------------------------------------------

And as a final touch, a DDL to build the tables:

JACKRABBIT-CORE, @ src/main/resources/org/apache/jackrabbit/core/journal, added a file named db2.dll as follows (actually I copied this from derby.dll)

----------------------------------------------------------------------------------------------------------
#  Licensed to the Apache Software Foundation (ASF) under one or more
#  contributor license agreements.  See the NOTICE file distributed with
#  this work for additional information regarding copyright ownership.
#  The ASF licenses this file to You under the Apache License, Version 2.0
#  (the ""License""); you may not use this file except in compliance with
#  the License.  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an ""AS IS"" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
create table ${schemaObjectPrefix}JOURNAL (REVISION_ID BIGINT NOT NULL, JOURNAL_ID varchar(255), PRODUCER_ID varchar(255), REVISION_DATA blob)
create unique index ${schemaObjectPrefix}JOURNAL_IDX on ${schemaObjectPrefix}JOURNAL (REVISION_ID)
create table ${schemaObjectPrefix}GLOBAL_REVISION (REVISION_ID BIGINT NOT NULL)
create unique index ${schemaObjectPrefix}GLOBAL_REVISION_IDX on ${schemaObjectPrefix}GLOBAL_REVISION (REVISION_ID)
create table ${schemaObjectPrefix}LOCAL_REVISIONS (JOURNAL_ID varchar(255) NOT NULL, REVISION_ID BIGINT NOT NULL)

# Inserting the one and only revision counter record now helps avoiding race conditions
insert into ${schemaObjectPrefix}GLOBAL_REVISION VALUES(0)
----------------------------------------------------------------------------------------------------------


Best regards,


J Marcos"
1,"testIWondiskfull checkindex failurelooks like charlie cron created a corrupt index on disk full.. can't reproduce with the seed on this machine, i can try on that VM with the same environment and see if i have better luck."
0,"[PATCH] usage feedback for IndexFiles demoJust a small patch that adds ""usage"" output if the demo is called without a 
parameter, makes it a little bit friendlier to beginners."
0,"[PATCH] remove unused variablesSeems I'm the only person who has the ""unused variable"" warning turned on in 
Eclipse :-) This patch removes those unused variables and imports (for now 
only in the ""search"" package). This doesn't introduce changes in 
functionality, but it should be reviewed anyway: there might be cases where 
the variables *should* be used, but they are not because of a bug."
1,"multitermquery scoring differences between 3x and trunktry this patch with a test, that applies clean to both 3x and trunk, but fails on trunk.

if you modify the test-data-generator to use TopTerms*BoostOnly* rewrite, then it acts like TestFuzzyQuery2, and passes.

So the problem is in TopTermsScoringBooleanRewrite, or BooleanQuery, or somewhere else.
"
1,"In NRT mode, and CFS enabled, IndexWriter incorrectly ties up disk spaceSpinoff of java-user thread titled ""searching while optimize""...

If IndexWriter is in NRT mode (you've called getReader() at least
once), and CFS is enabled, then internally the writer pools readers.
However, after a merge completes, it opens the reader against het
non-CFS segment files, and pools that.  It then builds the CFS file,
as well, thus tying up the storage for that segment twice.

Functionally the bug is harmless (it's only a disk space issue).
Also, when the segment is merged, the disk space is released again
(though the newly merged segment will also be double-tied-up).

Simple workaround is to use non-CFS mode, or, don't use getReader."
0,No support for Oracle schemasJackrabbit assumes that the table name (f.e.: prefix + fs_entry) is unique across all schemas  used on one oracle instance. So two Jackrabbit instances using two schemas on the same Oracle instance won't work. 
0,"PayloadNearQuery has hardwired explanation for 'AveragePayloadFunction'The 'explain' method in PayloadNearSpanScorer assumes the AveragePayloadFunction was used. This patch adds the 'explain' method to the 'PayloadFunction' interface, where the Scorer can call it. Added unit tests for 'explain' and for {Min,Max}PayloadFunction."
1,"cache getting out of sync with transientstore causes pathnotfoundexceptionDone some further debugging and think the problem is in the synchronization between cache and transientstore. When I retrieve a childnode when I just made its parent node transient (by removing a prop or something), it will not be added to the cache. When I then remove this node, its nodeid is not removed from cache since its stateId wasn't saved in the cache.  After that I add the same node node again with the same name. When I now try to retrieve this node, I get a path not found exception. I see that by retrieving it, its nodeit is resolved from the cache using its path. Only since the removed node was not removed from cache it returns the nodeid of the already removed node. There is no node present with this id in the transientstore and therefor it throws a pathnotfoundexception.

provided a failing junit test and repository.xml"
0,"Variant spelling ""Trasaction"" and ""Transaction"" now in codebaseIncorrectly spelled ""Trasaction"" now in codebase.  The correctly spelled ""Transaction"" appears a far greater number of times.

Isolated to package: org.apache.jackrabbit.jca

Incorrectly  spelled ""Trasaction"" found in:
(3x) JCAManagedConnection.java 
(7x) JCAManagedConnectionFActory.java

Correctly spelled ""Transaction"" found in:
(20x) JCAManagedConnection.java 
(2x) JCAManagedConnectionFActory.java 
(1x) JCAResourceAdapter.java 
(12x) TransactionBoundXAResource.java 
"
0,"Inconsistent, order dependant behaviour in HttpMethodBase.getResponse*`getReponseBodyAsString` is storing the body  and may therefore provide a valid result if the code is requesting the body as stream afterwards. If you switch the order and first call getResponseBodyAsStream and afterwards try to `getReponseBodyAsString`, the result will be `null`.

I wrote a unittest which hopefully describes the IMHO confusing behaviour:

    public void testHttpClientBodyVsStream() throws HttpException, IOException {
        final HttpClient httpClient = new HttpClient();
        final GetMethod getMethod = new GetMethod(""http://www.heise.de/"");
        final String bodyFromStream;
        final String body;
        try {
            httpClient.executeMethod(getMethod);
            body = getMethod.getResponseBodyAsString();
            bodyFromStream = IOUtils.toString(getMethod
                    .getResponseBodyAsStream());
        } finally {
            getMethod.releaseConnection();
        }
        assertEquals(body, bodyFromStream);
    }
    
    public void testHttpClientStreamVsBody() throws HttpException, IOException {
        final HttpClient httpClient = new HttpClient();
        final GetMethod getMethod = new GetMethod(""http://www.heise.de/"");
        final String bodyFromStream;
        final String body;
        try {
            httpClient.executeMethod(getMethod);
            bodyFromStream = IOUtils.toString(getMethod
                    .getResponseBodyAsStream());
            body = getMethod.getResponseBodyAsString();
        } finally {
            getMethod.releaseConnection();
        }
        // ** This will fail **
        assertEquals(body, bodyFromStream);
    }

Searching http://svn.apache.org/repos/asf/jakarta/commons/proper/httpclient/trunk/src/java/org/apache/commons/httpclient/HttpMethodBase.java I understand the outcome, but this is confusing.

I would expect the body data to be gone after calling one of the getResponse*-Methods and calling them again not to return null but even to throw an IllegalStateException. I would not store the body at all in the method.
"
0,"Kerberos Authentication SchemeHttpClient 4.1.2 has a SPNEGO authentication that uses the Negotiate keyword.  But the MS IIS that I must connect to does not send back an WWW-Authenticate: Negotiate, but, instead, does send an WWW-Authenticate: Kerberos

So I used the NegotiateScheme.java and NegotiateSchemeFactory.java as a base to create a ""new"" scheme, called, KerberosScheme.java and KerberosSchemeFactory.java to make it work.

Essentially I replaced every ""Negotiate"" scheme by ""Kerberos"", in the KerberosScheme.java, and removed the part of the code that tried, first, SPNEGO_OID, using KERBEROS_OID directly, instead.

It works fine for me, but took me a while to figure this out.

That why I think it could come on the new versions.

I'll attach my version but it has no package - it was made only for a test project.  It's trivial to put it in the right place/package.
"
1,"JCR2SPI: test regression for WorkspaceMoveReferenceableTest.testMoveNodesReferenceableNodesNewUUIDThe latest changes (up to 581637) seems to have broken TCK tests:


-------------------------------------------------------------------------------
Test set: org.apache.jackrabbit.test.TestAll
-------------------------------------------------------------------------------
Tests run: 1037, Failures: 6, Errors: 2, Skipped: 0, Time elapsed: 102.644 sec <<< FAILURE!
testMoveNodesReferenceableNodesNewUUID(org.apache.jackrabbit.test.api.WorkspaceMoveReferenceableTest)  Time elapsed: 0.03 sec  <<< ERROR!
javax.jcr.InvalidItemStateException: Item 'org.apache.jackrabbit.jcr2spi.NodeImpl@13ef9df' doesn't exist anymore
    at org.apache.jackrabbit.jcr2spi.ItemImpl.checkStatus(ItemImpl.java:428)
    at org.apache.jackrabbit.jcr2spi.NodeImpl.getName(NodeImpl.java:120)
    at org.apache.jackrabbit.test.api.WorkspaceMoveReferenceableTest.testMoveNodesReferenceableNodesNewUUID(WorkspaceMoveReferenceableTest.java:57) "
0,"OperandEvaluator should be able to handle Nodes as well, not just RowsOperandEvaluator is used to evaluate Operands values against given Rows, and in an effort to improve the sorting part of SQL2 (JCR-2959), I need it to handle plain Nodes as well.

This is a small change, as the OperandEvaluator already extracts the Node info from the Row, so there is no obvious reason no to expose the Node operations directly."
0,"CachingIndexReader.initializeParents() does not scale well with large indexesOn a 40+ GB index that I'm testing with, the time to initialize the parents cache is 40 minutes.

This is way to much, needs optimization and should be done in a background thread."
1,"FSDirectory.list() is inconsistentLUCENE-638 added a check to the FSDirectory.list() method to only return files that are Lucene related. I think this change made the FSDirectory implementation inconsistent with all other methods in Directory. E.g. you can create a file with an arbitrary name using FSDirectory, fileExists() will report that it is there, deleteFile() will remove it, but the array returned by list() will not contain the file.

The actual issue that was reported in LUCENE-638 was about sub directories. Those should clearly not be listed, but IMO it is not the responsibility of a Directory implementation to decide what kind of files can be created or listed. The Directory class is an abstraction of a directory and it should't to more than that.
"
0,"Allow CFS be emptysince we changed CFS semantics slightly closing a CFS directory on an error can lead to an exception. Yet, an empty CFS is still a valid CFS so for consistency we should allow CFS to be empty.
here is an example:

{noformat}
1 tests failed.
REGRESSION:  org.apache.lucene.index.TestIndexWriterOnDiskFull.testAddDocumentOnDiskFull

Error Message:
CFS has no entries

Stack Trace:
java.lang.IllegalStateException: CFS has no entries
       at org.apache.lucene.store.CompoundFileWriter.close(CompoundFileWriter.java:139)
       at org.apache.lucene.store.CompoundFileDirectory.close(CompoundFileDirectory.java:181)
       at org.apache.lucene.store.DefaultCompoundFileDirectory.close(DefaultCompoundFileDirectory.java:58)
       at org.apache.lucene.index.SegmentMerger.createCompoundFile(SegmentMerger.java:139)
       at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4252)
       at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3863)
       at org.apache.lucene.index.SerialMergeScheduler.merge(SerialMergeScheduler.java:37)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:2715)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:2710)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:2706)
       at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3513)
       at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:2064)
       at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:2031)
       at org.apache.lucene.index.TestIndexWriterOnDiskFull.addDoc(TestIndexWriterOnDiskFull.java:539)
       at org.apache.lucene.index.TestIndexWriterOnDiskFull.testAddDocumentOnDiskFull(TestIndexWriterOnDiskFull.java:74)
       at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1277)
       at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1195)
{noformat}"
0,"move route computation from client to directorThe computation of routes should be done in the ClientRequestDirector, not in the Client.
The director needs to compute routes for redirects, so it should compute all routes.
"
0,"Add a constructor to org.apache.http.conn.ssl.SSLSocketFactory to allow for directly wrapping a javax.net.ssl.SSLSocketFactory socketfactoryOur application use Java Webstart for deployment.  Amoung other things, Webstart gives us the ability to access the system's (in our case, Windows) certificate system.  For instance, one of our client is using certificate based authentication to their webserver.  This is done through a hardware device they attach to their system.  Window's already has a way to interface with this device, and Webstart has a way to interface with the Windows API.

I don't think we can get by with using any SocketFactory that we create.  (We would have to check with Oracle to be sure.)  I think we need to use the one that is set as the default in HttpsURLConnection.

What I am suggesting is that another constructor be added to allow for just wrapping this one.  I was not planning on putting a dependancy on HttpsURLConnection, but rather just add the ability to wrap any javax.net.ssl.SSLSocketFactory.

This will not be a big change to the API.  I will get a patch ready soon."
1,"ConcurrentModificationException in FineGrainedISMLockingWe have a report where the FineGrainedISMLocking throws a ConcurrentModificationException (stacktrace
from a Jackrabbit 2.2.x):

java.util.ConcurrentModificationException
	at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
	at java.util.HashMap$KeyIterator.next(HashMap.java:828)
	at org.apache.jackrabbit.core.state.FineGrainedISMLocking$LockMap.hasDependency(FineGrainedISMLocking.java:388)
	at org.apache.jackrabbit.core.state.FineGrainedISMLocking.acquireWriteLock(FineGrainedISMLocking.java:138)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.acquireWriteLock(SharedItemStateManager.java:1848)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.access$200(SharedItemStateManager.java:113)
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:563)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:1457)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:1487)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:351)
	at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:354)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:326)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:289)
	at org.apache.jackrabbit.core.ItemSaveOperation.perform(ItemSaveOperation.java:258)
	at org.apache.jackrabbit.core.session.SessionState.perform(SessionState.java:200)
	at org.apache.jackrabbit.core.ItemImpl.perform(ItemImpl.java:91)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:329)
	at org.apache.jackrabbit.core.session.SessionSaveOperation.perform(SessionSaveOperation.java:42)
	at org.apache.jackrabbit.core.session.SessionState.perform(SessionState.java:200)
	at org.apache.jackrabbit.core.SessionImpl.perform(SessionImpl.java:355)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:758)"
1,"NPE Exception Thrown By AbstractJournal During Commit OperationThis seems related to JCR-712 (which was apparently fixed in 1.2.2), but I see the following error now-and-then on JR 1.2.2 (I'm using the DB based journal implementation with Oracle 10g):

java.lang.NullPointerException
        at org.apache.jackrabbit.core.cluster.AbstractJournal.commit(AbstractJournal.java:525)
        at org.apache.jackrabbit.core.cluster.ClusterNode.updateCommitted(ClusterNode.java:424)
        at org.apache.jackrabbit.core.cluster.ClusterNode$WorkspaceUpdateChannel.updateCommitted(ClusterNode.java:565)
        at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:712)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:808)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:326)
        at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:313)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:302)
        at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:308)
        at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1204)
        at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:823)
"
0,"building trunk  fails with javacc plugin version 2.2

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[23,38] cannot find symbol
symbol: class JCRSQLParserVisitor
class DefaultParserVisitor implements JCRSQLParserVisitor {

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[25,24] cannot find symbol
symbol  : class SimpleNode
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[29,24] cannot find symbol
symbol  : class ASTQuery
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[33,24] cannot find symbol
symbol  : class ASTSelectList
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[37,24] cannot find symbol
symbol  : class ASTFromClause
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[41,24] cannot find symbol
symbol  : class ASTWhereClause
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTPredicate.java:[21,34] cannot find symbol
symbol: class SimpleNode
public class ASTPredicate extends SimpleNode {

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[49,24] cannot find symbol
symbol  : class ASTOrExpression
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[53,24] cannot find symbol
symbol  : class ASTAndExpression
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[57,24] cannot find symbol
symbol  : class ASTNotExpression
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[61,24] cannot find symbol
symbol  : class ASTBracketExpression
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTLiteral.java:[19,32] cannot find symbol
symbol: class SimpleNode
public class ASTLiteral extends SimpleNode {

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTIdentifier.java:[21,35] cannot find symbol
symbol: class SimpleNode
public class ASTIdentifier extends SimpleNode {

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[73,24] cannot find symbol
symbol  : class ASTOrderByClause
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTContainsExpression.java:[21,43] cannot find symbol
symbol: class SimpleNode
public class ASTContainsExpression extends SimpleNode {

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[81,24] cannot find symbol
symbol  : class ASTOrderSpec
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[85,24] cannot find symbol
symbol  : class ASTAscendingOrderSpec
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[89,24] cannot find symbol
symbol  : class ASTDescendingOrderSpec
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[93,24] cannot find symbol
symbol  : class ASTLowerFunction
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[97,24] cannot find symbol
symbol  : class ASTUpperFunction
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[101,24] cannot find symbol
symbol  : class ASTExcerptFunction
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTPredicate.java:[37,22] cannot find symbol
symbol  : class JCRSQLParser
location: class org.apache.jackrabbit.core.query.sql.ASTPredicate

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTPredicate.java:[82,26] cannot find symbol
symbol  : class JCRSQLParserVisitor
location: class org.apache.jackrabbit.core.query.sql.ASTPredicate

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTLiteral.java:[30,22] cannot find symbol
symbol  : class JCRSQLParser
location: class org.apache.jackrabbit.core.query.sql.ASTLiteral

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTLiteral.java:[54,28] cannot find symbol
symbol  : class JCRSQLParserVisitor
location: class org.apache.jackrabbit.core.query.sql.ASTLiteral

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTIdentifier.java:[29,23] cannot find symbol
symbol  : class JCRSQLParser
location: class org.apache.jackrabbit.core.query.sql.ASTIdentifier

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTIdentifier.java:[42,26] cannot find symbol
symbol  : class JCRSQLParserVisitor
location: class org.apache.jackrabbit.core.query.sql.ASTIdentifier

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTContainsExpression.java:[31,33] cannot find symbol
symbol  : class JCRSQLParser
location: class org.apache.jackrabbit.core.query.sql.ASTContainsExpression

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTContainsExpression.java:[54,28] cannot find symbol
symbol  : class JCRSQLParserVisitor
location: class org.apache.jackrabbit.core.query.sql.ASTContainsExpression

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/lucene/LuceneQueryBuilder.java:[44,56] cannot find symbol
symbol  : class QueryParser
location: package org.apache.jackrabbit.core.query.lucene.fulltext

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/lucene/LuceneQueryBuilder.java:[45,56] cannot find symbol
symbol  : class ParseException
location: package org.apache.jackrabbit.core.query.lucene.fulltext

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[55,42] cannot find symbol
symbol: class XPathVisitor
public class XPathQueryBuilder implements XPathVisitor, XPathTreeConstants {

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[55,56] cannot find symbol
symbol: class XPathTreeConstants
public class XPathQueryBuilder implements XPathVisitor, XPathTreeConstants {

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[23,35] cannot find symbol
symbol: class Node
public class SimpleNode implements Node {

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[24,14] cannot find symbol
symbol  : class Node
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[25,14] cannot find symbol
symbol  : class Node
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[27,14] cannot find symbol
symbol  : class XPath
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[33,22] cannot find symbol
symbol  : class XPath
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[39,33] cannot find symbol
symbol  : class XPath
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[39,18] cannot find symbol
symbol  : class Node
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[49,29] cannot find symbol
symbol  : class Node
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[53,11] cannot find symbol
symbol  : class Node
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[57,28] cannot find symbol
symbol  : class Node
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[68,11] cannot find symbol
symbol  : class Node
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[79,28] cannot find symbol
symbol  : class XPathVisitor
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[86,33] cannot find symbol
symbol  : class XPathVisitor
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[132,29] cannot find symbol
symbol  : class Token
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/lucene/JQOM2LuceneQueryBuilder.java:[53,56] cannot find symbol
symbol  : class QueryParser
location: package org.apache.jackrabbit.core.query.lucene.fulltext

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/lucene/fulltext/FastCharStream.java:[30,45] cannot find symbol
symbol: class CharStream
public final class FastCharStream implements CharStream {

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[60,43] cannot find symbol
symbol: class JCRSQLParserVisitor
public class JCRSQLQueryBuilder implements JCRSQLParserVisitor {

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[81,18] cannot find symbol
symbol  : class ASTQuery
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[121,31] cannot find symbol
symbol  : class ASTQuery
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[201,24] cannot find symbol
symbol  : class SimpleNode
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[206,24] cannot find symbol
symbol  : class ASTQuery
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[279,24] cannot find symbol
symbol  : class ASTSelectList
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[297,24] cannot find symbol
symbol  : class ASTFromClause
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[311,24] cannot find symbol
symbol  : class ASTWhereClause
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[447,24] cannot find symbol
symbol  : class ASTOrExpression
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[459,24] cannot find symbol
symbol  : class ASTAndExpression
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[469,24] cannot find symbol
symbol  : class ASTNotExpression
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[479,24] cannot find symbol
symbol  : class ASTBracketExpression
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[496,24] cannot find symbol
symbol  : class ASTOrderByClause
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[505,24] cannot find symbol
symbol  : class ASTOrderSpec
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[526,24] cannot find symbol
symbol  : class ASTAscendingOrderSpec
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[531,24] cannot find symbol
symbol  : class ASTDescendingOrderSpec
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[556,24] cannot find symbol
symbol  : class ASTLowerFunction
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[566,24] cannot find symbol
symbol  : class ASTUpperFunction
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[576,24] cannot find symbol
symbol  : class ASTExcerptFunction
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTPredicate.java:[87,15] cannot find symbol
symbol  : variable super
location: class org.apache.jackrabbit.core.query.sql.ASTPredicate

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTLiteral.java:[59,15] cannot find symbol
symbol  : variable super
location: class org.apache.jackrabbit.core.query.sql.ASTLiteral

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTIdentifier.java:[47,15] cannot find symbol
symbol  : variable super
location: class org.apache.jackrabbit.core.query.sql.ASTIdentifier

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/lucene/LuceneQueryBuilder.java:[376,12] cannot find symbol
symbol  : class QueryParser
location: class org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/lucene/LuceneQueryBuilder.java:[376,37] cannot find symbol
symbol  : class QueryParser
location: class org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/lucene/LuceneQueryBuilder.java:[378,31] cannot find symbol
symbol  : variable QueryParser
location: class org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/lucene/LuceneQueryBuilder.java:[440,17] cannot find symbol
symbol  : class ParseException
location: class org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[267,12] cannot find symbol
symbol  : class XPath
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[269,26] cannot find symbol
symbol  : class XPath
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[271,33] cannot find symbol
symbol  : class XPath
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[283,17] cannot find symbol
symbol  : class ParseException
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[356,17] cannot find symbol
symbol  : variable JJTXPATH2
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[359,17] cannot find symbol
symbol  : variable JJTROOT
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[360,17] cannot find symbol
symbol  : variable JJTROOTDESCENDANTS
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[368,17] cannot find symbol
symbol  : variable JJTSTEPEXPR
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[415,17] cannot find symbol
symbol  : variable JJTNAMETEST
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[429,17] cannot find symbol
symbol  : variable JJTELEMENTNAMEORWILDCARD
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[432,41] cannot find symbol
symbol  : variable JJTANYNAME
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[437,17] cannot find symbol
symbol  : variable JJTTEXTTEST
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[443,17] cannot find symbol
symbol  : variable JJTTYPENAME
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[456,17] cannot find symbol
symbol  : variable JJTOREXPR
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[463,17] cannot find symbol
symbol  : variable JJTANDEXPR
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[470,17] cannot find symbol
symbol  : variable JJTCOMPARISONEXPR
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[473,17] cannot find symbol
symbol  : variable JJTSTRINGLITERAL
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[474,17] cannot find symbol
symbol  : variable JJTDECIMALLITERAL
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[475,17] cannot find symbol
symbol  : variable JJTDOUBLELITERAL
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[476,17] cannot find symbol
symbol  : variable JJTINTEGERLITERAL
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[480,40] cannot find symbol
symbol  : variable JJTINTEGERLITERAL
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[490,17] cannot find symbol
symbol  : variable JJTUNARYMINUS
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[497,17] cannot find symbol
symbol  : variable JJTFUNCTIONCALL
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[500,17] cannot find symbol
symbol  : variable JJTORDERBYCLAUSE
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[505,17] cannot find symbol
symbol  : variable JJTORDERMODIFIER
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder
"
1,"setProperty access control evaluation does not properly cope with XA transactionsThis is another instance of the problems with ACL evaluation within transactions described in https://issues.apache.org/jira/browse/JCR-2999.
In this case PropertyImpl#getParent() called from PropertyImpl#checkSetValue() is trying to check read permissions of the yet uncommited parent and thus fails with an ItemNotFound exception.

The problem is reproducible with the following test:

public void testTransaction() throws Exception {

        // make sure testUser has all privileges
        Privilege[] privileges = privilegesFromName(Privilege.JCR_ALL);
        givePrivileges(path, privileges, getRestrictions(superuser, path));

        // create new node and lock it
        Session s = getTestSession();
        UserTransaction utx = new UserTransactionImpl(s);
        utx.begin();

        // add node and save it
        Node n = s.getNode(childNPath);
        if (n.hasNode(nodeName1)) {
            Node c = n.getNode(nodeName1);
            c.remove();
            s.save();
        }

        // create node and save
        Node n2 = n.addNode(nodeName1);
        s.save(); // -> node is NEW -> no failure

        // set a property on a child node of an uncommited parent
        n2.setProperty(propertyName1, ""testSetProperty"");
        s.save();  // -> fail because PropertyImpl#getParent called from PropertyImpl#checkSetValue
                       //    was checking read permission on the not yet commited parent

        // commit
        utx.commit();
    }"
1,"IndexWriterConfig does not allow readerTermsIndexDivisor to be -1, while the latest indicates the terms index should not be loadedWhile you can pass -1 to IR.open(), and it's documented, you cannot do the same for IndexWriter's readers (b/c IWC blocks it). Need to allow this setting as well as add support for it in our tests, e.g. we should randomly set it to -1. Robert also suggested RandomIW use -1 randomly when it opens readers.

I'll work on a patch"
0,"Deprecate ""create"" method in FSDirectory.getDirectory in favor of IndexWriter's ""create""It's confusing that there is a create=true|false at the FSDirectory
level and then also another create=true|false at the IndexWriter
level.  Which one should you use when creating an index?

Our users have been confused by this in the past:

  http://www.gossamer-threads.com/lists/lucene/java-user/4792

I think in general we should try to have one obvious way to achieve
something (like Python: http://en.wikipedia.org/wiki/Python_philosophy).

And the fact that there are now two code paths that are supposed to do
the same (similar?) thing, can more easily lead to sneaky bugs.  One
case of LUCENE-140 (already fixed in trunk but not past releases),
which inspired this issue, can happen if you send create=false to the
FSDirectory and create=true to the IndexWriter.

Finally, as of lockless commits, it is now possible to open an
existing index for ""create"" while readers are still using the old
""point in time"" index, on Windows.  (At least one user had tried this
previously and failed).  To do this, we use the IndexFileDeleter class
(which retries on failure) and we also look at the segments file to
determine the next segments_N file to write to.

With future issues like LUCENE-710 even more ""smarts"" may be required
to know what it takes to ""create"" a new index into an existing
directory.  Given that we have have quite a few Directory
implemenations, I think these ""smarts"" logically should live in
IndexWriter (not replicated in each Directory implementation), and we
should leave the Directory as an interface that knows how to make
changes to some backing store but does not itself try to make any
changes.
"
1,"Digest auth uses wrong uri in proxy authenticationI'm having a problem getting httpclient-rc1 to authenticate using
digest to our IAS server.  I've tried upgrading to rc3 without any
effect.  I also got our IT guys to upgrade IAS without luck.  I was
also able to have the GET method work under IAS and CONNECT to work
with a couple other proxy servers.  After examining ethereal logs for
my (commons) code and firefox to the same URLs I noticed that the
value for the ""uri"" setting in the ""Proxy-Authorization"" header was
the only significant difference.  After looking at RFC 2617 I noticed
that in section 3.2.2 (The Authorization Request Header) it states:

digest-uri
The URI from Request-URI of the Request-Line; duplicated here because
proxies are allowed to change the Request-Line in transit.

A re-examination of the headers showed that firefox was matching the Request-URI
with the digest-uri but that httpclient was not.  I reproduced partial headers
below.  I tried modifying the RC3 source to produce a hard-coded value for ""uri""
and demonstrated that it would successfully authenticate to that URI.  I also
checked that authentication would fail to any other URI and it did.

partial httpclient header (fails with 407):
CONNECT gmail.google.com:443 HTTP/1.1
Proxy-Authorization: Digest username=""proxytest"", realm=""Digest"",
nonce=""503902c343c8c501057a85cea6bad2734378fb44b4cbd1970bf320637871dae85373082cf70ac254"",
uri=""/"", response=""7717d0738332a3d8e83e9102b5ead6b9"", qop=""auth"", nc=00000001,
cnonce=""583aa0469b31290dc2acd7ec6cfc98f1"", algorithm=""MD5-sess"",
opaque=""bb319760fce84856e5648d3536502d81""

partial firefox header (succeeds with 200):
CONNECT mail1.combrio.local:443 HTTP/1.1
Proxy-Authorization: Digest username=""proxytest"", realm=""Digest"",
nonce=""0e61fe645ec8c5015aa3afe8cfe5219488ed473e277a8cddf8225ad66e74fd214f97d9d96ac99991"",
uri=""mail1.combrio.local:443"", algorithm=MD5-sess,
response=""bfac109287273e867531170475172ccf"",
opaque=""70cb2a1533b85882d0f1aa1e2ad1fbae"", qop=auth, nc=00000001,
cnonce=""b41aecd6e527e774"""
0,"Add support for 3.0 indexes in 2.9 branchThere was a lot of user requests to be able to read Lucene 3.0 indexes also with 2.9. This would make the migration easier. There is no problem in doing that, as the new stored fields version in Lucene 3.0 is only used to mark a segment's stored fields file as no longer containing compressed fields. But index format did not really change. This patch simply allows FieldsReader to pass a Lucene 3.0 version number, but still writes segments in 2.9 format (as you could suddenly turn on compression for added documents).

I added ZIP files for 3.0 indexes for TestBackwards. Without the patch it does not pass, as FieldsReader complains about incorrect version number (although it could read the file easily). If we would release maybe a 2.9.4 release of Lucene we should include that patch."
0,replace Vector with ArrayList in QueriesReplace Vector with ArrayList in Queries.  This can make a difference in heavily concurrent scenarios when Query objects are examined or compared (e.g. used as cache keys).
0,"duplicate package.html files in queryParser and analsysis.cn packagesThese files conflict with eachother when building the javadocs. there can be only one (of each) ...

{code}
hossman@brunner:~/lucene/java$ find src contrib -name package.html | perl -ple 's{.*src/java/}{}' | sort | uniq -c | grep -v "" 1 ""
   2 org/apache/lucene/analysis/cn/package.html
   2 org/apache/lucene/queryParser/package.html
hossman@brunner:~/lucene/java$ find src contrib -path \*queryParser/package.html
src/java/org/apache/lucene/queryParser/package.html
contrib/queryparser/src/java/org/apache/lucene/queryParser/package.html
hossman@brunner:~/lucene/java$ find src contrib -path \*cn/package.html
contrib/analyzers/common/src/java/org/apache/lucene/analysis/cn/package.html
contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/package.html
{code}

"
0,"MultiReader and ParallelReader accidently override doOpenIfChanged(boolean readOnly) with doOpenIfChanged(boolean doClone)I found this during adding deprecations for RW access in LUCENE-3606:

the base class defines doOpenIfChanged(boolean readOnly), but MultiReader and ParallelReader ""override"" this method with a signature doOpenIfChanged(doClone) and missing @Override. This makes consumers calling IR.openIfChanged(boolean readOnly) do the wrong thing. Instead they should get UOE like for the other unimplemented doOpenIfChanged methods in MR and PR.

Easy fix is to rename and hide this internal ""reopen"" method, like DirectoryReader,..."
0,"Deprecate/remove language-specific tokenizers in favor of StandardTokenizerAs of Lucene 3.1, StandardTokenizer implements UAX#29 word boundary rules to provide language-neutral tokenization.  Lucene contains several language-specific tokenizers that should be replaced by UAX#29-based StandardTokenizer (deprecated in 3.1 and removed in 4.0).  The language-specific *analyzers*, by contrast, should remain, because they contain language-specific post-tokenization filters.  The language-specific analyzers should switch to StandardTokenizer in 3.1.

Some usages of language-specific tokenizers will need additional work beyond just replacing the tokenizer in the language-specific analyzer.  

For example, PersianAnalyzer currently uses ArabicLetterTokenizer, and depends on the fact that this tokenizer breaks tokens on the ZWNJ character (zero-width non-joiner; U+200C), but in the UAX#29 word boundary rules, ZWNJ is not a word boundary.  Robert Muir has suggested using a char filter converting ZWNJ to spaces prior to StandardTokenizer in the converted PersianAnalyzer."
0,"Stop storing TermsEnum in CloseableThreadLocal inside Terms instanceWe have sugar methods in Terms.java (docFreq, totalTermFreq, docs,
docsAndPositions) that use a saved thread-private TermsEnum to do the
lookups.

But on apps that send many threads through Lucene, and/or have many
segments, this can add up to a lot of RAM, especially if the codecs
impl holds onto stuff.

Also, Terms has a close method (closes the CloseableThreadLocal) which
must be called, but we fail to do so in some places.

These saved enums are the cause of the recent OOME in TestNRTManager
(TestNRTManager.testNRTManager -seed
2aa27e1aec20c4a2:-4a5a5ecf46837d0e:-7c4f651f1f0b75d7 -mult 3
-nightly).

Really sharing these enums is a holdover from before Lucene queries
would share state (ie, save the TermState from the first pass, and use
it later to pull enums, get docFreq, etc.).  It's not helpful anymore,
and it can use gobbs of RAM, so I'd like to remove it.
"
1,"same named child nodes disappear on restoreWhen restoring a versionable node which has several (non-versionable) child nodes with the same name, some child nodes disappear. 

            Node node = session.getRootNode().addNode(""myNode"");
            node.addMixin(""mix:versionable"");
            for (int i = 1; i < 6; i++) {
                Node child = node.addNode(""child"");
                child.setProperty(""name"", ""child_""+i);
            }
            session.save();
            VersionManager versionManager = session.getWorkspace().getVersionManager();
            versionManager.checkin(node.getPath());
            System.out.println(""number of child nodes: "" + node.getNodes().getSize());

            versionManager.checkout(node.getPath());
            node.getNode(""child"").setProperty(""name"", ""modified"");
            session.save();
            Version baseVersion = versionManager.getBaseVersion(node.getPath());
            versionManager.restore(baseVersion, true);
            System.out.println(""number of child nodes in restored node: ""+node.getNodes().getSize());


produces the following output:

number of child nodes: 5
number of child nodes in restored node: 3

Giving unique names or adding the mixin versionable to the child nodes solves the problem.
"
1,"HttpMethodBase: Port mismatch in URL for redirect to absolute locationThe CVS version of latka was failing a test ( $ ant test, not maven ), when
using the CVS version of httpclient. The message was

 [java] WARN  [main] org.apache.commons.httpclient.HttpMethod
      - Redirect from port 80 to -1 is not supported: 21 Sep 2002 00:54:32,098

The request preceding this was:

 [java] DEBUG [main] httpclient.wire - >> ""GET /commons HTTP/1.1
     [java] "" [\r\n]: 21 Sep 2002 00:54:31,262
     [java] DEBUG [main] org.apache.commons.httpclient.HttpMethod - enter
[java] DEBUG [main] httpclient.wire - >> ""Host: jakarta.apache.org
     [java] "" [\r\n]: 21 Sep 2002 00:54:31,441
     [java] DEBUG [main] httpclient.wire - >> ""User-Agent: Jakarta
Commons-HttpClient/2.0M1
 [java] "" [\r\n]: 21 Sep 2002 00:54:31,442

And the response was:

 [java] DEBUG [main] org.apache.commons.httpclient.HttpMethod - enter
HttpMethodBase.readStatusLine(HttpState, HttpConnection): 21 Sep 2002 00:54:31,444
     [java] DEBUG [main] org.apache.commons.httpclient.HttpConnection - enter
HttpConnection.readLine(): 21 Sep 2002 00:54:31,444
[java] DEBUG [main] httpclient.wire - << ""HTTP/1.1 301 Moved Permanently""
[\r\n]: 21 Sep 2002 00:54:32,080
     [java] DEBUG [main] org.apache.commons.httpclient.HttpMethod - enter
HttpMethodBase.readResponseHeaders(HttpState,HttpConnection): 21 Sep 2002
00:54:32,086
     [java] DEBUG [main] org.apache.commons.httpclient.HttpConnection - enter
HttpConnection.readLine(): 21 Sep 2002 00:54:32,087
[java] DEBUG [main] httpclient.wire - << ""Date: Fri, 20 Sep 2002 23:54:30 GMT""
[\r\n]: 21 Sep 2002 00:54:32,088
     [java] DEBUG [main] org.apache.commons.httpclient.HttpConnection - enter
HttpConnection.readLine(): 21 Sep 2002 00:54:32,089
[java] DEBUG [main] httpclient.wire - << ""Server: Apache/2.0.42 (Unix)"" [\r\n]:
21 Sep 2002 00:54:32,090
     [java] DEBUG [main] org.apache.commons.httpclient.HttpConnection - enter
HttpConnection.readLine(): 21 Sep 2002 00:54:32,090
[java] DEBUG [main] httpclient.wire - << ""Location:
http://jakarta.apache.org/commons/"" [\r\n]: 21 Sep 2002 00:54:32,091
     [java] DEBUG [main] org.apache.commons.httpclient.HttpConnection - enter
HttpConnection.readLine(): 21 Sep 2002 00:54:32,091
[java] DEBUG [main] httpclient.wire - << ""Content-Length: 319"" [\r\n]: 21 Sep
2002 00:54:32,091
     [java] DEBUG [main] org.apache.commons.httpclient.HttpConnection - enter
HttpConnection.readLine(): 21 Sep 2002 00:54:32,092
[java] DEBUG [main] httpclient.wire - << ""Content-Type: text/html;
charset=iso-8859-1"" [\r\n]: 21 Sep 2002 00:54:32,092
     [java] DEBUG [main] org.apache.commons.httpclient.HttpConnection - enter
HttpConnection.readLine(): 21 Sep 2002 00:54:32,092
[java] DEBUG [main] org.apache.commons.httpclient.HttpMethod - enter
HttpMethodBase.processResponseHeaders(HttpState, HttpConnection): 21 Sep 2002
00:54:32,093
     [java] DEBUG [main] org.apache.commons.httpclient.methods.GetMethod - enter
GetMethod.readResponseBody(HttpState, HttpConnection): 21 Sep 2002 00:54:32,093
     [java] DEBUG [main] org.apache.commons.httpclient.HttpMethod - enter
HttpMethodBase.readResponseBody(HttpState, HttpConnection): 21 Sep 2002 00:54:32,093
     [java] DEBUG [main] org.apache.commons.httpclient.HttpMethod - enter
HttpMethodBase.readResponseBody(HttpState, HttpConnection): 21 Sep 2002 00:54:32,094
     [java] DEBUG [main] org.apache.commons.httpclient.HttpConnection - enter
HttpConnection.getRequestOutputStream(HttpMethod): 21 Sep 2002 00:54:32,094
     [java] DEBUG [main] org.apache.commons.httpclient.HttpMethod - enter
writeRemainingRequestBody(HttpState, HttpConnection): 21 Sep 2002 00:54:32,096
     [java] DEBUG [main] org.apache.commons.httpclient.HttpMethod - Redirect
required: 21 Sep 2002 00:54:32,097
     [java] DEBUG [main] org.apache.commons.httpclient.HttpMethod - Redirect
requested to location 'http://jakarta.apache.org/commons/': 21 Sep 2002 00:54:32,097
     [java] WARN  [main] org.apache.commons.httpclient.HttpMethod - Redirect
from port 80 to -1 is not supported: 21 Sep 2002 00:54:32,098


The problem appears to be in this section of code from HttpMethodBase

if (url == null) {
    //try to construct the new url based on the current url
    try {
        URL currentUrl = new URL(conn.getProtocol(),
                                 conn.getHost(),
                                 conn.getPort(), getPath());
        url = new URL(currentUrl, location);   <--- is this inheriting the port?
    } catch (Exception ex) {
        log.error(""Redirected location '""
                  + locationHeader.getValue()
                  + ""' is malformed"");
        return statusCode;
    }
}"
0,"Typos in method names in test classes (SetPropertyAssumeTypeTest)Misspelled: ""ConstraintVioloationException"".
"
1,"Property.setValue(InputStream) closes streamCurrently the Property.setValue(InputStream) - actually all methods setting a property value from an InputStream - method closes the stream when it has completely been read. While this might be a nice-to-have in some situations, it is IMHO not standard behaviour for stream consumers to close the stream when done.

My special use case is unpacking the contents of a ZIP file (ZIPInputStream). After streaming the contents of the first ZIP file entry into a property, the ZIPInputStream is closed by Jackrabbit and the rest of the file cannot be read.

Workaround: Instead of giving the original InputStream to the method, create a FileInputStream wrapper overwriting the close method to do nothing."
0,"The values for the Via header are created by httpclient-cache for each cached and backend requestThe Via header that gets generated and inserted by the caching layer is done repeatedly in the HTTP conversation, even if the constructed string is constant for each protocol version that is involved.

The proposed patch constructs a map of generated values held in memory with the associated ProtocolVersion as a key and uses read/write locks to access the data. This  solution minimizes the time to generate such a value from several milliseconds to 40-50 microseconds."
0,"Make license checking/maintenance easier/automatedInstead of waiting until release to check licenses are valid, we should make it a part of our build process to ensure that all dependencies have proper licenses, etc."
0,"Document SINGLE_COOKIE_HEADER param in the cookie guideIncluded is some sample code that shows the behaviour when loading pages from a phpBB powered 
site. Here are the results as i see them on my machine:

==== start results

==================================
Policy: rfc2109
==================================


        URL: http://www.sgboards.com/forums/viewtopic.php?t=12&view=next&mforum=str
        Response status code: 200
        Present cookies: 
                ForumSetCookie=str
                phpbb_str_data=a%3A0%3A%7B%7D
                phpbb_str_sid=c8da590cc4b1683b9079da3d82f4efa6

        URL: http://www.sgboards.com/forums/viewtopic.php?p=24&mforum=str
        Response status code: 200
        Present cookies: 
                phpbb_str_data=a%3A0%3A%7B%7D
                phpbb_str_sid=c8da590cc4b1683b9079da3d82f4efa6
                ForumSetCookie=str

        URL: http://www.sgboards.com/forums/posting.php?mode=quote&p=24&mforum=str
        Response status code: 200
        Present cookies: 
                phpbb_str_data=a%3A0%3A%7B%7D
                phpbb_str_sid=c8da590cc4b1683b9079da3d82f4efa6
                ForumSetCookie=str

        URL: http://www.sgboards.com/forums/viewtopic.php?p=25&mforum=str
        Response status code: 200
        Present cookies: 
                phpbb_str_data=a%3A0%3A%7B%7D
                phpbb_str_sid=c8da590cc4b1683b9079da3d82f4efa6
                ForumSetCookie=str

==================================
Policy: netscape
==================================


        URL: http://www.sgboards.com/forums/viewtopic.php?t=12&view=next&mforum=str
        Response status code: 200
        Present cookies: 
                phpbb_str_sid=e2604334a0022283333153f6879feb70

        URL: http://www.sgboards.com/forums/viewtopic.php?p=24&mforum=str
        Response status code: 200
        Present cookies: 
                phpbb_str_sid=e2604334a0022283333153f6879feb70

        URL: http://www.sgboards.com/forums/posting.php?mode=quote&p=24&mforum=str
        Response status code: 200
        Present cookies: 
                phpbb_str_sid=e2604334a0022283333153f6879feb70

        URL: http://www.sgboards.com/forums/viewtopic.php?p=25&mforum=str
        Response status code: 200
        Present cookies: 
                phpbb_str_sid=e2604334a0022283333153f6879feb70

==================================
Policy: compatibility
==================================


        URL: http://www.sgboards.com/forums/viewtopic.php?t=12&view=next&mforum=str
        Response status code: 200
        Present cookies: 
                ForumSetCookie=str
                phpbb_str_data=a%3A0%3A%7B%7D
                phpbb_str_sid=d156f6dbfa605320b5a250129fa0b22e

        URL: http://www.sgboards.com/forums/viewtopic.php?p=24&mforum=str
        Response status code: 200
        Present cookies: 
                ForumSetCookie=str
                phpbb_str_data=a%3A0%3A%7B%7D
                phpbb_str_sid=d5d5a46fd27fd783cdb4e324992bc9d2

        URL: http://www.sgboards.com/forums/posting.php?mode=quote&p=24&mforum=str
        Response status code: 200
        Present cookies: 
                phpbb_str_data=a%3A0%3A%7B%7D
                phpbb_str_sid=b4312fee4250f767cd1b34b11afadb3d
                ForumSetCookie=str

        URL: http://www.sgboards.com/forums/viewtopic.php?p=25&mforum=str
        Response status code: 200
        Present cookies: 
                ForumSetCookie=str
                phpbb_str_data=a%3A0%3A%7B%7D
                phpbb_str_sid=daf72685d35d851c3eec68b6b3bc3705

==== end results

As you can see the only cookie policy that ISN'T successfully tracking sessions is the COMPATIBILITY 
setting. There are a lot of these phpBB sites around, so that's where I've noticed the behaviour most. 
Trying another random php powered site I see that all policies work as expected.

It would be nice to know what's messing up the cookie handing on these phpBB sites. If you can't rely 
on the compatibility setting to reliably maintain session variables (and hence truly imitate a browser) 
then life get's a little complicated.

Both 3.0beta1 and the CVS version show the same behaviour.

Many thanks,

Garry

Example code below.

====== begin code
import org.apache.commons.httpclient.Cookie;
import org.apache.commons.httpclient.HttpClient;
import org.apache.commons.httpclient.HttpState;
import org.apache.commons.httpclient.cookie.CookiePolicy;
import org.apache.commons.httpclient.methods.GetMethod;


public class CookieProbe {
	static final String[] urls = {
		""http://www.sgboards.com/forums/viewtopic.php?t=12&view=next&mforum=str"",
		""http://www.sgboards.com/forums/viewtopic.php?p=24&mforum=str"",
		""http://www.sgboards.com/forums/posting.php?mode=quote&p=24&mforum=str"",
		""http://www.sgboards.com/forums/viewtopic.php?p=25&mforum=str""
	};
	static final String[] urls2 = {
		""http://www.virginmobilelouder.com/live/index.php"",
		""http://www.virginmobilelouder.com/live/index.php?page_id=214"",
		""http://www.virginmobilelouder.com/live/index.php?page_id=3"",
		""http://www.virginmobilelouder.com/live/index.php?page_id=116""
	};
	
	static final String[] policies = {
		CookiePolicy.RFC_2109, 
		CookiePolicy.NETSCAPE, 
		CookiePolicy.BROWSER_COMPATIBILITY, 
	};
	
	
	public static void main(String[] args) {
		try {
			for (int i = 0; i < policies.length; i++) {
				System.out.println(""\n=================================="");
				System.out.println(""Policy: "" + policies[i]);
				System.out.println(""==================================\n"");
				tryPolicy(policies[i]);
			}
		} catch (Exception e) {
			e.printStackTrace(System.err);
		}
	}
	
	public static void tryPolicy(String policy) throws Exception {
		HttpState initialState = new HttpState();
		HttpClient httpclient = new HttpClient();
		httpclient.getHttpConnectionManager().
			getParams().setConnectionTimeout(30000);
		httpclient.setState(initialState);
		
		httpclient.getParams().setCookiePolicy(policy);
		for (int i = 0; i < urls.length; i++) {
			System.out.println(""\n\tURL: "" + urls[i]);
			tryURL(httpclient, urls[i]);
			Thread.sleep(1000); // give server a break
		}
			
	}

	public static void tryURL(HttpClient httpclient, String strURL) throws Exception {
		GetMethod httpget = new GetMethod(strURL);
		int result = httpclient.executeMethod(httpget);
		System.out.println(""\tResponse status code: "" + result);
		// Get all the cookies
		Cookie[] cookies = httpclient.getState().getCookies();
		System.out.println(""\tPresent cookies: "");
		for (int i = 0; i < cookies.length; i++) {
			System.out.println(""\t\t"" + cookies[i].toExternalForm());
		}
		// Release current connection to the connection pool once you are done
		httpget.releaseConnection();
	}
}
====== end code"
0,"add Token.setTermText(), remove finalThe Token class should be more friendly to classes not in it's package:
  1) add setTermText()
  2) remove final from class and toString()
  3) add clone()

Support for (1):
  TokenFilters in the same package as Token are able to do things like 
   ""t.termText = t.termText.toLowerCase();"" which is more efficient, but more importantly less error prone.  Without the ability to change *only* the term text, a new Token must be created, and one must remember to set all the properties correctly.  This exact issue caused this bug:
http://issues.apache.org/jira/browse/LUCENE-437

Support for (2):
  Removing final allows one to subclass Token.  I didn't see any performance impact after removing final.
I can go into more detail on why I want to subclass Token if anyone is interested.

Support for (3):
  - support for a synonym TokenFilter, where one needs to make two tokens from one (same args that support (1), and esp important if instance is a subclass of Token)."
0,"Add org.apache.lucene.store.FSDirectory.getDirectory()On the Apache Lucene.Net side, we have done some clean up with the upcoming 2.9.1 such that we are now depreciating improperly use of parameter type for some public APIs.  When we release 3.0, those depreciated code will be removed.

One area where we had difficulty with required us to add a new method like so: Lucene.Net.Store.FSDirectory.GetDirectory().  This method does the same thing as Lucene.Net.Store.FSDirectory.GetFile().  This was necessary because we switched over from using System.IO.FileInfo to System.IO.DirectoryInfo.  Why?  In the .NET world, a file and a directory are two different things.

Why did we have to add Lucene.Net.Store.FSDirectory.GetDirectory()?  Because we can't change the return type of Lucene.Net.Store.FSDirectory.GetFile() and still remain backward compatible (API wise) to be depreciated with the next release.

Why ask for Java Lucene to add org.apache.lucene.store.FSDirectory.getDirectory()?  To keep the APIs 1-to-1 in par with Java Lucene and Lucene.Net."
0,"Let NameException extend RepositoryExceptionSince the NameExceptions (IllegalNameException, UnknownPrefixException, etc.) are typically thrown when parsing or formatting JCR names at the JCR API level, it would make sense for the NameException class to extend RepositoryException instead of the internal BaseException. This idea is supported by the fact that the majority of cases where NameExceptions are encountered simply rethrow the exceptions wrapped inside RepositoryException instances. Making NameException extend RepositoryException would reduce the amount of try-catch blocks and wrapped exceptions."
0,Remove deprecated TokenStream APII looked into clover analysis: It seems to be no longer used since I removed the tests yesterday - I am happy!
0,"Add a field-filtering FilterAtomicReader to 4.0 so ParallelReaders can be better tested (in LTC.maybeWrapReader)In addition to the filters in contrib/misc for horizontally filtering (by doc-id) AtomicReader, it would be good to have the same vertically (by field). For now I will add this implementation to test-framework, as it cannot stay in contrib/misc, because LTC will need it for maybeWrapReader.

LTC will use this FilterAtomicReader to construct a ParallelAtomicReader out of two (or maybe more) FieldFilterAtomicReaders."
1,Add delete term and query need to more precisely record the bytes usedDocumentsWriter's add delete query and add delete term add to the number of bytes used regardless of the query or term already existing in the respective map.
0,"random sampler is not random (and so facet SamplingWrapperTest occasionally fails)RandomSample is not random at all:
It does not even import java.util.Random, and its behavior is deterministic.

in addition, the test testCountUsingSamping() never retries as it was supposed to (for taking care of the hoped-for randomness)."
0,"Upgrade to Xerces 2.8.1Besides a number of bug fixes and new features in Xerces, upgrading to a newer version would also give us better dependency metadata, and thus avoid the need of having a separate xmlParserAPIs dependency in addition to the main xercesImpl dependency. "
0,"J2EE FORM authentication (also affects pluggable authentication)Add support for J2EE style FORM authentication type. 

Unlike the BASIC and DIGEST types this is not handled by HTTP headers so needs
an adjustment to the way in which the authentication is sent. As far as i can
tell from my testing with one or two J2EE servers the way to successfully login
requires request of a protected page which will respond with the login FORM and
then the submission of that form. The two requests must be associated with one
another using the jsessionid cookie.

It seems to me that this 'bug' must be solved in cooperation with the recent
discussions of pluggable authentication module. i suggestion the following
signature: 

PluggableAuthenticator.authenticate(HttpMethod method, HttpState state). 

This mirrors the existing Authenticator method but also requires change to the
state object to allow access to the connection properties (i dont know how this
affects MultiClient). Alternately we could go for: 

PluggableAuthenticator.authenticate(HttpMethod method, HttpClient client).

In either case Authenticator needs a way to know which plugin to call. I suggest
modification of HttpMethodBase to detect the 'j_security_check' form action in
the response and automatically submit credentials if they are provided using the
new class 

J2EEFormAuthenticator implements PluggableAuthenticator."
0,"add back Document.getValues()I'm porting some code to trunk's new Doc/Field apis, and i keep running into this pattern:
{noformat}
String[] values = doc.getValues(""field"");
{noformat}

But with the new apis, this becomes a little too verbose:

{noformat}
IndexableField[] fields = doc.getFields(""field"");
String[] values = new String[fields.length];
for (int i = 0; i < values.length; i++) {
  values[i] = fields[i].stringValue();
}
{noformat}

I think we should probably add back the sugar api (with the same name) ?
"
0,"repository.xml: throw an exception on errorCurrently, unsupported parameters in repository.xml and workspace.xml are ignored.
To find problems earlier, such problems should result in an exception,
and starting such a repository should not be possible.
The same should happen for unsupported values.

For currently unavailable options
(such as text extraction filter classes if the class is not in the classpath),
at least a warning should be written to the error log, or an error should be thrown.
"
0,"Remove code duplication in MultiReader/DirectoryReader, make everything inside finalAfter making IndexReader readOnly (LUCENE-3606) there is no need to have completely different DirectoryReader and MultiReader, the current code is heavy code duplication and violations against finalness patterns. There are only few differences in reopen and things like isCurrent/getDirectory/...

This issue will clean this up by introducing a hidden package-private base class for both and only handling reopen and incRef/decRef different. DirectoryReader is now final and all fields in BaseMultiReader, MultiReader and DirectoryReader are final now. DirectoryReader has now only static factories, no public ctor anymore."
0,"If setConfig(Config config) is called in resetInputs(), you can turn term vectors off and on by roundI want to be able to run one benchmark that tests things using term vectors and not using term vectors.

Currently this is not easy because you cannot specify term vectors per round.

While you do have to create a new index per round, this automation is preferable to me in comparison to running two separate tests.

If it doesn't affect anything else, it would be great to have setConfig(Config config) called in BasicDocMaker.resetInputs(). This would keep the term vector options up to date per round if you reset.

- Mark"
0,"QueryParser can produce empty sub BooleanQueries when Analyzer proudces no tokens for inputas triggered by SOLR-261, if you have a query like this...

   +foo:BBB  +(yak:AAA  baz:CCC)

...where the analyzer produces no tokens for the ""yak:AAA"" or ""baz:CCC"" portions of the query (posisbly because they are stop words) the resulting query produced by the QueryParser will be...

  +foo:BBB +()

...that is a BooleanQuery with two required clauses, one of which is an empty BooleanQuery with no clauses.

this does not appear to be ""good"" behavior.

In general, QueryParser should be smarter about what it does when parsing encountering parens whose contents result in an empty BooleanQuery -- but what exactly it should do in the following situations...

 a)  +foo:BBB +()
 b)  +foo:BBB ()
 c)  +foo:BBB -()

...is up for interpretation.  I would think situation (b) clearly lends itself to dropping the sub-BooleanQuery completely.  situation (c) may also lend itself to that solution, since semanticly it means ""don't allow a match on any queries in the empty set of queries"".  .... I have no idea what the ""right"" thing to do for situation (a) is."
1,"NullPointerException on startup if IndexingQueue has pending nodesThis happens because of the newly introduced index version, which is not yet set when the IndexingQueue is instanciated."
1,Connections are not release when a recoverable exception occurs.Please see the url for discussion details.
0,"Can not subscribeHello, 
I have sent email to lucene-dev-subscribe@jakarta.apache.org and it always 
returns failed: 
 
<lucene-dev-subscribe@jakarta.apache.org>: 
Sorry, no mailbox here by that name. (#5.1.1) 
 
Please help me subscribe."
0,"Rename OriginalQueryParserHelperWe should rename the new QueryParser so it's clearer that it's
Lucene's default QueryParser, going forward, and not just a temporary
""bridge"" to a future new QueryParser.

How about we rename oal.queryParser.original -->
oal.queryParser.standard (can't use ""default"": it's a Java keyword)?
Then, leave the OriginalQueryParserHelper under that package, but
simply rename it to QueryParser?

This way if we create other sub-packages in the future, eg
ComplexPhraseQueryParser, they too can have a QueryParser class under
them, to make it clear that's the ""top"" class you use to parse
queries.
"
0,"TCK: AbstractJCRTest fails if setUp/tearDown cannot remove children of test nodeIf the test node exists, the setUp and tearDown methods remove all its child nodes.  In some repositories these child nodes may be mandatory or protected, causing test setup/teardown to fail.

Proposal: tolerate exceptions thrown in removing a child node in test setup/teardown.

--- ../AbstractJCRTest.java     (revision 422074)
+++ ../AbstractJCRTest.java     (working copy)
@@ -344,7 +344,11 @@
                 // clean test root
                 testRootNode = root.getNode(testPath);
                 for (NodeIterator children = testRootNode.getNodes(); children.hasNext();) {
-                    children.nextNode().remove();
+                    try {
+                      children.nextNode().remove();
+                    } catch (RepositoryException e) {
+                      // consume
+                    }
                 }
             } else {
                 // create nodes to testPath
@@ -375,7 +379,11 @@
                         // clean test root
                         testRootNode = root.getNode(testPath);
                         for (NodeIterator children = testRootNode.getNodes(); children.hasNext();) {
-                            children.nextNode().remove();
+                            try {
+                              children.nextNode().remove();
+                            } catch (RepositoryException e) {
+                              // consume
+                            }
                         }
                         root.save();
                     }
"
1,"Text.isDescendant returns false if parent is '/'the method isDescendant(String, String) of the Text utility class returns false if the 
passed potential parent-path represents the root node (""/"").

"
1,"PlainTextExtractor returns an empty reader when encoding is unsupportedPlainTextExtractor is failing to index text files.  Searching for content in text files is not coming back with results.

On the extractText(InputStream stream, String type, String encoding) method, the encoding is coming in as an empty string, and it throws the java.io.UnsupportedEncodingException at line 40 ( return new InputStreamReader(stream, encoding); ).

modifying the following statement fixes the problem:
before:  if (encoding != null) {
after:  if (encoding != null && !encoding.equals("""")) {"
1,"Can no longer set a Date property using a Long valueAttempting to set a Date property with a Long value throws a javax.jcr.nodetype.ConstraintViolationException. This worked in Jackrabbit 1.6.2.

To reproduce:
  Node node = session.getItem(""/"");
  node = node.addNode(""dummy"", ""nt:resource"");
  ValueFactory vf = session.getValueFactory();
  Value = vf.createValue(""1234"", 3); // Create a LongValue
  node.setProperty(""jcr:lastModified"", value);
  System.out.println(node.getProperty(""jcr:lastModified""));

Expected result:
- A date around 1970 is printed to System.out

Actual result:
  javax.jcr.nodetype.ConstraintViolationException: no matching property definition found for {http://www.jcp.org/jcr/1.0}lastModified
       at org.apache.jackrabbit.core.nodetype.EffectiveNodeType.getApplicablePropertyDef(EffectiveNodeType.java:770)
       at org.apache.jackrabbit.core.NodeImpl.getApplicablePropertyDefinition(NodeImpl.java:911)
       at org.apache.jackrabbit.core.ItemManager.getDefinition(ItemManager.java:224)
       at org.apache.jackrabbit.core.ItemData.getDefinition(ItemData.java:97)
       at org.apache.jackrabbit.core.PropertyData.getPropertyDefinition(PropertyData.java:53)
       at org.apache.jackrabbit.core.PropertyImpl.getDefinition(PropertyImpl.java:729)
       at org.apache.jackrabbit.core.NodeImpl.setProperty(NodeImpl.java:2512)

According to Jukka Zitting [1], this might be a side-effect of JCR-2170.

[1] Mail thread from dev@jackrabbit.apache.org: http://markmail.org/message/hn3snufsogjvldad"
1,"HttpClient incorrectly handles Transfer-Encoding headerRFC2616, section 4.4 item 3 states:
     If a Content-Length header field (section 14.13) is present, its
     decimal value in OCTETs represents both the entity-length and the
     transfer-length. The Content-Length header field MUST NOT be sent
     if these two lengths are different (i.e., if a Transfer-Encoding
     header field is present). If a message is received with both a
     Transfer-Encoding header field and a Content-Length header field,
     the latter MUST be ignored.

This is not handled correctly in the case that a noncompliant HTTP server
returns both a Transfer-Encoding header and a Content-Length header.

I gave up on writing a TestCase for this as it would require a reliably
noncompliant HTTP Server."
0,"Back-compat break with non-ascii field namesIf a field name contains non-ascii characters in a 2.3.x index, then
on upgrade to 2.4.x unexpected problems are hit.  It's possible to hit
a ""read past EOF"" IOException; it's also possible to not hit an
exception but get an incorrect field name.

This was caused by LUCENE-510, because the FieldInfos (*.fnm) file is
not properly versioned.

Spinoff from http://www.nabble.com/Read-past-EOF-td23276171.html
"
1,"Problem with child order after restoring of parentThe following sequence leads to swapped child nodes in the mentioned trunk version (this worked in 1.0.1).
Specifically:

Add nodes:
 parent
   childA
   childB

Remove childA:
 parent
   childB

Restore initial version:
 parent
   childB
   childA

The parent node is of type nt:unstructured which carries an
OrderableChildNodes=true so I would assume that upon restoring the order
would be preserved.

Cheers,
Tanju

--------------

TESTCASE used with both Derby & InMemPMs (boiled down beyond sense :)

// Add parent & childA, childB
Node parent = session.getRootNode().addNode(""parent"", ""nt:unstructured"");
parent.addMixin(""mix:versionable"");
Node c1 = parent.addNode(""childA"", ""nt:unstructured"");
c1.addMixin(""mix:versionable"");
Node c2 = parent.addNode(""childB"", ""nt:unstructured"");
c2.addMixin(""mix:versionable"");
session.save();
c1.checkin();
c2.checkin();
Version v1 = parent.checkin();
// OK : parent.getNodes() -> childA, childB

// Remove childA
parent = session.getRootNode().getNode(""parent"");
parent.checkout();
c1 = parent.getNodes().nextNode();
c1.checkout();
c1.remove();
session.save();
Version v2 = parent.checkin();
// OK : parent.getNodes() -> childA, childB

// Remove childA
parent = session.getRootNode().getNode(""parent"");
parent.restore(v1, true);
// Not OK : parent.getNodes() -> childB, childA"
0,"preflex codec doesn't order terms correctlyThe surrogate dance in the preflex codec (which must dynamically remap terms from UTF16 order to unicode code point order) is buggy.

To better test it, I want to add a test-only codec, preflexrw, that is able to write indices in the pre-flex format.  Then we should also fix tests to randomly pick codecs (including preflexrw) so we better test all of our codecs."
0,fix generics violations in contrib/modulesThere are some generics violations... we should fix them.
0,"JSR 283: new Property Typesthe new property types are

- WEAKREFERENCE
- URI
- DECIMAL"
1,"Repository lock keeps file openThe RepositoryLock opens a RandomAccessFile, but does not close it. The problematic line is:

lock = new RandomAccessFile(file, ""rw"").getChannel().tryLock();

This is usually not a problem as the file will be closed when the RandomAccessFile object is garbage collected. However, if called a lot in a short time frame, this results in 'too many open files' in some environments (for example Linux). "
0,"Fix junit scope in maven pomPlease change the junit dependency to

    <dependency>
      <groupId>junit</groupId>
      <artifactId>junit</artifactId>
      <version>3.8.1</version>
      <url>http://www.junit.org/</url>
      <properties>
        <scope>test</scope>
      </properties>
    </dependency>

for better automatic conversion to maven 2"
0,"Move the XASession interface to jackrabbit-apiAs discussed in JCR-953, it would probably make sense to move the org.apache.jackrabbit.core.XASession from jackrabbit-core to jackrabbit-api as org.apache.jackrabbit.api.XASession."
0,"Use ConcurrentHashMap in RAMDirectoryRAMDirectory synchronizes on its instance in many places to protect access to map of RAMFiles, in addition to updating the sizeInBytes member. In many places the sync is done for 'read' purposes, while only in few places we need 'write' access. This looks like a perfect use case for ConcurrentHashMap

Also, syncing around sizeInBytes is unnecessary IMO, since it's an AtomicLong ...

I'll post a patch shortly."
1,"Freezes w/ MultiThreadedHttpConnectionManagerMy single threaded user of VFS (an HttpClient user, that uses
MultiThreadedHttpConnectionManager) hangs [I suspect indefinitely] on minor
activity.

I've turned on HttpClient debug and I see this, the last line
being the last thing I get...

2003/10/09 09:34:26:482 MDT [DEBUG] wire - -<< ""Content-Type:
text/html[\r][\n]""
2003/10/09 09:34:26:482 MDT [DEBUG] HttpMethodBase - -Resorting to protocol
version default close co
nnection policy
2003/10/09 09:34:26:492 MDT [DEBUG] HttpMethodBase - -Should NOT close
connection, using HTTP/1.1
2003/10/09 09:34:26:502 MDT [DEBUG] HttpMethodDirector - -Execute loop try 1
2003/10/09 09:34:26:512 MDT [DEBUG]
MultiThreadedHttpConnectionManager - -HttpConnectionManager.getC
onnection:  config = HostConfiguration[host=www.ibiblio.org,
protocol=http:80, port=80], timeout = 0

2003/10/09 09:34:26:522 MDT [DEBUG]
MultiThreadedHttpConnectionManager - -Unable to get a connection
, waiting..., hostConfig=HostConfiguration[host=www.ibiblio.org,
protocol=http:80, port=80]

This is pretty reproducible. When I hack VFS not to use the
MultiThreadedHttpConnectionManager I don't get the problem."
0,AbstractQueryTest does not handle unknown result size properlyIf Node/RowIterator.getSize() returns -1 the method AbstractQueryTest.checkResult() should count the nodes in the iterator manually.
1,"MockRandomMergePolicy optimizes segments not in the Set passed inThe test class MockRandomMergePolicy shuffles the whole SegmentInfos passed to the optimize callback and returns random segments for optimizing. This is fine, but it also returns segments, that are not listed in the Set<SegmentInfo> that is also passed in, containing the subset of segments to optimize.

This bug was found when writing a testcase for LUCENE-3082: The wrapper MergePolicy (when wrapped around MockRandomMergePolicy) only passes a subset of the segments to the delegate (the ones that are in old index format). But MockRandom created OneMerge in its return MergeSpecification having segments outside this set."
0,"UserManagement: Missing assertion that Principal name isn't """"Creating users/groups with a Principal having an empty name should not be allowed."
0,"[patch] add toString for NodeImpl and PropertyImpladd toString for NodeImpl and PropertyImpl with new format. see how it is liked, before adding more."
1,"Incorrect parsing by QueryParser.parse() when it encounters backslashes (always eats one backslash.)Test code and output follow. Tested  Lucene 1.9 version only. Affects hose who would index/search for Lucene's reserved characters.

Description: When an input search string has a sequence of N (java-escaped) backslashes, where N >= 2, the QueryParser will produce a query in which that sequence has N-1 backslashes.

TEST CODE:
    Analyzer analyzer = new WhitespaceAnalyzer();
    String[] queryStrs = {""item:\\\\"",
                          ""item:\\\\*"",
                          ""(item:\\\\ item:ABCD\\\\))"",
                          ""(item:\\\\ item:ABCD\\\\)""};
    for (String queryStr : queryStrs) {
      System.out.println(""--------------------------------------"");
      System.out.println(""String queryStr = "" + queryStr);
      Query luceneQuery = null;
      try {
        luceneQuery = new QueryParser(""_default_"", analyzer).parse(queryStr);
        System.out.println(""luceneQuery.toString() = "" + luceneQuery.toString());
      } catch (Exception e) {
        System.out.println(e.getClass().toString());
      }
    }

OUTPUT (with remarks in comment notation:) 
--------------------------------------
String queryStr = item:\\
luceneQuery.toString() = item:\             //One backslash has disappeared. Searcher will fail on this query.
--------------------------------------
String queryStr = item:\\*
luceneQuery.toString() = item:\*           //One backslash has disappeared. This query will search for something unintended.
--------------------------------------
String queryStr = (item:\\ item:ABCD\\))
luceneQuery.toString() = item:\ item:ABCD\)     //This should have thrown a ParseException because of an unescaped ')'. It did not.
--------------------------------------
String queryStr = (item:\\ item:ABCD\\)
class org.apache.lucene.queryParser.ParseException        //...and this one should not have, but it did.

"
1,"Nullpointer when creating URI from scheme specific part with null fragmentin org.apache.commons.httpclient.URI class constructor:

public URI(String scheme, String schemeSpecificPart, String fragment)
        throws URIException {
....
_fragment = fragment.toCharArray(); 

should be 

_fragment = fragment==null ? null : fragment.toCharArray();"
1,LevenshteinDistance code normalization is incorrectThe normalization of the edit distance should use the maximum of the two string being compared instead of the minimum.  Otherwise negative distances are possible.  The spell checker filters out edits below a certain threshold so this hasn't been a problem in practice.
1,"MockRAMDirectory (used only by unit tests) has some synchronization problemsComing out of a failure that Earwin noted on java-dev this morning, I reworked the synchronization on MockRAMDirectory."
0,"download section of website downmaybe you know this, but http://hc.apache.org/downloads.cgi (the downloads for httpcomponents and for httpclient) is broken (500 internal server error)..."
0,"Cleanup 'good' queries codeBefore moving some of the classes from the queries contrib to the queries module, I want to just pass over them and clean them up, since we want code in modules to be of the same calibre as core code."
0,"New flexible query parserFrom ""New flexible query parser"" thread by Micheal Busch

in my team at IBM we have used a different query parser than Lucene's in
our products for quite a while. Recently we spent a significant amount
of time in refactoring the code and designing a very generic
architecture, so that this query parser can be easily used for different
products with varying query syntaxes.

This work was originally driven by Andreas Neumann (who, however, left
our team); most of the code was written by Luis Alves, who has been a
bit active in Lucene in the past, and Adriano Campos, who joined our
team at IBM half a year ago. Adriano is Apache committer and PMC member
on the Tuscany project and getting familiar with Lucene now too.

We think this code is much more flexible and extensible than the current
Lucene query parser, and would therefore like to contribute it to
Lucene. I'd like to give a very brief architecture overview here,
Adriano and Luis can then answer more detailed questions as they're much
more familiar with the code than I am.
The goal was it to separate syntax and semantics of a query. E.g. 'a AND
b', '+a +b', 'AND(a,b)' could be different syntaxes for the same query.
We distinguish the semantics of the different query components, e.g.
whether and how to tokenize/lemmatize/normalize the different terms or
which Query objects to create for the terms. We wanted to be able to
write a parser with a new syntax, while reusing the underlying
semantics, as quickly as possible.
In fact, Adriano is currently working on a 100% Lucene-syntax compatible
implementation to make it easy for people who are using Lucene's query
parser to switch.

The query parser has three layers and its core is what we call the
QueryNodeTree. It is a tree that initially represents the syntax of the
original query, e.g. for 'a AND b':
  AND
 /   \
A     B

The three layers are:
1. QueryParser
2. QueryNodeProcessor
3. QueryBuilder

1. The upper layer is the parsing layer which simply transforms the
query text string into a QueryNodeTree. Currently our implementations of
this layer use javacc.
2. The query node processors do most of the work. It is in fact a
configurable chain of processors. Each processors can walk the tree and
modify nodes or even the tree's structure. That makes it possible to
e.g. do query optimization before the query is executed or to tokenize
terms.
3. The third layer is also a configurable chain of builders, which
transform the QueryNodeTree into Lucene Query objects.

Furthermore the query parser uses flexible configuration objects, which
are based on AttributeSource/Attribute. It also uses message classes that
allow to attach resource bundles. This makes it possible to translate
messages, which is an important feature of a query parser.

This design allows us to develop different query syntaxes very quickly.
Adriano wrote the Lucene-compatible syntax in a matter of hours, and the
underlying processors and builders in a few days. We now have a 100%
compatible Lucene query parser, which means the syntax is identical and
all query parser test cases pass on the new one too using a wrapper.


Recent posts show that there is demand for query syntax improvements,
e.g improved range query syntax or operator precedence. There are
already different QP implementations in Lucene+contrib, however I think
we did not keep them all up to date and in sync. This is not too
surprising, because usually when fixes and changes are made to the main
query parser, people don't make the corresponding changes in the contrib
parsers. (I'm guilty here too)
With this new architecture it will be much easier to maintain different
query syntaxes, as the actual code for the first layer is not very much.
All syntaxes would benefit from patches and improvements we make to the
underlying layers, which will make supporting different syntaxes much
more manageable.
"
0,"JSR 283: Access Property/Node from SessionNew methods to access properties and nodes from the Session:

- getNode(String absPath) Node
- getNodeByIdentifier(String id) Node
- getProperty(String absPath) Property

... test for their existence:

- nodeExists(String absPath) boolean
- propertyExists(String absPath) boolean

... and remove them:

- removeItem(String absPath) void

The functionality has been added at rev. 571494 and rev. 712984 but apart from Session.removeItem no
test cases are present so far."
0,"optimize contrib/regex for flex* changes RegexCapabilities match(String) to match(BytesRef)
* the jakarta and jdk impls uses CharacterIterator/CharSequence matching against the utf16result instead.
* i also reuse the matcher for jdk, i don't see why we didnt do this before but it makes sense esp since we reuse the CSQ
"
1,"Query throws UnsupportedOperationExceptionWhen executing an absolute XPath statement where the first location step is not jcr:root the Query may throw an UnsupportedOperationException:

Query: /foo//element(*, nt:unstructured)[@prop = 'bar']

Stacktrace:
java.lang.UnsupportedOperationException
        at org.apache.jackrabbit.core.query.lucene.CachingMultiReader$MultiTermDocs.skipTo(CachingMultiReader.java:281)
        at org.apache.lucene.search.TermScorer.skipTo(TermScorer.java:88)
        at org.apache.lucene.search.ConjunctionScorer.doNext(ConjunctionScorer.java:53)
        at org.apache.lucene.search.ConjunctionScorer.next(ConjunctionScorer.java:48)
        at org.apache.lucene.search.Scorer.score(Scorer.java:37)
        at org.apache.jackrabbit.core.query.lucene.ChildAxisQuery$ChildAxisScorer.calculateChildren(ChildAxisQuery.java:291)
        at org.apache.jackrabbit.core.query.lucene.ChildAxisQuery$ChildAxisScorer.next(ChildAxisQuery.java:251)
        at org.apache.lucene.search.Scorer.score(Scorer.java:37)
        at org.apache.jackrabbit.core.query.lucene.DescendantSelfAxisQuery$DescendantSelfAxisScorer.calculateSubHits(DescendantSelfAxisQuery.java:302)
        at org.apache.jackrabbit.core.query.lucene.DescendantSelfAxisQuery$DescendantSelfAxisScorer.next(DescendantSelfAxisQuery.java:237)
        at org.apache.lucene.search.Scorer.score(Scorer.java:37)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:92)
        at org.apache.lucene.search.Hits.getMoreDocs(Hits.java:64)
        at org.apache.lucene.search.Hits.<init>(Hits.java:43)
        at org.apache.lucene.search.Searcher.search(Searcher.java:33)
        at org.apache.lucene.search.Searcher.search(Searcher.java:27)
        at org.apache.jackrabbit.core.query.lucene.SearchIndex.executeQuery(SearchIndex.java:287)
        at org.apache.jackrabbit.core.query.lucene.QueryImpl.execute(QueryImpl.java:179)
        at org.apache.jackrabbit.core.query.QueryImpl.execute(QueryImpl.java:132)"
0,"consistency check should get node ids in chunks, not rely on total countThe PM consistency checker should use the paging feature to fetch nodeIds in chunks, and also not rely on the total number of ids for logging purposes."
0,"don't silently merge session-local transient changes with external changes before save().currently, external changes (i.e. changes committed by other sessions) are silently merged with transient changes. this might potentially cause concurrency issues/inconsistent transient state (see e.g. JCR-2632).

it would probably be better to isolate transient changes from external changes until they're saved (true copy-on-write). "
0,"Connection timeout logic redesignChangelog:

* CreateSocket method with timeout parameter added to the ProtocolSocketFactory
interface

* TimeoutController related code factored out of HttpConnection class and moved
into ControllerThreadSocketFactory helper class

* ReflectionSocketFactory helper class added. This factory encapsulates
reflection code to call JDK 1.4 Socket#connect method if supported

* All protocol socket factories now attempt to initially use
ReflectionSocketFactory if required to create a socket within a given limit of
time. If reflection fails protocol socket factories fall back onto the good ol'
ControllerThreadSocketFactory

Benefits:

* HttpConnection code got a lot cleaner
* When running in modern JREs expensive timeout controller thread per connection
attempt is no longer needed
* Ugly code intended to work around limitations of the older JREs is now
confined to a few helper classes that can be easily thrown away once we move
onto Java 1.4

Let me know what you think

Oleg"
0,Support system property to define the DefaultTransactionTimeout for a XASessionIt should be possible to define the DefaultTransactionTimeout for a XASession by a SystemProperty
1,"Escaped quotes inside a phrase cause a ParseExceptionQueryParser cannot handle escaped quotes when inside a phrase. Escaped quotes not in a phrase are not a problem. This can be added to TestQueryParser.testEscaped() to demonstrate the issue - the second assert throws an exception:

assertQueryEquals(""a \\\""b c\\\"" d"", a, ""a \""b c\"" d"");
assertQueryEquals(""\""a \\\""b c\\\"" d\"""", a, ""\""a \""b c\"" d\"""");

See also this thread:
http://www.nabble.com/ParseException-with-escaped-quotes-in-a-phrase-t1647115.html
"
0,"Fully decouple IndexWriter from analyzersIndexWriter only needs an AttributeSource to do indexing.

Yet, today, it interacts with Field instances, holds a private
analyzers, invokes analyzer.reusableTokenStream, has to deal with a
wide variety (it's not analyzed; it is analyzed but it's a Reader,
String; it's pre-analyzed).

I'd like to have IW only interact with attr sources that already
arrived with the fields.  This would be a powerful decoupling -- it
means others are free to make their own attr sources.

They need not even use any of Lucene's analysis impls; eg they can
integrate to other things like [OpenPipeline|http://www.openpipeline.org].
Or make something completely custom.

LUCENE-2302 is already a big step towards this: it makes IW agnostic
about which attr is ""the term"", and only requires that it provide a
BytesRef (for flex).

Then I think LUCENE-2308 would get us most of the remaining way -- ie, if the
FieldType knows the analyzer to use, then we could simply create a
getAttrSource() method (say) on it and move all the logic IW has today
onto there.  (We'd still need existing IW code for back-compat).
"
1,"Change Primitive Data Types from int to long in class SegmentMerger.javaHi

We are getting an exception while optimize. We are getting this exception ""mergeFields produced an invalid result: docCount is 385282378 but fdx file size is 3082259028; now aborting this merge to prevent index corruption""
 
I have  checked the code for class SegmentMerger.java and found this check 

***********************************************************************************************************************************************************************
if (4+docCount*8 != fdxFileLength)
        // This is most likely a bug in Sun JRE 1.6.0_04/_05;
        // we detect that the bug has struck, here, and
        // throw an exception to prevent the corruption from
        // entering the index.  See LUCENE-1282 for
        // details.
        throw new RuntimeException(""mergeFields produced an invalid result: docCount is "" + docCount + "" but fdx file size is "" + fdxFileLength + ""; now aborting this merge to prevent index corruption"");
}
***********************************************************************************************************************************************************************

In our case docCount is 385282378 and fdxFileLength size is 3082259028, even though 4+385282378*8 is equal to 3082259028, the above code will not work because number 3082259028 is out of int range. So type of variable docCount needs to be changed to long

I have written a small test for this 

************************************************************************************************************************************************************************

public class SegmentMergerTest {
public static void main(String[] args) {
int docCount = 385282378; 
long fdxFileLength = 3082259028L; 
if(4+docCount*8 != fdxFileLength) 
System.out.println(""No Match"" + (4+docCount*8));
else 
System.out.println(""Match"" + (4+docCount*8));
}
}

************************************************************************************************************************************************************************

Above test will print No Match but if you change the data type of docCount to long, it will print Match

Can you please advise us if this issue will be fixed in next release?

Regards
Deepak







 



"
0,"improve arabic analyzer: light8 -> light10Someone mentioned on the java user list that the arabic analysis was not as good as they would like.

This patch adds the - prefix (light10 algorithm versus light8 algorithm).
In the light10 paper, this improves precision from .390 to .413
They mention this is not statistically significant, but it makes linguistic sense and at least has been shown not to hurt.

In the future, I hope openrelevance will allow us to try some more approaches. 
"
0,"Add ability to run backwards-compatibility tests automaticallyThis is an idea Doug mentioned on LUCENE-1422.

This patch adds new targets to build.xml to automatically download the junit tests from a previous Lucene release and run them against the current core.
Execute tests like this:
ant -Dtag=lucene_2_4_0 test-tag

It will create a new directory tags/lucene_2_4_0 and fetch the tests from the svn repository and run them."
0,"HttpClient depends on jcip-annotations.jarWhen using Java 5 to compile code that uses HttpClient, jcip-annotations.jar must be in the classpath or else you get a compiler error:

    [javac] /path/to/src/SomeFile.java:129: cannot access net.jcip.annotations.GuardedBy
    [javac] file net/jcip/annotations/GuardedBy.class not found
    [javac]         DefaultHttpClient httpclient = new DefaultHttpClient();
    [javac]                                        ^


With Java 6, you get a bunch of warnings instead.
    [javac] org/apache/http/impl/client/AbstractHttpClient.class(org/apache/http/impl/client:AbstractHttpClient.class): warning: Cannot find annotation method 'value()' in type 'net.jcip.annotations.GuardedBy': class file for net.jcip.annotations.GuardedBy not found


This requirement doesn't seem to be documented anywhere, and jcip-annotations.jar is not included in the ""httpcomponents-client-4.0-bin-with-dependencies"" package.
"
0,"jcr-server-webapp: RMI Registration unstableRegistration of the repository to a RMI registry in RepositoryStartupServlet.registerRMI uses web application parameters inconsistently and may not always succeed registering the repository.

Today, the registerRMI uses these parameters for registration to RMI:

    rmi-host : The name of the host on which the registry is running
    rmi-port : The port on which the registry is running
    rmi-uri : An RMI URI to use for registration
    repository-name : The name to bind the repository to

The problem is, that rmi-port is used to try to create the registry to make sure a registry is running on the local host. The rmi-uri is used to register the repository using the static Naming.bind method. If the rmi-uri is not configured, the URI is created from rmi-host, rmi-port and repository-name.

This may now create a bunch of problems: If the rmi-port and rmi-uri configurations do not match, registration fails, if rmi-host does not resolve to an IP address to which the registry is bound, registration fails.

I encounter this issue, when trying to register the repository to an RMI registry using default rmi-port configuration (rmi-host and rmi-uri not configured) when running the web app in Jetty."
1,"SamplingWrapperTest failure with certain test seedBuild: https://builds.apache.org/job/Lucene-Solr-tests-only-trunk/12231/

1 tests failed.
REGRESSION:  org.apache.lucene.facet.search.SamplingWrapperTest.testCountUsingSamping

Error Message:
Results are not the same!

Stack Trace:
org.apache.lucene.facet.FacetTestBase$NotSameResultError: Results are not the same!
       at org.apache.lucene.facet.FacetTestBase.assertSameResults(FacetTestBase.java:333)
       at org.apache.lucene.facet.search.sampling.BaseSampleTestTopK.assertSampling(BaseSampleTestTopK.java:104)
       at org.apache.lucene.facet.search.sampling.BaseSampleTestTopK.testCountUsingSamping(BaseSampleTestTopK.java:82)
       at org.apache.lucene.util.LuceneTestCase$3$1.evaluate(LuceneTestCase.java:529)
       at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:165)
       at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)

NOTE: reproduce with: ant test -Dtestcase=SamplingWrapperTest -Dtestmethod=testCountUsingSamping -Dtests.seed=4a5994491f79fc80:-18509d134c89c159:-34f6ecbb32e930f7 -Dtests.multiplier=3 -Dargs=""-Dfile.encoding=UTF-8""
NOTE: test params are: codec=Lucene40: {$facets=PostingsFormat(name=MockRandom), $full_path$=PostingsFormat(name=MockSep), content=Pulsing40(freqCutoff=19 minBlockSize=65 maxBlockSize=209), $payloads$=PostingsFormat(name=Lucene40WithOrds)}, sim=RandomSimilarityProvider(queryNorm=true,coord=true): {$facets=LM Jelinek-Mercer(0.700000), content=DFR I(n)B3(800.0)}, locale=bg, timezone=Asia/Manila
"
1,"PathNotFoundException but item existsThe following test case (for jcr2spi) throws a PathNotFoundException for an item which exists. It does not throw if the marked line below is commented out. 

public void testBug24687() throws RepositoryException {
    String parentPath = testNode.getPath();
    String folderName = ""folder_"" + System.currentTimeMillis();
    Session session = getHelper().getReadWriteSession();

    Session session2 = getHelper().getReadOnlySession();
    session2.getItem(parentPath);  // removing this line makes the failure go away

    Node parent = (Node) session.getItem(parentPath);
    Node toDelete = parent.addNode(folderName, ""nt:folder"");
    parent.save();

    try {
        Item item2 = session2.getItem(parentPath + ""/"" + folderName);  // wrongly throws PathNotFoundException
        assertEquals(parentPath + ""/"" + folderName, item2.getPath());
    }
    finally {
        toDelete.remove();
        parent.save();
        assertFalse(parent.hasNode(folderName));
    }
}
"
0,"DisjunctionMaxQuery - Type safety  DisjunctionMaxQuery code has containers that are not type-safe . The comments indicate type-safety though. 

Better to express in the API and the internals the explicit type as opposed to type-less containers. 

Patch attached. 

Comments / backward compatibility concerns welcome.  "
1,"builtin_nodetypes.cnd contains non-ascii chars and is not correctly decoded using webspherejackrabbit can't be started within websphere with the following error:

Caused by: 
org.apache.jackrabbit.spi.commons.nodetype.compact.ParseException: IOException
while attempting to read input stream
(org/apache/jackrabbit/core/nodetype/builtin_nodetypes.cnd, line 272)
.
.
Caused by: 
sun.io.MalformedInputException
	at sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:262)
	at sun.nio.cs.StreamDecoder$ConverterSD.convertInto(StreamDecoder.java:314)
	at sun.nio.cs.StreamDecoder$ConverterSD.implRead(StreamDecoder.java:345)


I quickly checked the file and it contains some non ascii-7 characters. actually it should be stored, packaged and read as utf8. somthing was list in translation.
the simplest fix would be to remove all non-ascii7 characters from the file."
1,"Registering NodeType from templates throws exception about invalid decl. node type.when adding PropertyDefinitionTemplates to NodeTypeTemplates, the internal declaredNodeType field is not set and causes the registration fail with:

org.apache.jackrabbit.api.jsr283.nodetype.InvalidNodeTypeDefinitionException: [{}foo#{}test] invalid declaring node type specified
	at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.registerNodeTypes(NodeTypeManagerImpl.java:695)
	at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.registerNodeType(NodeTypeManagerImpl.java:615)"
0,"make collection element names configurable- add jcrElementName to CollectionDescriptor and Collection annotation
- make COLLECTION_ELEMENT_NAME protected instead of private
"
1,Fix for NPE's in Spatial Lucene for searching bounding box onlyNPE occurs when using DistanceQueryBuilder for minimal bounding box search without the distance filter.
1,"Superfluous AndQueryNode  in query tree built by SQL parserTest query (tested with <http://people.apache.org/~mreutegg/jcr-query-translator/translator.html>):

  SELECT * FROM nt:folder WHERE x = 1 

generates the following query tree:

+ Root node
+ Select properties: *
  + PathQueryNode
    + LocationStepQueryNode:  NodeTest=* Descendants=true Index=NONE
      + AndQueryNode
        + RelationQueryNode: Op: =  Prop=@{}x Type=LONG Value=1
      + NodeTypeQueryNode:  Prop={http://www.jcp.org/jcr/1.0}primaryType Value={http://www.jcp.org/jcr/nt/1.0}folder

It seems the AndQueryNode is superfluous.
"
0,"[HttpClient] Authenticator() - ability to perform alternate authenticationMy post to the user group.  The developer replied suggesting I enter an 
enhancement request.

-----Original Message-----
From: Gustafson, Vicki [mailto:vicki.gustafson@us.didata.com]
Sent: Thursday, 12 December 2002 5:03 AM
To: Jakarta Commons Users List
Subject: [HttpClient] Authentication using Basic

Is there a way to specify which authentication scheme you would like the client 
to use if several schemes are returned in the www-auth header?

I'm performing a simple post using the httpClient.  The server returns a 401 at 
which point the httpClient tries to authenticate with the server.  The 
following header is received:

Attempting to parse authenticate header: 'WWW-Authenticate: Negotiate, NTLM, 
Basic realm=""XXXwhateverXXX""

I need to authenticate using Basic, but the Authenticator class will only try 
the most secure scheme:  NTLM.  Is there a setting or parameter I can set to 
force the httpClient to use Basic?

thanks,
Vicki

// determine the most secure request header to add
Header requestHeader = null;
if (challengeMap.containsKey(""ntlm"")) {
    String challenge = (String) challengeMap.get(""ntlm"");
    requestHeader = Authenticator.ntlm(challenge, method, state,
    responseHeader);
} else if (challengeMap.containsKey(""digest"")) {
    String challenge = (String) challengeMap.get(""digest"");
    String realm = parseRealmFromChallenge(challenge);
    requestHeader = Authenticator.digest(realm, method, state,
    responseHeader);
} else if (challengeMap.containsKey(""basic"")) {
    String challenge = (String) challengeMap.get(""basic"");
    String realm = parseRealmFromChallenge(challenge);
    requestHeader = Authenticator.basic(realm, state, responseHeader);
} else if (challengeMap.size() == 0) {
    throw new HttpException(""No authentication scheme found in '""
    + authenticateHeader + ""'"");
} else {
    throw new UnsupportedOperationException(
    ""Requested authentication scheme "" + challengeMap.keySet()
    + "" is unsupported"");
}

--
To unsubscribe, e-mail:   <mailto:commons-user-unsubscribe@jakarta.apache.org>
For additional commands, e-mail: <mailto:commons-user-help@jakarta.apache.org>


--
To unsubscribe, e-mail:   <mailto:commons-user-unsubscribe@jakarta.apache.org>
For additional commands, e-mail: <mailto:commons-user-help@jakarta.apache.org>

**********developer response**********************************



Currently there isn't, however we probably should be more intelligent about 
falling back to other authentication schemes based on the type of credentials 
provided.  Having said this I'm not sure it conforms to the HTTP spec strictly 
(which states that the client must use the strongest authentication scheme it 
supports, there's a grey area here because if your application doesn't provide 
a dialog or similar for the user to enter NTLM credentials it can only support 
basic or digest authentication, despite HTTPClient supporting NTLM).

What I'd like to see happen is:

When NTLM authentication is requested as top priority but only 
UsernamePasswordCredentials are available instead of NTLMCredentials we fall 
back to one of the other schemes.  In general this would mean that:

if an authentication scheme is requested and a credentials object of the wrong 
type is provided, HTTPClient should assume (probably optionally or only in non-
strict mode) that the requested authentication scheme is not supported and fall 
through to other options.

Achieving this would require a reasonably amount of refactoring of the 
Authenticator class but shouldn't be impossible.  Unfortunately I don't have 
time to do it myself at the moment but I'd be happy to help out if you felt 
like doing it, otherwise logging an enhancement bug in Bugzilla would be a good 
way to record this request until someone has time to actually implement it.

Adrian Sutton, Software Engineer
Ephox Corporation
www.ephox.com"
0,"Remove superfluous comment in MMapDirectory.javaSee title, and I prefer my name to be removed from the source code."
1,"rep:excerpt() not working for attribute searchesexample: //element(*, nt:unstructured)[jcr:contains(@textProp, 'foobar')]/(rep:excerpt())

produces empty excerpts."
1,"Open-scoped locks may be lost on restart and might not be transferrableTwo issues with open-scoped locks were reported by Cdric Damioli:

(1) When open-scoped locks are being reapplied on repository startup, locks on non-referenceable nodes are lost.
(2) When a session holding an open-scoped lock logs out, the lock token is not automatically removed from the session and other sessions are not able to take responsibility for the lock, even when having the correct lock token.
"
0,"textfilters module patch: Support for text extraction for HTML,XML and RTF filesThis patch adds text extraction support form XML, RTF and HTML files.

The unique dependency is htmlparser library for handling HTML text extraction."
0,"Remove norms() support from non-atomic IndexReadersSpin-off from LUCENE-2769:
Currently all IndexReaders support norms(), but the core of Lucene never uses it and its even dangerous because of memory usage. We should do the same like with MultiFields and factor it out and throw UOE on non-atomic readers.

The SlowMultiReaderWrapper can then manage the norms. Also ParallelReader needs to be fixed."
0,"[PATCH] cleanup unwanted stream closing that isn't usedDue to refactoring, a stream is being closed that is never used. Isn't harmful, just is cruft."
1,"[contrib-bdb] initialization fails if directory doesn't existBerkeleyDBPersistenceManager initialization fails if the directory configured doesn't exist (this doesn't happen with other PMs).
This can easily be fixed in the persistence manager, by making it create all the directories in the path (actually it only creates the last -db- directory).

The trivial patch is to replace envDir.mkdir() to envDir.mkdirs() (note the final ""s"") at BerkeleyDBPersistenceManager  line 73:
        if (!envDir.exists())
            envDir.mkdir();
should be:
        if (!envDir.exists())
            envDir.mkdirs();

(I am not submitting any svn diff since the manual fix sounds so trivial, it's easier to change it manually)
"
0,"Lucene Scorer implementations should handle the 'advance' to NO_MORE_DOCS optimisation betterThis is from the lucene Scorer (actually DocIdSetIterator) api:
""NOTE: this method may be called with NO_MORE_DOCS for efficiency by some Scorers. If your implementation cannot efficiently determine that it should exhaust, it is recommended that you check for that value in each call to this method.""

None of the scorer implementations does that currently. Except for ChildAxisScorer thanks to JCR-3082.

This is a worthwhile effort, which can save us from bugs (JCR-3082) but also leverage some performance optimisation hints from the lucene api."
0,"Split DocMaker into ContentSource and DocMakerThis issue proposes some refactoring to the benchmark package. Today, DocMaker has two roles: collecting documents from a collection and preparing a Document object. These two should actually be split up to ContentSource and DocMaker, which will use a ContentSource instance.

ContentSource will implement all the methods of DocMaker, like getNextDocData, raw size in bytes tracking etc. This can actually fit well w/ 1591, by having a basic ContentSource that offers input stream services, and wraps a file (for example) with a bzip or gzip streams etc.

DocMaker will implement the makeDocument methods, reusing DocState etc.

The idea is that collecting the Enwiki documents, for example, should be the same whether I create documents using DocState, add payloads or index additional metadata. Same goes for Trec and Reuters collections, as well as LineDocMaker.
In fact, if one inspects EnwikiDocMaker and LineDocMaker closely, they are 99% the same and 99% different. Most of their differences lie in the way they read the data, while most of the similarity lies in the way they create documents (using DocState).
That led to a somehwat bizzare extension of LineDocMaker by EnwikiDocMaker (just the reuse of DocState). Also, other DocMakers do not use that DocState today, something they could have gotten for free with this refactoring proposed.

So by having a EnwikiContentSource, ReutersContentSource and others (TREC, Line, Simple), I can write several DocMakers, such as DocStateMaker, ConfigurableDocMaker (one which accpets all kinds of config options) and custom DocMakers (payload, facets, sorting), passing to them a ContentSource instance and reuse the same DocMaking algorithm with many content sources, as well as the same ContentSource algorithm with many DocMaker implementations.

This will also give us the opportunity to perf test content sources alone (i.e., compare bzip, gzip and regular input streams), w/o the overhead of creating a Document object.

I've already done so in my code environment (I extend the benchmark package for my application's purposes) and I like the flexibility I have. I think this can be a nice contribution to the benchmark package, which can result in some code cleanup as well."
0,"Lucene 2.0 requirements - Remove all deprecated codePer the move to Lucene 2.0 from 1.9, remove all deprecated code and update documentation, etc.

Patch to follow shortly."
0,"clean up build files so contrib tests are run more easilyPer mailing list discussion...

http://www.nabble.com/Tests%2C-Contribs%2C-and-Releases-tf3768924.html#a10655448

Tests for contribs should be run when ""ant test"" is used,  existing ""test"" target renamed to ""test-core""
"
0,counter field in segments file is not documented in fileformats.xmlThe counter field in the current segments file format is not documented.
0,"Replace BundleFsPersistenceManager with DerbyPersistenceManager in the JR Core indexing testsRunning the JR Core tests yields a deprecation warning on account of workspace config being outdated for some indexing tests:

  INFO  o.a.j.core.config.BeanConfig - org.apache.jackrabbit.core.persistence.pool.BundleFsPersistenceManager is deprecated. Please use org.apache.jackrabbit.core.persistence.bundle.BundleFsPersistenceManager instead

This shows up 3 times in the logs because there are 3 indexing related workspaces that need this config update."
1,"HttpMultiClient reuses closed connectionsIf a socket times out while sitting in the connection pool, 
HttpConnectionManager still attempts to reuse it resulting in an IOException 
being thrown when writing to the socket.  I believe this is a problem with both 
server side and client side timeouts (ie: we try to reuse a connection that we 
timed out) though am not certain of that.  At the very least server side 
timeouts cause the issue.

As yet I can't see how to fix this.  With the current code there doesn't even 
appear to be a suitable workaround because when the exception is thrown, the 
connection is added back into the pool to be reused (even though it is closed) 
which causes the next attempt to fail as well.

I can't see any reliable way to tell whether or not a connection is open, so 
would suggest the following as a fix:

1. In HttpMultiClient.executeMethod, close the connection if an exception is 
thrown (optionally, only if an IOException is thrown instead of an 
HttpException, but generally exceptions tend to leave things in an unknown 
state).

2. (optional) Add a retry loop to executeMethod to retry if an exception occurs 
(possibly only if an IOException is thrown, depending on exactly when a 
HttpException is thrown).

I'll attach a patch which does both of this to help clarify."
1,"UserInfo disapears after creating URII tested this using firefox (Where I have configured our proxy server)
I run the following URI: ftp://username:password@ftp.mytest.test/testdir/

I use a sniffer to look at the GET commond send to the proxy server. It looks as
follows:

GET ftp://username:password@ftp.mytest.test/testdir/ HTTP/1.1
Host: ftp.mytest.test
User-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.0; en-US; rv:1.7.12)
Gecko/20050915 Firefox/1.0.7
Accept:
text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain;q=0.8,image/png,*/*;q=0.5
Accept-Language: en-us,en;q=0.5
Accept-Encoding: gzip,deflate
Accept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7
Keep-Alive: 300
Proxy-Connection: keep-alive

Using this request we get access to the directory and see the contents displayed.
However, when I try the same in Java (using HttpClient) I get the following GET
request (Java code included below):

GET ftp://ftp.mytest.test/testdir/ HTTP/1.1
User-Agent: Jakarta Commons-HttpClient/3.0-rc3
Host: ftp.mytest.test
Proxy-Connection: Keep-Alive

Finally I get a ACCESS DENIED error. 
This seems to be because the GET request does not contain the USER / PASSWORD
info in the URL.

/// JAVA CODE:

package nl.essent.test.ftp.httptest;

import java.io.IOException;

import org.apache.commons.httpclient.Credentials;
import org.apache.commons.httpclient.DefaultHttpMethodRetryHandler;
import org.apache.commons.httpclient.HostConfiguration;
import org.apache.commons.httpclient.HttpClient;
import org.apache.commons.httpclient.HttpException;
import org.apache.commons.httpclient.HttpMethod;
import org.apache.commons.httpclient.HttpStatus;
import org.apache.commons.httpclient.NTCredentials;
import org.apache.commons.httpclient.UsernamePasswordCredentials;
import org.apache.commons.httpclient.auth.AuthScope;
import org.apache.commons.httpclient.methods.GetMethod;
import org.apache.commons.httpclient.params.HttpMethodParams;
import org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory;
import org.apache.commons.httpclient.protocol.Protocol;

public class TestClient {

    public static void main(String[] args) {
        new TestClient().testFtpViaHttp();
    }
    
    public void testFtpViaHttp() {
        
        HttpClient client = new HttpClient();
        
        HostConfiguration hostConfig = client.getHostConfiguration();
        hostConfig.setProxy(""proxy"", 8080);
        client.setHostConfiguration(hostConfig);
        
        Protocol protol = new Protocol(""ftp"", new
DefaultProtocolSocketFactory(), 21);
        Protocol.registerProtocol(""ftp"", protol);
        
        Credentials proxyCreds = new NTCredentials(""xxxx"", ""xxxxx"","""", ""xxxx"" );
        client.getState().setProxyCredentials(AuthScope.ANY, proxyCreds);
        
        GetMethod gmethod = new
GetMethod(""ftp://username:password@ftp.mytest.test/testdir/"");
        
        gmethod.getParams().setParameter(HttpMethodParams.RETRY_HANDLER, 
                new DefaultHttpMethodRetryHandler(3, false));
       
        try {
            // Execute the method.
            int statusCode = client.executeMethod(gmethod);

            if (statusCode != HttpStatus.SC_OK) {
              System.err.println(""Method failed: "" + gmethod.getStatusLine());
            }

            // Read the response body.
            byte[] responseBody = gmethod.getResponseBody();

            // Deal with the response.
            // Use caution: ensure correct character encoding and is not binary data
            System.out.println(new String(responseBody));

          } catch (HttpException e) {
            System.err.println(""Fatal protocol violation: "" + e.getMessage());
            e.printStackTrace();
          } catch (IOException e) {
            System.err.println(""Fatal transport error: "" + e.getMessage());
            e.printStackTrace();
          } finally {
            // Release the connection.
            gmethod.releaseConnection();
          } 

    }

}

//// END JAVA CODE"
1,"SnapshotDeletionPolicy.snapshot() throws NPE if no commits happenedSDP throws NPE if no commits occurred and snapshot() was called. I will replace it w/ throwing IllegalStateException. I'll also move TestSDP from o.a.l to o.a.l,index. I'll post a patch soon"
0,Test case for RTFTextExtractorThere should be a test case for the RTFTextExtractor.
0,"Session leak in API test casesIn setUp(). AbstractPropertyTest checks whether it can execute the test and potentially throws an NotExecutableException. In this case, the helper Session is lost (without logging out), and the parent's tearDown() code isn't executed. 

The same problem may be present in more test classes (will check).
"
0,Speedup of CharArraySet#copy if a CharArraySet instance is passed to copy.the copy method should use the entries array itself to copy the set internally instead of iterating over all values. this would speedup copying even small set 
0,"don't try to cache a composite reader's MultiBits deletedDocsMultiFields.getDeletedDocs now builds up a MultiBits instance (so that one can check if a top-level docID is deleted), but it now stuffs it into a private cache on IndexReader.

This is invalid when the composite reader is read/write, and can result in a MultiReader falsely claiming a doc was not deleted."
0,"CookiePolicy.registerCookieSpec(CookiePolicy.DEFAULT, <Some CookieSpec>); does not work as documentedI use HtmlUnit to access some quote information. Cookies shall be ignored,so I
set the default policy to CookiePolicy.IGNORE_COOKIES. Nevertheless I get error
messages after starting my program. 

The code reads:
---------------------------------------
CookiePolicy.registerCookieSpec(CookiePolicy.DEFAULT, IgnoreCookiesSpec.class);
final WebClient webClient = new WebClient();
final URL url = new URL(""http://de.finance.yahoo.com/q?s=CB3569.SG"");
final HtmlPage page = (HtmlPage) webClient.getPage(url);
----------------------------------------

The error messages are:
----------------------------------------
WARNUNG: Cookie rejected: ""$Version=0; PRF=&t=CB3569.SG; $Domain=finance.yahoo.c
om; $Path=/"". Domain attribute ""finance.yahoo.com"" violates RFC 2109: domain mus
t start with a dot
30.11.2005 10:28:20 org.apache.commons.httpclient.HttpMethodBase processResponse
Headers
WARNUNG: Cookie rejected: ""$Version=0; B=7fkoh9l1oqs4h&b=3&s=hb; $Domain=.yahoo.
com; $Path=/"". Domain attribute "".yahoo.com"" violates RFC 2109: host minus domai
n may not contain any dots
----------------------------------------"
0,"Don't create compound file for large segments by defaultSpinoff from LUCENE-2762.

CFS is useful for keeping the open file count down.  But, it costs
some added time during indexing to build, and also ties up temporary
disk space, causing eg a large spike on the final merge of an
optimize.

Since MergePolicy dictates which segments should be CFS, we can
change it to only build CFS for ""smallish"" merges.

I think we should also set a maxMergeMB by default so that very large
merges aren't done.
"
0,"Remove per-document multiply in FilteredQuerySpinoff of LUCENE-1536.

In LUCENE-1536, Uwe suggested using FilteredQuery under-the-hood to implement filtered search.

But this query is inefficient, it does a per-document multiplication (wrapped.score() * boost()).

Instead, it should just pass the boost down in its weight, like BooleanQuery does to avoid this per-document multiply."
0,"Add IndexReader.acquire() and release() methods using IndexReader's ref countingFrom: http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200807.mbox/%3cPine.OSX.4.64.0807170752080.1708@c5850-a3-2-62-147-22-102.dial.proxad.net%3e

I have a server where a bunch of threads are handling search requests. I
have a another process that updates the index used by the search server and
that asks the searcher server to reopen its index reader after the updates
completed.

When I reopen() the index reader, I also close the old one (if the reopen()
yielded a new instance). This causes problems for the other threads that
are currently in the middle of a search request.

I'd like to propose the addition of two methods, acquire() and release() 
(attached to this bug report), that increment/decrement the ref count that IndexReader 
instances currently maintain for related purposes. That ref count prevents 
the index reader from being actually closed until it reaches zero.

My server's search threads, thus acquiring and releasing the index reader 
can be sure that the index reader they're currently using is good until 
they're done with the current request, ie, until they release() it.
"
1,"Remove FieldMaskingSpanQuery (or fix its scoring)In Lucene 4.0 we added new scoring mechanisms, but FieldMaskingSpanQuery is a serious problem:

Because it lies about the fields of its terms, this sometimes results in totally bogus
statistics, cases where a single terms totalTermFreq exceeds sumTotalTermFreq for the entire field (since its lying about it).

Such lying could result in NaN/Inf/Negative scores, exceptions, divide by zero, and other problems,
because the statistics are impossibly bogus."
1,"JCR-Server: respect maximal value for timeoutRFC 2518 states:

""The timeout value for TimeType ""Second"" MUST NOT be greater than 2^32-1.""

->> adjust constant according.

BTW: sending 'Infinite' timeout in case of maximal value causes problems with microsoft builtin client, that will never unlock that resource."
1,Using the WeightedTerms option in the Highlighter can cause fragments to be supressed for indexes with deletesAn index with a few documents and many deletes can report a lower total docs than docFreq for a term - total docs will account for deletes while docFreq will not - this causes the idf to be negative and the fragment to score < 0.
1,"QueryResult's RowIterator.getSize returned the wrong size of the results after I implemented my own AccessManagerThe background is I have implemented my own AccessManager. After executing a query and get back the RowIterator from the result, if I call rowiterator.getSize, it will return the size of all nodes matching my query (without honoring the access control) . But if I iterate through the result, I find lots of duplicates in the results; and if I filter out those duplicate, the final result is quite off the original number from RowIteartor.getSize()

BTW, I also disabled Doc Order sorting.

 "
1,"EnhancementsPayloadIterator.getCategoryData(CategoryEnhancement) problematic usage of Object.equals()EnhancementsPayloadIterator has an internal list of category enhancemnets, and in getCategoryData(CategoryEnhancement) there is a lookup of the given CategoryEnhancement in the list. In order to make sure this lookup works, CategoryEnhancement must override Object.equals(Object)."
0,"Query scorers should not use MultiFieldsLucene does all searching/filtering per-segment, today, but there are a number of tests that directly invoke Scorer.scorer or Filter.getDocIdSet on a composite reader."
0,"A Linux-specific Directory impl that bypasses the buffer cacheI've been testing how we could prevent Lucene's merges from evicting
pages from the OS's buffer cache.  I tried fadvise/madvise (via JNI)
but (frustratingly), I could not get them to work (details at
http://chbits.blogspot.com/2010/06/lucene-and-fadvisemadvise.html).

The only thing that worked was to use Linux's O_DIRECT flag, which
forces all IO to bypass the buffer cache entirely... so I created a
Linux-specific Directory impl to do this.
"
0,FastVectorHighlighter: support DisjunctionMaxQueryAdd DisjunctionMaxQuery support in FVH. 
1,"HttpClient does not correctly handle escaped characters in HTTP header elementsAn excerpt from Microsoft's ""How Digest Authentication Works"":
http://www.microsoft.com/technet/prodtechnol/windowsserver2003/library/TechRef/717b450c-f4a0-4cc9-86f4-cc0633aae5f9.mspx

<quote>
* RFC 2617-compliant Digest Authentication challenges and responses must also
comply with RFC 2616: Hypertext Transfer Protocol -- HTTP/1.1 quoted string
requirements. This requirement particularly affects the use of backslash (\) and
embedded double quotes. Both must be preceded (escaped) with a backslash.

* For example, domain\username according to RFC 2616 is read as domainusername.
This reading is important because if an application sends information in this
format rather than as domain\\username, authentication fails.

* However, because this is a known issue with domain\username , if
authenticating with backslash encoding fails, Digest SSP attempts to
authenticate the response and assumes that the backslash is part of the string.
This behavior can be turned off by setting the ServerCompat registry key.
</quote>

Review and fix the ParameterParser class and classes implemeting CookieSpec or
AuthScheme interfaces

See also PR #34909"
0,"Define and implement Logging policyWhen to use info vs debug vs warning?  
When to log exceptions?
enter() and exit() messages on a per method basis?
Always log debug when swallowing an exception?"
0,"Port 3.x FieldCache.getDocsWithField() to trunk[Spinoff from LUCENE-3390]

I think the approach in 3.x for handling un-valued docs, and making it
possible to specify how such docs are sorted, is better than the
solution we have in trunk.

I like that FC has a dedicated method to get the Bits for docs with field
-- easy for apps to directly use.  And I like that the
bits have their own entry in the FC.

One downside is that it's 2 passes to get values and valid bits, but
I think we can fix this by passing optional bool to FC.getXXX methods
indicating you want the bits, and the populate the FC entry for the
missing bits as well.  (We can do that for 3.x and trunk). Then it's
single pass.
"
1,"Nodes that have properties marked for async extraction should be available for queryingThe problems only appears when dealing with nodes that have async extractors. In this case we return a lightweight copy of the node (without the property that will be processed in the background).

The copy algorithm ignores certain field types (that have been probably introduced during the Lucene 3 upgrade, not sure) such as SingletonTokenStream(s).
So the lightweight copy does not include all the existing properties, therefore the node will not appear in queries during the extraction time."
0,"Similarity#score deprecated method - javadoc reference + SimilarityDelegatorOld method  

  public float scorePayload(String fieldName, byte [] payload, int offset, int length)

has been deprecated by - 

  public float scorePayload(int docId, String fieldName, int start, int end, byte [] payload, int offset, int length)


References in PayLoadNearQuery (javadoc) changed. 

Also - SimilarityDelegator overrides the new method as opposed to the (deprecated) old one. "
0,"Deprecate Directory.touchFileLucene doesn't use this method, and, FindBugs reports that FSDirectory's impl shouldn't swallow the returned result from File.setLastModified."
1,"WriteLineDocTask should keep docs w/ just title and no bodyWriteLineDocTask throws away a document if it does not have a body element. However, if the document has a title, then it should be kept. Some documents, such as emails, may not have a body which is legitimate. I'll post a patch + a test case."
1,"JcrParser: use of bitwise instead of logical AND operatorJcrParser, line 134:

            if ((!insideSingleQuote & !insideDoubleQuote & Character
"
0,"Support for boost factor in MoreLikeThisThis is a patch I made to be able to boost the terms with a specific factor beside the relevancy returned by MoreLikeThis. This is helpful when having more then 1 MoreLikeThis in the query, so words in the field A (i.e. Title) can be boosted more than words in the field B (i.e. Description)."
0,"Jcr-server: Report#init limits the Report interface to DeltaV compliant resourcesAlthough the REPORT features is defined by RFC 3253 and required for DeltaV compliant resources it is not limited to deltaV. See RFC 3744 (WebDAV Access Control Protocol) for additional usage of the REPORT functionality.

In order to keep the Report interface open for later usage by resources providing support for RFC 3744,
the init method signature could be modified as follows:

Report#init(DavResource, ReportInfo) instead of
Report#init(DeltaVResource, ReportInfo)"
0,"Core Test should not have dependencies on the Demo codeThe TestDoc.java Test file has a dependency on the Demo FileDocument code.  Some of us don't keep the Demo code around after downloading, so this breaks the build.

Patch will be along shortly"
0,"All implementations of SchemeSocketFactory.createSocket(HttpParams params) ignore the paramsOnly TestTSCCMWithServer.StallingSocketFactory.createSocket(HttpParams params) ever uses the HttpParams parameter.

All non-test implementations of the method ignore the parameter.

Not sure why this version of the method exists if the parameter is never used - the parameterless method from SocketFactory could be used instead."
1,"Incorrect handling of InputStreams when connecting to a server that requires authenticationI'm trying to upload a file to a WebDav server (mod_dav on Apache Web Server 2.2.14) that has basic (or digest, the result is the same) authentication enabled.
I'm using the following code:
        String url = ""http://myserver/dir/test2.gif"";
        File file = new File(""d:/test2.gif"");
        DefaultHttpClient httpClient = new DefaultHttpClient();
        HttpPut put = new HttpPut(url);
        put.setEntity(new InputStreamEntity(new FileInputStream(file), file.length()));
        
        URI uri = put.getURI();
        httpClient.getCredentialsProvider().setCredentials(new AuthScope(uri.getHost(), uri.getPort()),
                getCredentials());
        put.getParams().setBooleanParameter(CoreProtocolPNames.USE_EXPECT_CONTINUE, true);
        HttpResponse response = httpClient.execute(put);
        System.out.println(response.getStatusLine());

When running the above code, I'm getting a org.apache.http.client.NonRepeatableRequestException: Cannot retry request with a non-repeatable request entity. I tested both the latest alpha & the svn head. Doing the same thing in HttpClient 3.1 worked as expected. 

This could be normal, as I'm using an InputStream that is indeed not repeatable, but as I'm also using Expect: 100-Continue, the stream shouldn't have been consumed with the first connection (the one that gets a code 401 from the WebDav server), and only in the second one, when the credentials are provided.

The problem is that DefaultRequestDirector.execute doesn't take this into account and assumes that if a request has been tried once, its associated entity (if any) has been consumed.
Here's the fix that I came up with:
Change DefaultRequestDirector.execute so that if the wrapper is an EntityEnclosingRequestWrapper, it checks if the entity has actually been consumed before throwing a NonRepeatableRequestException. I'm using the method isStreaming() from HttpEntity, as it's the closest thing to what I was looking for. Reading the JavaDoc, it could lead to the situation where an entity has started streaming but has not yet finished, and so is not in a state where it can be used. However I don't think that's a problem as the javadoc for HttpEntity.getContent() states that it can't be called two times on a non-repeatable entity, so it's just a matter of when the request will fail.
This lead me to also modify InputStreamEntity (from the httpCore project) as it didn't comply with the javadoc. With these two modifications, The file upload completes successfully.

I also modified:
 * TestInputStreamEntity.testBasics() (from the httpCore project) test so that it complies with getContent()'s Javadoc.
 * TestDefaultClientRequestDirector.FaultyHttpRequestExecutor because it didn't consume the entity's content.
All the tests from both httpCore and httpClient pass.
I tested both InputStreamEntity and BasicHttpEntity.
 
Please keep in mind that I am by no means an httpClient (or http, for that matter) expert, and these modifications may have some unexpected side-effects that I did not foresee, contain plain dumb code, or whatever, so it would be great if someone could review my changes and give their opinion.
"
0,"add IndexReader.getUniqueTermCountSimple API to return number of unique terms (across all fields).  Spinoff from here:

http://www.lucidimagination.com/search/document/536b22e017be3e27/term_limit"
1,"MultiIndexDocValues pretends it can merge sorted sourcesNightly build hit this failure:

{noformat}
ant test-core -Dtestcase=TestSort -Dtestmethod=testReverseSort -Dtests.seed=791b126576b0cfab:-48895c7243ecc5d0:743c683d1c9f7768 -Dtests.multiplier=3 -Dargs=""-Dfile.encoding=ISO8859-1""

    [junit] Testcase: testReverseSort(org.apache.lucene.search.TestSort):	Caused an ERROR
    [junit] expected:<[CEGIA]> but was:<[ACEGI]>
    [junit] 	at org.apache.lucene.search.TestSort.assertMatches(TestSort.java:1248)
    [junit] 	at org.apache.lucene.search.TestSort.assertMatches(TestSort.java:1216)
    [junit] 	at org.apache.lucene.search.TestSort.testReverseSort(TestSort.java:759)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$3$1.evaluate(LuceneTestCase.java:523)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:149)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:51)
{noformat}

It's happening in the test for reverse-sort of a string field with DocValues, when the test had gotten SlowMultiReaderWrapper.

I committed a fix to the test to avoid testing this case, but we need a better fix to the underlying bug.

MultiIndexDocValues cannot merge sorted sources (I think?), yet somehow it's pretending it can (in the above test, the three subs had BYTES_FIXED_SORTED type, and the TypePromoter happily claims to merge these to BYTES_FIXED_SORTED; I think MultiIndexDocValues should return null for the sorted source in this case?"
1,"IndexCommit.equals() bugIndexCommit.equals() checks for equality of Directories and versions, but it doesn't check IMHO the more important generation numbers. It looks like commits are really identified by a combination of directory and segments_XXX, which means the generation number, because that's what the DirectoryReader.open() checks for.

This bug leads to an unexpected behavior when the only change to be committed is in userData - we get two commits then that are declared equal, they have the same version but they have different generation numbers. I have no idea how this situation is treated in a few dozen references to IndexCommit.equals() across Lucene...

On the surface the fix is trivial - either add the gen number to equals(), or use gen number instead of version. However, it's puzzling why these two would ever get out of sync??? and if they are always supposed to be in sync then maybe we don't need both of them at all, maybe just generation or version is sufficient?"
0,"SPI: RepositoryService.getItemInfos should be allowed to return entries outside of the requested tree.michael duerig asked for for that extension of the semantic of RepositoryService.getItemInfos.
currently this doesn't work and leads to an inconsistent hierarchy."
0,"TCK: NamespaceRegistryTest#testUnregisterNamespaceExceptions doesn't fail if expected exception isn't thrownIn two places, the test doesn't fail if an expected exception isn't thrown.

Proposal: test should fail if expected exception isn't thrown.

--- NamespaceRegistryTest.java  (revision 422074)
+++ NamespaceRegistryTest.java  (working copy)
@@ -150,6 +154,7 @@
         for (int t = 0; t < SYSTEM_PREFIXES.length; t++) {
             try {
                 nsp.unregisterNamespace(SYSTEM_PREFIXES[t]);
+                fail(""Trying to unregister "" + SYSTEM_PREFIXES[t] + "" must fail"");
             } catch (NamespaceException e) {
                 // expected behaviour
             }
@@ -159,6 +164,7 @@
         // must throw a NamespaceException.
         try {
             nsp.unregisterNamespace(""ThisNamespaceIsNotCurrentlyRegistered"");
+            fail(""Trying to unregister an unused prefix must fail"");
         } catch (NamespaceException e) {
             // expected behaviour
         }
"
0,"HeaderElement#parse(String) implementation is not optimalThe cookie setted by the LocalDirector 416 Version 4.2.3 has a bug.
It sets for Tuesday and Thursday ""Tues"" and ""Thur"" instead of the 
canonical ""Tue"" and ""Thu"". This break the parsing stage and stop HttpClient to 
work for 2 days a week. Of course I modified the parse method in HeaderElement 
class, but everytime I download a new version, I have to remake the jar....
It's possible to include this into the CVS files ?"
0,"JSR 283: Node Type Attribute Subtyping Rules JCR 2.0 clarifies node type attribute subtyping rules whereas JCR 1.0 didn't mandate any specific behavior.

""3.7.6.7 Node Type Attribute Subtyping Rules"" states  (assume T' being a subtype of T):

- if T has orderable child nodes then T' must have orderable child nodes
- if T specifies a primary item then T' inherits that setting and must not override it

"
1,"BlockJoinQuery doesn't implement boostAfter reviewing LUCENE-3494, i checked other queries and noticed that BlockJoinQuery currently throws UOE for getBoost and setBoost:
{noformat}
throw new UnsupportedOperationException(""this query cannot support boosting; please use childQuery.setBoost instead"");
{noformat}

I don't think we can safely do that in queries, because other parts of lucene rely upon this working... for example BQs rewrite when
it has a single clause and erases itself.

So I think we should just pass down the boost to the inner weight.
"
1,"Retry on ConnectionException does not workI noticed that the Retry handler mechanism does not work when the client cannot
initiate a connection (which throws java.net.ConnectionException). This happens
for me for instance when there is proxy and a tunneling in the picture and
sometimes there are connectivity problems.

I had my own RetryHandler, however, the Connection Timeout exception never falls
in it. I took a look at the source code and noticed that the open() method and
any thrown exception at this level occurs outside the control of the Retry
Handler (which seems to be involved only after open() succeeds).

In fact, if the open() throws ConnectionException (as is my case), since the
try/catch wrapping the open() is not inside the while() but on top of it, it
stops the loop and the retry handler does not get a chance to be invoked .

riad"
0,"Exclude PrecedenceQueryParser from build or disable failing test casesAs Erik commented in LUCENE-885 the PrecendenceQueryParser is currently
unmaintained. Since some tests are failing we should either exclude PQP from the 
build or simply disable the failing tests."
0,"JCR2SPI does not provide actual size on RangeIterator.getSize()Currently, JCR2SPI always returns -1 on RangeIterator.getSize().

This return value is legal (meaning ""unknown""), but may cause clients to simply iterate through the whole list when what they really want is simply the count.

Use case:

""The use case is to count the number of members of a NT_FOLDER without having to open up the NT_FOLDER and count all the members (and I assume load them into memory) ""

To make this happen we probably need to move away from simple Iterators on the SPI level, and put quite some additional work into JCR2SPI.

Feedback appreciated."
0,"Add a NamespaceHelper in jcr-commonsWe have a number of code snippets in jackrabbit-core and many JCR clients that do something like the following:

* get the prefix/URI for a given namespace URI/prefix without throwing an exception if the namespace is not found (return null instead)
* get a Map containing all current namespace prefix->URI mappings
* get the prefixed name for a given URI + local name pair in a given session (without a dependency to the SPI)
* safely register a given namespace (don't throw if the namespace is already registered, automatically select an unused prefix if needed, etc.)

I'd like to introduce a NamespaceHelper class in jcr-commons to cover such common code."
1,"Some small fixes after the flex merge...Changes:

  * Re-introduced specialization optimization to FieldCacheRangeQuery;
    also fixed bug (was failing to check deletions in advance)

  * Changes 2 checkIndex methods from protected -> public

  * Add some missing null checks when calling MultiFields.getFields or
    IndexReader.fields()

  * Tweak'd CHANGES a bit

  * Removed some small dead code
"
1,"ICU collator thread-safety issuesThe ICU Collators (unlike the JDK ones) aren't thread safe: http://userguide.icu-project.org/collation/architecture , a little non-obvious since its not mentioned
in the javadocs, and its not clear if the docs apply to only the C code, but i looked
at the source and there is all kinds of internal state.

So in my opinion, we should clone the icu collators (which are passed in from the outside) 
when creating a new TokenStream/AttributeImpl to prevent problems. This shouldn't be a big
deal since everything uses reusableTokenStream anyway.
"
0,"more lenient behavior of Node#addMixin if mixin is already present Change implementation of addMixin() so that it doesn't fail when the mixin is already present.

See also:

jackrabbit core change: <http://svn.apache.org/viewvc?view=rev&revision=570149>

JSR-283 issue: <https://jsr-283.dev.java.net/issues/show_bug.cgi?id=353>

(this affects both the TCK and JCR2SPI, so I didn't specify a component)"
1,"NamespaceRegistry.registerNamespace(pre, uri)  might accidentally remove namespace in certain situationsassume the following mappings exist in the global NamespaceRegistry:

pre1 <-> uri1
pre2 <-> uri2

the following stmt correclty throws a NamespaceException, complaining that an existing prefix can not be remapped:

nsReg.registerNamespace(""pre2"", ""uri1"")

but, as a sideeffect, it has also removed the mapping pre1 <-> uri1."
1,"ConcurrentModificationException in SessionItemStateManager.getIdOfRootTransientNodeState()SessionItemStateManager.getIdOfRootTransientNodeState() is throwing a ConcurrentModificationException on line 607:

Here's a snippet of the code:
{code}
                    for (NodeId id : candidateIds) {
                        if (nodeId.equals(id) || hierMgr.isAncestor(id, nodeId)) {
                            // already a candidate or a descendant thereof
                            // => skip
                            skip = true;
                            break;
                        }
                        if (hierMgr.isAncestor(nodeId, id)) {
                            // candidate is a descendant => remove
                            candidateIds.remove(id);
                        }
                    }
{code}

Can't use Collection.remove(Object) in the middle of iterating. It should probably be changed to use Iterator.remove():
{code}
                    Iterator<NodeId> nodeIdItor = candidateIds.iterator();
                    while (nodeIdItor.hasNext()) {
                        NodeId id = nodeIdItor.next();
                        if (nodeId.equals(id) || hierMgr.isAncestor(id, nodeId)) {
                            // already a candidate or a descendant thereof
                            // => skip
                            skip = true;
                            break;
                        }
                        if (hierMgr.isAncestor(nodeId, id)) {
                            // candidate is a descendant => remove
                            nodeIdItor.remove();
                        }
                    }
{code}

Any idea what I could do differently to workaround the issue?"
1,"RangeQuery equals method does not compare collator property fullyThe equals method in the range query has the collator comparison implemented as:
(this.collator != null && ! this.collator.equals(other.collator))

When _this.collator = null_ and _other.collator = someCollator_  this method will incorrectly assume they are equal. 

So adding something like
|| (this.collator == null && other.collator != null)
would fix the problem
"
0,Possible slowdown of indexing/merging on 3.x vs trunkOpening an issue to pursue the possible slowdown Marc Sturlese uncovered.
0,"Add solr's artifact signing scripts into lucene's build.xml/common-build.xmlSolr has nice artifact signing scripts in its common-build.xml and build.xml.

For me as release manager of 3.0 it would have be good to have them also when building lucene artifacts. I will investigate how to add them to src artifacts and maven artifacts"
0,Add cause to ItemStateException in BundleDbPersistenceManager.store()Currently only the message of the causing exception is given to the ItemStateException. 
0,"TCK: XPath order by test uses non-standard column specifier mechanismorg.apache.jackrabbit.test.api.QueryXPathOrderByTest generates a queries of the following form: /jcr:root/*[@proname]/@propname order by @propname

This syntax for column specifiers is not mandated by the JCR specification and will fail on any implementation that does not support it.

Instead the tests should use the following query: /jcr:root/*[@proname] order by @propname and then read the results using QueryResults.getNodes and not QueryResults.getRows."
1,"FilterIndexReader in trunk does not implement getSequentialSubReaders() correctlySince LUCENE-2459, getSequentialSubReaders() in FilterIndexReader returns null, so it returns an atomic reader. But If you call then any of the enum methods, it throws Exception because the underlying reader is not atomic.

We should move the null-returning method to SlowMultiReaderWrapper and fix FilterIndexReader's default to return in.getSequentialSubReaders(). Ideally an implementation must of course also wrap the sub-readers.

If we change this we have to look into other Impls like the MultiPassIndexSplitter if we need to add atomicity."
0,"Incorrect Specification-Title headers in MANIFEST.MFThe Specification-Title headers in MANIFEST.MF should all include the full project name, i.e.

Apache HttpComponents ...

The ""HttpComponents"" qualifier is missing; at present the entries are:

Specification-Title: Apache HttpClient
Specification-Title: Apache HttpMime

If present, Implementation-Title should follow the same convention."
0,"add option to CheckIndex to only check certain segmentsSimple patch to add -segment option to CheckIndex tool, to have it only check the particular segment, instead of all segments, from your index."
0,"Deprecating InstantiatedIndexWriterhttp://markmail.org/message/j6ip266fpzuaibf7

I suppose that should have been suggested before 2.9 rather than  
after...

There are at least three reasons to why I want to do this:

The code is based on the behaviour or the Directory IndexWriter as of  
2.3 and I have not been touching it since then. If there will be  
changes in the future one will have to keep IIW in sync, something  
that's easy to forget.
There is no locking which will cause concurrent modification  
exceptions when accessing the index via searcher/reader while  
committing.
It use the old token stream API so it has to be upgraded in case it  
should stay.

The java- and package level docs have since it was committed been  
suggesting that one should consider using II as if it was immutable  
due to the locklessness. My suggestion is that we make it immutable  
for real.

Since II is ment for small corpora there is very little time lost by  
using the constructor that builts the index from an IndexReader. I.e.  
rather than using InstantiatedIndexWriter one would have to use a  
Directory and an IndexWriter and then pass an IndexReader to a new  
InstantiatedIndex.

Any objections?"
0,"stackable parametersImplement ""stackable parameters"" to allow for a parameter hierarchy without linking params instances."
1,"DefaultHttpParamsFactory.getDefaultParams() is not thread safeThe method getDefaultParams() in 
org.apache.commons.httpclient.params.DefaultHttpParamsFactory is not thread 
safe.  In this code:

    public HttpParams getDefaultParams() {
        if (httpParams == null) {
            httpParams = createParams();
        }

        return httpParams;
    }

it is possible that httpParams will be called by one thread which will set 
httpParams, then a second thread may call it and may find httpParams is 
non-null.  However, under both the old (Java Language Spec chapter 17) and 
new Java Memory Models, the second thread won't necessarily see the values 
the first thread has set in the referenced HttpParams object.

The easiest way to fix this for all JVMs and memory models is by declaring 
getDefaultParams() to be synchronized."
0,"Use IOContext.READONCE in VarGapTermsIndexReader to load FSTVarGapTermsIndexReader should pass READONCE context down when it
opens/reads the FST. Yet, it should just replace the ctx passed in, ie if we are merging vs reading we want to differentiate.
"
1,"Field.setValue(...) doesn't properly handle switching between byte[] and other typesThis came up in PyLucene testing, based on Lucene 2.4.1.  Thread here:

  http://pylucene.markmail.org/message/75jzxzqi3smp2s4z

The problem is that Field.setValue does not fix up the isBinary
boolean, so if you create a String field, and then do
setValue(byte[]), you'll get an exception when adding a document
containing that field to the index."
1,"unsynchronized access on 'itemCache' map in ItemManager the access 'itemCache' map in ItemManager is mostly synchronized by not via the ItemStateListener methods:
[...]
    public void stateCreated(ItemState created) {
        ItemImpl item = retrieveItem(created.getId());
        if (item != null) {
            item.stateCreated(created);
        }
    }
[...]
    private ItemImpl retrieveItem(ItemId id) {
        return (ItemImpl) itemCache.get(id);
    }
[...]

this can result in a corruption of a map (eg subsequent accesses may result in a endless loop)."
0,"Use an enumeration for QOM operatorsThe PFD version of QueryObjectModelConstants contains some incorrect constant values that make it unusable as a source of operator constants.

Since we are now using Java 5, I propose that instead of adding our own replacement constant strings, we implement a type-safe Operator enumeration that contains fixed versions of all the operator constants declared in QueryObjectModelConstants."
0,"Upgrade to Lucene 2.2Lucene 2.1 contains a number of useful enhancements, which could be benefical to jackrabbit:

- less locking on index updates -> less IO calls
- introduces FieldSelector -> allows jackrabbit to only load required fields"
0,"Incorrect outer join TCK testsThe TCK test cases for outer joins seem to be incorrect. More specifically the expected result sets for the testRightOuterJoin1() and testLeftOuterJoin2() test cases in EquiJoinConditionTest are invalid, as shown below:

* testRightOuterJoin1() result set {{null, n1}, {n1, n2}, {n2, n2}} --> The n1 node does not have the propertyName2 property set, so the first tuple can never occur regardless of the join type. And since n2 already matches existing nodes, even {null, n2} can not be included in the result set. The correct result set for this query seems to be {{n1, n2}, {n2, n2}}.

* testLeftOuterJoin2() result set {{n1, null}, {n2, n1}, {n2, n2}} --> Same as above, a tuple with n1 as the leftmost node is not possible. The correct result set would be {{n2, n1}, {n2, n2}}.

Unfortunately the correct result sets here don't actually exercise the outer join functionality, i.e. none of the nodes in the returned tuples are null. We'll need to modify the test case setup to fix this."
0,"make BaseCharFilter more efficient in performancePerformance degradation in Solr 1.4 was reported. See:

http://www.lucidimagination.com/search/document/43c4bdaf5c9ec98d/html_stripping_slower_in_solr_1_4

The inefficiency has been pointed out in BaseCharFilter javadoc by Mike:

{panel}
NOTE: This class is not particularly efficient. For example, a new class instance is created for every call to addOffCorrectMap(int, int), which is then appended to a private list. 
{panel}
"
0,Added the functionality to Map and Manage Type EnumOCM API does not come with a mapper that can map Type Enum.  I have added this functionality.  Attached patch has test cases that tests the feature for Simple fields and Collection fields (For both anotations and digester based implementations)
1,"ClassCastException when registering new node typejava.lang.ClassCastException: org.apache.jackrabbit.core.nodetype.NodeTypeImpl
	at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.registerNodeTypes(NodeTypeManagerImpl.java:708)
	at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.registerNodeType(NodeTypeManagerImpl.java:637)

"
0,Remove unnecessary array wrapping when calling varargs methodsvarargs method callers don't have to wrap args in arrays
0,"HttpClient Manual Simplified ChineseDuring Nov 2010, The Chinese user Nanlei translated the Apache HttpClient manual into Chinese and contribute it to HttpClient project freely. In the future, the Chinese translation of HttpCore manual will be finished and contribute to HttpCore project freely too. "
0,"Add/change warning comments in the javadocs of Payload APIsSince the payload API is still experimental we should change the comments
in the javadocs similar to the new search/function package."
1,"IOExeception can cause loss of data due to premature segment deletionIf you hit an IOException, e.g., disk full, while making a cfs from its constituent parts, you may not be able to rollback to the before-merge process. This happens via addIndexes.

I don't have a nice easy test for this; generating IOEs ain't so easy. But it does happen in the patch for the factored merge policy with the existing tests because the pseudo-randomly generated IOEs fall in a different place."
0,Enable passing a config into PKIndexSplitterI need to be able to pass the IndexWriterConfig into the IW used by PKIndexSplitter.
1,"NoSuchItemStateException if Node.checkin() is invoked within a transactionWhen you run a code that takes versionning outside transactions - everything goes ok. But when you run it inside transaction, it fails:
Here's the stacktrace:

15:41:14,434 ERROR (TransactionalItemStateManager.java:114) -
java.lang.Exception: Cannot commit transaction.
[...]
Caused by: org.apache.jackrabbit.core.state.TransactionException: Unable
to commit transaction.:
31f78b39-6422-4ec8-b41e-2571b6807b05/{http://www.jcp.org/jcr/1.0}isCheckedOut
[...]
Caused by: org.apache.jackrabbit.core.state.NoSuchItemStateException:
31f78b39-6422-4ec8-b41e-2571b6807b05/{http://www.jcp.org/jcr/1.0}isCheckedOut
[...]

When you dont checkin the node transaction commits well, but the node is left checked out..."
0,"Make TrieRange completely independent from Document/Field with TokenStream of prefix encoded valuesTrieRange has currently the following problem:
- To add a field, that uses a trie encoding, you can manually add each term to the index or use a helper method from TrieUtils. The helper method has the problem, that it uses a fixed field configuration
- TrieUtils currently creates per default a helper field containing the lower precision terms to enable sorting (limitation of one term/document for sorting)
- trieCodeLong/Int() creates unnecessarily String[] and char[] arrays that is heavy for GC, if you index lot of numeric values. Also a lot of char[] to String copying is involved.

This issue should improve this:
- trieCodeLong/Int() returns a TokenStream. During encoding, all char[] arrays are reused by Token API, additional String[] arrays for the encoded result are not created, instead the TokenStream enumerates the trie values.
- Trie fields can be added to Documents during indexing using the standard API: new Field(name,TokenStream,...), so no extra util method needed. By using token filters, one could also add payload and so and customize everything.

The drawback is: Sorting would not work anymore. To enable sorting, a (sub-)issue can extend the FieldCache to stop iterating the terms, as soon as a lower precision one is enumerated by TermEnum. I will create a ""hack"" patch for TrieUtils-use only, that uses a non-checked Exceptionin the Parser to stop iteration. With LUCENE-831, a more generic API for this type can be used (custom parser/iterator implementation for FieldCache). I will attach the field cache patch (with the temporary solution, until FieldCache is reimplemented) as a separate patch file, or maybe open another issue for it."
0,"LogMergePolicy should use the number of deleted docs when deciding which segments to mergeI found that IndexWriter.optimize(int) method does not pick up large segments with a lot of deletes even when most of the docs are deleted. And the existence of such segments affected the query performance significantly.

I created an index with 1 million docs, then went over all docs and updated a few thousand at a time.  I ran optimize(20) occasionally. What saw were large segments with most of docs deleted. Although these segments did not have valid docs they remained in the directory for a very long time until more segments with comparable or bigger sizes were created.

This is because LogMergePolicy.findMergeForOptimize uses the size of segments but does not take the number of deleted documents into consideration when it decides which segments to merge. So, a simple fix is to use the delete count to calibrate the segment size. I can create a patch for this.

"
0,"Avoid String.intern() for UUID termsCreating Lucene terms is somewhat expensive, because it will usually call String.intern() on the field String. Jackrabbit uses UUID terms quite heavily to resolve hierarchy constraints. Lucene also provides a factory method on a Term that will create a new term instance with a given value and the same field name, avoiding the String.intern(). Jackrabbit should use the factory method whenever it creates a term for a UUID field."
0,"Evil up MockDirectoryWrapper.checkIndexOnCloseMockDirectoryWrapper checks any indexes tests create on close(), if they exist.

The problem is the logic it uses to determine if an index exists could mask real bugs (e.g. segments file corrumption):
{code}
if (DirectoryReader.indexExists(this) {
  ...
  // evil stuff like crash()
  ...
  _TestUtil.checkIndex(this)
}
{code}

and for reference DirectoryReader.indexExists is:
{code}
try {
  new SegmentInfos().read(directory);
  return true;
} catch (IOException ioe) {
  return false;
}
{code}

So if there are segments file problems, we just silently do no checkIndex.
"
1,"DefaultHighlighter.java does not encode illegal XML charactersWhen merging excerpts (method protected String mergeFragments(...) in DefaultHighlighter.java), illegal XML characters are not encoded in all places."
0,"Reduce exposure of nightly build documentationFrom LUCENE-1157  -

 ..the nightly build documentation is too prominent. A search for ""indexwriter api"" on Google or Yahoo! returns nightly documentation before released documentation.

(https://issues.apache.org/jira/browse/LUCENE-1157?focusedCommentId=12565820#action_12565820)
"
0,"Do MultiTermQuery boolean rewrites per segmentMultiTermQuery currently rewrites FuzzyQuery (using TopTermsBooleanQueryRewrite), the auto constant rewrite method and the ScoringBQ rewrite methods using a MultiFields wrapper on the top-level reader. This is inefficient.

This patch changes the rewrite modes to do the rewrites per segment and uses some additional datastructures (hashed sets/maps) to exclude duplicate terms. All tests currently pass, but FuzzyQuery's tests should not, because it depends for the minimum score handling, that the terms are collected in order..

Robert will fix FuzzyQuery in this issue, too. This patch is just a start."
1,"DefaultRedirectHandler not resolving relative location URI wrt the request URIThe adjustment of a relative URI in the Location header value does not take the request URI into account. So you may want to replace ...
------------------------------
try {
    uri = new URI(
            target.getSchemeName(),
            null,
            target.getHostName(),
            target.getPort(),
            uri.getPath(),
            uri.getQuery(),
            uri.getFragment());
------------------------------
... with ...
------------------------------
HttpRequest request = (HttpRequest) context.getAttribute(ExecutionContext.HTTP_REQUEST);
try {
    URI requestURI = new URI(request.getRequestLine().getUri());
    URI absoluteRequestURI = new URI(
            target.getSchemeName(),
            null,
            target.getHostName(),
            target.getPort(),
            requestURI.getPath(),
            requestURI.getQuery(),
            requestURI.getFragment());
    uri = absoluteRequestURI.resolve(uri);
------------------------------
... or get the request URI from somewhere else."
0,"Respect Keep-Alive HeaderHttpClient currently does not respect the 'Keep-Alive' header tokens (timeout, max, etc..) and continues to use the persistent connection beyond limits the server requests.  This leads to failure and falling back to HttpRequestRetryHandler, when it should instead just use a new connection explicitly."
0,"Unreachable catch block for NameException in ValueConstraint.javaUnreachable catch block for NameException. Only more specific exceptions are thrown and handled by previous catch block(s). ValueConstraint.java	line 855
"
0,"[REFACTORING] FieldSortedHitQueue has too much duplicated codeThere's 40LOC duplicated in FieldDocSortedHitQueue::lessThan just to handle 
the reverse sort. It would be more readable to actually do something like 
(YMMV):

if (field.getReverse()) {
    c = -c;
}"
0,Improve javadocs for Numeric*I'm working on improving Numeric* javadocs.
0,"Weight.scorer() not passed doc offset for ""sub reader""Now that searching is done on a per segment basis, there is no way for a Scorer to know the ""actual"" doc id for the document's it matches (only the relative doc offset into the segment)

If using caches in your scorer that are based on the ""entire"" index (all segments), there is now no way to index into them properly from inside a Scorer because the scorer is not passed the needed offset to calculate the ""real"" docid

suggest having Weight.scorer() method also take a integer for the doc offset

Abstract Weight class should have a constructor that takes this offset as well as a method to get the offset
All Weights that have ""sub"" weights must pass this offset down to created ""sub"" weights


Details on workaround:
In order to work around this, you must do the following:
* Subclass IndexSearcher
* Add ""int getIndexReaderBase(IndexReader)"" method to your subclass
* during Weight creation, the Weight must hold onto a reference to the passed in Searcher (casted to your sub class)
* during Scorer creation, the Scorer must be passed the result of YourSearcher.getIndexReaderBase(reader)
* Scorer can now rebase any collected docids using this offset

Example implementation of getIndexReaderBase():
{code}
// NOTE: more efficient implementation can be done if you cache the result if gatherSubReaders in your constructor
public int getIndexReaderBase(IndexReader reader) {
  if (reader == getReader()) {
    return 0;
  } else {
    List readers = new ArrayList();
    gatherSubReaders(readers);
    Iterator iter = readers.iterator();
    int maxDoc = 0;
    while (iter.hasNext()) {
      IndexReader r = (IndexReader)iter.next();
      if (r == reader) {
        return maxDoc;
      } 
      maxDoc += r.maxDoc();
    } 
  }
  return -1; // reader not in searcher
}
{code}

Notes:
* This workaround makes it so you cannot serialize your custom Weight implementation
"
1,"System properties does not get replaced in a Cluster configurationSince JCR-1304 has been added to jackrabbit 1.4 I guess this should be reported as a bug...

Still not debugged deeply, but if I try to configure a Cluster using:
<Cluster id=""${server}"" syncDelay=""10"">

after setting a ""server"" system property I expect to have the cluster initialized properly using the value of such property... I just realized that my cluster node gets initialized with the final value of ""${server}"" instead :(

Cluster config is a very good place where to use system properties, since all the configuration is usually identical between cluster nodes while the ""id"" property must be different...

Is there anything I missed/did wrong in my configuration?
"
0,"javacc skeleton files not regeneratedCopies of the the character stream files for javacc are checked into svn. These files were generated under javacc 3.0 (at least that's what they say, though javacc 3.2 says this too). javacc 4 complains that they are out of date but won't replace them; they must be removed before it will regenerate them.

There is one side effect of removing them: local changes are lost.  r387550 removed a couple of deprecated methods. By using the files as generated by javacc, these deprecated  methods will be readded (at least until the javacc team removes them totally). There are other changes being made to the stream files, so I woudl think it's better to live with them unmodified than to keep local versions just for this change.

If we want javacc to recreate the files, the attached patch will remove them before running javacc.

All the tests pass using both javacc3.2 and 4.0.


"
1,"QueryParser escaping/parsin issue with strings starting/ending with ||There is a problem with query parser when search string starts/ends with ||.  When string contains || in the middle like 'something || something' everything runs without a problem.

Part of code: 
  searchText = QueryParser.escape(searchText);
  QueryParser parser = null;
  parser = new QueryParser(fieldName, new CustomAnalyser());
  parser.parse(searchText);

CustomAnalyser class extends Analyser. Here is the only redefined method: 

    @Override
    public TokenStream tokenStream(String fieldName, Reader reader) {
      return new PorterStemFilter( (new StopAnalyzer()).tokenStream(fieldName, reader));
    }

I have tested this on Lucene 2.1 and latest source I have checked-out from SVN (Revision 538867) and in both cases parsing exception was thrown.

Part of Stack Trace (Lucene - SVN checkout - Revision 538867):
Cannot parse 'someting ||': Encountered ""<EOF>"" at line 1, column 11.
Was expecting one of:
    <NOT> ...
    ""+"" ...
    ""-"" ...
    ""("" ...
    ""*"" ...
    <QUOTED> ...
    <TERM> ...
    <PREFIXTERM> ...
    <WILDTERM> ...
    ""["" ...
    ""{"" ...
    <NUMBER> ...
    
 org.apache.lucene.queryParser.ParseException: Cannot parse 'someting ||': Encountered ""<EOF>"" at line 1, column 11.
Was expecting one of:
    <NOT> ...
    ""+"" ...
    ""-"" ...
    ""("" ...
    ""*"" ...
    <QUOTED> ...
    <TERM> ...
    <PREFIXTERM> ...
    <WILDTERM> ...
    ""["" ...
    ""{"" ...
    <NUMBER> ...
    
        at org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:150)


Part of Stack Trace (Lucene 2.1):
Cannot parse 'something ||': Encountered ""<EOF>"" at line 1, column 12.
Was expecting one of:
    <NOT> ...
    ""+"" ...
    ""-"" ...
    ""("" ...
    ""*"" ...
    <QUOTED> ...
    <TERM> ...
    <PREFIXTERM> ...
    <WILDTERM> ...
    ""["" ...
    ""{"" ...
    <NUMBER> ...
    
 org.apache.lucene.queryParser.ParseException: Cannot parse 'something ||': Encountered ""<EOF>"" at line 1, column 12.
Was expecting one of:
    <NOT> ...
    ""+"" ...
    ""-"" ...
    ""("" ...
    ""*"" ...
    <QUOTED> ...
    <TERM> ...
    <PREFIXTERM> ...
    <WILDTERM> ...
    ""["" ...
    ""{"" ...
    <NUMBER> ...
    
        at org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:149)


"
0,"CachingHttpClient should have similar behavior as AbstractHttpClient when executing with ResponseHandlerWhen calling execute on the AbstractHttpClient with a  ResponseHandler, the AbstractHttpClient will attempt to Consume the Entity and close any open connections before returning. This behavior is not currently in the CachingHttpClient. 

This can lead to connection leaks when switching to CachingHttpClient, becuase the responsibility to fully consume the entity is now on the ResponseHandler instead on the HttpClient.

Here is the code that does the existing 'auto-close' behavior: ""org.apache.http.impl.client.AbstractHttpClient.java"" lines 1080-1111"
0,"More utility methods in JcrUtilsI'd like to add at least the following utility methods to JcrUtils:

For logging:

    // Utility method to simplify log messages and debug prints:
    // Node -> ""name [type]""
    // Property -> ""@name = value(s)""
    String toString(Item item)

For making sure that a node exists:

    // Returns the identified child node. If the child does not already exist,
    // it is added using the default node type from the parent.
    Node setNode(Node parent, String name)

    // Same as above, but ensures that isNodeType(type) is true for the
    // returned node, using addNode(name, type) or setPrimaryType(type)
    // if needed.
    Node setNode(Node parent, String name, String type)

For adding (or setting, see above) nt:folder nodes:

    // Adds a new nt:folder node with the given name
    Node addFolder(Node parent, String name)

    // Ensures that an nt:folder node with the given name exists
    Node setFolder(Node parent, String name)

For adding (or setting) nt:file nodes:

    // Adds a new nt:file/nt:resource structure
    // If the mime type contains a charset parameter, then the jcr:encoding property is also set
    Node addFile(Node parent, String name, String mime, InputStream data)
    Node addFile(Node parent, String name, String mime, Calendar date, InputStream data)

    // Ensures that an nt:file/nt:resource structure exists with the given data.
    // Note that the type of a potential existing jcr:content node is not modified
    Node setFile(Node parent, String name, String mime, InputStream data)
    Node setFile(Node parent, String name, String mime, Calendar date, InputStream data)
"
0,"Proxies improvement1) Improvement :
I need to be able to detect when a bean is an OCM proxy and if it has already been loaded. This kind of functionnality is for example on Hibernate with Hibernate.isInitialized(Object proxy).
I have developped something similar : I have modified ProxyManagerImpl so it uses an InvocationHandler instead of a LazyLoader. This way, I make my proxies implement a special interface whose methods are intercepted.

2) Bug :
If a BeanConverter is specified, ObjectConverterImpl should pass it to the proxy CallBack instead letting BeanLazyLoader use the default ObjectConverter. I think this is a bug, as the behavior is different is bean property is proxified or not.

3) Improvement :
If a jcrType mapped on a java type is specified, ObjectConverterImpl should make a proxy of this type, and not use the the bean property type. This is particularly useful when the bean property type is an interface.

Sorry for reporting this as a bundle instead of seperate items, but I developped my patch as a whole.  
Let me know if you need help on the enclosing patch.

Sincerely,

Stphane Landelle"
1,"HttpState#matchCredentials is brokenCredentials matching algorithm is flawed, generates unnecessary garbage by
instantiating intermediate object during lookup"
0,"[PATCH] fix compile errors in sandboxHere's a patch that fixes the compile problems in sandbox/analyzers starting 
shortly before the 1.4 release. The deprecation warnings are also fixed. I 
have not tested the changes (I don't use those analyzers) but the changes 
should be trivial enough so they don't break anything. 
 
Could someone apply the patch and also fix FrenchAnalyzer? It's the same 
change as for the other files, but I didn't manage to make a clean diff 
because of encoding problems."
0,"Add support for large number of users in a groupIn the current implementation there are several factors which limit the number of users in a group:

- group membership is recorded in a multi valued property which does not scale well
- members of groups are collected eagerly which does not scale well

I propose to add complementary support for recording group membership in a node structure to the current solution. That node structure would - similar to users and groups - add intermediate nodes when a group reaches a certain threshold on the number of its users."
0,"Code coverage reportsHi all,

We should be able to measure the code coverage of our unit testcases. I believe it would be very helpful for the committers, if they could verify before committing a patch if it does not reduce the coverage. 

Furthermore people could take a look in the code coverage reports to figure out where work needs to be done, i. e. where additional testcases are neccessary. It would be nice if we could add a page to the Lucene website showing the report, generated by the nightly build. Maybe you could add that to your preview page (LUCENE-707), Grant?

I attach a patch here that uses the tool EMMA to generate the code coverage reports. EMMA is a very nice open-source tool released under the CPL (same license as junit). The patch adds three targets to common-build.xml: 
- emma-check: verifys if both emma.jar and emma_ant.jar are in the ant classpath 
- emma-instrument: instruments the compiled code 
- generate-emma-report: generates an html code coverage report 

The following steps are neccessary in order to generate a code coverage report:
- add emma.jar and emma_ant.jar to your ant classpath (download emma from http://emma.sourceforge.net/)
- execute ant target 'emma-instrument' (depends on compile-test, so it will compile all core and test classes)
- execute ant target 'test' to run the unit tests
- execute ant target 'generate-emma-report'

To view the emma report open build/test/emma/index.html"
1,"AbstractClientConnAdapter#abortConnection() does not release the connection if called from the main execution thread while there is no blocking I/O operation #abortConnection() is usually expected to be  called from a helper thread in order to unblock the main execution thread blocked in an I/O operation. It may be unsafe to call #releaseConnection() from the helper thread, so we have to rely on an IOException thrown by the closed socket on the main thread to trigger the release of the connection back to the connection manager. However, if this method is called from the main execution thread it should be safe to release the connection immediately. Besides, this also helps ensure the connection gets released back to the manager if #abortConnection() is called from the main execution thread while there is no blocking I/O operation."
0,[PATCH] more verbose exception messages (BatchedItemOperations)added context to exception messages in BatchedItemOperations to aid debugging
1,"Connection.setAutoCommit(...) fails if connection is managed for JNDIDatabasePersistenceManagerInvoking setAutoCommit() on a db connection fails if the connection is managed.

I propose as a workaround to check if the auto commit must be set previous to setting it (a trivial patch will be provided).

This can happen eg. if you use JNDI (eg JNDIDatabasePersistenceManager) to fetch the connection on JBoss, and the persistent manager tries to reconnect (see stack trace below).

05 Jul 09:54:24 ERROR sePersistenceManager| failed to re-establish connection
java.sql.SQLException: You cannot set autocommit during a managed transaction!
        at org.jboss.resource.adapter.jdbc.BaseWrapperManagedConnection.setJdbcAutoCommit(BaseWrapperManagedConnection.java:482)
        at org.jboss.resource.adapter.jdbc.WrappedConnection.setAutoCommit(WrappedConnection.java:322)
        at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.initConnection(DatabasePersistenceManager.java:731)
        at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.reestablishConnection(DatabasePersistenceManager.java:806)
        at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.executeStmt(DatabasePersistenceManager.java:852)
        at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.exists(DatabasePersistenceManager.java:647)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.hasNonVirtualItemState(SharedItemStateManager.java:1102)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.hasItemState(SharedItemStateManager.java:289)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.hasItemState(LocalItemStateManager.java:180)
        at org.apache.jackrabbit.core.state.XAItemStateManager.hasItemState(XAItemStateManager.java:252)
        at org.apache.jackrabbit.core.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:174)"
0,"""JCR levels"" link on http://jackrabbit.apache.org/doc/index.html brokenThe ""JCR levels"" link on the Jackrabbit home page is broken.
"
0,"[PATCH] to store binary fields with compressionhi all,

as promised here is the enhancement for the binary field patch with optional
compression. The attachment includes all necessary diffs based on the latest
version from CVS. There is also a small junit test case to test the core
functionality for binary field compression. The base implementation for binary
fields where this patch relies on, can be found in patch #29370. The existing
unit tests pass fine.

For testing binary fields and compression, I'm creating an index from 2700 plain
text files (avg. 6kb per file) and store all file content within that index
without using compression. The test was created using the IndexFiles class from
the demo distribution. Setting up the index and storing all content without
compression took about 60 secs and the final index size was 21 MB. Running the
same test, switching compression on, the time to index increase to 75 secs, but
the final index size shrinks to 13 MB. This is less than the plain text files
them self need in the file system (15 MB)

Hopefully this patch helps people dealing with huge index and want to store more
than just 300 bytes per document to display a well formed summary.

regards
Bernhard"
1,"TestStressIndexing2 testMultiConfig failuretrunk: r1134311

reproducible

{code}
    [junit] Testsuite: org.apache.lucene.index.TestStressIndexing2
    [junit] Tests run: 1, Failures: 2, Errors: 0, Time elapsed: 0.882 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] java.lang.AssertionError: ram was 460908 expected: 408216 flush mem: 395100 active: 65808
    [junit]     at org.apache.lucene.index.DocumentsWriterFlushControl.assertMemory(DocumentsWriterFlushControl.java:102)
    [junit]     at org.apache.lucene.index.DocumentsWriterFlushControl.doAfterDocument(DocumentsWriterFlushControl.java:164)
    [junit]     at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:380)
    [junit]     at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1473)
    [junit]     at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1445)
    [junit]     at org.apache.lucene.index.TestStressIndexing2$IndexingThread.indexDoc(TestStressIndexing2.java:723)
    [junit]     at org.apache.lucene.index.TestStressIndexing2$IndexingThread.run(TestStressIndexing2.java:757)
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestStressIndexing2 -Dtestmethod=testMultiConfig -Dtests.seed=2571834029692482827:-8116419692655152763
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestStressIndexing2 -Dtestmethod=testMultiConfig -Dtests.seed=2571834029692482827:-8116419692655152763
    [junit] The following exceptions were thrown by threads:
    [junit] *** Thread: Thread-0 ***
    [junit] junit.framework.AssertionFailedError: java.lang.AssertionError: ram was 460908 expected: 408216 flush mem: 395100 active: 65808
    [junit]     at junit.framework.Assert.fail(Assert.java:47)
    [junit]     at org.apache.lucene.index.TestStressIndexing2$IndexingThread.run(TestStressIndexing2.java:762)
    [junit] NOTE: test params are: codec=RandomCodecProvider: {f33=Standard, f57=MockFixedIntBlock(blockSize=649), f11=Standard, f41=MockRandom, f40=Standard, f62=MockRandom, f75=Standard, f73=MockSep, f29=MockFixedIntBlock(blockSize=649), f83=MockRandom, f66=MockSep, f49=MockVariableIntBlock(baseBlockSize=9), f72=Pulsing(freqCutoff=7), f54=Standard, id=MockFixedIntBlock(blockSize=649), f80=MockRandom, f94=MockSep, f93=Pulsing(freqCutoff=7), f95=Standard}, locale=en_SG, timezone=Pacific/Palau
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestStressIndexing2]
    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=133324528,total=158400512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testMultiConfig(org.apache.lucene.index.TestStressIndexing2):     FAILED
    [junit] r1.numDocs()=17 vs r2.numDocs()=16
    [junit] junit.framework.AssertionFailedError: r1.numDocs()=17 vs r2.numDocs()=16
    [junit]     at org.apache.lucene.index.TestStressIndexing2.verifyEquals(TestStressIndexing2.java:308)
    [junit]     at org.apache.lucene.index.TestStressIndexing2.verifyEquals(TestStressIndexing2.java:278)
    [junit]     at org.apache.lucene.index.TestStressIndexing2.testMultiConfig(TestStressIndexing2.java:124)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1403)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1321)
    [junit] 
    [junit] 
    [junit] Testcase: testMultiConfig(org.apache.lucene.index.TestStressIndexing2):     FAILED
    [junit] Some threads threw uncaught exceptions!
    [junit] junit.framework.AssertionFailedError: Some threads threw uncaught exceptions!
    [junit]     at org.apache.lucene.util.LuceneTestCase.tearDown(LuceneTestCase.java:603)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1403)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1321)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.index.TestStressIndexing2 FAILED
{code}"
0,"Add NoOpMergePolicyI'd like to add a simple and useful MP implementation which does .... nothing ! :). I've came across many places where either the following is documented or implemented: ""if you want to prevent merges, set mergeFactor to a high enough value"". I think a NoOpMergePolicy is just as good, and can REALLY allow you disable merges (except for maybe set mergeFactor to Int.MAX_VAL).

As such, NoOpMergePolicy will be introduced as a singleton, and can be used for convenience purposes only. Also, for Parallel Index it's important, because I'd like the slices to never do any merges, unless ParallelWriter decides so. So they should be set w/ that MP.

I have a patch ready. Waiting for LUCENE-2320 to go in, so that I don't need to change it afterwards.

About the name - I like the name, but suggestions are welcome. I thought of a NullMergePolicy, but I don't like 'Null' used for a NoOp."
0,"Build environment configuration: Mavenize the build process- ease of building. HttpClient should be buildable without the user needing to
go away and download extra jars. Maven does a good job of this.
- automated site and build on a nightly basis to pick up changes
- move to j2sdk1.4"
0,"Build SegmentCodecs incrementally for consistent codecIDs during indexingcurrently we build the SegementCodecs during flush which is fine as long as no codec needs to know which fields it should handle. This will change with DocValues or when we expose StoredFields / TermVectors via Codec (see LUCENE-2621 or LUCENE-2935). The other downside it that we don't have a consistent view of which codec belongs to which field during indexing and all FieldInfo instances are unassigned (set to -1). Instead we should build the SegmentCodecs incrementally as fields come in so no matter when a codec needs to be selected to process a document / field we have the right codec ID assigned.

"
0,"Remove timeout handling from TransactionContextAs discussed in JCR-2861, the transaction timeout handling in the TransactionContext class should not be needed since that's the task of the transaction manager, not the context. We should simply remove the timeout handling."
1,small SentinelIntSet can cause infinite loop on resizeA small initial size of <=4 can cause the set to not rehash soon enough and thus go into an infinite loop searching the table for an open space.
0,Remove dependency on XercesClassloaders in certain J2EE servers do not play well with the Xerces requirement
0,"Bring Hunspell for Lucene into analysis moduleSome time ago I along with Robert and Uwe, wrote an Stemmer which uses the Hunspell algorithm.  It has the benefit of supporting dictionaries for a wide array of languages.   

It seems to still be being used but has fallen out of date.  I think it would benefit from being inside the analysis module where additional features such as decompounding support, could be added."
1,PrivilegeDefinition should implement equals and hashcode
0,"Provide utility for handling large number of child nodes/propertiesJackrabbit does not cope well with 'flat' hierarchies. That is with hierarchies where a node has many child nodes and/or properties. The current recommendation for such situations is to manually add intermediate nodes. 

It would be nice to have an utility which adds/removes intermediate nodes as needed and expose a 'flat' view to users. Such an utility should:

- expose a large number of nodes/properties as sequence
- parametrize the order of how nodes/properties appear in the sequence
- provide methods to lookup/add/remove nodes/properties by key 
- organize the node/properties in the underlying JCR hierarchy in a way which is both efficient for above operations and easily understandable to users looking at the hierarchy. "
0,"Implement caching mechanism for ItemInfo batchesCurrently all ItemInfos returned by RepositoryService#getItemInfos are placed into the hierarchy right away. For big batch sizes this is prohibitively expensive. The overhead is so great (*), that it quickly outweighs the overhead of network round trips. Moreover, SPI implementations usually choose the batch in a way determined by the backing persistence store and not by the requirements of the consuming application on the JCR side. That is, many of the items in the batch might never be actually needed. 

I suggest to implement a cache for ItemInfo batches. Conceptually such a cache would live inside jcr2spi right above the SPI API. The actual implementation would be provided by SPI implementations. This approach allows for fine tuning cache/batch sizes to a given persistence store and network environment. This would also better separate different concerns: the purpose of the existing item cache is to optimize for the requirement of the consumer of the JCR API ('the application'). The new ItemInfo cache is to optimize for the specific network environment and backing persistence store. 

(*) Numbers follow "
0,augment logging information around CachingEntryCollectoradd more logging information for the purpose of debugging CachingEntryCollector bottlenecks
0,"Consolidate all (Solr's & Lucene's) analyzers into modules/analysisWe've been wanting to do this for quite some time now...  I think, now that Solr/Lucene are merged, and we're looking at opening an unstable line of development for Solr/Lucene, now is the right time to do it.

A standalone module for all analyzers also empowers apps to separately version the analyzers from which version of Solr/Lucene they use, possibly enabling us to remove Version entirely from the analyzers.

We should also do LUCENE-2309 (decouple, as much as possible, indexer from the analysis API), but I don't think that issue needs to block this consolidation.

Once we do this, there is one place where our users can find all the analyzers that Solr/Lucene provide."
0,"improve windows defaults in FSDirectoryCurrently windows defaults to SimpleFSDirectory, but this is a problem due to the synchronization.

I have been benchmarking queries *sequentially* and was pretty surprised at how much faster
MMapDirectory is, for example for cases that do many seeks.

I think we should change the defaults for windows as such:

if (WINDOWS and UNMAP_SUPPORTED and 64-bit)
  use MMapDirectory
else
  use SimpleFSDirectory 

I think we should just consider doing this for 4.0 only and see how it goes.
"
0,"Remove WARN logs for missing text extractorsIn jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/lucene/JackrabbitTextExtractor.java, the method extractText logs at WARN level when no indexer is available.

Can we move this to DEBUG, as not all applications need full text indexing of the text nodes and this message fills the logs?

"
0,"website: 404 for several documentation pagesThere are several 404 Not Found pages linked from hc.apache.org.
Please fix them as it's important documentation for HttpClient.

Specifically:

linked from http://hc.apache.org/user-docs.html

not found:
http://hc.apache.org/httpcomponents-client/primer.html
http://hc.apache.org/httpcomponents-client-4.0.1/httpclient/apidocs/index.html
http://hc.apache.org/httpcomponents-client-4.0.1/httpmime/apidocs/index.html"
0,"[PATCH] Performance improvement to DisjunctionSumScorerA recent profile of the new BooleanScorer2 showed that 
quite a bit of CPU time is spent in the advanceAfterCurrent method 
of DisjunctionScorer, and in the PriorityQueue of scorers that 
is used there. 
 
This patch reduces the internal overhead of DisjunctionScorer 
to about 70% of the current one (ie. 30% saving in cpu time). 
It also reduces the number of calls to the subscorers, but 
that was not measured. 
 
To get this, it was necessary to specialize the PriorityQueue 
for a Scorer and to add move some code fragments from DisjunctionScorer 
to this specialized queue."
0,"Enable bzip compression in benchmarkbzip compression can aid the benchmark package by not requiring extracting bzip files (such as enwiki) in order to index them. The plan is to add a config parameter bzip.compression=true/false and in the relevant tasks either decompress the input file or compress the output file using the bzip streams.
It will add a dependency on ant.jar which contains two classes similar to GZIPOutputStream and GZIPInputStream which compress/decompress files using the bzip algorithm.

bzip is known to be superior in its compression performance to the gzip algorithm (~20% better compression), although it does the compression/decompression a bit slower.

I wil post a patch which adds this parameter and implement it in LineDocMaker, EnwikiDocMaker and WriteLineDoc task. Maybe even add the capability to DocMaker or some of the super classes, so it can be inherited by all sub-classes."
0,Similarity javadocs for scoring function to relate more tightly to scoring models in effectSee discussion in the related issue.
0,"Update site lucene-sandbox pageThe page has misleading/bad info. One thing I would like to do - but I won't attempt now (prob good for the modules issue) - is commit to one word - contrib or sandbox. I think sandbox should be purged myself.

The current page says that the sandbox is kind of a rats nest with various early stage software that one day may make it into core - that info is outdated I think. We should replace it, and also specify how the back compat policy works in contrib eg each contrib can have its own policy, with the default being no policy.

We should also drop the piece about being open to Lucene's committers and others - a bit outdated.

We should also either include the other contribs, or change the wording to indicate that the list is only a sampling of the many contribs."
0,"HttpRoutePlanner based on ProxySelectorSince we now require Java 5, we should have a route planner that uses the standard Java ProxySelector. That would allow us to automatically pick up proxy settings from system properties or the browser running an applet.
"
0,"repositoryConfig should use setter for its internal componentsFrom the mailing list (not archived at the moment):
--- Jukka's reply ---
I refactored the config classes last year but didn't change the way
the config instances are being used by Jackrabbit. In general I think
that a IoC approach (use setters to configure the Jackrabbit
components) would be better than passing config objects around and
letting the components to instantiate any subcomponents based on the
configuration. This is why I didn't really want to make the config
constructors public, otherwise we'd easily up with backwards
compatibility issues if we were to change the way configuration is
handled.
---

"
0,"unexpected session is used  in XATest.testAddNodeCommit()In org.apache.jackrabbit.core.XATest.java:137

        // assertion: node exists in this session
        try {
            otherSuperuser.getNodeByUUID(n.getUUID());
        } catch (ItemNotFoundException e) {
            fail(""Committed node not visible in this session"");
        }

        // assertion: node also exists in other session
        try {
            otherSuperuser.getNodeByUUID(n.getUUID());
        } catch (ItemNotFoundException e) {
            fail(""Committed node not visible in other session"");
        }

The session instance of 'otherSuperuser' is used two times. In the first case, I think that it is not 'otherSuperuser' but 'superuser'. 
"
0,ID Field Descriptor is not inherited as is the case with UUID Field DescriptorID Field descriptor when defined in the base class in jcr-mapping is not inherited. The child class also has to define it again. A patch for the same is attached herewith. Patch is on similar lines of UUID Field Descriptor
0,"Add m2e lifecycle mappings for Eclipse IndigoWhen importing Jackrabbit to the latest Eclipse release (Indigo) that comes with m2e version 1.0, many of the POMs are flagged red because m2e doesn't know what to do with the custom plugin mappings we use in many components.

This is a pretty contentious issue for m2e (see for example https://bugs.eclipse.org/bugs/show_bug.cgi?id=350414), and ideally it should just work without any custom workarounds on our part.

However, as a workaround until the core issue is solved, the best solution is to explicitly tell m2e what to do with these plugin mappings. The extra org.eclipse.m2e:lifecycle-mapping configuration is only active when used within Eclipse, so it doesn't affect
normal builds."
1,"CLONE -Merge error during add to index (IndexOutOfBoundsException)I've been batch-building indexes, and I've build a couple hundred indexes with 
a total of around 150 million records.  This only happened once, so it's 
probably impossible to reproduce, but anyway... I was building an index with 
around 9.6 million records, and towards the end I got this:

java.lang.IndexOutOfBoundsException: Index: 54, Size: 24
        at java.util.ArrayList.RangeCheck(ArrayList.java:547)
        at java.util.ArrayList.get(ArrayList.java:322)
        at org.apache.lucene.index.FieldInfos.fieldInfo(FieldInfos.java:155)
        at org.apache.lucene.index.FieldInfos.fieldName(FieldInfos.java:151)
        at org.apache.lucene.index.SegmentTermEnum.readTerm(SegmentTermEnum.java
:149)
        at org.apache.lucene.index.SegmentTermEnum.next
(SegmentTermEnum.java:115)
        at org.apache.lucene.index.SegmentMergeInfo.next
(SegmentMergeInfo.java:52)
        at org.apache.lucene.index.SegmentMerger.mergeTermInfos
(SegmentMerger.java:294)
        at org.apache.lucene.index.SegmentMerger.mergeTerms
(SegmentMerger.java:254)
        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:93)
        at org.apache.lucene.index.IndexWriter.mergeSegments
(IndexWriter.java:487)
        at org.apache.lucene.index.IndexWriter.maybeMergeSegments
(IndexWriter.java:458)
        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:310)
        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:294)"
1,"IndexMerger: Synchronization issue on repository shutdownAfter inserting a large number of nodes (~200000) into a repository and then closing the session, I get the following exception:

19:42:40.556 [jackrabbit-pool-5] DEBUG o.a.j.core.query.lucene.IndexMerger - # of busy merge workers: 2
19:42:40.556 [jackrabbit-pool-3] DEBUG o.a.j.core.query.lucene.IndexMerger - accepted merge request
19:42:40.556 [jackrabbit-pool-5] DEBUG o.a.j.core.query.lucene.IndexMerger - Worker finished
19:42:40.556 [jackrabbit-pool-3] DEBUG o.a.j.core.query.lucene.IndexMerger - create new index
19:42:40.557 [jackrabbit-pool-3] DEBUG o.a.j.core.query.lucene.IndexMerger - get index readers from MultiIndex
19:42:40.640 [main] INFO  c.a.kmp.generator.JpaToJcrImporter - end JCR save
19:42:40.849 [main] INFO  o.a.j.core.TransientRepository - Session closed
19:42:40.849 [main] INFO  o.a.jackrabbit.core.RepositoryImpl - Shutting down repository...
19:42:40.849 [main] DEBUG o.a.j.core.query.lucene.IndexMerger - dispose IndexMerger
19:42:40.849 [main] DEBUG o.a.j.core.query.lucene.IndexMerger - quit flag set
19:42:40.849 [main] INFO  o.a.j.core.query.lucene.SearchIndex - Index closed: repository/repository/index
19:42:40.850 [main] INFO  o.a.jackrabbit.core.RepositoryImpl - shutting down workspace 'default'...
19:42:40.850 [main] INFO  o.a.j.c.o.ObservationDispatcher - Notification of EventListeners stopped.
19:42:40.850 [main] DEBUG o.a.j.core.query.lucene.IndexMerger - dispose IndexMerger
19:42:40.850 [main] DEBUG o.a.j.core.query.lucene.IndexMerger - quit flag set
19:42:40.850 [main] DEBUG o.a.j.core.query.lucene.IndexMerger - IndexMerger.Worker thread stopped
19:42:40.855 [main] DEBUG o.a.j.core.query.lucene.IndexMerger - index added: name=_6h, numDocs=890
19:42:41.367 [jackrabbit-pool-3] DEBUG o.a.j.core.query.lucene.IndexMerger - deleting index _6g
19:42:41.393 [main] INFO  o.a.j.core.query.lucene.SearchIndex - Index closed: repository/workspaces/default/index
19:42:41.410 [jackrabbit-pool-3] ERROR o.a.j.core.query.lucene.IndexMerger - Error while merging indexes: 
org.apache.lucene.store.AlreadyClosedException: this IndexWriter is closed
	at org.apache.lucene.index.IndexWriter.ensureOpen(IndexWriter.java:412) ~[lucene-core-2.4.1.jar:2.4.1 750176 - 2009-03-04 21:56:52]
	at org.apache.lucene.index.IndexWriter.ensureOpen(IndexWriter.java:417) ~[lucene-core-2.4.1.jar:2.4.1 750176 - 2009-03-04 21:56:52]
	at org.apache.lucene.index.IndexWriter.startTransaction(IndexWriter.java:2511) ~[lucene-core-2.4.1.jar:2.4.1 750176 - 2009-03-04 21:56:52]
	at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:3273) ~[lucene-core-2.4.1.jar:2.4.1 750176 - 2009-03-04 21:56:52]
	at org.apache.jackrabbit.core.query.lucene.PersistentIndex.addIndexes(PersistentIndex.java:114) ~[jackrabbit-core-2.1.1.jar:2.1.1]
	at org.apache.jackrabbit.core.query.lucene.IndexMerger$Worker.run(IndexMerger.java:525) ~[jackrabbit-core-2.1.1.jar:2.1.1]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441) [na:1.6.0_20]
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) [na:1.6.0_20]
	at java.util.concurrent.FutureTask.run(FutureTask.java:138) [na:1.6.0_20]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98) [na:1.6.0_20]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:207) [na:1.6.0_20]
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) [na:1.6.0_20]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) [na:1.6.0_20]
	at java.lang.Thread.run(Thread.java:619) [na:1.6.0_20]
19:42:41.420 [jackrabbit-pool-3] DEBUG o.a.j.core.query.lucene.IndexMerger - Worker finished
19:42:41.839 [main] INFO  o.a.j.c.p.b.DerbyPersistenceManager - Database 'repository/workspaces/default/db' shutdown.


The problem is reproducible. Apparently, the Lucene index is closed before all IndexMerger worker threads are terminated. The root cause seems to be the AtomicBoolean IndexMerger.Worker.terminated which is always true. The enclosed patch solves the problem in my use case.

"
1,"java.lang.IllegalStateException: Connection already open.I am seeing many of the same problems noted in HTTPCLIENT-741 using the latest builds from the maven repo.

java.lang.IllegalStateException: Connection already open.
        at org.apache.http.impl.conn.AbstractPoolEntry.open(AbstractPoolEntry.java:150)
        at org.apache.http.impl.conn.AbstractPooledConnAdapter.open(AbstractPooledConnAdapter.java:119)
        at org.apache.http.impl.client.DefaultClientRequestDirector.execute(DefaultClientRequestDirector.java:308)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:501)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:456)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:422)
        at com.hi5.os.Hi5RemoteContentFetcher.fetch(Hi5RemoteContentFetcher.java:279)
"
1,"ParallelTermEnum is BROKENParallelTermEnum.next() fails to advance properly to new fields.  This is a serious bug. 

Christian Kohlschuetter diagnosed this as the root problem underlying LUCENE-398 and posted a first patch there.

I've addressed a couple issues in the patch (close skipped field TermEnum's, generate field iterator only once, integrated Christian's test case as a Lucene test) and packaged in all the revised patch here.

All Lucene tests pass, and I've further tested in this in my app, which makes extensive use of ParallelReader.
"
0,"Make DocsEnum subclass of DocIdSetIteratorSpinoff from LUCENE-1458:

One thing I came along long time ago, but now with a new API it get's interesting again: 
DocsEnum should extend DocIdSetIterator, that would make it simplier to use and implement e.g. in MatchAllDocQuery.Scorer, FieldCacheRangeFilter and so on. You could e.g. write a filter for all documents that simply returns the docs enumeration from IndexReader.

So it should be an abstract class that extends DocIdSetIterator. It has the same methods, only some methods must be a little bit renamed. The problem is, because java does not support multiple inheritace, we cannot also extends attributesource  Would DocIdSetIterator be an interface it would work (this is one of the cases where interfaces for really simple patterns can be used, like iterators).

The problem with multiple inheritance could be solved by an additional method attributes() that creates a new AttributeSource on first access then (because constructing an AttributeSource is costly).  The same applies for the other *Enums, it should be separated for lazy init.

DocsEnum could look like this:

{code}
public abstract class DocsEnum extends DocIdSetIterator {
  private AttributeSource atts = null;
  public int freq()
  public DontKnowClassName positions()
  public final AttributeSource attributes() {
   if (atts==null) atts=new AttributeSource();
   return atts;
  }
  ...default impl of the bulk access using the abstract methods from DocIdSetIterator
}
{code}
"
0,"Move MutableValues to Common ModuleSolr makes use of the MutableValue* series of classes to improve performance of grouping by FunctionQuery (I think).  As such they are used in ValueSource implementations.  Consequently we need to move these classes in order to move the ValueSources.

As Yonik pointed out, these classes have use beyond just FunctionQuerys and might be used by both Solr and other modules.  However I don't think they belong in Lucene core, since they aren't really related to search functionality.  Therefore I think we should put them into a Common module, which can serve as a dependency to Solr and any module."
1,"JCARepositoryHandle.login(...) methods never throw NoSuchWorkspaceExceptionCall sequence:
  JCARepositoryHandle.login(Credentials, String)      // (here non-existent workspace is specified for login)
    JCARepositoryHandle.login(JCAConnectionRequestInfo)
      ConnectionManager.allocateConnection(ManagedConnectionFactory, ConnectionRequestInfo)
        ...
          JCAManagedConnection.openSession(JCAConnectionRequestInfo)
            Repository.login(Credentials, String)        // here NoSuchWorkspaceException is thrown, catched by JCAManagedConnection.openSession(JCAConnectionRequestInfo), _set as linkedException_ to ResourceException, which is thrown
        ...
     Here (in JCARepositoryHandle.login(JCAConnectionRequestInfo)) ResourceException is caught, its _cause_ is retreived, and, if cause is NoSuchWorkspaceException, it's thrown, else another exception is thrown.

Note, that when exception occures on lower level, it's wrapped in ResourceException using setLinkedException(), but on upper level it's unwrapped using getCause(). But cause is not set by anyone, it's null, so NoSuchWorkspaceException is never thrown here.

Suggested fix is to use same mechanism on both ends: either change wrapping mechanism to exception chaining (new ResourceException(msg, cause)), or unwrap using ResourceException.getLinkedException()."
0,"The token types of the standard tokenizer is not accessibleThe StandardTokenizerImpl not being public, these token types are not accessible :

{code:java}
public static final int ALPHANUM          = 0;
public static final int APOSTROPHE        = 1;
public static final int ACRONYM           = 2;
public static final int COMPANY           = 3;
public static final int EMAIL             = 4;
public static final int HOST              = 5;
public static final int NUM               = 6;
public static final int CJ                = 7;
/**
 * @deprecated this solves a bug where HOSTs that end with '.' are identified
 *             as ACRONYMs. It is deprecated and will be removed in the next
 *             release.
 */
public static final int ACRONYM_DEP       = 8;

public static final String [] TOKEN_TYPES = new String [] {
    ""<ALPHANUM>"",
    ""<APOSTROPHE>"",
    ""<ACRONYM>"",
    ""<COMPANY>"",
    ""<EMAIL>"",
    ""<HOST>"",
    ""<NUM>"",
    ""<CJ>"",
    ""<ACRONYM_DEP>""
};
{code}

So no custom TokenFilter can be based of the token type. Actually even the StandardFilter cannot be writen outside the org.apache.lucene.analysis.standard package.
"
0,"Land DocValues on trunkIts time to move another feature from branch to trunk. I want to start this process now while still a couple of issues remain on the branch. Currently I am down to a single nocommit (javadocs on DocValues.java) and a couple of testing TODOs (explicit multithreaded tests and unoptimized with deletions) but I think those are not worth separate issues so we can resolve them as we go. 
The already created issues (LUCENE-3075 and LUCENE-3074) should not block this process here IMO, we can fix them once we are on trunk. 

Here is a quick feature overview of what has been implemented:
 * DocValues implementations for Ints (based on PackedInts), Float 32 / 64, Bytes (fixed / variable size each in sorted, straight and deref variations)
 * Integration into Flex-API, Codec provides a PerDocConsumer->DocValuesConsumer (write) / PerDocValues->DocValues (read) 
 * By-Default enabled in all codecs except of PreFlex
 * Follows other flex-API patterns like non-segment reader throw UOE forcing MultiPerDocValues if on DirReader etc.
 * Integration into IndexWriter, FieldInfos etc.
 * Random-testing enabled via RandomIW - injecting random DocValues into documents
 * Basic checks in CheckIndex (which runs after each test)
 * FieldComparator for int and float variants (Sorting, currently directly integrated into SortField, this might go into a separate DocValuesSortField eventually)
 * Extended TestSort for DocValues
 * RAM-Resident random access API plus on-disk DocValuesEnum (currently only sequential access) -> Source.java / DocValuesEnum.java
 * Extensible Cache implementation for RAM-Resident DocValues (by-default loaded into RAM only once and freed once IR is closed) -> SourceCache.java
 
PS: Currently the RAM resident API is named Source (Source.java) which seems too generic. I think we should rename it into RamDocValues or something like that, suggestion welcome!   


Any comments, questions (rants :)) are very much appreciated."
1,"Registering node type names with spaces fails in clustered environmentRegistering a node type name that contains at least one space in a clustered environment will cause a JournalException in cluster nodes trying to read that change back from the journal. The stack trace observed is:

JournalException: Parse error while reading node type definition.
       at AbstractRecord.readNodeTypeDef(AbstractRecord.java:245)
       ...
Caused by: ParseException: Missing '[' delimiter for beginning of node type name ((internal), line 47)
       at Lexer.fail(Lexer.java:148)
       ...

(package names and intermediate frames omitted for brevity)."
0,"support protected words in Stemming TokenFiltersThis is from LUCENE-1515

I propose that all stemming TokenFilters have an 'exclusion set' that bypasses any stemming for words in this set.
Some stemming tokenfilters have this, some do not.

This would be one way for Karl to implement his new swedish stemmer (as a text file of ignore words).
Additionally, it would remove duplication between lucene and solr, as they reimplement snowballfilter since it does not have this functionality.
Finally, I think this is a pretty common use case, where people want to ignore things like proper nouns in the stemming.

As an alternative design I considered a case where we generalized this to CharArrayMap (and ignoring words would mean mapping them to themselves), which would also provide a mechanism to override the stemming algorithm. But I think this is too expert, could be its own filter, and the only example of this i can find is in the Dutch stemmer.

So I think we should just provide ignore with CharArraySet, but if you feel otherwise please comment.
"
1,"ResidualProperties Converter uses wrong AtomicType Converter on updateWhen writing back data, the ResidualPropertiesCollectionConverterImpl.internalSetProperties method looks at the type of the Java object
to find the atomic type converter instead of getting the converter according to the collection descriptor.

This may lead to NullPointerExceptions in case the concrete type is an extension (or implementation) of the declared type.

I am currently working on a patch to attache to this bug."
0,"Deprecate and remove ShingleMatrixFilterSpin-off from LUCENE-1391: This filter is unmainatined and no longer up-to-date, has bugs nobody understands and does not work with attributes.

This issue deprecates it as of Lucene 3.1 and removes it from trunk."
1,"principalbased ACL editing fails if principalName differs from the authorizableIDthis issue has been reported by alexK:

editing the permissions for a principal whose name differs from the id of the corresponding user/group fails with AccessControlException.

i quickly had a look at it and the main problem is caused by the ACEditor that assumes that the last segment of the 
path corresponds to the principal name. this isn't true if the principalName differs from the id.



"
0,"Improve handling for missing text filter dependencyUsing a LazyReader in a TextFilter implementation will not always throw a NoClassDefFoundError if a depending jar file is missing.

The text filter implementations should therefore include a static block that forces an initialization of a depending class."
1,"Binding repository to a nameserver with RegistryHelper causes failure on lookup.Binding a repository to a nameserver using RegistryHelper causes the next subsequent lookup to fail.  This is what I observerd:

1. RegistryHelper.registerRepository creates a new BindableRepository and initializes it.  This, in turn, initializes the ""real"" repository (i.e. delagtee).  It then binds this reference with the nameserver.

2. On the next lookup, BindableRepositoryFactory.getObjectInstance is invoked.  Thie method checks it's cache for a repository.  Since one does not exist yet, it creates a new BindableRepository and tries to initialize it.  This fails since the call to RegistryHelper.registerRepository already initialized the repository.

The error message basically says the repository is already in use by another process because the .lock file is present.  To fix this, I modified RegistryHelper.registerRepository to NOT initialize the repository and simply bind the ""Reference""."
0,"BoostingTermQuery's BoostingSpanScorer class should be protected instead of package accessCurrently, BoostingTermScorer, an inner class of BoostingTermQuery is not accessible from outside the search.payloads
making it difficult to write an extension of BoostingTermQuery. The other inner classes are protected already, as they should be."
0,"Ability to ignore (reject) cookies altogetherI was looking for a way to ignore cookies altogether, but there doesn't appear
to be one.  I could definitely use this capability right now, and I can see
others making use of it at times."
0,"Minor spi2dav ExceptionConverter improvementsIt would be nice if the ExceptionConverter class in spi2dav returned UnsupportedRepositoryOperationExceptions instead of the undeclared UnsupportedOperationExceptions for HTTP 501 responses.

Besides that, the ExceptionConverter.generate() methods should be cleaned to always return the generated exception instead of in some cases returning and in others throwing it.

Finally, there's some unused code and chances for Java 5 cleanups.

I'll attach a patch, and commit it unless anyone objects."
0,"Add ability to specify compilation/matching flags to RegexCapabiltiies implementationsThe Jakarta Regexp and Java Util Regex packages both support the ability to provides flags that alter the matching behavior of a given regular expression. While the java.util.regex.Pattern implementation supports providing these flags as part of the regular expression string, the Jakarta Regexp implementation does not.  Therefore, this improvement request is to add the capability to provide those modification flags to either implementation. 

I've developed a working implementation that makes minor additions to the existing code. The default constructor is explicitly defined with no arguments, and then a new constructor with an additional ""int flags"" argument is provided. This provides complete backwards compatibility. For each RegexCapabilties implementation, the appropriate flags from the regular expression package is defined as  FLAGS_XXX static fields. These are pass through to the underlying implementation. They are re-defined to avoid bleeding the actual implementation classes into the caller namespace.

Proposed changes:

For the JavaUtilRegexCapabilities.java, the following is the changes made.

  private int flags = 0;
  
  // Define the optional flags from Pattern that can be used.
  // Do this here to keep Pattern contained within this class.
  
  public final int FLAG_CANON_EQ = Pattern.CANON_EQ;
  public final int FLAG_CASE_INSENSATIVE = Pattern.CASE_INSENSATIVE;
  public final int FLAG_COMMENTS = Pattern.COMMENTS;
  public final int FLAG_DOTALL = Pattern.DOTALL;
  public final int FLAG_LITERAL = Pattern.LITERAL;
  public final int FLAG_MULTILINE = Pattern.MULTILINE;
  public final int FLAG_UNICODE_CASE = Pattern.UNICODE_CASE;
  public final int FLAG_UNIX_LINES = Pattern.UNIX_LINES;
  
  /**
   * Default constructor that uses java.util.regex.Pattern 
   * with its default flags.
   */
  public JavaUtilRegexCapabilities()  {
    this.flags = 0;
  }
  
  /**
   * Constructor that allows for the modification of the flags that
   * the java.util.regex.Pattern will use to compile the regular expression.
   * This gives the user the ability to fine-tune how the regular expression 
   * to match the functionlity that they need. 
   * The {@link java.util.regex.Pattern Pattern} class supports specifying 
   * these fields via the regular expression text itself, but this gives the caller
   * another option to modify the behavior. Useful in cases where the regular expression text
   * cannot be modified, or if doing so is undesired.
   * 
   * @flags The flags that are ORed together.
   */
  public JavaUtilRegexCapabilities(int flags) {
    this.flags = flags;
  }
  
  public void compile(String pattern) {
    this.pattern = Pattern.compile(pattern, this.flags);
  }


For the JakartaRegexpCapabilties.java, the following is changed:

  private int flags = RE.MATCH_NORMAL;

  /**
   * Flag to specify normal, case-sensitive matching behaviour. This is the default.
   */
  public static final int FLAG_MATCH_NORMAL = RE.MATCH_NORMAL;
  
  /**
   * Flag to specify that matching should be case-independent (folded)
   */
  public static final int FLAG_MATCH_CASEINDEPENDENT = RE.MATCH_CASEINDEPENDENT;
 
  /**
   * Contructs a RegexCapabilities with the default MATCH_NORMAL match style.
   */
  public JakartaRegexpCapabilities() {}
  
  /**
   * Constructs a RegexCapabilities with the provided match flags.
   * Multiple flags should be ORed together.
   * 
   * @param flags The matching style
   */
  public JakartaRegexpCapabilities(int flags)
  {
    this.flags = flags;
  }
  
  public void compile(String pattern) {
    regexp = new RE(pattern, this.flags);
  }
"
0,"Remove old hooks for the implementation of hard linksEarly drafts of the JCR specification specified that repositories should support ""hard links"", which woud lead to the situation, that items might have multiple parent nodes. In the meantime hard links have been removed from the spec and are unlikely to be re-added in future revisions.

Nevertheless, Jackrabbit still contains some references to supporting this mechanism (e.g. the NodeState.parentUUIDs field), which should be removed."
0,"Add simple query method to ObjectContentManagerAs discussed in [1], I suggest a new method 

    ObejctContentManager.getObjectIterator(String query, String language)

to easily query the repository for objects using a predefined query. (I chose getObjectIterator instead of getObjects as I intend the method to return an Iterator and not a Collection)

[1] http://www.mail-archive.com/dev%40jackrabbit.apache.org/msg07475.html"
0,"make Query.createWeight public (or add back Query.createQueryWeight())Now that the QueryWeight class has been removed, the public QueryWeight createQueryWeight() method on Query was also removed

i have cases where i want to create a weight for a sub query (outside of the org.apache.lucene.search package) and i don't want the weight normalized (think BooleanQuery outside of the o.a.l.search package)

in order to do this, i have to create a static Utils class inside o.a.l.search, pass in the Query and searcher, and have the static method call the protected createWeight method
this should not be necessary

This could be fixed in one of 2 ways:
1. make createWeight() public on Query (breaks back compat)
2. add the following method:
{code}
public Weight createQueryWeight(Searcher searcher) throws IOException {
  return createWeight(searcher);
}
{code}

createWeight(Searcher) should then be deprectated in favor of the publicly accessible method
"
1,"org.apache.http.impl.client.cache.memcached.MemcachedHttpCacheStorage should verify class of returned object before castingorg.apache.http.impl.client.cache.memcached.MemcachedHttpCacheStorage

Original (in getEntry function): 
  byte[] data = (byte[]) client.get(url);

Should be:
  Object obj= client.get(url);
  if (null == obj || !(objinstanceof byte[])) {
    return null;
  }
  byte[] data = (byte[])obj;


Original (in updateEntry function):
  byte[] oldBytes = (v != null) ? (byte[]) v.getValue() : null;

Should be:
  byte[] oldBytes = (v != null && (v.getValue() instanceof byte[])) ? (byte[]) v.getValue() : null;



  
"
1,"HttpClient throws NPE on Invalid Port when used with MultiThreadedHttpConnectionManagerThe HttpClient throws NullPointerException in the main thread when an invalid port (like 80001) is used in the URL. An IllegalArgumentException is thrown in TimeoutGuard thread.
 
Exception in thread ""Timeout guard"" java.lang.IllegalArgumentException: port out of range:80001
	at java.net.InetSocketAddress.<init>(InetSocketAddress.java:118)
	at java.net.Socket.<init>(Socket.java:240)
	at org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory.createSocket(DefaultProtocolSocketFactory.java:80)
	at org.apache.commons.httpclient.protocol.ControllerThreadSocketFactory$1.doit(ControllerThreadSocketFactory.java:91)
	at org.apache.commons.httpclient.protocol.ControllerThreadSocketFactory$SocketTask.run(ControllerThreadSocketFactory.java:158)
	at java.lang.Thread.run(Thread.java:613)
Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.commons.httpclient.HttpConnection.open(HttpConnection.java:721)
	at org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpConnectionAdapter.open(MultiThreadedHttpConnectionManager.java:1361)
	at org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:387)
	at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:171)
	at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397)
	at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:323)
	at com.aol.test.HttpTest$PoolingHttpConnector.doGet(HttpTest.java:47)
	at com.aol.test.HttpTest.main(HttpTest.java:17)

It should throw a checked exception in main thread so caller can handle the error condition more gracefully.

The test program is attached. This is caused by a race condition and it's not always reproducible. Running in debugger shows a different behavior.

package com.aol.test;

import java.io.IOException;

import org.apache.commons.httpclient.HttpClient;
import org.apache.commons.httpclient.HttpStatus;
import org.apache.commons.httpclient.MultiThreadedHttpConnectionManager;
import org.apache.commons.httpclient.methods.GetMethod;
import org.apache.commons.httpclient.params.HttpConnectionManagerParams;

public class HttpTest {
	
	public static void main(String[] args) {
		PoolingHttpConnector conn = new PoolingHttpConnector();
		
		try {
			String response = conn.doGet(""http://www.aol.com:80001"");
			System.out.println(""Response='"" + response + ""'"");
		} catch (IOException e) {
			e.printStackTrace();
		}
	}


	static class PoolingHttpConnector {
		
		public static final int MAX_TOTAL_CONNECTIONS = 16;
		public static final int MAX_CONNECTIONS_PER_HOST = 8;
		public static final int CONNECT_TIMEOUT = 5000;
		public static final int SOCKET_TIMEOUT = 5000;
		public static final boolean TCP_NO_DELAY = true;
		
	    private static MultiThreadedHttpConnectionManager poolManager;
	    private static HttpConnectionManagerParams httpParams;
	    private static HttpClient httpClient;
	    private static boolean initialized = false;
	    
		public PoolingHttpConnector() 
		{
			initialize();
		}

		public String doGet(String url) throws IOException {
			GetMethod method = new GetMethod(url);
					
			try {
	            int status = httpClient.executeMethod(method);	            
		        String response = new String(method.getResponseBody());
	            
	            if (status != HttpStatus.SC_OK)
	            	throw new IOException(""HTTP error: "" + response);
	            
	            return response;
	            
			} finally {
	            method.releaseConnection();
			}
	 	} 	
	
		private synchronized void initialize() {	
			if (initialized)
				return;
			
	        poolManager = new MultiThreadedHttpConnectionManager();
	        httpParams = new HttpConnectionManagerParams();
	        
	        httpParams.setMaxTotalConnections(MAX_TOTAL_CONNECTIONS);
	        httpParams.setDefaultMaxConnectionsPerHost(MAX_CONNECTIONS_PER_HOST);
	        httpParams.setTcpNoDelay(TCP_NO_DELAY);
	        httpParams.setSoTimeout(SOCKET_TIMEOUT);
	        httpParams.setConnectionTimeout(CONNECT_TIMEOUT);
	        
	        poolManager.setParams(httpParams);
	        httpClient = new HttpClient(poolManager);

			initialized = true;
		}
		
	}
}



"
1,"DefaultClientRequestDirector doesn't release connections back to ClientConnectionManager on exceptionsSee HTTPCLIENT-747 for more info.  Basically the deal is that an entry is always allocated, but currently it's only released if execute(..) completes normally."
1,"DEFAULT_ATTRIBUTE_FACTORY faills to load implementation class when iterface comes from different classloaderThis is a followup for [http://www.lucidimagination.com/search/document/1724fcb3712bafba/using_the_new_tokenizer_api_from_a_jar_file]:

The DEFAULT_ATTRIBUTE_FACTORY should load the implementation class for a given attribute interface from the same classloader like the attribute interface. The current code loads it from the classloader of the lucene-core.jar file. In solr this fails when the interface is in a JAR file coming from the plugins folder. 

The interface is loaded correctly, because the addAttribute(FooAttribute.class) loads the FooAttribute.class from the plugin code and this with success. But as addAttribute tries to load the class from its local lucene-core.jar classloader it will not find the attribute.

The fix is to tell Class.forName to use the classloader of the corresponding interface, which is the correct way to handle it, as the impl and the attribute should always be in the same classloader and file.

I hope I can somehow add a test for that."
1,Node deleted while query is executed should not affect result sizeCurrently the QueryResultImpl counts result nodes as invalid when the access check throws a ItemNotFoundException (line 311). This leads to inconsistent total size. IMO it is sufficient to count them as invalid when the client iterates over the nodes (line 555).
1,"ClassCastException in ParallelReader classClassCastException in ParalleReader when calling getTermFreqVectors on line 153

Reason : 

 cast of key and value is swapped

Fixed with : 

      IndexReader reader = (IndexReader)e.getValue();
      String field = (String)e.getKey();
"
0,"Links Pointing to Javadocs Are Incorrect and Return 404There are various links on the ""Configuring Jackrabbit""  page (http://jackrabbit.apache.org/doc/config.html) that are invalid.  For example, I wanted to read the ""PersistenceManager javadocs"" link, but it returns a 404.  This will decrease the confidence in the project and hinder its adoption."
0,"Name and Path interfaces in SPIThe SPI interface currently has a dependency to QName and Path classes in jackrabbit-jcr-commons. Architecturally it would be better to have Name and Path interfaces in the SPI package, and have the implementing classes in commons."
1,"intermittent deadlock in TestAtomicUpdate,TestIndexWriterExceptionsWhile backporting issues for 2.9.x/3.0.x release I hit deadlocks in these two tests, under both test-core and test-tag."
0,"Extend Codec with a SegmentInfos writer / readerI'm trying to implement a Codec that works with append-only filesystems (HDFS). It's _almost_ done, except for the SegmentInfos.write(dir), which uses ChecksumIndexOutput, which in turn uses IndexOutput.seek() - and seek is not supported on append-only output. I propose to extend the Codec interface to encapsulate also the details of SegmentInfos writing / reading. Patch to follow after some feedback ;)"
0,"HttpConnection.isOpen() logging is not accurateisOpen() does not differentiate between stale and closed.  If the connection is closed isStale() will return 
true.  The logs will then indicate that the connection was stale, even though it was really just closed.  
close() is also called a second time unnecessarily.

This should be fixed for 3.0, and perhaps even 2.0.1.

<http://nagoya.apache.org/eyebrowse/ReadMsg?listName=commons-httpclient-
dev@jakarta.apache.org&msgNo=7205>"
1,"AssertionError on creating doc containing field with empty string as field nameSpinoff from here:

  http://www.gossamer-threads.com/lists/lucene/java-user/58496

Pre-2.3 you were allowed to add Fields to a Document where the field name is the empty string.  In 2.3.0 it broke: you hit this during flush:

{code}
java.lang.AssertionError
    at org.apache.lucene.index.TermInfosWriter.add(TermInfosWriter.java:143)
    at org.apache.lucene.index.DocumentsWriter.appendPostings(DocumentsWriter.java:2290)
    at org.apache.lucene.index.DocumentsWriter.writeSegment(DocumentsWriter.java:1985)
    at org.apache.lucene.index.DocumentsWriter.flush(DocumentsWriter.java:539)
    at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:2497)
    at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:2397)
    at org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:1204)
    at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1178)
    at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1153) 
{code}

The bug is just an over-aggressive assert statement.  I'll commit a fix shortly & port to 2.3 branch for 2.3.1 release."
0,"Remove shared doc storesWith per-thread DocumentsWriters sharing doc stores across segments doesn't make much sense anymore.

See also LUCENE-2324."
0,"Core Tests should call Version based ctors instead of deprecated default ctorsLUCENE-2183 introduced new ctors for all CharTokenizer subclasses. Core - tests should use those ctors with Version.LUCENE_CURRENT instead of the the deprecated ctors. Yet, LUCENE-2240 introduces more Version ctors For WhitespaceAnalyzer and SimpleAnalyzer. Test should also use their Version ctors instead the default ones."
0,"the unversioned site points to a dead trunkThe unversioned site needs to point to the new merged trunk.
Currently it points to the closed-off dead trunk in two different places.
"
1,"cache does not validate multiple cached variantsThere is a bug in CachingHttpClient, where when we attempt to collect all the etags for existing cached variants so we can send a conditional request to the origin, we accidentally don't find any, and send an unconditional request instead."
1,"encode/decodeAs I mention in my email executing <code>ISO9075.decode(""StringWith$inside"")</code> leads to exception:
java.lang.StringIndexOutOfBoundsException: String index out of range: 1
	at java.lang.String.charAt(String.java:444)
	at java.util.regex.Matcher.appendReplacement(Matcher.java:559)
	at com.day.crx.domino.util.NameEncoderDecoder.decode(NameEncoderDecoder.java:117)
	at integration.query.QueryTest.testQuery(QueryTest.java:49)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:324)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:128)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)

The problem is in Matcher.appendReplacement() method, because it didn't correctly interpret '$' and '\' sign. Both have to be escaped with '\' sign."
1,"IndexWriter & ConcurrentMergeScheduler deadlock case if starting a merge hits an exceptionIf you're using CMS (the default) and mergeInit hits an exception (eg
OOME), we are not properly clearing IndexWriter's internal tracking of
running merges.  This causes IW.close() to hang while it incorrectly
waits for these non-started merges to finish.

"
0,"Set up a release goal in MavenCreate a single Maven goal for building the Jackrabbit release packages. The goal should be based on the standard Maven dist goal, but include also the jackrabbit-commons and other modules we want to include in the release."
0,"Build problem with StrictSSLProtocolSocketFactoryStrictSSLProtocolSocketFactory requires jcert.jar to be in compile.classpath of
build.xml.  Here is a patch that will fix it:

Index: build.xml
===================================================================
RCS file: /home/cvspublic/jakarta-commons/httpclient/build.xml,v
retrieving revision 1.27
diff -u -r1.27 build.xml
--- build.xml   23 May 2003 02:49:01 -0000      1.27
+++ build.xml   26 May 2003 04:23:50 -0000
@@ -98,6 +98,7 @@
     <pathelement location=""${build.home}/classes""/>
     <pathelement location=""${junit.jar}""/>
     <pathelement location=""${jsse.jar}""/>
+    <pathelement location=""${jcert.jar}""/>
     <pathelement location=""${jce.jar}""/>
     <pathelement location=""${jnet.jar}""/>
     <pathelement location=""${commons-logging.jar}""/>"
0,"Log / trace wrapper: upgrade to JCR API 2.0The JCR Log wrapper (jcrlog) currently only supports the JCR 1.0 API. It should be upgraded to 2.0. Also, the dependency to jackrabbit-core should be removed, and generics should be used."
0,"CharTokenizer has bugs for large documents.Initially found by hudson from additional testing added in LUCENE-3894, but 
currently not reproducable (see LUCENE-3895).

But its easy to reproduce for a simple single-threaded case in TestDuelingAnalyzers."
0,"TokenSources.getTokenStream(Document...) Sometimes, one already has the Document, and just needs to generate a TokenStream from it, so I am going to add a convenience method to TokenSources.  Sometimes, you also already have just the string, so I will add a convenience method for that."
1,"IndexWriter commits update documents without corresponding deletewhile backporting the testcase from LUCENE-3348 I ran into this thread hazard in the 3.x branch. We actually fixed this issue in LUCENE-3348 for Lucene 4.0 but since DWPT has a slightly different behavior when committing segments I create a new issue to track this down in 3.x. when we prepare a commit we sync on IW flush the DW and apply all deletes then release the lock, maybeMerge and start the commit (IW#startCommit(userdata)). Yet, a new segment could be flushed via getReader and sneak into the SegementInfos which are cloned in IW#startCommit instead of in prepareCommit right after the flush. "
0,"Improve PhraseQuery.toString()PhraseQuery.toString() is overly simplistic, in that it doesn't correctly show phrases with gaps or overlapping terms. This may be misleading when presenting phrase queries built using complex analyzers and filters."
0,Move to a newer vesion of Commons CollectionsIt would be useful if the developer wants to use jackrabbit in an application that uses a newer version of this library.
1,"Unable to login with two different Credentials to same workspace in one TransactionI'm using the Jackrabbit 1.2.1 JCA adapter and trying to access in a SessionBean-Method with Container Transaction a Workspace with 2 different Credentials. 
The Method takes about 400ms to finish but no commit on TransactionContextr occurs (Debugging ..) only the prepare was called 2 times .
The Container hangs on the PostInvoke Method about 5 seconds and then i get a ""javax.transaction.xa.XAException"" 
with the Warn Message: Transaction rolled back because timeout expired

The code ..
Context ctx = new InitialContext(); 
Repository repository = (Repository) ctx.lookup(""java:comp/env/jackrabbit""); 
Credentials credentials = new SimpleCredentials(""user1"", ""password1"".toCharArray()); 
Credentials credentials2 = new SimpleCredentials(""user2"", ""password2"".toCharArray()); 
Session session1 = repository.login(credentials, ""default""); 
Session session2 = repository.login(credentials2, ""default""); 

Session1 adds a node to the workspace .. and with the session2 i do nothing except the login !
If i make no second login the Method works fine."
0,"Implement QueryObjectModelFactory.fullTextSearch() in QueryManagerImplWhile doing the JCR-1104 upgrade to JCR 2.0, we ran into an issue on how to best handle the QueryObjectModelFactory.fullTextSearch() method that seems to have changed a bit since the spi-commons version was written.

Marcel, can you take a look at this when you have time. The dummy implementation I added now is at line 97 of QueryManagerImpl.java in jackrabbit-core."
0,"addIndexes(Directory...) should not trigger merge on flush()IndexWriter.addIndexes(Directory..) calls flush() w/ triggerMerge=true. This beats the purpose of the changes done to addIndexes to not merge any segments and leave it as the application's choice. The change is very simple - pass false instead of true. I don't plan to post a patch, however opened an issue in case some want to comment about it."
0,"Mating Collector and Scorer on doc Id ordernessThis is a spin off of LUCENE-1593. This issue proposes to expose appropriate API on Scorer and Collector such that one can create an optimized Collector based on a given Scorer's doc-id orderness and vice versa. Copied from LUCENE-1593, here is the list of changes:

# Deprecate Weight and create QueryWeight (abstract class) with a new scorer(reader, scoreDocsInOrder), replacing the current scorer(reader) method. QueryWeight implements Weight, while score(reader) calls score(reader, false /* out-of-order */) and scorer(reader, scoreDocsInOrder) is defined abstract.
#* Also add QueryWeightWrapper to wrap a given Weight implementation. This one will also be deprecated, as well as package-private.
#* Add to Query variants of createWeight and weight which return QueryWeight. For now, I prefer to add a default impl which wraps the Weight variant instead of overriding in all Query extensions, and in 3.0 when we remove the Weight variants - override in all extending classes.
# Add to Scorer isOutOfOrder with a default to false, and override in BS to true.
# Modify BooleanWeight to extend QueryWeight and implement the new scorer method to return BS2 or BS based on the number of required scorers and setAllowOutOfOrder.
# Add to Collector an abstract _acceptsDocsOutOfOrder_ which returns true/false.
#* Use it in IndexSearcher.search methods, that accept a Collector, in order to create the appropriate Scorer, using the new QueryWeight.
#* Provide a static create method to TFC and TSDC which accept this as an argument and creates the proper instance.
#* Wherever we create a Collector (TSDC or TFC), always ask for out-of-order Scorer and check on the resulting Scorer isOutOfOrder(), so that we can create the optimized Collector instance.
# Modify IndexSearcher to use all of the above logic.

The only class I'm worried about, and would like to verify with you, is Searchable. If we want to deprecate all the search methods on IndexSearcher, Searcher and Searchable which accept Weight and add new ones which accept QueryWeight, we must do the following:
* Deprecate Searchable in favor of Searcher.
* Add to Searcher the new QueryWeight variants. Here we have two choices: (1) break back-compat and add them as abstract (like we've done with the new Collector method) or (2) add them with a default impl to call the Weight versions, documenting these will become abstract in 3.0.
* Have Searcher extend UnicastRemoteObject and have RemoteSearchable extend Searcher. That's the part I'm a little bit worried about - Searchable implements java.rmi.Remote, which means there could be an implementation out there which implements Searchable and extends something different than UnicastRemoteObject, like Activeable. I think there is very small chance this has actually happened, but would like to confirm with you guys first.
* Add a deprecated, package-private, SearchableWrapper which extends Searcher and delegates all calls to the Searchable member.
* Deprecate all uses of Searchable and add Searcher instead, defaulting the old ones to use SearchableWrapper.
* Make all the necessary changes to IndexSearcher, MultiSearcher etc. regarding overriding these new methods.

One other optimization that was discussed in LUCENE-1593 is to expose a topScorer() API (on Weight) which returns a Scorer that its score(Collector) will be called, and additionally add a start() method to DISI. That will allow Scorers to initialize either on start() or score(Collector). This was proposed mainly because of BS and BS2 which check if they are initialized in every call to next(), skipTo() and score(). Personally I prefer to see that in a separate issue, following that one (as it might add methods to QueryWeight)."
0,"Speed up Top-K sampling testsspeed up the top-k sampling tests (but make sure they are thorough on nightly etc still)

usually we would do this with use of atLeast(), but these tests are somewhat tricky,
so maybe a different approach is needed."
0,Improve Performance of DescendantSelfAxisQueryIn DescendantSelfAxisQuery.DescendantSelfAxisScorer.isValid(int) contextHits is populated with docs that are found on the way down the axis. The current algorithm unfortunately doesn't add any new docs at all because it only adds docs already present in contextHits. This leads to more calls to HierarchyResolver.getParent(int) than necessary.
1,SimpleText has a bulk enum buffer reuse bugtestBulkPostingsBufferReuse fails with SimpleText codec.
0,"In AbstractImportXmlTest, a bug in getUnusedUri() causes URI length to grow too quickly, causing test to fail when using ORM-PMTest fails when using ORM-PM because the URI exceeds the column size in the database.

Here is the current implementation:

    protected String getUnusedURI() throws RepositoryException {
        Set uris = new HashSet(Arrays.asList(nsp.getURIs()));
        String uri = TEST_URI;
        int i = 0;
        while (uris.contains(uri)) {
            uri += i++;
        }
        return uri;
    }

When running the test, the URI grows to become something like this:

When i=50,
""www.apache.org/jackrabbit/test/namespaceImportTest01234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950""

Here is the proposed fix:

    protected String getUnusedURI() throws RepositoryException {
        Set uris = new HashSet(Arrays.asList(nsp.getURIs()));
        String uri = TEST_URI;
        int i = 0;
        while (uris.contains(uri)) {
            uri = TEST_URI + i++;
        }
        return uri;
    }

When i=50,
""www.apache.org/jackrabbit/test/namespaceImportTest50"""
0,"Rework of the TermInfosReader class to remove the Terms[], TermInfos[], and the index pointer long[] and create a more memory efficient data structure.Basically packing those three arrays into a byte array with an int array as an index offset.  

The performance benefits are stagering on my test index (of size 6.2 GB, with ~1,000,000 documents and ~175,000,000 terms), the memory needed to load the terminfos into memory were reduced to 17% of there original size.  From 291.5 MB to 49.7 MB.  The random access speed has been made better by 1-2%, load time of the segments are ~40% faster as well, and full GC's on my JVM were made 7 times faster.

I have already performed the work and am offering this code as a patch.  Currently all test in the trunk pass with this new code enabled.  I did write a system property switch to allow for the original implementation to be used as well.

-Dorg.apache.lucene.index.TermInfosReader=default or small

I have also written a blog about this patch here is the link.

http://www.nearinfinity.com/blogs/aaron_mccurry/my_first_lucene_patch.html



"
0,"Dependency URL broken for commons-loggingOn http://jakarta.apache.org/commons/httpclient/dependencies.html there is a 
typo in the href to the logging dependency, this should be

http://jakarta.apache.org/commons/logging/"
1,"cache module does not completely handle upstream Warning headers correctlyThere are a couple of MUST requirements from the RFC for Warning headers that aren't correctly handled by the current implementation:

http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.46

1. ""However, if a cache successfully validates a cache entry, it SHOULD remove any Warning headers previously attached to that entry except as specified for specific Warning codes. It MUST then add any Warning headers received in the validating response.""

2. ""If an implementation receives a message with a warning-value that includes a warn-date, and that warn-date is different from the Date value in the response, then that warning-value MUST be deleted from the message before storing, forwarding, or using it. (This prevents bad consequences of naive caching of Warning header fields.) If all of the warning-values are deleted for this reason, the Warning header MUST be deleted as well."" "
0,non-recursive MultiTermDocsA non-recursive implementation of MultiTermDocs.next() and skipTo() would be nice as it's currently possible to get a stack overflow in very rare situations.
0,"HttpRoute.equals(Object o) is quite inefficient, as it does not take full advantage of shortcut logicHttpRoute.equals(Object o) is quite inefficient, as it does not take full advantage of shortcut logic.

It should return as soon as the first  false is detected.

Patch to follow implements short-circuit checking."
1,"RMI: Property.getValue() fails with EOFException after many readsWhen reading binary properties via RMI it can happen that it fails throwing an EOFException. This is caused by a server sided ""Too many open files"" bacause BinaryValue.writeObject() does not close the underlying value InputStream."
0,"Use the assembly plugin for packaging -tests jarsThe spi and jcr2spi components have test code that currently packaged using the jar:test-jar goal and used as a test dependency by other components.

The extra jar plugin invocation creates extra Maven build numbers and causes problems for snapshot dependencies from the repository.apache.org repository that we recently started using. Using the assembly plugin to create the test jars should avoid this problem."
0,"Add variable-gap terms index impl.PrefixCodedTermsReader/Writer (used by all ""real"" core codecs) already
supports pluggable terms index impls.

The only impl we have now is FixedGapTermsIndexReader/Writer, which
picks every Nth (default 32) term and holds it in efficient packed
int/byte arrays in RAM.  This is already an enormous improvement (RAM
reduction, init time) over 3.x.

This patch adds another impl, VariableGapTermsIndexReader/Writer,
which lets you specify an arbitrary IndexTermSelector to pick which
terms are indexed, and then uses an FST to hold the indexed terms.
This is typically even more memory efficient than packed int/byte
arrays, though, it does not support ord() so it's not quite a fair
comparison.

I had to relax the terms index plugin api for
PrefixCodedTermsReader/Writer to not assume that the terms index impl
supports ord.

I also did some cleanup of the FST/FSTEnum APIs and impls, and broke
out separate seekCeil and seekFloor in FSTEnum.  Eg we need seekFloor
when the FST is used as a terms index but seekCeil when it's holding
all terms in the index (ie which SimpleText uses FSTs for).
"
1,"CompactNodeTypeDefReader adds nt:base as declared supertype even if already extending(reported to the list by michael singer)

I wrote a simple program which uses the nt-ns-util contribution to
register custom node types written in CND language.

I defined the following (very simple) custom node types:

<test = 'http://foo.bar/test'>
[test:firstnodetype]
+ test:secondnodetype mandatory

<test = 'http://foo.bar/test'>
[test:secondnodetype] > test:firstnodetype
+ test:thirdnodetype

<test = 'http://foo.bar/test'>
[test:thirdnodetype] > test:secondnodetype
- test:catalog (string)  < 'URI', 'URN', 'DOI', 'ISBN', 'ISSN'
- test:entry (string) m


In the resulting custom_nodetypes.xml each of the custom nodes has a
supertype of ""nt:base"" but I didn't explicitely define a supertype of
""nt:base"" for [test:secondnodetype] and [test:thirdnodetype].

I think this behavior is wrong since the method getDeclaredSupertypes()
of class NodeType always returns ""nt:base"" plus the explicitely declared
Supertype (which it e.g. does not for ""nt:folder"").
"
1,HTMLStripCharFilter produces invalid final offsetNightly build found this... I boiled it down to a small test case that doesn't require the big line file docs.
0,"No need for NodeReferences in jcr2spiI happened to come across the org.apache.jackrabbit.jcr2spi.state.NodeReferences interface, and realized that with the current SPI definitions there's really no need for that abstraction. The PropertyId array returned by NodeInfo.getReferences() is quite good enough for jcr2spi without any NodeReferences wrapping around it."
0,"Nightly build archives do not contain Java source code.Under the Lucene News section of the Overview page, this item's link:

26 January 2006 - Nightly builds available
http://cvs.apache.org/dist/lucene/java/nightly/

goes to a directory with several 1.9M files, none of which have the src/java tree in them."
0,"Add relative path parameter to rep:excerpt()This allows one to create an excerpt not just for the node associated with a result node, but also for a node relative to the result node."

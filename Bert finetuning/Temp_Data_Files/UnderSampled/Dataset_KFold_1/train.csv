label,summarydescription
0,Return null for optional configuration elementsTwo recently introduced configuration elements are optional but the configuration parser still returns an instance when the elements are missing in the configuration. The parser should return null when an element is not there in order to distinguish it from the case where an empty element is present. 
0,"Connection timeout logic redesignChangelog:

* CreateSocket method with timeout parameter added to the ProtocolSocketFactory
interface

* TimeoutController related code factored out of HttpConnection class and moved
into ControllerThreadSocketFactory helper class

* ReflectionSocketFactory helper class added. This factory encapsulates
reflection code to call JDK 1.4 Socket#connect method if supported

* All protocol socket factories now attempt to initially use
ReflectionSocketFactory if required to create a socket within a given limit of
time. If reflection fails protocol socket factories fall back onto the good ol'
ControllerThreadSocketFactory

Benefits:

* HttpConnection code got a lot cleaner
* When running in modern JREs expensive timeout controller thread per connection
attempt is no longer needed
* Ugly code intended to work around limitations of the older JREs is now
confined to a few helper classes that can be easily thrown away once we move
onto Java 1.4

Let me know what you think

Oleg"
0,Add import-export toolWe at <GX> creative online development would like to contribute our command-line import-export tool to the Apache Jackrabbit project. This tool is capable of exporting and importing all kinds of repository content (including custom nodetypes and namespace mappings) in a persistence-layer independent way. 
0,"Summer of Code GDATA Server  --Project structure and simple version to start with--This is the initial issue for the GDATA - Server project (Google Summer of Code). 
The purpose of the issue is to create the project structure in the svn repository to kick off the project. The source e.g. the project will be located at URL: http://svn.apache.org/repos/asf/lucene/java/trunk/contrib
The attachment includes the diff text file and the jar files included in the lib directory as a tar.gz file.
To get some information about the project see http://wiki.apache.org/general/SimonWillnauer/SummerOfCode2006"
0,"Support for transactions when using JCR over RMI.At this time, the sessions obtained from o.a.j.rmi.client.LocalAdapterFactory do not implement the methods for the XASession.  Therefor the RMI access layer does not support a transactional session."
0,"IndexOutput.writeString() should write length in bytesWe should change the format of strings written to indexes so that the length of the string is in bytes, not Java characters.  This issue has been discussed at:

http://www.mail-archive.com/java-dev@lucene.apache.org/msg01970.html

We must increment the file format number to indicate this change.  At least the format number in the segments file should change.

I'm targetting this for 2.1, i.e., we shouldn't commit it to trunk until after 2.0 is released, to minimize incompatible changes between 1.9 and 2.0 (other than removal of deprecated features)."
0,Update Spatial Lucene sort to use FieldComparatorSourceUpdate distance sorting to use FieldComparator sorting as opposed to SortComparator
0,"Create ChainingCollectorChainingCollector allows chaining a bunch of Collectors, w/o them needing to know or care about each other, and be passed into Lucene's search API, since it is a Collector on its own. It is a convenient, yet useful, class. Will post a patch w/ it shortly."
0,"Improve how ConcurrentMergeScheduler handles too-many-merges caseCMS now lets you set ""maxMergeThreads"" to control max # simultaneous
merges.

However, when CMS hits that max, it still allows further merges to
run, by running them in the foreground thread.  So if you set this max
to 1, and use 1 thread to add docs, you can get 2 merges running at
once (which I think is broken).

I think, instead, CMS should pause the foreground thread, waiting
until the number of merge threads drops below the limit.  Then, kick
off the backlog merge in a thread and return control back to primary
thread.
"
0,ORM PersistenceManagers don't compileORM PMs are out of synch with the latest changes of the api. 
0,"Port to Java5For my needs I've updated Lucene so that it uses Java 5 constructs. I know Java 5 migration had been planned for 2.1 someday in the past, but don't know when it is planned now. This patch against the trunk includes :

- most obvious generics usage (there are tons of usages of sets, ... Those which are commonly used have been generified)
- PriorityQueue generification
- replacement of indexed for loops with for each constructs
- removal of unnececessary unboxing

The code is to my opinion much more readable with those features (you actually *know* what is stored in collections reading the code, without the need to lookup for field definitions everytime) and it simplifies many algorithms.

Note that this patch also includes an interface for the Query class. This has been done for my company's needs for building custom Query classes which add some behaviour to the base Lucene queries. It prevents multiple unnnecessary casts. I know this introduction is not wanted by the team, but it really makes our developments easier to maintain. If you don't want to use this, replace all /Queriable/ calls with standard /Query/.

"
0,JSR 283: Configurations and Baselines
0,New MsOutlook Message ExtractorSinse we are using poi 3.0.2 it will be useful to have a outlook message extractor
0,"Avoid Maven 3 warningsJackrabbit trunk builds fine with the Maven 3 alpha releases, but there are some warnings about deprecated ${pom....} variables and unspecified reporting plugin versions that we might want to fix."
0,"Public Suffix ListHi,

I just found this useful list: http://publicsuffix.org/
and thought it would be nice to validate cookie domains against it, basically serving as a black list of domain for which never to set any cookies. What do you think about the attached patch? The download/parsing of the list is of course not part of the implementation.

Ortwin"
0,Cut over SpanQuery#getSpans to AtomicReaderContextFollowup from LUCENE-2831 - SpanQuery#getSpans(IR) seems to be the last remaining artifact that doesn't enforce per-segments context while it should really work on AtomicReaderContext (SpanQuery#getSpans(AtomicReaderContext) instead of a naked IR.
0,"Support synonym searchesJackrabbit should support synonym searches in the jcr:contains function like Google does.

Example:

//element(*, nt:resource)[jcr:contains(., '~food')]

-> finds all nt:resource nodes that contain the word food or synonyms for food."
0,"Make FieldSortedHitQueue publicCurrently, those who utilize the ""advanced"" search API cannot sort results using
the handy FieldSortedHitQueue. I suggest making this class public to facilitate
this use, as I can't think of a reason not to."
0,BundleDumper to analyze broken bundlesThe BundleReader fails if it can't read a bundle. We should have a tool to analyze broken bundles.
0,"Contrib/Jcr-Server: Improve package structure+ org
  + apache
    + jackrabbit
       + webdav
         + <dav-specific packages as currently present>
         + spi_jcr (formerly spi)
         + spi_simple (formerly dav-package below server/simple        
         + client (webdav-client lib)

       + server
         + jcr (jcr-server-classes formerly below server)
         + simple
   
       + client
         + jcr (client-side jcr impl. without dav-dependency)
    
         
"
0,"refactor spatial contrib ""Filter"" ""Query"" classesFrom erik's comments in LUCENE-1387

    * DistanceQuery is awkwardly named. It's not an (extends) Query.... it's a POJO with helpers. Maybe DistanceQueryFactory? (but it creates a Filter also)

    * CartesianPolyFilter is not a Filter (but CartesianShapeFilter is)
"
0,"Merge jcr-benchmark into the performance test suiteThe jackrabbit-jcr-benchmark component currently lives in the JCR Commons area, but there have been no active plans to release the component and AFAIUI it's so far only been used for the performance test suite we set up in JCR-2695. To avoid the extra complexity of spreading the test code over multiple components and trunks, I'd like to merge the jcr-benchmark component back to Jackrabbit trunk into the performance test suite we have in tests/performance."
0,"Small speedups to DocumentsWriterSome small fixes that I found while profiling indexing Wikipedia,
mainly using our own quickSort instead of Arrays.sort.

Testing first 200K docs of Wikipedia shows a speedup from 274.6
seconds to 270.2 seconds.

I'll commit in a day or two."
0,"The repository-1.5.dtd is not well formedThe repository-1.5.dtd file at http://jackrabbit.apache.org/dtd/repository-1.5.dtd
is not well formed at the time of this writing 200/05/23 19:30GMT

1. It looks like a #REQUIRED is missing at line 173
2. Detected this while trying the 5minutes with ocm tutorial

Hope this helps,
 S."
0,"move PerFieldCodecWrapper into codecs packagePerFieldCodecWrapper is a codec, but its 'hardwired' as lucene's only codec currently (except for PreFlex/3.x case)

it lets you choose a format for the postings lists per-field.

I think we should move this to the codecs package as a start... just a rote refactor."
0,"Exclude system index for queries that restrict the result set to nodetypes not availble in the ""jcr:system"" subtreeWe already have code that is able to decide whether the system index needs to be included in a search or not (see JCR-967). If I execute a query like ""my:app//element(*, my:doc)"" this will only search the workspace index. Unfortunately this is slower than ""//element(*, my:doc)"", since the first query can not be optimized as the second. In our case both queries return the same result set because we use application specific node types. Even though the second query includes the system index it is still faster than the first one. But it could be even faster because it doesn't need to search the system index because nodes with the application specific node type can't be added to the ""jcr:system""-tree and are therefore are added never to the system index (am I right?)."
0,"sort missing string fields lastA SortComparatorSource for string fields that orders documents with the sort
field missing after documents with the field.  This is the reverse of the
default Lucene implementation.

The concept and first-pass implementation was done by Chris Hostetter."
0,"Incorrect outer join TCK testsThe TCK test cases for outer joins seem to be incorrect. More specifically the expected result sets for the testRightOuterJoin1() and testLeftOuterJoin2() test cases in EquiJoinConditionTest are invalid, as shown below:

* testRightOuterJoin1() result set {{null, n1}, {n1, n2}, {n2, n2}} --> The n1 node does not have the propertyName2 property set, so the first tuple can never occur regardless of the join type. And since n2 already matches existing nodes, even {null, n2} can not be included in the result set. The correct result set for this query seems to be {{n1, n2}, {n2, n2}}.

* testLeftOuterJoin2() result set {{n1, null}, {n2, n1}, {n2, n2}} --> Same as above, a tuple with n1 as the leftmost node is not possible. The correct result set would be {{n2, n1}, {n2, n2}}.

Unfortunately the correct result sets here don't actually exercise the outer join functionality, i.e. none of the nodes in the returned tuples are null. We'll need to modify the test case setup to fix this."
0,"Maven artifacts for Lucene 4 are not stored in the correct pathHello,

I would like to use the maven artifacts for Lucene 4.0 produced by the Hudson build machine. The artifacts are correctly produced (http://hudson.zones.apache.org/hudson/view/Lucene/job/Lucene-trunk/lastSuccessfulBuild/artifact/maven_artifacts/lucene/).
However, the artifacts which should be stored under the path ""org/apache/lucene/"" are currently stored under ""lucene"" which prevents a project using maven to correctly download the Lucene 4.0 artifacts.

Thanks again for your help.  "
0,"Javadoc in jackrabbit-jcr-rmi is missing an ending "">"" The javadoc file /jackrabbit-jcr-rmi/src/main/javadoc/apache/rmi/observation/package.html is missing the final "">"" from the ending body tag.
"
0,"Unreachable catch block for NameException in ValueConstraint.javaUnreachable catch block for NameException. Only more specific exceptions are thrown and handled by previous catch block(s). ValueConstraint.java	line 855
"
0,"Make FieldSelector usable from Searchable Seems reasonable that you would want to be able to specify a FieldSelector from Searchable because many systems do not use IndexSearcher (where you can get a Reader), but instead use Searchable or Searcher so that Searchers and MultiSearchers can be used in a polymorphic manner."
0,"Transparent Content Coding supportI would like to see HttpClient features brought up to parity with other libraries, both in Java and other languages. c.f. Python's httplib2 (not yet in the standard library, but many would like to see it in there). That library transparently handles gzip and compress content codings.

This issue is to capture possible solutions to providing this sort of innate functionality in HttpClient, so that users aren't required to know RFC2616 intimately. The HttpClient library should do the right thing and use the network in the most efficient manner possible."
0,"LoginModuleConfig should allow to specify principalProvider-name in addition to the classGilles Metz reported this issue regarding login module configuration with Day's CRX based on Jackrabbit, which in
previous versions allowed to specify multiple prinicpal providers of the same class but with different configurations.
With JR 2.0 and 2.1 this is not supported as the pp class name is used as key in the registry and does not allow
to specify a separate key/name.





"
0,"LargeDocHighlighter - another span highlighter optimized for large documentsThe existing Highlighter API is rich and well designed, but the approach taken is not very efficient for large documents.

I believe that this is because the current Highlighter rebuilds the document by running through and scoring every every token in the tokenstream.

With a break in the current API, an alternate approach can be taken: rebuild the document by running through the query terms by using their offsets. The benefit is clear - a large doc will have a large tokenstream, but a query will likely be very small in comparison.

I expect this approach to be quite a bit faster for very large documents, while still supporting Phrase and Span queries.

First rough patch to follow shortly."
0,"Handling sub-domain cookies.I noticed a difference in behaviour between httpclient and most common browsers 
(IE/Mozilla). If a web site sets a cookie for ""beta.gamma.com"", this cookie is 
not sent in requests to ""alpha.beta.gamma.com"". 
  I am not sure what the cookie specs say, but Mozilla, IE and even 
HTTP::Cookies module in LWP seem to behave differently from HttpClient. 
HttpClient seems to rely on the leading dot in the domain name 
(like "".beta.gamma.com"")."
0,"No documentation on how to use CookieSpecNone of http://hc.apache.org/httpcomponents-client/httpclient/apidocs/org/apache/http/cookie/CookieSpec.html, http://hc.apache.org/httpcomponents-client/httpclient/apidocs/org/apache/http/cookie/CookieSpecFactory.html, http://hc.apache.org/httpcomponents-client/httpclient/apidocs/org/apache/http/cookie/CookieSpecRegistry.html, or http://hc.apache.org/httpcomponents-client/httpclient/apidocs/org/apache/http/impl/client/DefaultHttpClient.html explain how to set the CookieSpec that the HttpClient actually uses. It looks like CookieSpecRegistry might be it, but it doesn't document what the ""names"" mean, so I don't know what to pick to make a factory actually get used."
0,"Build file for Highlighter contrib works when run in isolation, but not when core dist is runBuild.xml for Highlighter does not work when compilation is triggered by clean core dist call.

Patch has changes to fix this by updating build.xml to follow xml-query-parser build.xml"
0,"move o.a.l.index.codecs.* -> o.a.l.codecs.*These package names are getting pretty long, e.g.:

org.apache.lucene.index.codecs.lucene40.values.XXXXYYYY

I think we should move it to just the codecs package now while it won't cause anyone any trouble."
0,"BasicResponseHandler Javadoc Needs ClarificationThe class-level javadoc for BasicResponseHandler indicates that it reads the response body before throwing an Exception for responses with status code >= 300, which is not the case."
0,Migrate to Lucene 2.3
0,"Move *.log files to target/The jackrabbit-core component already puts the derby.log file in target/ along with other build  and test artifacts, but many other components don't do that yet. Having all generated files in target/ is good as it makes it very easy to clean things up. Also things like the RAT checks (JCR-1937) are easier when there's no need to worry about such extra files.
"
0,"Allow reuse of Q*DefinitionBuilder in QItemDefinitionsBuilderIt would be nice to reuse the builder implementations in QItemDefinitionsBuilder elsewhere when Q*Definitions need to be built.

I will extract the relevant classes so they can be used independently and make QItemDefinitionsBuilder use them."
0,"Fix PayloadProcessorProvider to no longer use Directory for lookup, instead AtomicReaderThe PayloadProcessorProvider has a broken API, this should be fixed. The current trunk mimics the old behaviour, but not 100%.

The PayloadProcessorProvider API should return a PayloadProcessor based on the AtomicReader instance that gets merged. As AtomicReader do no longer know the directory they are reside (they could be e.g. FilterIndexReaders, MemoryIndexes,...) a selection by Directory is no longer possible.

The current code in Lucene trunk mimics the old behavior by doing an instanceof SegmentReader check and then asking for a DirProvider. If something else is merged in, Payload processing is not supported. This should be changed, the old API could be kept backwards compatible by moving the instanceof check in a ""convenience class"" DirPayloadProcessorProvider, extending PayloadProcessorProvider."
0,"Poor performance race condition in FieldCacheImplA race condition exists in FieldCacheImpl that causes a significant performance degradation if multiple threads concurrently request a value that is not yet cached. The degradation is particularly noticable in large indexes and when there a many concurent requests for the cached value.

For the full discussion see the mailing list thread 'Poor performance ""race condition"" in FieldSortedHitQueue' (http://www.gossamer-threads.com/lists/lucene/java-user/38717)."
0,"Improved log message: include pathThe cluster logs a message for each appended operation. The log message is currently the revision number. A more interesting log message would be the user name, and the path of the change (the most specific path if the change contains multiple nodes)."
0,JSR 283: Simple versioning
0,"Things to be done now that Filter is independent from BitSet(Aside: where is the documentation on how to mark up text in jira comments?)

The following things are left over after LUCENE-584 :

For Lucene 3.0  Filter.bits() will have to be removed.

There is a CHECKME in IndexSearcher about using ConjunctionScorer to have the boolean behaviour of a Filter.

I have not looked into Filter caching yet, but I suppose there will be some room for improvement there.
Iirc the current core has moved to use OpenBitSetFilter and that is probably what is being cached.
In some cases it might be better to cache a SortedVIntList instead.

Boolean logic on DocIdSetIterator is already available for Scorers (that inherit from DocIdSetIterator) in the search package. This is currently implemented by ConjunctionScorer, DisjunctionSumScorer,
ReqOptSumScorer and ReqExclScorer.
Boolean logic on BitSets is available in contrib/misc and contrib/queries

DisjunctionSumScorer calls score() on its subscorers before the score value actually needed.
This could be a reason to introduce a DisjunctionDocIdSetIterator, perhaps as a superclass of DisjunctionSumScorer.

To fully implement non scoring queries a TermDocIdSetIterator will be needed, perhaps as a superclass of TermScorer.

The javadocs in org.apache.lucene.search using matching vs non-zero score:
I'll investigate this soon, and provide a patch when necessary.

An early version of the patches of LUCENE-584 contained a class Matcher,
that differs from the current DocIdSet in that Matcher has an explain() method.
It remains to be seen whether such a Matcher could be useful between
DocIdSet and Scorer.

The semantics of scorer.skipTo(scorer.doc()) was discussed briefly.
This was also discussed at another issue recently, so perhaps it is wortwhile to open a separate issue for this.

Skipping on a SortedVIntList is done using linear search, this could be improved by adding multilevel skiplist info much like in the Lucene index for documents containing a term.

One comment by me of 3 Dec 2008:

A few complete (test) classes are deprecated, it might be good to add the target release for removal there.
"
0,"Node type documentation tool (NTDoc)This is the first time I post a contrib here on Jira. Hope I do this the right way :-) 

Some weeks ago I postet a message on the forum about a node type documentation tool I had made. Now, finally I have cleaned up the code and fixed some bugs. It is now useful for the majority out there. I do not guarantee it to be bug-free, but will do my best to fix any bugs that is reported. 

A readme file is included in the distribution. Must build using maven 2. Have not done it maven 1 compliant."
0,More verbose message on reference constraint violation
0,"Provide limit on phrase analysis in FastVectorHighlighterWith larger documents, FVH can spend a lot of time trying to find the best-scoring snippet as it examines every possible phrase formed from matching terms in the document.  If one is willing to accept
less-than-perfect scoring by limiting the number of phrases that are examined, substantial speedups are possible.  This is analogous to the Highlighter limit on the number of characters to analyze.

The patch includes an artifical test case that shows > 1000x speedup.  In a more normal test environment, with English documents and random queries, I am seeing speedups of around 3-10x when setting phraseLimit=1, which has the effect of selecting the first possible snippet in the document.  Most of our sites operate in this way (just show the first snippet), so this would be a big win for us.

With phraseLimit = -1, you get the existing FVH behavior. At larger values of phraseLimit, you may not get substantial speedup in the normal case, but you do get the benefit of protection against blow-up in pathological cases.
"
0,"[PATCH] to store binary fields with compressionhi all,

as promised here is the enhancement for the binary field patch with optional
compression. The attachment includes all necessary diffs based on the latest
version from CVS. There is also a small junit test case to test the core
functionality for binary field compression. The base implementation for binary
fields where this patch relies on, can be found in patch #29370. The existing
unit tests pass fine.

For testing binary fields and compression, I'm creating an index from 2700 plain
text files (avg. 6kb per file) and store all file content within that index
without using compression. The test was created using the IndexFiles class from
the demo distribution. Setting up the index and storing all content without
compression took about 60 secs and the final index size was 21 MB. Running the
same test, switching compression on, the time to index increase to 75 secs, but
the final index size shrinks to 13 MB. This is less than the plain text files
them self need in the file system (15 MB)

Hopefully this patch helps people dealing with huge index and want to store more
than just 300 bytes per document to display a well formed summary.

regards
Bernhard"
0,"Connection not closed after ""Connection: close"" requestIn HTTP specification at http://www.w3.org/Protocols/rfc2616/rfc2616-
sec8.html , under chapter ""Negotiation"", it is stated :
""If either the client or the server sends the close token in the Connection 
header, that request becomes the last one for the connection.""

HttpClient (v2.0.2 and v3.0 alpha2) is currently closing connection only if 
server has sent ""Connection: close"" header, and not when request contains it."
0,"Sandbox remaining contrib queriesIn LUCENE-3271, I moved the 'good' queries from the queries contrib to new destinations (primarily the queries module).  The remnants now need to find their home.  As suggested in LUCENE-3271, these classes are not bad per se, just odd.  So lets create a sandbox contrib that they and other 'odd' contrib classes can go to.  We can then decide their fate at another time."
0,"Exception root cause is swallowed in various placesWhen re-throwing an exception, the root cause is swallowed in some places in Jackrabbit, mainly when converting to an IOException."
0,"Similarity.lengthNorm and positionIncrement=0Calculation of lengthNorm factor should in some cases take into account the number of tokens with positionIncrement=0. This should be made optional, to support two different scenarios:

* when analyzers insert artificially constructed tokens into TokenStream (e.g. ASCII-fied versions of accented terms, stemmed terms), and it's unlikely that users submit queries containing both versions of tokens: in this case lengthNorm calculation should ignore the tokens with positionIncrement=0.

* when analyzers insert synonyms, and it's likely that users may submit queries that contain multiple synonymous terms: in this case the lengthNorm should be calculated as it is now, i.e. it should take into account all terms no matter what is their positionIncrement.

The default should be backward-compatible, i.e. it should count all tokens.

(See also the discussion here: http://markmail.org/message/vfvmzrzhr6pya22h )"
0,"The 1.5.0 webapp points to 1.4 javadocsThere's a ""Jackrabbit API"" link on the Jackrabbit webapp 1.5.0 that points to http://jackrabbit.apache.org/api/1.4/. It should be updated to point to http://jackrabbit.apache.org/api/1.5/."
0,"Support for new Resources model in ant 1.7 in Lucene ant task.Ant Task for Lucene should use modern Resource model (not only FileSet child element).
There is a patch with required changes.

Supported by old (ant 1.6) and new (ant 1.7) resources model:
<index ....> <!-- Lucene Ant Task -->
  <fileset ... />
</index> 

Supported only by new (ant 1.7) resources model:
<index ....> <!-- Lucene Ant Task -->
  <filelist ... />
</index> 

<index ....> <!-- Lucene Ant Task -->
  <userdefinied-filesource ... />
</index> "
0,"Improved XML export handlingAs mentioned in JCR-1574, the current XML export functionality is generating workarounds like the new PropertyWrapper class. I'd like to refactor and clean up the XML export stuff so that such workarounds wouldn't be needed.

An additional bonus would be to make both core and jcr2spi use the same XML export mechanism. For example the one in core already supports JSR 283 shareable nodes, but the one in jcr2spi does not."
0,"Review pck names in the others ocm subprojectsReview package structure and graffito references in the other OCM subprojects : jcr-nodemanagement & spring. 
"
0,"Connection is not released back to the pool if a runtime exception is thrown in HttpMethod#releaseConnection methodthe default config of leaving the HttpClientParams.CONNECTION_MANAGER_TIMEOUT as zero means 
that the first time the connection manager fails to immediately get a connection you application hangs. 
(at least using MultiThreadedHttpConnectionManager.)

this is because the zero gets passed onto a call to Object.wait(long timeout) and, from the docs, ""If 
timeout is zero, however, then real time is not taken into consideration and the thread simply waits 
until notified."". 

since nothing ever ""notify()""s the thread everything just stops...

the default behaviour of the client more should be more predictable. you don't expect it to hang your 
entire app if it can't get a connection, you expect it to timeout then throw an exception or give some 
other kind of feedback.

it would make sense to give a default of, say, arbitrarily, 10 seconds or so. this would save every single 
user of the classes having to dig around in the code/documentation and explictly set this param. they 
might decide that the default value isn't right and hence change it, but that's tweaking behaviour, not 
correcting it. i certainly thought it was a bug in the code (yours or mine), not my config and have been 
fretting around it for a while.

best,
garry"
0,"Remove code duplication from Token class, just extend TermAttributeImplThis issue removes the code duplication from Token, as it shares the whole char[] buffer handling code with TermAttributeImpl. This issue removes this duplication by just extending TermAttributeImpl.

When the parent issue LUCENE-2302 will extend TermAttribute to support CharSequence and Appendable and also the new BytesRefAttribute gets added, Token will automatically provide this too, so no further code duplication.

This code should also be committed to trunk, as it has nothing to do with flex."
0,Enable DocValues by default for every CodecCurrently DocValues are enable with a wrapper Codec so each codec which needs DocValues must be wrapped by DocValuesCodec. The DocValues writer and reader should be moved to Codec to be enabled by default.
0,Random access non RAM resident IndexDocValues (CSF)There should be a way to get specific IndexDocValues by going through the Directory rather than loading all of the values into memory.
0,"Add shutdown method to SimpleHttpConnectionManagerIt would be useful to be able to close the connection in the
SimpleHttpConnectionManager. This could be achieved by adding a shutdown()
method as per the MultiThreadedConnectionManager.

Ideally this would be added to the HttpConnection interface, but this could
break existing implementations.

To avoid this, perhaps consider introducing a sub-interface with the method in it.

[Could also create an AbstractConnectionManager class - this would make it
easier to add more functions later]"
0,"Safe namespace registrationThe namespace registration methods provided by the JCR NamespaceRegistry API are cumbersome to use and vulnerable to race conditions in the event of conflicting prefix mappings. This problem was discussed lately on the mailing list (see http://article.gmane.org/gmane.comp.apache.jackrabbit.devel/6805) and one symptom of the problem is the new code in NodeTypeManagerImpl (see JCR-349):

    //  Registers a namespace...
    try {
        nsReg.getPrefix(uri);
    } catch (NamespaceException e1) {
        String original = prefix;
        for (int i = 2; true; i++) {
            try {
                nsReg.registerNamespace(prefix, uri);
                return;
            } catch (NamespaceException e2) {
                prefix = original + i;
            }
        }
    }

We should add an internal convenience method like NamespaceRegistryImpl.safeRegisterNamespace(String prefixHint, String uri) that, instead of throwing an exception, would automatically generate and use a unique prefix based on the given hint when a prefix conflict is detected."
0,"ChildNodeEntriesImpl.update logs incorrect errorsThe ChildNodeEntriesImpl logs errors on a correct update.

""ChildInfo iterator contains multiple entries with the same name|index or uniqueID -> ignore ChildNodeInfo.""  (line 186)"
0,"FieldInfos should be read-only if loaded from diskCurrently FieldInfos create a private FieldNumberBiMap when they are loaded from a directory which is necessary due to some limitation we need to face with IW#addIndexes(Dir). If we add an index via a directory to an existing index field number can conflict with the global field numbers in the IW receiving the directories. Those field number conflicts will remain until those segments are merged and we stabilize again based on the IW global field numbers. Yet, we unnecessarily creating a BiMap here where we actually should enforce read-only semantics since nobody should modify this FieldInfos instance we loaded from the directory. If somebody needs to get a modifiable copy they should simply create a new one and all all FieldInfo instances to it.

"
0,"Multi-level skipping on posting listsTo accelerate posting list skips (TermDocs.skipTo(int)) Lucene uses skip lists. 
The default skip interval is set to 16. If we want to skip e. g. 100 documents, 
then it is not necessary to read 100 entries from the posting list, but only 
100/16 = 6 skip list entries plus 100%16 = 4 entries from the posting list. This 
speeds up conjunction (AND) and phrase queries significantly.

However, the skip interval is always a compromise. If you have a very big index 
with huge posting lists and you want to skip over lets say 100k documents, then 
it is still necessary to read 100k/16 = 6250 entries from the skip list. For big 
indexes the skip interval could be set to a higher value, but then after a big 
skip a long scan to the target doc might be necessary.

A solution for this compromise is to have multi-level skip lists that guarantee a 
logarithmic amount of skips to any target in the posting list. This patch 
implements such an approach in the following way:

  Example for skipInterval = 3:
                                                      c            (skip level 2)
                  c                 c                 c            (skip level 1) 
      x     x     x     x     x     x     x     x     x     x      (skip level 0)
  d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d  (posting list)
      3     6     9     12    15    18    21    24    27    30     (df)
 
  d - document
  x - skip data
  c - skip data with child pointer
 
Skip level i contains every skipInterval-th entry from skip level i-1. Therefore the 
number of entries on level i is: floor(df / ((skipInterval ^ (i + 1))).
 
Each skip entry on a level i>0 contains a pointer to the corresponding skip entry in 
list i-1. This guarantees a logarithmic amount of skips to find the target document.


Implementations details:

   * I factored the skipping code out of SegmentMerger and SegmentTermDocs to 
     simplify those classes. The two new classes AbstractSkipListReader and 
	 AbstractSkipListWriter implement the skipping functionality.
   * While AbstractSkipListReader and Writer take care of writing and reading the 
     multiple skip levels, they do not implement an actual skip data format. The two 
	 new subclasses DefaultSkipListReader and Writer implement the skip data format 
	 that is currently used in Lucene (with two file pointers for the freq and prox 
	 file and with payload length information). I added this extra layer to be 
	 prepared for flexible indexing and different posting list formats. 
      
   
File format changes: 

   * I added the new parameter 'maxSkipLevels' to the term dictionary and increased the
     version of this file. If maxSkipLevels is set to one, then the format of the freq 
	 file does not change at all, because we only have one skip level as before. For 
	 backwards compatibility maxSkipLevels is set to one automatically if an index 
	 without the new parameter is read. 
   * In case maxSkipLevels > 1, then the frq file changes as follows:
     FreqFile (.frq) --> <TermFreqs, SkipData>^TermCount
	 SkipData        --> <<SkipLevelLength, SkipLevel>^(Min(maxSkipLevels, 
	                       floor(log(DocFreq/log(skipInterval))) - 1)>, SkipLevel>
	 SkipLevel       --> <SkipDatum>^DocFreq/(SkipInterval^(Level + 1))

	 Remark: The length of the SkipLevel is not stored for level 0, because 1) it is not 
	 needed, and 2) the format of this file does not change for maxSkipLevels=1 then.
	 
	 
All unit tests pass with this patch."
0,Move Function grouping collectors from Solr to grouping moduleMove the Function*Collectors from Solr (inside Grouping source file) to grouping module.
0,"TextFilterService uses Sun specific classesThe TextFilterService uses the Sun specific and actually undocumented class sun.misc.Service class to lookup TextFilter implementations. This approach will not work on all JVM implementations.

The Service should rather use javax.imageio.spi.ServiceRegistry, which is part of the standard J2SE API."
0,"URI uses  sun.security.action.GetPropertyActionURI uses a sun.* class but should not.  Use of this class should be removed.

Reported my Mark Wilcox"
0,"Configuration of CacheManager memory sizes(I already posted this as comments under JCR-619.)

The maximum size for all caches in CacheManager is hardcoded to 16 megabytes and there's no way to change that. It would be nice if this as well as other CacheManager parameters were configurable. It's just a waste running Jackrabbit on a server with gigabytes of memory and only using 16 megabytes for cache...

I have created a really simple and straightforward patch (jackrabbit-cachemanager-config.patch) which enables reaching the CacheManager instance through RepositoryImpl object and setting all three of its memory parameters. The memory parameters are no longer static constants, but instance fields getting initial values from constants (so the default behavior of the class remains the same).

(It would be even nicer if these parameters were configurable via configuration files, but that should probably be implemented by someone close to the project.)"
0,"o.a.jackrabbit.spi.commons.conversion.NameParser should not assume that namespace URI's are registeredaccording to JCR 2.0, ""3.4.3.4 Parsing Lexical Paths"":

<quote>
An otherwise valid path containing an expanded name with an unregistered 
namespace URI will always resolve into a valid internal representation of a path 
</quote>

the current implementation assumes that namespace URIs encountered in 
expanded form names are registered, otherwise the name is treated as
qualified name. "
0,"MemcachedHttpCacheStorage should throw IOExceptions instead of Runtime ExceptionsThe MemcachedHttpCacheStorage class implements HttpCacheStorage which defines that methods will throw IOExceptions, but the underlying net.spy.memcached.MemcachedClientIF throws runtime exceptions. These exceptions are not caught in the code where IOExceptions are expected causing these exception bubble up to the calling code. It seems like the MemcachedHttpCacheStorage class should treat at least some of these runtime exceptions as IOExceptions so that normal code execution paths can be followed.  

I'm proposing that MemcachedHttpCacheStorage treat a OperationTimeoutException from the memcached client as an IOException. This would allow the existing CachingHttpClient code to catch and log the exception as a warning, instead of bubbling the exception up the calling code.
"
0,"Remove lib directory from SVN trunkbuild.xml expects to find junit.jar in the lib directory for building and running tests.

The jar is not included in SVN, but nor is the jar ignored, so when it is downloaded it shows up as an unversioned file.

The file should be included in or excluded from SVN.

==

Note: In JMeter we use a lib/opt directory.
This is present in SVN - but all contents are ignored.

This can be used for extra jars that cannot be or are not included in SVN.

Could use the same approach for junit.jar..."
0,"Move UnInvertedField into Lucene coreSolr's UnInvertedField lets you quickly lookup all terms ords for a
given doc/field.

Like, FieldCache, it inverts the index to produce this, and creates a
RAM-resident data structure holding the bits; but, unlike FieldCache,
it can handle multiple values per doc, and, it does not hold the term
bytes in RAM.  Rather, it holds only term ords, and then uses
TermsEnum to resolve ord -> term.

This is great eg for faceting, where you want to use int ords for all
of your counting, and then only at the end you need to resolve the
""top N"" ords to their text.

I think this is a useful core functionality, and we should move most
of it into Lucene's core.  It's a good complement to FieldCache.  For
this first baby step, I just move it into core and refactor Solr's
usage of it.

After this, as separate issues, I think there are some things we could
explore/improve:

  * The first-pass that allocates lots of tiny byte[] looks like it
    could be inefficient.  Maybe we could use the byte slices from the
    indexer for this...

  * We can improve the RAM efficiency of the TermIndex: if the codec
    supports ords, and we are operating on one segment, we should just
    use it.  If not, we can use a more RAM-efficient data structure,
    eg an FST mapping to the ord.

  * We may be able to improve on the main byte[] representation by
    using packed ints instead of delta-vInt?

  * Eventually we should fold this ability into docvalues, ie we'd
    write the byte[] image at indexing time, and then loading would be
    fast, instead of uninverting
"
0,Add a ton of missing license headers throughout test/demo/contrib
0,"Update idea plugin versionWe are using a quite outdated version (2.0). The most recent idea plugin release is 2.2.

Index: jackrabbit-parent/pom.xml
===================================================================
--- jackrabbit-parent/pom.xml	(revision 802755)
+++ jackrabbit-parent/pom.xml	(working copy)
@@ -73,7 +73,7 @@
       <plugin>
         <!-- http://maven.apache.org/plugins/maven-idea-plugin/ -->
         <artifactId>maven-idea-plugin</artifactId>
-        <version>2.0</version>
+        <version>2.2</version>
         <configuration>
           <downloadSources>true</downloadSources>
           <jdkLevel>1.5</jdkLevel>
"
0,"BundleFsPersistenceManager: remove deprecated settingsSome settings of the BundleFsPersistenceManager are not used internally and should be removed:

blobFSInitialCache, blobFSMaximumCache, itemFSBlockSize, itemFSInitialCache, itemFSMaximumCache"
0,"Remove per-document multiply in FilteredQuerySpinoff of LUCENE-1536.

In LUCENE-1536, Uwe suggested using FilteredQuery under-the-hood to implement filtered search.

But this query is inefficient, it does a per-document multiplication (wrapped.score() * boost()).

Instead, it should just pass the boost down in its weight, like BooleanQuery does to avoid this per-document multiply."
0,"Upgrade to latest SLF4J and LogbackWhile fixing JCR-2836 I ran into LBCLASSIC-183 [1] that's fixed in a recent Logback release. To get this and other fixes we should upgrade Logback and SLF4J in Jackrabbit 2.3.

[1] http://jira.qos.ch/browse/LBCLASSIC-183"
0,Convert Batch implementation in spi-rmi from remote object into a local oneThe current implementation of the Batch interface in spi-rmi is very simple and just uses remotes to the server side batch. This should be changed to a local object on the client and only transmit the changes in a single call to the server on save.
0,"FastVectorHighlighter: add a FragmentBuilder to return entire field contentsIn Highlightrer, there is a Nullfragmenter. There is a requirement its counterpart in FastVectorhighlighter."
0,"warn on invalid set-cookie headerI had a problem on a WS server that comes from some proxy misconfiguration...
resulting in this reponse beeing received by HTTPclient :
17:26:36,489 DEBUG [header] << ""HTTP/1.1 200 OK[\r][\n]""
17:26:36,489 DEBUG [header] << ""Set-Cookie: =f448bb59feedbaaabaee; path=/[\r][\n]""
17:26:36,489 DEBUG [header] << ""Date: Tue, 15 Nov 2005 16:26:36 GMT[\r][\n]""
17:26:36,489 DEBUG [header] << ""Server: Apache[\r][\n]""
17:26:36,489 DEBUG [header] << ""Connection: close[\r][\n]""
17:26:36,489 DEBUG [header] << ""Transfer-Encoding: chunked[\r][\n]""
17:26:36,489 DEBUG [header] << ""Content-Type: text/xml;charset=utf-8[\r][\n]""

The set-cookie header is malformed, as cookie has no name, so the HTTP head may
be considered invalid.

This results in an error when building the NEXT request. I'd expect httpclient
to WARN on malformed header and drop it."
0,"Remove PropDefId and NodeDefIdthe PropDefIds and NodeDefIds are used to quickly lookup a childnode- or property definition in the nodetype registry (or effective nodetype).
this is heavily used during reading, when calling Property.getDefinition() usually when checking the isMultiple() flag. and of course while writing when getting the definition for the property or childnode. 

however, this poses problems when a nodetype is changed that is still used in the content. if a property definition is changed due to an altered nodetype, subsequent accesses to that property result in a ""invalid propdefid"" warning in the log - but the id is recomputed. this is especially a problem when upgrade jackrabbit from 1.x to 2.0, where some of the builtin nodetypes are defined differently.

i think that it should be feasible to remove the propdefids and nodedefids and compute the definition on demand. i think this can be implemented without performance loss, when some sort of 'signatures' of the items are computed to quickly find the definitions in the effective node type. furthermore, the most common usecase for using the property definition is probably the isMultiple() check - which is now on the Property interface itself - which does not need a definition lookup at all.

and last but not least, it saves 8 bytes per item in the persistence layer."
0,"Use a separate JFlex generated Unicode 4 by Java 5 compatible StandardTokenizerThe current trunk version of StandardTokenizerImpl was generated by Java 1.4 (according to the warning). In Java 3.0 we switch to Java 1.5, so we should regenerate the file.

After regeneration the Tokenizer behaves different for some characters. Because of that we should only use the new TokenizerImpl when Version.LUCENE_30 or LUCENE_31 is used as matchVersion."
0,"""reproduce with"" on test failure isn't right if you manually overrided anythingIf you run a test with eg -Dtests.codec=SimpleText...

If it fails, the ""reproduce with"" fails to include that manual override (-Dtests.codec=SimpleText), ie it only includes the seed / test class / test method.  So it won't actually reproduce the fail, in general.

We just need to fix the ""reproduce with"" to add any manual overrides...."
0,"Enable maven-source-pluginCurrently the maven-source-plugin is enabled by default in jackrabbit-jcr-rmi, but it would be good to enable it globally for all Jackrabbit components."
0,"MultiStatus response for PROPPATCH (copied from JCR-175)Rob Owen commented on JCR-175:
--------------------------------------------------

doPropPatch in AbstractWebdavServlet still needs to send back a multistatus (207) response even in the successful case.

I didn't see a way to collect the success/failure status for each property, but instead created a multistatus response and added a propstat (SC_OK) for each of the properties in the setProperties and removeProperties. This allowed a WebDAV client, which expected a multistatus response from PROPPATCH, to work correctly with jcr-server. In the more general case the actual property status code will need to be used ."
0,"Proxy tunneling/auth with CONNECT for non-HTTP protocolsHttpClient would be even more useful if it supported connections tunneled 
through proxies and proxy authentication for non-HTTP protocols. E.g. Binary 
protocols such as SSH or JXTA-TCP could be tunneled through a web proxy if 
HttpClient provided access to the underlying Socket after the negotiations 
(auth, CONNECT) with the web proxy were complete."
0,"random sampler is not random (and so facet SamplingWrapperTest occasionally fails)RandomSample is not random at all:
It does not even import java.util.Random, and its behavior is deterministic.

in addition, the test testCountUsingSamping() never retries as it was supposed to (for taking care of the hoped-for randomness)."
0,"Random Failure TestSizeBoundedOptimize#testFirstSegmentTooLargeI am seeing this on trunk  

{noformat}

[junit] Testsuite: org.apache.lucene.index.TestSizeBoundedOptimize
    [junit] Testcase: testFirstSegmentTooLarge(org.apache.lucene.index.TestSizeBoundedOptimize):	FAILED
    [junit] expected:<2> but was:<1>
    [junit] junit.framework.AssertionFailedError: expected:<2> but was:<1>
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:882)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:848)
    [junit] 	at org.apache.lucene.index.TestSizeBoundedOptimize.testFirstSegmentTooLarge(TestSizeBoundedOptimize.java:160)
    [junit] 
    [junit] 
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.658 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestSizeBoundedOptimize -Dtestmethod=testFirstSegmentTooLarge -Dtests.seed=7354441978302993522:-457602792543755447 -Dtests.multiplier=3
    [junit] NOTE: test params are: codec=Standard, locale=sv_SE, timezone=Mexico/BajaNorte
    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestSizeBoundedOptimize]
    [junit] ------------- ---------------- ---------------
    [junit] Test org.apache.lucene.index.TestSizeBoundedOptimize FAILED
{noformat}

when running with this seed
ant test -Dtestcase=TestSizeBoundedOptimize -Dtestmethod=testFirstSegmentTooLarge -Dtests.seed=7354441978302993522:-457602792543755447 -Dtests.multiplier=3"
0,"Observation logs error when a node is moved in placeAn error message is written to the log when the following sequence of operations is executed:

- create node 'parent'
- create node 'child' as a child of 'parent'
- save
- create node 'tmp'
- move 'child' under 'tmp'
- remove 'parent'
- move 'tmp' to former path of 'parent'

The log will say: EventStateCollection: Unable to calculate old path of moved node

This is because the zombie path of 'child' is equal to the new path after the move. The EventStateCollection detects a new parentId assigned to 'child' and expects a new path that is different from the zombie path. The above case however shows that there is a use case where the paths are equal and events should be generated."
0,Add customizable filtering to GQLCurrently GQL is not very flexible because it does not have any hooks that  allows you to modify the query that gets generated from the GQL syntax. As a first step I'd like to introduce a filtering mechanism that can be used to post process the result set and exclude certain rows. This is useful when you cannot express an application constraint in GQL.
0,"SQL2 parser may infer type for UncastLiteral from static analysisThe spec says:

""An UncastLiteral is always interpreted as a Value of property type STRING. A CastLiteral, on the other hand, is interpreted as the string form of a Value of the PropertyType indicated.""

There are also two test cases in NodeNameTest that need to be fixed accordingly: testLongLiteral and testBooleanLiteral
"
0,"DirListingExportHandler: Should not implement PropertyHandlerissue found by Roland Porath:

if the DirListingExportHandler is used with some other collection nodetype that nt:folder (that may allow other properties) the list of dav properties obtained upon PROPFIND (being delegated to PropertyHandler) results in an imcomplete list.

since the only benefit of the DirListingExportHandler is to display something nice(?) upon a GET to a folder, i'd suggest to remove the implementation of PropertyHandler from the DirListingExportHandler.

angela"
0,"TaxonomyReader/Writer and their Lucene* implementationThe facet module contains two interfaces TaxonomyWriter and TaxonomyReader, with two implementations Lucene*. We've never actually implemented two TaxonomyWriters/Readers, so I'm not sure if these interfaces are useful anymore. Therefore I'd like to propose that we do either of the following:

# Remove the interfaces and remove the Lucene part from the implementation classes (to end up with TW/TR impls). Or,
# Keep the interfaces, but rename the Lucene* impls to Directory*.

Whatever we do, I'd like to make the impls/interfaces impl also TwoPhaseCommit.

Any preferences?"
0,"benchmark/stats package is obsolete and unused - remove itThis seems like a leftover from the original benchmark implementation and can thus be removed.
"
0,"Lucene Java Site docsIt would be really nice if the Java site docs where consistent with the rest of the Lucene family (namely, with navigation tabs, etc.) so that one can easily go between Nutch, Hadoop, etc."
0,"Configure Maximum Connection LifetimesProvide a means of configuring a maximum lifetime for HttpClient connections.  Currently, it would appear as long as a connection is used it may persist indefinitely.

This would be useful for situations where HttpClient needs to react to DNS changes, such as the following situation that may occur when using DNS load balancing:
 - HttpClient maintains connections to example.com which resolves to IP A
 - Machine at IP A fails, and example.com now resolves to backup machine at IP B
 - Since IP A is failing, connections are destroyed, and new connections are made to IP B
 - Machine at IP A recovers, but HttpClient maintains connections to IP B since the connections are still healthy

The desired behavior would be that connections to IP B will reach their connection lifetime, and new connections could be created back to IP A according to the updated DNS settings."
0,"'ant javacc' in root project should also properly create contrib/queryparser Java files'ant javacc' in the project root doesn't run javacc in contrib/queryparser
'ant javacc' in contrib/queryparser does not properly create the Java files. What still needs to be done by hand is (partly!) described in contrib/queryparser/README.javacc. I think this process should be automated. Patch provided."
0,Annotation based implementation of jackrabbit ocmwe have created an annotation based implementation of jackrabbit-ocm that can be used instead of the digester one
0,Apply the supplied patch. Sets 2 variable in the base class to protectedThe patch attached to the main task contains minimal changes to allow the HttpMethodBase class to be overloaded by base class.
0,"jcr-commons: add cnd writer functionalitycurrently jcr-commons only provides an cnd-reader while the writer functionality is only present in spi-commons.
for JCR-2948 a implementation independent cnd-writer would be useful and i would therefore suggest to
add this to jcr-commons based on the code present in spi-commons and let the implementation in spi-commons
extend from the general functionality."
0,"J2EE FORM authentication (also affects pluggable authentication)Add support for J2EE style FORM authentication type. 

Unlike the BASIC and DIGEST types this is not handled by HTTP headers so needs
an adjustment to the way in which the authentication is sent. As far as i can
tell from my testing with one or two J2EE servers the way to successfully login
requires request of a protected page which will respond with the login FORM and
then the submission of that form. The two requests must be associated with one
another using the jsessionid cookie.

It seems to me that this 'bug' must be solved in cooperation with the recent
discussions of pluggable authentication module. i suggestion the following
signature: 

PluggableAuthenticator.authenticate(HttpMethod method, HttpState state). 

This mirrors the existing Authenticator method but also requires change to the
state object to allow access to the connection properties (i dont know how this
affects MultiClient). Alternately we could go for: 

PluggableAuthenticator.authenticate(HttpMethod method, HttpClient client).

In either case Authenticator needs a way to know which plugin to call. I suggest
modification of HttpMethodBase to detect the 'j_security_check' form action in
the response and automatically submit credentials if they are provided using the
new class 

J2EEFormAuthenticator implements PluggableAuthenticator."
0,Improve Javadoc
0,"File Formats Documentation is not correct for Term VectorsFrom Samir Abdou on the dev mailing list:

Hi, 

There is an inconsistency between the files format page (from Lucene
website) and the source code. It concerns the positions and offsets of term
vectors. It seems that documentation (website) is not up to date. According
to the file format page, offsets and positions are not stored! Is that
correct?

Many thanks,

Samir
-----
Indeed, in the file formats term vectors section it doesn't talk about the storing of position and offset info.
"
0,"clean up build files so contrib tests are run more easilyPer mailing list discussion...

http://www.nabble.com/Tests%2C-Contribs%2C-and-Releases-tf3768924.html#a10655448

Tests for contribs should be run when ""ant test"" is used,  existing ""test"" target renamed to ""test-core""
"
0,Use only one scheduler for repository tasksThere are still a few Timer instances being used by Jackrabbit. It would be better if all tasks were scheduled by the central ScheduledExecutorService thread pool of the repository.
0,Remove unnecessary array wrapping when calling varargs methodsvarargs method callers don't have to wrap args in arrays
0,"should allow receiving secure cookies from non-secure chanelCurrently, httpclient will throw an exception if a secure cookie is received 
from a non-secure chanel. Although RFC doesn't specify explicitly on if the 
client should allow receiving secure cookie from non-secure channel, the 
default setting in browser seems to allow it.

Try the following link in IE:

http://www.snapfish.com

The default cookie policy in httpclient should be the same."
0,Enable DataStore in default configurationCurrently the default configuration causes binary properties to be stored in the derby database. This is very inefficient. The standalone server (and the web application it contains) should run with a reasonable default configuration. 
0,"Change default value of SearchIndex extractorPoolSizeThe current default value for the extractorPoolSize is 0, which means it is disabled by default. I think we should change that default because it is a useful feature and people should not have to dig through documentation to make use of it.

The new default should be computed based on the available processors. I suggest we use: 2 * Runtime.availableProcessors()"
0,"Add support for 3.0 indexes in 2.9 branchThere was a lot of user requests to be able to read Lucene 3.0 indexes also with 2.9. This would make the migration easier. There is no problem in doing that, as the new stored fields version in Lucene 3.0 is only used to mark a segment's stored fields file as no longer containing compressed fields. But index format did not really change. This patch simply allows FieldsReader to pass a Lucene 3.0 version number, but still writes segments in 2.9 format (as you could suddenly turn on compression for added documents).

I added ZIP files for 3.0 indexes for TestBackwards. Without the patch it does not pass, as FieldsReader complains about incorrect version number (although it could read the file easily). If we would release maybe a 2.9.4 release of Lucene we should include that patch."
0,Add path encoding to ISO9075The utility class ISO9075 only allows you to encode and decode names. It should also have methods that allow you to pass a path. This is useful when a XPath query is created with a path constraint based on e.g. a Node.getPath().
0,"JCA project tests assume Windows pathsThe org.apache.jackrabbit.jca package in the top-level jca directory has unit tests that assume a Windows environment. It should be fixed to work in any environment. The best solution may be to use a test repository configuration file in the current directory.

The following is the start of the test case failures that I got running on MacOS X.

Testsuite: org.apache.jackrabbit.jca.test.ConnectionFactoryTest
Tests run: 3, Failures: 0, Errors: 3, Time elapsed: 0.778 sec

Testcase: testAllocation(org.apache.jackrabbit.jca.test.ConnectionFactoryTest): Caused an ERROR
org.apache.jackrabbit.core.config.ConfigurationException: Configuration file could not be read.: /Users/mark/Documents/school/whisper/jackrabbit/jackrabbit-trunk/jca/c:\dev\jcr\repository.xml (N
o such file or directory): /Users/mark/Documents/school/whisper/jackrabbit/jackrabbit-trunk/jca/c:\dev\jcr\repository.xml (No such file or directory)
org.apache.jackrabbit.core.config.ConfigurationException: Configuration file could not be read.: /Users/mark/Documents/school/whisper/jackrabbit/jackrabbit-trunk/jca/c:\dev\jcr\repository.xml (N
o such file or directory): /Users/mark/Documents/school/whisper/jackrabbit/jackrabbit-trunk/jca/c:\dev\jcr\repository.xml (No such file or directory)
        at org.apache.jackrabbit.jca.JCAManagedConnectionFactory.createRepository(JCAManagedConnectionFactory.java:278)
        at org.apache.jackrabbit.jca.JCAManagedConnectionFactory.createConnectionFactory(JCAManagedConnectionFactory.java:116)
        at org.apache.jackrabbit.jca.JCAManagedConnectionFactory.createConnectionFactory(JCAManagedConnectionFactory.java:108)
        at org.apache.jackrabbit.jca.test.ConnectionFactoryTest.testAllocation(ConnectionFactoryTest.java:43)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
"
0,"Optimize bundle serializationThere are a number of ways we could use to make bundle serialization more optimized. Thomas has already done some work on this in the Jackrabbit 3 sandbox, and I'd like to apply some of the optimizations also to the trunk."
0,Interface TermFreqVector has incomplete JavadocsWe should improve the Javadocs of org.apache.lucene.index.TermFreqVector
0,"Redesign NodeInfo.getReferences()The method returns an array of PropertyIds. When there are lots of references this may become an problem. As with any other return value that potentially is large we should return an iterator.

I suggest to redesign the handling of references in line with recent discussions how child infos are handled.

- A NodeInfo implementation must either return the complete list of PropertyIds or null if it does not want to return the PropertyIds at that time.
- Introduce a new method: Iterator<PropertyId> RepositoryService.getReferences(SessionInfo, NodeId)

This has the following advantages:

- loading of references can be delayed until it is really needed
- large collections of references can be streamed through the SPI"
0,"SQL2 queries are not loggedSQL2 queries are constructed via QueryObjectModel, and ran via QueryObjectModelImpl which does not log the run time.
I'll attach a run time log similar to the old one."
0,"TCK: NodeTest#testAddNodeItemExistsException fails if validation deferred until saveThe test expects addNode to fail if a same-name sibling already exists.  JSR-170 allows this validation to be deferred until save.

Proposal: call save in the ""try"" block.

--- NodeTest.java       (revision 422074)
+++ NodeTest.java       (working copy)
@@ -380,6 +391,7 @@
         try {
             // try to add a node with same name again
             defaultTestNode.addNode(nodeName3, testNodeType);
+            defaultRootNode.save();
             fail(""Adding a node to a location where same name siblings are not allowed, but a node with same name"" +
                     "" already exists should throw ItemExistsException "");
         } catch (ItemExistsException e) {
"
0,Documentation - jackrabbit features beyond the spec
0,"Eliminate synchronization contention on initial index reading in TermInfosReader ensureIndexIsRead synchronized method ensureIndexIsRead in TermInfosReader causes contention under heavy load

Simple to reproduce: e.g. Under Solr, with all caches turned off, do a simple range search e.g. id:[0 TO 999999] on even a small index (in my case 28K docs) and under a load/stress test application, and later, examining the Thread dump (kill -3) , many threads are blocked on 'waiting for monitor entry' to this method.

Rather than using Double-Checked Locking which is known to have issues, this implementation uses a state pattern, where only one thread can move the object from IndexNotRead state to IndexRead, and in doing so alters the objects behavior, i.e. once the index is loaded, the index nolonger needs a synchronized method. 

In my particular test, this uncreased throughput at least 30 times.

"
0,"Work around ThreadLocal's ""leak""Java's ThreadLocal is dangerous to use because it is able to take a
surprisingly very long time to release references to the values you
store in it.  Even when a ThreadLocal instance itself is GC'd, hard
references to the values you had stored in it are easily kept for
quite some time later.

While this is not technically a ""memory leak"", because eventually
(when the underlying Map that stores the values cleans up its ""stale""
references) the hard reference will be cleared, and GC can proceed,
its end behavior is not different from a memory leak in that under the
right situation you can easily tie up far more memory than you'd
expect, and then hit unexpected OOM error despite allocating an
extremely large heap to your JVM.

Lucene users have hit this many times.  Here's the most recent thread:

  http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200809.mbox/%3C6e3ae6310809091157j7a9fe46bxcc31f6e63305fcdc%40mail.gmail.com%3E

And here's another:

  http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200807.mbox/%3CF5FC94B2-E5C7-40C0-8B73-E12245B91CEE%40mikemccandless.com%3E

And then there's LUCENE-436 and LUCENE-529 at least.

A google search for ""ThreadLocal leak"" yields many compelling hits.

Sun does this for performance reasons, but I think it's a terrible
trap and we should work around it with Lucene."
0,"Use NativeFSLockFactory as default for new API (direct ctors & FSDir.open)A user requested we add a note in IndexWriter alerting the availability of NativeFSLockFactory (allowing you to avoid retaining locks on abnormal jvm exit). Seems reasonable to me - we want users to be able to easily stumble upon this class. The below code looks like a good spot to add a note - could also improve whats there a bit - opening an IndexWriter does not necessarily create a lock file - that would depend on the LockFactory used.


{code}  <p>Opening an <code>IndexWriter</code> creates a lock file for the directory in use. Trying to open
  another <code>IndexWriter</code> on the same directory will lead to a
  {@link LockObtainFailedException}. The {@link LockObtainFailedException}
  is also thrown if an IndexReader on the same directory is used to delete documents
  from the index.</p>{code}

Anyone remember why NativeFSLockFactory is not the default over SimpleFSLockFactory?"
0,"WriteLineDocTask improvementsMake WriteLineDocTask and LineDocSource more flexible/extendable:
* allow to emit lines also for empty docs (keep current behavior as default)
* allow more/less/other fields"
0,"Clean up old JIRA issues in component ""Other""A list of all JIRA issues in component ""Other"" that haven't been updated in 2007:

   *	 LUCENE-746  	 Incorrect error message in AnalyzingQueryParser.getPrefixQuery   
   *	LUCENE-644 	Contrib: another highlighter approach 
   *	LUCENE-574 	support for vjc java compiler, also known as J# 
   *	LUCENE-471 	gcj ant target doesn't work on windows 
   *	LUCENE-434 	Lucene database bindings 
   *	LUCENE-254 	[PATCH] pseudo-relevance feedback enhancement 
   *	LUCENE-180 	[PATCH] Language guesser contribution 
"
0,"Lucene 2.0 requirements - Remove all deprecated codePer the move to Lucene 2.0 from 1.9, remove all deprecated code and update documentation, etc.

Patch to follow shortly."
0,"contrib/benchmark - few improvements and a bug fixBenchmark byTask was slightly improved:

1. fixed a bug in the ""child-should-not-report"" mechanism. If a task sequence contained only simple tasks it worked as expected (i.e. child tasks did not report times/memory) but if a child was a task sequence, then its children would report - they should not - this was fixed, so this property is now ""penetrating/inherited"" all the way down.

2. doc size control now possible also for the Reuters doc maker. (allowing to index N docs of size C characters each.)

3. TrecDocMaker was added - it reads as input the .gz files used in Trec - e.g. .gov data - this can be handy to benchmark Lucene on these large collections.  Similar to the Reuters collection, the doc-maker scans the input directory for all the files and extracts documents from the files.  Here there are multiple documents in each input file. Unlike the Reuters collection, we cannot provide a 'loader' for these collections - they are available from http://trec.nist.gov - for research purposes.

4. a new BasicDocMaker abstract class handles most of doc-maker tasks, including creating docs with specific size, so adding new doc-makers for other data is now much simpler."
0,"Remove deprecated charset support from Greek and Russian analyzersThis removes the deprecated support for custom charsets.

One thing I found is that once these charsets are removed, RussianLowerCaseFilter is the same as LowerCaseFilter.
So I marked it deprecated to be removed in 3.1
"
0,"misleading contrib/tck-webapp/...RepositoryServlet javadocIn org.apache.jackrabbit.tck.j2ee.RepositoryServlet it says ""...puts the reference into the application context"" but according to the behavior it should say ""...puts the reference into a class variable"". In a j2ee environment the application context refers to the ServletConext instance bounded to the web application.
"
0,"BooleanScorer should not limit number of prohibited clausesToday it's limited to 32, because it uses a separate bit in the mask
for each clause.

But I don't understand why it does this; I think all prohibited
clauses can share a single boolean/bit?  Any match on a prohibited
clause sets this bit and the doc is not collected; we don't need each
prohibited clause to have a dedicated bit?

We also use the mask for required clauses, but this code is now
commented out (we always use BS2 if there are any required clauses);
if we re-enable this code (and I think we should, at least in certain
cases: I suspect it'd be faster than BS2 in many cases), I think we
can cutover to an int count instead of bit masks, and then have no
limit on the required clauses sent to BooleanScorer also.

Separately I cleaned a few things up about BooleanScorer: all of the
embedded scorer methods (nextDoc, docID, advance, score) now throw
UOE; pre-allocate the buckets instead of doing it lazily
per-sub-collect.
"
0,"Simplified Repository URI format for JNDI lookupsThe JndiRepositoryFactory class (together with JcrUtils) currently supports the following repository URI formats:

    JcrUtils.getRepository(""jndi:name-of-repository"");
    JcrUtils.getRepository(""jndi://ignored?org.apache.jackrabbit.repository.jndi.name=name-of-repository&other-parameters"");

The first uri formats allows no extra JNDI environment settings to be passed in, and the second one is pretty verbose and simply ignores the authority and path parts of the URI.

I'd like to add support for the following simplified format that makes it easy to provide the repository name along with the initial context factory from which the name is to be looked up:

    JcrUtils.getRepository(""jndi://initial-context-factory/name-of-repository"");

Extra JNDI environment settings could still be included as additional query parameters. Backwards compatibility with the previous formats would be guaranteed based on the presence or absence of the org.apache.jackrabbit.repository.jndi.name parameter in hierarchical URIs."
0,"Fix FuzzyQuery's defaults, so its fast.We worked a lot on FuzzyQuery, but you need to be a rocket scientist to ensure good results.

The main problem is that the default distance is 0.5f, which doesn't take into account the length of the string.
To add insult to injury, the default number of expansions is 1024 (traditionally from BooleanQuery maxClauseCount)

I propose:
* The syntax of FuzzyQuery is enhanced, so that you can specify raw edits too: such as foobar~2 (all terms within 2 levenshtein edits of foobar). Previously if you specified any amount >=1, you got IllegalArgumentException, so this won't break anyone. You can still use foobar~0.5, and it works just as before
* The default for minimumSimilarity then becomes LevenshteinAutomata.MAXIMUM_SUPPORTED_DISTANCE, which is 2. This way if you just do foobar~, its always fast.
* The size of the priority queue is reduced by default from 1024 to a much more reasonable value: 50. This is what FuzzyLikeThis uses.

I think its best to just change the defaults for this query, since it was so aweful before. We can add notes in migrate.txt that if you care about using the old values, then you should provide them explicitly, and you will get the same results!
"
0,"Allow servlet filters to specify custom session providersIn order to integrate the Jackrabbit davex server functionality with their custom authentication logic, the Sling project currently needs to embed and subclass the davex servlet classes. It would be cleaner if such tight coupling wasn't needed.

One way to achieve something like that would be to allow external components to provide a custom SessionProvider instance as an extra request attribute. This way for example a servlet filter that implements such custom authentication logic could easily make its functionality available to the standard davex servlet in Jackrabbit."
0,"Default lock timeouts should have static setter/getters
We recently stopped using Java system properties to derive defaults for things like the write/commit lock timeout, and switched to getter/setter's across all classes.  See here:

    http://www.gossamer-threads.com/lists/lucene/java-dev/27447

But, in the case at least of the write lock timeout, because it's marked ""public final static"", a consumer of this API can no longer change this value before instantiating the IndexWriter.  This is because the getter/setter for this is not static, which generally makes sense so you can change the timeout for each instance of IndexWriter.  But because IndexWriter on construction uses the timeout value, some uses cases need to change the value before getting an instance of IndexWriter.

This was actually a regression, in that Lucene users lost functionality they previously had, on upgrading.

I would propose that that we add getter/setter for the default value of this timeout, which would be static.  I'll attach a patch file.

See this thread for context that led to this issue:

   http://www.gossamer-threads.com/lists/lucene/java-dev/37421"
0,Backport JCR-1197: Node.restore() may throw InvalidItemStateExceptionBackport issue JCR-1197 (Node.restore() may throw InvalidItemStateException) to 1.3 branch for 1.3.4 (separate issue to avoid re-opening JCR-1197 which was already released with 1.4).
0,"[patch] Fix overly specific casting in coreseveral places in core, casts are made to overly concrete classes when, interfaces are only needed. Doing so ties the algorithms to specific implementations, unnecessarily. patch fixes these."
0,ID Field Descriptor is not inherited as is the case with UUID Field DescriptorID Field descriptor when defined in the base class in jcr-mapping is not inherited. The child class also has to define it again. A patch for the same is attached herewith. Patch is on similar lines of UUID Field Descriptor
0,"UserImporter should use User.changePasswordthe UserImporter lists a limitation that the password value is expected to be hashed already as it writes the
value as it was retrieved from the xml-import.

Instead it could make use of User#changePassword that (in the implementation present with JR) creates a 
pw-hash if the password is found to be plain text."
0,"WorkspaceUpdateChannel.updateCommitted logs too muchOn each cluster record update, an info message is logged.

I think this is too much and logging should be reduced to the DEBUG level."
0,"Trim whitespace from parameter names in configuration filesWe've had a couple of issues with extra whitespace in parameter names causing those configuration options being lost. Now with the more strict validation of configuration settings such mistakes can even prevent the repository from starting. On one hand that's a good thing, as the user would then explicitly need to fix such broken configurations, but it would be nice if no user intervention was needed.

Since leading and trailing whitespace is never allowed in parameter names, we can just as well trim it automatically."
0,decorator enhancementsadded some decorating enhancements as we discussed on the mailing list (apparently there is nothing yet in the gmane /marc archives).
0,"Test failure: org.apache.jackrabbit.test.TestAllSubsequent test runs fail unless doing a mvn clean first.

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.jackrabbit.spi2jcr.spi.TestAll
Tests run: 50, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.965 sec
Running org.apache.jackrabbit.test.TestAll
Tests run: 1038, Failures: 11, Errors: 0, Skipped: 0, Time elapsed: 44.925 sec <<< FAILURE!
Running org.apache.jackrabbit.spi2jcr.jcr2spi.TestAll
Tests run: 394, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.416 sec

Results :

Failed tests:
  testOrderByAscending(org.apache.jackrabbit.test.api.query.SQLOrderByTest)
  testOrderByDescending(org.apache.jackrabbit.test.api.query.SQLOrderByTest)
  testOrderByDefault(org.apache.jackrabbit.test.api.query.SQLOrderByTest)
  testDocOrderIndexedNotation(org.apache.jackrabbit.test.api.query.XPathPosIndexTest)
  testDocOrderPositionFunction(org.apache.jackrabbit.test.api.query.XPathDocOrderTest)
  testDocOrderPositionIndex(org.apache.jackrabbit.test.api.query.XPathDocOrderTest)
  testDocOrderLastFunction(org.apache.jackrabbit.test.api.query.XPathDocOrderTest)
  testDocOrderFirstFunction(org.apache.jackrabbit.test.api.query.XPathDocOrderTest)
  testOrderByAscending(org.apache.jackrabbit.test.api.query.XPathOrderByTest)
  testOrderByDescending(org.apache.jackrabbit.test.api.query.XPathOrderByTest)
  testOrderBy(org.apache.jackrabbit.test.api.query.XPathOrderByTest)

"
0,"The Field ctors that take byte[] shouldn't take Store, since it must be YESAPI silliness.  Makes you think you can set Store.NO for binary fields.  This used to be meaningful when we also accepted COMPRESS, but now it's an orphan."
0,Add user manager performance tests We should add some performance tests for validating JCR-2710 and related. That is we should measure performance for creating users and groups and adding/removing users to/from groups. This should be done for both repository configurations: one with the old content model (group membership in property) and one with the new content model introduced with JCR-2710 (group membership in b-tree like node structure). 
0,"Hardening of NativeFSLockNativeFSLock create a test lock file which its name might collide w/ another JVM that is running. Very unlikely, but still it happened a couple of times already, since the tests were parallelized. This may result in a false exception thrown from release(), when the lock file's delete() is called and returns false, because the file does not exist (deleted by another JVM already). In addition, release() should give a second attempt to delete() if it fails, since the file may be held temporarily by another process (like AntiVirus) before it fails. The proposed changes are:

1) Use ManagementFactory.getRuntimeMXBean().getName() as part of the test lock name (should include the process Id)
2) In release(), if delete() fails, check if the file indeed exists. If it is, let's attempt a re-delete() few ms later.
3) If (3) still fails, throw an exception. Alternatively, we can attempt a deleteOnExit.

I'll post a patch later today."
0,"Make SessionProvider pluggable in JCRWebdavServerServletAlthough there's a SessionProvider interface in o.a.j.server, the SessionProviderImpl implementation class is hard-coded into JCRWebdavServerServlet."
0,"Thread safety and visibility ImprovementsAbstractAuthenticationHandler.DEFAULT_SCHEME_PRIORITY is not protected against external changes.

Although the field is private, subclasses can obtain a reference to it and so may be able to change it.

Consider making the list read-only, or returning a copy instead."
0,Include the README file in the generated jar filesThe Incubator would prefer if we had the incubation notice included in the binary jar files we release. It should be a simple Maven configuration change to get the README.txt file included in the binary jars.
0,"HyphenationCompoundWordTokenFilter fails to load DTD in Crimson parser (JDK 1.4)HyphenationCompoundWordTokenFilter loads the DTD in its XML parser from memory by supplying EntityResolver. In Java 1.4 (affects Lucene 2.9, but also later versions if not Apache Xerces is used as XML parser) this does not work, because Cromson does not even ask the entity resolver, if no base URI is known. As the hyphenation file is loaded from Reader/InputStream no base URI is known. Crimson needs at least a non-null systemId to proceed.

This patch (Lucene 2.9 only)  fakes this by supplying a fake systemId to the InputSource."
0,"Wikipedia Document Generation ChangesThe EnwikiDocMaker currently produces a fair number of documents that are in the download, but are of dubious use in terms of both benchmarking and indexing.  

These issues are:

# Redirect (it currently only handles REDIRECT and redirect, but there are documents as Redirect
# Template files appear to be useless.  These are marked by the term Template: at the beginning of the body.  See for example: http://en.wikipedia.org/wiki/Template:=)
# Image only pages, as in http://en.wikipedia.org/wiki/Image:Sciencefieldnewark.jpg.jpg  These are about as useful as the Redirects and Templates
# Files pending deletion:  This one is a bit trickier to handle, but they are generally marked by ""Wikipedia:Votes for deletion"" or some variation of that depending where along it is in being deleted

I think I can implement this such that it is backward compatible, if there is such a need when it comes to the contrib/benchmark suite.



"
0,"Build fails on system without XThe failing test is: testFileContains(org.apache.jackrabbit.core.query.FulltextQueryTest)

caused by:

java.lang.InternalError: Can't connect to X11 window server using ':0.0' as the value of the DISPLAY variable.
	at sun.awt.X11GraphicsEnvironment.initDisplay(Native Method)
	at sun.awt.X11GraphicsEnvironment.access$000(X11GraphicsEnvironment.java:53)
	at sun.awt.X11GraphicsEnvironment$1.run(X11GraphicsEnvironment.java:142)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.awt.X11GraphicsEnvironment.<clinit>(X11GraphicsEnvironment.java:131)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:164)
	at java.awt.GraphicsEnvironment.getLocalGraphicsEnvironment(GraphicsEnvironment.java:68)
	at sun.awt.X11.XToolkit.<clinit>(XToolkit.java:96)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:164)
	at java.awt.Toolkit$2.run(Toolkit.java:821)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.awt.Toolkit.getDefaultToolkit(Toolkit.java:804)
	at java.awt.Toolkit.getEventQueue(Toolkit.java:1592)
	at java.awt.EventQueue.isDispatchThread(EventQueue.java:666)
	at javax.swing.SwingUtilities.isEventDispatchThread(SwingUtilities.java:1270)
	at javax.swing.text.StyleContext.reclaim(StyleContext.java:437)
	at javax.swing.text.StyleContext.addAttribute(StyleContext.java:294)
	at javax.swing.text.StyleContext$NamedStyle.addAttribute(StyleContext.java:1486)
	at javax.swing.text.StyleContext$NamedStyle.setName(StyleContext.java:1296)
	at javax.swing.text.StyleContext$NamedStyle.<init>(StyleContext.java:1244)
	at javax.swing.text.StyleContext.addStyle(StyleContext.java:90)
	at javax.swing.text.StyleContext.<init>(StyleContext.java:70)
	at javax.swing.text.DefaultStyledDocument.<init>(DefaultStyledDocument.java:88)
	at org.apache.tika.parser.rtf.RTFParser.parse(RTFParser.java:42)
"
0,"Add preemptive authenticationWishlist request for preemptive authentication to be included in the API, like HttpClient 3.x had.  There is an example ClientPreemptiveBasicAuthentication.java that uses HttpRequestInterceptor which I had adapted to my application and it works fine."
0,Fix SnowballAnalyzer casing behavior for Turkish LanguageLUCENE-2102 added a new TokenFilter to handle Turkish unique casing behavior correctly. We should fix the casing behavior in SnowballAnalyzer too as it supports a TurkishStemmer.
0,"MultithreadedConnectionManager and IdleConnectionTimeoutThread improvementsChanges to MultithreadedConnectionManager and IdleConnectionTimeoutThread following the suggestions of Balazs SZCS.


-------- Forwarded Message --------
From: SZCS Balazs <Balazs.Szuecs@wave-solutions.com>
Reply-To: HttpClient User Discussion
<httpclient-user@jakarta.apache.org>
To: 'HttpClient User Discussion' <httpclient-user@jakarta.apache.org>
Subject: RE: MultithreadedConnectionManager pooling strategy
Date: Mon, 15 May 2006 15:26:08 +0200

Hello,

I made two changes to the HttpClient code:

1) in the class ConnectionPool in the method getFreeConnection( ... ) I
changed freeConnections.removeFirst() to freeConnections.removeLast(). Now
the container for free connections behaves as a stack rather than a queue.

2) additionally I changed the IdleConnectionTimeoutThread, in the run()
method I added connectionManager.deleteClosedConnections() so that the pool
size is maintained correctly.

What do you think?
Balazs"
0,"building trunk  fails with javacc plugin version 2.2

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[23,38] cannot find symbol
symbol: class JCRSQLParserVisitor
class DefaultParserVisitor implements JCRSQLParserVisitor {

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[25,24] cannot find symbol
symbol  : class SimpleNode
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[29,24] cannot find symbol
symbol  : class ASTQuery
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[33,24] cannot find symbol
symbol  : class ASTSelectList
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[37,24] cannot find symbol
symbol  : class ASTFromClause
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[41,24] cannot find symbol
symbol  : class ASTWhereClause
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTPredicate.java:[21,34] cannot find symbol
symbol: class SimpleNode
public class ASTPredicate extends SimpleNode {

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[49,24] cannot find symbol
symbol  : class ASTOrExpression
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[53,24] cannot find symbol
symbol  : class ASTAndExpression
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[57,24] cannot find symbol
symbol  : class ASTNotExpression
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[61,24] cannot find symbol
symbol  : class ASTBracketExpression
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTLiteral.java:[19,32] cannot find symbol
symbol: class SimpleNode
public class ASTLiteral extends SimpleNode {

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTIdentifier.java:[21,35] cannot find symbol
symbol: class SimpleNode
public class ASTIdentifier extends SimpleNode {

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[73,24] cannot find symbol
symbol  : class ASTOrderByClause
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTContainsExpression.java:[21,43] cannot find symbol
symbol: class SimpleNode
public class ASTContainsExpression extends SimpleNode {

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[81,24] cannot find symbol
symbol  : class ASTOrderSpec
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[85,24] cannot find symbol
symbol  : class ASTAscendingOrderSpec
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[89,24] cannot find symbol
symbol  : class ASTDescendingOrderSpec
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[93,24] cannot find symbol
symbol  : class ASTLowerFunction
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[97,24] cannot find symbol
symbol  : class ASTUpperFunction
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/DefaultParserVisitor.java:[101,24] cannot find symbol
symbol  : class ASTExcerptFunction
location: class org.apache.jackrabbit.core.query.sql.DefaultParserVisitor

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTPredicate.java:[37,22] cannot find symbol
symbol  : class JCRSQLParser
location: class org.apache.jackrabbit.core.query.sql.ASTPredicate

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTPredicate.java:[82,26] cannot find symbol
symbol  : class JCRSQLParserVisitor
location: class org.apache.jackrabbit.core.query.sql.ASTPredicate

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTLiteral.java:[30,22] cannot find symbol
symbol  : class JCRSQLParser
location: class org.apache.jackrabbit.core.query.sql.ASTLiteral

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTLiteral.java:[54,28] cannot find symbol
symbol  : class JCRSQLParserVisitor
location: class org.apache.jackrabbit.core.query.sql.ASTLiteral

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTIdentifier.java:[29,23] cannot find symbol
symbol  : class JCRSQLParser
location: class org.apache.jackrabbit.core.query.sql.ASTIdentifier

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTIdentifier.java:[42,26] cannot find symbol
symbol  : class JCRSQLParserVisitor
location: class org.apache.jackrabbit.core.query.sql.ASTIdentifier

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTContainsExpression.java:[31,33] cannot find symbol
symbol  : class JCRSQLParser
location: class org.apache.jackrabbit.core.query.sql.ASTContainsExpression

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTContainsExpression.java:[54,28] cannot find symbol
symbol  : class JCRSQLParserVisitor
location: class org.apache.jackrabbit.core.query.sql.ASTContainsExpression

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/lucene/LuceneQueryBuilder.java:[44,56] cannot find symbol
symbol  : class QueryParser
location: package org.apache.jackrabbit.core.query.lucene.fulltext

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/lucene/LuceneQueryBuilder.java:[45,56] cannot find symbol
symbol  : class ParseException
location: package org.apache.jackrabbit.core.query.lucene.fulltext

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[55,42] cannot find symbol
symbol: class XPathVisitor
public class XPathQueryBuilder implements XPathVisitor, XPathTreeConstants {

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[55,56] cannot find symbol
symbol: class XPathTreeConstants
public class XPathQueryBuilder implements XPathVisitor, XPathTreeConstants {

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[23,35] cannot find symbol
symbol: class Node
public class SimpleNode implements Node {

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[24,14] cannot find symbol
symbol  : class Node
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[25,14] cannot find symbol
symbol  : class Node
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[27,14] cannot find symbol
symbol  : class XPath
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[33,22] cannot find symbol
symbol  : class XPath
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[39,33] cannot find symbol
symbol  : class XPath
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[39,18] cannot find symbol
symbol  : class Node
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[49,29] cannot find symbol
symbol  : class Node
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[53,11] cannot find symbol
symbol  : class Node
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[57,28] cannot find symbol
symbol  : class Node
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[68,11] cannot find symbol
symbol  : class Node
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[79,28] cannot find symbol
symbol  : class XPathVisitor
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[86,33] cannot find symbol
symbol  : class XPathVisitor
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/SimpleNode.java:[132,29] cannot find symbol
symbol  : class Token
location: class org.apache.jackrabbit.core.query.xpath.SimpleNode

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/lucene/JQOM2LuceneQueryBuilder.java:[53,56] cannot find symbol
symbol  : class QueryParser
location: package org.apache.jackrabbit.core.query.lucene.fulltext

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/lucene/fulltext/FastCharStream.java:[30,45] cannot find symbol
symbol: class CharStream
public final class FastCharStream implements CharStream {

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[60,43] cannot find symbol
symbol: class JCRSQLParserVisitor
public class JCRSQLQueryBuilder implements JCRSQLParserVisitor {

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[81,18] cannot find symbol
symbol  : class ASTQuery
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[121,31] cannot find symbol
symbol  : class ASTQuery
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[201,24] cannot find symbol
symbol  : class SimpleNode
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[206,24] cannot find symbol
symbol  : class ASTQuery
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[279,24] cannot find symbol
symbol  : class ASTSelectList
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[297,24] cannot find symbol
symbol  : class ASTFromClause
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[311,24] cannot find symbol
symbol  : class ASTWhereClause
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[447,24] cannot find symbol
symbol  : class ASTOrExpression
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[459,24] cannot find symbol
symbol  : class ASTAndExpression
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[469,24] cannot find symbol
symbol  : class ASTNotExpression
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[479,24] cannot find symbol
symbol  : class ASTBracketExpression
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[496,24] cannot find symbol
symbol  : class ASTOrderByClause
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[505,24] cannot find symbol
symbol  : class ASTOrderSpec
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[526,24] cannot find symbol
symbol  : class ASTAscendingOrderSpec
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[531,24] cannot find symbol
symbol  : class ASTDescendingOrderSpec
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[556,24] cannot find symbol
symbol  : class ASTLowerFunction
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[566,24] cannot find symbol
symbol  : class ASTUpperFunction
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/JCRSQLQueryBuilder.java:[576,24] cannot find symbol
symbol  : class ASTExcerptFunction
location: class org.apache.jackrabbit.core.query.sql.JCRSQLQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTPredicate.java:[87,15] cannot find symbol
symbol  : variable super
location: class org.apache.jackrabbit.core.query.sql.ASTPredicate

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTLiteral.java:[59,15] cannot find symbol
symbol  : variable super
location: class org.apache.jackrabbit.core.query.sql.ASTLiteral

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/sql/ASTIdentifier.java:[47,15] cannot find symbol
symbol  : variable super
location: class org.apache.jackrabbit.core.query.sql.ASTIdentifier

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/lucene/LuceneQueryBuilder.java:[376,12] cannot find symbol
symbol  : class QueryParser
location: class org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/lucene/LuceneQueryBuilder.java:[376,37] cannot find symbol
symbol  : class QueryParser
location: class org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/lucene/LuceneQueryBuilder.java:[378,31] cannot find symbol
symbol  : variable QueryParser
location: class org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/lucene/LuceneQueryBuilder.java:[440,17] cannot find symbol
symbol  : class ParseException
location: class org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[267,12] cannot find symbol
symbol  : class XPath
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[269,26] cannot find symbol
symbol  : class XPath
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[271,33] cannot find symbol
symbol  : class XPath
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[283,17] cannot find symbol
symbol  : class ParseException
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[356,17] cannot find symbol
symbol  : variable JJTXPATH2
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[359,17] cannot find symbol
symbol  : variable JJTROOT
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[360,17] cannot find symbol
symbol  : variable JJTROOTDESCENDANTS
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[368,17] cannot find symbol
symbol  : variable JJTSTEPEXPR
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[415,17] cannot find symbol
symbol  : variable JJTNAMETEST
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[429,17] cannot find symbol
symbol  : variable JJTELEMENTNAMEORWILDCARD
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[432,41] cannot find symbol
symbol  : variable JJTANYNAME
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[437,17] cannot find symbol
symbol  : variable JJTTEXTTEST
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[443,17] cannot find symbol
symbol  : variable JJTTYPENAME
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[456,17] cannot find symbol
symbol  : variable JJTOREXPR
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[463,17] cannot find symbol
symbol  : variable JJTANDEXPR
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[470,17] cannot find symbol
symbol  : variable JJTCOMPARISONEXPR
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[473,17] cannot find symbol
symbol  : variable JJTSTRINGLITERAL
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[474,17] cannot find symbol
symbol  : variable JJTDECIMALLITERAL
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[475,17] cannot find symbol
symbol  : variable JJTDOUBLELITERAL
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[476,17] cannot find symbol
symbol  : variable JJTINTEGERLITERAL
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[480,40] cannot find symbol
symbol  : variable JJTINTEGERLITERAL
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[490,17] cannot find symbol
symbol  : variable JJTUNARYMINUS
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[497,17] cannot find symbol
symbol  : variable JJTFUNCTIONCALL
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[500,17] cannot find symbol
symbol  : variable JJTORDERBYCLAUSE
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder

/Users/adc/dev/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/query/xpath/XPathQueryBuilder.java:[505,17] cannot find symbol
symbol  : variable JJTORDERMODIFIER
location: class org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder
"
0,"Jcr2Spi: UpdateTest#testUpdateRemovesExtraProperty and #testUpdateAddsMissingSubtree fail occasionallyissue reported by jukka:

Every now and then I see the following jcr2spi test failures on a
clean checkout:

    Tests in error:
      testUpdateRemovesExtraProperty(org.apache.jackrabbit.jcr2spi.UpdateTest)
      testUpdateAddsMissingSubtree(org.apache.jackrabbit.jcr2spi.UpdateTest)

The problem seems to be caused by the test cases automatically
choosing the ""security"" workspace for the update test. The reason why
the tests only fail occasionally is that currently the ordering of the
string array returned by getAccessibleWorkspaceNames() is not
deterministic.

I can work around the issue by making
RepositoryImpl.getWorkspaceNames() explicitly sort the returned array
of names, but a more proper fix would probably be to ensure that the
workspace selected by UpdateTest is useful for the test.


"
0,"SPI: Testsuite for the SPI Interfacesnow that people start writing SPI implementations we should provide a test-suite that runs on the SPI directly in order to provide the developers a way to assert basic compliance of their implementation without having the JCR api in between.
"
0,"Fixed README.txt on textfilters project- Fixed a little mistake: changed org.apache.jackrabbit.core.query..RTFTextFilter to org.apache.jackrabbit.core.query.RTFTextFilter

- Added the OpenOfficeTextFilter to the sample configuration line."
0,"Scorer.explain is deprecated but abstract, should have impl that throws UnsupportedOperationExceptionSuggest having Scorer implement explain to throw UnsupportedOperationException

right now, i have to implement this method (because its abstract), and javac yells at me for overriding a deprecated method

if the following implementation is in Scorer, i can remove my ""empty"" implementations of explain from my Scorers
{code}
  /** Returns an explanation of the score for a document.
   * <br>When this method is used, the {@link #next()}, {@link #skipTo(int)} and
   * {@link #score(HitCollector)} methods should not be used.
   * @param doc The document number for the explanation.
   *
   * @deprecated Please use {@link IndexSearcher#explain}
   * or {@link Weight#explain} instead.
   */
  public Explanation explain(int doc) throws IOException {
    throw new UnsupportedOperationException();
  }
{code}

best i figure, this shouldn't break back compat (people already have to recompile anyway) (2.9 definitely not binary compatible with 2.4)
"
0,"When adding a large (100MB) binary to the DbDataStore, it fails with an insufficient memory exceptionAttached is a small test case. It fails during save(). I think this is related to what I mentioned in http://mail-archives.apache.org/mod_mbox/jackrabbit-dev/200711.mbox/%3c00fc01c832b9$f1f08730$7309240a@goku%3e

The full stacktrace is the following:

javax.jcr.RepositoryException: /: unable to update item.: failed to write property state: cafebabe-cafe-babe-cafe-babecafebabe/{}bin: failed to write property state: cafebabe-cafe-babe-cafe-babecafebabe/{}bin
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1252)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:858)
	at org.apache.jackrabbit.core.data.BigBinaryTest.testBigBinary(BigBinaryTest.java:16)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at org.apache.jackrabbit.test.AbstractJCRTest.run(AbstractJCRTest.java:404)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:128)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)
Caused by: org.apache.jackrabbit.core.state.ItemStateException: failed to write property state: cafebabe-cafe-babe-cafe-babecafebabe/{}bin
	at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.store(DatabasePersistenceManager.java:487)
	at org.apache.jackrabbit.core.persistence.AbstractPersistenceManager.store(AbstractPersistenceManager.java:75)
	at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.store(DatabasePersistenceManager.java:282)
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:687)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:856)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:324)
	at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:313)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:300)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:306)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1244)
	... 21 more
Caused by: org.apache.jackrabbit.core.data.DataStoreException: Can not read identifier a2ada2d96d0b05214288efa03be9005a5bb98c9b: Memoria insuficiente. Utilice cursores del servidor para result sets grandes:Java heap space. Tamao de result set:104.857.723. Tamao memoria total JVM:66.650.112.: Memoria insuficiente. Utilice cursores del servidor para result sets grandes:Java heap space. Tamao de result set:104.857.723. Tamao memoria total JVM:66.650.112.
	at org.apache.jackrabbit.core.data.db.DbDataStore.convert(DbDataStore.java:438)
	at org.apache.jackrabbit.core.data.db.DbDataStore.getInputStream(DbDataStore.java:481)
	at org.apache.jackrabbit.core.data.db.DbDataRecord.getStream(DbDataRecord.java:61)
	at org.apache.jackrabbit.core.value.BLOBInDataStore.getStream(BLOBInDataStore.java:93)
	at org.apache.jackrabbit.core.persistence.util.Serializer.serialize(Serializer.java:198)
	at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.store(DatabasePersistenceManager.java:476)
	... 30 more
Caused by: com.microsoft.sqlserver.jdbc.SQLServerException: Memoria insuficiente. Utilice cursores del servidor para result sets grandes:Java heap space. Tamao de result set:104.857.723. Tamao memoria total JVM:66.650.112.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDriverError(Unknown Source)
	at com.microsoft.sqlserver.jdbc.DBComms.receive(Unknown Source)
	at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.doExecutePreparedStatement(Unknown Source)
	at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement$PreparedStatementExecutionRequest.executeStatement(Unknown Source)
	at com.microsoft.sqlserver.jdbc.CancelableRequest.execute(Unknown Source)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeRequest(Unknown Source)
	at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.execute(Unknown Source)
	at org.apache.jackrabbit.core.persistence.bundle.util.ConnectionRecoveryManager.executeStmtInternal(ConnectionRecoveryManager.java:362)
	at org.apache.jackrabbit.core.persistence.bundle.util.ConnectionRecoveryManager.executeStmtInternal(ConnectionRecoveryManager.java:292)
	at org.apache.jackrabbit.core.persistence.bundle.util.ConnectionRecoveryManager.executeStmt(ConnectionRecoveryManager.java:257)
	at org.apache.jackrabbit.core.persistence.bundle.util.ConnectionRecoveryManager.executeStmt(ConnectionRecoveryManager.java:237)
	at org.apache.jackrabbit.core.data.db.DbDataStore.getInputStream(DbDataStore.java:474)
	... 34 more
org.apache.jackrabbit.core.state.ItemStateException: failed to write property state: cafebabe-cafe-babe-cafe-babecafebabe/{}bin
	at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.store(DatabasePersistenceManager.java:487)
	at org.apache.jackrabbit.core.persistence.AbstractPersistenceManager.store(AbstractPersistenceManager.java:75)
	at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.store(DatabasePersistenceManager.java:282)
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:687)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:856)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:324)
	at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:313)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:300)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:306)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1244)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:858)
	at org.apache.jackrabbit.core.data.BigBinaryTest.testBigBinary(BigBinaryTest.java:16)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at org.apache.jackrabbit.test.AbstractJCRTest.run(AbstractJCRTest.java:404)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:128)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)
Caused by: org.apache.jackrabbit.core.data.DataStoreException: Can not read identifier a2ada2d96d0b05214288efa03be9005a5bb98c9b: Memoria insuficiente. Utilice cursores del servidor para result sets grandes:Java heap space. Tamao de result set:104.857.723. Tamao memoria total JVM:66.650.112.: Memoria insuficiente. Utilice cursores del servidor para result sets grandes:Java heap space. Tamao de result set:104.857.723. Tamao memoria total JVM:66.650.112.
	at org.apache.jackrabbit.core.data.db.DbDataStore.convert(DbDataStore.java:438)
	at org.apache.jackrabbit.core.data.db.DbDataStore.getInputStream(DbDataStore.java:481)
	at org.apache.jackrabbit.core.data.db.DbDataRecord.getStream(DbDataRecord.java:61)
	at org.apache.jackrabbit.core.value.BLOBInDataStore.getStream(BLOBInDataStore.java:93)
	at org.apache.jackrabbit.core.persistence.util.Serializer.serialize(Serializer.java:198)
	at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.store(DatabasePersistenceManager.java:476)
	... 30 more
Caused by: com.microsoft.sqlserver.jdbc.SQLServerException: Memoria insuficiente. Utilice cursores del servidor para result sets grandes:Java heap space. Tamao de result set:104.857.723. Tamao memoria total JVM:66.650.112.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDriverError(Unknown Source)
	at com.microsoft.sqlserver.jdbc.DBComms.receive(Unknown Source)
	at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.doExecutePreparedStatement(Unknown Source)
	at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement$PreparedStatementExecutionRequest.executeStatement(Unknown Source)
	at com.microsoft.sqlserver.jdbc.CancelableRequest.execute(Unknown Source)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeRequest(Unknown Source)
	at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.execute(Unknown Source)
	at org.apache.jackrabbit.core.persistence.bundle.util.ConnectionRecoveryManager.executeStmtInternal(ConnectionRecoveryManager.java:362)
	at org.apache.jackrabbit.core.persistence.bundle.util.ConnectionRecoveryManager.executeStmtInternal(ConnectionRecoveryManager.java:292)
	at org.apache.jackrabbit.core.persistence.bundle.util.ConnectionRecoveryManager.executeStmt(ConnectionRecoveryManager.java:257)
	at org.apache.jackrabbit.core.persistence.bundle.util.ConnectionRecoveryManager.executeStmt(ConnectionRecoveryManager.java:237)
	at org.apache.jackrabbit.core.data.db.DbDataStore.getInputStream(DbDataStore.java:474)
	... 34 more

"
0,"Checks for optional features in test cases are wrongReported by David Sanders:

The TCK for JSR-170 Final Release
(http://jcp.org/aboutJava/communityprocess/final/jsr170/index.html)
checks for level 2 and optional features by comparing
Repository.getDescriptor to null.  According to the
spec and javadoc, getDescriptor must return either
""true"" or ""false"" for the ""capability"" keys.

Example in AbsractJCRest.java:

        // setup custom namespaces
        if
(helper.getRepository().getDescriptor(Repository.LEVEL_2_SUPPORTED)
!= null) {
            NamespaceRegistry nsReg =
superuser.getWorkspace().getNamespaceRegistry();


I think the above if statement should be:
       if
(helper.getRepository().getDescriptor(Repository.LEVEL_2_SUPPORTED)
.equals(""true"")) "
0,"Mark Fieldable as allowing some changes in 2.x future releasesSee http://lucene.markmail.org/message/4k2gqs3n7coh4lmd?q=Fieldable

1. We mark Fieldable as being subject to change. We heavily advertise (on java-dev and java-user and maybe general) that in the next minor release of Lucene (2.4), Fieldable will be changing. It is also marked at the top of CHANGES.txt very clearly for all the world to see. Since 2.4 is probably at least a month away, I think this gives anyone with a pulse enough time to react. "
0,"reopen support for SegmentReaderReopen for SegmentReader can be supported simply as the following:

  @Override
  public synchronized IndexReader reopen() throws CorruptIndexException,
		IOException {
	return reopenSegment(this.si,false,readOnly);
  }

  @Override
  public synchronized IndexReader reopen(boolean openReadOnly)
		throws CorruptIndexException, IOException {
	return reopenSegment(this.si,false,openReadOnly);
  }
"
0,"Add NoOpMergePolicyI'd like to add a simple and useful MP implementation which does .... nothing ! :). I've came across many places where either the following is documented or implemented: ""if you want to prevent merges, set mergeFactor to a high enough value"". I think a NoOpMergePolicy is just as good, and can REALLY allow you disable merges (except for maybe set mergeFactor to Int.MAX_VAL).

As such, NoOpMergePolicy will be introduced as a singleton, and can be used for convenience purposes only. Also, for Parallel Index it's important, because I'd like the slices to never do any merges, unless ParallelWriter decides so. So they should be set w/ that MP.

I have a patch ready. Waiting for LUCENE-2320 to go in, so that I don't need to change it afterwards.

About the name - I like the name, but suggestions are welcome. I thought of a NullMergePolicy, but I don't like 'Null' used for a NoOp."
0,Include to jackrabbit-jcr-rmi and jackrabbit-jcr-servlet in main trunkJackrabbit 2.0 should include the 2.0 version of the RMI component and the related jcr-servlet updates.
0,"Remove/deprecate Tokenizer's default ctorI was working on a new Tokenizer... and I accidentally forgot to call super(input) (and super.reset(input) from my reset method)... which then meant my correctOffset() calls were silently a no-op; this is very trappy.

Fortunately the awesome BaseTokenStreamTestCase caught this (I hit failures because the offsets were not in fact being corrected).

One minimal thing we can do (but it sounds like from Robert there may be reasons why we can't) is add {{assert input != null}} in Tokenizer.correctOffset:

{noformat}
Index: lucene/core/src/java/org/apache/lucene/analysis/Tokenizer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/analysis/Tokenizer.java	(revision 1242316)
+++ lucene/core/src/java/org/apache/lucene/analysis/Tokenizer.java	(working copy)
@@ -82,6 +82,7 @@
    * @see CharStream#correctOffset
    */
   protected final int correctOffset(int currentOff) {
+    assert input != null: ""subclass failed to call super(Reader) or super.reset(Reader)"";
     return (input instanceof CharStream) ? ((CharStream) input).correctOffset(currentOff) : currentOff;
   }
{noformat}

But best would be to remove the default ctor that leaves input null..."
0,"CartesianTierPlotter fieldPrefix should be configurableCartesianTierPlotter field prefix is currrently hardcoded to ""_localTier"" -- this should be configurable"
0,"bogus javadocs for FieldValueHitQuery.fillFieldsFieldValueHitQuery.fillFields has javadocs that seem to be left over from a completely different method...

{code}
  /**
   * Given a FieldDoc object, stores the values used to sort the given document.
   * These values are not the raw values out of the index, but the internal
   * representation of them. This is so the given search hit can be collated by
   * a MultiSearcher with other search hits.
   * 
   * @param doc
   *          The FieldDoc to store sort values into.
   * @return The same FieldDoc passed in.
   * @see Searchable#search(Weight,Filter,int,Sort)
   */
  FieldDoc fillFields(final Entry entry) {
    final int n = comparators.length;
    final Comparable[] fields = new Comparable[n];
    for (int i = 0; i < n; ++i) {
      fields[i] = comparators[i].value(entry.slot);
    }
    //if (maxscore > 1.0f) doc.score /= maxscore;   // normalize scores
    return new FieldDoc(entry.docID, entry.score, fields);
  }

{code}"
0,add support for RFC 3253 to the simple serverhttp://www.ietf.org/rfc/rfc3253.txt
0,Avoid using BitSets in ChildAxisQuery to minimize memory usageWhen doing ChildAxisQueries on large indexes the internal BitSet instance (hits) may consume a lot of memory because the BitSet is always as large as IndexReader.maxDoc(). In our case we had a query consisting of 7 ChildAxisQueries which combined to a total of 14MB. Since we have multiple users executing this query simultaneously this caused an out of memory error.
0,"Make ObjectIterator implement RangeIterator interfaceCurrently, it's not possible to skip a part of results returned in the form of ObjectIterator (for example, to implement db-like pagination feature with offset/max parameters).

It would be great if ObjectIterator implement RangeIterator interface, and it's trivial enough since underlying NodeIterator implements this interface."
0,"improve termquery ""pk lookup"" performanceFor things that are like primary keys and don't exist in some segments (worst case is primary/unique key that only exists in 1)
we do wasted seeks.

While LUCENE-2694 tries to solve some of this issue with TermState, I'm concerned we could every backport that to 3.1 for example.

This is a simpler solution here just to solve this one problem in termquery... we could just revert it in trunk when we resolve LUCENE-2694,
but I don't think we should leave things as they are in 3.x
"
0,RTFTextExtractor should also support mime type text/rtfThere exist two mime types for RTF documents: application/rtf and text/rtf. The current RTFTextExtractor currently only recognizes the first.
0,"no classes with default visibilityThere should be no classes with default (package) visibility. They cause problems when classes using them are extended. All classes should either be public, or nested with protected visibility where they are used. Nesting with private visibility may be acceptable in certain cases, for example in final classes.
"
0,"QueryWrapperFilter should not do scoringThe purpose of QueryWrapperFilter is to simply filter to include the docIDs that match the query.

Its implementation is wasteful now because it computes scores for those matching docs even though the score is unused.  We could fix this by getting a Scorer and iterating through the docs without asking for the score:

{code}
Index: src/java/org/apache/lucene/search/QueryWrapperFilter.java
===================================================================
--- src/java/org/apache/lucene/search/QueryWrapperFilter.java	(revision 707060)
+++ src/java/org/apache/lucene/search/QueryWrapperFilter.java	(working copy)
@@ -62,11 +62,9 @@
   public DocIdSet getDocIdSet(IndexReader reader) throws IOException {
     final OpenBitSet bits = new OpenBitSet(reader.maxDoc());
 
-    new IndexSearcher(reader).search(query, new HitCollector() {
-      public final void collect(int doc, float score) {
-        bits.set(doc);  // set bit for hit
-      }
-    });
+    final Scorer scorer = query.weight(new IndexSearcher(reader)).scorer(reader);
+    while(scorer.next())
+      bits.set(scorer.doc());
     return bits;
   }
{code}

Maybe I'm missing something, but this seams like a simple win?
"
0,"SpanQueryFilter additionSimilar to the QueryFilter (or whatever it is called now) the SpanQueryFilter is a regular Lucene Filter, but it also can return Spans-like information.  This is useful if you not only want to filter based on a Query, but you then want to be able to compare how a given match from a new query compared to the positions of the filtered SpanQuery.  Patch to come shortly also contains a caching mechanism for the SpanQueryFilter"
0,"DefaultItemStateProvider contains grow-only cacheThe DefaultItemStateProvider class contains a private HashMap ""items"" which contains references to ItemState objects. The bad thing about this cache is, that it only grows, but is not being managed to forget about ""unused"" items.

Example: A repository which is filled with 9350 nodes and 52813 properties grows this items map to 1'667'557 (!) entries. In this concrete case, the VM all13ates 213MB to the heap of which 57MB is referenced by the DefaultItemStateProvider.items map."
0,augment logging information around CachingEntryCollectoradd more logging information for the purpose of debugging CachingEntryCollector bottlenecks
0,Put JavaDoc resources in src/main/javadocThe Maven javadoc plugin suggests that Javadoc resources like package.html files and doc-files subdirectories should be placed in the src/main/javadoc folder (see http://maven.apache.org/plugins/maven-javadoc-plugin/faq.html). I'll move the javadoc files unless anyone argues otherwise.
0,"Invalid redirects are not correctedIf a get is made to a page with a query argument containing a space, many web
servers, notably including Tomcat 5 can generate a redirect in which the space
in the query argument is not escaped correctly.  Most browsers including IE and
Firefox compensate for this by quoting any included spaces in the redirect
location.  Http client does not.  When this broken URL is presented to a
subsequent server, the GET command is interprted incorrectly resulting (usually)
in a 505.

The fix is to replace spaces in redirect locations with +'s.  This doesn't
entirely fix the problem but that is the job of the web server developers."
0,"[PATCH] reduce duplicate conversions from OffsetCharSequence to (lower/upper) stringscode repetitively converts OffsetCharSequence to strings, and then repetitively converts to lower/upper case, when generating search terms.

Patch fixes this."
0,JSR 283 NodeType Management
0,PropertyTypeRegistry should also yield if property is multi-valuedCurrently only the PropertyType is available for a certain property name. In some cases it is also required to know if the property is single- or multi-valued.
0,"Consolidate compare behaviour for Value(s) and Comparable(s)There are 2 different implementations of Value comparison (ValueComparator and Util). With the introduction of JCR-2906 which introduces arrays into the mix, I'd like to refactor all of them into one place, namely o.a.j.core.query.lucene.Util.

This will also allow for a wider scope of comparison for Value[], marked as TODO in the ValueComparator class.

Will attach patch shortly"
0,"[PATCH] BitSetQuery, FastPrefixQuery, FastWildcardQuery and FastQueryParserFastPrefixQuery and FastWildcardQuery rewrites to BitSetQuery instead of OR'ed
BooleanQuery's.  A BitSetQuery contains a BitSet that desginates which document
should be included in the search result.  BitSetQuery cannot be used by itself
with MultiSearcher as of now."
0,"terms index should not store useless suffixesThis idea came up when discussing w/ Robert how to improve our terms index...

The terms dict index today simply grabs whatever term was at a 0 mod 128 index (by default).

But this is wasteful because you often don't need the suffix of the term at that point.

EG if the 127th term is aa and the 128th (indexed) term is abcd123456789, instead of storing that full term you only need to store ab.  The suffix is useless, and uses up RAM since we load the terms index into RAM.

The patch is very simple.  The optimization is particularly easy because terms are now byte[] and we sort in binary order.

I tested on first 10M 1KB Wikipedia docs, and this reduces the terms index (tii) file from 3.9 MB -> 3.3 MB = 16% smaller (using StandardAnalyzer, indexing body field tokenized but title / date fields untokenized).  I expect on noisier terms dicts, especially ones w/ bad terms accidentally indexed, that the savings will be even more.

In the future we could do crazier things.  EG there's no real reason why the indexed terms must be regular (every N terms), so, we could instead pick terms more carefully, say ""approximately"" every N, but favor terms that have a smaller net prefix.  We can also index more sparsely in regions where the net docFreq is lowish, since we can afford somewhat higher seek+scan time to these terms since enuming their docs will be much faster."
0,"Add SimpleText codecInspired by Sahin Buyrukbilen's question here:

  http://www.lucidimagination.com/search/document/b68846e383824653/how_to_export_lucene_index_to_a_simple_text_file#b68846e383824653

I made a simple read/write codec that stores all postings data into a
single text file (_X.pst), looking like this:

{noformat}
field contents
  term file
    doc 0
      pos 5
  term is
    doc 0
      pos 1
  term second
    doc 0
      pos 3
  term test
    doc 0
      pos 4
  term the
    doc 0
      pos 2
  term this
    doc 0
      pos 0
END
{noformat}

The codec is fully funtional -- all Lucene & Solr tests pass with
-Dtests.codec=SimpleText -- but, its performance is obviously poor.

However, it should be useful for debugging, transparency,
understanding just what Lucene stores in its index, etc.  And it's a
quick way to gain some understanding on how a codec works...
"
0,"When we move to java 1.5 in 3.0 we should replace all Interger, Long, etc construction with .valueOf-128 to 128 are guaranteed to be cached and using valueOf in that case is 3.5 times faster than using contructor"
0,"Avoid ${project.version} in dependenciesAnother one for Jackrabbit 1.5, we should avoid using ${project.version} for our dependencies and override the versions of any transitive dependencies that use ${project.version} (notably the Jetty dependencies in jackrabbit-standalone) to avoid problems with Maven < 2.0.9 caused by MNG-2339 [1].

[1] http://jira.codehaus.org/browse/MNG-2339
"
0,"ShingleFilter: don't output all-filler shingles/unigrams; also, convert from TermAttribute to CharTermAttributeWhen the input token stream to ShingleFilter has position increments greater than one, filler tokens are inserted for each position for which there is no token in the input token stream.  As a result, unigrams (if configured) and shingles can be filler-only.  Filler-only output tokens make no sense - these should be removed.

Also, because TermAttribute has been deprecated in favor of CharTermAttribute, the patch will also convert TermAttribute usages to CharTermAttribute in ShingleFilter."
0,"Dependency URL broken for commons-loggingOn http://jakarta.apache.org/commons/httpclient/dependencies.html there is a 
typo in the href to the logging dependency, this should be

http://jakarta.apache.org/commons/logging/"
0,"Allow to disable referential integrity checking for workspaceSome operations like clone, remove operating on huge subtree of nodes requires a lot of memory. To copy, clone, remove subtree all nodes are loaded into transient spaces. It allows such operations to be transactional, from other side it requires a lot of heap size and this memory size is directly dependent on the size of subtree (number of nodes). In result of this in some cases it is impossible to make such operations in one step. In our environment sometimes 1 GB of java heap is not enough to succesfully clone subtree  from one workspace to another.

You can always clone (copy, remove) tree in chunks, but if you have references between subtrees such approach fails. Possibilty of temporary disabling referential integrity checking for experienced JCR user could be very usefull then.

Another use case is to allow to clone selected subtrees of the whole structure between worskpaces. In our application we need to clone only some selected subtrees from one workspace to another. But we can not do that because of existing references. We need to clone the whol estructure first, then remove all unwanted nodes, which is really time expensive and memory consuming.
"
0,EventImpl should implement toStringThis would simplify logging and debugging.
0,"Fix IndexCommit hashCode() and equals() to be consistentIndexCommit's impl of hashCode() and equals() is inconsistent. One uses Dir + version and the other uses Dir + equals. According to hashCode()'s javadoc, if o1.equals(o2), then o1.hashCode() == o2.hashCode(). Simple fix, and I'll add a test case."
0,Add a testing implementation for DocumentsWriterPerThreadPoolcurrently we only have one impl for DocumentsWriterPerThreadPool. We should add some more to make sure the interface is sufficient and to beef up tests. For testing I'm working on a randomized impl. selecting and locking states randomly.
0,"remove RoutedRequest from ClientRequestDirector interfaceRemove the RoutedRequest from ClientRequestDirector.execute, pass the request and route/target separately."
0,"ConstantScoreQuery should directly support wrapping Query and simply strip off scoresEspecially in MultiTermQuery rewrite modes we often simply need to strip off scores from Queries and make them constant score. Currently the code to do this looks quite ugly: new ConstantScoreQuery(new QueryWrapperFilter(query))

As the name says, QueryWrapperFilter should make any other Query constant score, so why does it not take a Query as ctor param? This question was aldso asked quite often by my customers and is simply correct, if you think about it.

Looking closer into the code, it is clear that this would also speed up MTQs:
- One additional wrapping and method calls can be removed
- Maybe we can even deprecate QueryWrapperFilter in 3.1 now (it's now only used in tests and the use-case for this class is not really available) and LUCENE-2831 does not need the stupid hack to make Simon's assertions pass
- CSQ now supports out-of-order scoring and topLevel scoring, so a CSQ on top-level now directly feeds the Collector. For that a small trick is used: The score(Collector) calls are directly delegated and the scores are stripped by wrapping the setScorer() method in Collector

During that I found a visibility bug in Scorer (LUCENE-2839): The method ""boolean score(Collector collector, int max, int firstDocID)"" should be public not protected, as its not solely intended to be overridden by subclasses and is called from other classes, too! This leads to no compiler bugs as the other classes that calls it is mainly BooleanScorer(2) and thats in same package, but visibility is wrong. I will open an issue for that and fix it at least in trunk where we have no backwards-requirement."
0,ant test won't run in 'out of the box' installationone possible solution would be to remove 'lib' from the junit.classpath
0,"fail build if contrib tests fail to compilespinoff of LUCENE-885, from Steven's comments...

Looking at the current build (r545324) it looks like the some contrib failures are getting swallowed. Things like lucli are throwing errors along the lines of

 [subant] /home/barronpark/smparkes/work/lucene/trunk/common-build.xml:366: srcdir ""/home/barronpark/smparkes/work/lucene/trunk/contrib/lucli/src/test"" does not exist!

but these don't make it back up to the top level status.

It looks like the current state will bubble up junit failures, but maybe not build failures?

...

It's ""test-compile-contrib"" (if you will) that fails and rather being contrib-crawled, that's only done as the target of ""test"" in each contrib directory, at which point, it's running in the protected contrib-crawl.

Easy enough to lift this loop into another target, e.g., build-contrib-test. And that will start surfacing errors, which I can work through.
"
0,"TermVectorMapper.setDocumentNumber()Passes down the index of the document whose term vector is currently beeing mapped, once for each top level call to a term vector reader.  

See http://www.nabble.com/Allowing-IOExceptions-in-TermVectorMapper--tf4687704.html#a13397341"
0,"Patch to JCR-RMI contribution adding Version/VersionHistory supportHi Jukka,

You contributed the famous RMI extension to Jackrabbit. Many thanks. On my way to implement an Eclipse plugin to access repositories this provides great help. Unfortunately your contribution does not include support for versioning yet.

I took the freedom to add this missing piece and provide it to you to add it to your contribution. Thanks."
0,"Convert some tests to new TokenStream API, better support of cross-impl AttributeImpl.copyTo()This patch converts some remaining tests to the new TokenStream API and non-deprecated classes.
This patch also enhances AttributeImpl.copyTo() of Token and TokenWrapper to also support copying e.g. TermAttributeImpl into Token. The target impl must only support all interfaces but must not be of the same type. Token and TokenWrapper use optimized coping without casting to 6 interfaces where possible.
Maybe the special tokenizers in contrib (shingle matrix and so on using tokens to cache may be enhanced by that). Also Yonik's request for optimized copying of states between incompatible AttributeSources may be enhanced by that (possibly a new issue)."
0,"[PATCH] Use entrySet iterators to avoid map look ups in loopsCode uses a keySet iterator in a loop, then does a map look up using the key retrieved from the iterator. 

Might as well use an entrySet iterator to avoid n map lookups.

Patch does this."
0,"Updates to connectionStaleCheckingEnabled docs.Comments from Itai Brickner:

In the Threading section of the UserGuide
(
http://jakarta.apache.org/commons/httpclient/threading.html
)

There is no mentioning of the
'setConnectionStaleCheckingEnabled'
I also felt that it wasn't clear from the APIDOC
(http://jakarta.apache.org/commons/httpclient/apidocs/org/apache/commons/httpclient/MultiThreadedHttpConnectionManager.html)
that staleCheckingEnabled will cause a stale
connection to be reconnected by the
MultiThreadedHttpConnectionManager

thanks,

Itai"
0,"thread pool implementation of parallel queriesThis component is a replacement for ParallelMultiQuery that runs a thread pool
with queue instead of starting threads for every query execution (so its
performance is better)."
0,"Numeric range searching with large value setsI have a set of enhancements that build on the numeric sorting cache introduced
by Tim Jones and that provide integer and floating point range searches over
numeric ranges that are far too large to be implemented via the current term
range rewrite mechanism.  I'm new to Apache and trying to find out how to attach
the source files for the changes for your consideration."
0,"Make ReqExclScorer package private, and use DocIdSetIterator for excluded part."
0,"simple improvements to testsSimon had requested some docs on what all our test options do, so lets clean it up and doc it.

i propose:
# change all vars to be tests.xxx (e.g. tests.threadspercpu, tests.multiplier, ...)
# ensure all 6 build systems (lucene, solr, each solr contrib) respect these.
# add a simple wiki page listing what these do."
0,Stop text extraction when the maxFieldLength limit is reachedWhen indexing large documents the text extraction often takes quite a while and uses lots of memory even if only the first maxFieldLength (by default 10000) tokens are used. I'd like to add a maxExtractLength parameter that can be used to set the maximum number of characters to extract from a binary. The default value of this parameter could be something like ten times the maxFieldLength setting.
0,"IndexReader's add/removeCloseListener should not use ConcurrentHashMap, just a synchronized setThe use-case for ConcurrentHashMap is when many threads are reading and less writing to the structure. Here this is just funny: The only reader is close(). Here you can just use a synchronized HashSet. The complexity of CHM is making this just a joke :-)"
0,"replace collation/lib/icu4j.jar with a smaller icu jarCollation does not need all the icu data.
we can shrink the jar file a bit by using the data customizer, and excluding things like character set conversion tables."
0,"New Jackrabbit site skinSome time ago Michael Eppelheimer from Day created a new skin for the Jackrabbit web site, and I've now streamlined it a bit and integrated it with the Maven site build mechanism.

The templates should be easy to adapt also for Confluence when we get around to that migration."
0,"Node Type Management subproject : Default namespace should be emtpyWhen creating node types matching to the class descriptors,  the default namespace should be empty instead of  'ocm'."
0,Some Workspace tests require a second workspaceSome workspace test require a second workspace even though it is not used in the test cases.
0,"Enforce TokenStream impl / Analyzer finalness by an assertionAs noted in LUCENE-1753 and other issues, TokenStream and Analyzers are based on the decorator pattern. At least all TokenStream and Analyzer implementations in Lucene and Solr should be final.

The attached patch adds an assertion to the ctors of both classes that does the corresponding checks:
- Analyzers must be final or private classes or anonymous inner classes
- TokenStreams must be final or private classes or anonymous inner classes or have a final incrementToken()

I will commit this after robert have fixed solr streams."
0,"Provide More of Lucene For MavenPlease provide javadoc & source jars for lucene-core.  Also, please provide the rest of lucene (the jars inside of ""contrib"" in the download bundle) if possible."
0,"CompoundFileWriter should pre-set its file lengthI've read that if you are writing a large file, it's best to pre-set
the size of the file in advance before you write all of its contents.
This in general minimizes fragmentation and improves IO performance
against the file in the future.

I think this makes sense (intuitively) but I haven't done any real
performance testing to verify.

Java has the java.io.File.setLength() method (since 1.2) for this.

We can easily fix CompoundFileWriter to call setLength() on the file
it's writing (and add setLength() method to IndexOutput).  The
CompoundFileWriter knows exactly how large its file will be.

Another good thing is: if you are going run out of disk space, then,
the setLength call should fail up front instead of failing when the
compound file is actually written.  This has two benefits: first, you
find out sooner that you will run out of disk space, and, second, you
don't fill up the disk down to 0 bytes left (always a frustrating
experience!).  Instead you leave what space was available
and throw an IOException.

My one hesitation here is: what if out there there exists a filesystem
that can't handle this call, and it throws an IOException on that
platform?  But this is balanced against possible easy-win improvement
in performance.

Does anyone have any feedback / thoughts / experience relevant to
this?
"
0,"Occasional IndexingQueueTest failuresEvery now and then, when doing a clean build of the latest jackrabbit trunk I see the following test failure in jackrabbit-core:

-------------------------------------------------------------------------------
Test set: org.apache.jackrabbit.core.query.lucene.TestAll
-------------------------------------------------------------------------------
Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.665 sec <<< FAILURE!
testQueue(org.apache.jackrabbit.core.query.lucene.IndexingQueueTest)  Time elapsed: 1.654 sec  <<< FAILURE!
junit.framework.AssertionFailedError
        at junit.framework.Assert.fail(Assert.java:47)
        at junit.framework.Assert.assertTrue(Assert.java:20)
        at junit.framework.Assert.assertTrue(Assert.java:27)
        at org.apache.jackrabbit.core.query.lucene.IndexingQueueTest.testQueue(IndexingQueueTest.java:69)

Typically the problem disappears when I rebuild, but the test should still not have failed."
0,"ItemImpl#validateTransientItems: Incomplete validation of mandatory child itemItemImpl#validateTransientItems iterates over all mandatory child node/property definitions in order to assert that those items have
been created. However, it only checks if an item with the name defined by the mandatory item definition is present and not if that
existing item really has the mandatory definition.

the example i had:
- mandatory single-value property.
- there is the possibility to add residual props
- added a residual property with the name of the mandatory prop but with multiple values

-> changes are saved without exception.
-> the node doesn't have a property with the mandatory definition.

((without having tried it out, i think the same would be possible with child nodes))

suggested fix:
if there is a child item with the mandatory-item-name -> make sure it's definition is mandatory (or the expected one...)
patch will follow.
"
0,"org.apache.jackrabbit.server.SessionProvider needs 'releaseSession()'the SessionProvider does not have a 'releaseSession()' method, thus the DavSessionProviderImpl just calls repSession.logout() after is DavSession is released. This should rather be handled over to the given SessionProvider."
0,"Define and implement Logging policyWhen to use info vs debug vs warning?  
When to log exceptions?
enter() and exit() messages on a per method basis?
Always log debug when swallowing an exception?"
0,"Improve lifecycle management of JCA connectorthe shutdown mechanism doesn't work correctly. It shutdowns the repository when the RA (resource adapter) is garbage collected. It causes redeployment to fail because sometimes the new RA is redeployed before the old one is garbage collected.

Implementing the JCA 1.5 interface to manage the lifecycle would be useful."
0,"[PATCH] LockFactory implementation based on OS native locks (java.nio.*)The current default locking for FSDirectory is SimpleFSLockFactory.
It uses java.io.File.createNewFile for its locking, which has this
spooky warning in Sun's javadocs:

    Note: this method should not be used for file-locking, as the
    resulting protocol cannot be made to work reliably. The FileLock
    facility should be used instead.

So, this patch provides a LockFactory implementation based on FileLock
(using java.nio.*).

All unit tests pass with this patch, on OS X (10.4.8), Linux (Ubuntu
6.06), and Windows XP SP2.

Another benefit of native locks is the OS automatically frees them if
the JVM exits before Lucene can free its locks.  Many people seem to
hit this (old lock files still on disk) now.

I've created this new class:

  org.apache.lucene.store.NativeFSLockFactory

and added a couple test cases to the existing TestLockFactory.

I've left SimpleFSLockFactory as the default locking for FSDirectory
for now.  I think we should get some usage / experience with
NativeFSLockFactory and then later on make it the default locking
implementation?

I also tested changing FSDirectory's default locking to
NativeFSLockFactory and all unit tests still pass (on the above
platforms).

One important note about locking over NFS: some NFS servers and/or
clients do not support it, or, it's a configuration option or mode
that must be explicitly enabled.  When it's misconfigured it's able to
take a long time (35 seconds in my case) before throwing an exception.
To handle this, I acquire & release a random test lock on creating the
NativeFSLockFactory to verify locking is configured properly.

A few other small changes in the patch:

    - Added a ""failure reason"" to Lock.java so that in
      obtain(lockWaitTimeout), if there is a persistent IOException
      in trying to obtain the lock, this can be messaged & included in
      the ""Lock obtain timed out"" that's raised.

    - Corrected javadoc in SimpleFSLockFactory: it previously said the
      wrong system property for overriding lock class via system
      properties

    - Fixed unhandled IOException when opening an IndexWriter for
      create, if the locks dir does not exist (just added
      lockDir.exists() check in clearAllLocks method of
      SimpleFSLockFactory & NativeFSLockFactory.

    - Fixed a few small unrelated issues with TestLockFactory, and
      also fixed tests to accept NativeFSLockFactory as the default
      locking implementation for FSDirectory.

    - Fixed a typo in javadoc in FieldsReader.java

    - Added some more javadoc for the LockFactory.setLockPrefix
"
0,"Fix incorrect IndexingQueueTest logicThe IndexingQueueTest class assumes that a Session.save() call will push all pending text extraction tasks to the indexing queue, when in fact those can still be kept waiting in the VolatileIndex."
0,"Back-compat break with non-ascii field namesIf a field name contains non-ascii characters in a 2.3.x index, then
on upgrade to 2.4.x unexpected problems are hit.  It's possible to hit
a ""read past EOF"" IOException; it's also possible to not hit an
exception but get an incorrect field name.

This was caused by LUCENE-510, because the FieldInfos (*.fnm) file is
not properly versioned.

Spinoff from http://www.nabble.com/Read-past-EOF-td23276171.html
"
0,"rename jcr-browser contrib projectThere's a project called jcr-browser at sourceforge, it's a desktop browser mantained by sandro boehme. I'll rename the contrib project to jcr-navigator unless someone proposes a better name :). "
0,"rename expungeDeletesSimilar to optimize(), expungeDeletes() has a misleading name.

We already had problems with this on the user list because TieredMergePolicy
didn't 'expunge' all their deletes.

Also I think expunge is the wrong word, because expunge makes it seem
like you just wrangle up the deletes and kick them out of the party and
that it should be fast.



"
0,"Add a new TestBackwardsCompatibility index for flex backwards (a 3.0 one with also numeric fields)In flex we change also the encode/decoder for numeric fields (NumericTokenSteam) using BytesRef and also the collation filters. We should add a test index from 3.0 that contains these fields and do some validation, that field contents did not change when read with flex."
0,"Initializing SeededSecureRandom may be slowFor systems where reading from /dev/random is very slow (so that the alternative seed algorithm is used), initializing the org.apache.jackrabbit.core.id.SeededSecureRandom singleton may be very slow, because it is not synchronized. Each thread that calls SeededSecureRandom.getInstance() will wait up to 1 second until the singleton is initialized.

At the same time, I would like to add more entropy to the alternative seed algorithm.
"
0,"contrib/remote tests fail randomlyThe contrib/remote tests will fail randomly.

This is because they use this _TestUtil.getRandomSocketPort() which
simply generates a random number, but if this is already in use, it will fail.

Additionally there is duplicate RMI logic across all 3 test classes."
0,"make compoundfilewriter publicCompoundFileReader is public, but CompoundFileWriter is not.

I propose we make it public + @lucene.internal instead (just in case someone 
else finds themselves wanting to manipulate cfs files)
"
0,"TCK: RestoreTest.testRestoreLabelAccording to tobi the jackrabbit implementation of 'Node.restoreByLabel' is an interpretation of the
specification regarding the restore behaviour of versionable child nodes. while that interpetration might
be legal unless the specification is violated, i would argue that the TCK should not test the interpretation.

therefore i suggest to modify

org.apache.jackrabbit.test.api.version.RestoreTest.testRestoreLabel

by skipping line 334 - 345 in order to limit the test case to the behaviour that is defined by the specification.

regards
angela

ps: the mentioned test is also executed within the scope of WorkspaceRestoreTest because the latter  extends RestoreTest.... that's misleading."
0,Rename package namesRename package names  *graffito* into *jackrabbit*
0,"lock token validityThere are several minor issues in the mapping between JCR lock tokens and WebDAV lock tokens:

1) WebDAV lock tokens are supposed to use URI syntax (such as opaquelocktoken: or urn:uuid:)

2) The server currently computes lock tokens for session-scoped locks based on the node id; these are not valid JCR lock tokens though and cause exceptions when they are re-added when they appear in a Lock-Token header or an If header. This will likely cause requests to fail that use both types of locks (yes, maybe academic but should be fixed anyway)

Proposal:

a) Map lock tokens to oqaquelocktoken URIs, using a constant UUID plus a postfix encoding the original lock token
b) Use a syntax that allows to distinguish between tokens for open-scoped locks or session-scoped locks, so that we do not try to add the latter type to the Session (alternatively, handle exceptions doing so gracefully)"
0,"Add Warnlog on Extraction FailureIt will be fine to have a feedback if a exception occurs in the TextExtractors.
At the moment only a empty StringReader will be returned.
We had the issue that we updated the content and in the textextractor a exception occured
so the index was not updated and the document was searchable by its old content."
0,"Further parallelizaton of ParallelMultiSearcherWhen calling {{search(Query, Filter, int)}} on a ParallelMultiSearcher, the {{createWeights}} function of MultiSearcher is called, and sequentially calls {{docFreqs()}} on every sub-searcher. This can take a significant amount of time when there are lots of remote sub-searchers.

"
0,"contrib/benchmark QueryMaker and Task RefactoringsIntroduce an abstract QueryMaker implementation that shares much of the common code between the various QueryMaker implementations.

Add in a new QueryMaker for reading queries from a file that is specified in the properties.

Patch shortly, and if no concerns, will commit tomorrow or Wed."
0,QueryHandler.init() should take a context argumentCurrently the QueryHandler.init() method takes a bunch of arguments which are needed by the single jackrabbit implementation for the query handler. To make further extensions easier the arguments should be packaged into a context class which can be extended without effect on the QueryHandler interface.
0,"Make it posible not to include TF information in indexTerm Frequency is typically not needed  for all fields, some CPU (reading one VInt less and one X>>>1...) and IO can be spared by making pure boolen fields possible in Lucene. This topic has already been discussed and accepted as a part of Flexible Indexing... This issue tries to push things a bit faster forward as I have some concrete customer demands.

benefits can be expected for fields that are typical candidates for Filters, enumerations, user rights, IDs or very short ""texts"", phone  numbers, zip codes, names...

Status: just passed standard test (compatibility), commited for early review, I have not tried new feature, missing some asserts and one two unit tests

Complexity: simpler than expected

can be used via omitTf() (who used omitNorms() will know where to find it :)  "
0,make NamespaceContext#getPrefix(java.lang.String) iterative instead of recursiveCurrently the method org.apache.jackrabbit.core.xml.NamespaceContext#getPrefix(java.lang.String) uses recursion. For very large XML files (50 MB Magnolia website exports) this causes a stack overflow. The method can easily be rewritten using iteration.
0,"CMS should default its maxThreadCount to 1 (not 3)From rough experience, I think the current default of 3 is too large.  I think we get the most bang for the buck going from 0 to 1.

I think this will especially impact optimize on an index with many segments -- in this case the MergePolicy happily exposes concurrency (multiple pending merges), and CMS will happily launch 3 threads to carry that out."
0,Change contrib tests to use the special LuceneTestCase(J4) constant for the current version used a matchVersion parameterSub issue for contrib changes
0,"Determination of property state difference should skip binary valueso.a.j.jcr2spi.state.PropertyState.diffPropertyData, PropertyData) should alway consider two binary values to be different. The current implementation compares two binary values with equals(). An implementation will in general have to do a byte by byte comparison of both values. This is most likely always more expensive than considering the values different right from the start. 

"
0,"intermittent failures of  TestTimeLimitedCollector.testTimeoutMultiThreaded in nightly testsOccasionly TestTimeLimitedCollector.testTimeoutMultiThreaded fails. e.g. with this output:

{noformat}
   [junit] ------------- Standard Error -----------------
   [junit] Exception in thread ""Thread-97"" junit.framework.AssertionFailedError: no hits found!
   [junit]     at junit.framework.Assert.fail(Assert.java:47)
   [junit]     at junit.framework.Assert.assertTrue(Assert.java:20)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.doTestTimeout(TestTimeLimitedCollector.java:152)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.access$100(TestTimeLimitedCollector.java:38)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector$1.run(TestTimeLimitedCollector.java:231)
   [junit] Exception in thread ""Thread-85"" junit.framework.AssertionFailedError: no hits found!
   [junit]     at junit.framework.Assert.fail(Assert.java:47)
   [junit]     at junit.framework.Assert.assertTrue(Assert.java:20)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.doTestTimeout(TestTimeLimitedCollector.java:152)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.access$100(TestTimeLimitedCollector.java:38)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector$1.run(TestTimeLimitedCollector.java:231)
   [junit] ------------- ---------------- ---------------
   [junit] Testcase: testTimeoutMultiThreaded(org.apache.lucene.search.TestTimeLimitedCollector):      FAILED
   [junit] some threads failed! expected:<50> but was:<48>
   [junit] junit.framework.AssertionFailedError: some threads failed! expected:<50> but was:<48>
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.doTestMultiThreads(TestTimeLimitedCollector.java:255)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.testTimeoutMultiThreaded(TestTimeLimitedCollector.java:220)
   [junit]
{noformat}

Problem either in test or in TimeLimitedCollector."
0,"TCK: AbstractImportXmlTest incorrectly assumes mix:referenceable can be added to created nodeisMixRefRespected() assumes that if the NodeTypeManager contains mix:referenceable, addMixin can be called to add mix:referenceable to a created node.  This assumption is incorrect for at least two reasons.  First, the created node might already be mix:referenceable, either because its primary node type is a subtype of mix:referenceable or because the implementation added mix:referenceable as a mixin type in creating the node.  Second, a repository may restrict the nodes to which mix:referenceable can be added.  In the extreme case, the repository may not allow mix:referenceable to be added to any node using addMixin, in which case the only referenceable nodes would be those which are mix:referenceable by virtue of primary type or the implementation's adding mix:referenceable as a mixin type at node creation.

Proposal: test canAddMixin before calling addMixin.
"
0,"specialize payload processing from of DocsAndPositionsEnumIn LUCENE-2760 i started working to try to improve the speed of a few spanqueries.
In general the trick there is to avoid processing positions if you dont have to.

But, we can improve queries that read lots of positions further by cleaning up SegmentDocsAndPositionsEnum, 
in nextPosition() this has no less than 3 payloads-related checks.

however, a large majority of users/fields have no payloads at all.
I think we should specialize this case into a separate implementation and speed up the common case.

edit: dyslexia with the jira issue number."
0,"Add ""testpackage"" to common-build.xmlOne can define ""testcase"" to execute just one test class, which is convenient. However, I didn't notice any equivalent for testing a whole package. I find it convenient to be able to test packages rather than test cases because often it is not so clear which test class to run.

Following patch allows one to ""ant test -Dtestpackage=search"" (for example) and run all tests under the \*/search/\* packages in core, contrib and tags, or do ""ant test-core -Dtestpackage=search"" and execute similarly just for core, or do ""ant test-core -Dtestpacakge=lucene/search/function"" and run all the tests under \*/lucene/search/function/\* (just in case there is another o.a.l.something.search.function package out there which we want to exclude."
0,Replace xerces for serialization by JAXPThe org.apache.jackrabbit.rmi.xml.ImportContentHandler class currently uses Xerces to implement the SAX DocumentHandler and serialize XML into a byte[]. This dependency should be dropped and JAXP be used instead for this functionality.
0,IndexReader.FieldOption has incomplete JavadocsIndexReader.FieldOption has no javadocs at all.
0,ReadOnlyIndexReaders are re-created on every accessAbstractIndex.getReadOnlyIndexReader() creates a new instance on every call. The returned index reader should instead be cached and kept open as long as there are no changes on the underlying index.
0,"Fix charset problems in XML loading in HyphenationCompoundWordTokenFilter (also Solr's loader from schema)As said in LUCENE-2731, the handling of XML in HyphenationCompoundWordTokenFilter is broken and breaks XML 1.0 (5th edition) spec totally. You should never supply a Reader to any XML api, unless you have internal character data (e.g. created programmatically). Also you should supply a system id, as resolving external entities does not work. The loader from files is much more broken, it always open the file as a Reader and then passes it to InputSource. Instead it should point filename directly to InputSource.

This issue will fix it in trunk and use InputSource in Solr, but will still supply the Reader possibility in previous versions (deprecated)."
0,"Visibility of Scorer.score(Collector, int, int) is wrongThe method for scoring subsets in Scorer has wrong visibility, its marked protected, but protected methods should not be called from other classes. Protected methods are intended for methods that should be overridden by subclasses and are called by (often) final methods of the same class. They should never be called from foreign classes.

This method is called from another class out-of-scope: BooleanScorer(2) - so it must be public, but it's protected. This does not lead to a compiler error because BS(2) is in same package, but may lead to problems if subclasses from other packages override it. When implementing LUCENE-2838 I hit a trap, as I thought tis method should only be called from the class or Scorer itsself, but in fact its called from outside, leading to bugs, because I had not overridden it. As ConstantScorer did not use it I have overridden it with throw UOE and suddenly BooleanQuery was broken, which made it clear that it's called from outside (which is not the intention of protected methods).

We cannot fix this in 3.x, as it would break backwards for classes that overwrite this method, but we can fix visibility in trunk.
"
0,"Split PrivilegeRegistry in a per-session manager instance and a repository level registryin order to resolve the privilegeregistry related TODOs within jackrabbit-core, i would like to split off those 
methods from PrivilegeRegistry  that are used on a per-session level (including jcr-names) and add them
to a manager class that was present with each session context. consequently the responsibility of the
registry was then limited to read/build the privilege definitions and would be present on the repositorycontext
deprecating those methods that would be covered by the manager).
in addition the naming was then consistent with what we use to have for nodetypes and namespaces."
0,AbstractWebdavServlet: add protected method sendUnauthorized
0,"Lucene Scorer implementations should handle the 'advance' to NO_MORE_DOCS optimisation betterThis is from the lucene Scorer (actually DocIdSetIterator) api:
""NOTE: this method may be called with NO_MORE_DOCS for efficiency by some Scorers. If your implementation cannot efficiently determine that it should exhaust, it is recommended that you check for that value in each call to this method.""

None of the scorer implementations does that currently. Except for ChildAxisScorer thanks to JCR-3082.

This is a worthwhile effort, which can save us from bugs (JCR-3082) but also leverage some performance optimisation hints from the lucene api."
0,Spellchecker uses default IW mergefactor/ramMB settings of 300/10These settings seem odd - I'd like to investigate what makes most sense here.
0,"Speed up SegementDocsEnum by making it more friendly for JIT optimizationsSince we moved the bulk reading into the codec ie. make all  bulk reading codec private in LUCENE-3584 we have seen some performance [regression|http://people.apache.org/~mikemccand/lucenebench/Term.html] on different CPUs. I tried to optimize the implementation to make it more eligible for runtime optimizations, tried to make loops JIT friendly by moving out branches where I can, minimize member access in all loops, use final members where possible and specialize the two common cases With & Without LiveDocs.

I will attache a patch and my benchmark results in a minute."
0,"Incorrect/incomplete product name in META-INF/NOTICE fileThe NOTICE file in the HttpClient jars is incorrect.

It states:

=====

HttpClient
Copyright 1999-2009 Apache Software Foundation
<snip/>
======

The leading blank line should be deleted, and ""HttpClient"" should be ""Apache HttpComponents Client - HttpClient""  (or similar) as is the case for the source archive.

Similarly for HttpMime"
0,Add workspace population toolAdd a simple tool to jackrabbit-webapp to populate the workspace with content.
0,"Add support for terms in BytesRef format to Term, TermQuery, TermRangeQuery & Co.It would be good to directly allow BytesRefs in TermQuery and TermRangeQuery (as both queries convert the strings to BytesRef internally). For NumericRange support in Solr it will be needed to support numerics as ByteRef in single-term queries.

When this will be added, don't forget to change TestNumericRangeQueryXX to use the BytesRef ctor of TRQ."
0,Make HighFreqTerms.TermStats class publicIt's not possible to use public methods in contrib/misc/... /HighFreqTerms from outside the package because the return type has package visibility. I propose to move TermStats class to a separate file and make it public.
0,"Allow importing of ACL with unknown principalsIt should be possible to import ACLs with principals that are not known to the principal provider, yet."
0,"Investigate ways to compile the refactored jcr-mapping for Java 1.4The last refactoring of the jcr-mapping project included the annotation based mapping description into the main code based thus requiring compilation with Java 5 or higher.

There are still some use cases, which require Java 1.4. The goal is to investigate, whether it would be possible to define a build profile in the pom, which compiles for 1.4 by ignoring the annotation classes."
0,"add infrastructure for longer running nightly test casesI'm spinning this out of LUCENE-2762...

The patch there adds initial infrastructure for tests to pull documents from a line file, and adds a longish running test case using that line file to test NRT.

I'd like to see some tests run on more substantial indices based on real data... so this is just a start."
0,"investigate solr test failures using flexWe have a branch of Solr located here: https://svn.apache.org/repos/asf/lucene/solr/branches/solr

Currently all the tests pass with lucene trunk jars.

I plopped in the flex jars and they do not, so I thought these might be interesting to look at.
"
0,"Performance improvement for SegmentMerger.mergeNorms()This patch makes two improvements to SegmentMerger.mergeNorms():

1) When the SegmentMerger merges the norm values it allocates a new byte array to buffer the values for every field of every segment. The size of such an array equals the size of the corresponding segment, so if large segments are being merged, those arrays become very large, too.
We can easily reduce the number of array allocations by reusing a byte array to buffer the norm values that only grows, if a segment is larger than the previous one.

2) Before a norm value is written it is checked if the corresponding document is deleted. If not, the norm is written using IndexOutput.writeByte(byte[]). This patch introduces an optimized case for segments that do not have any deleted docs. In this case the frequent call of IndexReader.isDeleted(int) can be avoided and the more efficient method IndexOutput.writeBytes(byte[], int) can be used.


This patch only changes the method SegmentMerger.mergeNorms(). All unit tests pass."
0,"Add ability to open prior commits to IndexReaderIf you use a customized DeletionPolicy, which keeps multiple commits
around (instead of the default which is to only preserve the most
recent commit), it's useful to be able to list all such commits and
then open a reader against one of these commits.

I've added this API to list commits:

  public static Collection IndexReader.listCommits(Directory)

and these two new open methods to IndexReader to open a specific commit:

  public static IndexReader open(IndexCommit)
  public static IndexReader open(IndexCommit, IndexDeletionPolicy)

Spinoff from here:

  http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200806.mbox/%3c85d3c3b60806161735o207a3238sa2e6c415171a8019@mail.gmail.com%3e

"
0,"Add files generated by eclipse or maven to svn:ignoreTo make life easier for eclipse and maven users please add the following files to svn:ingore:

jackrabbit-jcr-rmi:
-------------------
.settings
.classpath
.project
jackrabbit-jcr-rmi-pom-snapshot-version
project.xml.md5

jackrabbit-core:
----------------
.settings
.classpath
.project
jackrabbit-core-pom-snapshot-version
project.xml.md5

Maybe there some files missing in this list that could help developers using IDEA?"
0,"add Terms.docCountspinoff from LUCENE-3290, where yonik mentioned:

{noformat}
Is there currently a way to get the number of documents that have a value in the field?
Then one could compute the average length of a (sparse) field via sumTotalTermFreq(field)/docsWithField(field)
docsWithField(field) would be useful in other contexts that want to know how sparse a field is (automatically selecting faceting algorithms, etc).
{noformat}

I think this is a useful stat to add, in case you have sparse fields for heuristics or scoring."
0,"Http Client give sme message when proxy/http endpoint is downWhether Http sever endpoint is down or the proxy server is down we get the same stack trace as:

java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333)
	at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
	at java.net.Socket.connect(Socket.java:518)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.commons.httpclient.protocol.ReflectionSocketFactory.createSocket(ReflectionSocketFactory.java:139)
	at org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory.createSocket(DefaultProtocolSocketFactory.java:124)
	at org.apache.commons.httpclient.HttpConnection.open(HttpConnection.java:706)
	at org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpConnectionAdapter.open(MultiThreadedHttpConnectionManager.java:1321)
	at org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:386)
	at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:170)
	at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:396)
	at com.approuter.module.http.protocol.HttpTransportSender.perform(HttpTransportSender.java:214)
	at 


It will be good if we can get information whether the proxy was down or the Http endpoint.

"
0,"test granularity for calendar (date) propertiesThere are repositories out there that do support properties of type Date, but not Calendar (the main difference being that Calendar also captures the time zone). Also, some repositories may not be able to store timestamps with millisecond resolution.

Although both these restrictions make a repository non-compliant, it would be useful for the tests to test these aspects as separate issues. Thus I propose to simplify the existing tests so that they just compare timestamps (factoring out the time zone), and do not require resolution finer than 1s. These two aspects then should be tested in a separate test case (thinking of it, they currently may not test sub-second resolution, in which case I propose to leave things as they are with respect to this).

"
0,Rename EasySimilarity to SimilarityBase
0,"Optimization for FieldDocSortedHitQueueWhen updating core for generics,  I found the following as a optimization of FieldDocSortedHitQueue:

All FieldDoc values are Compareables (also the score or docid, if they
appear as SortField in a MultiSearcher or ParallelMultiSearcher). The code
of lessThan seems very ineffective, as it has a big switch statement on the
SortField type, then casts the value to the underlying numeric type Object,
calls Number.xxxValue() & co for it and then compares manually. As
j.l.Number is itself Comparable, I see no reason to do this. Just call
compareTo on the Comparable interface and we are happy. The big deal is that
it prevents casting and the two method calls xxxValue(), as Number.compareTo
works more efficient internally.

The only special cases are String sort, where the Locale may be used and the
score sorting which is backwards. But these are two if statements instead of
the whole switch.

I had not tested it now for performance, but in my opinion it should be
faster for MultiSearchers. All tests still pass (because they should).
"
0,"Make CachingTokenFilter fasterThe LinkedList used by CachingTokenFilter is accessed using the get() method. Direct access on a LinkedList is slow and an Iterator should be used instead. For more than a handful of tokens, the difference in speed grows exponentially."
0,"Add QueryParser.newFieldQueryNote: this patch changes no behavior, just makes QP more subclassable.

Currently we have Query getFieldQuery(String field, String queryText, boolean quoted)
This contains very hairy methods for producing a query from QP's analyzer.

I propose we factor this into newFieldQuery(Analyzer analyzer, String field, String queryText, boolean quoted)
Then getFieldQuery just calls newFieldQuery(this.analyzer, field, queryText, quoted);

The reasoning is: it can be quite useful to consider the double quote as more than phrases, but a ""more exact"" search.
In the case the user quoted the terms, you might want to analyze the text with an alternate analyzer that:
doesn't produce synonyms, doesnt decompose compounds, doesn't use WordDelimiterFilter 
(you would need to be using preserveOriginal=true at index time for the WDF one), etc etc.

This is similar to the way google's double quote operator works, its not defined as phrase but ""this exact wording or phrase"".
For example compare results to a query of tests versus ""tests"".

Currently you can do this without heavy code duplication, but really only if you make a separate field (which is wasteful),
and make your custom QP lie about its field... in the examples I listed above you can do this with a single field, yet still
have a more exact phrase search.
"
0,"more lenient behavior of Node#addMixin if mixin is already present Change implementation of addMixin() so that it doesn't fail when the mixin is already present.

See also:

jackrabbit core change: <http://svn.apache.org/viewvc?view=rev&revision=570149>

JSR-283 issue: <https://jsr-283.dev.java.net/issues/show_bug.cgi?id=353>

(this affects both the TCK and JCR2SPI, so I didn't specify a component)"
0,"Test failures when running ""mvn cobertura:check""It looks like the bytecode instrumentation done by Cobertura interferes with the rather complex XPathTokenManager class produced by JavaCC.

The easiest workaround seems to be to simply exclude XPathTokenManager from being instrumented by Cobertura."
0,"Add set/getLocalAddress methods to HostConfigurationOn clustered or multi-homed systems, there's a need to specify the local bind
address of sockets, to ensure that they're created on the right interface.  To
do this, the local address needs to be passed to the 4-argument version of
ProtcolSocketFactory.createSocket.

After discussion on the mailing list, the best approach for this seems to be
adding the local address as a property on HostConfiguration and HttpConnection.  

I've attached a patch which does the following:
- Add public set/getLocalAddress methods to HostConfiguration and HttpConnection.
- HttpConnection uses the local address when opening connections.
- Modify HostConfiguration.equals and hostEquals to compare the local address too.
- SimpleHttpConnectionManager uses the local address from the provided config. 
I also cleaned up its getConnection method a bit.
- HttpClient.executeMethod uses the local address from its default
HostConfiguration if the method's config doesn't specify one."
0,counter field in segments file is not documented in fileformats.xmlThe counter field in the current segments file format is not documented.
0,TestFieldsReader - TestLazyPerformance problems w/ permissions in temp dir in multiuser environmentWas trying to setup some enhancements to the nightly builds and the testLazyPerformance test failed in TestFieldsReader since it couldn't delete the lazyDir directory from someone else's running of the test.  Will change it to append user.name System property to the directory name.
0,"Move ocm documentation to jackrabbit-siteThe OCM documentation from jackrabbit-ocm/xdocs should be moved to jackrabbit-site.

Also all old references to Graffito should be replaced with Jackrabbit."
0,"Change all FilteredTermsEnum impls into TermsEnum decoratorsCurrently, FilteredTermsEnum has two ctors:
* FilteredTermsEnum(IndexReader reader, String field)
* FilteredTermsEnum(TermsEnum tenum)

But most of our concrete implementations (e.g. TermsRangeEnum) use the IndexReader+field ctor

In my opinion we should remove this ctor, and switch over all FilteredTermsEnum implementations to just take a TermsEnum.

Advantages:
* This simplifies FilteredTermsEnum and its subclasses, where they are more decorator-like (perhaps in the future we could compose them)
* Removes silly checks such as if (tenum == null) in every next()
* Allows for consumers to pass in enumerators however they want: e.g. its their responsibility if they want to use MultiFields or not, it shouldnt be buried in FilteredTermsEnum.

I created a quick patch (all core+contrib+solr tests pass), but i think this opens up more possibilities for refactoring improvements that haven't yet been done in the patch: we should explore these too.
"
0,"Move listeners from item state to item state managersClients interested in item state modifications directly subscribe to the item states, which is a very flexible approach. On the other side, it increases the memory consumption of an item state, because every item state holds a collection of its listeners. It further increases complexity, because item state listeners can potentially have a shorter life and might be garbage collected.

Listeners should therefore be moved to their associated item state manager. At the same time, this enables an item state manager to completely remove an item state from its cache and resurrect it at a later time without losing the listeners interested in notifications."
0,"[PATCH] cleaner API for Field.TextCurrently there are four methods named Field.Text(). As those methods have 
the same name and a very similar method signature, everyone will think these 
are just convenience methods that do the same thing. But they behave 
differently: the one that takes a Reader doesn't store the data, the one that 
takes a String does. I know that this is documented, but it's still not a nice 
API. Methods that behave differently should have diffent names. The attached 
patch deprecates two of the old methods and adds two new ones named 
Field.StoredText(). I think this is much easier to understand from the 
programmer's point-of-view and will help avoid bugs."
0,"improve test coverage for Multi*It seems like an easy win that when the test calls newSearcher(), 
it should sometimes wrap the reader with a SlowMultiReaderWrapper.
"
0,"Missing derby dependencythe derby dependency is missing in the OCM subproject. So, the unit tests cannot be executed. "
0,"Typo in PropertyDefinitionTemplatesetValueConstarints should read setValueConstraints


"
0,"Deprecate all non-bundle persistence managersBundle persistence has been the recommended default since Jackrabbit 1.3, and there is little reason for anyone to be using non-bundle persistence anymore. Thus I'd like to deprecate all non-bundle PMs in Jackrabbit 2.2 and target for their removal in Jackrabbit 3.0."
0,"TCK: TextNodeTest and jcr:xmltext/jcr:xmlcharactersTest creates jcr:xmltext nodes without jcr:xmlcharacters properties.  Some repositories may require jcr:xmltext nodes to have jcr:xmlcharacters properties, causing this test to fail.

Proposal: add a jcr:xmlcharacters property to each jcr:xmltext node.

--- TextNodeTest.java   (revision 422074)
+++ TextNodeTest.java   (working copy)
@@ -62,6 +62,7 @@
      */
     public void testTextNodeTest() throws RepositoryException {
         Node text1 = testRootNode.addNode(jcrXMLText);
+        text1.setProperty(jcrXMLCharacters, ""foo"");
         testRootNode.save();
         String xpath = ""/"" + jcrRoot + testRoot + ""/text()"";
         executeXPathQuery(superuser, xpath, new Node[]{text1});
@@ -73,7 +74,9 @@
      */
     public void testTextNodeTestMultiNodes() throws RepositoryException {
         Node text1 = testRootNode.addNode(jcrXMLText);
+        text1.setProperty(jcrXMLCharacters, ""foo"");
         Node text2 = testRootNode.addNode(nodeName1, testNodeType).addNode(jcrXMLText);
+        text2.setProperty(jcrXMLCharacters, ""foo"");
         testRootNode.save();
         String xpath = ""/"" + jcrRoot + testRoot + ""//text()"";
         executeXPathQuery(superuser, xpath, new Node[]{text1, text2});
@@ -105,11 +108,13 @@
             throw new NotExecutableException(""Repository does not support position index"");
         }
         Node text1 = testRootNode.addNode(jcrXMLText);
+        text1.setProperty(jcrXMLCharacters, ""foo"");
         if (!text1.getDefinition().allowsSameNameSiblings()) {
             throw new NotExecutableException(""Node at path: "" + testRoot + "" does not allow same name siblings with name: "" + jcrXMLText);
         }
         testRootNode.addNode(nodeName1, testNodeType);
         Node text2 = testRootNode.addNode(jcrXMLText);
+        text1.setProperty(jcrXMLCharacters, ""foo"");
         testRootNode.save();
         String xpath = ""/"" + jcrRoot + testRoot + ""/text()[2]"";
         executeXPathQuery(superuser, xpath, new Node[]{text2});
"
0,"CachingSpanFilter synchronizing on a none final protected objectCachingSpanFilter and CachingWrapperFilter expose their internal cache via a protected member which is lazily instantiated in the getDocSetId method. The current code yields the chance to double instantiate the cache and internally synchronizes on a protected none final member. My first guess is that this member was exposed for testing purposes so it should rather be changed to package private. 

This patch breaks backwards compat while I guess the cleanup is kind of worth breaking it."
0,"internal hashing improvementsInternal power-of-two closed hashtable traversal in DocumentsWriter and CharArraySet could be better.

Here is the current method of resolving collisions:
    if (text2 != null && !equals(text, len, text2)) {
      final int inc = code*1347|1;
      do {
        code += inc;
        pos = code & mask;
        text2 = entries[pos];
      } while (text2 != null && !equals(text, len, text2));

The problem is that two different hashCodes with the same lower bits will keep picking the same slots (the upper bits will be ignored).
This is because multiplication (*1347) only really shifts bits to the left... so given that the two codes already matched on the right, they will both pick the same increment, and this will keep them on the same path through the table (even though it's being added to numbers that differ on the left).  To resolve this, some bits need to be moved to the right when calculating the increment.

"
0,"Alternative depth-based DOT layout ordering in FST's UtilsUtils.toDot() dumps GraphViz's DOT file, but it can be quite difficult to read. This patch provides an alternative layout that is probably a little bit easier on the eyes (well, as far as larger FSTs can be ;)"
0,"HttpParams doesn't document its key valueshttp://hc.apache.org/httpcomponents-core/httpcore/apidocs/org/apache/http/params/HttpParams.html should either list the meaningful parameter names with the meanings of their values, or should link to the other classes like HttpClientParams and AuthParams that make it usable. Probably, each class that uses HttpParams should also describe the meaningful values so that human readers find the descriptions however we look for them."
0,"Change BooleanFilter to have only a single clauses ArrayList (so toString() works fine, clauses() method could be added) so it behaves more lik BooleanQueryThis is unrelated to the other BF changes, but should be done"
0,"TopTermsScoringBooleanQueryRewrite minscorewhen using the TopTermsScoringBooleanQueryRewrite (LUCENE-2123), it would be nice if MultiTermQuery could set an attribute specifying the minimum required score once the Priority Queue is filled. 

This way, FilteredTermsEnums could adjust their behavior accordingly based on the minimal score needed to actually be a useful term (i.e. not just pass thru the pq)

An example is FuzzyTermsEnum: at some point the bottom of the priority queue contains words with edit distance of 1 and enumerating any further terms is simply a waste of time.
This is because terms are compared by score, then termtext. So in this case FuzzyTermsEnum could simply seek to the exact match, then end.

This behavior could be also generalized for all n, for a different impl of fuzzyquery where it is only looking in the term dictionary for words within edit distance of n' which is the lowest scoring term in the pq (they adjust their behavior during enumeration of the terms depending upon this attribute).

Other FilteredTermsEnums could make use of this minimal score in their own way, to drive the most efficient behavior so that they do not waste time enumerating useless terms.
"
0,"improve how MTQs interact with the terms dict cacheSome small improvements:

  * Adds a TermsEnum.cacheCurrentTerm ""hint"" (codec can make this a no-op)

  * Removes the FTE.useTermsCache

  * Changes MTQ's TermCollector API to accept the TermsEnum so collectors can eg call .docFreq directly

  * Adds expert ctor to TermQuery allowing you to pass in the docFreq"
0,"New Analyzer for buffering tokensIn some cases, it would be handy to have Analyzer/Tokenizer/TokenFilters that could siphon off certain tokens and store them in a buffer to be used later in the processing pipeline.

For example, if you want to have two fields, one lowercased and one not, but all the other analysis is the same, then you could save off the tokens to be output for a different field.

Patch to follow, but I am still not sure about a couple of things, mostly how it plays with the new reuse API.

See http://www.gossamer-threads.com/lists/lucene/java-dev/54397?search_string=BufferingAnalyzer;#54397"
0,"Move SimpleWebdavServlet to jcr-server and make it abstractIn line with isse JCR-417, I suggest to partially move the SimpleWebdavServlet from the jcr-webapp project to the jcr-server project. By partially I mean, that the new (moved) servlet will be abstract and the getRepository() method will be abstract. The jcr-webapp project will still contain a SimpleWebdavServlet (for backwards compatibility maintaing the same name) which just extends the new servlet and implements the getRepository() method using the RepositoryAccess servlet.

This allows for the reuse of the jcr-server project including the abstract SimpleWebdavServlet in other environments. My intention is to include this project (along with the webdav) project in Sling.

Will provide a patch for this proposal

(This issue is separated out of JCR-1262 as suggested by Angela)
"
0,"Backport FilteredQuery/IndexSearcher changes to 3.x: Remove filter logic from IndexSearcher and delegate to FilteredQuerySpinoff from LUCENE-1536: We simplified the code in IndexSearcher to no longer do the filtering there, instead wrap all Query with FilteredQuery, if a non-null filter is given. The conjunction code would then only exist in FilteredQuery which makes it easier to maintain. Currently both implementations differ in 3.x, in trunk we used the more optimized IndexSearcher variant with addition of a simplified in-order conjunction code.

This issue will backport those changes (without random access bits)."
0,"All spatial contrib shape classes implement equals but not hashCodeviolates contract - at a min, need to implement return constant."
0,"Collapse Searcher/Searchable/IndexSearcher; remove contrib/remote; merge PMS into IndexSearcherWe've discussed cleaning up our *Searcher stack for some time... I
think we should try to do this before releasing 4.0.

So I'm attaching an initial patch which:

  * Removes Searcher, Searchable, absorbing all their methods into IndexSearcher

  * Removes contrib/remote

  * Removes MultiSearcher

  * Absorbs ParallelMultiSearcher into IndexSearcher (ie you can now
    pass useThreads=true, or a custom ES to the ctor)

The patch is rough -- I just ripped stuff out, did search/replace to
IndexSearcher, etc.  EG nothing is directly testing using threads with
IndexSearcher, but before committing I think we should add a
newSearcher to LuceneTestCase, which randomly chooses whether the
searcher uses threads, and cutover tests to use this instead of making
their own IndexSearcher.

I think MultiSearcher has a useful purpose, but as it is today it's
too low-level, eg it shouldn't be involved in rewriting queries: the
Query.combine method is scary.  Maybe in its place we make a higher
level class, with limited API, that's able to federate search across
multiple IndexSearchers?  It'd also be able to optionally use thread
per IndexSearcher.
"
0,"Add Google Analytics to Jackrabbit web siteI'd like to add Google Analytics to our web site to better track how the site is used and how much traffic we are generating.

Currently the best stats we have are at http://people.apache.org/~vgritsenko/stats/projects/jackrabbit.html, which is nice but not nearly as good as we could have."
0,Versioning operations should be done on the workspacecurrently all versioning operations modify the transient states of the items where the operation is executed although all operations are workspace operations.
0,"Highlighter should support all MultiTermQuery subclasses without castsIn order to support MultiTermQuery subclasses the Highlighter component applies instanceof checks for concrete classes from the lucene core. This prevents classes like RegexQuery in contrib from being supported. Introducing dependencies on other contribs is not feasible just for being supported by the highlighter.

While the instanceof checks and subsequent casts might hopefully go somehow away  in the future but for supporting more multterm queries I have a alternative approach using a fake IndexReader that uses a RewriteMethod to force the MTQ to pass the field name to the given reader without doing any real work. It is easier to explain once you see the patch - I will upload shortly.
"
0,Get rid of (another) hard coded path
0,"Core Tests should call Version based ctors instead of deprecated default ctorsLUCENE-2183 introduced new ctors for all CharTokenizer subclasses. Core - tests should use those ctors with Version.LUCENE_CURRENT instead of the the deprecated ctors. Yet, LUCENE-2240 introduces more Version ctors For WhitespaceAnalyzer and SimpleAnalyzer. Test should also use their Version ctors instead the default ones."
0,"FastVectorHighlighter: support for additional queriesI am using fastvectorhighlighter for some strange languages and it is working well! 

One thing i noticed immediately is that many query types are not highlighted (multitermquery, multiphrasequery, etc)
Here is one thing Michael M posted in the original ticket:

{quote}
I think a nice [eventual] model would be if we could simply re-run the
scorer on the single document (using InstantiatedIndex maybe, or
simply some sort of wrapper on the term vectors which are already a
mini-inverted-index for a single doc), but extend the scorer API to
tell us the exact term occurrences that participated in a match (which
I don't think is exposed today).
{quote}

Due to strange requirements I am using something similar to this (but specialized to our case).
I am doing strange things like forcing multitermqueries to rewrite into boolean queries so they will be highlighted,
and flattening multiphrasequeries into boolean or'ed phrasequeries.
I do not think these things would be 'fast', but i had a few ideas that might help:

* looking at contrib/highlighter, you can support FilteredQuery in flatten() by calling getQuery() right?
* maybe as a last resort, try Query.extractTerms() ?
"
0,"Tests fail with NoClassDefFoundError: org/w3c/dom/ranges/DocumentRangeThe nekohtml dependency in jackrabbit-text-extractors brings in a transitive xerces 2.4.0 dependency without the extra XML API classes required by Xerces. This causes test failures in jackrabbit-core and jackrabbit-jca because the Xerces dependency included in the test classpath overrides the default XML parser. Then, when the test cases try to parse XML documents, the missing XML API dependency causes a NoClassDefFoundError."
0,"References to old repository-1.x.dtdSome components still reference old version of the repository-1.x.dtd.
All components should be upgraded to repository-1.6.dtd"
0,"caching module should use HttpParams-style configurationThe constructor for CachingHttpClient currently accepts combinations of:
* HttpCache
* HttpClient
* integer for max object size in bytes

As I started looking at being able to configure this for behaving as a non-shared cache, I realized that we actually want to be replacing that last int with an HttpParams argument, and tracking all the various options in that style. I have a patch with this update which I will upload shortly.
"
0,"ServerQuery does not use RemoteAdapterFactory for creating ServerQueryResultThe ServerQuery sould use the Factory for creating ServerQueryResult.

Siehe the method ServerQuery.execute():

{code}
public RemoteQueryResult execute() throws RepositoryException, RemoteException {
        return new ServerQueryResult(query.execute(), getFactory());
    }
{code}

it should be:
{code}
    public RemoteQueryResult execute() throws RepositoryException, RemoteException {
        return getFactory().getRemoteQueryResult(this.query.execute());
    }
{code}"
0,"GData Server - Milestone 3 Patch, Bugfixes, DocumentationFor Milestone 3 added Features:

- Update Delete Concurrency
- Version control
- Second storage impl. based on Db4o. (Distributed Storage)
- moved all configuration in one single config file.
- removed dependencies in testcases.
- added schema validation for and all  xml files in the project (Configuration etc.)
- added JavaDoc
- much better Performance after reusing some resources
- added recovering component to lucene based storage to recover entries after a server crash or OOM Error (very simple)

- solved test case fail on hyperthread / multi core machines (@ hossman: give it a go)

@Yonik && Doug could you get that stuff in the svn please

regards simon

"
0,"make similarities/term/collectionstats take long (for > 2B docs)As noted by Yonik and Andrzej on SOLR-1632, this would be useful for distributed scoring.

we can also add a sugar method add() to both of these to make it easier to sum."
0,"move DocumentStoredFieldsVisitor to o.a.l.documentwhen examining the changes to the field/document API, i noticed this class was in o.a.l.index

I think it should be in o.a.l.document, its more intuitive packaging"
0,"Consolidate type safe wrappers for commons-collection classesVarious places define their own type safe wrappers for classes from commons-collections (i.e. FilterIterator, TransformIterator and the like). I would like to consolidate them into one single place. "
0,Sessions are not logged out in case of exceptionsSome test cases do not logout sessions if an exception occurs.
0,"Allow Directory.copy() to accept a collection of file names to be copiedPar example, I want to copy files pertaining to a certain commit, and not everything there is in a Directory."
0,"Improper deprecation of Locked classThe Locked class in the jcr-commons package has been deprecated with 1.4 and moved to the spi-commons.
However as this is a common class which does not depend on the spi, it should rather stay in jcr-commons.
The dependencies to spi can simply be removed again."
0,"dependencies for route planner implementationsThe implementations of HttpRoutePlanner that we have depend on the ConnectionManager, but use it only to look up the SchemeRegistry. Consider to depend only on the SchemeRegistry.

"
0,"Add reopen(IndexCommit) methods to IndexReaderAdd reopen(IndexCommit) methods to IndexReader to be able to reopen an index on any previously saved commit points with all advantages of LUCENE-1483.

Similar to open(IndexCommit) & company available in 2.4.0.
"
0,"[PATCH] Refactoring of SpanScorerRefactored some common code in next() and skipTo(). 
Removed dependency on score value for next() and skipTo(). 
Passes all current tests at just about the same speed 
as the current version. Added minimal javadoc. 
 
Iirc, there has been some discussion on the dependency of next() 
and skipTo() on the score value, but I don't remember the conclusion. 
In case that dependency should stay in, it can be adapted 
in the refactored code."
0,"Polish AnalyzerAndrzej Bialecki has written a Polish stemmer and provided stemming tables for it under Apache License.

You can read more about it here: http://www.getopt.org/stempel/

In reality, the stemmer is general code and we could use it for more languages too perhaps."
0,"Via NTLM proxy to SSL Apache/BasicAuth. - worked in may 22nd, but broken in beta1Hi there,

This morning I downloaded beta 1 and tried a small piece of code to connect to 
a SSLified apache server (using basic authentication) via a MS-Proxy 2.0 with 
NTLM enabled. The sourcecode of my crashme is based on the 1st attachment for 
HTTPCLIENT-153. It differs from the original in using basic authentication for the 
webserver instead of NTLM.

It failed with this error:

--
10-jun-2003 16:39:05 org.apache.commons.httpclient.HttpMethodBase 
processAuthenticationResponse
INFO: Already tried to authenticate to ""website#"" but still receiving 407.
Status: 407 : Proxy authentication required
--

Then I downloaded a fresh night build (commons-httpclient-20030605) which also 
failed :/

Then I went back to an old build from May (commons-httpclient-20030522) which 
worked like a charm!!!

Using MSIE I can succesfully connect to the apache server. I know it's not a 
problem with typos because I have MSIE ask me for all creds.

Seems somethings got broken along the way. If I can help, please ask!

Cheers."
0,"concurrent read-only access to a sessionEven though the JCR specification does not make a statement about Sessions shared across a number of threads I think it would be great for many applications if we could state that sharing a read-only session is supported by Jackrabbit.
On many occasions in the mailing lists we stated that there should not be an issue with sharing a read-only session, however I think it has never been thoroughly tested or even specified as a ""design goal"".

If we can come to an agreement that this is desirable I think it would be great to start including testcases to validate that behaviour and update the documentation respectively."
0,"speed up core testsOur core tests have gotten slower and slower, if you don't have a really fast computer its probably frustrating.

I think we should:
1. still have random parameters, but make the 'obscene' settings like SimpleText rarer... we can always make them happen more on NIGHTLY
2. tests that make a lot of documents can conditionalize on NIGHTLY so that they are still doing a reasonable test on ordinary runs e.g. numdocs = (NIGHTLY ? 10000 : 1000) * multiplier
3. refactor some of the slow huge classes with lots of tests like TestIW/TestIR, at least pull out really slow methods like TestIR.testDiskFull into its own class. this gives better parallelization.
"
0,"Tests failing when run with tests.iter > 1TestMultiLevelSkipList and TestsFieldReader are falling if run with -Dtests.iter > 1 - not all values are reset though
I will attach a patch in a second."
0,"Configurable actions upon authorizable creation and removali would like to add the possibility to configure custom actions that are executed upon user (and group) creation before 
the operation is persisted. this would allow applications to run custom code without the need of subclassing the
usermanager implementation. e.g.: creating additional mandatory properties, setting up permissions, calculating default 
group membership. the same applies for user/group removal."
0,"Contributed utility for determing content type from file type extension/*
 * ====================================================================
 *
 * The Apache Software License, Version 1.1
 *
 * Copyright (c) 2002-2003 The Apache Software Foundation.  All rights
 * reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 *
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 *
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in
 *    the documentation and/or other materials provided with the
 *    distribution.
 *
 * 3. The end-user documentation included with the redistribution, if
 *    any, must include the following acknowlegement:
 *       ""This product includes software developed by the
 *        Apache Software Foundation (http://www.apache.org/).""
 *    Alternately, this acknowlegement may appear in the software itself,
 *    if and wherever such third-party acknowlegements normally appear.
 *
 * 4. The names ""The Jakarta Project"", ""Commons"", and ""Apache Software
 *    Foundation"" must not be used to endorse or promote products derived
 *    from this software without prior written permission. For written
 *    permission, please contact apache@apache.org.
 *
 * 5. Products derived from this software may not be called ""Apache""
 *    nor may ""Apache"" appear in their names without prior written
 *    permission of the Apache Group.
 *
 * THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESSED OR IMPLIED
 * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
 * DISCLAIMED.  IN NO EVENT SHALL THE APACHE SOFTWARE FOUNDATION OR
 * ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
 * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
 * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF
 * USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
 * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
 * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT
 * OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 * ====================================================================
 *
 * This software consists of voluntary contributions made by many
 * individuals on behalf of the Apache Software Foundation.  For more
 * information on the Apache Software Foundation, please see
 * <http://www.apache.org/>.
 *
 * [Additional notices, if required by prior licensing conditions]
 *
 */

package org.apache.commons.httpclient.contrib.utils;

import java.io.File;
import java.io.IOException;

/**
 * This class provides mappings from file name extensions to content types.
 *
 * @author <a href=""mailto:emdevlin@charter.net"">Eric Devlin</a>
 * 
 * DISCLAIMER: HttpClient developers DO NOT actively support this component.
 * The component is provided as a reference material, which may be inappropriate
 * to be used without additional customization.
 */

public class ContentType {

	/** Mime Type mappings 'liberated' from Tomcat4.1.18/conf/web.xml*/
	public static final String[][] MIME_TYPE_MAPPINGS =	
		{	{ ""abs"",		""audio/x-mpeg"" },
			{ ""ai"",			""application/postscript"" },
			{ ""aif"",		""audio/x-aiff"" },
			{ ""aifc"",		""audio/x-aiff"" },
			{ ""aiff"",		""audio/x-aiff"" },
			{ ""aim"",		""application/x-aim"" },
			{ ""art"",		""image/x-jg"" },
			{ ""asf"",		""video/x-ms-asf"" },
			{ ""asx"",		""video/x-ms-asf"" },
			{ ""au"",			""audio/basic"" },
			{ ""avi"",		""video/x-msvideo"" },
			{ ""avx"",		""video/x-rad-screenplay"" },
			{ ""bcpio"",		""application/x-bcpio"" },
			{ ""bin"",		""application/octet-stream"" },
			{ ""bmp"",		""image/bmp"" },
			{ ""body"",		""text/html"" },
			{ ""cdf"",		""application/x-cdf"" },
			{ ""cer"",		""application/x-x509-ca-cert"" },
			{ ""class"",		""application/java"" },
			{ ""cpio"",		""application/x-cpio"" },
			{ ""csh"",		""application/x-csh"" },
			{ ""css"",		""text/css"" },
			{ ""dib"",		""image/bmp"" },
			{ ""doc"",		""application/msword"" },
			{ ""dtd"",		""text/plain"" },
			{ ""dv"",			""video/x-dv"" },
			{ ""dvi"",		""application/x-dvi"" },
			{ ""eps"",		""application/postscript"" },
			{ ""etx"",		""text/x-setext"" },
			{ ""exe"",		""application/octet-stream"" },
			{ ""gif"",		""image/gif"" },
			{ ""gtar"",		""application/x-gtar"" },
			{ ""gz"",			""application/x-gzip"" },
			{ ""hdf"",		""application/x-hdf"" },
			{ ""hqx"",		""application/mac-binhex40"" },
			{ ""htc"",		""text/x-component"" },
			{ ""htm"",		""text/html"" },
			{ ""html"",		""text/html"" },
			{ ""hqx"",		""application/mac-binhex40"" },
			{ ""ief"",		""image/ief"" },
			{ ""jad"",		""text/vnd.sun.j2me.app-
descriptor"" },
			{ ""jar"",		""application/java-archive"" },
			{ ""java"",		""text/plain"" },
			{ ""jnlp"",		""application/x-java-jnlp-
file"" },
			{ ""jpe"",		""image/jpeg"" },
			{ ""jpeg"",		""image/jpeg"" },
			{ ""jpg"",		""image/jpeg"" },
			{ ""js"",			""text/javascript"" },
			{ ""jsf"",		""text/plain"" },
			{ ""jspf"",		""text/plain"" },
			{ ""kar"",		""audio/x-midi"" },
			{ ""latex"",		""application/x-latex"" },
			{ ""m3u"",		""audio/x-mpegurl"" },
			{ ""mac"",		""image/x-macpaint"" },
			{ ""man"",		""application/x-troff-man"" },
			{ ""me"",			""application/x-troff-me"" },
			{ ""mid"",		""audio/x-midi"" },
			{ ""midi"",		""audio/x-midi"" },
			{ ""mif"",		""application/x-mif"" },
			{ ""mov"",		""video/quicktime"" },
			{ ""movie"",		""video/x-sgi-movie"" },
			{ ""mp1"",		""audio/x-mpeg"" },
			{ ""mp2"",		""audio/x-mpeg"" },
			{ ""mp3"",		""audio/x-mpeg"" },
			{ ""mpa"",		""audio/x-mpeg"" },
			{ ""mpe"",		""video/mpeg"" },
			{ ""mpeg"",		""video/mpeg"" },
			{ ""mpega"",		""audio/x-mpeg"" },
			{ ""mpg"",		""video/mpeg"" },
			{ ""mpv2"",		""video/mpeg2"" },
			{ ""ms"",			""application/x-wais-source"" },
			{ ""nc"",			""application/x-netcdf"" },
			{ ""oda"",		""application/oda"" },
			{ ""pbm"",		""image/x-portable-bitmap"" },
			{ ""pct"",		""image/pict"" },
			{ ""pdf"",		""application/pdf"" },
			{ ""pgm"",		""image/x-portable-graymap"" },
			{ ""pic"",		""image/pict"" },
			{ ""pict"",		""image/pict"" },
			{ ""pls"",		""audio/x-scpls"" },
			{ ""png"",		""image/png"" },
			{ ""pnm"",		""image/x-portable-anymap"" },
			{ ""pnt"",		""image/x-macpaint"" },
			{ ""ppm"",		""image/x-portable-pixmap"" },
			{ ""ps"",			""application/postscript"" },
			{ ""psd"",		""image/x-photoshop"" },
			{ ""qt"",			""video/quicktime"" },
			{ ""qti"",		""image/x-quicktime"" },
			{ ""qtif"",		""image/x-quicktime"" },
			{ ""ras"",		""image/x-cmu-raster"" },
			{ ""rgb"",		""image/x-rgb"" },
			{ ""rm"",			""application/vnd.rn-
realmedia"" },
			{ ""roff"",		""application/x-troff"" },
			{ ""rtf"",		""application/rtf"" },
			{ ""rtx"",		""text/richtext"" },
			{ ""sh"",			""application/x-sh"" },
			{ ""shar"",		""application/x-shar"" },
			{ ""smf"",		""audio/x-midi"" },
			{ ""snd"",		""audio/basic"" },
			{ ""src"",		""application/x-wais-source"" },
			{ ""sv4cpio"",	""application/x-sv4cpio"" },
			{ ""sv4crc"",		""application/x-sv4crc"" },
			{ ""swf"",		""application/x-shockwave-
flash"" },
			{ ""t"",			""application/x-troff"" },
			{ ""tar"",		""application/x-tar"" },
			{ ""tcl"",		""application/x-tcl"" },
			{ ""tex"",		""application/x-tex"" },
			{ ""texi"",		""application/x-texinfo"" },
			{ ""texinfo"",	""application/x-texinfo"" },
			{ ""tif"",		""image/tiff"" },
			{ ""tiff"",		""image/tiff"" },
			{ ""tr"",			""application/x-troff"" },
			{ ""tsv"",		""text/tab-separated-values"" },
			{ ""txt"",		""text/plain"" },
			{ ""ulw"",		""audio/basic"" },
			{ ""ustar"",		""application/x-ustar"" },
			{ ""xbm"",		""image/x-xbitmap"" },
			{ ""xml"",		""text/xml"" },
			{ ""xpm"",		""image/x-xpixmap"" },
			{ ""xsl"",		""text/xml"" },
			{ ""xwd"",		""image/x-xwindowdump"" },
			{ ""wav"",		""audio/x-wav"" },
			{ ""svg"",		""image/svg+xml"" },
			{ ""svgz"",		""image/svg+xml"" },
			{ ""wbmp"",		""image/vnd.wap.wbmp"" },
			{ ""wml"",		""text/vnd.wap.wml"" },
			{ ""wmlc"",		""application/vnd.wap.wmlc"" },
			{ ""wmls"",		""text/vnd.wap.wmlscript"" },
			{ ""wmlscriptc"",	""application/vnd.wap.wmlscriptc"" },
			{ ""wrl"",		""x-world/x-vrml"" },
			{ ""Z"",			""application/x-compress"" },
			{ ""z"",			""application/x-compress"" },
			{ ""zip"",		""application/zip"" } };

    /**
     * Get the content type based on the extension of the file name<br>
     *
     * @param fileName for which the content type is to be determined.
     *
     * @return the content type for the file or null if no mapping was
     * possible.
     */
	public static String get( String fileName  ) {
		String contentType = null;

		if ( fileName != null ) {
			int extensionIndex = fileName.lastIndexOf( '.' );
			if ( extensionIndex != -1 ) {
				if ( extensionIndex + 1 < fileName.length() ) {
					String extension = fileName.substring( 
extensionIndex + 1 );
					for( int i = 0; i < 
MIME_TYPE_MAPPINGS.length; i++ ) {
						if ( extension.equals( 
MIME_TYPE_MAPPINGS[i][0] ) ) {
							contentType = 
MIME_TYPE_MAPPINGS[i][1];
							break;
						}
					}
				}
			}
		}

		return contentType;
	}

    /**
     * Get the content type based on the extension of the file name<br>
     *
     * @param file for which the content type is to be determined.
     *
     * @return the content type for the file or null if no mapping was
     * possible.
     *
     * @throws IOException if the construction of the canonical path for 
	 * the file fails.
     */
	public static String get( File file ) 
		throws IOException
	{
		String contentType = null;

		if ( file != null ) {

			contentType = get( file.getCanonicalPath() );
		}

		return contentType;
	}
}"
0,"Cleanup highlighter test classcleanup highlighter test class - did some of this in another issue, but there is a bit more to do"
0,"[PATCH] fixes for gcj target.I've modified the Makefile so that it compiles with GCJ-4.0.

This involved fixing the CORE_OBJ macro to match the generated jar file as well
as excluding FieldCacheImpl from being used from its .java source (GCJ has
problems with anonymous inner classes, I guess).

Also, I changed the behaviour of FieldInfos.fieldInfo(int). It depended on
catching IndexOutOfBoundsException exception. I've modified it to test the
bounds first, returning -1 in that case. This helps with gcj since we build with
-fno-bounds-check.

I compiled with;

GCJ=gcj-4.0 GCJH=gcjh-4.0 GPLUSPLUS=g++-4.0 ant clean gcj

patch to follow."
0,"Provide possibility to import protected items using Session import functionalitySessionImporter and WorkspaceImporter currently skip all protected items encountered during import except for some special cases
(see JCR-2172 and WorkspaceImporter#postProcessNode).
The specification only mandates that protected content is treated in a consistent manner, but allows the implementation to either import or ignore it.

Find attached a patch containing some initials steps to allow to extend the default import behavior:
Instead of skipping protected items (and in case of nodes the complete tree below it), they should be passed to a separate handler,
that may or may not be able to deal with them and needs to assert, that they are in a valid format.

The patch includes:

- Abstract classes for that protected item import
- Default implementations that never import protected nodes (same behavior as we have today)
- An example implementation for the AC-content (just to see if it works for simple cases) + some trivial tests.
- Changes to SessionImporter to demonstrate how import of protected items would be enabled.

The patch doesn't include yet:

- Changes to WorkspaceImporter (would +- be according to SessionImporter)
- Changes to WorkspaceImpl/SessionImpl as well as configuration that would allow to modify the default behavior.
- Examples for import of protected properties.
- Examples for workspace import.

The patch has the following limitations or TODOs:

- Proper handling of protected references properties or non-protected ref properties with the tree defined by a protected node.
- Test / Careful review if the various ImportUUIDBehaviors are/can properly be covered, specially in case of ""replace-existing"".

The patch in addition addresses:

- An inconsistency in the SessionImporter:
  > Attempt to import protected content below an existing protected node => skipped
  > Attempt to import protected content that doesn't yet exist => first node is imported, ConstraintViolationException for child-nodes.
  > This behavior is also reflected in the Node-stack... where in the first case 'null' is pushed, in the second case the first protected node.
     (see also JCR-2172 for details).
"
0,"WebApp: Ease first access for new users looking for a WebDAV serversuggestion posted by mike oliver in the user list:

> I know that JackRabbit isn't the same as Jakarta Slide and not expecting it to be, but one thing we did right on 
> that project was create a runnable war file that doesn't require any learning curve to get started.  Install the war file, 
> create the network place and login as the root:root user and start creating content folders and documents. 
> If JackRabbit did that, then I think more people would try it and use it and then spend the time to learn how to make 
> it all it can be."
0,Avoid item state reads during Session.logout()Local item states are discarded during Session.logout(). Currently the CachingHierarchyManager is still registered as a item state listener at that time and will cause numerous ItemStateManager.hasItemState() calls. This is unnecessary and just adds overhead to the logout call. In addition it will also contribute to a potential lock contention on the SharedItemStateManager.
0,"equals and hashCode implementation in org.apache.lucene.search.* packageI would like to talk about the implementation of equals and hashCode method  in org.apache.lucene.search.* package. 

Example One:

org.apache.lucene.search.spans.SpanTermQuery (Super Class)
	<- org.apache.lucene.search.payloads.BoostingTermQuery (Sub Class)

Observation:

* BoostingTermQuery defines equals but inherits hashCode from SpanTermQuery. Definition of equals is a code clone of SpanTermQuery with a change in class name. 

Intention:

I believe the intention of equals redefinition in BoostingTermQuery is not to make the objects of SpanTermQuery and BoostingTermQuery comparable. ie. spanTermQuery.equals(boostingTermQuery) == false && boostingTermQuery.equals(spanTermQuery) == false.


Problem:

With current implementation, the intention might not be respected as a result of symmetric property violation of equals contract i.e.
spanTermQuery.equals(boostingTermQuery) == true (can be) && boostingTermQuery.equals(spanTermQuery) == false. (always)
(Note: Provided their state variables are equal)

Solution:

Change implementation of equals in SpanTermQuery from:

{code:title=SpanTermQuery.java|borderStyle=solid}
  public boolean equals(Object o) {
    if (!(o instanceof SpanTermQuery))
      return false;
    SpanTermQuery other = (SpanTermQuery)o;
    return (this.getBoost() == other.getBoost())
      && this.term.equals(other.term);
  }
{code}

To:
{code:title=SpanTermQuery.java|borderStyle=solid}
  public boolean equals(Object o) {
  	if(o == this) return true;
  	if(o == null || o.getClass() != this.getClass()) return false;
//    if (!(o instanceof SpanTermQuery))
//      return false;
    SpanTermQuery other = (SpanTermQuery)o;
    return (this.getBoost() == other.getBoost())
      && this.term.equals(other.term);
  }
{code}

Advantage:

* BoostingTermQuery.equals and BoostingTermQuery.hashCode is not needed while still preserving the same intention as before.
 
* Any further subclassing that does not add new state variables in the extended classes of SpanTermQuery, does not have to redefine equals and hashCode. 

* Even if a new state variable is added in a subclass, the symmetric property of equals contract will still be respected irrespective of implementation (i.e. instanceof / getClass) of equals and hashCode in the subclasses.


Example Two:


org.apache.lucene.search.CachingWrapperFilter (Super Class)
	<- org.apache.lucene.search.CachingWrapperFilterHelper (Sub Class)

Observation:
Same as Example One.

Problem:
Same as Example one.

Solution:
Change equals in CachingWrapperFilter from:
{code:title=CachingWrapperFilter.java|borderStyle=solid}
  public boolean equals(Object o) {
    if (!(o instanceof CachingWrapperFilter)) return false;
    return this.filter.equals(((CachingWrapperFilter)o).filter);
  }
{code}

To:
{code:title=CachingWrapperFilter.java|borderStyle=solid}
  public boolean equals(Object o) {
//    if (!(o instanceof CachingWrapperFilter)) return false;
    if(o == this) return true;
    if(o == null || o.getClass() != this.getClass()) return false;
    return this.filter.equals(((CachingWrapperFilter)o).filter);
  }
{code}

Advantage:
Same as Example One. Here, CachingWrapperFilterHelper.equals and CachingWrapperFilterHelper.hashCode is not needed.


Example Three:

org.apache.lucene.search.MultiTermQuery (Abstract Parent)
	<- org.apache.lucene.search.FuzzyQuery (Concrete Sub)
	<- org.apache.lucene.search.WildcardQuery (Concrete Sub)

Observation (Not a problem):

* WildcardQuery defines equals but inherits hashCode from MultiTermQuery.
Definition of equals contains just super.equals invocation. 

* FuzzyQuery has few state variables added that are referenced in its equals and hashCode.
Intention:

I believe the intention here is not to make objects of FuzzyQuery and WildcardQuery comparable. ie. fuzzyQuery.equals(wildCardQuery) == false && wildCardQuery.equals(fuzzyQuery) == false.

Proposed Implementation:
How about changing the implementation of equals in MultiTermQuery from:

{code:title=MultiTermQuery.java|borderStyle=solid}
    public boolean equals(Object o) {
      if (this == o) return true;
      if (!(o instanceof MultiTermQuery)) return false;

      final MultiTermQuery multiTermQuery = (MultiTermQuery) o;

      if (!term.equals(multiTermQuery.term)) return false;

      return getBoost() == multiTermQuery.getBoost();
    }
{code}

To:
{code:title=MultiTermQuery.java|borderStyle=solid}
    public boolean equals(Object o) {
      if (this == o) return true;
//      if (!(o instanceof MultiTermQuery)) return false;
      if(o == null || o.getClass() != this.getClass()) return false;

      final MultiTermQuery multiTermQuery = (MultiTermQuery) o;

      if (!term.equals(multiTermQuery.term)) return false;

      return getBoost() == multiTermQuery.getBoost();
    }
{code}

Advantage:

Same as above. Here, WildcardQuery.equals is not needed as it does not define any new state. (FuzzyQuery.equals is still needed because FuzzyQuery defines new state.) 
"
0,"Various places do map lookups in loop instead of using entrySet iteratorVarious places loop over a keyset iterator and do a map look up each time thru the loop, I plan to convert these places to use an entryset iterator to avoid this."
0,"Split DocMaker into ContentSource and DocMakerThis issue proposes some refactoring to the benchmark package. Today, DocMaker has two roles: collecting documents from a collection and preparing a Document object. These two should actually be split up to ContentSource and DocMaker, which will use a ContentSource instance.

ContentSource will implement all the methods of DocMaker, like getNextDocData, raw size in bytes tracking etc. This can actually fit well w/ 1591, by having a basic ContentSource that offers input stream services, and wraps a file (for example) with a bzip or gzip streams etc.

DocMaker will implement the makeDocument methods, reusing DocState etc.

The idea is that collecting the Enwiki documents, for example, should be the same whether I create documents using DocState, add payloads or index additional metadata. Same goes for Trec and Reuters collections, as well as LineDocMaker.
In fact, if one inspects EnwikiDocMaker and LineDocMaker closely, they are 99% the same and 99% different. Most of their differences lie in the way they read the data, while most of the similarity lies in the way they create documents (using DocState).
That led to a somehwat bizzare extension of LineDocMaker by EnwikiDocMaker (just the reuse of DocState). Also, other DocMakers do not use that DocState today, something they could have gotten for free with this refactoring proposed.

So by having a EnwikiContentSource, ReutersContentSource and others (TREC, Line, Simple), I can write several DocMakers, such as DocStateMaker, ConfigurableDocMaker (one which accpets all kinds of config options) and custom DocMakers (payload, facets, sorting), passing to them a ContentSource instance and reuse the same DocMaking algorithm with many content sources, as well as the same ContentSource algorithm with many DocMaker implementations.

This will also give us the opportunity to perf test content sources alone (i.e., compare bzip, gzip and regular input streams), w/o the overhead of creating a Document object.

I've already done so in my code environment (I extend the benchmark package for my application's purposes) and I like the flexibility I have. I think this can be a nice contribution to the benchmark package, which can result in some code cleanup as well."
0,Remove SegmentReader.document synchronizationThis is probably the last synchronization issue in Lucene.  It is the document method in SegmentReader.  It is avoidable by using a threadlocal for FieldsReader.  
0,"NearSpansOrdered does not lazy load payloads as the PayloadSpans javadoc impliesBest would be to lazy load, but I don't see how with the current algorithm. Short that, we should add an option to ignore payloads - otherwise, if you are doing non payload searching, but the payloads are present, they will be needlessly loaded.

Already added this to LUCENE-1748, but spinning from that issue to this - patch to follow when LUCENE-1748 is committed."
0,"Resolve JUnit assert deprecationsMany tests use assertEquals methods which have been deprecated.  The culprits are assertEquals(float, float), assertEquals(double, double) and assertEquals(Object[], Object[]).  Although not a big issue, they annoy me every time I see them so I'm going to fix them."
0,"Revise NIOFSDirectory and its usage due to NIO limitations on Thread.interruptI created this issue as a spin off from http://mail-archives.apache.org/mod_mbox/lucene-java-dev/201001.mbox/%3Cf18c9dde1001280051w4af2bc50u1cfd55f85e50914f@mail.gmail.com%3E

We should decide what to do with NIOFSDirectory, if we want to keep it as the default on none-windows platforms and how we want to document this.

"
0,"[CONTRIB] SSL authenticating protocol socket factoryHere's the long promised SSL client/server authenticating socket factory. This
socket factory can be used to enforce client/server authentication during the
SSL context negotiation. Let me know what you think. Please, please someone
proof-read the accompanying javadocs and let me know if the text is comprehensible 

I have also tweaked EasySSLProtocolSocketFactory a little

The patch is against HTTPCLIENT_2_0_BRANCH

Oleg"
0,"wildcardquery rewrite improvementswildcardquery has logic to rewrite to termquery if there is no wildcard character, but
* it needs to pass along the boost if it does this
* if the user asked for a 'constant score' rewriteMethod, it should rewrite to a constant score query for consistency.

additionally, if the query is really a prefixquery, it would be nice to rewrite to prefix query.
both will enumerate the same number of terms, but prefixquery has a simpler comparison function."
0,"Add <a name=""""> anchors to documentation sectionsIn all docs, sections are missing <a name> anchors. I see that the xdocs stylesheet in the repository is supposed to generate them, yet the site is missing them at this moment.

See https://svn.apache.org/repos/asf/jakarta/site/xdocs/stylesheets/site.xsl 
template match=""section"""
0,It's not possible to register event listeners that filters on mixin supertypesThe current implementation of blocks() in EventFilter does not check if the given EventState has a mixin that is derived from one of the given node types.
0,Use bundle persistence in default configurationThe default repository configuration files in jackrabbit-core and -webapp still use the old simple database persistence. They should be updated to use bundle persistence in the 1.4 release.
0,"Contrib/Module-uptodate assume name matches path and jarWith adding a new 'queries' module, I am trying to change the project name of contrib/queries to queries-contrib.  However currently the contrib-uptodate assumes that the name property is used in the path and in the jar name.

By using the name in the path, I must set the value to 'queries' (since the path is contrib/queries).  However because the project name is now queries-contrib, the actual jar file will be lucene-queries-contrib-${version}.jar, not lucene-queries-${version}.jar, as is expected.

Consequently I think we need to separate the path name from the jar name properties.  For simplicity I think adding a new jar-name property will suffice, which can be optional and if omitted, is filled in with the name property."
0,"Define clear semantics for Directory.fileLengthOn this thread: http://mail-archives.apache.org/mod_mbox/lucene-java-dev/201003.mbox/%3C126142c1003121525v24499625u1589bbef4c0792e7@mail.gmail.com%3E it was mentioned that Directory's fileLength behavior is not consistent between Directory implementations if the given file name does not exist. FSDirectory returns a 0 length while RAMDirectory throws FNFE.

The problem is that the semantics of fileLength() are not defined. As proposed in the thread, we'll define the following semantics:

* Returns the length of the file denoted by <code>name</code> if the file exists. The return value may be anything between 0 and Long.MAX_VALUE.
* Throws FileNotFoundException if the file does not exist. Note that you can call dir.fileExists(name) if you are not sure whether the file exists or not.

For backwards we'll create a new method w/ clear semantics. Something like:

{code}
/**
 * @deprecated the method will become abstract when #fileLength(name) has been removed.
 */
public long getFileLength(String name) throws IOException {
  long len = fileLength(name);
  if (len == 0 && !fileExists(name)) {
    throw new FileNotFoundException(name);
  }
  return len;
}
{code}

The first line just calls the current impl. If it throws exception for a non-existing file, we're ok. The second line verifies whether a 0 length is for an existing file or not and throws an exception appropriately."
0,"DisjunctionScorerThis disjunction scorer can match a minimum nr. of docs, 
it provides skipTo() and it uses skipTo() on the subscorers. 
The score() method is abstract in DisjunctionScorer and implemented 
in DisjunctionSumScorer as an example."
0,"Add configurable hook for password validationit's a common use case that applications would like to enforce additional logic associated with 
changing a user password. currently this can only be achieved by using a derived user implementation.
by extending the functionality added with JCR-3118 it was fairly trivial to provide a hook for those
custom password validation checks, writing password expiration date etc.etc. 
"
0,Use configured credentials in RepositoryFactoryImplTest The test currently uses hard coded credentials. It should rather use configured credentials like all other tests do.
0,"HttpMethodBase does not compile on JDK prior to 1.3reason is the use of URL.getPath() and URL.getQuery() within method
processRedirectResponse.

should use URIUtil.getPath and URIUtil.getQuery instead.

so, HttpMethodBase around line 952:

//update the current location with the redirect location
setPath(URIUtil.getPath(redirectUrl.toString()));
setQueryString(URIUtil.getQuery(redirectUrl.toString()));

thanks,

marius"
0,"Prepare CharArraySet for Unicode 4.0CharArraySet does lowercaseing if created with the correspondent flag. This causes that  String / char[] with uncode 4 chars which are in the set can not be retrieved in ""ignorecase"" mode.
"
0,"Let NameException extend RepositoryExceptionSince the NameExceptions (IllegalNameException, UnknownPrefixException, etc.) are typically thrown when parsing or formatting JCR names at the JCR API level, it would make sense for the NameException class to extend RepositoryException instead of the internal BaseException. This idea is supported by the fact that the majority of cases where NameExceptions are encountered simply rethrow the exceptions wrapped inside RepositoryException instances. Making NameException extend RepositoryException would reduce the amount of try-catch blocks and wrapped exceptions."
0,"TermVectorAccessor, transparent vector space access This class visits TermVectorMapper and populates it with information transparent by either passing it down to the default terms cache (documents indexed with Field.TermVector) or by resolving the inverted index."
0,"contrib/xml-query-parser, BoostingTermQuery supportI'm not 100% on this patch. 

BooleanTermQuery is a part of the spans family, but I generally use that class as a replacement for TermQuery.  Thus in the DTD I have stated that it can be a part of the root queries as well as a part of a span. 

However, SpanFooQueries xml elements are named <SpanFoo/> rather than <SpanFooQuery/>, I have however chosen to call it <BoostingTermQuery/>. It would be possible to set it up so it would be parsed as <SpanBoostingTerm/> when inside of a <SpanSomething>, but I just find that confusing.
"
0,"Modified QueryImpl to enable external query builders to read and write JCR expressions in the orderBy ClauseThe QueryImpl does not create the JCR expression on the fly. The OrderByExpression does the job. If an external querybuilder class needs to dynamically collect properties against which order by is required, QueryImpl does not support updating the JCR Expression. It can only return the built expression since arrayList is used for collecting the properties. The change enables one to add JCRExpression to the QueryImpl object. A test has been added.

Changed files:
Path
src/main/java/org/apache/jackrabbit/ocm/query/impl/QueryImpl.java
src/test/java/org/apache/jackrabbit/ocm/manager/query/DigesterSimpleQueryTest.java
"
0,"DatabaseJournal refactoring for subclassing capabilityIn the 1.3 upgrade to JackRabbit, the DatabasePersistenceManager class was refactored to allow easy subclassing.  On my project, the subclassing is required because the DBAs have a specific naming convention for database columns, and the default JackRabbit columns don't fit within the naming convention.

At this point, we're cutting over to a clustered setup in preparation for production.  In my design, I would like to use the database for journaling.  But once again, the DBAs will want to change the column names to their own naming convention.  The existing DatabaseJournal class is not set up for the same type of subclassing that the PersistenceManager (or even the FileSystem) hierarchies.  I'd like the DatabaseJournal class to be updated accordingly.

In specific, here are the changes I'm looking for:

* Extract protected instance variables for selectRevisionsStmtSql, updateGlobalStmtSql, selectGlobalStmtSql, and insertRevisionStmtSql.
* Extract method protected void buildSQLStatements() which sets up the above sqls, and allows subclasses to override.
* Update the existing prepareStatements method to use the above instance variables.
* Update the init method to call the buildSQLStatements method before the call to prepareStatements."
0,"Override method MultipartEntity.addPart so that applications may use FormBodyPartFormBodyPart is similar to Part in HttpClient 3.x in that it couples the form name with the value.  Some applications may find this useful, but cannot really utilize these objects since there is only MultipartEntity.addPart(String name,ContentBody) and FormBodyPart does not have a getContent method:

  entity.addPart(part.getName(), part.getContent()); // Almost but there is no getContent method

How about overriding addPart to take a FormBodyPart object:

  entity.addPart(part);"
0,"Change Term to use bytesin LUCENE-2426, the sort order was changed to codepoint order.

unfortunately, Term is still using string internally, and more importantly its compareTo() uses the wrong order [utf-16].
So MultiTermQuery, etc (especially its priority queues) are currently wrong.

By changing Term to use bytes, we can also support terms encoded as bytes such as numerics, instead of using
strange string encodings.
"
0,"FST.BYTE2 should save as fixed 2 byte not as vIntWe currently write BYTE1 as a single byte, but BYTE2/4 as vInt, but I think that's confusing.  Also, for the FST for the new Kuromoji analyzer (LUCENE-3305), writing as 2 bytes instead shrank the FST and ran faster, presumably because more values were >= 16384 than were < 128.

Separately the whole INPUT_TYPE is very confusing... really all it's doing is ""declaring"" the allowed range of the characters of the input alphabet, and then the only thing that uses that is the write/readLabel methods (well and some confusing sugar methods in Builder!).  Not sure how to fix that yet...

It's a simple change but it changes the FST binary format so any users w/ FSTs out there will have to rebuild (FST is marked experimental...).
"
0,Privilege content representation should be of property type NAMEthe content representation of jcr privileges should reflect that fact that privilege names changed from simple string to JCR name.
0,"httpclient build requires jdk 1.4 or jce in classpathCurrently when a 'ant dist'
is performed httpclient is looking for javax.crypt.* which is in jce.jar

The build.xml and build.properties.sample need to be patched
so they allow the jce.jar file to be specified
just like the jsse.jar is specified.

will attach two patch files made from todays cvs"
0,"Upgrade to Maven 2If you are interested in migrating to maven2 (or adding optional maven 2 build scripts) this is a full maven 2 pom.xml for the main jackrabbit jar.

All the xpath/javacc stuff, previously done in maven.xml, was pretty painfull to reproduce in maven2... the attached pom exactly reproduces the m1 build by using the maven2 javacc plugin + a couple of antrun executions.
Test configuration is not yet complete, I think it will be a lot better to reproduce the previous behaviour (init tests run first) without any customization (maybe using a single junit test suite with setUp tasks). Also custom packaging goals added to maven.xml (that can be esily done in m2 by using the assembly plugin) are not yet reproduced too.

If there is interest, I can also provide poms for the contribution projects (that will be easy, the only complex pom is the main one).
"
0,"drop java 5 ""support""its been discussed here and there, but I think we need to drop java 5 ""support"", for these reasons:
* its totally untested by any continual build process. Testing java5 only when there is a release candidate ready is not enough. If we are to claim ""support"" then we need a hudson actually running the tests with java 5.
* its now unmaintained, so bugs have to either be hacked around, tests disabled, warnings placed, but some things simply cannot be fixed... we cannot actually ""support"" something that is no longer maintained: we do find JRE bugs (http://wiki.apache.org/lucene-java/SunJavaBugs) and its important that bugs actually get fixed: cannot do everything with hacks.
* because of its limitations, we do things like allow 20% slower grouping speed. I find it hard to believe we are sacrificing performance for this.

So, in summary: because we don't test it at all, because its buggy and unmaintained, and because we are sacrificing performance, I think we need to cutover the build system for the next release to require java 6.
"
0,"Prevent logins during repository shutdownRelated to the last comment in JCR-445, should we prevent new sessions from being created during repository shutdown? It is an odd chance to run into a problem like that, but it seems like the issue could be easily solved by making getWorkspaceInfo() synchronized and adding sanityCheck() calls to the createSession() methods."
0,"remove IndexSearcher.docFreq/maxDocAs pointed out by Mark on SOLR-1632, these are no longer used by the scoring system.

We've added new stats to Lucene, so having these methods on indexsearcher makes no sense.
Its confusing to people upgrading if they subclassed IndexSearcher to provide distributed stats,
only to find these are not used (LUCENE-3555 has a correct API for them to do this).

So I think we should remove these in 4.0."
0,"Single-pass grouping collector based on doc blocksLUCENE-3112 enables adding/updating a contiguous block of documents to
the index, guaranteed (yet, experimental!) to retain adjacent docID
assignment through the full life of the index as long the app doesn't
delete individual docs from the block.

When an app does this, it can enable neat features like LUCENE-2454
(nested documents), post-group facet counting (LUCENE-3097).

It also makes single-pass grouping possible, when you group by
the ""identifier"" field shared by the doc block, since we know we will
see a given group only once with all of its docs within one block.

This should be faster than the fully general two-pass collectors we
already have.

I'm working on a patch but not quite there yet...
"
0,"Use ConcurrentHashMap in RAMDirectoryRAMDirectory synchronizes on its instance in many places to protect access to map of RAMFiles, in addition to updating the sizeInBytes member. In many places the sync is done for 'read' purposes, while only in few places we need 'write' access. This looks like a perfect use case for ConcurrentHashMap

Also, syncing around sizeInBytes is unnecessary IMO, since it's an AtomicLong ...

I'll post a patch shortly."
0,"Need setURI() methods in HttpMethod interfaceI'd like to have the methods setURI( URI ) and setURI( String ) methods. Also a 
method like getRequestURI() because the uri I get now with the method getURI() 
changes if I execute a request which will be automatically forwarded.

The methods setURI can throw an exception if it has already been executed."
0,Add supported for Wikipedia English as a corpus in the benchmarker stuffAdd support for using Wikipedia for benchmarking.
0,"Make it easier to run Test2BTermsCurrently, Test2BTerms has an @Ignore annotation which means that the only way to run it as a test is to edit the file.

There are a couple of options to fix this:
# Add a main() so it can be invoked via the command line outside of the test framework
# Add some new annotations that mark it as slow or weekly or something like that and have the test target ignore @slow (or whatever) by default, but can also turn it on."
0,"[PATCH] Use filter bits for next() and skipTo() in FilteredQueryThis improves performance of FilteredQuery by not calling score() 
on documents that do not pass the filter. 
This passes the current tests for FilteredQuery, but these tests 
have not been adapted/extended."
0,"Contribution: Efficient Sorting of DateField/DateTools Encoded Timestamp Long ValuesHello Tim,

As promised, the sort functionality for ""long"" values is included in the
attached files.

patchTestSort.txt contains the diff info. for my modifications to the
TestSort.java class

org.apache.lucene.search.ZIP contains the three new class files for
efficient sorting of ""long"" field values and of encoded timestamp
field values as ""long"" values.

Let me know if you have any questions.

Regards,
Rus"
0,"Make TrieRange completely independent from Document/Field with TokenStream of prefix encoded valuesTrieRange has currently the following problem:
- To add a field, that uses a trie encoding, you can manually add each term to the index or use a helper method from TrieUtils. The helper method has the problem, that it uses a fixed field configuration
- TrieUtils currently creates per default a helper field containing the lower precision terms to enable sorting (limitation of one term/document for sorting)
- trieCodeLong/Int() creates unnecessarily String[] and char[] arrays that is heavy for GC, if you index lot of numeric values. Also a lot of char[] to String copying is involved.

This issue should improve this:
- trieCodeLong/Int() returns a TokenStream. During encoding, all char[] arrays are reused by Token API, additional String[] arrays for the encoded result are not created, instead the TokenStream enumerates the trie values.
- Trie fields can be added to Documents during indexing using the standard API: new Field(name,TokenStream,...), so no extra util method needed. By using token filters, one could also add payload and so and customize everything.

The drawback is: Sorting would not work anymore. To enable sorting, a (sub-)issue can extend the FieldCache to stop iterating the terms, as soon as a lower precision one is enumerated by TermEnum. I will create a ""hack"" patch for TrieUtils-use only, that uses a non-checked Exceptionin the Parser to stop iteration. With LUCENE-831, a more generic API for this type can be used (custom parser/iterator implementation for FieldCache). I will attach the field cache patch (with the temporary solution, until FieldCache is reimplemented) as a separate patch file, or maybe open another issue for it."
0,"RemoveVersionTest.testReferentialIntegrityException assumes availability of ref properties and same name sibilingsThis test case:

- assumes availability of Reference properties (should throw NotExecutable when not available), and

- takes advantage if same name siblings (the child node identified by the nodename2 config variable has already been created by the test setup code)"
0,"Intermittent failure in TestThreadedOptimizeFailure looks like this:

{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestThreadedOptimize
    [junit] Testcase: testThreadedOptimize(org.apache.lucene.index.TestThreadedOptimize):	FAILED
    [junit] null
    [junit] junit.framework.AssertionFailedError: null
    [junit] 	at org.apache.lucene.index.TestThreadedOptimize.runTest(TestThreadedOptimize.java:125)
    [junit] 	at org.apache.lucene.index.TestThreadedOptimize.testThreadedOptimize(TestThreadedOptimize.java:149)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:253)
{noformat}

I just committed some verbosity so next time it strikes we'll have more details."
0,"Remove ImportContextImpl#getDetectorthe method ImportContextImpl#getDetector refers an interface method on ImportContext that does not exist.
according to jukka that is a leftover. since the method is not used at all i would therefore suggest to remove it altogether, remove the instance field and deprecate the constructor taking the detector param."
0,"Update overview example codeSee http://lucene.apache.org/java/2_4_1/api/core/overview-summary.html - need to update for non-deprecated best-practices/recommended API usage.

Also, double-check that the demo app works as documented."
0,"Change value for SearchIndex#DEFAULT_EXTRACTOR_BACK_LOGThe value is currently 100. This means that once 100 extractor jobs are pending in the indexing queue additional extractor jobs are executed with the current thread. I think it would be more useful to change this value to Integer.MAX_VALUE (or in other words: unbounded).

If the backlog is filled up then this indicates that the repository is very busy and we should not put additional burden on the current thread in that case."
0,"Improve BenchmarkBenchmark can be improved by incorporating recent suggestions posted
on java-dev. M. McCandless' Python scripts that execute multiple
rounds of tests can either be incorporated into the codebase or
converted to Java."
0,"Misleading method names in SetValueBinaryTestSome of the method names in SetValueBinaryTest say ""Boolean"" when they should say ""Binary"" (copy&paste error from SetValueBooleanTest?).

"
0,"TestTermInfosReaderIndex failing (always reproducible)Always fails on branch (use reproduce string below):
git clone --depth 1 -b rr git@github.com:dweiss/lucene_solr.git

{noformat}
[junit4] Running org.apache.lucene.codecs.lucene3x.TestTermInfosReaderIndex
[junit4] FAILURE 0.04s J0 | TestTermInfosReaderIndex.testSeekEnum
[junit4]    > Throwable #1: java.lang.AssertionError: expected:<field9:z91ob3wozm6d> but was:<:>
[junit4]    > 	at __randomizedtesting.SeedInfo.seed([C7597DFBBE0B3D7D:C6D9CEDD0700AAFF]:0)
[junit4]    > 	at org.junit.Assert.fail(Assert.java:93)
[junit4]    > 	at org.junit.Assert.failNotEquals(Assert.java:647)
[junit4]    > 	at org.junit.Assert.assertEquals(Assert.java:128)
[junit4]    > 	at org.junit.Assert.assertEquals(Assert.java:147)
[junit4]    > 	at org.apache.lucene.codecs.lucene3x.TestTermInfosReaderIndex.testSeekEnum(TestTermInfosReaderIndex.java:137)
[junit4]    > 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[junit4]    > 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
[junit4]    > 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
[junit4]    > 	at java.lang.reflect.Method.invoke(Method.java:597)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1766)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner.access$1000(RandomizedRunner.java:141)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:728)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:789)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:803)
[junit4]    > 	at org.apache.lucene.util.LuceneTestCase$SubclassSetupTeardownRule$1.evaluate(LuceneTestCase.java:744)
[junit4]    > 	at org.apache.lucene.util.LuceneTestCase$InternalSetupTeardownRule$1.evaluate(LuceneTestCase.java:636)
[junit4]    > 	at org.apache.lucene.util.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:22)
[junit4]    > 	at org.apache.lucene.util.LuceneTestCase$TestResultInterceptorRule$1.evaluate(LuceneTestCase.java:550)
[junit4]    > 	at org.apache.lucene.util.LuceneTestCase$RememberThreadRule$1.evaluate(LuceneTestCase.java:600)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:735)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner.access$600(RandomizedRunner.java:141)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner$3$1.run(RandomizedRunner.java:586)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:605)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:641)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:652)
[junit4]    > 	at org.apache.lucene.util.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:22)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner.runSuite(RandomizedRunner.java:533)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner.access$400(RandomizedRunner.java:141)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner$2.run(RandomizedRunner.java:479)
[junit4]    > 
[junit4]   2> NOTE: reproduce with: ant test -Dtests.filter=*.TestTermInfosReaderIndex -Dtests.filter.method=testSeekEnum -Drt.seed=C7597DFBBE0B3D7D -Dargs=""-Dfile.encoding=UTF-8""
[junit4]   2>
[junit4]    > (@AfterClass output)
[junit4]   2> NOTE: test params are: codec=Appending, sim=DefaultSimilarity, locale=en, timezone=Atlantic/Stanley
[junit4]   2> NOTE: all tests run in this JVM:
[junit4]   2> [TestLock, TestFileSwitchDirectory, TestWildcardRandom, TestVersionComparator, TestTermdocPerf, TestBitVector, TestParallelTermEnum, TestSimpleSearchEquivalence, TestNumericRangeQuery64, TestSort, TestIsCurrent, TestToken, TestIntBlockCodec, TestDocumentsWriterDeleteQueue, TestPagedBytes, TestThreadedForceMerge, TestOmitTf, TestSegmentTermEnum, TestIndexWriterConfig, TestCheckIndex, TestTermVectorsWriter, TestNumericTokenStream, TestSearchAfter, TestRegexpQuery, InBeforeClass, InAfterClass, InTestMethod, NonStringProperties, TestIndexWriterMergePolicy, TestVirtualMethod, TestFieldCache, TestSurrogates, TestSegmentTermDocs, TestMultiValuedNumericRangeQuery, TestBasicOperations, TestCodecs, TestDateSort, TestPositiveScoresOnlyCollector, TestBooleanQuery, TestIndexInput, TestMinimize, TestNumericRangeQuery32, TestBoolean2, TestSloppyPhraseQuery, TestNoDeletionPolicy, TestFieldCacheTermsFilter, TestRandomStoredFields, TestDocBoost, TestTransactionRollback, TestUnicodeUtil, TestIndexWriterLockRelease, TestUTF32ToUTF8, TestFixedBitSet, TestDoubleBarrelLRUCache, TestTimeLimitingCollector, TestSpanFirstQuery, TestDirectory, TestSpansAdvanced2, TestConcurrentMergeScheduler, TestIndexWriterExceptions, TestDocValues, TestCustomNorms, TestFieldValueFilter, TestTermVectors, TestTermInfosReaderIndex]
[junit4]   2> NOTE: Linux 2.6.32-38-server amd64/Sun Microsystems Inc. 1.6.0_20 (64-bit)/cpus=4,threads=1,free=100102360,total=243859456
[junit4]   2> 
{noformat}"
0,"Current implementation of fuzzy and wildcard queries inappropriately implemented as Boolean query rewritesThe implementation of MultiTermQuery in terms of BooleanQuery introduces several problems:

1) Collisions with maximum clause limit on boolean queries which throws an exception.  This is most problematic because it is difficult to ascertain in advance how many terms a fuzzy query or wildcard query might involve.

2) The boolean disjunctive scoring is not appropriate for either fuzzy or wildcard queries.  In effect the score is divided by the number of terms in the query which has nothing to do with the relevancy of the results.

3) Performance of disjunctive boolean queries for large term sets is quite sub-optimal"
0,"Documentation improvements for 1.9 releaseI've poked arround the 1.9-rc1 builds and noticed a few simple documentation things that could be cleaned up, a patch will follow that...

1) Adds some additional info to the README.txt
2) Updates the version info in queryparsersyntax.xml and fileformats.html, and advises people 
     with older versions how to find the correct documentation for their version
3) Builds javadocs for all of the contrib modules (the list was incomplete)"
0,"SQL2 joins on empty sets are not efficientIt seems that in the cases where the LEFT side of the join doesn't contain any hits, the QueryEngine in unable to generate an efficient query for the RIGHT side, so it basically select all the possible nodes.
See this discussion as context [0].

Example:
LEFT side has hits, RIGHT side select is fast given some conditions: 
> SQL2 JOIN LEFT SIDE took 18 ms. fetched 145 rows.
> SQL2 JOIN RIGHT SIDE took 67 ms. fetched 0 rows.

LEFT side has no hits, RIGHT select everything
> SQL2 JOIN LEFT SIDE took 8 ms. fetched 0 rows.
> SQL2 JOIN RIGHT SIDE took 845 ms. fetched 13055 rows.
...so it fetches 130k nodes and doesn't keep any of them.


[0] http://jackrabbit.510166.n4.nabble.com/Strange-Search-Performance-problem-with-OR-td4507121.html
"
0,"need members of MultipartRequestEntity to be ""protected"" instead of ""private"" to make it extendable for multipart/relatedAs explained in the mailing-list[1], I'd like to have some of 
MultipartRequestEntity move from ""private"" visibility to ""protected"" visibility,
to be able to extend as MultipartRelatedRequestEntity. Namely, the attribute
""parts"" and the method ""getMultipartBoundary"" would be needed.

Thank you.

[1]
http://mail-archives.apache.org/mod_mbox/jakarta-httpclient-dev/200510.mbox/%3c87irw18ndm.fsf@meuh.mnc.ch%3e"
0,"broken test in AddEventListenerHere's the test code, comments inline prefixed with ""reschke""

    /**
     * Tests if {@link javax.jcr.observation.Event#NODE_ADDED} is created only
     * for the specified path if <code>isDeep</code> is <code>false</code>.
     */
    public void testIsDeepFalseNodeAdded() throws RepositoryException {
        EventResult listener = new EventResult(log);

        // reschke: we are listening for changes at testRoot/nodeName1, with isDeep==false 
        obsMgr.addEventListener(listener, Event.NODE_ADDED, testRoot + ""/"" + nodeName1, false, null, null, false);

        // reschke; node at ""testRoot/nodeName1"" being created, the associated parent node for this event is ""testRoot""
        Node n = testRootNode.addNode(nodeName1, testNodeType);

        // reschke: node at ""testRoot/nodeName1/nodeName2"" being created, the associated parent node for this event is ""testRoot/nodeName1""
        n.addNode(nodeName2);
        testRootNode.save();

        Event[] events = listener.getEvents(DEFAULT_WAIT_TIMEOUT);
        obsMgr.removeEventListener(listener);

        // reschke: test case expects event with path ""testRoot/nodeName1""
        checkNodeAdded(events, new String[]{nodeName1});
    }

So, in plain english:

- test case listens for events where the associated parent node equals ""testRoot/nodeName1"", but
- it expects a single event where the Event.getPath() returns ""testRoot/nodeName1"".

This is incorrect (IMHO), because the associated parent node for *that* event is ""testRoot"". 

So the correct test would be to check for:

        checkNodeAdded(events, new String[]{nodeName1 + ""/"" + nodeName2});

Making this change of course leads to a test failure reported against the RI.

Feedback appreciated.
"
0,"TestBasicCookieAttribHandlers fails on non-english Locale systemsThe Test checks for written dates in the format for cookies which unfortunately includes a two character abbreviation of the day. This differs by locale, so the dateformat has to be constructed with Locale.US (as in DateUtils)"
0,"When sorting by field, IndexSearcher should not compute scores by defaultIn 2.9 we've added the ability to turn off scoring (maxScore &
trackScores, separately) when sorting by field.

I expect most apps don't use the scores when sorting by field, and
there's a sizable performance gain when scoring is off, so I think for
2.9 we should not score by default, and add show in CHANGES how to
enable scoring if you rely on it.

If there are no objections, I'll commit that change in a day or two
(it's trivial).
"
0,"cache entry resource management should be extracted from CachingHttpClientAs we have built in support for stream-based management of cached response bodies, the CachingHttpClient class has its fingers in too many pies and is involved in resource management but not storage of the actual HttpCacheEntries.

I have a patch forthcoming. :)
"
0,"Norm codec strategy in SimilarityThe static span and resolution of the 8 bit norms codec might not fit with all applications. 

My use case requires that 100f-250f is discretized in 60 bags instead of the default.. 10?
"
0,"deprecate ChineseAnalyzerThe ChineseAnalyzer, ChineseTokenizer, and ChineseFilter (not the smart one, or CJK) indexes chinese text as individual characters and removes english stopwords, etc.

In my opinion we should simply deprecate all of this in favor of StandardAnalyzer, StandardTokenizer, and StopFilter, which does the same thing."
0,"Typo in NodeTypeRegistryIt seems a little typo has been introduced in the NodeTypeRegistry, as illustrated in this stack trace : 

Caused by: javax.jcr.RepositoryException: internal error: invalid resource: nodetypes/custom_nodetypes.xml
	at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.<init>(NodeTypeRegistry.java:703) ~[jackrabbit-core-2.2-SNAPSHOT.jar:2.2-SNAPSHOT]
	at org.apache.jackrabbit.core.RepositoryImpl.createNodeTypeRegistry(RepositoryImpl.java:422) ~[jackrabbit-core-2.2-SNAPSHOT.jar:na]
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:294) ~[jackrabbit-core-2.2-SNAPSHOT.jar:na]

This happens when using a DbFileSystem for the root filesystem. This didn't cause a problem in 2.1.1

The patch attached to this ticket correct the issue."
0,"Default configuration not suitable for demo web applicationThe default configuration is not suitable for the demo application. There are no text extractors configured, which makes the populate and search demos useless.

Proposed solution: create a new repository.xml in jackrabbit-webapp with text extractors configured.

I know we should actually try to reduce the number of repository.xml files, but having one dedicated to jackrabbit-webapp seems reasonable, while we should try to achieve the same for the jackrabbit-core module."
0,"Use of google-code-prettify for Lucene/Solr JavadocMy company, RONDHUIT uses google-code-prettify (Apache License 2.0) in Javadoc for syntax highlighting:

http://www.rondhuit-demo.com/RCSS/api/com/rondhuit/solr/analysis/JaReadingSynonymFilterFactory.html

I think we can use it for Lucene javadoc (java sample code in overview.html etc) and Solr javadoc (Analyzer Factories etc) to improve or simplify our life."
0,"repository.xml: throw an exception on errorCurrently, unsupported parameters in repository.xml and workspace.xml are ignored.
To find problems earlier, such problems should result in an exception,
and starting such a repository should not be possible.
The same should happen for unsupported values.

For currently unavailable options
(such as text extraction filter classes if the class is not in the classpath),
at least a warning should be written to the error log, or an error should be thrown.
"
0,"lucene benchmark has some unnecessary fileslucene/contrib/benchmark/.rsync-filter is only in the source pack (and in SVN), I was not aware of this file, though it was added long ago in https://issues.apache.org/jira/browse/LUCENE-848?focusedCommentId=12491404&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-12491404
Not a blocker for this RC, just interesting to note.

maybe this is related to LUCENE-3155 too, in that we could consider this one for automatic exclusion (like DS_Store), but we should fix it if its committed in SVN too.
"
0,Use FieldSelector in Sorted/LuceneQueryHits when reading UUIDLuceneQueryHits currently reads the complete lucene document. This also prevents usage of an underlying UUID cache.
0,"Remove unnecessary NodeImpl references from LuceneQueryFactoryLuceneQueryFactory casts to NodeImpl just to get the node id. 
This info is available via the api as well, so the cast seems unnecessary.
I'll attach a patch for this tiny issue."
0,"generate-maven-artifacts target should include all non-Mavenized Lucene & Solr dependenciesCurrently, in addition to deploying artifacts for all of the Lucene and Solr modules to a repository (by default local), the {{generate-maven-artifacts}} target also deploys artifacts for the following non-Mavenized Solr dependencies (lucene_solr_3_1 version given here):

# {{solr/lib/commons-csv-1.0-SNAPSHOT-r966014.jar}} as org.apache.solr:solr-commons-csv:3.1
# {{solr/lib/apache-solr-noggit-r944541.jar}} as org.apache.solr:solr-noggit:3.1
\\ \\
The following {{.jar}}'s should be added to the above list (lucene_solr_3_1 version given here):
\\ \\
# {{lucene/contrib/icu/lib/icu4j-4_6.jar}}
# {{lucene/contrib/benchmark/lib/xercesImpl-2.9.1-patched-XERCESJ}}{{-1257.jar}}
# {{solr/contrib/clustering/lib/carrot2-core-3.4.2.jar}}**
# {{solr/contrib/uima/lib/uima-an-alchemy.jar}}
# {{solr/contrib/uima/lib/uima-an-calais.jar}}
# {{solr/contrib/uima/lib/uima-an-tagger.jar}}
# {{solr/contrib/uima/lib/uima-an-wst.jar}}
# {{solr/contrib/uima/lib/uima-core.jar}}
\\ \\
I think it makes sense to follow the same model as the current non-Mavenized dependencies:
\\ \\
* {{groupId}} = {{org.apache.solr/.lucene}}
* {{artifactId}} = {{solr-/lucene-}}<original-name>,
* {{version}} = <lucene-solr-release-version>.

**The carrot2-core jar doesn't need to be included in trunk's release artifacts, since there already is a Mavenized Java6-compiled jar.  branch_3x and lucene_solr_3_1 will need this Solr-specific Java5-compiled maven artifact, though."
0,"Cached filter for a single term fieldThese classes implement inexpensive range filtering over a field containing a single term. They do this by building an integer array of term numbers (storing the term->number mapping in a TreeMap) and then implementing a fast integer comparison based DocSetIdIterator.

This code is currently being used to do age range filtering, but could also be used to do other date filtering or in any application where there need to be multiple filters based on the same single term field. I have an untested implementation of single term filtering and have considered but not yet implemented term set filtering (useful for location based searches) as well. 

The code here is fairly rough; it works but lacks javadocs and toString() and hashCode() methods etc. I'm posting it here to discover if there is other interest in this feature; I don't mind fixing it up but would hate to go to the effort if it's not going to make it into Lucene.

"
0,"Add API for selective bundle consistency check (Jackrabbit-specific)Add a jackrabbit-specific API for doing a selective consistencyCheck, ie. on single nodes. The current entire-workspace check can be very slow if there workspace is large enough. Also it should be easy to write a tool to invoke that feature programmatically rather than by configuration + restart (see below).

Existing Implementation:
The current bundle consistencyCheck feature is enabled by setting a bundle PM parameter and restarting Jackrabbit, it will then run upon startup (see JCR-972 for the only issue regarding bundle consistency check). This check looks for broken parent-child relationships, ie. it will remove any child node entries that reference non-existing parent nodes. For non-existing parent UUIDs and other problems in bundles it will log those.

Outlook:
An advanced consistencyCheck could also check for non-existing version nodes and vice-versa (see JCR-630), but this is not the focus of this issue and could be a later addition to the API."
0,Allow o.a.j.jca.JCARepositoryManager to load repository configuration from the classpath.The current implementation of o.a.j.jca.JCARepositoryManager is only able to load configuration files from the file system. It would be useful to allow the configuration to be loaded from the classpath also.
0,"Jar manifest should not contain ${user.name} of the person buildingNot sure if it is a big deal, but I don't particularly like that my user id for my build machine is in the manifest of the JAR that I constructed.  It's a stretch, security-wise, I know, but I don't see how it serves any useful purpose.  We have signatures/logs/SVN tags so we know who built the particular item w/o needing to know what their local user account name is.

The fix is:

{code}
Index: common-build.xml
===================================================================
--- common-build.xml    (revision 661027)
+++ common-build.xml    (working copy)
@@ -281,7 +281,7 @@
                <attribute name=""Implementation-Title"" value=""org.apache.lucene""/>
                <!-- impl version can be any string -->
                <attribute name=""Implementation-Version""
-                          value=""${version} ${svnversion} - ${user.name} - ${DSTAMP} ${TSTAMP}""/>
+                          value=""${version} ${svnversion} - ${DSTAMP} ${TSTAMP}""/>
                <attribute name=""Implementation-Vendor""
                           value=""The Apache Software Foundation""/>
                <attribute name=""X-Compile-Source-JDK"" 
{code} "
0,"all references to incubator need to be replaced with new locationsThe following files under 1.0 branch refer to incubator in one way or another.  Some of them may be benign.

./contrib/bdb-persistence/project.properties
./contrib/bdb-persistence/project.xml
./contrib/bdb-persistence/README.txt
./contrib/classloader/project.properties
./contrib/classloader/project.xml
./contrib/examples/project.xml
./contrib/extension-framework/project.properties
./contrib/extension-framework/project.xml
./contrib/jcr-commands/jmeter-chain/project.properties
./contrib/jcr-commands/project.properties
./contrib/jcr-commands/xdocs/navigation.xml
./contrib/jcr-ext/project.xml
./contrib/jcrtaglib/project.properties
./contrib/orm-persistence/project.properties
./contrib/orm-persistence/project.xml
./jackrabbit/applications/test/cnd-reader-test-input.cnd
./jackrabbit/project.properties
./jackrabbit/project.xml
./jackrabbit/README.txt
./jackrabbit/src/site/fml/faq.fml
./jackrabbit/src/site/xdoc/doc/arch/overview/jcrlevels.xml
./jackrabbit/src/site/xdoc/doc/building.xml
./jackrabbit/src/site/xdoc/doc/config.xml
./jackrabbit/src/site/xdoc/downloads.xml
./jackrabbit/src/site/xdoc/index.xml
./jackrabbit/src/site/xdoc/tasks.xml
./jackrabbit/src/test/java/org/apache/jackrabbit/core/nodetype/compact/CompactNodeTypeDefTest.java
./jca/project.properties
./jca/project.xml
./textfilters/project.properties
./textfilters/project.xml

I'd edit them myself, but I need to sleep... maybe tomorrow if nobody beats me to it.
"
0,"Make QueryAutoStopWordAnalyzer immutable and reusableCurrently QueryAutoStopWordAnalyzer allows its list of stop words to be changed after instantiation through its addStopWords() methods.  This stops the Analyzer from being reusable since it must instantiate its StopFilters every time.

Having these methods means that although the Analyzer can be instantiated once and reused between IndexReaders, the actual analysis stack is not reusable (which is probably the more expensive part).

So lets change the Analyzer so that its stop words are set at instantiation time, facilitating reuse."
0,Bundle cache is not cleared when *BundlePersistenceManager is closedClose method of persistence managers is responsible for releasing all acquired resources. In case of BundlePersistenceManager it should also free memory by clearing the bundle cache.
0,"MergeException from CMS threads should record the DirectoryWhen you hit an unhandled exception in ConcurrentMergeScheduler, it
throws a MergePolicy.MergeException, but there's no easy way to figure
out which index caused this (if you have more than one).

I plan to add the Directory to the MergeException.  I also made a few
other small changes to ConcurrentMergeScheduler:

  * Added handleMergeException method, which is called on exception,
    so that you can subclass ConcurrentMergeScheduler to do something
    when an exception occurs.

  * Added getMergeThread() method so you can override how the threads
    are created (eg, if you want to make them in a different thread
    group, use a pool, change priorities, etc.).

  * Added doMerge(...) to actually do this merge, so you can do
    something before starting and after finishing a merge.

  * Changed private -> protected on a few attrs

I plan to commit in a day or two.
"
0,"Performance improvement: Lazy skipping on proximity fileHello,

I'm proposing a patch here that changes org.apache.lucene.index.SegmentTermPositions to avoid unnecessary skips and reads on the proximity stream. Currently a call of next() or seek(), which causes a movement to a document in the freq file also moves the prox pointer to the posting list of that document.  But this is only necessary if actual positions have to be retrieved for that particular document. 

Consider for example a phrase query with two terms: the freq pointer for term 1 has to move to document x to answer the question if the term occurs in that document. But *only* if term 2 also matches document x, the positions have to be read to figure out if term 1 and term 2 appear next to each other in document x and thus satisfy the query. 

A move to the posting list of a document can be quite expensive. It has to be skipped to the last skip point before that document and then the documents between the skip point and the desired document have to be scanned, which means that the VInts of all positions of those documents have to be read and decoded. 

An improvement is to move the prox pointer lazily to a document only if nextPosition() is called. This will become even more important in the future when the size of the proximity file increases (e. g. by adding payloads to the posting lists).

My patch implements this lazy skipping. All unit tests pass. 


I also attach a new unit test that works as follows:
Using a RamDirectory an index is created and test docs are added. Then the index is optimized to make sure it only has a single segment. This is important, because IndexReader.open() returns an instance of SegmentReader if there is only one segment in the index. The proxStream instance of SegmentReader is package protected, so it is possible to set proxStream to a different object. I am using a class called SeeksCountingStream that extends IndexInput in a way that it is able to count the number of invocations of seek(). 

Then the testcase searches the index using a PhraseQuery ""term1 term2"". It is known how many documents match that query and the testcase can verify that seek() on the proxStream is not called more often than number of search hits.

Example:
Number of docs in the index: 500
Number of docs that match the query ""term1 term2"": 5

Invocations of seek on prox stream (old code): 29
Invocations of seek on prox stream (patched version): 5

- Michael
"
0,Keep WebDAV exception causesThe DavMethodBase and ExceptionConverter classes in jackrabbit-webdav and jackrabbit-spi2dav don't include the cause when throwing an exception based on some caught cause. This makes it harder to identify what is causing a  particular problem. The attached patch fixes that.
0,"[PATCH] Indexing on Hadoop distributed file systemIn my current project we needed a way to create very large Lucene indexes on Hadoop distributed file system. When we tried to do it directly on DFS using Nutch FsDirectory class - we immediately found that indexing fails because DfsIndexOutput.seek() method throws UnsupportedOperationException. The reason for this behavior is clear - DFS does not support random updates and so seek() method can't be supported (at least not easily).
 
Well, if we can't support random updates - the question is: do we really need them? Search in the Lucene code revealed 2 places which call IndexOutput.seek() method: one is in TermInfosWriter and another one in CompoundFileWriter. As we weren't planning to use CompoundFileWriter - the only place that concerned us was in TermInfosWriter.
 
TermInfosWriter uses IndexOutput.seek() in its close() method to write total number of terms in the file back into the beginning of the file. It was very simple to change file format a little bit and write number of terms into last 8 bytes of the file instead of writing them into beginning of file. The only other place that should be fixed in order for this to work is in SegmentTermEnum constructor - to read this piece of information at position = file length - 8.
 
With this format hack - we were able to use FsDirectory to write index directly to DFS without any problems. Well - we still don't index directly to DFS for performance reasons, but at least we can build small local indexes and merge them into the main index on DFS without copying big main index back and forth. 

"
0,"Minor refactoring to IndexFileNameFilterIndexFileNameFilter looks like it's designed to be a singleton, however its constructor is public and its singleton member is package visible. The proposed patch changes the constructor and member to private. Since it already has a static getFilter() method, and no code in Lucene references those two, I don't think it creates any problems from an API perspective."
0,Make CFS appendable  Currently CFS is created once all files are written during a flush / merge. Once on disk the files are copied into the CFS format which is basically a unnecessary for some of the files. We can at any time write at least one file directly into the CFS which can save a reasonable amount of IO. For instance stored fields could be written directly during indexing and during a Codec Flush one of the written files can be appended directly. This optimization is a nice sideeffect for lucene indexing itself but more important for DocValues and LUCENE-3216 we could transparently pack per field files into a single file only for docvalues without changing any code once LUCENE-3216 is resolved.
0,Namespace handling in AbstractSession should be synchronizedThe AbstractSession base class in o.a.j.commons implicitly assume that the session is never accessed concurrently from more than one thread and thus doesn't synchronize access to the namespace map. This causes problems when the session *is* accessed concurrently. Instead of relying on client code we should enforce thread-safety by explicitly synchronizing potentially unsafe operations on the session instance.
0,"IndexableBinaryStringTools: convert arbitrary byte sequences into Strings that can be used as index terms, and vice versaProvides support for converting byte sequences to Strings that can be used as index terms, and back again. The resulting Strings preserve the original byte sequences' sort order (assuming the bytes are interpreted as unsigned).

The Strings are constructed using a Base 8000h encoding of the original binary data - each char of an encoded String represents a 15-bit chunk from the byte sequence.  Base 8000h was chosen because it allows for all lower 15 bits of char to be used without restriction; the surrogate range [U+D800-U+DFFF] does not represent valid chars, and would require complicated handling to avoid them and allow use of char's high bit.

This class is intended to serve as a mechanism to allow CollationKeys to serve as index terms."
0,"Setting SSLSocket parametersIn HttpClient 4.0.3, it was easy to subclass SSLSocketFactory, and set SSLSocket options (e.g. setEnabledCipherSuites() or setSSLParameterse()) before the SSL handshake happened. This way it was possible to e.g. restrict cipher suites on per-HttpClient basis (instead of JVM-wide system properties).

In HttpClient 4.1.1, the design has changed quite a lot, and copy-pasting of several long methods is needed. 

Ideally, SSLSocketFactory should support applying SSLParameters to the socket. However, SSLParameters is Java 1.6, so if we want to keep compatibility with 1.5, that's out.

However, it'd be nice to at least have a method (e.g. ""protected SSLSocket prepareSSLSocket(SSLSocket s)"") that would get called immediately after a socket is retrieved from the socket factory. The default implementation could be just ""return s;"", but subclasses could do something like s.setEnabledCipherSuites() s.setSSLParameters()."
0,"move contrib/benchmark to modules/benchmarkI think we should move lucene/contrib/benchmark to a shared modules/benchmark, so you can easily benchmark anything (lucene, solr, other modules like analysis or whatever).

For example, if you want to do some benchmarking of something in solr (LUCENE-2844) you should be able to do this.
Another example is simply being able to benchmark an analyzer definition from a schema.xml, its more convenient than writing the equivalent java analyzer just for benchmarking.

"
0,"FieldsInfo uses deprecated APIThe class FieldsInfo.java uses deprecated API in method ""public void add(Document doc)""
I rused the replacement and created the patch -> see attachment"
0,WriteLineDocTask should write gzip/bzip2/txt according to the extension of specified output file nameSince the readers behave this way it would be nice and handy if also this line writer would.
0,"Root exception not logged in ClusterNode for ClusterExceptionWhen our MySQL server is down or failed queries we have the following log :
ERROR (ClusterNode-node1) [org.apache.jackrabbit.core.cluster.ClusterNode] Periodic sync of journal failed: Unable to return record iterater.

So the root exception (SQLException in my case) is missing from the log and this prevent me from quickly finding the reason."
0,"ant generate-maven-artifacts target broken for contribWhen executing 'ant generate-maven-artifacts' from a pristine checkout of branch_3x/lucene or trunk/lucene the following error is encountered:

{code}
dist-maven:
     [copy] Copying 1 file to /home/drew/lucene/branch_3x/lucene/build/contrib/analyzers/common
[artifact:install-provider] Installing provider: org.apache.maven.wagon:wagon-ssh:jar:1.0-beta-2:runtime
[artifact:pom] An error has occurred while processing the Maven artifact tasks.
[artifact:pom]  Diagnosis:
[artifact:pom] 
[artifact:pom] Unable to initialize POM pom.xml.template: Cannot find parent: org.apache.lucene:lucene-contrib for project: org.apache.lucene:lucene-analyzers:jar:3.1-SNAPSHOT for project org.apache.lucene:lucene-analyzers:jar:3.1-SNAPSHOT
[artifact:pom] Unable to download the artifact from any repository
{code}


The contrib portion of the ant build is executed in a subant task which does not pick up the pom definitions for lucene-parent and lucene-contrib from the main build.xml, so the lucene-parent and lucene-controb poms must be loaded specifically as a part of the contrib build using the artifact:pom task.
"
0,"build.xml: result of ""dist-src"" should support ""build-contrib""Currently the packed src distribution would fail to run ""ant build-contrib"".
It would be much nicer if that work.
In fact, would be nicer if you could even ""re-pack"" with it.

For now I marked this for 2.1, although I am not yet sure if this is a stopper."
0,"PostMethod Java doc refers to wrong section of RFC1945""The HTTP POST method is defined in section 9.5 of RFC1945"" should read ""The
HTTP POST method is defined in section 8.3 of RFC1945""

Change 9.5 to 8.3."
0,"[PATCH] HttpClient#getHost & HttpClient#getPort methods are misleadingHttpClient#getHost & HttpClient#getPort methods are misleading, accompanied by
obsolete, factually wrong javadocs and as such should be deprecated.

Oleg"
0,"Upgrade to Tika 0.8Apache Tika version 0.8 is now available, and we should upgrade to benefit from the various fixes and improvements included in that version."
0,SPNEGO authentication schemeConsider integrating the SPNEGO auth scheme from Commons HttpClient contrib package into HttpClient 4.0
0,Add getTotalSize() to QueryResultsAs discussed in http://www.nabble.com/Total-size-of-a-query-result-and-setLimit%28%29-tf4280909.html#a12185543 a getTotalSize() method should be added to QueryResults.
0,"Add some ligatures (ff, fi, fl, ft, st) to ISOLatin1AccentFilterISOLatin1AccentFilter removes common diacritics and some ligatures. This patch adds support for additional common ligatures: ff, fi, fl, ft, st."
0,Move privilege reader/writer to spi-commons and use qualified namesthe current privilege reader in jcr-commons uses a PrivilegeDefinition that is based on pure string rather than qualified names. suggest to move that to the spi-commons and use org.apache.jackrabbit.spi.Name for the privilege names.
0,"Update SPI locking to match JCR 2.0jcr2spi currently uses the JSR 170 way to determine whether a given Session owns the lock by checking of the lock token is null.
with JSR 283 a new Lock method has been defined for this, while on the other hand the lock token is always null for session-scoped
locks.

In addition 283-locking allows to specify a timeout hint and hint about the owner info that should be displayed
for information purpose.

Proposed changes to SPI:

- extend org.apache.jackrabbit.spi.LockInfo to cover the new functionality added with JSR 283
- add an variant of RepositoryService.lock that allows to specify timeout and owner hint.

Proposed changes to JCR2SPI:
- change jcr2spi to make use of the new functionality and modify the test for session being lock holder.
  this mainly affects
  > LockOperation
  > LockManager impl
  > Lock impl"
0,Improve docs for deployment Models 1 and 2 on Tomcat 5.5.x and provide an example webapp.New users would find a small webapp and associated documentation that walks them through the process of setting up a model 1 or model 2 (or both) deployment scheme.
0,"PhraseQuery/TermQuery/SpanQuery use IndexReader specific stats in their explainsPhraseQuery uses IndexReader in explainfor top level stats - as mentioned by Mike McCandless in LUCENE-1837.
TermQuery uses IndexReader in explain for top level stats

Always been a bug with MultiSearcher, but per segment search makes it worse.

"
0,"MergePolicy.OneMerge.segments should be List<SegmentInfo> not SegmentInfos, Remove Vector<SI> subclassing from SegmentInfos & more refactoringSegmentInfos carries a bunch of fields beyond the list of SI, but for merging purposes these fields are unused.

We should cutover to List<SI> instead.

Also SegmentInfos subclasses Vector<SI>, this should be removed and the collections be hidden inside the class. We can add unmodifiable views on it (asList(), asSet())."
0,"Change defaultValues format in NodeTypes XML to jcr valuecurrently, the defaultValues serialization in the nodetypes.xml is the only one that uses internal value serialization, rather than the jcr string serialization.
eg:

<propertyDef name=""jcr:requiredPrimaryTypes"" ..... >
  <defaultValues>
    <defaultValue>{http://www.jcp.org/jcr/nt/1.0}base</defaultValue>
  </defaultValues>
</propertyDef>

this in not very handy, when the custom_nodetypes.xml should be written automatically.
i suggest to change the serialization to use the jcr value one:

<propertyDef name=""jcr:requiredPrimaryTypes"" ..... >
  <defaultValues>
    <defaultValue>nt:base</defaultValue>
  </defaultValues>
</propertyDef>
"
0,"Optimize PhraseQueryLooking the scorers for PhraseQuery, I think there are some speedups
we could do:

  * The AND part of the scorer (which advances to the next doc that
    has all the terms), in PhraseScorer.doNext, should do the same
    optimizing as BooleanQuery's ConjunctionScorer, ie sort terms from
    rarest to most frequent.  I don't think it should use a linked
    list/firstToLast() that it does today.

  * We do way too much work now when .score() is not called, because
    we go and find all occurrences of the phrase in the doc, whereas
    we should stop only after finding the first and then go and count
    the rest if .score() is called.

  * For the exact case, I think we can use two int arrays to find the
    matches.  The first array holds the count of how many times a term
    in the phrase ""matched"" a phrase starting at that position.  When
    that count == the number of terms in the phrase, it's a match.
    The 2nd is a ""gen"" array (holds docID when that count was last
    touched), to avoid clearing.  Ie when incrementing the count, if
    the docID != gen, we reset count to 0.  I think this'd be faster
    than the PQ we now use.  Downside of this is if you have immense
    docs (position gets very large) we'd need 2 immense arrays.

It'd be great to do LUCENE-1252 along with this, ie factor
PhraseScorer into two AND'd sub-scorers (LUCENE-1252 is open for
this).  The first one should be ConjunctionScorer, and the 2nd one
checks the positions (ie, either the exact or sloppy scorers).  This
would mean if the PhraseQuery is AND'd w/ other clauses (or, a filter
is applied) we would save CPU by not checking the positions for a doc
unless all other AND'd clauses accepted the doc.
"
0,Use type StaticOperand for fullTextSearchExpressionSee: https://jsr-283.dev.java.net/issues/show_bug.cgi?id=691
0,"make sure no static loggers are usedReview all loggers used in the component, make sure they are stored in non-static attributes only.
http://wiki.apache.org/jakarta-commons/Logging/StaticLog
"
0,"DocValues cleanup: constructor & getInnerArray()DocValues constructor taking a numDocs parameter is not very clean.
Get rid of this.

Also, it's optional getInnerArray() method is not very clean.
This is necessary for better testing, but currently tests will fail if it is not implemented.
Modify it to throw UnSupportedOp exception (rather than returning an empty array).
Modify tests to not fail but just warn if the tested iml does not override it.

These changes should make it easier to implement DocValues for other ValueSource's, e.g. above payloads, with or without caching.
"
0,"Rename Field.Index.UN_TOKENIZED/TOKENIZED/NO_NORMSThere is confusion about these current Field options and I think we
should rename them, deprecating the old names in 2.4/2.9 and removing
them in 3.0.  How about this:

{code}
TOKENIZED --> ANALYZED
UN_TOKENIZED --> NOT_ANALYZED
NO_NORMS --> NOT_ANALYZED_NO_NORMS
{code}

Should we also add ANALYZED_NO_NORMS?

Spinoff from here:

    http://mail-archives.apache.org/mod_mbox/lucene-java-user/200808.mbox/%3C48a3076a.2679420a.1c53.ffffa5c4%40mx.google.com%3E
    "
0,"DavMethods.POST should be public, not privateDavMethods.POST is declared as a private constant, when it should be public. attached is a patch to fix this."
0,"OCM:Add the ability to specify name of a Collection Element through XML Mapping files.Collection elements get mapped to a node ""collection-element"" when the mappings are specified through XML config files.  We need the ability to control this name through configuration.  Without that feature querying object structures is painful.  For example I have structure as below :

class Foo{
String id;
 List<Foo> children
 List<Foo> friends
}

And I have a need to query a Foo with id : 100 .  If I am interested only in child nodes with id = 110 , I could specify through the Filter that look at only node names , ""childFoo"" ; If I have the flexibility of adding a child node name."
0,"the demo application does not work as of 3.0the demo application does not work. QueryParser needs a Version argument.

While I am here, remove @author too"
0,Javadoc improvements for Payload classSome methods in org.apache.lucene.index.Payload don't have javadocs
0,"Populating exception message with InetSocketAddress.getHostName() can take a long timeIn the PlainSocketFactory class, when a SocketTimeoutException occurs a call is made to InetSocketAddress.getHostName() when generating the exception message. Unfortunately, this call can take a long time. In my case, the address I am specifying is an IP address, which InetSocketAddress attempts to perform a reverse-lookup on to determine the hostname; however, since  the address does not have a hostname assigned to it, the operation takes a long time to return.

I'm attaching a patch for trunk with my proposed fix. Viewing the source history, it looks like the code used to have the behavior I'm proposing, but it was changed in revision 1070943. Based on the source commits and linked issues, I cannot determine a specific reason for the change. If there is a reason the code needs to be the way it is, then I apologize for inconvenience I have caused."
0,Jcr-Server: remove jcr depedency from dav-library
0,"ProtocolException thrown on slightly broken headersHTTPClient throws an exception when parsing headers returned by GET from the
following URL:

 http://butler.cit.nih.gov/hembase/hembase.taf

The headers returned are as follows:

HTTP/1.0 200 OK\r\nServer: WebSTAR/1.0 ID/ACGI\r\nMIME-Version:
1.0\r\nContent-Type: text/html\r\nSet-Cookie:
Tango_UserReference=ADC5871C57FABEDEC63DD47B; path=/\n\r\r\n\r\n<!DOCTYPE HTML
PUBLIC ""-//W3C//DTD HTML 4.01 Transitional//EN"">...

Please note the superfluous \r in the line separating headers from the body.
IMHO this type of error should generate a warning, but then it should cause a
graceful recovery. Currently a ProtocolException is thrown.

Standard java.net.HttpURLConnection handles this just fine, without giving any
warning."
0,"QueryParser throws new exceptions even if custom parsing logic threw a better oneWe have subclassed QueryParser and have various custom fields.  When these fields contain invalid values, we throw a subclass of ParseException which has a more useful message (and also a localised message.)

Problem is, Lucene's QueryParser is doing this:

{code}
    catch (ParseException tme) {
        // rethrow to include the original query:
        throw new ParseException(""Cannot parse '"" +query+ ""': "" + tme.getMessage());
    }
{code}

Thus, our nice and useful ParseException is thrown away, replaced by one with no information about what's actually wrong with the query (it does append getMessage() but that isn't localised.  And it also throws away the underlying cause for the exception.)

I am about to patch our copy to simply remove these four lines; the caller knows what the query string was (they have to have a copy of it because they are passing it in!) so having it in the error message itself is not useful.  Furthermore, when the query string is very big, what the user wants to know is not that the whole query was bad, but which part of it was bad.

"
0,"Add signature and major/minor version to the journal files used for clusteringJournal files used for clustering should contain a signature, and a major/minor version that helps identifying them."
0,"Each TransactionContext creates new threadThe rollback threads are not stopped when the transaction commits, but only when the timeout occurs. This has the effect that lots of threads are created and sleeping when many transactions are committed in a short time frame. The rollback thread should be signaled when the transaction is committed or even better a Timer should be used with a single thread for all transaction contexts."
0,"need the ability to also sort SpellCheck results by freq, instead of just by Edit Distance+freqThis issue was first noticed and reported in this Solr thread; http://lucene.472066.n3.nabble.com/spellcheck-issues-td489776.html#a489788

Basically, there are situations where it would be useful to sort by freq first, instead of the current ""sort by edit distance, and then subsort by freq if edit distance is equal""

The author of the thread suggested ""What I think would work even better than allowing a custom compareTo function would be to incorporate the frequency directly into the distance function.  This would allow for greater control over the trade-off between frequency and edit distance""

However, custom compareTo functions are not always be possible (ie if a certain version of Lucene must be used, because it was release with Solr) and incorporating freq directly into the distance function may be overkill (ie depending on the implementation)

it is suggested that we have a simple modification of the existing compareTo function in Lucene to allow users to specify if they want the existing sort method or if they want to sort by freq.

"
0,Can't specify AttributeSource for TokenizerOne can't currently specify the attribute source for a Tokenizer like one can with any other TokenStream.
0,"David Spencer Spell Checker improvedhy,
i developed a SpellChecker based on the David Spencer code (DSc) but more flexible.
the structure of the index is inspired of the DSc (for a 3-4 gram):
word:
gram3:
gram4:
 
3start:
4start:
..
3end:
4end:
..
transposition:
 
This index is a dictonary so there isn't the ""freq"" field like with DSc version.
it's independant of the user index. So we can add words becoming to several
fields of several index for example or, why not, to a file with a list of words.
The suggestSimilar method return a list of suggests word sorted by the
Levenshtein distance and optionaly to the popularity of the word for a specific
field in a user index. More of that, this list can be restricted only to words
present in a specific field of a user index.
 
See the test case.
 
i hope this code will be put in the lucene sandbox. 
 
Nicolas Maisonneuve"
0,JSR 283 Query
0,"Move tika-parsers dependency to deployment packagesAs discussed on the mailing list, it would be better if the tika-parsers dependency (and all the parser libraries it pulls in transitively) was included in our deployment packages but not directly in jackrabbit-core. This would make it easier for people to set up custom lightweight deployments with no or only partial full text extraction functionality.

To do this we'll first need to wait for Tika 0.9, as we currently have a custom PDFParser class in jackrabbit-core as a workaround to a problem in Tika 0.8.

At the same time we should do a more thorough review of the transitive parser dependencies we include. At least the rome and bouncycastle libraries were flagged as potentially unnecessary."
0,"Variant spelling ""Trasaction"" and ""Transaction"" now in codebaseIncorrectly spelled ""Trasaction"" now in codebase.  The correctly spelled ""Transaction"" appears a far greater number of times.

Isolated to package: org.apache.jackrabbit.jca

Incorrectly  spelled ""Trasaction"" found in:
(3x) JCAManagedConnection.java 
(7x) JCAManagedConnectionFActory.java

Correctly spelled ""Transaction"" found in:
(20x) JCAManagedConnection.java 
(2x) JCAManagedConnectionFActory.java 
(1x) JCAResourceAdapter.java 
(12x) TransactionBoundXAResource.java 
"
0,"Add support for distributed stats(its a bug in a way, since we broke this, temporarily).

There is no way to do this now (distributed IDF, etc) with the new API.

But we should do it right:
* having the sim ask the searcher for docfreq of a term is wasteful and dangerous, 
  usually we have already seek'd to the term and already collected the 'raw' stuff.
* the situation is more than just docfreq, because you should be able to implement
  distributed scoring for all of the new sim models (or your own), that use any
  of Lucene's stats.
"
0,"unit tests should use private directoriesThis only affects our unit tests...

I run ""ant test"" and ""ant test-tag"" concurrently, but some tests have false failures (eg TestPayloads) because they use a fixed test directory in the filesystem for testing.

I've added a simple method to _TestUtil to get a temp dir, and switched over those tests that I've hit false failures on."
0,"terminology: source uses ""protected  property"" for something that only only indirectly has to do with that termDocumentation and method names (DavProperty) use ""protected"" as pseudonym for ""return upon PROPFIND/allprop"". This isn't really accurate, because the live properties defined in RFC2518/4918 *are* protected, but are returned with PROPFIND/allprop nevertheless.

Proposal: update documentation and method names to say something like ""visibleInAllprop"".
"
0,"Reduce log level in MultiIndex for deleting obsolete indexThe MultiIndex class issues a logging message (with info level) that the obsolete index cannot be deleted (quite often).
As the segments are deleted later (with a retry) and this ""warning"" can be ignored (http://dev.day.com/kb/content/wiki/kb/Crx/Troubleshooting/UnableToDeleteObsoleteIndex.html ), it would be nice to reduce the logging level to debug. People, who are maintaining projects, are not aware of Jackarabbit details and are sometimes scared about this ""warning"" :-)

Thank you in advance!

Kind regards
Sergiy"
0,"Deprecate non-pooled bundle DB persistence managersIn JCR-1456 and Jackrabbit 2.0 we introduced database connection pooling, but decided to keep the existing database bundle persistence managers intact to avoid potential regressions. We haven't seen such problems even though pooled bundle persistence has been the default since the 2.0 release, so I think it would be safe to deprecate all the non-pooled bundle DB PMs.

And in order to remove duplicate code (that has already complicated some changes within o.a.j.persistence), I'd also take the extra step of  making the o.a.j.p.bundle.* classes extend respective the o.a.j.p.pool.* classes. This would automatically allow also old non-pooled configurations to benefit from connection pooling."
0,Add a filtered RangeIteratorIt would be useful to have a FilteredRangeIterator utility class that can be used to apply arbitrary filters on other RangeIterators.
0,Replace SegmentReader.Ref with AtomicIntegerI think the patch should be applied to backcompat tag in its entirety.
0,"Create EMPTY_ARGS constsant in SnowballProgram instead of allocating new Object[0]Instead of allocating new Object[0] create a proper constant in SnowballProgram. The same (for new Class[0]) is created in Among, although it's less critical because Among is called from static initializers ... Patch will follow shortly."
0,"common interface for HttpRoute and RouteTrackerClasses HttpRoute and RouteTracker have many identical getters. There should be a common interface, for example RouteInfo, to define these getters and a toRoute() method that returns an unmodifiable representation. Some portions of the API may then accept the interface instead of the specific class HttpRoute.
"
0,"Add matchVersion to StandardAnalyzerI think we should add a matchVersion arg to StandardAnalyzer.  This
allows us to fix bugs (for new users) while keeping precise back
compat (for users who upgrade).

We've discussed this on java-dev, but I'd like to now make it concrete
(patch attached).  I think it actually works very well, and is a
simple tool to help us carry out our back-compat policy.

I coded up an example with StandardAnalyzer:

  * The ctor now takes a required arg (Version matchVersion).  You
    pass Version.LUCENE_CURRENT to always get lates & greatest, or eg
    Version.LUCENE_24 to match 2.4's bugs/settings/behavior.

  * StandardAalyzer conditionalizes the ""replace invalid acronym"" and
    ""enable position increment in StopFilter"" based on matchVersion.

  * It also prevents creating zillions of ctors, over time, as we need
    to change settings in the class.  EG StandardAnalyzer now has 2
    settings that are version dependent, and there's at least another
    2 issues open on fixing some more of its bugs.

The migration is also very clean: we'd only add this to classes on an
""as needed"" basis.  On the first release that adds the arg, the
default remains back compatible with the prior release.  Then, going
forward, we are free to fix issues on that class and conditionalize by
matchVersion.

The javadoc at the top of StandardAnalyzer clearly calls out what
version specific behavior is done:

{code}
 * <p>You must specify the required {@link Version}
 * compatibility when creating StandardAnalyzer:
 * <ul>
 *   <li> As of 2.9, StopFilter preserves position
 *        increments by default
 *   <li> As of 2.9, Tokens incorrectly idenfied as acronyms
 *        are corrected (see <a href=""https://issues.apache.org/jira/browse/LUCENE-1068"">LUCENE-1608</a>
 * </ul>
 *
{code}
"
0,"Build failed in the flexscoring branch because of Javadoc warningsAnt build log:
  [javadoc] Standard Doclet version 1.6.0_24
  [javadoc] Building tree for all the packages and classes...
  [javadoc] /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/src/java/org/apache/lucene/search/Similarity.java:93: warning - Tag @link: can't find tf(float) in org.apache.lucene.search.Similarity
  [javadoc] /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/src/java/org/apache/lucene/search/TFIDFSimilarity.java:588: warning - @param argument ""term"" is not a parameter name.
  [javadoc] /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/src/java/org/apache/lucene/search/TFIDFSimilarity.java:588: warning - @param argument ""docFreq"" is not a parameter name.
  [javadoc] /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/src/java/org/apache/lucene/search/TFIDFSimilarity.java:618: warning - @param argument ""terms"" is not a parameter name.
  [javadoc] Generating /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/org/apache/lucene/store/instantiated//package-summary.html...
  [javadoc] Copying file /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/doc-files/classdiagram.png to directory /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/org/apache/lucene/store/instantiated/doc-files...
  [javadoc] Copying file /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/doc-files/HitCollectionBench.jpg to directory /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/org/apache/lucene/store/instantiated/doc-files...
  [javadoc] Copying file /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/doc-files/classdiagram.uxf to directory /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/org/apache/lucene/store/instantiated/doc-files...
  [javadoc] Generating /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/serialized-form.html...
  [javadoc] Copying file /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/prettify/stylesheet+prettify.css to file /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/stylesheet+prettify.css...
  [javadoc] Building index for all the packages and classes...
  [javadoc] Building index for all classes...
  [javadoc] Generating /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/help-doc.html...
  [javadoc] 4 warnings
"
0,"SimpleSelectionTest assumes RowIterator.getSize() not to return -1Test case ""testSingleProperty"" assumes that RowIterator.getSize() will not return -1. This is an incorrect assumption, according to the JavaDoc for RangeIterator.

Suggested change:

        long size = result.getRows().getSize();
        if (size != -1) {
            assertEquals(""Should have only 1 result"", 1, size);
        }
"
0,"improve windows defaults in FSDirectoryCurrently windows defaults to SimpleFSDirectory, but this is a problem due to the synchronization.

I have been benchmarking queries *sequentially* and was pretty surprised at how much faster
MMapDirectory is, for example for cases that do many seeks.

I think we should change the defaults for windows as such:

if (WINDOWS and UNMAP_SUPPORTED and 64-bit)
  use MMapDirectory
else
  use SimpleFSDirectory 

I think we should just consider doing this for 4.0 only and see how it goes.
"
0,"ms-sql tablespace support for FileSystem and PersistenceManagerTrunk and released version 1.5.0 does not have complete support for ms-sqlserver tablespaces.  This patch was originally submitted via JCR-1295 but was not applied to the 1.4 trunk.

"
0,refactor consistency checks in BundleDBPersistenceManager into a standalone class that could be re-used for other PMssee subject
0,"Allow configuration of SO_LINGERThere is currently no way to configure the SO_LINGER option on a socket.

Please change the HttpClient class to allow the configuration of the SO_LINGER
option on a socket, similar to the way the SO_TIMEOUT can be configured.

Suggested extension to the interface of the HttpClient class:
- Add method setSoLinger() to set the current setting for SO_LINGER. The method
could accept one argument. A negative value could indicate that the SO_LINGER
should be disabled.
- Add method getSoLinger() that returns the current setting for SO_LINGER. A
negative value would indicate that the SO_LINGER option is disabled.

See:
http://java.sun.com/j2se/1.4.2/docs/api/java/net/Socket.html#setSoLinger(boolean,%20int)"
0,"[PATCH] jackrabbit-webapp pom.xml patch to create an additional jar artifactModifies the jackrabbit-webapp pom.xml to create a jar artifact in addition to the existing war artifact, to allow the jackrabbit-webapp utility servlets to be reused in other modules.

The right way would be to create a separate jar module for the servlets (or move them to jackrabbit-jcr-commons?), and reuse that jar as a dependency in the jackrabbit-webapp. So I'm not sure if this patch deserves to be applied to the trunk, but it can be useful as a workaround before a cleaner solution is implemented.

See also http://mail-archives.apache.org/mod_mbox/jackrabbit-dev/200705.mbox/%3C510143ac0705151453t7a0eb4cam859a40fb106e81f5@mail.gmail.com%3E which discusses possible improvements to these jackrabbit-webapp utility servlets.

"
0,"Make tests using java.util.Random reproducible on failureThis is a patch for LuceneTestCase to support logging of the Random seed used in randomized tests. The patch also includes an example implementation in TestTrieRangeQuery.

It overrides the protected method runTest() and inserts a try-catch around the super.runTest() call. Two new methods newRandom() and newRandom(long) are available for the test case. As each test case is run in an own TestCase object instance (so 5 test methods in a class instantiate 5 instances each method working in separate), the random seed is saved on newRandom() and when the test fails with any Throwable, a message with the seed (if not null) is printed out. If newRandom was never called no message will be printed.

This patch has only one problem: If a single test method calls newRandom() more than once, only the last seed is saved and printed out. But each test method in a Testcase should call newRandom() exactly once for usage during the execution of this test method. And it is not thread save (no sync, no volatile), but for tests it's unimportant.

I forgot to mention: If a test fails, the message using the seed is printed to stdout. The developer can then change the test temporarily:

{code}LuceneTestCase.newRandom() -> LuceneTestCase.newRandom(long){code}

using the seed from the failed test printout.

*Reference:*
{quote}
: By allowing Random to randomly seed itself, we effectively test a much
: much larger space, ie every time we all run the test, it's different.  We can
: potentially cast a much larger net than a fixed seed.

i guess i'm just in favor of less randomness and more iterations.

: Fixing the bug is the ""easy"" part; discovering a bug is present is where
: we need all the help we can get ;)

yes, but knowing a bug is there w/o having any idea what it is or how to 
trigger it can be very frustrating.

it would be enough for tests to pick a random number, log it, and then use 
it as the seed ... that way if you get a failure you at least know what 
seed was used and you can then hardcode it temporarily to reproduce/debug

-Hoss
{quote}"
0,"Improve error messages for index aggregatesIn the case where an index aggregate fails because of a node that doesn't exist the logged warn messages contain a full stack-trace.
Besides the fact that this can be misleading (you may think that there's something wrong that you need to fix right away) it is also borderline useless.

The desired behavior would be to just log an ""info"" message mentioning that a certain node was skipped, similar to what the SeachManager does."
0,"NTLM class registers Sun JCE implementation by defaultCurrently the NTLM class attempts to load and register the Sun JCE implementation unless a 
System property is set to indicate a different JCE to use.  We should remove this entirely and leave 
the installation and configuration of the JCE to the application rather than trying to do it ourselves 
as this could cause problems with other implementations of JCE.  I'll attach an initial patch for this 
in a moment, with a patch for the documentation in the morning.  (Writing docs at 1am is never a 
good idea.)"
0,create test case to verify we support > 2.1B termsI created a test case for this... I'm leaving it as @Ignore because it takes more than four hours on a faaast machine (beast) to run.  I think we should run this before each release.
0,"Flex on non-flex emulation of TermsEnum incorrectly seeks/nexts beyond current fieldSpinoff of LUCENE-2111, where Uwe found this issue with the flex on non-flex emulation."
0,"variables should be accessed through gettersSome attention should be placed on classes who shared their variables directly (as opposed to through a getter). This is sometimes OK for subclasses, but rarely good for other classes that use the objects. There's a small number of classes that have non-private variables, especially in the impl.conn & impl.conn.tsccm packages.

See HTTPCLIENT-745 ."
0,"JCRTest.java (First Steps example code) creates a StringValue with ""new""The JCRTest.java file described in the First Steps document (http://incubator.apache.org/jackrabbit/firststeps.html) on the jackrabbit incubator website contains a line that attempts to create a StringValue using new, rather than using the ValueFactory interface. This causes the code to fail to compile - perhaps an initiative test, but could be off-putting...

Simple fix is to swap the line:

 n.setProperty(""testprop"", new StringValue(""Hello, World.""));

to 

n.setProperty(""testprop"", session.getValueFactory().createValue(""Hello, World.""));

"
0,"overhaul connection manager and associated connection interfaceMultiThreadedHttpConnectionManager/HttpHostConnection needs to be overhauled to provide a layer on top of OperatedClientConnection.
Preliminary working names: ThreadSafeClientConnManager/ManagedClientConnection

This implies some work on former HttpMethodDirector and HttpClient to verify completeness of the new connection management API.
"
0,"XMLPersistenceManager fails after creating too many directories on linuxWhen using the  XMLPersistenceManager it creates a bunch of directories in jackrabbit/home/version/data. Eventually I reach 32000 directories in the data directory and subsequent writes fail.

I believe this is caused by XMLPersistenceManager.buildNodeFolderPath() method where it does 
   if (cnt == 4 || cnt == 8) {
      sb.append('/');
   }

This causes the subdirectories to be 4 characters, 0-f i.e. 16^4 which is 65536, if what I'm seeing is correct, on linux ext3, it's limited to 32000 entries. If the XMLPersistence manager used 2 or 3 characters this should fix the problem, or if it were configurable it would also solve this (I think).

an 
   ls jackrabbit/home/version/data | wc -l
returns 
   32001

A stack trace for when this happens is as follows :
Caused by: javax.jcr.RepositoryException: failed to write node state: da2cd5d1-9776-4dbe-a42b-842b0134dbfb: failed to write node state: da2cd5d1-9776-4dbe-a42b-842b0134dbfb
        at org.apache.jackrabbit.core.version.AbstractVersionManager.createVersionHistory(AbstractVersionManager.java:181)
        at org.apache.jackrabbit.core.version.VersionManagerImpl$1.run(VersionManagerImpl.java:194)
        at org.apache.jackrabbit.core.version.VersionManagerImpl$DynamicESCFactory.doSourced(VersionManagerImpl.java:526)
        at org.apache.jackrabbit.core.version.VersionManagerImpl.createVersionHistory(VersionManagerImpl.java:191)
        at org.apache.jackrabbit.core.version.XAVersionManager.createVersionHistory(XAVersionManager.java:140)
        at org.apache.jackrabbit.core.ItemImpl.initVersionHistories(ItemImpl.java:754)
        at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1166)
        at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:805)
        ... 166 more
Caused by: org.apache.jackrabbit.core.state.ItemStateException: failed to write node state: da2cd5d1-9776-4dbe-a42b-842b0134dbfb
        at org.apache.jackrabbit.core.state.xml.XMLPersistenceManager.store(XMLPersistenceManager.java:579)
        at org.apache.jackrabbit.core.state.AbstractPersistenceManager.store(AbstractPersistenceManager.java:66)
        at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:574)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:697)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:315)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:291)
        at org.apache.jackrabbit.core.version.AbstractVersionManager.createVersionHistory(AbstractVersionManager.java:174)
        ... 173 more
Caused by: org.apache.jackrabbit.core.fs.FileSystemException: failed to create folder /home/cms/pepsiaccess/jackrabbit/home/version/data/da2c/d5d1/97764dbea42b842b0134dbfb
        at org.apache.jackrabbit.core.fs.local.LocalFileSystem.createFolder(LocalFileSystem.java:208)
        at org.apache.jackrabbit.core.fs.BasedFileSystem.createFolder(BasedFileSystem.java:99)
        at org.apache.jackrabbit.core.fs.FileSystemResource.makeParentDirs(FileSystemResource.java:100)
        at org.apache.jackrabbit.core.state.xml.XMLPersistenceManager.store(XMLPersistenceManager.java:517)
        ... 179 more"
0,"SegmentInfos shouldn't blindly increment version on commitSegmentInfos currently increments version on the assumption that there are always changes.

But, both DirReader and IW are more careful about tracking whether there are changes.  DirReader has hasChanges and IW has changeCount.  I think these classes should notify the SIS when there are in fact changes; this will fix the case Simon hit on fixing LUCENE-2082 when the NRT reader thought there were changes, but in fact there weren't because IW simply committed the exact SIS it already had.
"
0,"New Token filter for adding payloads ""in-stream""This TokenFilter is able to split a token based on a delimiter and use one part as the token and the other part as a payload.  This allows someone to include payloads inline with tokens (presumably setup by a pipeline ahead of time).  An example is apropos.  Given a | delimiter, we could have a stream that looks like:
{quote}The quick|JJ red|JJ fox|NN jumped|VB over the lazy|JJ brown|JJ dogs|NN{quote}

In this case, this would produce tokens and payloads (assuming whitespace tokenization):
Token: the
Payload: null

Token: quick
Payload: JJ

Token: red
Pay: JJ.

and so on.

This patch will also support pluggable encoders for the payloads, so it can convert from the character array to byte arrays as appropriate."
0,"[patch] add toString for NodeImpl and PropertyImpladd toString for NodeImpl and PropertyImpl with new format. see how it is liked, before adding more."
0,"HTTPClient 4.1 auto slash removalI've put the same comment as in the following issue.

https://issues.apache.org/jira/browse/HTTPCLIENT-929?focusedCommentId=13001748#comment-13001748

I am using httpclient 4.1. I had a problem with this fix. In DefaultRequestDirector.rewriteRequestURI method, for non-proxied URI and when it is a absolute URI, it will call the URIUtils.rewriteURI, which then take the ""RawPath"" from an uri and normalize it. So when I pass an uri, for example, http://www.whatever.com/1//3, it will automatically remove the extra slash and become http://www.whatever.com/1/3. I've got a REStful service to accept the uri (/{param1}/{param2}/{param3}) and it takes when there is an empty value past in. Now because of the auto slash removal, the ""3"" value shift left for a position and match to the {param2}. I wouldn't say the above solution is wrong, but I guess it should not change what value that user pass in."
0,"Initial size of ConcurrentCache depends on number of segments (available processors)This causes a build failure on my machine. Tests run into an OOME because the initial memory footprint of a ConcurrentCache on my machine is 8k. Many of the tests keep references to some kind of repository objects (node, session, x-manager), which means ConcurrentCache instances  cannot be garbage collected immediately after a test run.

I think the overall initial size of the cache should be independent of the number of segments. See proposed patch."
0,"o.a.l.messages should be moved to corecontrib/queryParser contains an org.apache.lucene.messages package containing some generallized code that (claims in it's javadocs) is not specific to the queryParser.

If this is truely general purpose code, it should probably be moved out of hte queryParser contrib -- either into it's own contrib, or into the core (it's very small)

*EDIT:* alternate suggestion to rename package to fall under the o.a.l.queryParser namespace retracted due to comments in favor of (eventually) promoting to it's own contrib"
0,"Text Search Syntax Deviates from SpecOriginal JSR 170 EG Email by David B Victor 2005/03/23:

For Query test XPathQueryLevel2Test.java (src\java\org\apache\jackrabbit\test\api\query) in the TCK, method getFullTextStatement() (used by testFullTextSearch()) uses the word ""AND"" in the syntax in its test that is not in the spec (/*[jcrfnContains(""'quick brown' AND -cat"")]...).  Section ""6.6.4.2 contains function"" of v0.16.3, page 100, outlines the EBNF, which does not include the word ""AND"".  Additionally, the paragraphs here go out of their way to explain that AND is implicit.

At this point, I think it would be best to omit ""AND"" from the TCK method and let it test the implicit AND.

------------------------------------------------------------
David Neuscheler Reply 2005/03/24:

thanks for pointing that out.

i think we should probably track all the tck bugs in jackrabbit jira.
http://issues.apache.org/jira/browse/JCR

could you open a bug for that?

this actually is because we used an non-spec compliant query
parser in the RI, so it actually is even a bug in the RI and the TCK.

thanks again.

regards,
david"
0,"Wire log is incomplete if HttpParser detects an errorIf HttpParser detects an error in any of the headers, it throws a ProtocolException

Although the failing header is included in the Exception detail, the headers leading up to the failure are not logged, which makes it hard to debug (and is quite confusing, as the PE does not appear to be related to the data that has been received).

This is because the wire-logging is done in the caller (HttpMethodDirector) which only logs the header if the parse succeeds.

Perhaps the Wire logging should be done at the point where the HttpParser reads the line."
0,"full text search tests use incorrect character for escaping phrasesThe query test cases use single quotes to escape phrases. The grammar in 6.6.5.2 however requires double quotes.
"
0,IndexReader.listCommits should return a List and not an abstract CollectionSpinoff from here: http://www.mail-archive.com/dev@lucene.apache.org/msg07509.html
0,Add init method to CloseableThreadLocalJava ThreadLocal has an init method that allows subclasses to easily instantiate an initial value.  
0,"isValid should be invoked after analyze rather than before it so it can validate the output of analyzeThe Synonym map has a protected method String analyze(String word) designed for custom stemming.

However, before analyze is invoked on a word, boolean isValid(String str) is used to validate the word - which causes the program to discard words that maybe useable by the custom analyze method. 

I think that isValid should be invoked after analyze rather than before it so it can validate the output of analyze and allow implemters to decide what is valid for the overridden analyze method. (In fact, if you look at code snippet below, isValid should really go after the empty string check)

This is a two line change in org.apache.lucene.index.memory.SynonymMap

      /*
       * Part B: ignore phrases (with spaces and hyphens) and
       * non-alphabetic words, and let user customize word (e.g. do some
       * stemming)
       */
      if (!isValid(word)) continue; // ignore
      word = analyze(word);
      if (word == null || word.length() == 0) continue; // ignore"
0,Use java.util.UUIDReplace the use of org.apache.jackrabbit.uuid.UUID with the new java.util.UUID class introduced in Java 5.
0,"Maintain norms in a single file .nrmNon-compound indexes are ~10% faster at indexing, and perform 50% IO activity comparing to compound indexes. But their file descriptors foot print is much higher. 

By maintaining all field norms in a single .nrm file, we can bound the number of files used by non compound indexes, and possibly allow more applications to use this format.

More details on the motivation for this in: http://www.nabble.com/potential-indexing-perormance-improvement-for-compound-index---cut-IO---have-more-files-though-tf2826909.html (in particular http://www.nabble.com/Re%3A-potential-indexing-perormance-improvement-for-compound-index---cut-IO---have-more-files-though-p7910403.html).
"
0,"Add more unit tests on BeanConvertersSome BeanConverters are not yet stable. We have to add more unit tests.  It seems that null values for bean attributes are not well supported. We have to test that for all BeanConverters. 

Here is a good scenario to test : 

Model : Class A contains an attribute ""b"" based on class B. 

Create an instance of A with ""b"" = null
insert A / save
Get instance of A
set the attribute of B
update A /save




"
0,Allow using FST to hold terms data in DocValues.BYTES_*_SORTED
0,"Use the Jackrabbit RMI extensions by default in jackrabbit-webappUsing the Jackrabbit RMI extensions by default in jackrabbit-webapp

Ref :  http://www.nabble.com/Custom-node-types-with-RMI-tf3728625.html

"
0,"Source distribution packaging targets should make a tarball from ""svn export""Instead of picking and choosing which stuff to include from a local working copy, Lucene's dist-src/package-tgz-src target and Solr's package-src target should simply perform ""svn export"" with the same revision and URL as the local working copy."
0,"Use ConcurrentHashMap instead of HashMap wherever thread-safe access is neededConsider using ConcurrentHashMap instead of HashMap for any Maps that are used by multiple threads.

For example SchemeRegistry and AuthSchemeRegistry."
0,"Remove superfluous comment in MMapDirectory.javaSee title, and I prefer my name to be removed from the source code."
0,"Change access to internal maps of HttpState to protected.To be able to serialize the conversational state of a http session access to the internal maps of HttpState is required. Currently they are all ""private"", so subclasses cannot access them. Changing the access to ""protected"" will allow any subclass to access those maps."
0,"Change log level in UserManagerImpl#getAuthorizable(NodeImpl) and UserImporter#handlePropInfoThis is current implementation:

Authorizable getAuthorizable(NodeImpl n) throws RepositoryException {
        Authorizable authorz = null;
        if (n != null) {
            String path = n.getPath();
            if (n.isNodeType(NT_REP_USER) && Text.isDescendant(usersPath, path)) {
                authorz = createUser(n);
            } else if (n.isNodeType(NT_REP_GROUP) && Text.isDescendant(groupsPath, path)) {
                authorz = createGroup(n);
            } else {
                /* else some other node type or outside of the valid user/group
                   hierarchy  -> return null. */
                log.debug(""Unexpected user nodetype "" + n.getPrimaryNodeType().getName());
            }
        } /* else no matching node -> return null */
        return authorz;
    }


It seems that 'else' branch can be improved, at least by increasing log level. But I think, that best way is to throw exception.
Current message can also be misleading, in case when user type is correct but check Text.isDescendant fails.

Above method is called from within UserImporter#handlePropInfo

...
Authorizable a = userManager.getAuthorizable(parent);
if (a == null) {
     log.debug(""Cannot handle protected PropInfo "" + protectedPropInfo + "". Node "" + parent + "" doesn't represent a valid Authorizable."");
     return false;
} 
....

Here again log level is debug. Because at this point we have return statement, property 'principalName' is not set, and if we try to save session following exception will be thrown:

javax.jcr.nodetype.ConstraintViolationException: /home/public/users/b/bb2: mandatory property {internal}password does not exist
     at org.apache.jackrabbit.core.ItemSaveOperation.validateTransientItems(ItemSaveOperation.java:537)
     at org.apache.jackrabbit.core.ItemSaveOperation.perform(ItemSaveOperation.java:216)
     at org.apache.jackrabbit.core.session.SessionState.perform(SessionState.java:200)
     at org.apache.jackrabbit.core.ItemImpl.perform(ItemImpl.java:91)
     at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:329)
    ...
 

So if the log level is not set to 'debug' it is not obvious why mentioned property is missing. Use case and root cause is that 'path' (/home/public/users/b/bb2)  is not descendant of 'usersPath' (/home/users).

Regards,
Miroslav"
0,"the jcr:frozenUuid property is of type REFERENCE instead of STRINGThe spec says that jcr:frozenUuid is a STRING but jackrabbit 1.0.1 uses a REFERENCE for it.
"
0,"NullPointerException when temporary directory not readableWe have customers reporting errors such as:

Caused by: java.lang.NullPointerException
	at org.apache.lucene.store.FSDirectory.create(FSDirectory.java:200)
	at org.apache.lucene.store.FSDirectory.getDirectory(FSDirectory.java:144)
	at org.apache.lucene.store.FSDirectory.getDirectory(FSDirectory.java:117)
	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:205)
	at com.atlassian.jira.util.LuceneUtils.getIndexWriter(LuceneUtils.java:46)
	at com.atlassian.jira.issue.index.DefaultIndexManager.getIndexWriter(DefaultIndexManager.java:568)
	at com.atlassian.jira.issue.index.DefaultIndexManager.indexIssuesAndComments(DefaultIndexManager.java:287)
	... 59 more


This occurs when the lock directory is unreadable (eg. because Tomcat sets java.io.tmpdir to temp/ and the permissions here are broken). Attached is "
0,Cut Norms over to DocValuessince IR is now fully R/O and norms are inside codecs we can cut over to use a IDV impl for writing norms. LUCENE-3606 has some [ideas|https://issues.apache.org/jira/browse/LUCENE-3606?focusedCommentId=13160559&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13160559] about how this could be implemented
0,Support SortedSource in MultiDocValuesMultiDocValues doesn't support Sorted variant ie. SortedSource but throws UnsupportedOperationException. This forces users to work per segment. For consistency we should support sortedsource also if we wrap the DocValues in MDV.
0,"""ant dist"" no longer generates md5's for the top-level artifactsMark hit this for 2.9.0, and I just hit it again for 2.9.1.  It used to work..."
0,"Setup nightly build website links and docsPer discussion on mailing list, we are going to setup a Nightly Build link on the website linking to the docs (and javadocs) generated by the nightly build process.  The build process may need to be modified to complete this task.

Going forward, the main website will, for the most part, only be updated per releases (I imagine exceptions will be made for News items and per committer's discretion).  The Javadocs linked to from the main website will always be for the latest release."
0,"Transfer-Encoding: identity not supported + possible patchIn HttpMethodBase.readResponseBody only chunked transfer encoding is 
supported.  Some proxy servers like Privoxy, etc send a Transfer-Encoding: 
identity header and HttpClient fails quietly and returns a null result input 
stream.  At line 2037 in HttpMethodBase.java revision 1.160 I inserted the 
following two lines and it appeared to work fine:

} else if (""identity"".equalsIgnoreCase(transferEncodingHeader.getValue())) {
   result = is;

I think it should at least throw an exception or do something when it 
encounters an unsupported Transfer-Encoding instead of returning a null input 
stream."
0,Fix javadocs after deprecation removalThere are a lot of @links in Javadocs to methods/classes that no longer exist. javadoc target prints tons of warnings. We should fix that.
0,"Promote solr's PrefixFilter into Java Lucene's coreSolr's PrefixFilter class is not specific to Solr and seems to be of interest to core lucene users (PyLucene in this case).
Promoting it into the Lucene core would be helpful."
0,"Path should not be encoded in HttpMethodBaseI suggest to change the protocol or add a new method for this one

protected static String generateRequestLine(HttpConnection connection,
	String name, String reqPath,
	String qString, String protocol);
so that we can choose to use URIUtil.encode(reqPath, URIUtil.pathSafe()) or not

The reason is that after the encoding process, some server cannot recognize this
Actually, I am handling a project of the Method Propfind(for getting mail from 
Hotmail) and I find that the restriction of Hotmail server is quite high, and 
if the address is encoded, it does not work."
0,"New flexible query parserFrom ""New flexible query parser"" thread by Micheal Busch

in my team at IBM we have used a different query parser than Lucene's in
our products for quite a while. Recently we spent a significant amount
of time in refactoring the code and designing a very generic
architecture, so that this query parser can be easily used for different
products with varying query syntaxes.

This work was originally driven by Andreas Neumann (who, however, left
our team); most of the code was written by Luis Alves, who has been a
bit active in Lucene in the past, and Adriano Campos, who joined our
team at IBM half a year ago. Adriano is Apache committer and PMC member
on the Tuscany project and getting familiar with Lucene now too.

We think this code is much more flexible and extensible than the current
Lucene query parser, and would therefore like to contribute it to
Lucene. I'd like to give a very brief architecture overview here,
Adriano and Luis can then answer more detailed questions as they're much
more familiar with the code than I am.
The goal was it to separate syntax and semantics of a query. E.g. 'a AND
b', '+a +b', 'AND(a,b)' could be different syntaxes for the same query.
We distinguish the semantics of the different query components, e.g.
whether and how to tokenize/lemmatize/normalize the different terms or
which Query objects to create for the terms. We wanted to be able to
write a parser with a new syntax, while reusing the underlying
semantics, as quickly as possible.
In fact, Adriano is currently working on a 100% Lucene-syntax compatible
implementation to make it easy for people who are using Lucene's query
parser to switch.

The query parser has three layers and its core is what we call the
QueryNodeTree. It is a tree that initially represents the syntax of the
original query, e.g. for 'a AND b':
  AND
 /   \
A     B

The three layers are:
1. QueryParser
2. QueryNodeProcessor
3. QueryBuilder

1. The upper layer is the parsing layer which simply transforms the
query text string into a QueryNodeTree. Currently our implementations of
this layer use javacc.
2. The query node processors do most of the work. It is in fact a
configurable chain of processors. Each processors can walk the tree and
modify nodes or even the tree's structure. That makes it possible to
e.g. do query optimization before the query is executed or to tokenize
terms.
3. The third layer is also a configurable chain of builders, which
transform the QueryNodeTree into Lucene Query objects.

Furthermore the query parser uses flexible configuration objects, which
are based on AttributeSource/Attribute. It also uses message classes that
allow to attach resource bundles. This makes it possible to translate
messages, which is an important feature of a query parser.

This design allows us to develop different query syntaxes very quickly.
Adriano wrote the Lucene-compatible syntax in a matter of hours, and the
underlying processors and builders in a few days. We now have a 100%
compatible Lucene query parser, which means the syntax is identical and
all query parser test cases pass on the new one too using a wrapper.


Recent posts show that there is demand for query syntax improvements,
e.g improved range query syntax or operator precedence. There are
already different QP implementations in Lucene+contrib, however I think
we did not keep them all up to date and in sync. This is not too
surprising, because usually when fixes and changes are made to the main
query parser, people don't make the corresponding changes in the contrib
parsers. (I'm guilty here too)
With this new architecture it will be much easier to maintain different
query syntaxes, as the actual code for the first layer is not very much.
All syntaxes would benefit from patches and improvements we make to the
underlying layers, which will make supporting different syntaxes much
more manageable.
"
0,"Releasing a connection is unconfirmedWhen a connection is attempted to be released using
HttpConnection.releaseConnection(), it is unclear whether this is actually done.
The implementation for the method is as follows in 3.0-beta1:

    /**
     * Release the connection.
     */
    public void releaseConnection() {
        LOG.trace(""enter HttpConnection.releaseConnection()"");
        if (locked) {
            LOG.debug(""Connection is locked.  Call to releaseConnection() ignore
        } else if (httpConnectionManager != null) {
            LOG.debug(""Releasing connection back to connection manager."");
            httpConnectionManager.releaseConnection(this);
        } else {
            LOG.warn(""HttpConnectionManager is null.  Connection cannot be relea
        }
    }

Silently ignoring a request (to release the connection, in this case) is hardly
ever the right thing to do, in my opinion. Instead, I suggest the method
indicates whether the connection was actually closed or not.

I see at least 2 alternatives:

1) throw an exception to indicate the connection could not be released;
2) return a flag indicating whether the connection could actually be released."
0,Config: make all elements in the security configuration optionalin order not to introduce new mandatory elements in the security configuration.
0,Improve logging in LazyItemIterator#prefetchNext When LazyItemIterator#prefetchNext fails an item it should spell out the name of that item in  the respective log message. 
0,"Evil up MockDirectoryWrapper.checkIndexOnCloseMockDirectoryWrapper checks any indexes tests create on close(), if they exist.

The problem is the logic it uses to determine if an index exists could mask real bugs (e.g. segments file corrumption):
{code}
if (DirectoryReader.indexExists(this) {
  ...
  // evil stuff like crash()
  ...
  _TestUtil.checkIndex(this)
}
{code}

and for reference DirectoryReader.indexExists is:
{code}
try {
  new SegmentInfos().read(directory);
  return true;
} catch (IOException ioe) {
  return false;
}
{code}

So if there are segments file problems, we just silently do no checkIndex.
"
0,"JcrUtils.getRepository(...) for simple repository accessAs discussed on the mailing list, it would be nice to have a trivially simple way (one line of code) to connect to a repository. The RepositoryFactory interface in JCR 2.0 defines a way for clients to get a repository reference without a direct implementation dependency, but a client still needs extra code to handle the Service Provider lookup and the iteration through all the available repository factories.

To simplify client code I'd like to introduce a JcrUtils.getRepository(Map<String, String>) method that takes care of the tasks mentioned above:

    Map<String, String> parameters = ...; // repository settings
    Repository repository = JcrUtils.getRepository(parameters);

As a further simplification, I'd also like to introduce a JcrUtils.getRepository(String) method that builds the parameter map based on a given ""repository URI"".

    Repository repository = JcrUtils.getRepository(""file:///path/to/repository"");

    Repository repository = JcrUtils.getRepository(""http://localhost:8080/server"");

The set of supported URI types is still to be defined."
0,"Add option to ReverseStringFilter to mark reversed tokensThis patch implements additional functionality in the filter to ""mark"" reversed tokens with a special marker character (Unicode 0001). This is useful when indexing both straight and reversed tokens (e.g. to implement efficient leading wildcards search)."
0,"JavaDoc getConnection methods in Connection ManagersThe JavaDoc for the getConnection() methods in the Simple and MultiThreaded
Connection managers is taken from the interface, and so is too generic.

The Javadoc for the doGetConnection() method in the MultiThreaded manager is
fine, but is not visible in the JavaDoc

The Simple Mangager JavaDoc could likewise be improved

[I hope to provide patches shortly]"
0,"Preserving UUID and document version history on repository migrationI have been working I an migration utility for OpenKM and I performed some changes in jackrabit-core to enable version import, preserving
the modification date. Also modified org.apache.jackrabbit.core.NodeImpl to preserve UUID in the migration process.

This migration process is needed because there are changes in repository node definition, and Jackrabbit can't deal with this actually.

I've attache a PDF with the changes needed in Jackrabbit-core. It works and there was no problems with the migrated repository."
0,"Refactor DBMS support for JNDI datasourcesOur shop currently uses Oracle for most projects, most commonly in an application server (Tomcat, WebSphere, etc.), and use configured J2EE datasources. Unfortunately, many of the classes that fix quirks on specific DBMS force you to configure a JDBC connection (look at org.apache.jackrabbit.core.fs.db.OracleFileSystem for instance), which is a ""bad idea"" on an application server -- the application server should be managing resources like DB connections, etc.  If you want to use an DbFileSystem based on an Oracle database, you can't use a datasource from a JNDI lookup.  This in effect makes Jackrabbit unusable in clustered enterprise environments.

It would be much better to refactor the current database support to separate the method that an implementation obtains its connection from its functionality."
0,JSR 283 Node Identifier
0,Use the remote-resources-plugin to add LICENSE and NOTICE files to binariesSince JCRSITE-13 the remote resources plugin is configured to automatically add LICENSE and NOTICE files to all of our binary artifacts (including -sources and -javadoc jars). We should adapt the configuration so that these files get to include all the correct licensing metadata we currently maintain in src/main/resources/META-INF.
0,Update LICENSE and NOTICE files to match the updated dependenciesWe've made quite a few dependency updates since Jackrabbit 1.6 and need to update the license metadata accordingly.
0,"Update dependency versions for commons-collections, slf4j and derbySome of the dependencies used by the 2.0-beta1 could be upgraded:
commons-collections from 3.1 to 3.2.1
slf4j from 1.5.3 to 1.5.8
derby from 10.2.1.6 to 10.5.3.0

Not sure about derby but the other two seems to be just drop in replacements for their older verisons."
0,"Signature changes in AttributeSource for better Generics support of AddAttribute/getAttributeThe last update of Attribute API using AttributeImpl as implementation oif Attributes changed the API a little bit. This change leads to the fact, that in Java 1.5 using generics we are no longer able to add Attributes without casting. addAttribute and getAttribute should return the Attribute interface because the implementation of the attribute is not interesting to the caller. By that in 1.5 using generics, one could add a TermAttribute without casting using:
{code}
TermAttribute termAtt = addAttribute(TermAttribute.class);
{code}
The signature to do this is:
{code}
public <T extends Attribute> T addAttribute(Class<T>)
{code}

The attached patch applies the mentioned change to the signature (without generic, only returning Attribute). No other code changes are needed, as current code always casts the result to the requested interface. I also added the 1.5 method signature for all these methods to the javadocs.

All tests pass."
0,"Improved background text extractionAs recently discussed on the mailing list (see http://markmail.org/message/syt7lc2guzapt7la), the current approach to text extraction in background threads doesn't work that well especially with the Tika-based extractors that support streamed parsing of many document types.

Also, we currently *all* of the extracted text streams are buffered into Strings before being passed into the Lucene index. It would be good if we could somehow get back to passing just Readers to Lucene."
0,"Omit default BatchReadConfig in Spi2davexRepositoryServiceFactoryi'd like to remove the default batchread configuration created in Spi2davexRepositoryServiceFactory (ll 79) and instead pass 
null if the service configuration doesn't define the batch-read-config.

for test execution e.g. the given default isn't really optimal as sessions only have a short life time and only read
a very limited amount of items (in general)... always reading with depth 4 doesn't add any benefit in this case.
running the level1 jcr tests in jcr2dav (that as far as i saw doesn't define an extra batchread-config took 1.5, 2.5 and 13 minutes
from null-config -> depth2 -> depth4.

if there is a strong reason for keeping that default in the factory we should at least change that for the tests.
michael, what do you think?"
0,"Generify Security APIThe current security api, namely the Authorizable and Group interface use non-generic collections/iterators.
suggest to change this."
0,FieldCache should support longs and doublesWould be nice if FieldCache supported longs and doubles
0,"JCR2SPI does not provide actual size on RangeIterator.getSize()Currently, JCR2SPI always returns -1 on RangeIterator.getSize().

This return value is legal (meaning ""unknown""), but may cause clients to simply iterate through the whole list when what they really want is simply the count.

Use case:

""The use case is to count the number of members of a NT_FOLDER without having to open up the NT_FOLDER and count all the members (and I assume load them into memory) ""

To make this happen we probably need to move away from simple Iterators on the SPI level, and put quite some additional work into JCR2SPI.

Feedback appreciated."
0,"download section of website downmaybe you know this, but http://hc.apache.org/downloads.cgi (the downloads for httpcomponents and for httpclient) is broken (500 internal server error)..."
0,"Cookie.compare(...) uses single instance STRING_COLLATOR to do blocking comparesI am using a MultiThreadedHttpConnectionManager with a single HttpClient instance and multiple GetMethod objects.  I have a 500 thread max.  I recently noticed that all 500 threads are in the same place and seem to be blocking each other - the stack trace is below.  I dug into the Cookie.compare(...) method and saw that it is using STRING_COLLARTOR.compare(c1.getPath(), c2.getPath()).  STRING_COLLATOR is defined as a single instance object, 'private static final RuleBasedCollator STRING_COLLATOR = (RuleBasedCollator) RuleBasedCollator.getInstance(new Locale(""en"", ""US"", """"));'.  I also saw that RuleBasedCollator.compare is synchronized.  That means that every thread that is trying to make a request is getting blocked while it tries to add cookies to the request method.  I do not see a workaround because this is the same static final object in every Cookie instance.  So, the more threads, the more synchronized comparisons.  At times I am fetching URLs all from the same site so I am going through this code a lot.  I need it to be much faster than it currently is because all of my threads are getting eaten up on this call and backlogging my system.  Can a different RuleBasedCollator be used for each compare (use the RuleBasedCollator.getInstance() for every compare?  I think that would solve things.

Name: pool-1-thread-1443: 72.21.206.5
State: BLOCKED on java.text.RuleBasedCollator@190330a owned by: pool-1-thread-1867: 72.21.206.5
Total blocked: 9,598  Total waited: 381

Stack trace: 
java.text.RuleBasedCollator.compare(RuleBasedCollator.java:396)
org.apache.commons.httpclient.Cookie.compare(Cookie.java:484)
org.apache.commons.httpclient.cookie.CookieSpecBase.addInPathOrder(CookieSpecBase.java:578)
org.apache.commons.httpclient.cookie.CookieSpecBase.match(CookieSpecBase.java:557)
org.apache.commons.httpclient.HttpMethodBase.addCookieRequestHeader(HttpMethodBase.java:1179)
org.apache.commons.httpclient.HttpMethodBase.addRequestHeaders(HttpMethodBase.java:1305)
org.apache.commons.httpclient.HttpMethodBase.writeRequestHeaders(HttpMethodBase.java:2036)
org.apache.commons.httpclient.HttpMethodBase.writeRequest(HttpMethodBase.java:1919)
org.apache.commons.httpclient.HttpMethodBase.execute(HttpMethodBase.java:993)
org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:397)
org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:170)
org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:396)
org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:324)"
0,"Move text extraction into a background threadEven though text extraction is not done right on save() most of the extraction work is later done by a client thread. There is a mechanism in place that commits the deferred work in a background thread. But the background thread is only triggered by a timer and does not constantly write back pending index changes. For regular index changes this is done on purpose and should not be changed. However text extraction work should be moved completely into a background thread because it often takes a fair amount of time to index a large document.

Outline of a possible solution:
- all text filtering is tasks are put into a work queue
- the work queue is processed by a background thread
- basic indexing of nt:resource without text filtering takes place
- the background thread updates the index when text filtering completed for a nt:resource

There should be a configuration parameter that allows to execute text filtering without the background thread. This way it is possible to get the existing behaviour of Jackrabbit: the fulltext index is always up-to-date and can be used.
With the background process this is no longer the case."
0,"Database Data Store: clean up the codeThere is some unnecessary code in the DbDataStore that should be removed.
Also, some more tests should be added."
0,changes-to-html: fixes and improvementsThe [Lucene Hudson Changes.html|http://hudson.zones.apache.org/hudson/job/Lucene-trunk/lastSuccessfulBuild/artifact/lucene/build/docs/changes/Changes.html] looks bad because changes2html.pl doesn't properly handle some new usages in CHANGES.txt.
0,"JCRTest.java (First Steps example code): to few parameters in session.importXMLIn the code on the First Steps page:

if (!rn.hasNode(""importxml"")) {
        System.out.println(""importing xml"");
        Node n=rn.addNode(""importxml"", ""nt:unstructured"");
        session.importXML(""/importxml"", new FileInputStream(""repotest/test.xml""));
        session.save();
      }

The importXML needs a third parameter, compare to: 

http://www.day.com/maven/jsr170/javadocs/jcr-1.0/javax/jcr/Session.html

This prevents the code from the First Steps page from compiling."
0,"Improve ArrayUtil/CollectionUtil.*Sort() methods to early-reaturn on empty or one-element lists/arraysIt might be a good idea to make CollectionUtil or ArrayUtil return early if the passed-in list or array's length <= 1 because sorting is unneeded then. This improves maybe automaton or other places, as for empty or one-element lists no SorterTermplate is created."
0,"Incorrect transitive snapshot dependenciesUsing ${version} in dependency declarations causes troubles since for snapshot dependencies the version variable apparently gets replaced by the exact timestamp of a deployed snapshot instead of the ""x.y-SNAPSHOT"" string. Typically the timestamps of different artifacts are not the same, causing broken dependencies."
0,"Enable flexible scoringThis is a first step (nowhere near committable!), implementing the
design iterated to in the recent ""Baby steps towards making Lucene's
scoring more flexible"" java-dev thread.

The idea is (if you turn it on for your Field; it's off by default) to
store full stats in the index, into a new _X.sts file, per doc (X
field) in the index.

And then have FieldSimilarityProvider impls that compute doc's boost
bytes (norms) from these stats.

The patch is able to index the stats, merge them when segments are
merged, and provides an iterator-only API.  It also has starting point
for per-field Sims that use the stats iterator API to compute boost
bytes.  But it's not at all tied into actual searching!  There's still
tons left to do, eg, how does one configure via Field/FieldType which
stats one wants indexed.

All tests pass, and I added one new TestStats unit test.

The stats I record now are:

  - field's boost

  - field's unique term count (a b c a a b --> 3)

  - field's total term count (a b c a a b --> 6)

  - total term count per-term (sum of total term count for all docs
    that have this term)

Still need at least the total term count for each field.
"
0,"Callback for intercepting merging segments in IndexWriterFor things like merging field caches or bitsets, it's useful to
know which segments were merged to create a new segment.

"
0,"org.apache.http.impl.client.cache.memcached.MemcachedHttpCacheStorage uses URL as cache key - shouldn't.Spy memcached has 250 defined as max key length:
http://dustin.github.com/java-memcached-client/apidocs/constant-values.html#net.spy.memcached.MemcachedClientIF.MAX_KEY_LENGTH

URLs can be (and often are) much longer than 250 characters.

URLs should be hashed before being used as keys."
0,"Make ItemIds more stableThe ItemIds returned by spi2dav are currently not stable in the sense that they are sometimes uuid based and sometimes not: If a node is referenceable some of its properties will receive fully path based ids while others will receive ids based on the uuid of its parent node. 

The efficiency of caching introduced with JCR-2498 depends on stable ids. I therefore suggest to improve spi2dav such that property ids are always uuid based if the parent's node has a uuid. "
0,"Default merge policy should take deletions into accountLUCENE-1634 added a calibrateSizeByDeletes; we had a TODO to default this to true as of 3.0 but we missed it.  I'll fix it now for 3.1 and 4.0.  While this is technically a change in back-compat (for 3.x), I think it's fine to make an exception here; this should be a big win for indices that have high doc turnover with time."
0,"provide a (relatively) simple way to disable anonymous access to the security workspaceAs discussed in this thread: http://sling.markmail.org/thread/st52jejjuxykfxtj, the security workspace is, by default, configured with an AccessControlProvider which provides a fixed access control policy (i.e. o.a.j.core.security.user.UserAccessControlProvider). In order to prevent anonymous access to security-related nodes requires the use of an alternate AccessControlProvider.

The attached patch provides a simpler mechanism. By adding

<param name=""anonymousAccessToSecurityWorkspace"" value=""false"" />

to the configuration of the DefaultSecurityManager, anonymous access to the security workspace is forbidden.
"
0,"Coarser granularity of node type unregistration notificationsWhen unregistering multiple node types at a time, the internal notification methods are called separately for each type. This causes some problems as the first notifications triggers the regeneration of the full virtual node type tree, and later notification calls will fail (logging an error) in VirtualNodeTypeStateManager because the removed type is no longer there. A better approach would be to send the names of all the unregistered node types as a collection."
0,"jcr:like() does not scale well on large value rangesThere are two major issues with the current WildcardQuery implementation:

1) A wildcard expression is restricted to match at most 1024 terms, otherwise a TooManyClauses exception is thrown. Similar to the RangeQuery issue: JCR-111
2) The enumeration over the terms that match the wildcard pattern is slow"
0,"repository-1.5.dtd: change order of main elementsCurrently the order of elements in repository.xml is:
<!ELEMENT Repository (FileSystem,Security,Workspaces,Workspace,Versioning,SearchIndex?,Cluster?,DataStore?)>

I would like to change it to
<!ELEMENT Repository (Cluster?,FileSystem,DataStore?,Security,Workspaces,Workspace,Versioning,SearchIndex?)>
because I think that makes more sense.

Currently XML validation is disabled, and therefore the order of elements in the DTD does not need to match the repository.xml file. However as soon as XML validation is enabled, repository.xml files that use the wrong order will no longer work (the repository can not be started).

There is a request to enable XML validation at http://issues.apache.org/jira/browse/JCR-1462
"
0,provide option to automatically dispose idle workspaces
0,"[PATCH] Efficiently retrieve sizes of field valuesSometimes an application would like to know how large a document is before retrieving it.  This can be important for memory management or choosing between algorithms, especially in cases where documents might be very large.

This patch extends the existing FieldSelector mechanism with two new FieldSelectorResults:  SIZE and SIZE_AND_BREAK.  SIZE creates fields on the retrieved document that store field sizes instead of actual values.  SIZE_AND_BREAK is especially efficient if one field comprises the bulk of the document size (e.g., the body field) and can thus be used as a reasonable size approximation.
"
0,"StopFilter should not create a new CharArraySet if the given set is already an instance of CharArraySetWith LUCENE-2094 a new CharArraySet is created no matter what type of set is passed to StopFilter. This does not behave as  documented and could introduce serious performance problems. Yet, according to the javadoc, the instance of CharArraySet should be passed to CharArraySet.copy (which is very fast for CharArraySet instances) instead of ""copied"" via ""new CharArraySet()"""
0,Upgrade to Tika 0.10Apache Tika 0.10 was released some while ago. It contains lots of improvements and fixes for full text extraction.
0,Make it possible to get multiple nodes in one call via davexI'm working on this currently
0,"FastVectorHighlighter truncates words at beginning and end of fragmentsFastVectorHighlighter does not take word boundaries into consideration when building fragments, so that in most cases the first and last word of a fragment are truncated.  This makes the highlights less legible than they should be.  I will attach a patch to BaseFragmentBuilder that resolves this by expanding the start and end boundaries of the fragment to the first whitespace character on either side of the fragment, or the beginning or end of the source text, whichever comes first.  This significantly improves legibility, at the cost of returning a slightly larger number of characters than specified for the fragment size."
0,"Drop deprecations from trunksubj.
Also, to each remaining deprecation add release version when it first appeared.

Patch incoming."
0,IndexWriter.deleteDocuments bugIndexWriter.deleteDocuments() fails random testing
0,"Build problem with StrictSSLProtocolSocketFactoryStrictSSLProtocolSocketFactory requires jcert.jar to be in compile.classpath of
build.xml.  Here is a patch that will fix it:

Index: build.xml
===================================================================
RCS file: /home/cvspublic/jakarta-commons/httpclient/build.xml,v
retrieving revision 1.27
diff -u -r1.27 build.xml
--- build.xml   23 May 2003 02:49:01 -0000      1.27
+++ build.xml   26 May 2003 04:23:50 -0000
@@ -98,6 +98,7 @@
     <pathelement location=""${build.home}/classes""/>
     <pathelement location=""${junit.jar}""/>
     <pathelement location=""${jsse.jar}""/>
+    <pathelement location=""${jcert.jar}""/>
     <pathelement location=""${jce.jar}""/>
     <pathelement location=""${jnet.jar}""/>
     <pathelement location=""${commons-logging.jar}""/>"
0,"Add method to remove mappings from NamespaceMappingo.a.j.spi.commons.namespace.NamespaceMapping has currently no means to remove a mapping. I suggest to add a method
public String removeMapping(String uri) "
0,"SweetSpotSimiliarityThis is a new Similarity implimention for the contrib/miscellaneous/ package, it provides a Similiarty designed for people who know the ""sweetspot"" of their data.  three major pieces of functionality are included:

1) a lengthNorm which creates a ""platuea"" of values.
2) a baseline tf that provides a fixed value for tf's up to a minimum, at which point it becomes a sqrt curve (this is used by the tf(int) function.
3) a hyperbolic tf function which is best explained by graphing the equation.  this isn't used by default, but is available for subclasses to call from their own tf functions.

All constants used in all functions are configurable.  In the case of lengthNorm, the constants are configurable per field, as well as allowing for defaults for unspecified fields."
0,"Cookie class cannot handle IPv6 literalsWhen performing requests using IPv6 literals, Cookie.setDomain() will attempt to trim the port number by cutting off the domain string at the first colon. This leads to MalformedCookieExceptions being thrown by CookieSpecBase later on."
0,"add IndexReader.getUniqueTermCountSimple API to return number of unique terms (across all fields).  Spinoff from here:

http://www.lucidimagination.com/search/document/536b22e017be3e27/term_limit"
0,DisjunctionMaxQuery -  Iterator code to  for ( A  a : container ) constructFor better readability  - converting the Iterable<T> to  for ( A  a : container ) constructs that is more intuitive to read. 
0,"MockDirectoryWrapper should track open file handles of IndexOutput tooMockDirectoryWrapper currently tracks open file handles of IndexInput only. Therefore IO files that are not closed do not fail our tests, which can then lead to test directories fail to delete on Windows. We should make sure all open files are tracked and if they are left open, fail the test. I'll attach a patch shortly."
0,"Improve PhraseQuery.toString()PhraseQuery.toString() is overly simplistic, in that it doesn't correctly show phrases with gaps or overlapping terms. This may be misleading when presenting phrase queries built using complex analyzers and filters."
0,"Failures during contrib builds, when classes in core were changed without ant cleanFrom java-dev by Shai Erera:

{quote}
I've noticed that sometimes, after I run test-core and test-contrib, and then change core code, test-contrib fail on NoSuchMethodError and stuff like that. I've noticed that core.jar exists under build, and I assumed it's used by test-contrib, and probably is not recreated after core code has changed.

I verified it when looking in contrib-build.xml, which defines a property lucene.jar.present which is set to true if the jar is ... well, present. Which I believe is the reason for these failures. I've been thinking how to resolve that, and I can think of two ways:

(1) have test-core always delete that file, but that has two issues:
(1.1) It's redundant if the code hasn't changed.
(1.2) It forces you to either jar-core or test-core before you test-contrib, if you want to make sure you run w/ the latest jar.

or

(2) have test-contrib always call jar-core, which will first delete the file and then re-create it by compiling first. Compiling should not do anything if the code hasn't changed. So the only waste would be to create the .jar, but I think that's quite fast?

Does anyone, with more Ant skills than me, know of a better way to detect from test-contrib that core code has changed and only then rebuild the jar?
{quote}"
0,"Some tests assume that an implementation of javax.jcr.Item overrides equals()The following 3 tests (followed by the line number containing the bad assertion):

org.apache.jackrabbit.test.api.ReferencesTest.testReferenceTarget:135
org.apache.jackrabbit.test.api.ReferencesTest.testAlterReference:169
org.apache.jackrabbit.test.api.version.VersionHistoryTest:152

assume that an implementation of javax.jcr.Item overrides equals(), such that 

Assert.assertEquals(n1, n2) or 
java.util.Set.contains(n1) 

works for two ""equal"" nodes n1,n2 or for some node n1 that has been previously put into a set. However, there is no section in the specification that would mandate this. The tests above should therefore replace assertEquals() with one of the other mechanism that officially supported, such as javax.jcr.Node.isSame().

"
0,Add basic I/O counters to query handlerThere should be a couple of simple counters that track the number of I/O operations that are performed during a query execution. This will help debug query performance issues.
0,"[PATCH] cleanup unwanted stream closing that isn't usedDue to refactoring, a stream is being closed that is never used. Isn't harmful, just is cruft."
0,FastVectorHighlighter: support DisjunctionMaxQueryAdd DisjunctionMaxQuery support in FVH. 
0,"Maven 2 POM includes junit in default ""compile"" scope, rather than ""test"" scopeThe POM at the URL above declares a dependency on JUnit in the default scope, rather than the ""test"" scope."
0,"ClientConnectionRelease example is incorrecthttp://svn.apache.org/repos/asf/httpcomponents/httpclient/tags/4.0.1/httpclient/src/examples/org/apache/http/examples/client/ClientConnectionRelease.java

is incorrect: 

1. if error happens in BufferedReader constructor (OutOfMemoryError, StackOverflowError), reader.close() is not called and connection is not released

2. if error happens in reader.readLine(), reader.close() is called, but httpget.abort() is not."
0,promote TestExternalCodecs.PerFieldCodecWrapper to corePerFieldCodecWrapper lets you set the Codec for each field; I'll promote to core & mark experimental.
0,"Add narrow API for loading stored fields, to replace FieldSelectorI think we should ""invert"" the FieldSelector API, with a ""push"" API
whereby FieldsReader invokes this API once per field in the document
being visited.

Implementations of the API can then do arbitrary things like save away
the field's size, load the field, clone the IndexInput for later lazy
loading, etc.

This very thin API would be a mirror image of the very thin index time
API we now have (IndexableField) and, importantly, it would have no
dependence on our ""user space"" Document/Field/FieldType impl, so apps
are free to do something totally custom.

After we have this, we should build the ""sugar"" API that rebuilds a
Document instance (ie IR.document(int docID)) on top of this new thin
API.  This'll also be a good test that the API is sufficient.

Relevant discussions from IRC this morning at
http://colabti.org/irclogger/irclogger_log/lucene-dev?date=2011-07-13#l76
"
0,"spi2dav Improve performance for large binary propertiesSending large binary properties over spi2dav is slow and requires a lot of heap space in both client and server.
One problematic part is base64 conversion of the property value.

On the contrary, using 'normal' webdav interface (/repository/default/ instead of /server) for uploading a file (through traditional webdav client) it is pretty fast and don't have such impact on heap space.

Some suggestions from the previous discussion:
 - avoid temporary copies of the data, and persist large objects as early as possible. 
 - transfer large objects in blocks from the Jackrabbit SPI client to the server (and back).
 - make usage of the global data store (JCR-926). 
 - straight forward PUT for single-valued properties

Link to discussion: http://www.mail-archive.com/dev@jackrabbit.apache.org/msg09481.html
"
0,"[PATCH] Clear ThreadLocal instances in close()As already found out in LUCENE-436, there seems to be a garbage collection problem with ThreadLocals at certain constellations, resulting in an OutOfMemoryError.
The resolution there was to remove the reference to the ThreadLocal value when calling the close() method of the affected classes (see FieldsReader and TermInfosReader).
For Java < 5.0, this can effectively be done by calling threadLocal.set(null); for Java >= 5.0, we would call threadLocal.remove()

Analogously, this should be done in *any* class which creates ThreadLocal values

Right now, two classes of the core API make use of ThreadLocals, but do not properly remove their references to the ThreadLocal value
1. org.apache.lucene.index.SegmentReader
2. org.apache.lucene.analysis.Analyzer

For SegmentReader, I have attached a simple patch.
For Analyzer, there currently is no patch because Analyzer does not provide a close() method (future to-do?)

"
0,"cache module should strip 'Content-Encoding: identity' from responsesPer the RFC, the ""identity"" content coding SHOULD NOT be used in the Content-Encoding header:

http://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.5

The current implementation will pass 'Content-Encoding: identity' through unchanged, although it would be simple enough to filter this out.
"
0,"CompoundFileReader's openInput produces streams that may do an extra buffer copySpinoff of LUCENE-888.

The class for reading from a compound file (CompoundFileReader) has a
primary stream which is a BufferedIndexInput when that stream is from
an FSDirectory (which is the norm).  That is one layer of buffering.

Then, when its openInput is called, a CSIndexInput is created which
also subclasses from BufferedIndexInput.  That's a second layer of
buffering.

When a consumer actually uses that CSIndexInput to read, and a call to
readByte or readBytes runs out of what's in the first buffer, it will
go to refill its buffer.  But that refill calls the first
BufferedIndexInput which in turn may refill its buffer (a double
copy) by reading the underlying stream.

Not sure how to fix it yet but we should change things to not do the
extra buffer copy.
"
0,"When using QueryWrapperFilter with CachingWrapperFilter, QueryWrapperFilter returns a DocIdSet that creates a Scorer, which gets cached rather than a bit setthere is a large performance cost to this.

The old impl for this type of thing, QueryFilter, recommends :

@deprecated use a CachingWrapperFilter with QueryWrapperFilter

The deprecated QueryFilter itself also suffers from the problem because its now implemented using a CachingWrapperFilter and QueryWrapperFilter.

see http://search.lucidimagination.com/search/document/7f54715f14b8b7a/lucene_2_9_0rc4_slower_than_2_4_1"
0,"Remove commons-collections and slf4j-api dependencies from jcr-commonsAs noted in JCR-1615 and discussed on the mailing list [1] it would be good if jackrabbit-jcr-commons didn't come with extra dependencies beyond the standard Java class libraries and the JCR API.

Currently jackrabbit-jcr-commons depends on both commons-collections and slf4j-api, but both dependencies are relatively isolated and could be dropped with relatively little effort. Both dependency changes may be backwards incompatible with existing clients, but since the impact is reasonably small and easy to resolve I'd be OK doing this in 1.5.

[1] http://markmail.org/message/724ruk4l7b5rjtan"
0,"Expose registered node types below /jcr:system/jcr:nodeTypesspec says:

6.8 System Node
[...]
For example, if a repository exposes node type definitions in content, then those node type definitions should be located at /jcr:system/jcr:nodeTypes.
"
0,"Completeness/Freshness of NamespaceRegistry and NodeTypeRegistryWe need to define the requirements on completeness and freshness of RepositoryService.getRegisteredNamespaces().

Right now the optimistic assumption seems to be that an SPI provider is able to report all namespaces that can occur in a repository beforehand. Even if it can do that (and I know of potential targets for SPI that simply can't), this seems to be quite a waste of time if these namespace prefixes aren't actually used later on.

Furthermore, in SPI namespace prefixes aren't really relevant, except to enable the transient layer to return ""meaningful"" prefixes instead of automatically generated ones.

Therefore my propoal would be to:

1) Clarify that the Map returned from getRegisteredNamespaces() isn't required to be complete,

2) Enhance JCR2SPI to auto-generate prefixes when it encounters namespaces not in the registry.

I expect this to also affect RepositoryService.(un)registerNamespace(...), but let's discuss the underlying issue first...

"
0,"add IndexWriter.removeUnferencedFiles, so apps can more immediately delete index files when readers are closedThis has come up several times on the user's list.

On Windows, which prevents deletion of still-open files, IndexWriter cannot remove files that are in-use by open IndexReaders.  This is fine, and IndexWriter periodically retries the delete, but it doesn't retry very often (only on open, on flushing a new segment, and on committing a merge).  So it lacks immediacy.

With this expert method, apps that want faster deletion can call this method."
0,A faster JFlex-based replacement for StandardAnalyzerJFlex (http://www.jflex.de/) can be used to generate a faster (up to several times) replacement for StandardAnalyzer. Will add a patch and a simple benchmark code in a while.
0,"add concurrent merge policyProvide the ability to handle merges in one or more concurrent threads, i.e., concurrent with other IndexWriter operations.

I'm factoring the code from LUCENE-847 for this."
0,"Add FieldCache.getTermBytes, to load term data as byte[]With flex, a term is now an opaque byte[] (typically, utf8 encoded unicode string, but not necessarily), so we need to push this up the search stack.

FieldCache now has getStrings and getStringIndex; we need corresponding methods to load terms as native byte[], since in general they may not be representable as String.  This should be quite a bit more RAM efficient too, for US ascii content since each character would then use 1 byte not 2."
0,"JSR 283: adopt CND syntax changesthe CND syntax has changed from Public Review Draft to Public Final Draft.

old and new syntax are incompatible."
0,"Add Payload retrieval to SpansIt will be nice to have access to payloads when doing SpanQuerys.

See http://www.gossamer-threads.com/lists/lucene/java-dev/52270 and http://www.gossamer-threads.com/lists/lucene/java-dev/51134

Current API, added to Spans.java is below.  I will try to post a patch as soon as I can figure out how to make it work for unordered spans (I believe I have all the other cases working).

{noformat}
 /**
   * Returns the payload data for the current span.
   * This is invalid until {@link #next()} is called for
   * the first time.
   * This method must not be called more than once after each call
   * of {@link #next()}. However, payloads are loaded lazily,
   * so if the payload data for the current position is not needed,
   * this method may not be called at all for performance reasons.<br>
   * <br>
   * <p><font color=""#FF0000"">
   * WARNING: The status of the <b>Payloads</b> feature is experimental.
   * The APIs introduced here might change in the future and will not be
   * supported anymore in such a case.</font>
   *
   * @return a List of byte arrays containing the data of this payload
   * @throws IOException
   */
  // TODO: Remove warning after API has been finalized
  List/*<byte[]>*/ getPayload() throws IOException;

  /**
   * Checks if a payload can be loaded at this position.
   * <p/>
   * Payloads can only be loaded once per call to
   * {@link #next()}.
   * <p/>
   * <p><font color=""#FF0000"">
   * WARNING: The status of the <b>Payloads</b> feature is experimental.
   * The APIs introduced here might change in the future and will not be
   * supported anymore in such a case.</font>
   *
   * @return true if there is a payload available at this position that can be loaded
   */
  // TODO: Remove warning after API has been finalized
  public boolean isPayloadAvailable();
{noformat}"
0,"Speed up Top-K sampling testsspeed up the top-k sampling tests (but make sure they are thorough on nightly etc still)

usually we would do this with use of atLeast(), but these tests are somewhat tricky,
so maybe a different approach is needed."
0,Text.escapeIllegalJCRChars should be adjusted to match the 2.0 set of illegal charsText.escapeIllegalJCRChars still contains chars that were illegal in JCR 1.0 but were removed from the set for JCR 2.0
0,"Small imprecision in Search package JavadocsSearch package Javadocs states that Scorer#score(Collector) will be abstract in Lucene 3.0, which is not accurate anymore."
0,Avoid String.intern() when indexingLucene 3.0 now allows to create Fields with a String name that is already interned. We should use the new constructor in NodeIndexer to avoid unnecessary String.intern() calls. The field names Jackrabbit uses are available in FieldNames and already interned.
0,"[PATCH] don't delete all files in index directory on index creationMany people use Lucene to index a part of their file system. The chance that  
you some day mix up index directory and document directory isn't that bad.  
Currently Lucene will delete *all* files in the index directory when the  
create paramater passed to IndexWriter is true, thus deleting your documents 
if you mixed up the parameters. I'll attach a patch that fixes  
this. Any objections?"
0,"Minimize calls to PersistenceManagerIn some situations the PersistenceManager is called even though it is not necessary.

E.g. when new items are created the method NodeImpl.getOrCreateProperty() will always check if there is an already existing property state. If the node is new the call will always go down the full item state stack and ask the PersistenceManager if it knows the property id. This is unnessessary because there will never exist properties in the persistence manager for a new node that has not been saved yet.

I propose to add a check to the method to see if  the node is new and does not yet have a property with the given name. In that case the property can be created without further checks.

With the patch applied the time to transiently create 1000 nodes with 4 properties each drops from 1485 ms to 422 ms."
0,"JCR2SPI: add JNDI supportadding jndi support to jcr2spi was one of the improvements that came up during the f2f.
julian volunteered to take a look at it."
0,"Optimize copies between IndexInput and OutputWe've created an optimized copy of files from Directory to Directory. We've also optimized copyBytes recently. However, we're missing the opposite side of the copy - from IndexInput to Output. I'd like to mimic the FileChannel API by having copyTo on IndexInput and copyFrom on IndexOutput. That way, both sides can optimize the copy process, depending on the type of the IndexInput/Output that they need to copy to/from.

FSIndexInput/Output can use FileChannel if the two are FS types. RAMInput/OutputStream can copy to/from the buffers directly, w/o going through intermediate ones. Actually, for RAMIn/Out this might be a big win, because it doesn't care about the type of IndexInput/Output given - it just needs to copy to its buffer directly.

If we do this, I think we can consolidate all Dir.copy() impls down to one (in Directory), and rely on the In/Out ones to do the optimized copy. Plus, it will enable someone to do optimized copies between In/Out outside the scope of Directory.

If this somehow turns out to be impossible, or won't make sense, then I'd like to optimize RAMDirectory.copy(Dir, src, dest) to not use an intermediate buffer."
0,Remove the unneeded cqfs dependenciesThere's still unneeded dependencies to the cqfs libraries in jcr-server/webapp and jca.
0,"Incorrect error message in AnalyzingQueryParser.getPrefixQueryThe error message of  getPrefixQuery is incorrect when tokens were added, for example by a stemmer. The message is ""token was consumed"" even if tokens were added.
Attached is a patch, which when applied gives a better description of what actually happened."
0,"add ElisionsFilter to ItalianAnalyzerwe set this up for french by default, but we don't for italian.
we should enable it with the standard italian contractions (e.g. definite articles).

the various stemmers for these languages assume this is already being taken care of
and don't do anything about it... in general things like snowball assume really dumb
tokenization, that you will split on the word-internal ', and they add these to stoplists."
0,Stack traces from failed tests are messed up on ANT 1.7.x
0,"ParallelReader is now atomic, rename to ParallelAtomicReader and also add a ParallelCompositeReader (that requires LogDocMergePolicy to have identical subreader structure)The plan is:
- Move all subreaders to ctor (builder-like API. First build reader-set, then call build)
- Rename ParallelReader to ParallelAtomicReader
- Add a ParallelCompositeReader with same builder API, but taking any CompositeReader-set and checks them that they are aligned (docStarts identical). The subreaders are ParallelAtomicReaders."
0,"NTLM implementation lacks support for NTLMv1, NTLMv2, and NTLM2 Session forms of NTLMThe current HttpClient implementation lacks support for all enhancements to NTLM after Windows 95.  That includes NTLMv1, NTLMv2, and NTLM2 Session Response varieties of the protocol.

This seriously impacts the usability of HttpClient in enterprise situations, which has required the Lucene Connector Framework team to extend HttpClient to address the issue.

I've attached a patch which contains the implementation used by LCF.
"
0,"UAX29URLEmailTokenizer fails to recognize emails as such when the mailto: scheme is prependedAs [reported by Kai Glzau on solr-user|http://markmail.org/message/n32kji3okqm2c5qn]:

UAX29URLEmailTokenizer seems to split at the wrong place:

{noformat}mailto:test@example.org{noformat} ->
{noformat}mailto:test{noformat}
{noformat}example.org{noformat}

As a workaround I use

{code:xml}
<charFilter class=""solr.PatternReplaceCharFilterFactory"" pattern=""mailto:"" replacement=""mailto: ""/>
{code}"
0,JE Directory ImplementationI've created a port of DbDirectory to JE
0,"Support all of unicode in StandardTokenizerStandardTokenizer currently only supports the BMP.

If it encounters characters outside of the BMP, it just discards them... 
it should instead implement fully implement UAX#29 across all of unicode."
0,"[HttpClient] Authenticator() - ability to perform alternate authenticationMy post to the user group.  The developer replied suggesting I enter an 
enhancement request.

-----Original Message-----
From: Gustafson, Vicki [mailto:vicki.gustafson@us.didata.com]
Sent: Thursday, 12 December 2002 5:03 AM
To: Jakarta Commons Users List
Subject: [HttpClient] Authentication using Basic

Is there a way to specify which authentication scheme you would like the client 
to use if several schemes are returned in the www-auth header?

I'm performing a simple post using the httpClient.  The server returns a 401 at 
which point the httpClient tries to authenticate with the server.  The 
following header is received:

Attempting to parse authenticate header: 'WWW-Authenticate: Negotiate, NTLM, 
Basic realm=""XXXwhateverXXX""

I need to authenticate using Basic, but the Authenticator class will only try 
the most secure scheme:  NTLM.  Is there a setting or parameter I can set to 
force the httpClient to use Basic?

thanks,
Vicki

// determine the most secure request header to add
Header requestHeader = null;
if (challengeMap.containsKey(""ntlm"")) {
    String challenge = (String) challengeMap.get(""ntlm"");
    requestHeader = Authenticator.ntlm(challenge, method, state,
    responseHeader);
} else if (challengeMap.containsKey(""digest"")) {
    String challenge = (String) challengeMap.get(""digest"");
    String realm = parseRealmFromChallenge(challenge);
    requestHeader = Authenticator.digest(realm, method, state,
    responseHeader);
} else if (challengeMap.containsKey(""basic"")) {
    String challenge = (String) challengeMap.get(""basic"");
    String realm = parseRealmFromChallenge(challenge);
    requestHeader = Authenticator.basic(realm, state, responseHeader);
} else if (challengeMap.size() == 0) {
    throw new HttpException(""No authentication scheme found in '""
    + authenticateHeader + ""'"");
} else {
    throw new UnsupportedOperationException(
    ""Requested authentication scheme "" + challengeMap.keySet()
    + "" is unsupported"");
}

--
To unsubscribe, e-mail:   <mailto:commons-user-unsubscribe@jakarta.apache.org>
For additional commands, e-mail: <mailto:commons-user-help@jakarta.apache.org>


--
To unsubscribe, e-mail:   <mailto:commons-user-unsubscribe@jakarta.apache.org>
For additional commands, e-mail: <mailto:commons-user-help@jakarta.apache.org>

**********developer response**********************************



Currently there isn't, however we probably should be more intelligent about 
falling back to other authentication schemes based on the type of credentials 
provided.  Having said this I'm not sure it conforms to the HTTP spec strictly 
(which states that the client must use the strongest authentication scheme it 
supports, there's a grey area here because if your application doesn't provide 
a dialog or similar for the user to enter NTLM credentials it can only support 
basic or digest authentication, despite HTTPClient supporting NTLM).

What I'd like to see happen is:

When NTLM authentication is requested as top priority but only 
UsernamePasswordCredentials are available instead of NTLMCredentials we fall 
back to one of the other schemes.  In general this would mean that:

if an authentication scheme is requested and a credentials object of the wrong 
type is provided, HTTPClient should assume (probably optionally or only in non-
strict mode) that the requested authentication scheme is not supported and fall 
through to other options.

Achieving this would require a reasonably amount of refactoring of the 
Authenticator class but shouldn't be impossible.  Unfortunately I don't have 
time to do it myself at the moment but I'd be happy to help out if you felt 
like doing it, otherwise logging an enhancement bug in Bugzilla would be a good 
way to record this request until someone has time to actually implement it.

Adrian Sutton, Software Engineer
Ephox Corporation
www.ephox.com"
0,"Improve parallel testsAs mentioned on the dev@ mailing list here: http://www.lucidimagination.com/search/document/93432a677917b9bd/lucenejunitresultformatter_sometimes_fails_to_lock

It would be useful to not create a lockfactory for each test suite (As they are run sequentially in the same separate JVM).
Additionally, we create a lot of JVMs (26) for each batch, because we have to run one for each letter.
Instead, we use a technique here to divide up the tests with a custom selector: http://blog.code-cop.org/2009/09/parallel-junit.html
(I emailed the blog author and received permission to use this code)

This gives a nice boost to the speed of overall tests, especially Solr tests, as many start with an ""S"", but this is no longer a problem.

"
0,"Allow to override LockManager creationCurrently, Repository.getLockManager() internally creates a new lock manager if needed.

Jackrabbit should provide an extension point so that a JCR repository that extends it can create a custom lock manager."
0,"Impl toString() in MergePolicy and its extensionsThese can be important to see for debugging.

We lost them in the cutover to IWC.

Just opening this issue to remind us to get them back, before releasing..."
0,"Allow storing user data when IndexWriter.commit() is calledSpinoff from here:

    http://www.mail-archive.com/java-user@lucene.apache.org/msg22303.html

The idea is to allow optionally passing an opaque String commitUserData to the IndexWriter.commit method.  This String would be stored in the segments_N file, and would be retrievable by an IndexReader.  Applications could then use this to assign meaning to each commit.

It would be nice to get this done for 2.4, but I don't think we should hold the release for it."
0,"HttpClient OSGi Export-Package doesn't specify versionThe ""Export-Package"" manifest entry doesn't specify the version of the package being exported.  This means that packages importing it can't specify a version to import."
0,Improve javadocs for Numeric*I'm working on improving Numeric* javadocs.
0,"Add a variable-sized int block codecWe already have support for fixed block size int block codecs, making it very simple to create a codec from a int encoder algorithms like FOR/PFOR.

But algorithms like Simple9/16 are not fixed -- they encode a variable number of adjacent ints at once, depending on the specific values of those ints."
0,"Various improvment to Path and PathImplThere are various issues with Path and PathImpl which the following patch addresses:
- Fixed problem with normalization of some paths in PathImpl. 
- Fixed handling of relative paths in PathImpl. 
- Fixed wrong return value for depth and ancestor count in PathImpl. 
- Added method for determining equivalence of paths in PathImpl.
- Fixed subPath method in PathImpl. 
- Clarified blurry contract for Path.
- Added many new test cases

For many of the fixes credits are due to Angela."
0,Support for JNDI configuration of BundleDbPersistenceManagerIt would be nice to have the option to configure BundleDbPersistenceManager database specifying a JNDI name of a DataSource.
0,"Adding DerbyDataStore to handle proper close of the embedded databaseWhen using embedded Derby in conjunction with DbDataStore, the Derby database is never shutdown, as it requires special code to be executed (creating a Connection with "";shutdown=true"")
We may provide a DerbyDataStore extending standard DbDataStore for handling that."
0,"remove deprecated classes from spatialspatial has not been released, so we can remove the deprecated classes"
0,"DbDataStore: delete temporary files using finalize()Currently, reading from the DbDataStore creates a temporary file by default. If the application doesn't fully read or close the input stream, the file is not deleted. The best solution is to use finally { in.close() } in the application, but this is easily forgotten.

I suggest to delete the temp file using finalize(). There is a small performance penalty when creating the temporary object, but compared to I/O it is very small. Note that FileInputStream and FileOutputStream also use finalize()."
0,"openReaderPassed not populated in CheckIndex.Status.SegmentInfoStatusWhen using CheckIndex programatically, the openReaderPassed flag on the SegmentInfoStatus is never populated (so it always comes back false)

looking at the code, its clear that openReaderPassed is defined, but never used

furthermore, it appears that not all information that is propagated to the ""InfoStream"" is available via SegmentIinfoStatus

All of the following information should be able to be gather from public properties on the SegmentInfoStatus:
test: open reader.........OK
test: fields, norms.......OK [2 fields]
test: terms, freq, prox...OK [101 terms; 133 terms/docs pairs; 133 tokens]
test: stored fields.......OK [100 total field count; avg 1 fields per doc]
test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
"
0,"Workspace{Copy|Move}VersionableTest assumptions on versioningThese test cases assume that an ancestor of a versioned node can be made versioned. This may not be true for all JCR compliant stores.

There should be a way to skip the test when it can not be executed.

One obvious approach would be to throw a NotExecutableException when the attempt to enable versioning on the parent fails. However this has the drawback that it can mask configuration errors.

Thoughts?
"
0,"lucenetestcase ease of use improvementsI started working on this in LUCENE-2658, here is the finished patch.

There are some problems with LuceneTestCase:
* a tests beforeClass, or the test itself (its @befores and its method), might have some
  random behavior, but only the latter can be reproduced with -Dtests.seed
* if you want to do things in beforeClass, you have to use a different API: newDirectory(random)
  instead of newDirectory, etc.
* for a new user, the current output can be verbose, confusing and overwhelming.

So, I refactored this class to address these problems. 
A class still needs 2 seeds internally, as the beforeClass will only run once, 
but the methods or setUp() might run many times, especially when increasing iterations.

but lucenetestcase deals with this, and the ""seed"" is 128-bit (UUID): 
the MSB is initialized in beforeClass, the LSB varied for each method run.
if you provide a seed with a -D, they are both fixed to the UUID you provided.

I fixed the API to be consistent, so you should be able to migrate a test from 
setUp() to beforeClass() [junit3 to junit4] without changing parameters.

The codec, locale, timezone is only printed once at the end if any tests fail, 
as its per-class anyway (setup in beforeClass)

finally, when a test fails, you get a single ""reproduce with"" command line you can copy and paste to reproduce.
this way you dont have to spend time trying to figure out what the command line should be.

{noformat}
    [junit] Tests run: 2, Failures: 2, Errors: 0, Time elapsed: 0.197 sec
    [junit]
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestExample -Dtestmethod=testMethodA 
              -Dtests.seed=a51e707b-6550-7800-9f8c-72622d14bf5f
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestExample -Dtestmethod=testMethodB 
              -Dtests.seed=a51e707b-6550-7800-f7eb-2efca3820738
    [junit] NOTE: test params are: codec=PreFlex, locale=ar_LY, timezone=Etc/UCT
    [junit] ------------- ---------------- ---------------
    [junit] Test org.apache.lucene.util.TestExample FAILED
{noformat}
"
0,"move intblock/sep codecs into testThe intblock and sep codecs in core exist to make it easy for people to try different low-level algos for encoding ints.

Sep breaks docs, freqs, pos, skip data, payloads into 5 separate files (vs 2 files that standard codec uses).

Intblock further enables the docs, freqs, pos files to encode fixed-sized blocks of ints at a time.

So an app can easily ""subclass"" these codecs, using their own int encoder.

But these codecs are now concrete, and they use dummy low-level block int encoder (eg encoding 128 ints as separate vints).

I'd like to change these to be abstract, and move these dummy codecs into test.

The tests would still test these dummy codecs, by rotating them in randomly for all tests.

I'd also like to rename IntBlock -> FixedIntBlock, because I'm trying to get a VariableIntBlock working well (for int encoders like Simple9, Simple16, whose block size varies depending on the particular values).
"
0,"add TCK test for Info map of NODE_MOVED event on node reorderingadd the TCK test for this problem, and mark this as known test failure for now"
0,"jUnit test-cases: success of some DocumentViewImportTest tests depends on Xerxes version being usedif e.g. xerxes v. 2.4.0 is used instead of v. 2.6.2 as specified in project.xml certain tests in DocumentViewImportTest fail.

e.g. 

Testcase: testWorkspaceImportXml(org.apache.jackrabbit.test.api.DocumentViewImportTest):	FAILED
Xml text is not correctly stored. expected:<......> but was:<...
       ...>
junit.framework.ComparisonFailure: Xml text is not correctly stored. expected:<......> but was:<...
       ...>
	at org.apache.jackrabbit.test.api.DocumentViewImportTest.checkXmlTextNode(DocumentViewImportTest.java:240)
	at org.apache.jackrabbit.test.api.DocumentViewImportTest.checkImportSimpleXMLTree(DocumentViewImportTest.java:174)
	at org.apache.jackrabbit.test.api.DocumentViewImportTest.performTests(DocumentViewImportTest.java:143)
	at org.apache.jackrabbit.test.api.DocumentViewImportTest.doTestImportXML(DocumentViewImportTest.java:115)
	at org.apache.jackrabbit.test.api.DocumentViewImportTest.testWorkspaceImportXml(DocumentViewImportTest.java:73)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at org.apache.jackrabbit.test.AbstractJCRTest.run(AbstractJCRTest.java:401)
"
0,"ProtocolSocketFactory equals and hashCode don't support subclassingIn the implemenation of equals and hashCode for the classes
org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory
org.apache.commons.httpclient.protocol.SSLProtocolSocketFactory

The implementation of equals and hashCode attempts to make all instances of the classes equal.  However, the manner in which the methods are coded makes it necessary for any subclass to implement equals and hashCode themselves.  A minor change to the methods in these classes will make possible to subclass these factories without re-implementing the equals and hashCode.  The method equals should be written as

        return ((obj != null) && obj.getClass().equals(getClass()));

rather than

        return ((obj != null) && obj.getClass().equals(DefaultProtocolSocketFactory.class));

And similarly, the hashCode method should be

        return getClass().hashCode();

rather than

        return DefaultProtocolSocketFactory.class.hashCode();"
0,"move SmartChineseAnalyzer into the smartcn packagean offshoot of LUCENE-1862, org.apache.lucene.analysis.cn.SmartChineseAnalyzer should become org.apache.lucene.analysis.cn.smartcn.SmartChineseAnalyzer"
0,"queryparser makes all CJK queries phrase queries regardless of analyzerThe queryparser automatically makes *ALL* CJK, Thai, Lao, Myanmar, Tibetan, ... queries into phrase queries, even though you didn't ask for one, and there isn't a way to turn this off.

This completely breaks lucene for these languages, as it treats all queries like 'grep'.

Example: if you query for f:abcd with standardanalyzer, where a,b,c,d are chinese characters, you get a phrasequery of ""a b c d"". if you use cjk analyzer, its no better, its a phrasequery of  ""ab bc cd"", and if you use smartchinese analyzer, you get a phrasequery like ""ab cd"". But the user didn't ask for one, and they cannot turn it off.

The reason is that the code to form phrase queries is not internationally appropriate and assumes whitespace tokenization. If more than one token comes out of whitespace delimited text, its automatically a phrase query no matter what.

The proposed patch fixes the core queryparser (with all backwards compat kept) to only form phrase queries when the double quote operator is used. 

Implementing subclasses can always extend the QP and auto-generate whatever kind of queries they want that might completely break search for languages they don't care about, but core general-purpose QPs should be language independent.
"
0,Configurable SimilarityThe similarity implementation for indexing and searching should be configurable.
0,"nightly builds depend on cloveras reported by Michael Pelz Sherman on java-dev@lucene and solr-user@lucene the nightly builds coming out of hudson current depend on clover...

  [root@crm.test.bbhmedia.net tmp]# strings lucene-core-nightly.jar | grep -i clover|more
org/apache/lucene/LucenePackage$__CLOVER_0_0.class
org/apache/lucene/analysis/Analyzer$__CLOVER_1_0.class
...

the old nightly.sh dealt with this by running ant nightly twice, first without clover to get the jars and then with clover to get the report.  it loks like maybe this logic never made it into the hudson setup.

someone with hudson admin access/knowledge will need to look into this."
0,"Refactoring of the Persistence Managerscurrently the persistence managers reside in:
 org.apache.jackrabbit.core.state
 org.apache.jackrabbit.core.state.db
 org.apache.jackrabbit.core.state.mem
 org.apache.jackrabbit.core.state.obj
 org.apache.jackrabbit.core.state.xml
 (org.apache.jackrabbit.core.state.util)

there are also a lot of other classes that deal with states (eg:
SharedItemStateManager) in the state package that do not relate to
pms.

i would like to move all persistencemanagers and pm related stuff to:

 org.apache.jackrabbit.core.persistence

I'd keep the current classes as deprecated subclasses within
jackrabbit-core.jar until Jackrabbit 2.0. There may (?) be people who
are extending the existing classes, so I'd avoid breaking binary
compatibility there even though we've never promised to actually honor
compatiblity within o.a.j.core."
0,"Filter to process output of ICUTokenizer and create overlapping bigrams for CJK The ICUTokenizer produces unigrams for CJK. We would like to use the ICUTokenizer but have overlapping bigrams created for CJK as in the CJK Analyzer.  This filter would take the output of the ICUtokenizer, read the ScriptAttribute and for selected scripts (Han, Kana), would produce overlapping bigrams."
0,Generate jar containing test classes.The test classes are useful for writing unit tests for code external to the Lucene project. It would be helpful to build a jar of these classes and publish them as a maven dependency.
0,"Avoid element arrays in PathImplThe path handling code in spi-commons shows quite often in thread dumps and profiling results, as the current implementation does quite a bit of repetitive allocating and copying of path element arrays. We should be able to streamline and simplify the path handling code by only tracking the latest path element and a reference to the parent path. To do this efficiently we may need to adjust some of the Path and PathFactory method declarations (that currently assume element array -based paths) also in the SPI.
"
0,"allow automatontermsenum to work on full byte rangeAutomatonTermsEnum is really agnostic to whats in your byte[], only that its in binary order.
so if you wanted to use this on some non-utf8 terms, thats just fine.

the patch just does some code cleanup and removes ""utf8"" references, etc.
additionally i changed the pkg-private, lucene-internal byte-oriented ctor, to public, lucene.experimental.
"
0,"Ability to ignore (reject) cookies altogetherI was looking for a way to ignore cookies altogether, but there doesn't appear
to be one.  I could definitely use this capability right now, and I can see
others making use of it at times."
0,"Plug-in authentication modulesCurrently only basic authentication is supported.  A Authentication interface
should be provided to allow for plug-in support for other authenticaiton
schemes, some of which may be application specific and therefore have no place
in httpclient itself, but would be required by some users."
0,"Move document type definition out of repository.xmlHello!

Here at Cognifide, Przemo and I we got a bit confused while trying to solve JCR-202. There was a need to modify repository.xml configuration file and it's DTD, and we have found that there are different repository.xml files within trunk that differs this definition. I think that it is a good idea to extract this definition to a one separate file (and maybe .xsd instead of .dtd) and then link it in other files. It would be also nice to put this file somewhere on the Web and reference it via URL.

I am waiting for your comments.

Regards, Jan"
0,Update the Highlighter to use the new TokenStream API
0,"Demo HTML parser gives incorrect summaries when title is repeated as a headingIf you have an html document where the title is repeated as a heading at the top of the document, the HTMLParser will return the title as the summary, ignoring everything else that was added to the summary. Instead, it should keep the rest of the summary and chop off the title part at the beginning (essentially the opposite). I don't see any benefit to repeating the title in the summary for any case.

In HTMLParser.jj's getSummary():

    String sum = summary.toString().trim();
    String tit = getTitle();
    if (sum.startsWith(tit) || sum.equals(""""))
      return tit;
    else
      return sum;

change it to: (* denotes a line that has changed)

    String sum = summary.toString().trim();
    String tit = getTitle();
*    if (sum.startsWith(tit))             // don't repeat title in summary
*      return sum.substring(tit.length()).trim();
    else
      return sum;
"
0,"Index Writer constructor flags unclear - and annoying in certain casesWouldn't it make more sense if the constructor for the IndexWriter always
created an index if it doesn't exist - and the boolean parameter should be
""clear"" (instead of ""create"")

So instead of this (from javadoc):

IndexWriter

public IndexWriter(Directory d,
                   Analyzer a,
                   boolean create)
            throws IOException

    Constructs an IndexWriter for the index in d. Text will be analyzed with a.
If create is true, then a new, empty index will be created in d, replacing the
index already there, if any.

Parameters:
    d - the index directory
    a - the analyzer to use
    create - true to create the index or overwrite the existing one; false to
append to the existing index 
Throws:
    IOException - if the directory cannot be read/written to, or if it does not
exist, and create is false


We would have this:

IndexWriter

public IndexWriter(Directory d,
                   Analyzer a,
                   boolean clear)
            throws IOException

    Constructs an IndexWriter for the index in d. Text will be analyzed with a.
If clear is true, and a index exists at location d, then it will be erased, and
a new, empty index will be created in d.

Parameters:
    d - the index directory
    a - the analyzer to use
    clear - true to overwrite the existing one; false to append to the existing
index 
Throws:
    IOException - if the directory cannot be read/written to, or if it does not
exist.



Its current behavior is kind of annoying, because I have an app that should
never clear an existing index, it should always append.  So I want create set to
false.  But when I am starting a brand new index, then I have to change the
create flag to keep it from throwing an exception...  I guess for now I will
have to write code to check if a index actually has content yet, and if it
doesn't, change the flag on the fly."
0,"IndexWriter commits unnecessarily on fresh DirectoryI've noticed IndexWriter's ctor commits a first commit (empty one) if a fresh Directory is passed, w/ OpenMode.CREATE or CREATE_OR_APPEND. This seems unnecessarily, and kind of brings back an autoCommit mode, in a strange way ... why do we need that commit? Do we really expect people to open an IndexReader on an empty Directory which they just passed to an IW w/ create=true? If they want, they can simply call commit() right away on the IW they created.

I ran into this when writing a test which committed N times, then compared the number of commits (via IndexReader.listCommits) and was surprised to see N+1 commits.

Tried to change doCommit to false in IW ctor, but it got IndexFileDeleter jumping on me .. so the change might not be that simple. But I think it's manageable, so I'll try to attack it (and IFD specifically !) back :)."
0,Add join query to LuceneSolr has (psuedo) join query for a while now. I think this should also be available in Lucene.  
0,"Support for boost factor in MoreLikeThisThis is a patch I made to be able to boost the terms with a specific factor beside the relevancy returned by MoreLikeThis. This is helpful when having more then 1 MoreLikeThis in the query, so words in the field A (i.e. Title) can be boosted more than words in the field B (i.e. Description)."
0,"Add contrib libs to classpath for javadocI don't know Ant well enough to just do this easily, so I've labeled a wish - would be nice to get rid of all the errors/warnings that not finding these classes generates when building javadoc."
0,"Better integration of the TestWebApp-HowTo into the documentationThe excellent webapp howto written by Olegolas needs to be integrated better
into httpclient documentation.  Currently it is in the docs directory as a html
file, but it would be better if it was in the xdocs directory as an xml file."
0,"Consider making HostConfiguration immutableHostConfiguration class should be immutable. This should also allow methods of this class to be non-synchronized.

Oleg"
0,SPI: prefer 'Iterator' instead of specialized subclassesin the F2F we agreed that the SPI should rather use 'Iterator' instead of specialized subclassed (or RangeIterator).
0,Speedup of CharArraySet#copy if a CharArraySet instance is passed to copy.the copy method should use the entries array itself to copy the set internally instead of iterating over all values. this would speedup copying even small set 
0,spi2dav: create RepositoryFactory implementation
0,"smartcn HHMM doc translationMy coworker Patricia Peng translated the documentation and code comments for smartcn HHMM package.
"
0,"waitForResponse is using busy waitIn HttpConnection, the method waitForResponse is using busywait, instead of 
blocking until the response is arriving.

Is this on purpose, or shouldn't it handle this by blocking instead ??"
0,"Database persistence managers: log database and driver name and versionDatabase related problems can be solved more easily when we know what database and driver version is used. Sometimes multiple database drivers are installed in an app server environment, and the user may not even know it. 

Currently the driver class name is logged. I suggest to log the driver and database name and version as well."
0,Remove QueryResultImpl and rename LazyQueryResultImpl to QueryResultImplQueryResultImpl isn't used in Jackrabbit anymore. Instead LazyQueryResultImpl is now used. See the discussion in JCR-1073.
0,"Default retry count three even if documentation says it's fiveThe exception handling documentation (http://hc.apache.org/httpclient-3.x/exception-handling.html) says ""HttpClient will automatically retry up to 5 times those methods..."", but in DefaultHttpMethodRetryHandler  e.g. in trunk (http://svn.apache.org/viewvc/httpcomponents/oac.hc3x/trunk/src/java/org/apache/commons/httpclient/DefaultHttpMethodRetryHandler.java?revision=608014&view=markup) you can see that the retry count is three:

    public DefaultHttpMethodRetryHandler(int retryCount, boolean requestSentRetryEnabled) {
        super();
        this.retryCount = retryCount;
        this.requestSentRetryEnabled = requestSentRetryEnabled;
    }
    
    /**
     * Creates a new DefaultHttpMethodRetryHandler that retries up to 3 times
     * but does not retry methods that have successfully sent their requests.
     */
    public DefaultHttpMethodRetryHandler() {
        this(3, false);
    }"
0,"Introduce a temprary cache for intermediate query resultsSometimes queries execute the same sub query multiple times.

e.g. //element(*, nt:resource)[@jcr:mimeType != 'text/plain' and @jcr:mimeType != 'text/html']

will result in two internal MatchAllQuery on jcr:mimeType. The query should re-use the previously calculated results when possible."
0,"Pass ClientConnectionManager to DefaultHttpClient constructorCopied from my mailing list post, Oleg suggested I post it to JIRA for 4.1 fix.

I'm trying to find the least verbose way of configuring a DefaultHttpClient with a ThreadSafeClientConnManager.

The example code given for this goes through a manual process of configuring HttpParams and SchemeRegistry objects, which is more or less copied from the DefaultHttpClient.createHttpParams() and createClientConnectionManager() methods.

It's a bit of a chicken and egg situation - DefaultHttpClient can create its own HttpParams and SchemeRegistry, which are themselves fine, but only once its been constructed, and the constructor requires the ThreadSafeClientConnManager, but that in turn requires the HttpParams and SchemeRegistry objects.  The only way out is to manually construct the HttpParams and SchemeRegistry, which is a waste.

It seems to me that DefaultHttpClient's constructor should take a ClientConnectionManagerFactory instead of a ClientConnectionManager. That way, the createClientConnectionManager() method already has the factory reference, and doesn't have to grub around in the HttpParams object to find it.

The code would then become:

new DefaultHttpClient(new ThreadSafeClientConnManagerFactory(), null);

where ThreadSafeClientConnManagerFactory.newInstance() just constructs ThreadSafeClientConnManager.  There's no manual construction of HttpParams and SchemeRegistry, you just leave it up to DefaultHttpClient.
"
0,"Spatial uses java util logging that causes needless minor work (multiple string concat, a method call) due to not checking log levelNot sure there should be logging here - just used in two spots and looks more for debug - but if its going to be there, should check for isFineEnabled."
0,Using explain may double ram reqs for fieldcaches when using ValueSourceQuery/CustomScoreQuery or for ConstantScoreQuerys that use a caching Filter.
0,"A tokenfilter to decompose compound wordsA tokenfilter to decompose compound words you find in many germanic languages (like German, Swedish, ...) into single tokens.

An example: Donaudampfschiff would be decomposed to Donau, dampf, schiff so that you can find the word even when you only enter ""Schiff"".

I use the hyphenation code from the Apache XML project FOP (http://xmlgraphics.apache.org/fop/) to do the first step of decomposition. Currently I use the FOP jars directly. I only use a handful of classes from the FOP project.

My question now:
Would it be OK to copy this classes over to the Lucene project (renaming the packages of course) or should I stick with the dependency to the FOP jars? The FOP code uses the ASF V2 license as well.

What do you think?"
0,"TCK: SetPropertyAssumeTypeTest doesn't allow ValueFormatException upon type conversion failureSetPropertyAssumeTypeTest# testValuesConstraintVioloationExceptionBecauseOfInvalidTypeParameter

This test should allow an implementation to throw ValueFormatException.  In Section 7.1.5, the Javadoc for setProperty(String, Value[] int) states: ""If the property type of the supplied Value objects is different from that specified, then a best-effort conversion is attempted. If the conversion fails, a ValueFormatException is thrown.""

Proposal: catch and consume ValueFormatException.

--- SetPropertyAssumeTypeTest.java      (revision 422074)
+++ SetPropertyAssumeTypeTest.java      (working copy)
@@ -28,6 +28,7 @@
 import javax.jcr.PropertyType;
 import javax.jcr.RepositoryException;
 import javax.jcr.Property;
+import javax.jcr.ValueFormatException;
 import java.util.Calendar;
 import java.util.Date;
  
@@ -525,6 +526,9 @@
         catch (ConstraintViolationException e) {
             // success
         }
+        catch (ValueFormatException e) {
+            // success
+        }
     }
"
0,"remove RoutedRequest from APIRemove RoutedRequest from the Client API. It can be moved to impl, or dropped altogether.
HttpClient could accept separate request and target arguments instead of RoutedRequest.
No routes should be passed in the API. "
0,"InternalVersion.getFrozenNode confused about root version?It seems the javadoc for InternalVersion.getFrozenNode  is confused:

     * Returns the frozen node of this version or <code>null</code> if this is
     * the root version.

AFAIU, the frozen node of the root version is always present to capture the node type of the versionable node.

Does anybody recall how this got here? (SVN says it has been there forever)"
0,"Add toString() or getName() method to IndexReaderIt would be very useful for debugging if IndexReader either had a getName() method, or a toString() implementation that would get a string identification for the reader.

for SegmentReader, this would return the same as getSegmentName()
for Directory readers, this would return the ""generation id""?
for MultiReader, this could return something like ""multi(sub reader name, sub reader name, sub reader name, ...)

right now, i have to check instanceof for SegmentReader, then call getSegmentName(), and for all other IndexReader types, i would have to do something like get the IndexCommit and get the generation off it (and this may throw UnsupportedOperationException, at which point i have would have to recursively walk sub readers and try again)

I could work up a patch if others like this idea"
0,"Genericize DirectIOLinuxDir -> UnixDirToday DirectIOLinuxDir is tricky/dangerous to use, because you only want to use it for indexWriter and not IndexReader (searching).  It's a trap.

But, once we do LUCENE-2793, we can make it fully general purpose because then a single native Dir impl can be used.

I'd also like to make it generic to other Unices, if we can, so that it becomes UnixDirectory."
0,separate java code from .jj fileIt would make development easier to move most of the java code out from the .jj file and into a real java file.
0,Fixes a handful of misspellings/mistakes in changes.txtThere are a handful of misspellings/mistakes in changes.txt. This patch fixes them. Avoided the one or two British to English conversions <g>
0,"Optimizations to TopScoreDocCollector and TopFieldCollectorThis is a spin-off of LUCENE-1575 and proposes to optimize TSDC and TFC code to remove unnecessary checks. The plan is:
# Ensure that IndexSearcher returns segements in increasing doc Id order, instead of numDocs().
# Change TSDC and TFC's code to not use the doc id as a tie breaker. New docs will always have larger ids and therefore cannot compete.
# Pre-populate HitQueue with sentinel values in TSDC (score = Float.NEG_INF) and remove the check if reusableSD == null.
# Also move to use ""changing top"" and then call adjustTop(), in case we update the queue.
# some methods in Sort explicitly add SortField.FIELD_DOC as a ""tie breaker"" for the last SortField. But, doing so should not be necessary (since we already break ties by docID), and is in fact less efficient (once the above optimization is in).
# Investigate PQ - can we deprecate insert() and have only insertWithOverflow()? Add a addDummyObjects method which will populate the queue without ""arranging"" it, just store the objects in the array (this can be used to pre-populate sentinel values)?

I will post a patch as well as some perf measurements as soon as I have them."
0,"jira notificationsI have set up Jackrabbit's jira so that notifications will
go to the dev mailing list.  If that gets annoying, I can easily
switch it to a different list or turn off notifications such that
people have to register as watchers.

Let me know what is best for you.

....Roy
"
0,"Automatic staging of the non-Maven release artefactsCurrently the Jackrabbit release process includes the following manual steps in addition to the standard Maven release plugin invocations:

<script>
VERSION=x.y.z  # Release version number

# Prepare the release directory
mkdir target/$VERSION

# Copy the main release artifacts created in the release:perform stage
cp target/checkout/RELEASE-NOTES.txt target/$VERSION
cp target/checkout/target/jackrabbit-$VERSION-src.zip* target/$VERSION
cp target/checkout/jackrabbit-webapp/target/jackrabbit-webapp-$VERSION.war* target/$VERSION
cp target/checkout/jackrabbit-jca/target/jackrabbit-jca-$VERSION.rar* target/$VERSION
cp target/checkout/jackrabbit-standalone/target/jackrabbit-standalone-$VERSION.jar* target/$VERSION

# Add MD5 and SHA1 checksums
for BINARY in target/$VERSION/*.zip target/$VERSION/*ar; do
  openssl md5 < $BINARY > $BINARY.md5
  openssl sha1 < $BINARY > $BINARY.sha
done

# Upload the release directory to people.apache.org
scp -r target/$VERSION people.apache.org:public_html/jackrabbit/$VERSION
</script>

I'd like to automate these steps."
0,"contrib/javascript is not packaged into releasesthe contrib/javascript directory is (apparently) a collection of javascript utilities for lucene .. but it has not build files or any mechanism to package it, so it is excluded form releases.

"
0,"Core Test should not have dependencies on the Demo codeThe TestDoc.java Test file has a dependency on the Demo FileDocument code.  Some of us don't keep the Demo code around after downloading, so this breaks the build.

Patch will be along shortly"
0,"SerializationTest and AbstractImportXmlTest leak temporary filesBoth test classes leak temporary files when setUp() fails.
"
0,"Data store garbage collection: ScanEventListener not workingThe ScanEventListener is currently only called when using the 'scan all nodes recursively' strategy. It is not called when all persistence managers implement IterablePersistenceManager (GarbageCollector.scanPersistenceManagers). The ScanEventListener should be called in every case, otherwise it is not possible to see the progress of the garbage collection.

However there is a problem: IterablePersistenceManager.getAllNodeIds() doesn't return Node objects, and it would make little sense to create real node objects (the performance advantage of scanPersistenceManagers would be lost).

Therefore, I propose a workaround: the ScanEventListener is called using a 'PseudoNode'. This is a class that implements Node but only has meaningful getUUID() and toString() methods. This allows to create a meaningful progress bar (as the UUIDs are returned in order)."
0,"test cases relying on Node.equals()Several test cases rely on Node.equals to compare nodes, where instead isSame() should be used:

org.apache.jackrabbit.test.api.NodeTest.testNodeIdentity(NodeTest.java:751)
org.apache.jackrabbit.test.api.NodeTest.testNodeIdentity(NodeTest.java:753)
org.apache.jackrabbit.test.api.version.VersionHistoryTest.testInitallyGetAllVersionsContainsTheRootVersion(VersionHistoryTest.java:126)
org.apache.jackrabbit.test.api.version.VersionHistoryTest.testGetVersion(VersionHistoryTest.java:180)
org.apache.jackrabbit.test.api.version.CheckinTest.testMultipleCheckinHasNoEffect(CheckinTest.java:93)
org.apache.jackrabbit.test.api.version.VersionGraphTest.testInitialBaseVersionPointsToRootVersion(VersionGraphTest.java:47)
org.apache.jackrabbit.test.api.version.RemoveVersionTest.testRemoveVersionAdjustPredecessorSet(RemoveVersionTest.java:120)
org.apache.jackrabbit.test.api.version.RemoveVersionTest.testRemoveVersionAdjustSucessorSet(RemoveVersionTest.java:144)

 "
0,"Rewrite TrieRange to use MultiTermQueryIssue for discussion here: http://www.lucidimagination.com/search/document/46a548a79ae9c809/move_trierange_to_core_module_and_integration_issues

This patch is a rewrite of TrieRange using MultiTermQuery like all other core queries. This should make TrieRange identical in functionality to core range queries."
0,"Change visibility of getComparator method in SortField from protected to publicHi,

Currently I'm using SortField for the creation of FieldComparators, but I ran into an issue.
I cannot invoke SortField.getComparator(...) directly from my code, which forces me to use a  workaround. (subclass SortField and override the getComparator method with visiblity public)
I'm proposing to make this method public. Currently I do not see any problems changing the visibility to public, I do not know if there are any (and the reason why this method is currently protected)
I think that this is a cleaner solution then the workaround I used and also other developers can benefit from it. I will also attach a patch to this issue based on the code in the trunk (26th of May). place). 
Please let me know your thoughts about this.

Cheers,

Martijn

 "
0,SQL Azure support: clustered indexesWe tried to install JackRabbit in the Windows Azure cloud using SQL Azure. One of the limitations of SQL Azure is that it needs clustered indexes to work but the current implementation of the JackRabbit creates the indexes not clustered.
0,"Typo in API_CHANGES_3_0.txtDigestSheme should be DigestScheme :)  Also, why is there not an httpclient
component in the list?"
0,"Add workaround for ICU bug in combination with Java7 to LuceneTestCaseThere is a bug in ICU that makes it fail to load it ULocale class in Java7: http://bugs.icu-project.org/trac/ticket/8734

The problem is caused by some new locales in Java 7, that lead to a chicken-and-egg problem in the static initializer of ULocale. It initializes its default locale from the JDK locale in a static ctor. Until the default ULocale instance is created, the default is not set in ULocale. But ULocales ctor itsself needs the default locale to fetch some ressource bundles and throws NPE.

The code in LuceneTestCase that randomizes the default locale should classload ULocale before it tries to set another random locale, using a defined, safe locale (Locale.US). Patch is easy."
0,"Allow controllable printing of the hitsAdds ""print.hits.field"" property to the alg.  If set, then the hits retrieved by Search* tasks are printed, along with the value of the specified field, for each doc."
0,"Upgrade to SLF4J 1.3Version 1.1 of the SLF4J logging facade was recently released. It contains no functional improvements that we'd need, but is split to a separate slf4j-api library and a set of backend-specic logging adapters. This would allow us to avoid exposing log4j as a transitive dependency for projects that depend on Jackrabbit."
0,"Deploy JCA JAR file to maven repositoryPlease deploy the JCA JAR file to the maven repository (ibiblio) whenever deploying the RAR artifact.

The JAR is need for non managed usage of the Jackrabbit JCA, eg. for embedding the resource adapter in your application with Spring JCA in order to use XA for Jackrabbit.

It would be nice if this could be done starting at the current 1.3 version (and for future versions, too).

Thanks!"
0,cipher 
0,"Add SimpleFragListBuilder constructor with margin parameter{{SimpleFragListBuilder}} would benefit from an additional constructor that takes in {{margin}}. Currently, the margin is defined as a constant, so to ""implement"" a {{FragListBuilder}} with a different margin, one has no choice but to copy and paste {{SimpleFragListBuilder}} into a new class that must be placed in the {{org.apache.lucene.search.vectorhighlight}} package due to accesses of package-protected fields in other classes.

If this change were made, the precondition check of the constructor's {{fragCharSize}} should probably be altered to ensure that it's less than {{max(1, margin*3)}} to allow for a margin of 0."
0,"SQL2 parser: Support CASTSome CAST(...) data conversions are not yet implemented, for example String to Decimal."
0,"Update copyright years in READMEs and NOTICEsThe README.txt files of Jackrabbit components contain copyright lines like this:

    Collective work: Copyright 2007 The Apache Software Foundation.

The year should be updated."
0,"QValueFactory improvements1) Allow all create methods to throw RepositoryException.

2) Further document that create(value,type) can throw ValueFormatException.

3) Remove special case create(File)
"
0,Create docvalues based grouped facet collectorCreate docvalues based grouped facet collector. Currently only term based collectors have been implemented (LUCENE-3802).
0,"Windows specific implementation of the Digest auth schemeMicrosoft Windows 2003 implementation of digest auth scheme is essentially a
superset of RFC 2617 with Windows specific aspects:
http://www.microsoft.com/technet/prodtechnol/windowsserver2003/library/TechRef/717b450c-f4a0-4cc9-86f4-cc0633aae5f9.mspx

Provide a super class of DigestScheme with Windows 2003 specific extensions,
which can be plugged in instead of the standard Digest impl

For details see PR #34909"
0,"MultiReader.norm() takes up too much memory: norms byte[] should be made into an ObjectMultiReader.norms() is very inefficient: it has to construct a byte array that's as long as all the documents in every segment.  This doubles the memory requirement for scoring MultiReaders vs. Segment Readers.  Although this is cached, it's still a baseline of memory that is unnecessary.

The problem is that the Normalization Factors are passed around as a byte[].  If it were instead replaced with an Object, you could perform a whole host of optimizations
a.  When reading, you wouldn't have to construct a ""fakeNorms"" array of all 1.0fs.  You could instead return a singleton object that would just return 1.0f.
b.  MultiReader could use an object that could delegate to NormFactors of the subreaders
c.  You could write an implementation that could use mmap to access the norm factors.  Or if the index isn't long lived, you could use an implementation that reads directly from the disk.

The patch provided here replaces the use of byte[] with a new abstract class called NormFactors.  
NormFactors has two methods on it
    public abstract byte getByte(int doc) throws IOException;  // Returns the byte[doc]
    public float getFactor(int doc) throws IOException;            // Calls Similarity.decodeNorm(getByte(doc))

There are four implementations of this abstract class
1.  NormFactors.EmptyNormFactors - This replaces the fakeNorms with a singleton that only returns 1.0
2.  NormFactors.ByteNormFactors - Converts a byte[] to a NormFactors for backwards compatibility in constructors.
3.  MultiNormFactors - Multiplexes the NormFactors in MultiReader to prevent the need to construct the gigantic norms array.
4.  SegmentReader.Norm - Same class, but now extends NormFactors to provide the same access.

In addition, Many of the Query and Scorer classes were changes to pass around NormFactors instead of byte[], and to call getFactor() instead of using the byte[].  I have kept around IndexReader.norms(String) for backwards compatibiltiy, but marked it as deprecated.  I believe that the use of ByteNormFactors in IndexReader.getNormFactors() will keep backward compatibility with other IndexReader implementations, but I don't know how to test that.
"
0,"Cache jcr name to QName mappingsCurrently jcr names are always parsed and resolved into QName instances. Introducing a cache would increase performance and also save memory because well known and often used jcr names would always return the same QName instance from cache.

Testing with common read operations shows a performance improvement of about 25%.
The test involved the following methods on Node interface:

- getProperty()
- getProperties()
- getName()
- getPath()
- isLocked()
- isNodeType()
- getPrimaryNodeType()
- hasNodes()
- getNodes()

Attached proposed implementation of a QNameResolver.

Please comment."
0,"[PATCH] better exception messages when generating schemaWhen a statement fails to execute generating the schema, patch outputs the statement that failed."
0,"User definable default headers supportProvide the ability to set default headers to be sent on every request.  Should
be used whenever an object is created or recycled.  Needs to be user
configurable."
0,"[OCM] Add unit tests with BundleDbPersistenceManagerUntil now, we have not yet check the ocm framework with the BundleDbPersistenceManager"
0,"refactor HttpClientConnection and HttpProxyConnectionInstead of trying to define a full abstraction for client connections, let's define only a minimal interface in HttpCore with only those methods actually needed in the core. In particular, the core does not need to open connections (since HTTPCORE-11), and it does not care whether a connection is direct or through a proxy. An abstraction for client connections can be defined in HttpConn.

(original description:)
As discussed on the mailing list, separating the responsibility for establishing connections from the connection objects could improve the design and help with proxy support.
"
0,Change all contrib TokenStreams/Filters to use the new TokenStream APINow that we have the new TokenStream API (LUCENE-1422) we should change all contrib modules to use it.
0,"LocalTestServer and supporting classes should be available as a separate jarLocalTestServer and it's supporting classes are useful to anyone who wants to easily ""mock""/test simple http calls without having to embed a full jetty or something.
It would be awesome if these were available in a separate http-localtestserver.jar that could be used in projects outside of httpclient."
0,"textfilters module patch: Support for text extraction for HTML,XML and RTF filesThis patch adds text extraction support form XML, RTF and HTML files.

The unique dependency is htmlparser library for handling HTML text extraction."
0,"Improve InfoStream class in trunk to be more consistent with logging-frameworks like slf4j/log4j/commons-loggingFollowup on a [thread by Shai Erea on java-dev@lao|http://lucene.472066.n3.nabble.com/IndexWriter-infoStream-is-final-td3537485.html]: I already discussed with Robert about that, that there is one thing missing. Currently the IW only checks if the infoStream!=null and then passes the message to the method, and that *may* ignore it. For your requirement it is the case that this is enabled or disabled dynamically. Unfortunately if the construction of the message is heavy, then this wastes resources.

I would like to add another method to this class: abstract boolean isEnabled() that can also be implemented. I would then replace all null checks in IW by this method. The default config in IW would be changed to use a NoOutputInfoStream that returns false here and ignores the message.

A simple logger wrapper for e.g. log4j / slf4j then could look like (ignoring component, could be enabled):

{code:java}
Loger log = YourLoggingFramework.getLogger(IndexWriter.class);

public void message(String component, String message) {
  log.debug(component + "": "" + message);
}

public boolean isEnabled(String component) {
  return log.isDebugEnabled();
}
{code}

Using this you could enable/disable logging live by e.g. the log4j management console of your app server by enabling/disabling IndexWriter.class logging.

The changes are really simple:
- PrintStreamInfoStream returns true, always, mabye make it dynamically enable/disable to allow Shai's request
- infoStream.getDefault() is never null and can never be set to null. Instead the default is a singleton NoOutputInfoStream that returns false of isEnabled(component).
- All null checks on infoStream should be replaced by infoStream.isEanbled(component), this is possible as always != null. There are no slowdowns by this - it's like Collections.emptyList() instead stupid null checks."
0,"Lucene's nightly Hudson builds don't have svn version in MANIFEST.MFSolr had the same issue but apparently made a configuration change to the Hudson configuration to get it working:

    https://issues.apache.org/jira/browse/SOLR-684

Also I opened this INFRA issue:

    https://issues.apache.org/jira/browse/INFRA-1721

which says the svnversion exe is located in this path:

    /opt/subversion-current/bin

In that INRA issue, /etc/init.d/tomcat was also fixed in theory so that svnversion would be on the path the next time Hudson is restarted.  Still, in case that doesn't work, or it changes in the future, it seems a good idea to make the same change that Solr made to Lucene's Hudson configuration.

Hoss can you detail what you needed to do for Solr?  Or maybe just do it also for Lucene ;)  Thanks!"
0,"Remove verbosity from tests and make configureableThe parent issue added the functionality to LuceneTestCase(J4), this patch applies it to most tests."
0,"move deletes under codecAfter LUCENE-3631, this should be easier I think.

I haven't looked at it much myself but i'll play around a bit, but at a glance:
* SegmentReader to have Bits liveDocs instead of BitVector
* address the TODO in the IW-using ctors so that SegmentReader doesn't take a parent but just an existing core.
* we need some type of minimal ""MutableBits"" or similar subinterface of bits. BitVector and maybe Fixed/OpenBitSet could implement it
* BitVector becomes an impl detail and moves to codec (maybe we have a shared base class and split the 3.x/4.x up rather than the conditional backwards)
* I think the invertAll should not be used by IndexWriter, instead we define the codec interface to say ""give me a new MutableBits, by default all are set"" ?
* redundant internally-consistent checks in checkLiveCounts should be done in the codec impl instead of in SegmentReader.
* plain text impl in SimpleText."
0,"encoding of GermanAnalyzer.java and GermanStemmer.java isn't utf-8For PyLucene, the gcj/swig - based python integration of java lucene, it would
be good if java source files didn't use encodings other than utf-8.
On Windows - and systems without iconv support in general - compiling code  
with gcj where the java source text is in another encoding than utf-8 is    
difficult if not impossible.

To change the encoding on these files:

 iconv -f iso-8859-1 -t utf-8 GermanAnalyzer.java > GermanAnalyzer.java.utf-8
 iconv -f iso-8859-1 -t utf-8 GermanStemmer.java > GermanStemmer.java.utf-8"
0,"DocValues.type() -> DocValues.getType()This makes the method easier to find and more clear that it has no side effects... on a
few occasions I've looked for this getter and missed it because of the name.

"
0,"throw exception for fieldcache on a non-atomic readerIn Lucene 4.0, we go through a lot of effort to prevent slow uses of non-atomic readers:

DirectoryReader/MultiReader etc throw exception if you don't try to access postings or docvalues apis per-segment, etc.

But the biggest trap of all is still too easy to fall into, we don't do the same for FieldCache.

I think we should throw exception, forcing the user to either change their code or use a SlowMultiReaderWrapper.
"
0,"Some enhancements to jackrabbit commonsI would like to suggest a couple of  enhancements to the commons module. 

The patch was created against rev. 417443 and the tests did not reveal any 
problems.

Summary of suggestion modifications:

QName
-------------------------------------------------------------------------------------------------------------------------
- reduce QName to its core functionality and put conversion from and to JCR name to
  a separate class 'NameFormat'
- in order not to break existing code, all methods that deal with the conversion in QName
  are marked deprecated.
- add constant for the name of the root node.

Path
-------------------------------------------------------------------------------------------------------------------------
- reduce Path to its core functionality and put conversion from and to JCR path to
  a separate class 'PathFormat'
- in order not to break existing code, all methods that deal with the conversion in Path
  are marked deprecated.
- introduce new constants for UNDEFINED_INDEX (0) and DEFAULT_INDEX (1), that
   are currently hardcoded throughout  the jackrabbit project.
- new method Path.getElement(int) [PathElement]
- make PathElement constants public (used by PathFormat)

Path.PathBuilder
-------------------------------------------------------------------------------------------------------------------------
- additional constructor  PathBuilder(Path)

Path.PathElement
-------------------------------------------------------------------------------------------------------------------------
- add PathElement.getNormalizedIndex() that always asserts a 1-based index.
- change subclasses to be private (no usage within the jackrabbit, except inside Path).

PathMap
-------------------------------------------------------------------------------------------------------------------------
- move o.a.j.core.PathMap  to o.a.j.util.PathMap in order to make it available in the
  commons module.

NamespaceResolver
-------------------------------------------------------------------------------------------------------------------------
- add methods for resolution of paths:
   > getQPath(String jcrPath) [Path]
   > getJCRPath(Path qPath) [String]

NamespaceListener
-------------------------------------------------------------------------------------------------------------------------
- add method 'namespaceRemove(String)'

ValueHelper
-------------------------------------------------------------------------------------------------------------------------
currently  JCR value objects are 'manually' created in the ValueHelper despite the
fact, that JSR170 defines a ValueFactory interface. Consequently the ValueHelper
present in the commons module can only be used by implementations that use
the same value implementations.

- add new helper methods that take a ValueFactory as argument.
- in order not to break existing code the original methods are marked deprecated and
  may be removed at a later time.

consequently:
- modify signature of  InternalValue.create that include a value conversion to take a
  ValueFactory param and adjust all usages inside the core package.

ValueFactoryImpl
-------------------------------------------------------------------------------------------------------------------------
- createValue(String, int ): used to call the conversion on ValueHelper. with the 
   changes suggested to ValueHelper, the code must be changed in order to
   created instances of the Value implementations within the factory.
- together with the modification to ValueHelper, stefan suggested to replace the public 
  constructor with a static 'getInstance' method. All usages within jackrabbit.core, were
   modified accordingly.

Text
-------------------------------------------------------------------------------------------------------------------------
- add getName(String, boolean) where the boolean flag indicates whether  a trailing slash 
   should be ignored.
- add getRelativeParent(String, int, boolean) where the boolean flag indicates whether  a 
  trailing slash should be ignored."
0,"jcr-server: make auth-header configurable for JCR-ServerIn WEB-INF/web.xml, there is a section (commented-out by default) that reads:
        <init-param>
            <param-name>authenticate-header</param-name>
            <param-value>Basic realm=""Jackrabbit Webdav Server""</param-value>
            <description>
                Defines the value of the 'WWW-Authenticate' header.
            </description>
        </init-param>

This parameter is ignored (not loaded) by the code - the default string is always used instead
Note: this was more of a problem before JCR-286 had been fixed
"
0,DatabasePersistenceManager & DatabaseFileSystem: try to gracefully recover from connection loss
0,"Avoid String.intern() for UUID termsCreating Lucene terms is somewhat expensive, because it will usually call String.intern() on the field String. Jackrabbit uses UUID terms quite heavily to resolve hierarchy constraints. Lucene also provides a factory method on a Term that will create a new term instance with a given value and the same field name, avoiding the String.intern(). Jackrabbit should use the factory method whenever it creates a term for a UUID field."
0,"Add missing license headersThere are a few source files and a number of other files with missing or incorrect license headers within the Jackrabbit source tree. See the  discusssion at http://thread.gmane.org/gmane.comp.apache.jackrabbit.devel/8698 for details. The missing license headers need to be added. 

There are also W3C licensed files. This needs to be mentioned in the NOTICE file."
0,"Unreferenced VersionHistory should be deleted automatically.since the creation of a VersionHistory is triggered by the creation of a mix:versionable node, the removal should happen automatically, as soon as no references to that version histroy exist anymore. this is the case, when all mix:versionable nodes (in all workspaces) belonging to that VH are deleted, and all the versions in the VH are removed i.e. only the jcr:rootVersion is left. At this point, the VH should be deleted aswell."
0,"Fix junit scope in maven pomPlease change the junit dependency to

    <dependency>
      <groupId>junit</groupId>
      <artifactId>junit</artifactId>
      <version>3.8.1</version>
      <url>http://www.junit.org/</url>
      <properties>
        <scope>test</scope>
      </properties>
    </dependency>

for better automatic conversion to maven 2"
0,"NGramFilter -- construct n-grams from a TokenStreamThis filter constructs n-grams (token combinations up to a fixed size, sometimes
called ""shingles"") from a token stream.

The filter sets start offsets, end offsets and position increments, so
highlighting and phrase queries should work.

Position increments > 1 in the input stream are replaced by filler tokens
(tokens with termText ""_"" and endOffset - startOffset = 0) in the output
n-grams. (Position increments > 1 in the input stream are usually caused by
removing some tokens, eg. stopwords, from a stream.)

The filter uses CircularFifoBuffer and UnboundedFifoBuffer from Apache
Commons-Collections.

Filter, test case and an analyzer are attached."
0,"Remove jdk 1.4 restriction for jcr-testsThis restriction only exist because these tests form the TCK for JSR-283 which needed to support JDK 1.4. If maintenance on the JSR-283 TCK is needed, it can happen in a previous branch (2.3?).

"
0,"Post grouping facetingThis issues focuses on implementing post grouping faceting.
* How to handle multivalued fields. What field value to show with the facet.
* Where the facet counts should be based on
** Facet counts can be based on the normal documents. Ungrouped counts. 
** Facet counts can be based on the groups. Grouped counts.
** Facet counts can be based on the combination of group value and facet value. Matrix counts.   

And properly more implementation options.

The first two methods are implemented in the SOLR-236 patch. For the first option it calculates a DocSet based on the individual documents from the query result. For the second option it calculates a DocSet for all the most relevant documents of a group. Once the DocSet is computed the FacetComponent and StatsComponent use one the DocSet to create facets and statistics.  

This last one is a bit more complex. I think it is best explained with an example. Lets say we search on travel offers:
|||hotel||departure_airport||duration||
|Hotel a|AMS|5
|Hotel a|DUS|10
|Hotel b|AMS|5
|Hotel b|AMS|10

If we group by hotel and have a facet for airport. Most end users expect (according to my experience off course) the following airport facet:
AMS: 2
DUS: 1

The above result can't be achieved by the first two methods. You either get counts AMS:3 and DUS:1 or 1 for both airports."
0,"[patch] Support for digest auth MD5-sessI was attempting to access a device that requires Digest authentication using
MD5-sess, which does not seem to be supported."
0,"WebDAV: pack AbstractWebdavServlet with the jackrabbit-webdav projectsuggestion posted by alan cabrera on the dev list:

""Quite a handy servlet.  Too bad it's in jackrabbit-server.  Would this not be better placed in jackrabbit-webdav?  I'm writing my own server bits under WEBDAV and would prefer not to have JCR/Jackrabbit stuff.  I realize that this is a fussy preference.""

"
0,"Weight is not serializable for some of the queriesIn order to work with ParallelMultiSearcher, Query weights need to be serializable.  The interface Weight extends java.io.Serializable, but it appears that some of the newer queries unnecessarily store global state in their weights, thus causing serialization errors."
0,JSR 283 Observation
0,"Add JMX support to register a JCR RMI Server into Jboss I added two classes and one descriptor file to the jcr-rmi project. These files provide support to make the generated jar deployable into a Jboss server. 

 The deployment descriptor contains two parameters, the address of the local repository instance, and the target address where the rmi server should be registered. 

e.g.

<server>
 <mbean code=""org.apache.jackrabbit.rmi.server.jmx.JCRServer""
     name=""Jackrabbit.services:RMIServer = JCR RMI Server"">
    <attribute name=""Local"">java:jcr/local</attribute>
    <attribute name=""Target"">jnp://localhost:1099/jcrServer</attribute>	
<depends>jboss.jca:service=ManagedConnectionFactory,name=jcr/local</depends>					
  </mbean>
</server>	

this configuration registers an RMI server at /jcrServer that wraps the local repository at java:jcr/local.

br,
Edgar"
0,"Extend the client's redirect handling interface to allow control of the content of the redirectThe existing RedirectHandler interface provides the ability influence which situations cause redirects, but gives you no control over the content of the redirect itself.  For example, if you want the client follow the redirect of a POST request with a POST request to the new location, you can't do it.  DefaultRequestDirector decides what method will be used on the redirect request and as of the most recent patch, it's always either a HEAD or a GET.

One option for resolving this might be extending the RedirectHandler interface to be a factory for creating the redirect request object.  The the DefaultRequestDirector could then be changed to ask the RedirectHandler to create the appropriate request for the situation.

Thanks,
Ben"
0,"Add Boosting Function Term Query and Some Payload Query refactoringsSimilar to the BoostingTermQuery, the BoostingFunctionTermQuery is a SpanTermQuery, but the difference is the payload score for a doc is not the average of all the payloads, but applies a function to them instead.  BoostingTermQuery becomes a BoostingFunctionTermQuery with an AveragePayloadFunction applied to it.

Also add marker interface to indicate PayloadQuery types.  Refactor Similarity.scorePayload to also take in the doc id."
0,"CharArraySet.clear()I needed CharArraySet.clear() for something I was working on in Solr in a tokenstream.

instead I ended up using CharArrayMap<Boolean> because it supported .clear()

it would be better to use a set though, currently it will throw UOE for .clear() because AbstractSet will call iterator.remove() which throws UOE.

In Solr, the very similar CharArrayMap.clear() looks like this:
{code}
  @Override
  public void clear() {
    count = 0;
    Arrays.fill(keys,null);
    Arrays.fill(values,null);
  }
{code}

I think we can do a similar thing as long as we throw UOE for the UnmodifiableCharArraySet

will submit a patch later tonight (unless someone is bored and has nothing better to do)"
0,"Make open scoped locks recoverableThe lock tokens for open scoped locks are currently tied to the session which created the lock. If the session dies (for whatever reason) there is no way to recover the lock and unlock the node.
There is a theoretical way of adding the lock token to another session, but in most cases the lock token is not available.

Fortunately, the spec allows to relax this behaviour and I think it would make sense to allow all sessions from the same user to unlock the node - this is still in compliance with the spec but would make unlocked locked nodes possible in a programmatic way."
0,"Flexibility to turn on/off any flush triggersSee discussion at http://www.gossamer-threads.com/lists/lucene/java-dev/53186

Provide the flexibility to turn on/off any flush triggers - ramBufferSize, maxBufferedDocs and maxBufferedDeleteTerms. One of ramBufferSize and maxBufferedDocs must be enabled."
0,"don't reuse byte[] in IndexInput/Output for read/writeStringIndexInput now holds a private ""byte[] bytes"", which it re-uses for reading strings.  Likewise, IndexOutput holds a UTF8Result (which holds ""byte[] bytes""), re-used for writing strings.

These are both dangerous, since on reading or writing immense strings, we never free this storage.

We don't use read/writeString in very perf sensitive parts of the code, so, I think we should not reuse the byte[] at all.

I think this is likely the cause of the recent ""IndexWriter and memory usage"" thread, started by Ross Woolf on java-user@."
0,"create a simple test that indexes and searches byte[] termsCurrently, the only good test that does this is Test2BTerms (disabled by default)

I think we should test this capability, and also have a simpler example for how to do this.
"
0,"Deprecate SimilarityDelegator and Similarity.lengthNormSimilarityDelegator is a back compat trap (see LUCENE-2828).

Apps should just [statically] subclass Sim or DefaultSim; if they really need ""runtime subclassing"" then they can make their own app-level delegator.

Also, Sim.computeNorm subsumes lengthNorm, so we should deprecate lengthNorm in favor of computeNorm."
0,"Add VERBOSE to LuceneTestCase and LuceneTestCaseJ4component-build.xml allows to define tests.verbose as a system property when running tests. Both LuceneTestCase and LuceneTestCaseJ4 don't read that property. It will be useful for overriding tests to access one place for this setting (I believe currently some tests do it on their own). Then (as a separate issue) we can move all tests that don't check the parameter to only print if VERBOSE is true.

I will post a patch soon."
0,"A new Greek Analyzer for LuceneI would like to contribute a greek analyzer for lucene. It is based on the
existing Russian analyzer and features:

- most common greek character sets, such as Unicode, ISO-8859-7 and Windows-1253
- a collection of common greek stop words
- conversion of characters with diacritics (accent, diaeresis) in the lower case
filter, as well as handling of special characters, such as small final sigma

For the character sets I used RFC 1947 (Greek Character Encoding for Electronic
Mail Messages) as a reference. I have incorporated this analyzer in Luke as well
as used it successfully in a recent project of my company (EBS Ltd.).

I hope you will find it a useful addition to the project."
0,"Allow setting arbitrary objects on PerfRunDataPerfRunData is used as the intermediary objects between PerfRunTasks. Just like we can set IndexReader/Writer on it, it will be good if it allows setting other arbitrary objects that are e.g. created by one task and used by another.

A recent example is the enhancement to the benchmark package following the addition of the facet module. We had to add TaxoReader/Writer.

The proposal is to add a HashMap<String, Object> that custom PerfTasks can set()/get(). I do not propose to move IR/IW/TR/TW etc. into that map. If however people think that we should, I can do that as well."
0,Add support for benchmarking CollectorsAs the title says.
0,"IndexReader.close should forcefully evict entries from FieldCacheSpinoff of java-user thread ""heap memory issues when sorting by a string field"".

We rely on WeakHashMap to hold our FieldCache, keyed by reader.  But this lacks immediacy on releasing the reference, after a reader is closed.

WeakHashMap can't free the key until the reader is no longer referenced by the app. And, apparently, WeakHashMap has a further impl detail that requires invoking one of its methods for it to notice that a key has just become only weakly reachable.

To fix this, I think on IR.close we should evict entries from the FieldCache, as long as the sub-readers are truly closed (refCount dropped to 0)."
0,"SPI-commons:  QValueTest.testDateValueEquality2 fails due to changes made with JCR-1018with the introduction of QValue.getCalendar() the internal value for DATE-properties is now a Calendar (was
String). however, the equals() method has not been adjusted."
0,"Pass resultFetchSize/limit hint to SortedLuceneQueryHitsThe SortedLuceneQueryHits currently uses a default value of 100 (taken from lucene) for initially retrieved and sorted results. For larger result sets this is not optimal because it will cause re-execution of the underlying query with values 200, 400, 800, 1600, 3200, 6400, etc. Instead the query hits should get the limit that is set on the query or the resultFetchSize configured for the SearchIndex."
0,"Clover setup currently has some problems(tracking as a bug before it get lost in email...
  http://www.nabble.com/Clover-reports-missing-from-hudson--to15510616.html#a15510616
)

The clover setup for Lucene currently has some problems, 3 i think...

1) instrumentation fails on contrib/db/ because it contains java packages the ASF Clover lscence doesn't allow instrumentation of.  i have a patch for this.

2) running instrumented contrib tests for other contribs produce strange errors...

{{monospaced}}
    [junit] Testsuite: org.apache.lucene.analysis.el.GreekAnalyzerTest
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.126 sec
    [junit]
    [junit] ------------- Standard Error -----------------
    [junit] [CLOVER] FATAL ERROR: Clover could not be initialised. Are you sure you have Clover
in the runtime classpath? (class
java.lang.NoClassDefFoundError:com_cenqua_clover/CloverVersionInfo)
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testAnalyzer(org.apache.lucene.analysis.el.GreekAnalyzerTest):    Caused
an ERROR
    [junit] com_cenqua_clover/g
    [junit] java.lang.NoClassDefFoundError: com_cenqua_clover/g
    [junit]     at org.apache.lucene.analysis.el.GreekAnalyzer.<init>(GreekAnalyzer.java:157)
    [junit]     at
org.apache.lucene.analysis.el.GreekAnalyzerTest.testAnalyzer(GreekAnalyzerTest.java:60)
    [junit]
    [junit]
    [junit] Test org.apache.lucene.analysis.el.GreekAnalyzerTest FAILED
{{monospaced}}

...i'm not sure what's going on here.  the error seems to happen both when
trying to run clover on just a single contrib, or when doing the full
build ... i suspect there is an issue with the way the batchtests fork
off, but I can't see why it would only happen to contribs (the regular
tests fork as well)

3) according to Grant...

{{quote}}
...There is also a bit of a change on Hudson during the migration to the new servers that needs to be ironed  out. 
{{quote}}
"
0,"Add solr's artifact signing scripts into lucene's build.xml/common-build.xmlSolr has nice artifact signing scripts in its common-build.xml and build.xml.

For me as release manager of 3.0 it would have be good to have them also when building lucene artifacts. I will investigate how to add them to src artifacts and maven artifacts"
0,"Use an enumeration for QOM operatorsThe PFD version of QueryObjectModelConstants contains some incorrect constant values that make it unusable as a source of operator constants.

Since we are now using Java 5, I propose that instead of adding our own replacement constant strings, we implement a type-safe Operator enumeration that contains fixed versions of all the operator constants declared in QueryObjectModelConstants."
0,Remove deprecated DocIdSetIterator methods
0,extract test content loading from JackrabbitRepositoryStubdiscussed here: http://markmail.org/message/vl5ldnfbocccccxw
0,"jcr ext doesn't compile with jdk 1.4IllegalStateException(String str, Exception e) isn't supported."
0,"Support lower and upper case functions in ""order by"" clauseThe query languages should support lower- and upper-case functions within the ""order by"" clause.  This would provide case-insensitive ordering of query results.

Example:  Find all ""nt:base"" nodes ordered by the ""foo"" property, but ignoring case

In XPath:

//element(*,nt:base) order by fn:lower-case(@foo)

In SQL:

SELECT * FROM nt:base ORDER BY lower(foo)

"
0,"NamespaceRegistryTest makes assumptions about legal namesorg.apache.jackrabbit.test.api.NamespaceRegistryTest.testRegisterNamespace() makes the assumption that once a namespace is registered, it can be used in a node name. In practice, many repositories have their own restrictions on node naming, in particular may not support namespace prefixes in node names at all.

Proposal: change the test case so that it's independant of the repository's ability to create new nodes in that namespace.
"
0,"Some concurrency improvements for NRTSome concurrency improvements for NRT

I found & fixed some silly thread bottlenecks that affect NRT:

  * Multi/DirectoryReader.numDocs is synchronized, I think so only 1
    thread computes numDocs if it's -1.  I removed this sync, and made
    numDocs volatile, instead.  Yes, multiple threads may compute the
    numDocs for the first time, but I think that's harmless?

  * Fixed BitVector's ctor to set count to 0 on creating a new BV, and
    clone to copy the count over; this saves CPU computing the count
    unecessarily.

  * Also strengthened assertions done in SR, testing the delete docs
    count.

I also found an annoying thread bottleneck that happens, due to CMS.
Whenever CMS hits the max running merges (default changed from 3 to 1
recently), and the merge policy now wants to launch another merge, it
forces the incoming thread to wait until one of the BG threads
finishes.

This is a basic crude throttling mechanism -- you force the mutators
(whoever is causing new segments to appear) to stop, so that merging
can catch up.

Unfortunately, when stressing NRT, that thread is the one that's
opening a new NRT reader.

So, the first serious problem happens when you call .reopen() on your
NRT reader -- this call simply forwards to IW.getReader if the reader
was an NRT reader.  But, because DirectoryReader.doReopen is
synchronized, this had the horrible effect of holding the monitor lock
on your main IR.  In my test, this blocked all searches (since each
search uses incRef/decRef, still sync'd until LUCENE-2156, at least).
I fixed this by making doReopen only sync'd on this if it's not simply
forwarding to getWriter.  So that's a good step forward.

This prevents searches from being blocked while trying to reopen to a
new NRT.

However... it doesn't fix the problem that when an immense merge is
off and running, opening an NRT reader could hit a tremendous delay
because CMS blocks it.  The BalancedSegmentMergePolicy should help
here... by avoiding such immense merges.

But, I think we should also pursue an improvement to CMS.  EG, if it
has 2 merges running, where one is huge and one is tiny, it ought to
increase thread priority of the tiny one.  I think with such a change
we could increase the max thread count again, to prevent this
starvation.  I'll open a separate issue....
"
0,"""JCR levels"" link on http://jackrabbit.apache.org/doc/index.html brokenThe ""JCR levels"" link on the Jackrabbit home page is broken.
"
0,"Graduate appendingcodec from contrib/misc* All tests pass with this codec (at least once, maybe we don't test that two-phase commit stuff very well!)
* It doesn't require special client side configuration anymore to work (just set it on indexwriter and go)
* it now works with the compound file format.

I don't think it needs to live in contrib anymore."
0,"Calls to SegmentInfos.message should be wrapped w/ infoStream != null checksTo avoid the expensive message creation (which involves the '+' operator on strings, calls to message should be wrapped w/ infoStream != null check, rather than inside message(). I'll attach a patch which does that."
0,"Remove circular dependency between VersionManagerImpl and VersionItemStateProviderFrom a architectural perspective the VersionManagerImpl (VMgr) is at a higher level as the VersionItemStateProvider (VISP). While the VMgr deals with Items the VISP deals with ItemState object. Nonetheless the VISP has a reference to the VMgr and also calls the method setNodeReferences(), which violates the rule of a strictly layered system. E.g. one negative effect of this was a deadlock as described in JCR-672. It also makes it hard to solve JCR-962.

The attached patch includes the following changes:

- Move method VersionManagerImpl.setNodeReferences() VersionItemStateManager. The method can operate on ItemStates only and does not need to be in VersionManagerImpl. As can be seen in the current method it directly calls the PeristenceManager, which indicates it should be located in a lower layer.
- Promote the class VersionItemStateManager to a top level class
- Change method VersionManagerImpl.createSharedStateManager to return a VersionItemStateManager
- Remove VersionManagerImpl instance variable from VersionItemStateProvider
- In VersionItemStateProvider.setNodeReferences() call VersionItemStateManager.setNodeReferences()
- Instead of using the PersistenceManager in VersionManagerImpl.getItemReferences() use the ItemStateManager. It also seems that locking is not necessary for this method."
0,Similarity javadocs for scoring function to relate more tightly to scoring models in effectSee discussion in the related issue.
0,"Various small improvements to contrib/benchmarkI've worked out a few small improvements to contrib/benchmark:

  * Refactored the common code in Open/CreateIndexTask that sets the
    configuration for the IndexWriter.  This also fixes a bug in
    OpenIndexTasks that prevented you from disabling flushing by RAM.

  * Added a new config property for LineDocMaker:

      doc.reuse.fields=true|false

    which turns on/off reusing of Field/Document by LineDocMaker.
    This lets us measure performance impact of sharing Field/Document
    vs not, and also turn it off when necessary (eg if you have your
    own consumer that uses private threads).

  * Added merge.scheduler & merge.policy config options.

  * Added param for OptimizeTask, which expects an int and calls
    optimize(maxNumSegments) with that param.

  * Added param for CloseIndex(true|false) -- if you pass false that
    means close the index, aborting any running merges
"
0,Add DataInput/DataOutput subclasses that delegate to an InputStream/OutputStream.Such classes would be handy for FST serialization/deserialization.
0,"FST apis out of sync between trunk/3.xLooks like the offender is LUCENE-3030 :)

Not sure if everything is generally useful but it does change the public API (e.g. you can specify FreezeTail to the super-scary Builder ctor among other things).

Maybe we should sync up for 3.x? "
0,"Add IW.add/updateDocuments to support nested documentsI think nested documents (LUCENE-2454) is a very compelling addition
to Lucene.  It's also a popular (many votes) issue.

Beyond supporting nested document querying, which is already an
incredible addition since it preserves the relational model on
indexing normalized content (eg, DB tables, XML docs), LUCENE-2454
should also enable speedups in grouping implementation when you group
by a nested field.

For the same reason, it can also enable very fast post-group facet
counting impl (LUCENE-3097) when you what to
count(distinct(nestedField)), instead of unique documents, as your
""identifier"".  I expect many apps that use faceting need this ability
(to count(distinct(nestedField)) not distinct(docID)).

To support these use cases, I believe the only core change needed is
the ability to atomically add or update multiple documents, which you
cannot do today since in between add/updateDocument calls a flush (eg
due to commit or getReader()) could occur.

This new API (addDocuments(Iterable<Document>), updateDocuments(Term
delTerm, Iterable<Document>) would also further guarantee that the
documents are assigned sequential docIDs in the order the iterator
provided them, and that the docIDs all reside in one segment.

Segment merging never splits segments apart, so this invariant would
hold even as merges/optimizes take place.
"
0,"Make Lucene - Java 1.9.1 Available in Maven2 repository in iBibilio.orgPlease upload 1.9.1 release to iBiblio so that Maven users can easily use the latest release.  Currently 1.4.3 is the most recently available version: http://www.ibiblio.org/maven2/lucene/lucene/

Please read the following FAQ for more information: http://maven.apache.org/project-faq.html"
0,"Internal Timeout Handling in the TransactionContext is not XA Spec. conformThe problem here is that in a 2 phase transaction the xa spec does not  
permit a RB* return code in response to xa_commit().  The xa spec says  
the following about RB* return codes in the xa_commit() section:        
                                                                        
""The resource manager did not commit the work done on behalf of the     
transaction branch.  Upon return, the resource manager has rolled back  
the branch?s work and has released all held resources.  These values may
be returned only if TMONEPHASE is set in flags""                         
                                                                        
Essentially, the only two return codes from xa_commit that J2EE Containers can     
handle sensibly are XA_OK (normal case) and XA_RMFAIL.  RMFAIL will     
cause the containers to retry to commit the  transaction.  Any other return code will result in a heuristic          
transaction outcome (non-atomic).  

In a xa environment the TMONEPHASE is not set on the flags and so XA_RBTIMEOUT is 
not a permitted return code. A Container  transaction service cannot do anything to ensure an atomic     
outcome if an XAResource fails to honour its promise to be able to commit it made when it answer XA_OK in response to xa_prepare(). 

The internal timeout handling will rollback the Jackrabbit XAResource if the time exceeds between prepare and commit.
and in the commit Method will always throw a XA_RBTIMEOUT.

We should not handle the timeout internal because this should make the container in a 2 Phase transaction."
0,"Use StringBuilder instead of StringBuffer in benchmarkMinor change - use StringBuilder instead of StringBuffer in benchmark's code. We don't need the synchronization of StringBuffer in all the places that I've checked.

The only place where it _could_ be a problem is in HtmlParser's API - one method accepts a StringBuffer and it's an interface. But I think it's ok to change benchmark's API, back-compat wise and so I'd like to either change it to accept a String, or remove the method altogether -- no code in benchmark uses it, and if anyone needs it, he can pass StringReader to the other method."
0,Javadoc mistake in SegmentMerger
0,Grouping collector that computes grouped facet countsSpinoff from issue SOLR-2898. 
0,"Refactor DocumentsWriterI've been working on refactoring DocumentsWriter to make it more
modular, so that adding new indexing functionality (like column-stride
stored fields, LUCENE-1231) is just a matter of adding a plugin into
the indexing chain.

This is an initial step towards flexible indexing (but there is still
alot more to do!).

And it's very much still a work in progress -- there are intemittant
thread safety issues, I need to add tests cases and test/iterate on
performance, many ""nocommits"", etc.  This is a snapshot of my current
state...

The approach introduces ""consumers"" (abstract classes defining the
interface) at different levels during indexing.  EG DocConsumer
consumes the whole document.  DocFieldConsumer consumes separate
fields, one at a time.  InvertedDocConsumer consumes tokens produced
by running each field through the analyzer.  TermsHashConsumer writes
its own bytes into in-memory posting lists stored in byte slices,
indexed by term, etc.

DocumentsWriter*.java is then much simpler: it only interacts with a
DocConsumer and has no idea what that consumer is doing.  Under that
DocConsumer there is a whole ""indexing chain"" that does the real work:

  * NormsWriter holds norms in memory and then flushes them to _X.nrm.

  * FreqProxTermsWriter holds postings data in memory and then flushes
    to _X.frq/prx.

  * StoredFieldsWriter flushes immediately to _X.fdx/fdt

  * TermVectorsTermsWriter flushes immediately to _X.tvx/tvf/tvd

DocumentsWriter still manages things like flushing a segment, closing
doc stores, buffering & applying deletes, freeing memory, aborting
when necesary, etc.

In this first step, everything is package-private, and, the indexing
chain is hardwired (instantiated in DocumentsWriter) to the chain
currently matching Lucene trunk.  Over time we can open this up.

There are no changes to the index file format.

For the most part this is just a [large] refactoring, except for these
two small actual changes:

  * Improved concurrency with mixed large/small docs: previously the
    thread state would be tied up when docs finished indexing
    out-of-order.  Now, it's not: instead I use a separate class to
    hold any pending state to flush to the doc stores, and immediately
    free up the thread state to index other docs.

  * Buffered norms in memory now remain sparse, until flushed to the
    _X.nrm file.  Previously we would ""fill holes"" in norms in memory,
    as we go, which could easily use way too much memory.  Really this
    isn't a solution to the problem of sparse norms (LUCENE-830); it
    just delays that issue from causing memory blowup during indexing;
    memory use will still blowup during searching.

I expect performance (indexing throughput) will be worse with this
change.  I'll profile & iterate to minimize this, but I think we can
accept some loss.  I also plan to measure benefit of manually
re-cycling RawPostingList instances from our own pool, vs letting GC
recycle them.

"
0,RAMDirectory implements SerializableRAMDirectory is for some reason not serializable.
0,"Provide feedback mechanism to CredentialsProviderIf the remote server is using BASIC or NT authentication and you pass in 
invalid credentials you get stuck in an infinite for loop, repeatedly sending 
the same authentication request again and again to the server.  The for loop is 
in the executeMethod method of the HttpMethodDirector class.

Sample code:
=================================================================


import org.apache.commons.httpclient.Credentials;
import org.apache.commons.httpclient.NTCredentials;
import org.apache.commons.httpclient.UsernamePasswordCredentials;
import org.apache.commons.httpclient.HttpClient;
import org.apache.commons.httpclient.methods.GetMethod;
import org.apache.commons.httpclient.auth.*;

import java.io.IOException;
import java.io.BufferedInputStream;
import java.io.ByteArrayOutputStream;

/**
 * Created by IntelliJ IDEA.
 * User: dmartineau
 * Date: Nov 8, 2005
 * Time: 1:43:21 PM
 */
public class ShowProblem
{

    private String location;
    private String user;
    private String pass;
    private String domain;

    public ShowProblem(String location, String user, String pass, String domain)
    {
        this.location = location;
        this.user=user;
        this.pass=pass;
        this.domain=domain;

    }

    public int getFile()
    {
        int status = 500;
        HttpClient client = new HttpClient();
        client.getParams().setParameter(
            CredentialsProvider.PROVIDER, new CProvider(user,pass,domain));
        GetMethod httpget = new GetMethod(location);
        httpget.setDoAuthentication(true);

        try
        {
            // execute the GET
            status = client.executeMethod(httpget);
            if (status==200)
            {
                BufferedInputStream bin = new BufferedInputStream
(httpget.getResponseBodyAsStream());

                ByteArrayOutputStream bos = new ByteArrayOutputStream();
                int bytesRead = 0;
                byte[] buff = new byte[16384];

                while ( (bytesRead = bin.read(buff)) != -1) {
                    bos.write(buff, 0, bytesRead);
                }

                // display the results.
                System.out.println(new String(bos.toByteArray()));
            }
        }
        catch (Throwable t)
        {
            t.printStackTrace();
        }
        finally
        {
            // release any connection resources used by the method
            httpget.releaseConnection();
        }
        return status;

    }

    public static void main(String[] args)
    {
        ShowProblem showProblem = new ShowProblem(args[0],args[1],args[2],args
[3]);
        int response = showProblem.getFile();
        
    }



    class CProvider implements CredentialsProvider
    {
        private String user;
        private String password;
        private String domain;

        public CProvider(String user, String password, String domain)
        {
            super();
            this.user = user;
            this.password = password;
            this.domain = domain;
        }

        public Credentials getCredentials(final AuthScheme authscheme,final 
String host,int port,boolean proxy)
        throws CredentialsNotAvailableException
        {
            if (authscheme == null)
            {
                return null;
            }
            try
            {
                if (authscheme instanceof NTLMScheme)
                {
                    return new NTCredentials(user, password, host, domain);
                }
                else if (authscheme instanceof RFC2617Scheme)
                {
                    return new UsernamePasswordCredentials(user, password);
                }
                else
                {
                    throw new CredentialsNotAvailableException(""Unsupported 
authentication scheme: "" +
                        authscheme.getSchemeName());
                }
            }
            catch (IOException e)
            {
                throw new CredentialsNotAvailableException(e.getMessage(), e);
            }
        }

    }
}"
0,JSR 283: Built-In Node Typessync definitions of built-in node types with spec
0,"Repository does not start if text filter dependencies are missingWhen the search index is configured with a text filter class that requries another jar file and that jar file is missing the repository will not start and log the following misleading error:

Caused by: javax.jcr.RepositoryException
    at org.apache.jackrabbit.core.SearchManager.initializeQueryHandler(SearchManager.java:536)
    at org.apache.jackrabbit.core.SearchManager.<init>(SearchManager.java:278)
    at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.getSearchManager(RepositoryImpl.java:1430)
    at org.apache.jackrabbit.core.RepositoryImpl.initWorkspace(RepositoryImpl.java:538)
    at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:245)
    at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:482)
    at org.jbpm.jcr.impl.JackrabbitJcrService.start(JackrabbitJcrService.java:119)
    ... 63 more
Caused by: java.lang.IllegalArgumentException
    at org.apache.commons.collections.BeanMap.put(BeanMap.java:374)
    at org.apache.jackrabbit.core.config.BeanConfig.newInstance(BeanConfig.java:97)
    at org.apache.jackrabbit.core.SearchManager.initializeQueryHandler(SearchManager.java:530)
    ... 69 more "
0,ValueSourceQuery hits synchronization bottleneck in IndexReader.isDeletedI plan to fix it the same way we did in LUCENE-1316 for MatchAllDocsQuery (use TermDocs(null)).
0,AbstractQueryTest.evaluateResultOrder() should fail if workspace does not contain enough contentThe method AbstractQueryTest.evaluateResultOrder() currently throws a NotExecutableException if the workspace contains less than two nodes that can be used for ordering. It should rather fail with an error message telling that the workspace does not contain sufficient content to run the test.
0,EnwikiQueryMaker
0,"Remove timeout handling from TransactionContextAs discussed in JCR-2861, the transaction timeout handling in the TransactionContext class should not be needed since that's the task of the transaction manager, not the context. We should simply remove the timeout handling."
0,"ShingleFilter benchmarkSpawned from LUCENE-2218: a benchmark for ShingleFilter, along with a new task to instantiate (non-default-constructor) ShingleAnalyzerWrapper: NewShingleAnalyzerTask.

The included shingle.alg runs ShingleAnalyzerWrapper, wrapping the default StandardAnalyzer, with 4 different configurations over 10,000 Reuters documents each.  To allow ShingleFilter timings to be isolated from the rest of the pipeline, StandardAnalyzer is also run over the same set of Reuters documents.  This set of 5 runs is then run 5 times.

The patch includes two perl scripts, the first to output JIRA table formatted timing information, with the minimum elapsed time for each of the 4 ShingleAnalyzerWrapper runs and the StandardAnalyzer run, and the second to compare two runs' JIRA output, producing another JIRA table showing % improvement."
0,"cache should not generate stale responses to requests explicitly requesting first-hand or fresh onesThe current implementation will serve a stale response in the case that it has a stale cache entry but revalidation with the origin fails. However, the RFC says we SHOULD NOT do this if the client explicitly requested a first-hand or fresh response (via no-cache, max-age, max-stale, or min-fresh).
"
0,"Use bulk-byte-copy when merging term vectorsIndexing all of Wikipedia, with term vectors on, under the YourKit
profiler, shows that 26% of the time (!!) was spent merging the
vectors.  This was without offsets & positions, which would make
matters even worse.

Depressingly, merging, even with ConcurrentMergeScheduler, cannot in
fact keep up with the flushing of new segments in this test, and this
is on a strong IO system (Mac Pro with 4 drive RAID 0 array, 4 CPU
cores).

So, just like Robert's idea to merge stored fields with bulk copying
whenever the field name->number mapping is ""congruent"" (LUCENE-1043),
we can do the same with term vectors.

It's a little trickier because the term vectors format doesn't quite
make it easy to bulk-copy because it doesn't directly encode the
offset into the tvf file.

I worked out a patch that changes the tvx format slightly, by storing
the absolute position in the tvf file for the start of each document
into the tvx file, just like it does for tvd now.  This adds an extra
8 bytes (long) in the tvx file, per document.

Then, I removed a vLong (the first ""position"" stored inside the tvd
file), which makes tvd contents fully position independent (so you can
just copy the bytes).

This adds up to 7 bytes per document (less for larger indices) that
have term vectors enabled, but I think this small increase in index
size is acceptable for the gains in indexing performance?

With this change, the time spent merging term vectors dropped from 26%
to 3%.  Of course, this only applies if your documents are ""regular"".
I think in the future we could have Lucene try hard to assign the same
field number for a given field name, if it had been seen before in the
index...

Merging terms now dominates the merge cost (~20% over overall time
building the Wikipedia index).

I also beefed up TestBackwardsCompatibility unit test: test a non-CFS
and a CFS of versions 1.9, 2.0, 2.1, 2.2 index formats, and added some
term vector fields to these indices.
"
0,"Java 1.4 compile error in EclipseAs reported by Ate Douma in JCR-804, the revision 520841 introduced code that causes the Eclipse compiler to fail in Java 1.4 compliance mode. However, the same code compiles with the Sun JDK 1.4.

The problem is a enclosing class reference that an anonymous innner class instantiated in the constructor of a named inner class contains. Apparently (and understandably), in Eclipse 1.4 complier the instance references are not yet available when evaluating the super() arguments in the constructor."
0,"Test tooling updatesTo leverage advances in test tooling I'd like to upgrade our JUnit dependency to 4.x and switch to using the Maven Failsafe plugin  [1] instead of our current custom POM settings for integration tests. I'll also upgrade the Easymock dependency to 3.0.

[1] http://maven.apache.org/plugins/maven-failsafe-plugin/"
0,"Add N-Gram String Matching for Spell CheckingN-Gram version of edit distance based on paper by Grzegorz Kondrak, ""N-gram similarity and distance"". Proceedings of the Twelfth International Conference on String Processing and Information Retrieval (SPIRE 2005), pp. 115-126,  Buenos Aires, Argentina, November 2005. 
http://www.cs.ualberta.ca/~kondrak/papers/spire05.pdf
"
0,"Separate SegmentReaders (and other atomic readers) from composite IndexReadersWith current trunk, whenever you open an IndexReader on a directory you get back a DirectoryReader which is a composite reader. The interface of IndexReader has now lots of methods that simply throw UOE (in fact more than 50% of all methods that are commonly used ones are unuseable now). This confuses users and makes the API hard to understand.

This issue should split ""atomic readers"" from ""reader collections"" with a separate API. After that, you are no longer able, to get TermsEnum without wrapping from those composite readers. We currently have helper classes for wrapping (SlowMultiReaderWrapper - please rename, the name is really ugly; or Multi*), those should be retrofitted to implement the correct classes (SlowMultiReaderWrapper would be an atomic reader but takes a composite reader as ctor param, maybe it could also simply take a List<AtomicReader>). In my opinion, maybe composite readers could implement some collection APIs and also have the ReaderUtil method directly built in (possibly as a ""view"" in the util.Collection sense). In general composite readers do not really need to look like the previous IndexReaders, they could simply be a ""collection"" of SegmentReaders with some functionality like reopen.

On the other side, atomic readers do not need reopen logic anymore? When a segment changes, you need a new atomic reader? - maybe because of deletions thats not the best idea, but we should investigate. Maybe make the whole reopen logic simplier to use (ast least on the collection reader level).

We should decide about good names, i have no preference at the moment."
0,"isHttp11 should have HttpClient scope-----Original Message-----
From: Kalnichevski, Oleg [mailto:oleg.kalnichevski@bearingpoint.com] 
Sent: Wednesday, January 15, 2003 8:24 AM
To: Commons HttpClient Project
Cc: Rob Owen
Subject: RE: isHttp11 and HTTP/1.0 servers 

Rob
You are basically right hands down. It does make sense for the HTTP version 
flag to have HttpClient scope. We should address this shortcoming as a part of 
the post-2.0-release redesign

Feel free to file a bug report to make sure the issue does not go forgotten

http://nagoya.apache.org/bugzilla/enter_bug.cgi?product=Commons

Many thanks for bring it up

Cheers

Oleg

-----Original Message-----
From: Rob Owen [mailto:Rob.Owen@sas.com]
Sent: Monday, January 13, 2003 18:31
To: Commons HttpClient Project
Subject: isHttp11 and HTTP/1.0 servers 


The boolean variable http11 is set on a method by method basis. For PutMethod, 
decisions (eg. Expect: 100-continue request header) are made prior to 
determining the value for Http11 (chicken and egg problem) and so the default 
(true) is used to produce the request. An HTTP/1.0 server hangs waiting for 
the extra data on the PUT method body. 

For applications that are using HttpClient (ie. they do not manipulate the 
HTTP methods directly and cannot be expected to set the value of Http11 for 
each method instance), shouldn't http11 have HttpClient scope ? This would 
allow an interaction (eg. OPTIONS) to set http11 and all methods thereafter 
would use this setting?
  
------
Rob Owen
SAS Institute Inc.
email: Rob.Owen@sas.com"
0,JFlex-based HTMLStripCharFilter replacementA JFlex-based HTMLStripCharFilter replacement would be more performant and easier to understand and maintain.
0,"Analysis for IrishAdds analysis for Irish.

The stemmer is generated from a snowball stemmer. I've sent it to Martin Porter, who says it will be added during the week."
0,"Data Store: enable and fix testsCurrently the unit test TestTwoGetStreams fails in the trunk (it worked in older versions). This should be fixed. 

Also, the data store is disabled by default, so this test doesn't run by default. The data store should be enabled for testing."
0,"TokenStream/Tokenizer/TokenFilter/Token javadoc improvementsSome of the javadoc for the new TokenStream/Tokenizer/TokenFilter/Token APIs had javadoc errors.  To the best of my knowledge, I corrected these and refined the copy a bit."
0,"getPayloadSpans on org.apache.lucene.search.spans.SpanQuery should be abstractI just spent a long time tracking down a bug resulting from upgrading to Lucene 2.4.1 on a project that implements some SpanQuerys of its own and was written against 2.3.  Since the project's SpanQuerys didn't implement getPayloadSpans, the call to that method went to SpanQuery.getPayloadSpans which returned null and caused a NullPointerException in the Lucene code, far away from the actual source of the problem.  

It would be much better for this kind of thing to show up at compile time, I think.

Thanks!"
0,"Wrong schemaObjectPrefix parameter in default repository.xmlThe object schema prefix is hard-coded in the default configuration file (I think this taken from the jackrabbit-core.jar):

        <PersistenceManager class=""org.apache.jackrabbit.core.persistence.bundle.DerbyPersistenceManager"">
          <param name=""url"" value=""jdbc:derby:${wsp.home}/db;create=true""/>
          <param name=""schemaObjectPrefix"" value=""Jackrabbit Core_""/>
        </PersistenceManager>

This is probably caused by JCR-945, though I've no idea why ${wsp.name} is replaced with the name of the module...

I have marked this issue as minor because it still works with the DerbyPersistenceManager. There are separate database instances for each workspace, but it will become a problem if a data base persistence manager on a dedicated server is used."
0,"Explore streaming Viterbi search in KuromojiI've been playing with the idea of changing the Kuromoji viterbi
search to be 2 passes (intersect, backtrace) instead of 4 passes
(break into sentences, intersect, score, backtrace)... this is very
much a work in progress, so I'm just getting my current state up.
It's got tons of nocommits, doesn't properly handle the user dict nor
extended modes yet, etc.

One thing I'm playing with is to add a double backtrace for the long
compound tokens, ie, instead of penalizing these tokens so that
shorter tokens are picked, leave the scores unchanged but on backtrace
take that penalty and use it as a threshold for a 2nd best
segmentation...
"
0,"add a test for PorterStemFilterThere are no tests for PorterStemFilter, yet svn history reveals some (very minor) cleanups, etc.
The only thing executing its code in tests is a test or two in SmartChinese tests.

This patch runs the StemFilter against Martin Porter's test data set for this stemmer, checking for expected output.

The zip file is 100KB added to src/test, if this is too large I can change it to download the data instead.
"
0,"Expose FilteredTermsEnum from MTQ MTQ#getEnum() is protected and in order to access it you need to be in the o.a.l.search package. 

here is a relevant snipped from the mailing list discussion

{noformat}
getEnum() is protected so it is intended to be called *only* by subclasses (that's the idea behind protected methods). They are also accessible by other classes from the same package, but that's more a Java bug than a feature. The problem with MTQ is that RewriteMethod is a separate *class* and *not a subclass* of MTQ, so the method cannot be called (it can because of the ""java bug"" called from same package). So theoretically it has to be public otherwise you cannot call getEnum().

Another cleaner fix would be to add a protected final method to RewriteMethod that calls this method from MTQ. So anything subclassing RewriteMethod can get the enum from inside the RewriteMethod class which is the ""correct"" way to handle it. Delegating to MTQ is then ""internal"".
{noformat}"
0,"Lazy field loading breaks backward compatDocument.getField() and Document.getFields() have changed in a non backward compatible manner.
Simple code like the following no longer compiles:
 Field x = mydoc.getField(""x"");"
0,"Add JCR-RMI documentation to the Jackrabbit web siteUse the org.apache.jackrabbit.rmi.{client,server} package javadocs as a base for a JCR-RMI document page on the Jackrabbit web site."
0,"Add javadoc notes about ICUCollationKeyFilter's advantages over CollationKeyFiltercontrib/collation's ICUCollationKeyFilter, which uses ICU4J collation, is faster than CollationKeyFilter, the JVM-provided java.text.Collator implementation in the same package.  The javadocs of these classes should be modified to add a note to this effect.

My curiosity was piqued by [Robert Muir's comment|https://issues.apache.org/jira/browse/LUCENE-1581?focusedCommentId=12720300#action_12720300] on LUCENE-1581, in which he states that ICUCollationKeyFilter is up to 30x faster than CollationKeyFilter.

I timed the operation of these two classes, with Sun JVM versions 1.4.2/32-bit, 1.5.0/32- and 64-bit, and 1.6.0/64-bit, using 90k word lists of 4 languages (taken from the corresponding Debian wordlist packages and truncated to the first 90k words after a fixed random shuffling), using Collators at the default strength, on a Windows Vista 64-bit machine.  I used an analysis pipeline consisting of WhitespaceTokenizer chained to the collation key filter, so to isolate the time taken by the collation key filters, I also timed WhitespaceTokenizer operating alone for each combination.  The rightmost column represents the performance advantage of the ICU4J implemtation (ICU) over the java.text.Collator implementation (JVM), after discounting the WhitespaceTokenizer time (WST): (JVM-ICU) / (ICU-WST). The best times out of 5 runs for each combination, in milliseconds, are as follows:

||Sun JVM||Language||java.text||ICU4J||WhitespaceTokenizer||ICU4J Improvement||
|1.4.2_17 (32 bit)|English|522|212|13|156%|
|1.4.2_17 (32 bit)|French|716|243|14|207%|
|1.4.2_17 (32 bit)|German|669|264|16|163%|
|1.4.2_17 (32 bit)|Ukranian|931|474|25|102%|
|1.5.0_15 (32 bit)|English|604|176|16|268%|
|1.5.0_15 (32 bit)|French|817|209|17|317%|
|1.5.0_15 (32 bit)|German|799|225|20|280%|
|1.5.0_15 (32 bit)|Ukranian|1029|436|26|145%|
|1.5.0_15 (64 bit)|English|431|89|10|433%|
|1.5.0_15 (64 bit)|French|562|112|11|446%|
|1.5.0_15 (64 bit)|German|567|116|13|438%|
|1.5.0_15 (64 bit)|Ukranian|734|281|21|174%|
|1.6.0_13 (64 bit)|English|162|81|9|113%|
|1.6.0_13 (64 bit)|French|192|92|10|122%|
|1.6.0_13 (64 bit)|German|204|99|14|124%|
|1.6.0_13 (64 bit)|Ukranian|273|202|21|39%|
"
0,"Allow (or bring back) the ability to setRAMBufferSizeMB on an open IndexWriterIn 3.1 the ability to setRAMBufferSizeMB is deprecated, and removed in trunk. It would be great to be able to control that on a live IndexWriter. Other possible two methods that would be great to bring back are setTermIndexInterval and setReaderTermsIndexDivisor. Most of the other setters can actually be set on the MergePolicy itself, so no need for setters for those (I think)."
0,BytesRefHash#get() should expect a BytesRef instances for consistencyBytesRefHash#get should use a provided BytesRef instances instead of the internally used scratch. This is how all other APIs currently work and we should be consistent.
0,"Change remaining contrib streams/filters to use new TokenStream APIAll other contrib streams/filters have already been converted with LUCENE-1460.

The two shingle filters are the last ones we need to convert."
0,"speed up automaton seeking in nextStringWhile testing, i found there are some queries (e.g. wildcard ?????????) that do quite a lot of backtracking.

nextString doesn't handle this particularly well, when it walks the DFA, if it hits a dead-end and needs to backtrack, it increments the bytes, and starts over completely.

alternatively it could save the path information in an int[], and backtrack() could return a position to restart from, instead of just a boolean.
"
0,"Factor out ByteSliceWriter from DocumentsWriterFieldDataDocumentsWriter uses byte slices into shared byte[]'s to hold the
growing postings data for many different terms in memory.  This is
probably the trickiest (most confusing) part of DocumentsWriter.

Right now it's not cleanly factored out and not easy to separately
test.  In working on this issue:

  http://mail-archives.apache.org/mod_mbox/lucene-java-user/200805.mbox/%3c126142c0805061426n1168421ya5594ef854fae5e4@mail.gmail.com%3e

which eventually turned out to be a bug in Oracle JRE's JIT compiler,
I factored out ByteSliceWriter and created a unit test to stress test
the writing & reading of byte slices.  The test just randomly writes N
streams interleaved into shared byte[]'s, then reads them back
verifying the results are correct.

I created the stress test to try to find any bugs in that code.  The
test ran fine (no bugs were found) but I think the refactoring is
still very much worthwhile.

I expected the changes to reduce indexing throughput, so I ran a test
indexing first 200K Wikipedia docs using this alg:

{code}
analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
doc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker

docs.file=/Volumes/External/lucene/wiki.txt
doc.stored = true
doc.term.vector = true
doc.add.log.step=2000

directory=FSDirectory
autocommit=false
compound=true

ram.flush.mb=256

{ ""Rounds""
  ResetSystemErase
  { ""BuildIndex""
    - CreateIndex
     { ""AddDocs"" AddDoc > : 200000
    - CloseIndex
  }
  NewRound
} : 4

RepSumByPrefRound BuildIndex

{code}

Ok trunk it produces these results:
{code}
Operation   round   runCnt   recsPerRun        rec/s  elapsedSec    avgUsedMem    avgTotalMem
BuildIndex      0        1       200000        791.7      252.63   338,552,096  1,061,814,272
BuildIndex -  - 1 -  -   1 -  -  200000 -  -   793.1 -  - 252.18 - 605,262,080  1,061,814,272
BuildIndex      2        1       200000        794.8      251.63   601,966,528  1,061,814,272
BuildIndex -  - 3 -  -   1 -  -  200000 -  -   782.5 -  - 255.58 - 608,699,712  1,061,814,272
{code}

and with the patch:

{code}
Operation   round   runCnt   recsPerRun        rec/s  elapsedSec    avgUsedMem    avgTotalMem
BuildIndex      0        1       200000        745.0      268.47   338,318,784  1,061,814,272
BuildIndex -  - 1 -  -   1 -  -  200000 -  -   792.7 -  - 252.30 - 605,331,776  1,061,814,272
BuildIndex      2        1       200000        786.7      254.24   602,915,712  1,061,814,272
BuildIndex -  - 3 -  -   1 -  -  200000 -  -   795.3 -  - 251.48 - 602,378,624  1,061,814,272
{code}

So it looks like the performance cost of this change is negligible (in
the noise).

"
0,IOContext should be part of the SegmentReader cache key Once IOContext (LUCENE-2793) is landed the IOContext should be part of the key used to cache that reader in the pool
0,"Jackrabbit depends on Oracle driver for BLOB support in Oracle versions previous than 10.2In Oracle versions previous to 10.2, Jackrabbit explicitly uses a class from the Oracle driver to provide BLOB support (see OracleFileSystem.init()). This special handling is no longer necesary for Oracle 10.2+, so we should provide a new implementation. As discussed on the list, we can create a new class for Oracle 10.2+, make it inherit from DbFileSystem, and override the createSchema(), and table space related methods, which are the ones that need special handling. Furthermore, we could refactor the current OracleFileSystem and break it into two clases, one of them to keep the current behavior and a new one to keep the common code (which we could rename to OracleBaseFileSystem or similar, to maintain compatiblity with code that uses OracleFileSystem for versions previous to 10.2). Then we make the Oracle10FileSystem inherit from the latter."
0,Allow random seed to be set in DeleteByPercentTaskNeed this to make index identical on multiple runs.  
0,Exclude JavaCC-generated code from static analysisThe JavaCC-generated code we have in spi-commons should be excluded from static analysis done by tools like Sonar.
0,"BalancedSegmentMergePolicy, contributed from the Zoie project for realtime indexingWritten by Yasuhiro Matsuda for Zoie realtime indexing system used to handle high update rates to avoid large segment merges.
Detailed write-up is at:

http://code.google.com/p/zoie/wiki/ZoieMergePolicy
"
0,"Drop Maven 1 compatibilityWe migrated from Maven 1 to Maven 2 as the build system in Jackrabbit 1.2, but we kept compatibility with related Maven 1 build with the maven-one-plugin that deployed all builds also to the local Maven 1 repository.

Hardly any downstream project uses Maven 1 anymore, so it's safe for us to simplify our build now by dropping the use of the maven-one-plugin."
0,"Deprecate all String/File ctors/opens in IndexReader/IndexWriter/IndexSearcherDuring investigation of LUCENE-1658, I found out, that even LUCENE-1453 is not completely fixed.
As 1658 deprecates all FSDirectory.getDirectory() static factories, we should not use them anymore. As the user is now free to choose the correct directory implementation using direct instantiation or using FSDir.open() he should no longer use all ctors/methods in IndexWriter/IndexReader/IndexSearcher & Co. that simply take path names as String or File and always instantiate the Directory himself.

LUCENE-1453 currently works for the cached directory implementations from FSDir.getDirectory, but not with uncached, non refcounting FSDirs. Sometime reopen() closes the directory (as far as I see, when a SegmentReader changes to a MultiSegmentReader and/or deletes apply). This is hard to track. In Lucene 3.0 we then can remove the whole bunch of closeDirectory parameters/fields in these classes and simply do not care anymore about closing directories.

To remove this closeDirectory parameter now (before 3.0) and also fix 1453 correctly, an additional idea would be to change these factories that take the File/String to return the IndexReader wrapped by a FilteredIndexReader, that keeps track on closing the underlying directory after close and reopen. This is simplier than passing this boolean between different DirectoryIndexReader instances. The small performance impact by wrapping with FilterIndexReader should not be so bad, because the method is deprecated and we can state, that it is better to user the factory method with Directory parameter."
0,"Make MMapDirectory.MAX_BBUF user configureable to support chunking the index files in smaller partsThis is a followup for java-user thred: http://www.lucidimagination.com/search/document/9ba9137bb5d8cb78/oom_with_2_9#9bf3b5b8f3b1fb9b

It is easy to implement, just add a setter method for this parameter to MMapDir."
0,"JSR 283: References and Dereferencing of Property ValuesReferences
--------------------------------------------------------------------------------------------------------------------------
new methods are:

- Node.getReferences(String name) PropertyIterator 
- Node.getWeakReferences() PropertyIterator 
- Node.getWeakReferences(String name) PropertyIterator


Derferencing
--------------------------------------------------------------------------------------------------------------------------
As of JSR 283 the following property types may be dereferenced to a Node:

- REFERENCE
- WEAKREFERENCE
- PATH
- any type that can be converted to either of the types above

The new method
- Property.getProperty() returns the Property pointed to by a PATH value.
- any type that can be converted to PATH




"
0,"Tokenizers (which are the source of Tokens) should call AttributeSource.clearAttributes() firstThis is a followup for LUCENE-1796:
{quote}
Token.clear() used to be called by the consumer... but then it was switched to the producer here: LUCENE-1101 
I don't know if all of the Tokenizers in lucene were ever changed, but in any case it looks like at least some of these bugs were introduced with the switch to the attribute API - for example StandardTokenizer did clear it's reusableToken... and now it doesn't.
{quote}

As alternative to changing all core/contrib Tokenizers to call clearAttributes first, we could do this in the indexer, what would be a overhead for old token streams that itsself clear their reusable token. This issue should also update the Javadocs, to clearly state inside Tokenizer.java, that the source TokenStream (normally the Tokenizer) should clear *all* Attributes. If it does not do it and e.g. the positionIncrement is changed to 0 by any TokenFilter, but the filter does not change it back to 1, the TokenStream would stay with 0. If the TokenFilter would call PositionIncrementAttribute.clear() (because he is responsible), it could also break the TokenStream, because clear() is a general method for the whole attribute instance. If e.g. Token is used as AttributeImpl, a call to clear() would also clear offsets and termLength, which is not wanted. So the source of the Tokenization should rest the attributes to default values.

LUCENE-1796 removed the iterator creation cost, so clearAttributes should run fast, but is an additional cost during Tokenization, as it was not done consistently before, so a small speed degradion is caused by this, but has nothing to do with the new TokenStream API."
0,"AbstractHttpClient.addRequestInterceptor should document in what order the interceptors runhttp://hc.apache.org/httpcomponents-client/httpclient/apidocs/org/apache/http/impl/client/AbstractHttpClient.html#addRequestInterceptor(org.apache.http.HttpRequestInterceptor) has no documentation. It should at least say what order new interceptors run in. Presumably they run in order by index, but does the lowest or highest index run first?

This class or DefaultHttpClient should also say what interceptors are added by default. That is, what would I be getting rid of by calling clearResponseInterceptors()?"
0,"'ant generate-maven-artifacts' should work for lucene+solr 3.x+The maven build scripts need to be updated so that solr uses the artifacts from lucene.

For consistency, we should be able to have a different 'maven_version' then the 'version'  That is, we want to build: 3.1-SNAPSHOT with a jar file: 3.1-dev"
0,"[PATCH] Enable application-level management of IndexWriter.ramDirectory sizeIndexWriter currently only supports bounding of in the in-memory index cache using maxBufferedDocs, which limits it to a fixed number of documents.  When document sizes vary substantially, especially when documents cannot be truncated, this leads either to inefficiencies from a too-small value or OutOfMemoryErrors from a too large value.

This simple patch exposes IndexWriter.flushRamSegments(), and provides access to size information about IndexWriter.ramDirectory so that an application can manage this based on total number of bytes consumed by the in-memory cache, thereby allow a larger number of smaller documents or a smaller number of larger documents.  This can lead to much better performance while elimianting the possibility of OutOfMemoryErrors.

The actual job of managing to a size constraint, or any other constraint, is left up the applicatation.

The addition of synchronized to flushRamSegments() is only for safety of an external call.  It has no significant effect on internal calls since they all come from a sychronized caller.
"
0,"Similarity javadocs look ugly if created with java7's javadocThe captions used to illustrate the formulas are tables here:
in jdk 5/6 the table is centered nicely.

But with java7's javadocs (I think due to some css styles changes?),
the table is not centered but instead stretched.

I think we just need to center this table with a different technique?

Have a look at http://people.apache.org/~rmuir/java7-style-javadocs/org/apache/lucene/search/Similarity.html to see what I mean.

NOTE: these javadocs are under TFIDFSimilarity.java in trunk."
0,"Handle virtual hosts, relative urls, multi-homingNeed to be able to open a socket to one ipaddress (or hostname) and then include
a virtual hostname in the Host header. Use InetAddress class perhaps."
0,"Change default value for maxMergeDocsThis is actually a left over from the time before JCR-197 was implemented. Back then index merges were performed with the client thread and would hold up execution for a long time if a large number of nodes were merged. The default value for maxMergeDocs limited this to 100'000 nodes, resulting in a couple of seconds for the merge operation.

This default value does not make sense anymore because index merges are performed in a background thread and may take a long time without an effect on regular workspace operations. If a workspace grows large it may cause performance degradation because the number of index segments increases linearly when there are more than 100'000 nodes.

I propose to set the new default to Integer.MAX_VALUE"
0,"Use a System.arraycopy more than a forIn org.apache.lucene.index.DocumentWriter. The patch will explain by itself. I didn't make any performance test, but I think it is obvious that it will be faster.
All tests passed."
0,"Index SplitterIf an index has multiple segments, this tool allows splitting those segments into separate directories.  "
0,"Source packaging fails if ${dist.dir} does not existpackage-tgz-src and package-zip-src fail if ${dist.dir} does not exist, since these two targets do not call the package target, which is responsible for making the dir.

I have a fix and will commit shortly."
0,"Remove Serializable on ItemState classesItemStates are never directly serialized, which means they don't have to implement Serializable anymore.

See also: http://markmail.org/message/wsqnih2lembkcrdf"
0,Review test cases and cross check with 1.0 specificationThis jira task is meant to collect issues with the TCK test cases.
0,"Node.setProperty(String, ...) implementation not according to the specificationto illustrate the issue assume the following  property definition:

name: someText
type: String
non-mandatory
non-autocreate

the following call would throw a ConstraintViolationException
if the property doesn't exist yet:

node.setProperty(""someText"", 12345);

the rules used to find an applicable definition in this case are too strict."
0,"typos on FAQI found out the following typos on the FAQ (http://lucene.sourceforge.net/cgi-
bin/faq/faqmanager.cgi) of lucene:
in 8. Will Lucene work with my Java application ?
- felxible
- applciations"
0,Make all classes that have a close() methods instanceof Closeable (Java 1.5)This should be simple.
0,"expose shutdown method in o.a.j.jndi.BindableRepositorysee http://thread.gmane.org/gmane.comp.apache.jackrabbit.devel/3680
"
0,Better 'invalid format' exception messages for value classesThe valueOf() methods of the Value classes throw an exception without information on the desired type and without the String value that gave the error.
0,"Move FuzzyQuery rewrite as separate RewriteMode into MTQ, was: Highlighter fails to highlight FuzzyQueryAs FuzzyQuery does not allow to change the rewrite mode, highlighter fails with UOE in flex since LUCENE-2110, because it changes the rewrite mode to Boolean query. The fix is: Allow MTQ to change rewrite method and make FUZZY_REWRITE public for that.

The rewrite mode will live in MTQ as TOP_TERMS_SCORING_BOOLEAN_REWRITE. Also the code will be refactored to make heavy reuse of term enumeration code and only plug in the PQ for filtering the top terms."
0,"remove DocsAndPositionsEnum.getPayloadLengthThis was an accidental leftover; now that getPayload returns a BytesRef, this method is not needed."
0,"Drop the Dumpable interfaceI belive the o.a.j.core.util.Dumpable interface was originally used for diagnostic purposes, but AFAIUI we don't use it anywhere anymore. I'd like to drop the interface and refactor the dump() methods in various Jackrabbit classes to more detailed toString() methods that would be more useful to debuggers and other general-purpose diagnostic tools."
0,"ResourceConfig: read additional parameters for IOHandler and PropertyHandler that are covered by public settersthe reason for this is that currently default node types used in DefaultHandler cannot be changed using the resource config, which is
a bit cumbersome and leads to useless copies of DefaultIOManager."
0,"Explicit management of public APII'd like to start using the Clirr Maven plugin [1] to make sure that we don't accidentally break backwards compatibility in our public APIs, most notably in jackrabbit-api and jackrabbit-jcr-commons.

Also, we should start explicitly managing the API versions exposed as a part of the OSGi package metadata. Currently all our public packages simply get the latest project version as their version number, but it would be better if the version was explicitly managed and only updated if the API actually changes. To do this I propose we use @Version annotations from the bnd tool on the package-info.java files in all packages considered a part of our public API.

The Clirr plugin should flag all changes made in the API, so we have an easy way to tell which packages need to have their version numbers updated.

[1] http://mojo.codehaus.org/clirr-maven-plugin/"
0,"separate IndexDocValues interface from implementationCurrently the o.a.l.index.values contains both the abstract apis and Lucene40's current implementation.

I think we should move the implementation underneath Lucene40Codec, leaving only the abstract apis.

For example, simpletext might have a different implementation, and we might make a int8 implementation
underneath preflexcodec to support norms."
0,Jcr-Server Module: Remove Dependency from Jackrabbit-Core
0,"MockCharFilter offset correction is wrongThis is a fake charfilter used in basetokenstreamtestcase.

it occasionally doubles some characters, and corrects offsets.

its used to find bugs where analysis components would fail otherwise with charfilters,
but its correctOffset is actually wrong (harmless to any tests today, but still wrong).
"
0,"Introduce QValueConstraint and change return type of QPropertyDefinition.getValueConstraints()public interface QValueConstraint {
+
+    /**
+     * Empty array of <code>QValueConstraint</code>.
+     */
+    public static final QValueConstraint[] EMPTY_ARRAY = new QValueConstraint[0];
+
+    /**
+     * Check if the specified value matches this constraint.
+     *
+     * @param value The value to be tested.
+     * @throws ConstraintViolationException If the specified value is
+     * <code>null</code> or does not matches the constraint.
+     * @throws RepositoryException If another error occurs.
+     */
+    void check(QValue value) throws ConstraintViolationException, RepositoryException;
+
+    /**
+     * For constraints that are not namespace prefix mapping sensitive this
+     * method returns the same defined in
+     * <code>{@link PropertyDefinition#getValueConstraints()}</code>.
+     * <p/>
+     * Those that are namespace prefix mapping sensitive (e.g.
+     * <code>NameConstraint</code>, <code>PathConstraint</code> and
+     * <code>ReferenceConstraint</code>) return an expanded string.
+     *
+     * @return the expanded definition String
+     */
+    String getExpandedDefinition();
+
+}


+++ jackrabbit-spi/src/main/java/org/apache/jackrabbit/spi/QPropertyDefinition.java	(working copy)
@@ -45,7 +45,7 @@
      *
      * @return the array of value constraints.
      */
-    public String[] getValueConstraints();
+    public QValueConstraint[] getValueConstraints();
"
0,"Disable SearchManagerIn previous versions (e.g. SVN tag 0.1-spec0.14) it was possible to disable the SearchManagers by not configuring a search index path. In the current revision, a NullPointerException is thrown, if the search index configuration is missing, tough the rest of the system would support missing search index configuration as before.

I suggest to extend search index configuration interpretation in WorkspaceCfg.init as follows:

        Element srchConfig = wspElem.getChild(SEARCH_INDEX_ELEMENT);
        if (srchConfig != null) {
            String pathAttr = srchConfig.getAttributeValue(PATH_ATTRIB);
            if (pathAttr != null && pathAttr.length() > 0) {
                searchIndexDir = replaceVars(pathAttr, vars);
            }
        }

This only reads search index configuration if available.

The reason to switch of the SearchManager is, that in my use case enabling the SearchManager yields a performance degradation of a factor of 10 ! Instead of taking around 500ms (which is still too long :-) to save 3 nodes and 15 properties, it would take around 5 seconds to save the same amount of data. And I do not need the SearchManager in my use case."
0,"Standardize on a common mocking framework (either EasyMock or Mockito)We are currently using EasyMock in the caching module and Mockito in the main module. While Mockito appears to have a somewhat nicer API, the sheer number of test cases based on EasyMock in the caching module makes it much simpler to replace Mockito with EasyMock than the other way around."
0,"Deprecate NamespaceListener and AbstractNamespaceResolverThe NamespaceListener interface is no longer used with the JSR 283 style namespace handling that avoids lots of the synchronization that was previously to keep the local namespace mappings up to date.

Also, the only (remaining) purpose of the AbstractNamespaceResolver class is to add support for managing NamespaceListeners. Since that functionality is nowhere used anymore, we can make all subclasses use the NamespaceResolver interface directly.

Since NamespaceListener and AbstractNamespaceResolver are public in jackrabbit-spi-commons, I will for now only mark them as deprecated. We can get rid of them in Jackrabbit 2.0."
0,"Revise internal data structures of ThreadSafeClientConnManagerThreadSafeClientConnManager internal data structures can be improved:
- keep track of issued connections with weak references
- use class derived from WeakReference instead of a lookup table for callbacks from ReferenceThread
  (or drop ReferenceThread in favor of occasionally polling the issued connections for leaks)
"
0,"TCK: SetPropertyValueTest#testCompactValueArrayWithNulls does not respect nodename1 and nodetype configuration propertiesTest doesn't respect value of nodename1 and nodetype configuration properties.

Proposal: create property under testnode instead of testrootnode.

--- SetPropertyValueTest.java   (revision 422074)
+++ SetPropertyValueTest.java   (working copy)
@@ -374,11 +374,11 @@
      * the value array by removing all null values
      */
     public void testCompactValueArrayWithNulls() throws Exception {
-        testRootNode.setProperty(propertyName2, vArrayWithNulls);
+        testNode.setProperty(propertyName2, vArrayWithNulls);
         superuser.save();
         assertEquals(""Node.setProperty(String, valueArrayWithNulls[]) did not compact the value array by removing the null values"",
                 2,
-                testRootNode.getProperty(propertyName2).getValues().length);
+                testNode.getProperty(propertyName2).getValues().length);
     }
"
0,"Mating Collector and Scorer on doc Id ordernessThis is a spin off of LUCENE-1593. This issue proposes to expose appropriate API on Scorer and Collector such that one can create an optimized Collector based on a given Scorer's doc-id orderness and vice versa. Copied from LUCENE-1593, here is the list of changes:

# Deprecate Weight and create QueryWeight (abstract class) with a new scorer(reader, scoreDocsInOrder), replacing the current scorer(reader) method. QueryWeight implements Weight, while score(reader) calls score(reader, false /* out-of-order */) and scorer(reader, scoreDocsInOrder) is defined abstract.
#* Also add QueryWeightWrapper to wrap a given Weight implementation. This one will also be deprecated, as well as package-private.
#* Add to Query variants of createWeight and weight which return QueryWeight. For now, I prefer to add a default impl which wraps the Weight variant instead of overriding in all Query extensions, and in 3.0 when we remove the Weight variants - override in all extending classes.
# Add to Scorer isOutOfOrder with a default to false, and override in BS to true.
# Modify BooleanWeight to extend QueryWeight and implement the new scorer method to return BS2 or BS based on the number of required scorers and setAllowOutOfOrder.
# Add to Collector an abstract _acceptsDocsOutOfOrder_ which returns true/false.
#* Use it in IndexSearcher.search methods, that accept a Collector, in order to create the appropriate Scorer, using the new QueryWeight.
#* Provide a static create method to TFC and TSDC which accept this as an argument and creates the proper instance.
#* Wherever we create a Collector (TSDC or TFC), always ask for out-of-order Scorer and check on the resulting Scorer isOutOfOrder(), so that we can create the optimized Collector instance.
# Modify IndexSearcher to use all of the above logic.

The only class I'm worried about, and would like to verify with you, is Searchable. If we want to deprecate all the search methods on IndexSearcher, Searcher and Searchable which accept Weight and add new ones which accept QueryWeight, we must do the following:
* Deprecate Searchable in favor of Searcher.
* Add to Searcher the new QueryWeight variants. Here we have two choices: (1) break back-compat and add them as abstract (like we've done with the new Collector method) or (2) add them with a default impl to call the Weight versions, documenting these will become abstract in 3.0.
* Have Searcher extend UnicastRemoteObject and have RemoteSearchable extend Searcher. That's the part I'm a little bit worried about - Searchable implements java.rmi.Remote, which means there could be an implementation out there which implements Searchable and extends something different than UnicastRemoteObject, like Activeable. I think there is very small chance this has actually happened, but would like to confirm with you guys first.
* Add a deprecated, package-private, SearchableWrapper which extends Searcher and delegates all calls to the Searchable member.
* Deprecate all uses of Searchable and add Searcher instead, defaulting the old ones to use SearchableWrapper.
* Make all the necessary changes to IndexSearcher, MultiSearcher etc. regarding overriding these new methods.

One other optimization that was discussed in LUCENE-1593 is to expose a topScorer() API (on Weight) which returns a Scorer that its score(Collector) will be called, and additionally add a start() method to DISI. That will allow Scorers to initialize either on start() or score(Collector). This was proposed mainly because of BS and BS2 which check if they are initialized in every call to next(), skipTo() and score(). Personally I prefer to see that in a separate issue, following that one (as it might add methods to QueryWeight)."
0,"Authorization credentials should be sent pre-emptivelyWhen a web browser receives a <i>401: Unauthorized</i> response code, the
browser prompts for the user and password credentials for the requested
authentication realm.  An Authorization header is then sent for this request. 
HttpClient models this behaviour quite well.

After the web browser has the authentication credentials for a given host, port
and realm, it then sends the Authorization header for subsequent requests
pre-emptively, whithout need for a 401 response.  HttpClient always reqires a
401 response before it will send out the Authorization header.

As <code>HttpClient.startSession()</code> will take a <code>Credentials</code>
object as a parameter as the default credentials, the default credentials should
be sent as part of every request in that session.  Some mechanisim for
over-riding the default credentials should also be provided to be sent
pre-emptively.

The point of this enhancement request is to minimize the number of unnecessisary
401 responses.

It appears that the simple solution might be to modify the logic of when
<code>Authenticator.authenticate()</code> gets called in
<code>HttpMethodBase.addAuthorizationRequestHeader()</code>"
0,"remove dependencies of XPathQueryBuilder on core Jackrabbit codeThe XPath query parser currently has a single dependency on SearchManager, for the sole purpose of importing two namespace URIs (for XML Schema and XPath 2.0 functions). This makes it harder than it should be to use it stand-alone.

I propose to copy the two namespace URIs into XPathQueryBuilder, getting rid of the dependency.

"
0,"Change Log-Level in DefaultIOListenerPlease change loglevel for method onEnd(IOHandler handler, IOContext ioContext, boolean success) to debug"
0,"QueryHandler should use lucene Input-/OutputStream implementationsCurrently the QueryHandler uses a jackrabbit specific implementation of the lucene Directory interface to make use of the jackrabbit FileSystem abstraction. Lucene operations on the file system however requires quite often random access on the index files. With the current FileSystem interface / abstraction random access is not possible on a FileSystemResource, therefore it is simulated by re-aquiring the InputStream and then seeking to the desired position. This it not efficient at all.

With respect to performance any other use than file based index storage does not make sense with lucene. Hence, the current abstraction using FileSystem should be dropped in favour of direct file access."
0,"update tests so that both Query.XPATH and Query:SQL are treated as optional featuresIn JCR 2.0, both Query.XPATH and Query.SQL are optional (or, actually, deprecated).

We either need to modify the tests so that they pass on a repository that doesn't support them (-> NotExecutableException), or remove them altogether."
0,"Decouple Filter from BitSet{code}
package org.apache.lucene.search;

public abstract class Filter implements java.io.Serializable 
{
  public abstract AbstractBitSet bits(IndexReader reader) throws IOException;
}

public interface AbstractBitSet 
{
  public boolean get(int index);
}

{code}

It would be useful if the method =Filter.bits()= returned an abstract interface, instead of =java.util.BitSet=.

Use case: there is a very large index, and, depending on the user's privileges, only a small portion of the index is actually visible.
Sparsely populated =java.util.BitSet=s are not efficient and waste lots of memory. It would be desirable to have an alternative BitSet implementation with smaller memory footprint.

Though it _is_ possibly to derive classes from =java.util.BitSet=, it was obviously not designed for that purpose.
That's why I propose to use an interface instead. The default implementation could still delegate to =java.util.BitSet=.

"
0,"Deprecate Spatial ContribThe spatial contrib is blighted by bugs.  The latest series, found by Grant and discussed [here|http://search.lucidimagination.com/search/document/c32e81783642df47/spatial_rethinking_cartesian_tiers_implementation] shows that we need to re-think the cartesian tier implementation.

Given the need to create a spatial module containing code taken from both lucene and Solr, it makes sense to deprecate the spatial contrib, and start from scratch in the new module.


"
0,"remove IndexSearcher.closeNow that IS is never ""heavy"" (since you have to pass in your own IR), IS.close is truly a no-op... I think we should remove it.

"
0,"Avoid string concatenation in AbstractBundlePersistenceManagerThe following line:

        log.debug(""stored bundle "" + bundle.getId());

should be changed to:

        log.debug(""stored bundle {}"", bundle.getId());
"
0,"Remove old byte[] norms api from IndexReaderFollowup to LUCENE-3628.

We should remove this api and just use docvalues everywhere, to allow for norms of arbitrary size in the future (not just byte[])"
0,"deprecate Document.fields(), add getFields()A simple API improvement that I'm going to commit if nobody objects."
0,"[PATCH] Add StopFilter ignoreCase optionWanted to have the ability to ignore case in the stop filter.  In some cases, I
don't want to have to lower case before passing through the stop filter, b/c I
may need case preserved for other analysis further down the stream, yet I don't
need the stopwords and I don't want to have to apply stopword filters twice."
0,"[PATCH] Improved javadoc for maxClauseCountAs discussed on lucene-dev before, queries with lots of terms can use 
up a lot of unused buffer space for their TermDocs, because most terms 
have few documents."
0,"IndexWriter.getReader() allocates file handlesI am not sure if this is a ""bug"" or really just me not reading the Javadocs right...

The IR returned by IW.getReader() leaks file handles if you do not close() it, leading to starvation of the available file handles/process. If it was clear from the docs that this was a *new* reader and not some reference owned by the writer then this would probably be ok. But as I read the docs the reader is internally managed by the IW, which at first shot lead me to believe that I shouldn't close it.

So perhaps the docs should be amended to clearly state that this is a caller-owns reader that *must* be closed? Attaching a simple app that illustrates the problem."
0,"o.a.j.webdav.jcr.DavResourceFactoryImpl#createResource creates VersionControlledResource instances regardless of mix:versionable statusDavResourceFactoryImpl#createResource() first calls createResourceForItem() which threats all nodes as version-controlled. 
it then calls isVersionControlled() which indirectly triggers a call to Node#getVersionHistroy(). 
getVersionHistroy throws a UnsupportedRepositoryException if the node is non-versionable, leading to a DavException further up the call stack.

as a consequence, every request for a non-versionable node leads to unnecessary (and expensive) exception generation which could be avoided by checking the mix:versionable status of a node.


"
0,"DbClusterTest failure due to network configurationAs reported by Serge, the DbClusterTest case fails when run with certain network configuration.

Thomas already suggested a fix:

### Eclipse Workspace Patch 1.0
#P jackrabbit-core
Index: src/test/java/org/apache/jackrabbit/core/cluster/DbClusterTest.java
===================================================================
--- 
src/test/java/org/apache/jackrabbit/core/cluster/DbClusterTest.java (revisi
on 1067983)
+++ 
src/test/java/org/apache/jackrabbit/core/cluster/DbClusterTest.java (workin
g copy)
@@ -37,9 +37,9 @@
     public void setUp() throws Exception {
         deleteAll();
         server1 = Server.createTcpServer(""-tcpPort"", ""9001"", ""-baseDir"",
-                ""./target/dbClusterTest/db1"").start();
+                ""./target/dbClusterTest/db1"", ""-tcpAllowOthers"").start();
         server2 = Server.createTcpServer(""-tcpPort"", ""9002"", ""-baseDir"",
-                ""./target/dbClusterTest/db2"").start();
+                ""./target/dbClusterTest/db2"", ""-tcpAllowOthers"").start();
         FileUtils.copyFile(
                 new
File(""./src/test/resources/org/apache/jackrabbit/core/cluster/repository-h2
.xml""),
                 new File(""./target/dbClusterTest/node1/repository.xml""));
"
0,"LogMergePolicy should use the number of deleted docs when deciding which segments to mergeI found that IndexWriter.optimize(int) method does not pick up large segments with a lot of deletes even when most of the docs are deleted. And the existence of such segments affected the query performance significantly.

I created an index with 1 million docs, then went over all docs and updated a few thousand at a time.  I ran optimize(20) occasionally. What saw were large segments with most of docs deleted. Although these segments did not have valid docs they remained in the directory for a very long time until more segments with comparable or bigger sizes were created.

This is because LogMergePolicy.findMergeForOptimize uses the size of segments but does not take the number of deleted documents into consideration when it decides which segments to merge. So, a simple fix is to use the delete count to calibrate the segment size. I can create a patch for this.

"
0,Improved error reporting from JcrUtils.getRepositoryThe service provider mechanism and the null return value used by the RepositoryFactory API makes it a bit difficult to troubleshoot cases where a repository can not be accessed. It would be helpful if the JcrUtils.getRepository methods reported as accurate failure information as possible in case the requested repository is not found.
0,Codec is not consistently passed in internal APIWhile working on SOLR-1942 I ran into a couple of glitches with codec which is not consistently passed to SegmentsInfo and friends. Codecs should really be consistently passed though. I have fixed the pieces which lead to errors in Solr but I  guess there might be others too. Patch is coming up... 
0,"Improve performance of CharTermAttribute(Impl) and also fully implement AppendableThe Appendable.append(CharSequence) method in CharTermAttributes is good for general use. But like StringBuilder has for some common use cases specialized methods, this does the same and adds separate append methods for String, StringBuilder and CharTermAttribute itsself. This methods enable the compiler to directly link the specialized methods and don't use the instanceof checks. The unspecialized method only does the instanceof checks for longer CharSequences (>8 chars), else it simply iterates.

This patch also fixes the required special ""null"" handling. append() methods are required by Appendable to append ""null"", if the argument is null. I dont like this, but its required. Maybe we should document, that we dont dont support it. Otherwise, JDK's formatter fails with formatting null."
0,"Remove deprecated Scorer.explain(int) methodThis is the only remaining deprecation in core, but is not so easy to handle, because lot's of code in core still uses the explain() method in Scorer. So e.g. in PhraseQuery, the explain method has to be moved from Scorer to the Weight."
0,"Implement Connection TimeoutsI was writing test code to use the setSoTimeout(int millis) method to set a
timeout value when connecting to a URL.  It appears to me that no matter what I
set the timeout to be a HttpConnection will try to connect but uses some other
timeout value(I'm guessing the OS's default value).  I looked at the code for
HttpConnection and it uses the Socket(host,port) constructor which tries to
connect write away.  I'd like to suggest the following code below so the timeout
is set before first the connection is even made.

/* Compile the code as is and it should timeout within a sec.  If you
 * uncomment the first two lines after the try statement and comment
 * out the other socket connect statements and run the code again you will
 * notice write away that the timeout is something else because it connects
 * right away in the constructor.  Its like the timeout is worthless at this
 * point.  As a matter a fact the code should never get there.
 * This all assumes that 192.168.168.50 is not on your network.
 */

import java.io.*;
import java.net.*;

public class SocketTest {
    public static void main(String[] args) {
        long start = System.currentTimeMillis();

        try {
            //Socket socket = new Socket(""192.168.168.50"",80);
            //socket.setSoTimeout(1000);

            //Setting timeout before the connection is made.
            Socket socket = new Socket();
            InetSocketAddress sAddress =
                new InetSocketAddress(""192.168.168.50"",80);
            socket.connect(sAddress,1000);

        } catch (UnknownHostException e) {
            System.out.println(e);
        } catch (SocketException e) {
            System.out.println(e);
        } catch (IOException e) {
            System.out.println(e);
        }

        System.out.println(System.currentTimeMillis() - start);
    }
}"
0,"NodeStateMerger.merge should abort if the primary type of the 2 states to be compare are not the samethe NodeStateMerger#merge currently aborts if the mixin types of the passed state and its overlayed state are not equal.
as of jsr 283 not the only the mixin types but also the primary type of a node can be modified.

for consistency reasons NodeStateMerger#merge should abort and return false if the primary types are not the same."
0,"date encoding limitation removingcurrently there is some limitation to date encoding in lucene. I think it's 
because dates should preserve lexicografical ordering, i.e. if one date precedes 
another date then encoded values should keep same ordering.

I know that it can be difficult to integrate it into existing version but there 
is way to remove this limitation.
Date milliseconds can be encoded as unsigned values with prefix that indicates 
positive or negative value.

In more details:
I used hex encoding and prefix &#8216;p&#8217; and &#8216;n&#8217; for positive and negative values. I 
got following results:

Value -10000 is encoded with nffffffffffffd8f0, 
-100	- nffffffffffffff9c
0	- p0000000000000000
100	- p0000000000000064
10000	- p0000000000002710

This preserves ordering between values and theirs encoding.

Also hex encoding can be replaced with Character.MAX_RADIX encoding.

Part of code that do this work:
   final static char[] digits = {
	'0' , '1' , '2' , '3' , '4' , '5' ,
	'6' , '7' , '8' , '9' , 'a' , 'b' ,
	'c' , 'd' , 'e' , 'f' , 'g' , 'h' ,
	'i' , 'j' , 'k' , 'l' , 'm' , 'n' ,
	'o' , 'p' , 'q' , 'r' , 's' , 't' ,
	'u' , 'v' , 'w' , 'x' , 'y' , 'z'
    };


    char prefix;
    if (time >= 0) {
      prefix = 'p';
    } else {
      prefix = 'n';
    }

    char[] chars = new char[DATE_LEN + 1];
    int index = DATE_LEN;
    while (time != 0) {
      int b = (int) (time & 0x0F);
      chars[index--] = digits[b];
      time = time >>> 4;
    }

    while (index >= 0) {
      chars[index--] = '0';
    }
    chars[0] = prefix;

    return new String(chars);"
0,"Scorer.skipTo(current) remains on current for some scorersBackground in http://www.nabble.com/scorer.skipTo%28%29-contr-tf3880986.html

It appears that several scorers do not strictly follow the spec of Scorer.skipTo(n), and skip to current location remain in current location whereas the spec says: ""beyond current"". 

We should (probably) either relax the spec or fix the implementations."
0,"Add ability to specify compilation/matching flags to RegexCapabiltiies implementationsThe Jakarta Regexp and Java Util Regex packages both support the ability to provides flags that alter the matching behavior of a given regular expression. While the java.util.regex.Pattern implementation supports providing these flags as part of the regular expression string, the Jakarta Regexp implementation does not.  Therefore, this improvement request is to add the capability to provide those modification flags to either implementation. 

I've developed a working implementation that makes minor additions to the existing code. The default constructor is explicitly defined with no arguments, and then a new constructor with an additional ""int flags"" argument is provided. This provides complete backwards compatibility. For each RegexCapabilties implementation, the appropriate flags from the regular expression package is defined as  FLAGS_XXX static fields. These are pass through to the underlying implementation. They are re-defined to avoid bleeding the actual implementation classes into the caller namespace.

Proposed changes:

For the JavaUtilRegexCapabilities.java, the following is the changes made.

  private int flags = 0;
  
  // Define the optional flags from Pattern that can be used.
  // Do this here to keep Pattern contained within this class.
  
  public final int FLAG_CANON_EQ = Pattern.CANON_EQ;
  public final int FLAG_CASE_INSENSATIVE = Pattern.CASE_INSENSATIVE;
  public final int FLAG_COMMENTS = Pattern.COMMENTS;
  public final int FLAG_DOTALL = Pattern.DOTALL;
  public final int FLAG_LITERAL = Pattern.LITERAL;
  public final int FLAG_MULTILINE = Pattern.MULTILINE;
  public final int FLAG_UNICODE_CASE = Pattern.UNICODE_CASE;
  public final int FLAG_UNIX_LINES = Pattern.UNIX_LINES;
  
  /**
   * Default constructor that uses java.util.regex.Pattern 
   * with its default flags.
   */
  public JavaUtilRegexCapabilities()  {
    this.flags = 0;
  }
  
  /**
   * Constructor that allows for the modification of the flags that
   * the java.util.regex.Pattern will use to compile the regular expression.
   * This gives the user the ability to fine-tune how the regular expression 
   * to match the functionlity that they need. 
   * The {@link java.util.regex.Pattern Pattern} class supports specifying 
   * these fields via the regular expression text itself, but this gives the caller
   * another option to modify the behavior. Useful in cases where the regular expression text
   * cannot be modified, or if doing so is undesired.
   * 
   * @flags The flags that are ORed together.
   */
  public JavaUtilRegexCapabilities(int flags) {
    this.flags = flags;
  }
  
  public void compile(String pattern) {
    this.pattern = Pattern.compile(pattern, this.flags);
  }


For the JakartaRegexpCapabilties.java, the following is changed:

  private int flags = RE.MATCH_NORMAL;

  /**
   * Flag to specify normal, case-sensitive matching behaviour. This is the default.
   */
  public static final int FLAG_MATCH_NORMAL = RE.MATCH_NORMAL;
  
  /**
   * Flag to specify that matching should be case-independent (folded)
   */
  public static final int FLAG_MATCH_CASEINDEPENDENT = RE.MATCH_CASEINDEPENDENT;
 
  /**
   * Contructs a RegexCapabilities with the default MATCH_NORMAL match style.
   */
  public JakartaRegexpCapabilities() {}
  
  /**
   * Constructs a RegexCapabilities with the provided match flags.
   * Multiple flags should be ORed together.
   * 
   * @param flags The matching style
   */
  public JakartaRegexpCapabilities(int flags)
  {
    this.flags = flags;
  }
  
  public void compile(String pattern) {
    regexp = new RE(pattern, this.flags);
  }
"
0,"InstantiatedIndex - faster but memory consuming indexRepresented as a coupled graph of class instances, this all-in-memory index store implementation delivers search results up to a 100 times faster than the file-centric RAMDirectory at the cost of greater RAM consumption.

Performance seems to be a little bit better than log2n (binary search). No real data on that, just my eyes.

Populated with a single document InstantiatedIndex is almost, but not quite, as fast as MemoryIndex.    

At 20,000 document 10-50 characters long InstantiatedIndex outperforms RAMDirectory some 30x,
15x at 100 documents of 2000 charachters length,
and is linear to RAMDirectory at 10,000 documents of 2000 characters length.

Mileage may vary depending on term saturation.


"
0,"Remove build.xml from jackrabbit-coreAfter JCR-1203 the build.xml within jackrabbit-core contains only a single Ant task, that could just as well be moved into the pom.xml file to be run inline with the maven antrun plugin."
0,"Some improvements to contrib/benchmarkI've made some small improvements to the contrib/benchmark, mostly
merging in the ad-hoc benchmarking code I've been using in LUCENE-843:

  - Fixed thread safety of DirDocMaker's usage of SimpleDateFormat

  - Print the props in sorted order

  - Added new config ""autocommit=true|false"" to CreateIndexTask

  - Added new config ""ram.flush.mb=int"" to AddDocTask

  - Added new configs ""doc.term.vector.positions=true|false"" and
    ""doc.term.vector.offsets=true|false"" to BasicDocMaker

  - Added WriteLineDocTask.java, so you can make an alg that uses this
    to build up a single file containing one document per line in a
    single file.  EG this alg converts the reuters-out tree into a
    single file that has ~1000 bytes per body field, saved to
    work/reuters.1000.txt:

      docs.dir=reuters-out
      doc.maker=org.apache.lucene.benchmark.byTask.feeds.DirDocMaker
      line.file.out=work/reuters.1000.txt
      doc.maker.forever=false
      {WriteLineDoc(1000)}: *

    Each line has tab-separted TITLE, DATE, BODY fields.

  - Created feeds/LineDocMaker.java that creates documents read from
    the file created by WriteLineDocTask.java.  EG this alg indexes
    all documents created above:

      analyzer=org.apache.lucene.analysis.SimpleAnalyzer
      directory=FSDirectory
      doc.add.log.step=500

      docs.file=work/reuters.1000.txt
      doc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker
      doc.tokenized=true
      doc.maker.forever=false

      ResetSystemErase
      CreateIndex
      {AddDoc}: *
      CloseIndex

      RepSumByPref AddDoc

I'll attach initial patch shortly.
"
0,"Improve Spatial Utility like classes- DistanceUnits can be improved by giving functionality to the enum, such as being able to convert between different units, and adding tests.  

- GeoHashUtils can be improved through some code tidying, documentation, and tests.

- SpatialConstants allows us to move all constants, such as the radii and circumferences of Earth, to a single consistent location that we can then use throughout the contrib.  This also allows us to improve the transparency of calculations done in the contrib, as users of the contrib can easily see the values being used.  Currently this issues does not migrate classes to use these constants, that will happen in issues related to the appropriate classes."
0,"A Linux-specific Directory impl that bypasses the buffer cacheI've been testing how we could prevent Lucene's merges from evicting
pages from the OS's buffer cache.  I tried fadvise/madvise (via JNI)
but (frustratingly), I could not get them to work (details at
http://chbits.blogspot.com/2010/06/lucene-and-fadvisemadvise.html).

The only thing that worked was to use Linux's O_DIRECT flag, which
forces all IO to bypass the buffer cache entirely... so I created a
Linux-specific Directory impl to do this.
"
0,"Position based TermVectorMapperAs part of the new TermVectorMapper approach to TermVectors, the ensuing patch loads term vectors and stores the term info by position.  This should let people directly index into a term vector given a position.  Actually, it does it through Maps, b/c the array based bookkeeping is a pain given the way positions are stored.  

The map looks like:
Map<String,   Map<Integer, TVPositionInfo>>

where the String is the field name, the integer is the position, and TVPositionInfo is a storage mechanism for the terms and offsets that occur at a position.  It _should_ handle multiple terms per position (which is always my downfall! )

I have not tested performance of this approach.
"
0,JSR 283: Locking
0,"Enhance SnapshotDeletionPolicy to allow taking multiple snapshotsA spin off from here: http://www.gossamer-threads.com/lists/lucene/java-dev/99161?do=post_view_threaded#99161

I will:
# Replace snapshot() with snapshot(String), so that one can name/identify the snapshot
# Add some supporting methods, like release(String), getSnapshots() etc.
# Some unit tests of course.

This is mostly written already - I want to contribute it. I've also written a PersistentSDP, which persists the snapshots on stable storage (a Lucene index in this case) to support opening an IW with existing snapshots already, so they don't get deleted. If it's interesting, I can contribute it as well.

Porting my patch to the new API. Should post it soon."
0,"Consolidate Lucene's QueryParsers into a moduleLucene has a lot of QueryParsers and we should have them all in a single consistent place.  

The following are QueryParsers I can find that warrant moving to the new module:

- Lucene Core's QueryParser
- AnalyzingQueryParser
- ComplexPhraseQueryParser
- ExtendableQueryParser
- Surround's QueryParser
- PrecedenceQueryParser
- StandardQueryParser
- XML-Query-Parser's CoreParser

All seem to do a good job at their kind of parsing with extensive tests.

One challenge of consolidating these is that many tests use Lucene Core's QueryParser.  One option is to just replicate this class in src/test and call it TestingQueryParser.  Another option is to convert all tests over to programmatically building their queries (seems like alot of work)."
0,"consolidate FieldCache and ExtendedFieldCache instancesIt's confusing and error prone having two instances of FieldCache... FieldCache .DEFAULT and ExtendedFieldCache .EXT_DEFAULT.
Accidentally use the wrong one and you silently double the memory usage for that field.  Since ExtendedFieldCache extends FieldCache, there's no reason not to share the same instance across both."
0,"Some small javadocs/extra import fixesTwo things that Uwe Schindler caught, plus fixes for javadoc warnings in core.  I plan to commit to trunk & 2.4."
0,Optimize queries with relative path in order by clauseThis is a follow up to JCR-800 and adds a way to configure relative property paths for aggregates in the indexing configuration. Aggregated properties are handled much more efficiently when used in an order by. The implementation from JCR-800 is used as a fallback when no aggregate is configured. See attached patch for details.
0,"WikipediaTokenizer needs a way of not tokenizing certain parts of the textIt would be nice if the WikipediaTokenizer had a way of, via a flag, leaving categories, links, etc. as single tokens (or at least some parts of them)

Thus, if we came across [[Category:My Big Idea]] there would be a way of outputting, as a single token ""My Big Idea"".  

Optionally, it would be good to output both ""My Big Idea"" and the individual tokens as well.

I am not sure of how to do this in JFlex, so any insight would be appreciated."
0,IndexWriter has incomplete JavadocsA couple of getter methods in IndexWriter have no javadocs.
0,Expose namespace registry via workspace instead via session in spi2jcrspi2jcr/SessionInfoImpl.getNamespaceResolver() returns the namespace registry through the current session of the wrapped repository. Since session scoped namespace remapping is not visible to the SPI I think the method should return the namespace registry through the current workspace. 
0,"Convert NumericUtils and NumericTokenStream to use BytesRef instead of Strings/char[]After LUCENE-2302, we should use TermToBytesRefAttribute to index using NumericTokenStream. This also should convert the whole NumericUtils to use BytesRef when converting numerics."
0,spi2davex NodeInfoImpl should use HashSet instead of ArrayList for childInfosThe subsequent contains call is prohibitively expensive since it returns in an equals call for all existing child infos. 
0,"crank up faceting module testsThe faceting module has a large set of good tests.

lets switch them over to use all of our test infra (randomindexwriter, random iwconfig, mockanalyzer, newDirectory, ...)
I don't want to address multipliers and atLeast() etc on this issue, I think we should follow up with that on a separate issue, that also looks at speed and making sure the nightly build is exhaustive.

for now, lets just get the coverage in, it will be good to do before any refactoring.
"
0,"Break the spi2dav dependency to jcr-serverCurrently the spi2dav component has a dependency on the jcr-server component, which is troublesome due to the extra transitive dependencies and which strictly speaking should not be necessary from an architectural point of view.

The dependency exists mostly for sharing a number of JCR-specific WebDAV constants. I'd like to push those constants down to jackrabbit-webdav as they are essentially just shared strings and as jackrabbit-webdav already contains a number of constants used by JCR extensions.

In addition to constant values, code in the following classes is shared between jcr-server and spi2dav: JcrValueType, NamespacesProperty, NodeTypesProperty, SearchResultProperty, SubscriptionImpl, ValuesProperty. The shared code in JcrValueType and SubscriptionImpl is mostly just about mapping constant value mappings and could fairly easily be moved to jackrabbit-webdav. The Property classes are a but trickier, but it looks like it would be possible to split the code to separate server- and client-side classes for jcr-server and spi2dav."
0,Remove deprecated methods in PriorityQueue
0,"SPI: RepositoryService.getItemInfos should be allowed to return entries outside of the requested tree.michael duerig asked for for that extension of the semantic of RepositoryService.getItemInfos.
currently this doesn't work and leads to an inconsistent hierarchy."
0,"Add test for Node.restore() may throw InvalidStateExceptionAdd a unit test for JCR-1399 in the 1.3 branch.

A test for the original feature in the trunk/1.4 (JCR-1197) needs a separate issue. "
0,"Changes.html not explicitly included in releaseNone of the release related ant targets explicitly call cahnges-to-html ... this seems like an oversight.  (currently it's only called as part of the nightly target)

"
0,"[PATCH] FSDirectory create() method deletes all fileshi all,

the current implementation of FSDirectory.create(...) silently deletes all files
(even empty directories) within the index directory when setting up a new index
with create option enabled. Lucene doesn't care when deleting files in the index
directory if they  belong to lucene or not. I don't think that this is a real
bug, but it can be a pain if somebody whants to store some private information
in the lucene index directory, e.g some configuration files.

Therefore i implemented a FileFilter which knows about the internal lucene file
extensions, so that all other files would never get touched when creating a new
index. The current patch is an enhancement in FSDirectory only. I don't think
that there is a need to make it available in the Directory class and change all
it's depending classes.

regards
Bernhard"
0,NodeBasedGroup#isMember(Principal) should have shortcut for the everyone group.
0,"Add getIndexCommit method to IndexReaderSpinoff from this thread:

  http://markmail.org/message/bojgqfgyxkkv4fyb

I think it makes sense ask an IndexReader for the commit point it has
open.  This enables the use case described in the above thread, which
is to create a deletion policy that is able to query all open readers
for what commit points they are using, and prevent deletion of them.

"
0,Add Rewriteable Support to SortField.toStringI missed adding support for the new Rewriteable SortField type to toString().
0,cutover oal.index.* tests to use a random IWC to tease out bugs
0,Remove remaining deprecations from indexer package
0,"gcj ant target doesn't work on windowsIn order to fix it I made two changes, both really simple.

First I added to org/apache/lucene/store/GCJIndexInput.cc some code to use windows memory-mapped I/O instead than unix mmap().

Then I had to rearrange the link order in the Makefile in order to avoid unresolved symbol errors. Also to build repeatedly I had to instruct make to ignore the return code for the mkdir command as on windows it fails if the directory already exists.

I'm attaching two patches corresponding to the changes; please note that with the patches applied, the gcj target still works on linux. Both patches apply cleanly to the current svn head."
0,"Add User#changePassword(String newPw, String oldPw)... where the oldPw must match in order to have the password of the user successfully changed.

while this could be done by applications with quite some effort, the implementation can easily achieve this
as the functionality required is already present."
0,'ant javacc' in root project should also properly create contrib/surround Java filesFor consistency after LUCENE-1829 which did the same for contrib/queryparser
0,"Make shutdown hooks in TransientFileFactory removableTransientFileFactory class always registers shutdown hook. So, if jackrabbit classes were loaded by web-app classloader, they will not be released when web-app is undeployed (if jackrabbit-jcr-commons JAR is inside WAR). This causes classloader leak.
It seems to be useful to have ability to cancel TransientFileFactory's shutdown hook when application is going to be unloaded to avoid classloader leak."
0,"Missing jackrabbit-rmi-service.xml from jackrabbit-jcr-rmi-1.2.1.jarThe file jackrabbit-rmi-service.xml is missing from the jackrabbit-jcr-rmi-1.2.1.jar.

The cause of the issue appears that the directory structure of the jackrabbit-jcr-rmi sub-project doesn't match the Maven 2 standard.  

To fix: src/resources should be moved to src/main/resources."
0,"Access to version history results in reading all versions of versionable nodeInternalVersionHistoryImpl loads all versions at once during initialization. Because of that all versioning operations (incl. checkin, label, restore) are significantly slower when node has many versions.

"
0,"Path should implement SerializableQName already implements Serializable, for ease of use Path should also support Serializable."
0,"Generalize SearcherManagerI'd like to generalize SearcherManager to a class which can manage instances of a certain type of interfaces. The reason is that today SearcherManager knows how to handle IndexSearcher instances. I have a SearcherManager which manages a pair of IndexSearcher and TaxonomyReader pair.

Recently, few concurrency bugs were fixed in SearcherManager, and I realized that I need to apply them to my version as well. Which led me to think why can't we have an SM version which is generic enough so that both my version and Lucene's can benefit from?

The way I see SearcherManager, it can be divided into two parts: (1) the part that manages the logic of acquire/release/maybeReopen (i.e., ensureOpen, protect from concurrency stuff etc.), and (2) the part which handles IndexSearcher, or my SearcherTaxoPair. I'm thinking that if we'll have an interface with incRef/decRef/tryIncRef/maybeRefresh, we can make SearcherManager a generic class which handles this interface.

I will post a patch with the initial idea, and we can continue from there."
0,"Add ShingleFilter option to output unigrams if no shingles can be generatedCurrently if ShingleFilter.outputUnigrams==false and the underlying token stream is only one token long, then ShingleFilter.next() won't return any tokens. This patch provides a new option, outputUnigramIfNoNgrams; if this option is set and the underlying stream is only one token long, then ShingleFilter will return that token, regardless of the setting of outputUnigrams.

My use case here is speeding up phrase queries. The technique is as follows:

First, doing index-time analysis using ShingleFilter (using outputUnigrams==true), thereby expanding things as follows:

""please divide this sentence into shingles"" ->
 ""please"", ""please divide""
 ""divide"", ""divide this""
 ""this"", ""this sentence""
 ""sentence"", ""sentence into""
 ""into"", ""into shingles""
 ""shingles""

Second, do query-time analysis using ShingleFilter (using outputUnigrams==false and outputUnigramIfNoNgrams==true). If the user enters a phrase query, it will get tokenized in the following manner:

""please divide this sentence into shingles"" ->
 ""please divide""
 ""divide this""
 ""this sentence""
 ""sentence into""
 ""into shingles""

By doing phrase queries with bigrams like this, I can gain a very considerable speedup. Without the outputUnigramIfNoNgrams option, then a single word query would tokenize like this:

""please"" ->
   [no tokens]

But thanks to outputUnigramIfNoNgrams, single words will now tokenize like this:

""please"" ->
  ""please""

****

The patch also adds a little to the pre-outputUnigramIfNoNgrams option tests.

****

I'm not sure if the patch in this state is useful to anyone else, but I thought I should throw it up here and try to find out.
"
0,"All implementations of SchemeSocketFactory.createSocket(HttpParams params) ignore the paramsOnly TestTSCCMWithServer.StallingSocketFactory.createSocket(HttpParams params) ever uses the HttpParams parameter.

All non-test implementations of the method ignore the parameter.

Not sure why this version of the method exists if the parameter is never used - the parameterless method from SocketFactory could be used instead."
0,"FST should allow controlling how hard builder tries to share suffixesToday we have a boolean option to the FST builder telling it whether
it should share suffixes.

If you turn this off, building is much faster, uses much less RAM, and
the resulting FST is a prefix trie.  But, the FST is larger than it
needs to be.  When it's on, the builder maintains a node hash holding
every node seen so far in the FST -- this uses up RAM and slows things
down.

On a dataset that Elmer (see java-user thread ""Autocompletion on large
index"" on Jul 6 2011) provided (thank you!), which is 1.32 M titles
avg 67.3 chars per title, building with suffix sharing on took 22.5
seconds, required 1.25 GB heap, and produced 91.6 MB FST.  With suffix
sharing off, it was 8.2 seconds, 450 MB heap and 129 MB FST.

I think we should allow this boolean to be shade-of-gray instead:
usually, how well suffixes can share is a function of how far they are
from the end of the string, so, by adding a tunable N to only share
when suffix length < N, we can let caller make reasonable tradeoffs. 
"
0,"improve performance of contrib/TestCompoundWordTokenFiltercontrib/analyzers/compound has some tests that use a hyphenation grammar file.

The tests are currently for german, and they actually are nice, they show how the combination of the hyphenation rules and dictionary work in tandem.
The issue is that the german grammar file is not apache licensed: http://offo.sourceforge.net/hyphenation/licenses.html
So the test must download the entire offo zip file from sourceforge to execute.

I happen to think the test is a great example of how this thing works (with a language where it matters), but we could consider using a different grammar file, for a language that is apache licensed.
This way it could be included in the source with the test and would be more practical.
"
0,"Move XML QueryParser to queryparser moduleThe XML QueryParser will be ported across to queryparser module.

As part of this work, we'll move the QP's demo into the demo module."
0,SQL2: Implement LIKE support for node namesDoing a LIKE constraint on the local name of a node that throws javax.jcr.UnsupportedRepositoryOperationException.
0,"Mark pending nodes in IndexingQueue directly in indexThe index currently writes an indexing_queue.log file which contains all nodes that timed out while text was extracted. Instead, the index itself should mark an indexed node as pending. This is more robust because no additional file must be written."
0,"Some improvements to _TestUtil and its usageI've started this issue because I've noticed that _TestUtil.getRandomMultiplier() is called from many loops' condition check, sometimes hundreds and thousands of times. Each time it does Integer.parseInt after calling System.getProperty. This really can become a constant IMO, either in LuceneTestCase(J4) or _TestUtil, as it's not expected to change while tests are running ...

I then reviewed the class and spotted some more things that I think can be fixed/improved:
# getTestCodec() can become a constant as well
# arrayToString is marked deprecated. I've checked an no one calls them, so I'll delete them. This is a 4.0 code branch + a test-only class. No need to deprecate anything.
# getTempDir calls new Random(), instead of newRandom() in LuceneTestCaseJ4, which means that if something fails, we won't know the random seed used ...
#* In that regard, we might want to output all the classes that obtained a static seed in reportAdditionalFailures(), instead of just the class that ran the test.
# rmDir(String) can be removed IMO, and leave only rmDir(File)
# I suggest we include some recursion in rmDir(File) to handle the deletion of nested directories.
#* Also, it does not check whether the dir deletion itself succeeds (but it does so for the files). This can bite us on Windows, if some test did not close things properly.

I'll work out a patch."
0,"Restore top level disjunction performanceThis patch restores the performance of top level disjunctions. 
The introduction of BooleanScorer2 had impacted this as reported
on java-user on 21 Nov 2006 by Stanislav Jordanov.
"
0,"Allow whitespaces in base64 encoded binary fields of XML import filesWhen importing files using Session.importXML(), the Binary property values are Base64 encoded.  However you cannot put whitespaces in them, and XML files with binaries in them become very long lines.  The files are more manageable if whilespaces could be put in them, as is common to do in base base64 encoded files."
0,"questionable default value for BufferedOutputStream size in HttpConnectionFrom the dev list

--

Hi Eric

Thanks for bringing this up. HttpClient 3.0 allows for parameterization
of SO_SNDBUF and SO_RCVBUF settings. For HttpClient 2.0 (as well as for
3.0 when falling back onto the system defaults), however, it would make
sense to set a cap on the size of the send and receive buffers.

Feel free to open a ticket for this issue with Bugzilla

Oleg


On Fri, 2004-07-02 at 18:39, Eric Bloch wrote:

>> Hi httpclient folks,
>> 
>> I've been looking at 2.0 source code and the default value for the 
>> BufferedOutputStream that is used in an HttpConnectionn is coming from 
>> socket.getSendBufferSize().  My hunch, is that, in general, this is 
>> bigger than you'd want.
>> 
>> Most HTTP ""sends"" are less than 1KByte ('cept for big POSTs).
>> The default value I get for socket.getSendBufferSize for this is 8192.
>> I would think a better default for this buffer would be 1K, no?
>> 
>> Also, fyi, if someone happens to dork the system send buffer size hi 
>> (say MB) and you are using the MultiThreadedConnectionManager in 2.0 
>> (dunno about 3.0), you will use up a lot of memory for each connection 
>> since the pool doesn't let idle connections (or their buffers) be gced. 
>>   I just got bit bad by that.
>> 
>> -Eric
>> 
>"
0,"Deprecate IndexModifierSee discussion at http://www.gossamer-threads.com/lists/lucene/java-dev/52017?search_string=deprecating%20indexmodifier;#52017

This is to deprecate IndexModifier before 3.0 and remove it in 3.0.

This patch includes:
  1 IndexModifier and TestIndexModifier are deprecated.
  2 TestIndexWriterModify is added. It is similar to TestIndexModifer but uses IndexWriter and has a few other changes. The changes are because of the difference between IndexModifier and IndexWriter.
  3 TestIndexWriterLockRelease and TestStressIndexing are switched to use IndexWriter instead of IndexModifier."
0,"davex remoting has  a performance bottleneck due limit of 2 http connectionsThe spi2dav service implementation use of HttpClient did not support configuration of the maximum amount of http connections to the server.  The default value, in the HttpClient code, is two. This was a performance bottleneck.  This work makes the number of connections configurable via a parameter to the map passed to the repository factory.  

It also fixes a concurrency issue which was exposed by the increased concurrency effected by this work.  This fix is a replacement of a HashMap cache of client connections with a ConcurrentHashMap, thanks to the java 1.5 available in Jackrabbit 2.x

USAGE: 
Set the number of connections (Spi2davRepositoryServiceFactory.PARAM_MAX_HTTP_CONNECTIONS) when creating a factory via the dav or davex rep factories.  Default is 20.

NOTE: 
See also the server side fixes: JCR-3027  The patch on that ticket allows configuration of the concurrency level on the server, which should be tuned in conjunction with the client side connection levels.  
 "
0,"Pre-analyzed fieldsAdds the possibility to set a TokenStream at Field constrution time, available as tokenStreamValue in addition to stringValue, readerValue and binaryValue.

There might be some problems with mixing stored fields with the same name as a field with tokenStreamValue."
0,"preflex codec doesn't order terms correctlyThe surrogate dance in the preflex codec (which must dynamically remap terms from UTF16 order to unicode code point order) is buggy.

To better test it, I want to add a test-only codec, preflexrw, that is able to write indices in the pre-flex format.  Then we should also fix tests to randomly pick codecs (including preflexrw) so we better test all of our codecs."
0,"Scan method signatures and add varargs where possibleI changed a lot of signatures, but there may be more. The important ones like MultiReader and MultiSearcher are already done. This applies also to contrib. Varargs are no backwards break, they stay arrays as before."
0,"shouldn't throw exception on bad cookiesCurrently, HttpClient throws Exception on bad cookie. This is not expected. The 
user will expect HttpClient to ignore such cookies, but not getting an 
exception. Once exception is throw, user has no way to know if he can continue."
0,"In J2SDK 1.5.0 (Tiger) enum is a keywordHi!

Tiger adds extensions to the Java Programming Language (JSR201). One is
""Enumerations"", which required to add the new keyword enum.

I just made a grep (grep -lrw) over some sources and found some Apache projects
using enum as a word.

To be compliant with the new specification, please check that enum is not used
as a variable, field or method name.

Regards,
Robert"
0,"factor CharTokenizer/CharacterUtils into analyzers moduleCurrently these analysis components are in the lucene core, but should really
be .util in the analyzers module.

Also, with MockTokenizer extending Tokenizer directly, we can add some additional
checks in the future to try to ensure our consumers are being good consumers (e.g. calling reset).

This is mentioned in http://wiki.apache.org/lucene-java/TestIdeas, I didn't implement it here yet,
this is just the factoring. I think we should try to do this before LUCENE-3040.
"
0,"NetscapeDraftSpec is too strict about cookie expires date formatThe Netscape Draft specification (http://curl.haxx.se/rfc/cookie_spec.html) specifies clearly that the date format for Set-Cookie expires is ""Wdy, DD-Mon-YYYY HH:MM:SS GMT"". But on the other hand, in the examples section of the same document, the only example header that contains ""Expires"" is the following:

Set-Cookie: CUSTOMER=WILE_E_COYOTE; path=/; expires=Wednesday, 09-Nov-99 23:12:40 GMT

Note that the weekday is fully spelled out and that the year is written as two digits only. I would say that the specification therefore makes the 2 or 4 digit year optional. I think NetscapeDraftSpec should reflect this. An example of a product that uses the 2 digit version is jetty 6 and 7. When using httpclient 4 talking to a jetty server, any Set-Cookie headers for persistent cookies will be interpreted as a 4 digit year in the date and the cookie will immediately be disregarded as expired by some 2,000 years or so. Httpclient 3 on the other hand had no problem understanding the persistent cookies from jetty. I filed a bug report https://bugs.eclipse.org/bugs/show_bug.cgi?id=304698 on jetty to change their date format, but on the other hand I also think httpclient 4 is too strict about the date format when even the original specification uses two alternatives.

Workaround is easy by setting CookieSpecPNames.DATE_PATTERNS, but I really think that projects like jetty and httpclient should be compatible by default. Also, since the date format used by jetty is parsable but misinterpreted and disregarded by httpclient makes it especially hard to detect the first time on encounters the problem."
0,"Lazy initialize ItemDefinitionThe item definition is currently set immediately when an ItemData is instantiated. Accessing nodes usually does not require reading the item definition, thus it is not necessary to load/set it that early.

Lazy initialization also has the benefit that content migration in an upgrade scenario becomes easier. Instead of throwing an exception early, jackrabbit could allow access to the item until an item definition is really required for the operation."
0,"Cloned SegmentReaders fail to share FieldCache entriesI just hit this on LUCENE-1516, which returns a cloned readOnly
readers from IndexWriter.

The problem is, when cloning, we create a new [thin] cloned
SegmentReader for each segment.  FieldCache keys directly off this
object, so if you clone the reader and do a search that requires the
FieldCache (eg, sorting) then that first search is always very slow
because every single segment is reloading the FieldCache.

This is of course a complete showstopper for LUCENE-1516.

With LUCENE-831 we'll switch to a new FieldCache API; we should ensure
this bug is not present there.  We should also fix the bug in the
current FieldCache API since for 2.9, users may hit this.
"
0,"Enhance Ingres persistence bundle to handle unicodeTiny change to ingres.ddl for persistent bundles to handle unicode strings.

"
0,"add shutdown() or logoutAll() method to TransientRepositoryIt would be usefull to be able to explicitly ask a TransientRepository to shut down, instead of relying on all sessions to be closed by session.logout()."
0,"Spellchecker should take IndexWriterConfig... deprecate old methods?When looking at LUCENE-3490, i realized there was no way to specify the codec for the spellchecker to use.

It has the following current methods:
* indexDictionary(Dictionary dict): this causes optimize!
* indexDictionary(Dictionary dict, int mergeFactory, int ramMB): this causes optimize!
* indexDictionary(Dictionary dict, int mergeFactor, int ramMB, boolean optimize)

But no way to specify an IndexwriterConfig. Additionally, I don't like that several of these ctors force an optimize in a tricky way,
even though it was like this all along.

So I think we should add indexDictionary(Dictionary dict, IndexWriterConfig config, boolean optimize).

We should either deprecate all the other ctors in 3.x and nuke in trunk, or at least add warnings to the ones that optimize."
0,"Make WordDelimiterFilter's instantiation more readableCurrently WordDelimiterFilter's constructor is:

{code}
public WordDelimiterFilter(TokenStream in,
	                             byte[] charTypeTable,
	                             int generateWordParts,
	                             int generateNumberParts,
	                             int catenateWords,
	                             int catenateNumbers,
	                             int catenateAll,
	                             int splitOnCaseChange,
	                             int preserveOriginal,
	                             int splitOnNumerics,
	                             int stemEnglishPossessive,
	                             CharArraySet protWords) {
{code}

which means its instantiation is an unreadable combination of 1s and 0s.  

We should improve this by either using a Builder, 'int flags' or an EnumSet."
0,"TCK: SessionReadMethodsTest#testIsLive calls logout() more than onceSessionReadMethodsTest#testIsLive calls logout more than once in a session (once in the test, once in tearDown).  JSR-170 doesn't prohibit an implementation from throwing an unchecked exception (such as IllegalStateException) if logout is called more than once.

Proposal: change tearDown to test isLive before calling logout.

--- SessionReadMethodsTest.java (revision 422074)
+++ SessionReadMethodsTest.java (working copy)
@@ -57,7 +57,7 @@
      * Releases the session aquired in {@link #setUp()}.
      */
     protected void tearDown() throws Exception {
-        if (session != null) {
+        if (session != null && session.isLive()) {
             session.logout();
         }
         super.tearDown();
"
0,"Make DocsEnum subclass of DocIdSetIteratorSpinoff from LUCENE-1458:

One thing I came along long time ago, but now with a new API it get's interesting again: 
DocsEnum should extend DocIdSetIterator, that would make it simplier to use and implement e.g. in MatchAllDocQuery.Scorer, FieldCacheRangeFilter and so on. You could e.g. write a filter for all documents that simply returns the docs enumeration from IndexReader.

So it should be an abstract class that extends DocIdSetIterator. It has the same methods, only some methods must be a little bit renamed. The problem is, because java does not support multiple inheritace, we cannot also extends attributesource  Would DocIdSetIterator be an interface it would work (this is one of the cases where interfaces for really simple patterns can be used, like iterators).

The problem with multiple inheritance could be solved by an additional method attributes() that creates a new AttributeSource on first access then (because constructing an AttributeSource is costly).  The same applies for the other *Enums, it should be separated for lazy init.

DocsEnum could look like this:

{code}
public abstract class DocsEnum extends DocIdSetIterator {
  private AttributeSource atts = null;
  public int freq()
  public DontKnowClassName positions()
  public final AttributeSource attributes() {
   if (atts==null) atts=new AttributeSource();
   return atts;
  }
  ...default impl of the bulk access using the abstract methods from DocIdSetIterator
}
{code}
"
0,"Observation tests should throw NotExecutableException when repository does not support observationThe observation tests should throw NotExecutableException when repository does not support observation.
"
0,"Add an explicit method to invoke IndexDeletionPolicyToday, if one uses an IDP which holds onto segments, such as SnapshotDeletionPolicy, or any other IDP in the tests, those segments are left in the index even if the IDP no longer references them, until IW.commit() is called (and actually does something). I'd like to add a specific method to IW which will invoke the IDP's logic and get rid of the unused segments w/o forcing the user to call IW.commit(). There are a couple of reasons for that:

* Segments take up sometimes valuable HD space, and the application may wish to reclaim that space immediately. In some scenarios, the index is updated once in several hours (or even days), and waiting until then may not be acceptable.
* I think it's a cleaner solution than waiting for the next commit() to happen. One can still wait for it if one wants, but otherwise it will give you the ability to immediately get rid of those segments.
* TestSnapshotDeletionPolicy includes this code, which only strengthens (IMO) the need for such method:
{code}
// Add one more document to force writer to commit a
// final segment, so deletion policy has a chance to
// delete again:
Document doc = new Document();
doc.add(new Field(""content"", ""aaa"", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
writer.addDocument(doc);
{code}

If IW had an explicit method, that code would not need to exist there at all ...

Here comes the fun part - naming the baby:
* invokeDeletionPolicy -- describes exactly what is going to happen. However, if the user did not set IDP at all (relying on default, which I think many do), users won't understand what is it.
* deleteUnusedSegments - more user-friendly, assuming users understand what 'segments' are.

BTW, IW already has deleteUnusedFiles() which only tries to delete unreferenced files that failed to delete before (such as on Windows, due to e.g. open readers). Perhaps instead of inventing a new name, we can change IW.deleteUnusedFiles to call IndexFileDeleter.checkpoint (instead of deletePendingFiles) which deletes those files + calls IDP.onCommit()."
0,"Better name and path factory exception messagesI've ran across a few cases where the name and path factories throw an exception about an invalid path or name, but fail to include the actual path or name in the exception message. It would be very helpful to have that extra bit of information included."
0,Backport FSTs to 3.x
0,"PayloadNearQuery has hardwired explanation for 'AveragePayloadFunction'The 'explain' method in PayloadNearSpanScorer assumes the AveragePayloadFunction was used. This patch adds the 'explain' method to the 'PayloadFunction' interface, where the Scorer can call it. Added unit tests for 'explain' and for {Min,Max}PayloadFunction."
0,"convert automaton to char[] based processing and TermRef / TermsEnum apiThe automaton processing is currently done with String, mostly because TermEnum is based on String.
it is easy to change the processing to work with char[], since behind the scenes this is used anyway.

in general I think we should make sure char[] based processing is exposed in the automaton pkg anyway, for things like pattern-based tokenizers and such.
"
0,HttpClient 'ParamBeans' for easier configurationAs I did for a 'core' here I would like to contribute for 'client' part as few 'ParamBeans' for easier external configuration... Any comment or improvement is very welcome...
0,"indexing-rules should allow wildcards for (global) property nameseg:

<indexing-rule nodeType=""*"">
  <property>text</property>
  <property>*Text</property>
</indexing-rule>

defines that all properties named 'text' and all that end with 'Text' should be fulltext indexed.
if the property name includes namespace prefixes, wildcards are only allowed for 'any' namespace. eg:

*:title

but not: j*:title
"
0,"Provide access to cluster recordsCluster records are read/written inside o.a.j.core.cluster.ClusterNode in private methods. In order to support tools such as a journal walker that would display human readable descriptions of cluster records, these inner workings should be made public. "
0,"Move jackrabbit/trunk/contrib to jackrabbit/sandboxAs discussed on the mailing list (see http://www.nabble.com/Moving-contrib-outside-trunk-and-rename-to-sandbox-tf4635301.html), we should do the following:

    svn move https://svn.apache.org/repos/asf/jackrabbit/trunk/contrib https://svn.apache.org/repos/asf/jackrabbit/sandbox

I will do this in a few days unless anyone objects."
0,"SPI: Get rid of unused method ItemInfo.getParentId()Looking at the various SPI impls in the trunk and in the sandbox reveals that ItemInfo.getParentId is not used at all.
I'd like to suggest to get rid of that method.

Any objections/concerns?
angela

"
0,Query Stats should use the TimeSeries mechanismRefactor the Query Stats to use TimeSeries for the average query duration.
0,spi2davex: reduce memory footprint of Node/PropertyInfoImplthe in-memory footprint of o.a.jackrabbit.spi2davex.NodeinfoImpl & PropertyInfoImp is quite big. 
0,"HuperDuperSynonymsFilterThe current synonymsfilter uses a lot of ram and cpu, especially at build time.

I think yesterday I heard about ""huge synonyms files"" three times.

So, I think we should use an FST-based structure, sharing the inputs and outputs.
And we should be more efficient with the tokenStream api, e.g. using save/restoreState instead of cloneAttributes()
"
0,"loadURI compile error with Maven 1.0.2As reported on the mailing list by Ashley Martens:

----
C:\apache\jackrabbit-contrib\nt-ns-util>maven
 __  __
|  \/  |__ _Apache__ ___
| |\/| / _` \ V / -_) ' \  ~ intelligent projects ~
|_|  |_\__,_|\_/\___|_||_|  v. 1.0.2

Attempting to download jackrabbit-1.0-SNAPSHOT.jar.
Artifact /org.apache.jackrabbit/jars/jackrabbit-1.0-SNAPSHOT.jar doesn't exists in remote repository, but it exists locally
Attempting to download jackrabbit-commons-1.0-SNAPSHOT.jar.
Artifact /org.apache.jackrabbit/jars/jackrabbit-commons-1.0-SNAPSHOT.jar doesn't exists in remote repository, but it exists locally
build:start:

java:prepare-filesystem:

java:compile:
   [echo] Compiling to C:\apache\jackrabbit-contrib\nt-ns-util/target/classes
   [javac] Compiling 1 source file to C:\apache\jackrabbit-contrib\nt-ns-util\target\classes
C:\apache\jackrabbit-contrib\nt-ns-util\src\main\java\org\apache\jackrabbit\util\nodetype\SchemaConverter.java:71: cannot resolve symbol
symbol  : method loadURI (java.lang.String)
location: class org.apache.xerces.impl.xs.XMLSchemaLoader
       XSModel xsModel = loader.loadURI(uri);
                               ^
1 error

BUILD FAILED
File...... C:\Documents and Settings\ashleym\.maven\cache\maven-java-plugin-1.5\plugin.jelly
Element... ant:javac
Line...... 63
Column.... 48
Compile failed; see the compiler error output for details.
Total time: 8 seconds
Finished at: Mon Jan 02 10:40:47 EST 2006
----

Peeter Piegaze found out the problem:

----
I was able to build it without a problem using maven-1.1-beta-2 and JDK 1.4.2.

However, it sounds to me like in your case maven has set up its
on-build classpath so that it sees the older xerces-2.4.0.jar before
the new xerxesImpl.-2.6.2.jar. Maven seems to download the old
xerces-2.4.0 into its repository for internal use, while my code uses
the newer xerxesImpl-2.6.2.jar. The old jar overlaps class-wise with
the new one, but the new one implements the additional loadURI method
(among others).

I am not sure exactly why your maven build process is looking in the
wrong jar. But that is what is doing, almost certainly.
----
"
0,"Remove JakarteRegExCapabilities shim to access package protected fieldTo access the prefix in Jakarta RegExes we use a shim class in the same package as jakarta. I will remove this and replace by reflection like Robert does in his ICUTokenizer rule compiler.

Shim classes have the problem wth signed artifacts, as you cannot insert a new class into a foreign package if you sign regex classes.

This shim-removal also allows users to use later jakarta regex versions, if they are in classpath and cannot be removed (even if they have bugs). Performance is no problem, as the prefix is only get once per TermEnum."
0,"Helper Method to escape illegal XPath Search TermIf you try to perform a search like this

//element(*, nt:base)[jcr:contains(., 'test!')]

you get this exception

javax.jcr.RepositoryException: Exception building query: org.apache.jackrabbit.core.query.lucene.fulltext.ParseException: Encountered ""<EOF>"" at line 1, column 6.
"
0,"IndexReader subclasses must implement flex APIsTo be fixed only on trunk...

I made IndexReader's base flex APIs abstract, fixed all core/contrib/solr places that subclassed IR and didn't already implement flex (including contrib/memory, contrib/instantiated), and remove all the classes for the back-compat layer that emulated flex APIs on top of pre-flex APIs."
0,"Similarity.java javadocs and simplifications for 4.0As part of adding additional scoring systems to lucene, we made a lower-level Similarity
and the existing stuff became e.g. TFIDFSimilarity which extends it.

However, I always feel bad about the complexity introduced here (though I do feel there
are some ""excuses"", that its a difficult challenge).

In order to try to mitigate this, we also exposed an easier API (SimilarityBase) on top of 
it that makes some assumptions (and trades off some performance) to try to provide something 
consumable for e.g. experiments.

Still, we can cleanup a few things with the low-level api: fix outdated documentation and
shoot for better/clearer naming etc.
"
0,"Move JCRWebdavServerServlet to jcr-server and make it abstractIn line with isse JCR-417, I suggest to partially move the JCRWebdavServerServlet from the jcr-webapp project to the jcr-server project. By partially I mean, that the new (moved) servlet will be abstract and the getRepository() method will be abstract. The jcr-webapp project will still contain a JCRWebdavServerServlet (for backwards compatibility maintaing the same name) which just extends the new servlet and implements the getRepository() method using the RepositoryAccess servlet.

This allows for the reuse of the jcr-server project including the abstract JCRWebdavServerServlet in other environments.
"
0,"javacc-maven-plugin version in jackrabbit-core pom fileHi, I noticed that the pom.xml file of the jackrabbit-core project needs to specify version ""2.1"" for the javacc-maven-plugin because if it takes the 2.2-SNAPSHOT it won't compile. I put the 2.1 version and it worked fine.

<plugin>
        <groupId>org.codehaus.mojo</groupId>
        <artifactId>javacc-maven-plugin</artifactId>
        <version>2.1</version>
        <executions>


Im working with revision 529712 [April 17, 2007, 15:05 EST]"
0,"populate.jsp uses Java 1.5 methodThe method is URLConnection.setReadTimeout()
"
0,"Initialization error of Junit tests with solr-test-framework with IDEs and MavenI'm currently developping a new component for Solr. And in my Netbeans project, I have created two Test classes for this component: one class for simple unit tests (derived from  SolrTestCaseJ4 class) and a second one for tests with sharding (derived from  BaseDistributedSearchTestCase).
When I launch a test with these two classes, I have an error in the initialization of the second class of tests (no matter the class is, this is always the second executed class which fails). The error comes from an ""assert"" which failed in the begining of the function ""initRandom()"" of LuceneTestCase class :

assert !random.initialized;

But, if I launch each test class separatly, all the tests succeed!

After a discussion with Mr. Muir, the problems seems to be related to the incompatibility of the class LuceneTestCase with the functioning of Maven projects in IDEs.

According to mister Muir:

""
The problem is that via ant, tests work like this (e.g. for 3 test classes):
computeTestMethods
beforeClass
afterClass
computeTestMethods
beforeClass
AfterClass
computeTestMethods
beforeClass
afterClass

but via an IDE, if you run it from a folder like you did, then it does this:
computeTestMethods
computeTestMethods
computeTestMethods
beforeClass
afterClass
beforeClass
afterClass
beforeClass
afterClass 
"""
0,"reusing connections is unreliableHttpConnection reuse is unreliable. Because of the following:

1) There is currently no way to determine if a connection is still open on the
server side.
2) If an IOException occurs while writing to a connection it cannot be reused."
0,Use jackrabbit 1.2.1Use Jackarabit 1.2.1
0,"JSR 283: JCR Pathwith jsr 283 the jcr path is defined to consist of a combination of the following segments

	a name segment, (J, I), where J is a JCR name and I is an integer index (I  1).
	an identifier segment, U, where U is a JCR identifier.
	the root segment.
	the self segment.
	the parent segment.

-> the name segment can be in extended or qualified form -> see issue JCR-1712
-> the identifier segment is new for jsr283 and always identifies a node (-> see new method Node.getIdentifier())

Non-standard parts always need to be standardized. Any of the following makes a path non-standard:
- expanded name segments
- trailing /
- index [1]

Identifier-segments
- get resolved upon being passed to any API calls that take path to an existing Node
- don't get resolved when being used to create a PATH value object.

Except for PATH values, all jcr paths returned by the API are normalized and standard, thus never identifier-based.

PATH values in contrast:
- must be converted to standard form
- must NOT be normalized. i.e. redundant segments and identifiers must be preserved.
"
0,"single norm file still uses up descriptorsThe new index file format with a single .nrm file for all norms does not decrease file descriptor usage.
The .nrm file is opened once for each field with norms in the index segment."
0,"Improve org.apache.lucene.search.Filter Documentation and Tests to reflect per segment readersFilter Javadoc does not mention that the Reader passed to getDocIDSet(Reader) could be on a per-segment basis.
This caused confusion on the users-list -- see http://lucene.markmail.org/message/6knz2mkqbpxjz5po?q=date:200912+list:org.apache.lucene.java-user&page=1
We should improve the javadoc and also add a testcase that reflects filtering on a per-segment basis."
0,"JCR2SPI: several performance improvements pointed out by FindbugsFindBug report:

M P Bx: Method org.apache.jackrabbit.jcr2spi.nodetype.BitsetENTCacheImpl.getBitNumber(QName) invokes inefficient Integer(int) constructor; use Integer.valueOf(int) instead	src/main/java/org/apache/jackrabbit/jcr2spi/nodetype	BitsetENTCacheImpl.java	line 177	1190981544656	1666284
M P Bx: Method org.apache.jackrabbit.jcr2spi.query.RowIteratorImpl$RowImpl.getValue(String) invokes inefficient Integer(int) constructor; use Integer.valueOf(int) instead	src/main/java/org/apache/jackrabbit/jcr2spi/query	RowIteratorImpl.java	line 247	1190981544671	1666292
M P Bx: Method org.apache.jackrabbit.jcr2spi.WorkspaceManager.onEventReceived(EventBundle[], InternalEventListener[]) invokes inefficient Integer(int) constructor; use Integer.valueOf(int) instead	src/main/java/org/apache/jackrabbit/jcr2spi	WorkspaceManager.java	line 616	1190981544640	1666279
M P WMI: Method org.apache.jackrabbit.jcr2spi.name.NamespaceCache.syncNamespaces(Map) makes inefficient use of keySet iterator instead of entrySet iterator	src/main/java/org/apache/jackrabbit/jcr2spi/name	NamespaceCache.java	line 193	1190981544656	1666283
M P WMI: Method org.apache.jackrabbit.jcr2spi.nodetype.NodeTypeRegistryImpl.internalRegister(Map) makes inefficient use of keySet iterator instead of entrySet iterator	src/main/java/org/apache/jackrabbit/jcr2spi/nodetype	NodeTypeRegistryImpl.java	line 524	1190981544656	1666285
M P WMI: Method org.apache.jackrabbit.jcr2spi.observation.ObservationManagerImpl.onEvent(EventBundle) makes inefficient use of keySet iterator instead of entrySet iterator	src/main/java/org/apache/jackrabbit/jcr2spi/observation	ObservationManagerImpl.java	line 189	1190981544656	1666286
M P WMI: Method org.apache.jackrabbit.jcr2spi.state.NodeState.persisted(ChangeLog) makes inefficient use of keySet iterator instead of entrySet iterator	src/main/java/org/apache/jackrabbit/jcr2spi/state	NodeState.java	line 275	1190981544671	1666297
"
0,"3.0 not compile-time compatible with 2.0 library usageTo my surprise Oleg says this was the intent, yet the Jakarta-Slide webdavclient
libraries do not compile out of the box.  Patch for that issue to follow."
0,Introduce SessionInfo parameter for AbstractRepositoryService.createRootNodeDefinition()  SPI implementations might require access to the state of the current session in order to fulfill the contract of AbstractRepositoryService.createRootNodeDefinition(). I therefore suggest to add a SessionInfo parameter to this method. 
0,"Benchmark's ContentSource should not rely on file suffixes to be lower cased when detecting file type (gzip/bzip2/text)file.gz is correctly handled as gzip, but file.GZ handled as text which is wrong.
"
0,"Ability to group search results by fieldIt would be awesome to group search results by specified field. Some functionality was provided for Apache Solr but I think it should be done in Core Lucene. There could be some useful information like total hits about collapsed data like total count and so on.

Thanks,
Artyom"
0,"GData Server IndexComponentNew Feature added:

-> Indexcomponent.
-> Content extraction from entries.
-> Custom content ext. strategies added.
-> user defined index schema.
-> extended gdata-config.xml schema (xsd)
-> Indexcomponent UnitTests
-> Spellchecking on some JavaDoc.

##############
New jars included:

nekoHTML.jar 
xercesImpl.jar

@yonik: don't miss the '+' button to add directories :)"
0,"Several final classes have non-overriding protected membersProtected member access in final classes, except where a protected method overrides a superclass's protected method, makes little sense.  The attached patch converts final classes' protected access on fields to private, removes two final classes' unused protected constructors, and converts one final class's protected final method to private."
0,"Remove/Uncommit SegmentingTokenizerBaseI added this class in LUCENE-3305 to support analyzers like Kuromoji,
but Kuromoji no longer needs it as of LUCENE-3767. So now nothing uses it.

I think we should uncommit before releasing, svn doesn't forget so
we can add this back if we want to refactor something like Thai or Smartcn
to use it."
0,Add timing information to event deliveryThere should be debug messages that contain information on how long event listeners spend iterating over the delivered events.
0,"Download: improve user experienceThe download section at http://jackrabbit.apache.org/downloads.html
contains many files. The number of files should be reduced.

Some of them contain other files, for example the .rar files, and the .war files. 
The file jackrabbit-jca-1.4.rar  contains an old version of jackrabbit-core:
jackrabbit-core-1.4.jar - however the newest version of jackrabbit-core is 
jackrabbit-core-1.4.5.jar

This often leads to problems."
0,"Remove unused (and untested) methods from ReaderUtil that are also veeeeery ineffectiveReaderUtil contains two methods that are nowhere used and not even tested. Additionally those are implemented with useless List->array copying; ineffective docStart calculation for a binary search later instead directly returning the reader while scanning -- and I am not sure if they really work as expected. As ReaderUtil is @lucene.internal we should remove them in 3.x and trunk, alternatively the useless array copy / docStarts handling should be removed and tests added:

{code:java}
public static IndexReader subReader(int doc, IndexReader reader)
public static IndexReader subReader(IndexReader reader, int subIndex)
{code}
"
0,"ReorderReferenceableSNSTest failureI have checked out the Jackrabbit 1.4 branch to a new directory, and called:

mvn clean install

The error is:

Building Jackrabbit JCR to SPI
  task-segment: [clean, install]
---------------------------------
...
testRevertReorder(org.apache.jackrabbit.jcr2spi.ReorderReferenceableSNSTest)
junit.framework.AssertionFailedError: Reorder added a child node.
       at junit.framework.Assert.fail(Assert.java:47)
       at org.apache.jackrabbit.jcr2spi.ReorderTest.testOrder(ReorderTest.java:90)
       at org.apache.jackrabbit.jcr2spi.ReorderTest.testRevertReorder(ReorderTest.java:122)
"
0,"Lucene benchmark: objective performance test for LuceneWe need an objective way to measure the performance of Lucene, both indexing and querying, on a known corpus. This issue is intended to collect comments and patches implementing a suite of such benchmarking tests.

Regarding the corpus: one of the widely used and freely available corpora is the original Reuters collection, available from http://www-2.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz or http://people.csail.mit.edu/u/j/jrennie/public_html/20Newsgroups/20news-18828.tar.gz. I propose to use this corpus as a base for benchmarks. The benchmarking suite could automatically retrieve it from known locations, and cache it locally."
0,"Support http URL inline authenticationIf you try to execute a method with the httpClient using a valid url, conform to the schema http://username:password@host:port/ the authentication will fail, and you will get a 401 error."
0,"Indexing performance tests with realtime branchWe should run indexing performance tests with the DWPT changes and compare to trunk.

We need to test both single-threaded and multi-threaded performance.

NOTE:  flush by RAM isn't implemented just yet, so either we wait with the tests or flush by doc count."
0,"Move some *TermsEnum.java from oal.search to oal.indexI think FilteredTermsEnum, SingleTermsEnum should move?

I left TermRangeTermsEnum and FuzzyTermsEnum and PrefixTermsEnum since they seemed search specific."
0,"A Property and a Node Can Have the Same Name according to paragraph ""3.3.4"" of the of the JSR 283 specification (Public Review Draft), a property and a node can have the same name. "
0,"add maxtf to fieldinvertstatethe maximum within-document TF is a very useful scoring value, 
we should expose it so that people can use it in scoring

consider the following sim:
{code}
@Override
public float idf(int docFreq, int numDocs) {
  return 1.0F; /* not used */
}

@Override
public float computeNorm(String field, FieldInvertState state) {
  return state.getBoost() / (float) Math.sqrt(state.getMaxTF());
}
{code}

which is surprisingly effective, but more interesting for practical reasons.

"
0,"TermScorer caches values unnecessarilyTermScorer aggressively caches the doc and freq of 32 documents at a time for each term scored.  When querying for a lot of terms, this causes a lot of garbage to be created that's unnecessary.  The SegmentTermDocs from which it retrieves its information doesn't have any optimizations for bulk loading, and it's unnecessary.

In addition, it has a SCORE_CACHE, that's of limited benefit.  It's caching the result of a sqrt that should be placed in DefaultSimilarity, and if you're only scoring a few documents that contain those terms, there's no need to precalculate the SQRT, especially on modern VMs.

Enclosed is a patch that replaces TermScorer with a version that does not cache the docs or feqs.  In the case of a lot of queries, that saves 196 bytes/term, the unnecessary disk IO, and extra SQRTs which adds up."
0,"Contrib Analyzer Setters should be deprecated and replace with ctor argumentsSome analyzers in contrib provide setters for stopword / stem exclusion sets / hashtables etc. Those setters should be deprecated as they yield unexpected behaviour. The way they work is they set the reusable token stream instance to null in a thread local cache which only affects the tokenstream in the current thread. Analyzers itself should be immutable except of the threadlocal. 

will attach a patch soon."
0,"Deprecate and replace SimpleHttpConnection with the SimpleHttpServer based testing frameworkThanks to Christian Kohlschuetter and Odi we now have a very flexible testing
framework, which enables us to emulate pretty much all the aspects of a HTTP
server functionality including non-compliant behavior and various vendor
specific implementation quirks. 

Many, many thanks go to Christian Kohlschuetter for having contributed the
original code. 

I propose SimpleHttpConnection be deprecated and eventually be phased out. I
took the first steps toward this goal by migrating Basic authentication test
cases. I urge all committers and contributors to use SimpleHttpServer for all
the new cases from now on. Ideally in the future we should even be able to get
rid of Tomcat as a dependency for testing.

I also took liberty of tweaking the SimpleHttpServer API a little. I factored
SimpleRequest and SimpleResponse classes out and provided a new interface called
HttpService, which can be used instead of HttpRequestHandler to implement test
cases in a way very similar to writing servlets. 

I'll commit the patch shortly as it does not really touch any _productive_ code. 

Oleg"
0,PropertyReadMethodsTest should also work on NAME propertySome test cases in PropertyReadMethodsTest require a String property even though a NAME property like jcr:primaryType would be sufficient.
0,"Log creation impairs performanceRunning JProfiler on a program that uses HttpClient with a ThreadSafeClientConnManager, revealed that 5% of the time was spent constructing Log instances in class ClientParamsStack.

Oleg did some further investigation and found that DefaultRequestDirector also has the same problem.

A simple solution would be to make the Log a static member variable, and do this on all classes for consistency.  However this might not be the best solution for interoperating with some frameworks (see http://wiki.apache.org/jakarta-commons/Logging/StaticLog)

Another solution would be to simply remove the Log from the affected classes, although they are presumably there for a reason...
"
0,"Typo in log outputjackrabbit/src/java/org/apache/jackrabbit/core/fs/local/LocalFileSystem.java:
133c133
<         log.info(""LocaaFileSystem initialized on "" + root.getPath());
---
>         log.info(""LocalFileSystem initialized on "" + root.getPath());
"
0,"Simple toString() for BooleanFilterWhile working with BooleanFilter I wanted a basic toString() for debugging.

This is what I came up.  It works ok for me."
0,[PATCH] more verbose exception messages (BatchedItemOperations)added context to exception messages in BatchedItemOperations to aid debugging
0,"Minor spi2dav ExceptionConverter improvementsIt would be nice if the ExceptionConverter class in spi2dav returned UnsupportedRepositoryOperationExceptions instead of the undeclared UnsupportedOperationExceptions for HTTP 501 responses.

Besides that, the ExceptionConverter.generate() methods should be cleaned to always return the generated exception instead of in some cases returning and in others throwing it.

Finally, there's some unused code and chances for Java 5 cleanups.

I'll attach a patch, and commit it unless anyone objects."
0,"wrong assumptions in test cases about lock tokensSeveral test cases assume that Lock.getLockToken has to return null for locks not attached to the current session. However, this is optional. Citing the Javadoc for getLockToken:

     * May return the lock token for this lock. If this lock is open-scoped and
     * the current session either holds the lock token for this lock, or the
     * repository chooses to expose the lock token to the current session, then
     * this method will return that lock token. Otherwise this method will
     * return <code>null</code>."
0,"TCK: DocumentViewImportTest does not call refresh after direct-to-workspace importAfter performing a direct-to-workspace import, the test does not call refresh to ensure the transient layer doesn't contain stale data.

Proposal: call refresh(false) after performing direct-to-workspace imports.

--- DocumentViewImportTest.java (revision 422074)
+++ DocumentViewImportTest.java (working copy)
@@ -106,6 +106,12 @@
             SAXException, NotExecutableException {
  
         importXML(target, createSimpleDocument(), uuidBehaviour, withWorkspace);
+
+        if (withWorkspace)
+        {
+          session.refresh(false);
+        }
+
         performTests();
     }
  
@@ -127,6 +133,12 @@
             SAXException, IOException, NotExecutableException {
  
         importWithHandler(target, createSimpleDocument(), uuidBehaviour, withWorkspace);
+
+        if (withWorkspace)
+        {
+          session.refresh(false);
+        }
+
         performTests();
     }
"
0,Remove deprecated TokenStream APII looked into clover analysis: It seems to be no longer used since I removed the tests yesterday - I am happy!
0,"Add tests.iter.min to improve controlling tests.iter's behaviorAs discussed here: http://lucene.472066.n3.nabble.com/Stop-iterating-if-testsFailed-td2747426.html, this issue proposes to add tests.iter.min in order to allow one better control over how many iterations are run:

* Keep tests.iter as it is today
* Add tests.iter.min (default to tests.iter) to denote that at least N instances of the test should run until there's either a failure or tests.iter is reached.

If one wants to run until the first failure, he can set tests.iter.min=1 and tests.iter=X -- up to X instances of the test will run, until the first failure.

Similarly, one can set tests.iter=N to denote that at least N instances should run, regardless if there were failures, but if after N runs a failure occurred, the test should stop.

Note: unlike what's proposed on the thread, tests.iter.max is dropped from this proposal as it's exactly like tests.iter, so no point in having two similar parameters.

I will work on a patch tomorrow."
0,"Remove background initialization of hierarchy cacheThis is a follow up to JCR-1998.

Rethinking the initialization in a background thread again, I now come to the conclusion that it should be initialized either completely on startup or not at all. A background thread puts additional load on the process, possibly fighting for I/O with other startup procedures.
"
0,"norms file can become unexpectedly enormous
Spinoff from this user thread:

   http://www.gossamer-threads.com/lists/lucene/java-user/46754

Norms are not stored sparsely, so even if a doc doesn't have field X
we still use up 1 byte in the norms file (and in memory when that
field is searched) for that segment.  I think this is done for
performance at search time?

For indexes that have a large # documents where each document can have
wildly varying fields, each segment will use # documents times # fields
seen in that segment.  When optimize merges all segments, that product
grows multiplicatively so the norms file for the single segment will
require far more storage than the sum of all previous segments' norm
files.

I think it's uncommon to have a huge number of distinct fields (?) so
we would need a solution that doesn't hurt the more common case where
most documents have the same fields.  Maybe something analogous to how
bitvectors are now optionally stored sparsely?

One simple workaround is to disable norms.
"
0,"Lucene Search not scallingI've noticed that when doing thousands of searches in a single thread the average time is quite low i.e. a few milliseconds. When adding more concurrent searches doing exactly the same search the average time increases drastically. 
I've profiled the search classes and found that the whole of lucene blocks on 

org.apache.lucene.index.SegmentCoreReaders.getTermsReader
org.apache.lucene.util.VirtualMethod
  public synchronized int getImplementationDistance 
org.apache.lucene.util.AttributeSourcew.getAttributeInterfaces

These cause search times to increase from a few milliseconds to up to 2 seconds when doing 500 concurrent searches on the same in memory index. Note: That the index is not being updates at all, so not refresh methods are called at any stage.


Some questions:
  Why do we need synchronization here?
  There must be a non-lockable solution for these, they basically cause lucene to be ok for single thread applications but disastrous for any concurrent implementation.

I'll do some experiments by removing the synchronization from the methods of these classes."
0,"Extract JDBC Connection InitAn intermediate step to allowing a PM to be easily configurable through JNDI would be to extract the connection init. This will allow system integrators to subclass/wrap and dynamically configure a customized Simple PM. In org.apache.jackrabbit.core.state.db.SimpleDbPersistenceManager:

Replace lines (296-298) with

        initConnection();

Add:
	/**
	* Initialize the JDBC connection
	**/
	protected void initConnection() throws Exception {
            Class.forName(driver);
            con = DriverManager.getConnection(url, user, password);
            con.setAutoCommit(false);
	}"
0,"Add reflection API to AttributeSource/AttributeImplAttributeSource/TokenStream inspection in Solr needs to have some insight into the contents of AttributeImpls. As LUCENE-2302 has some problems with toString() [which is not structured and conflicts with CharSequence's definition for CharTermAttribute], I propose an simple API that get a default implementation in AttributeImpl (just like toString() current):

- Iterator<Map.Entry<String,?>> AttributeImpl.contentsIterator() returns an iterator (for most attributes its a singleton) of a key-value pair, e.g. ""term""->""foobar"",""startOffset""->Integer.valueOf(0),...
- AttributeSource gets the same method, it just concat the iterators of each getAttributeImplsIterator() AttributeImpl

No backwards problems occur, as the default toString() method will work like before (it just gets iterator and lists), but we simply remove the documentation for the format. (Char)TermAttribute gets a special impl fo toString() according to CharSequence and a corresponding iterator.

I also want to remove the abstract hashCode() and equals() methods from AttributeImpl, as they are not needed and just create work for the implementor."
0,"Omit positions but keep termFreqit would be useful to have an option to discard positional information but still keep the term frequency - currently setOmitTermFreqAndPositions discards both. Even though position-dependent queries wouldn't work in such case, still any other queries would work fine and we would get the right scoring."
0,SPI: provide batch read functionalityextend RepositoryService interface to allow for BatchRead and modify jcr2spi accordingly.
0,"configurable User-Agent stringUser configurable item to set the user agent without haveing to set it on a per
HttpMethod basis."
0,"Jcr2Spi: Avoid extra round trip to the SPI upon Node.getNode and Session.getItemUpon Session.getItem/itemExists and Node.getNode/hasNode JCR2SPI currently tries to load the Node from the persistent layer (SPI) if no corresponding entry exists in the hierarchy.

Since with JCR-1638 a flag has been introduced indicating if the child node entries are complete. In this case, the extra round trip could be omitted."
0,"Rethink LocalizedTestCaseRunner with JUnit 4 - Clover OOMAs a spinn off from this [conversation|http://www.lucidimagination.com/search/document/ae20885bf5baedc5/build_failed_in_hudson_lucene_3_x_116#7ed351341152ee2d] we should rethink the way how we execute testcases with different locals since glover reports appears to throw OOM errors b/c Junit treats each local as a single test case run.

Here are some options:
* select the local at random only run the test with a single local
* set the local via system property -Dtest.locale=en.EN
* run with the default locale only -Dtest.skiplocale=true
* one from the above but only if instrumented with clover (let common tests run all the locale)

"
0,"Grouping module should allow subclasses to set the group key per documentThe new grouping module can only group by a single-valued indexed field.

But, if we make the 'getGroupKey' a method that a subclass could override, then I think we could refactor Solr over to the module, because it could do function queries and normal queries via subclass (I think).

This also makes the impl more extensible to apps that might have their own interesting group values per document."
0,"ValueFormat should provide method getJCRStringIn order to retrieve the JCR String representation of a QValue currently the following calls are required:

ValueFormat.getJCRValue(QValue, NamePathResolver, ValueFactory)
Value.getString()

This could be simplified if the ValueFormat would provide

ValueFormat.getJCRString(QValue, NamePathResolver)

"
0,"Add/change warning comments in the javadocs of Payload APIsSince the payload API is still experimental we should change the comments
in the javadocs similar to the new search/function package."
0,"Litmus prophighunicode test failure on JRE 1.5The WebDAV Litmus test suite contains a test case for writing and reading the Unicode character &#x10000; which can't be represented as a single 16-bit char in Java. Instead the character is stored as a surrogate pair of two 16-bit chars. Unfortunately the Xalan XML serializer used by Sun JRE 1.5 incorrectly encodes these as two separate characters in UTF-8, which leads to the following Litmus test failure:

-> running `props':
[...]
17. prophighunicode....... pass
18. propget............... FAIL (PROPFIND on `/default/litmus/prop2': XML parse error at line 1: not well-formed (invalid token))
"
0,"Improve read/write concurrencyI'd like to set up a few performance tests to help identify our worst bottlenecks for various kinds of concurrent read-only and read-write access patterns.

Once identified, I'm hoping to fix at least some of those bottlenecks."
0,EnwikiConentSource does not work with parallel tasks
0,"SPI implementations currently need to provide implementations of both ValueFactory and QValueFactoryThis should be simplified so that an implementation of QValueFactory is sufficient.
"
0,"Unnecessary parsing of Name valueWhen a Name value is created for a call like Property.getValue() the internal QName if formatted, parsed and formatted again."
0,"Some files are missing the license headersJukka provided the following list of files that are missing the license headers.
In addition there might be other files (like build scripts) that don't have the headers.

src/java/org/apache/lucene/document/MapFieldSelector.java
src/java/org/apache/lucene/search/PrefixFilter.java
src/test/org/apache/lucene/TestHitIterator.java
src/test/org/apache/lucene/analysis/TestISOLatin1AccentFilter.java
src/test/org/apache/lucene/index/TestAddIndexesNoOptimize.java
src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
src/test/org/apache/lucene/index/TestFieldInfos.java
src/test/org/apache/lucene/index/TestIndexFileDeleter.java
src/test/org/apache/lucene/index/TestIndexWriter.java
src/test/org/apache/lucene/index/TestIndexWriterDelete.java
src/test/org/apache/lucene/index/TestIndexWriterLockRelease.java
src/test/org/apache/lucene/index/TestIndexWriterMergePolicy.java
src/test/org/apache/lucene/index/TestNorms.java
src/test/org/apache/lucene/index/TestParallelTermEnum.java
src/test/org/apache/lucene/index/TestSegmentTermEnum.java
src/test/org/apache/lucene/index/TestTerm.java
src/test/org/apache/lucene/index/TestTermVectorsReader.java
src/test/org/apache/lucene/search/TestRangeQuery.java
src/test/org/apache/lucene/search/TestTermScorer.java
src/test/org/apache/lucene/store/TestBufferedIndexInput.java
src/test/org/apache/lucene/store/TestWindowsMMap.java
src/test/org/apache/lucene/store/_TestHelper.java
src/test/org/apache/lucene/util/_TestUtil.java
contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleSloppyPhraseQueryMaker.java
contrib/gdata-server/src/core/src/java/org/apache/lucene/gdata/server/FeedNotFoundException.java
contrib/gdata-server/src/core/src/java/org/apache/lucene/gdata/server/registry/ComponentType.java
contrib/gdata-server/src/core/src/java/org/apache/lucene/gdata/server/registry/RegistryException.java
contrib/gdata-server/src/core/src/java/org/apache/lucene/gdata/storage/lucenestorage/StorageAccountWrapper.java
contrib/gdata-server/src/core/src/test/org/apache/lucene/gdata/storage/lucenestorage/TestModifiedEntryFilter.java
contrib/gdata-server/src/gom/src/test/org/apache/lucene/gdata/gom/core/AtomUriElementTest.java
contrib/gdata-server/src/gom/src/test/org/apache/lucene/gdata/gom/core/GOMEntryImplTest.java
contrib/gdata-server/src/gom/src/test/org/apache/lucene/gdata/gom/core/GOMFeedImplTest.java
contrib/gdata-server/src/gom/src/test/org/apache/lucene/gdata/gom/core/GOMGenereatorImplTest.java
contrib/gdata-server/src/gom/src/test/org/apache/lucene/gdata/gom/core/GOMSourceImplTest.java
contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources.java
contrib/javascript/queryConstructor/luceneQueryConstructor.js
contrib/javascript/queryEscaper/luceneQueryEscaper.js
contrib/javascript/queryValidator/luceneQueryValidator.js
contrib/queries/src/java/org/apache/lucene/search/BooleanFilter.java
contrib/queries/src/java/org/apache/lucene/search/BoostingQuery.java
contrib/queries/src/java/org/apache/lucene/search/FilterClause.java
contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery.java
contrib/queries/src/java/org/apache/lucene/search/TermsFilter.java
contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThisQuery.java
contrib/queries/src/test/org/apache/lucene/search/BooleanFilterTest.java
contrib/regex/src/test/org/apache/lucene/search/regex/TestSpanRegexQuery.java
contrib/snowball/src/java/net/sf/snowball/Among.java
contrib/snowball/src/java/net/sf/snowball/SnowballProgram.java
contrib/snowball/src/java/net/sf/snowball/TestApp.java
contrib/spellchecker/src/test/org/apache/lucene/search/spell/TestSpellChecker.java
contrib/surround/src/test/org/apache/lucene/queryParser/surround/query/BooleanQueryTst.java
contrib/surround/src/test/org/apache/lucene/queryParser/surround/query/ExceptionQueryTst.java
contrib/surround/src/test/org/apache/lucene/queryParser/surround/query/SingleFieldTestDb.java
contrib/surround/src/test/org/apache/lucene/queryParser/surround/query/Test01Exceptions.java
contrib/surround/src/test/org/apache/lucene/queryParser/surround/query/Test02Boolean.java
contrib/surround/src/test/org/apache/lucene/queryParser/surround/query/Test03Distance.java
contrib/wordnet/src/java/org/apache/lucene/wordnet/SynExpand.java
contrib/wordnet/src/java/org/apache/lucene/wordnet/SynLookup.java
contrib/wordnet/src/java/org/apache/lucene/wordnet/Syns2Index.java
"
0,Text extraction may congest thread pool in the repositoryText extraction congests the thread pool in the repository when e.g. many PDFs are loaded into the workspace. Tasks submitted by the index merger are delayed because of that and will result in many index segment folders.
0,"Create a Codec to work with streaming and append-only filesystemsSince early 2.x times Lucene used a skip/seek/write trick to patch the length of the terms dict into a place near the start of the output data file. This however made it impossible to use Lucene with append-only filesystems such as HDFS.

In the post-flex trunk the following code in StandardTermsDictWriter initiates this:
{code}
    // Count indexed fields up front
    CodecUtil.writeHeader(out, CODEC_NAME, VERSION_CURRENT); 

    out.writeLong(0);                             // leave space for end index pointer
{code}
and completes this in close():
{code}
      out.seek(CodecUtil.headerLength(CODEC_NAME));
      out.writeLong(dirStart);
{code}

I propose to change this layout so that this pointer is stored simply at the end of the file. It's always 8 bytes long, and we known the final length of the file from Directory, so it's a single additional seek(length - 8) to read it, which is not much considering the benefits."
0,"BUILD.txt instructions wrong for JavaCCThe text in BUILD.txt for javacc says to set the property to the bin directory in the javacc installation. It should actually be set to the javacc installation directory, the directory containing the bin directory. The comments common-build.xml correctly state this."
0,"QueryParser can produce empty sub BooleanQueries when Analyzer proudces no tokens for inputas triggered by SOLR-261, if you have a query like this...

   +foo:BBB  +(yak:AAA  baz:CCC)

...where the analyzer produces no tokens for the ""yak:AAA"" or ""baz:CCC"" portions of the query (posisbly because they are stop words) the resulting query produced by the QueryParser will be...

  +foo:BBB +()

...that is a BooleanQuery with two required clauses, one of which is an empty BooleanQuery with no clauses.

this does not appear to be ""good"" behavior.

In general, QueryParser should be smarter about what it does when parsing encountering parens whose contents result in an empty BooleanQuery -- but what exactly it should do in the following situations...

 a)  +foo:BBB +()
 b)  +foo:BBB ()
 c)  +foo:BBB -()

...is up for interpretation.  I would think situation (b) clearly lends itself to dropping the sub-BooleanQuery completely.  situation (c) may also lend itself to that solution, since semanticly it means ""don't allow a match on any queries in the empty set of queries"".  .... I have no idea what the ""right"" thing to do for situation (a) is."
0,"[PATCH] to remove synchronized code from TermVectorsReaderOtis,

here the latest and last patch to get rid of all synchronized code from
TermVectorsReader. It should include at least 3 files, TermVectorsReader.diff,
SegmentReader.diff and the new junit test case TestMultiThreadTermVectors.java.
The patch was generated against the current CVS version of TermVectorsReader and
SegmentReader. All lucene related junit tests pass fine.

best regards
Bernhard"
0,"TCK: Transfer of lock token should be tested using open-scoped locksdespite the fact that jsr170 does not limit the usage of Session.removeLockToken(String) and Session.addLockToken(String) to tokens obtained from open-scoped locks, i don't see too much benefit of it. Therefore (and due to the fact that this issue will be addressed within the scope of jsr283), i would  like to suggest to modify those test-cases dealing with transfer of lock tokens and create open-scoped locks.
"
0,"Make collecting group membership information lazyJCR-2710 added a more scalable content model for storing group membership information. To further leverage the new model it would be preferable when group membership collecting where lazy. (i.e. Group#getDeclaredMembers() and Group#getMembers() should not construct the list of all members up front). 
"
0,"Implement caching mechanism for ItemInfo batchesCurrently all ItemInfos returned by RepositoryService#getItemInfos are placed into the hierarchy right away. For big batch sizes this is prohibitively expensive. The overhead is so great (*), that it quickly outweighs the overhead of network round trips. Moreover, SPI implementations usually choose the batch in a way determined by the backing persistence store and not by the requirements of the consuming application on the JCR side. That is, many of the items in the batch might never be actually needed. 

I suggest to implement a cache for ItemInfo batches. Conceptually such a cache would live inside jcr2spi right above the SPI API. The actual implementation would be provided by SPI implementations. This approach allows for fine tuning cache/batch sizes to a given persistence store and network environment. This would also better separate different concerns: the purpose of the existing item cache is to optimize for the requirement of the consumer of the JCR API ('the application'). The new ItemInfo cache is to optimize for the specific network environment and backing persistence store. 

(*) Numbers follow "
0,"The values for the Via header are created by httpclient-cache for each cached and backend requestThe Via header that gets generated and inserted by the caching layer is done repeatedly in the HTTP conversation, even if the constructed string is constant for each protocol version that is involved.

The proposed patch constructs a map of generated values held in memory with the associated ProtocolVersion as a key and uses read/write locks to access the data. This  solution minimizes the time to generate such a value from several milliseconds to 40-50 microseconds."
0,"AttributeSource/TokenStream API improvementsThis patch makes the following improvements to AttributeSource and
TokenStream/Filter:

- introduces interfaces for all Attributes. The corresponding
  implementations have the postfix 'Impl', e.g. TermAttribute and
  TermAttributeImpl. AttributeSource now has a factory for creating
  the Attribute instances; the default implementation looks for
  implementing classes with the postfix 'Impl'. Token now implements
  all 6 TokenAttribute interfaces.

- new method added to AttributeSource:
  addAttributeImpl(AttributeImpl). Using reflection it walks up in the
  class hierarchy of the passed in object and finds all interfaces
  that the class or superclasses implement and that extend the
  Attribute interface. It then adds the interface->instance mappings
  to the attribute map for each of the found interfaces.

- removes the set/getUseNewAPI() methods (including the standard
  ones). Instead it is now enough to only implement the new API,
  if one old TokenStream implements still the old API (next()/next(Token)),
  it is wrapped automatically. The delegation path is determined via
  reflection (the patch determines, which of the three methods was
  overridden).

- Token is no longer deprecated, instead it implements all 6 standard
  token interfaces (see above). The wrapper for next() and next(Token)
  uses this, to automatically map all attribute interfaces to one
  TokenWrapper instance (implementing all 6 interfaces), that contains
  a Token instance. next() and next(Token) exchange the inner Token
  instance as needed. For the new incrementToken(), only one
  TokenWrapper instance is visible, delegating to the currect reusable
  Token. This API also preserves custom Token subclasses, that maybe
  created by very special token streams (see example in Backwards-Test).

- AttributeImpl now has a default implementation of toString that uses
  reflection to print out the values of the attributes in a default
  formatting. This makes it a bit easier to implement AttributeImpl,
  because toString() was declared abstract before.

- Cloning is now done much more efficiently in
  captureState. The method figures out which unique AttributeImpl
  instances are contained as values in the attributes map, because
  those are the ones that need to be cloned. It creates a single
  linked list that supports deep cloning (in the inner class
  AttributeSource.State). AttributeSource keeps track of when this
  state changes, i.e. whenever new attributes are added to the
  AttributeSource. Only in that case will captureState recompute the
  state, otherwise it will simply clone the precomputed state and
  return the clone. restoreState(AttributeSource.State) walks the
  linked list and uses the copyTo() method of AttributeImpl to copy
  all values over into the attribute that the source stream
  (e.g. SinkTokenizer) uses. 

- Tee- and SinkTokenizer were deprecated, because they use
Token instances for caching. This is not compatible to the new API
using AttributeSource.State objects. You can still use the old
deprecated ones, but new features provided by new Attribute types
may get lost in the chain. A replacement is a new TeeSinkTokenFilter,
which has a factory to create new Sink instances, that have compatible
attributes. Sink instances created by one Tee can also be added to
another Tee, as long as the attribute implementations are compatible
(it is not possible to add a sink from a tee using one Token instance
to a tee using the six separate attribute impls). In this case UOE is thrown.

The cloning performance can be greatly improved if not multiple
AttributeImpl instances are used in one TokenStream. A user can
e.g. simply add a Token instance to the stream instead of the individual
attributes. Or the user could implement a subclass of AttributeImpl that
implements exactly the Attribute interfaces needed. I think this
should be considered an expert API (addAttributeImpl), as this manual
optimization is only needed if cloning performance is crucial. I ran
some quick performance tests using Tee/Sink tokenizers (which do
cloning) and the performance was roughly 20% faster with the new
API. I'll run some more performance tests and post more numbers then.

Note also that when we add serialization to the Attributes, e.g. for
supporting storing serialized TokenStreams in the index, then the
serialization should benefit even significantly more from the new API
than cloning. 

This issue contains one backwards-compatibility break:
TokenStreams/Filters/Tokenizers should normally be final
(see LUCENE-1753 for the explaination). Some of these core classes are 
not final and so one could override the next() or next(Token) methods.
In this case, the backwards-wrapper would automatically use
incrementToken(), because it is implemented, so the overridden
method is never called. To prevent users from errors not visible
during compilation or testing (the streams just behave wrong),
this patch makes all implementation methods final
(next(), next(Token), incrementToken()), whenever the class
itsself is not final. This is a BW break, but users will clearly see,
that they have done something unsupoorted and should better
create a custom TokenFilter with their additional implementation
(instead of extending a core implementation).

For further changing contrib token streams the following procedere should be used:

    *  rewrite and replace next(Token)/next() implementations by new API
    * if the class is final, no next(Token)/next() methods needed (must be removed!!!)
    * if the class is non-final add the following methods to the class:
{code:java}
      /** @deprecated Will be removed in Lucene 3.0. This method is final, as it should
       * not be overridden. Delegates to the backwards compatibility layer. */
      public final Token next(final Token reusableToken) throws java.io.IOException {
        return super.next(reusableToken);
      }

      /** @deprecated Will be removed in Lucene 3.0. This method is final, as it should
       * not be overridden. Delegates to the backwards compatibility layer. */
      public final Token next() throws java.io.IOException {
        return super.next();
      }
{code}
Also the incrementToken() method must be final in this case
(and the new method end() of LUCENE-1448)
"
0,"NewAnalyzerTaskNewAnalyzerTask (patch to follow) allows a contrib/benchmark algorithm to change Analyzers during a run.  This is useful when comparing Analyzers

{""NewAnalyzer"" NewAnalyzer(WhitespaceAnalyzer, SimpleAnalyzer, StopAnalyzer, standard.StandardAnalyzer) >

is a sample declaration in an algorithm file."
0,"Contrib queryparser should not use CharSequence as Map keyToday, contrib query parser uses Map<CharSequence,...> in many different places, which may lead to problems, since CharSequence interface does not enforce the implementation of hashcode and equals methods. Today, it's causing a problem with QueryTreeBuilder.setBuilder(CharSequence,QueryBuilder) method, that does not works as expected."
0,"Make IndexReader really read-only in Lucene 4.0As we change API completely in Lucene 4.0 we are also free to remove read-write access and commits from IndexReader. This code is so hairy and buggy (as investigated by Robert and Mike today) when you work on SegmentReader level but forget to flush in the DirectoryReader, so its better to really make IndexReaders readonly.

Currently with IndexReader you can do things like:
- delete/undelete Documents -> Can be done by with IndexWriter, too (using deleteByQuery)
- change norms -> this is a bad idea in general, but when we remove norms at all and replace by DocValues this is obsolete already. Changing DocValues should also be done using IndexWriter in trunk (once it is ready)"
0,ConfigurationException constructors are package privateConfigurationException constructors are package private which prevents reusing them in other packages. eg. when extending the configuration.
0,"Create Jcr-Client Moduletask copied from JCR-1877:

i think it would be wise to create a new module that mainly consists of a RepositoryFactory and combines jackrabbit-jcr2spi with the known (and also any other) spi implementations."
0,"Restructure the Jackrabbit source treeReintroduce some of the changes in JCR-157 as a more general restructuring to simplify the Jackrabbit project structure. See http://thread.gmane.org/gmane.comp.apache.jackrabbit.devel/9170/ for the rationale and discussion. The main parts of this restructuring would be:

1. Create a Jackrabbit ""super-project"" (artifactId: jackrabbit) in trunk/

2. Use the super-project POM as the parent of all Jackrabbit component POMs

3. Move the contents of trunk/jackrabbit/src/site directly to trunk/src/site, and use the super-project to generate the web site

4. Create independent subprojects for the the jackrabbit-api and jackrabbit-commons components, moving the the corresponding parts of the source tree

5. Move the jcr-server subprojects on level up

6. Rename the subproject directories to match their artifactIds

Note that this restructuring depends on JCR-611 and JCR-332, since the best way to implement this by utilizing a snapshot repository for the component dependencies."
0,"Add SearcherLifetimeManager, so you can retrieve the same searcher you previously usedThe idea is similar to SOLR-2809 (adding searcher leases to Solr).

This utility class sits above whatever your source is for ""the
current"" searcher (eg NRTManager, SearcherManager, etc.), and records
(holds a reference to) each searcher in recent history.

The idea is to ensure that when a user does a follow-on action (clicks
next page, drills down/up), or when two or more searcher invocations
within a single user search need to happen against the same searcher
(eg in distributed search), you can retrieve the same searcher you
used ""last time"".

I think with the new searchAfter API (LUCENE-2215), doing follow-on
searches on the same searcher is more important, since the ""bottom""
(score/docID) held for that API can easily shift when a new searcher
is opened.

When you do a ""new"" search, you record the searcher you used with the
manager, and it returns to you a long token (currently just the
IR.getVersion()), which you can later use to retrieve the same
searcher.

Separately you must periodically call prune(), to prune the old
searchers, ideally from the same thread / at the same time that
you open a new searcher."
0,"[PATCH] Decouple locking implementation from Directory implementationThis is a spinoff of http://issues.apache.org/jira/browse/LUCENE-305.

I've opened this new issue to capture that it's wider scope than
LUCENE-305.

This is a patch originally created by Jeff Patterson (see above link)
and then modified as described here:

  http://issues.apache.org/jira/browse/LUCENE-305#action_12418493

with some small additional changes:

  * For each FSDirectory.getDirectory(), I made a corresponding
    version that also accepts a LockFactory instance.  So, you can
    construct an FSDirectory with your own LockFactory.

  * Cascaded defaulting for FSDirectory's LockFactory implementation:
    if you pass in a LockFactory instance, it's used; else if
    setDisableLocks was called, we use NoLockFactory; else, if the
    system property ""org.apache.lucene.store.FSDirectoryLockFactoryClass""
    is defined, we use that; finally, we'll use the original locking
    implementation (SimpleFSLockFactory).

The gist is that all locking code has been moved out of *Directory and
into subclasses of a new abstract LockFactory class.  You can now set
the LockFactory of a Directory to change how it does locking.  For
example, you can create an FSDirectory but set its locking to
SingleInstanceLockFactory (if you know all writing/reading will take
place a single JVM).

The changes pass all unit tests (on Ubuntu Linux Sun Java 1.5 and
Windows XP Sun Java 1.4), and I added another TestCase to test the
LockFactory code.

Note that LockFactory defaults are not changed: FSDirectory defaults
to SimpleFSLockFactory and RAMDirectory defaults to
SingleInstanceLockFactory.

Next step (separate issue) is to create a LockFactory that uses the OS
native locks (through java.nio).
"
0,"Text extractor classes are obsolete in webText extractor classes are obsolete in http://jackrabbit.apache.org/doc/components/index-filters.html

""org.apache.jackrabbit.core.query"" are actually ""org.apache.jackrabbit.extractor""

Plain text extractor continue being ""org.apache.jackrabbit.core.query.lucene.TextPlainTextFilter""? "
0,"Use POIExtractor wherever possiblePOI scratchpad comes with a couple of text extractor utilities, which makes it easier to extract text. We should rather use those utilities than writing our own extractor code. This helps avoid issues like JCR-1530."
0,"LayeredSchemeSocketFactory.createLayeredSocket() should have access to HttpParamsWe use a custom implementation of LayeredSchemeSocketFactory that manages a keystore location through HttpParams. That allows us to use different keystores on a per connection basis.

When a proxy is used LayeredSchemeSocketFactory.createLayeredSocket() is invoked which does not have a parameter that passes the HttpParams along. In consequence certificate authentication fails in our implementation. Is there a reason why all other factory methods in the super class have an HttpParams parameter except for LayeredSchemeSocketFactory.createLayeredSocket()?

The downstream bug is here:

369805: certificate authentication with custom keystore fails behind proxy
https://bugs.eclipse.org/bugs/show_bug.cgi?id=369805

Any input would be greatly appreciated."
0,"Jackrabbit Utilites upgrade to Jackrabbit 2.1.0I'm including a patch for the jcrutil project in the sandbox, for the S3 DataStore to work with 2.1.0, as well as the VFS.  Also using Tika for MIME type resolution.  Please look this over and feel free to improve, this is something I played with but didn't stress test."
0,"better diagnostics when version storage is brokenIn InternalVersionManagerBase, the code doesn't do a null-check on the predecessors property, assuming it'll always be present. When this is not the case due to a repository problem, we'll see a NPE.

Proposal: add code that generates a more meaningful error message; making it easier to debug in production."
0,"Open IndexWriter API to allow custom MergeScheduler implementationIndexWriter's getNextMerge() and merge(OneMerge) are package-private, which makes it impossible for someone to implement his own MergeScheduler. We should open up these API, as well as any other that can be useful for custom MS implementations."
0,"Documentation on SingleClientConnManager(SchemeRegistry schreg) constructor is wrongSeems that the documentation for single-arg constructor SingleClientConnManager(SchemeRegistry schreg) is wrong.

Documentation says that incoming SchemeRegistry parameter can be null:
    schreg - the scheme registry, or null for the default registry

However, the constructor throws an exception in incoming schreg param is null:

    /**
     * Creates a new simple connection manager.
     *
     * @param params    the parameters for this manager
     * @param schreg    the scheme registry, or
     *                  <code>null</code> for the default registry
     */
    public SingleClientConnManager(HttpParams params,
                                   SchemeRegistry schreg) {
        if (schreg == null) {
            throw new IllegalArgumentException
                (""Scheme registry must not be null."");
        }


So this is likely a documentation bug..."
0,DescendantSelfAxisQuery creates too many object instancesIn DescendantSelfAxisQuery.DescendantSelfAxisScorer.isValid() there is an ArrayList and an Integer instance created on every call. Since this method gets called really often during queries the object creation/gc affects performance.
0,"JCR2SPI: Remove validation check for same-named Node and PropertyJSR 170 disallowed a parent node to have a property and a child node with the same name.

This limitation has been removed with JSR 283 and the RI (jackrabbit-core) already removed the check.
I would suggest to change Jcr2Spi accordingly and leave this validation to the underlying SPI impl.

If I'm not mistaken this JSR 170 requirement is asserted in a single place (ItemStateValidator)."
0,"spi2davex: use srcWorkspaceName to build srcPath for clone and copyspi2davex provides as simple workaround for the missing clone and cross-workspace-copy in spi2dav.
however, the src workspace name isn't used to build the src path."
0,"Bad transitive dependencies in commons-httpclientAs reported in HTTPCLIENT-605, the commons-httpclient 3.0 dependency introduces junit as a transitive ""compile"" scope dependency. The library also uses commons-logging, which sidesteps Jackrabbit's use of slf4j for logging.

To avoid these issues we should locally override the junit dependency in commons-httpclient and replace the commons-logging dependency with jcl104-over-slf4j."
0,"cache should invalidate obsoleted entries mentioned in Content-LocationFrom http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.6:

If a cache receives a successful response whose Content-Location field matches that of an existing cache entry for the same Request-URI, whose entity-tag differs from that of the existing entry, and whose Date is more recent than that of the existing entry, the existing entry SHOULD NOT be returned in response to future requests and SHOULD be deleted from the cache.

Current caching module doesn't do this (yet). As this is a recommendation (SHOULD) and not a requirement (MUST) I am marking this as an improvement rather than a bug.
"
0,"Multiple DIGEST authentication attempts with same credentialsHttpMethodBase's processAuthenticationResponse uses a set of realms to which
attempts to authenticate have already been made. The elements of the set are a
concatenation of the requested path and the value of the Authentication response
header.

For digest authentication this response header contains a nonce value, which is
uniquely generated by the server each time a 401 response is made. This makes it
impossible to recognize that authentication against this realm has been
attempted before and so all 100 attempts are made before returning. The nonce
should probably not be used in the realmsUsed element

Reported by Rob Owen <Rob.Owen@sas.com>"
0,"duplicate package.html files in queryParser and analsysis.cn packagesThese files conflict with eachother when building the javadocs. there can be only one (of each) ...

{code}
hossman@brunner:~/lucene/java$ find src contrib -name package.html | perl -ple 's{.*src/java/}{}' | sort | uniq -c | grep -v "" 1 ""
   2 org/apache/lucene/analysis/cn/package.html
   2 org/apache/lucene/queryParser/package.html
hossman@brunner:~/lucene/java$ find src contrib -path \*queryParser/package.html
src/java/org/apache/lucene/queryParser/package.html
contrib/queryparser/src/java/org/apache/lucene/queryParser/package.html
hossman@brunner:~/lucene/java$ find src contrib -path \*cn/package.html
contrib/analyzers/common/src/java/org/apache/lucene/analysis/cn/package.html
contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/package.html
{code}

"
0,"TwoPhaseCommit interfaceI would like to propose a TwoPhaseCommit interface which declares the methods necessary to implement a 2-phase commit algorithm:
* prepareCommit()
* commit()
* rollback()

The prepare/commit ones have variants that take a (Map<String,String> commitData) following the ones we have in IndexWriter.

In addition, a TwoPhaseCommitTool which implements a 2-phase commit amongst several TPCs.

Having IndexWriter implement that interface will allow running the 2-phase commit algorithm on multiple IWs or IW + any other object that implements the interface.

We should mark the interface @lucene.internal so as to not block ourselves in the future. This is pretty advanced stuff anyway.

Will post a patch soon"
0,"Setting CONNECTION_TIMEOUT and SO_TIMEOUT on a per-method basisThe capability of setting connection timeout and socket timeout on a per-method
basis should be provided. This would enable different threads, sharing the same
HttpClient, to set different timeouts for their methods executions."
0,"Avoid exceptions thrown in finalize handler of RepositoryImpl constructorIf an exception happens during initialization of the repository, it might be overlayed by an exception thrown in the finalize handler of the RepositoryImpl constructor (see line 382 ff in [1]). The latter exception wins and the original exception is lost (if you don't have a log). This makes it hard to figure out the real problem.

This problem is actually a bit self-enforcing: if something goes wrong during startup, the code in the shutdown() method that is called is actually very prone to fail as it might not expect such a broken-startup state. In my case the overlaying NPE happened in ObservationManagerImpl.getRegisteredEventListeners, where this.dispatcher was unexpectedly null [2].

I think both places should be fixed (NPE guard in ObservationManagerImpl constructor for ""dispatcher"") and a try/catch block in the finalizer, just logging the exception:

    } finally {
        if (!succeeded) {
            try {
                // repository startup failed, clean up...
                shutdown();
            } catch (Throwable t) {
                // shutdown() likely to fail now, as startup was broken...
                log.error(""In addition to startup fail, another problem occurred while shutting down the repository again."", e);
            }
        }
    }


[1] http://svn.apache.org/viewvc/jackrabbit/trunk/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/RepositoryImpl.java?view=markup

[2] Overlaying exception's stacktrace:
Caused by: java.lang.NullPointerException
	at org.apache.jackrabbit.core.observation.ObservationManagerImpl.getRegisteredEventListeners(ObservationManagerImpl.java:143)
	at org.apache.jackrabbit.core.SessionImpl.removeRegisteredEventListeners(SessionImpl.java:1190)
	at org.apache.jackrabbit.core.SessionImpl.logout(SessionImpl.java:1215)
	at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.doDispose(RepositoryImpl.java:2153)
	at com.day.crx.core.CRXRepositoryImpl$CRXWorkspaceInfo.doDispose(CRXRepositoryImpl.java:1095)
	at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.dispose(RepositoryImpl.java:2108)
	at org.apache.jackrabbit.core.RepositoryImpl.doShutdown(RepositoryImpl.java:1146)
	at com.day.crx.core.CRXRepositoryImpl.doShutdown(CRXRepositoryImpl.java:845)
	at org.apache.jackrabbit.core.RepositoryImpl.shutdown(RepositoryImpl.java:1098)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:387)
	at com.day.crx.core.CRXRepositoryImpl.<init>(CRXRepositoryImpl.java:201)
	at com.day.crx.core.CRXRepositoryImpl.create(CRXRepositoryImpl.java:190)
	... 28 more
"
0,"rev. 169301: wrong directory name in build.xmlbuild.xml mentions non-existing directory contrib/WordNet/ which should read
contrib/wordnet in line 418.

below the result of svn diff against the corrected and working version of build.xml

--- build.xml   (revision 169301)
+++ build.xml   (working copy)
@@ -415,7 +415,7 @@
         <!-- TODO: find a dynamic way to do include multiple source roots -->
         <packageset dir=""src/java""/>
         <packageset dir=""contrib/analyzers/src/java""/>
-        <packageset dir=""contrib/WordNet/src/java""/>
+        <packageset dir=""contrib/wordnet/src/java""/>
         <packageset dir=""contrib/highlighter/src/java""/>
         <packageset dir=""contrib/similarity/src/java""/>
         <packageset dir=""contrib/spellchecker/src/java""/>"
0,"Automatic license header checking using the Apache Rat PluginTo avoid problems with incorrect license headers, we should include some automated header check in the Maven build and have Hudson run the check whenever changes are committed."
0,"Node.addNode() does not scale with increasing contentWith increasing repository content (and versions), the time to create new nodes increases. For example with around 6500 nodes and 33500 properties, it takes around 3 seconds (!) to just add one single node !

When attaching to the application with a Debugger and delibaretly suspending the VM, this stack trace is displayed all the times :

   [ changing internals of access List iterator ]
   PersistentNodeState(NodeState).getChildNodeEntries(String) line: 362
   PersistentNode.getName() line: 84
   PersistentVersionManager.getVersion(String) line: 278
   VersionManager.getVersion(String) line: 304
   VersionItemStateProvider.getNodeState(NodeId) line: 124
   VersionItemStateProvider.hasPropertyState(PropertyId) line: 154
   VersionItemStateProvider.hasItemState(ItemId) line: 174
   SessionItemStateManager.getItemState(ItemId) line: 246
   ItemManager.createItemInstance(ItemId) line: 563
   ItemManager.getItem(ItemId) line: 332
   NodeImpl.getProperty(QName) line: 876
   NodeImpl.hasProperty(QName) line: 893
   NodeImpl.safeIsCheckedOut() line: 2515
   NodeImpl.internalAddChildNode(QName, NodeTypeImpl, String) line: 527
   NodeImpl.internalAddNode(String, NodeTypeImpl, String) line: 475
   NodeImpl.internalAddNode(String, NodeTypeImpl) line: 436
   NodeImpl.addNode(String, String) line: 1145
   ...

It seems, that virtual item state providers are asked for whatever property is looked for and this in return calls into the version handler, which loops over some child entries (currently around 1100 entries) to find one single entry with a given UUID.

Besides the latter not being optimal and certainly not scaling, the former has its problems in its own right."
0,"Build SegmentCodecs incrementally for consistent codecIDs during indexingcurrently we build the SegementCodecs during flush which is fine as long as no codec needs to know which fields it should handle. This will change with DocValues or when we expose StoredFields / TermVectors via Codec (see LUCENE-2621 or LUCENE-2935). The other downside it that we don't have a consistent view of which codec belongs to which field during indexing and all FieldInfo instances are unassigned (set to -1). Instead we should build the SegmentCodecs incrementally as fields come in so no matter when a codec needs to be selected to process a document / field we have the right codec ID assigned.

"
0,"Set up a release goal in MavenCreate a single Maven goal for building the Jackrabbit release packages. The goal should be based on the standard Maven dist goal, but include also the jackrabbit-commons and other modules we want to include in the release."
0,"TSCCM code cleanupThe ThreadSafeClientConnectionManager, or rather it's ConnPoolByRoute, needs plenty of cleanup.
- use long + TimeUnit for timeout intervals (Java 5 style)
- compute timeout end date once instead of remaining interval
- review which methods should acquire the pool lock,
  and which should expect the caller to have done that
- use factory methods to instantiate some of the helper objects
"
0,"Fix buggy stemmers and Remove duplicate analysis functionalitywould like to remove stemmers in the following packages, and instead in their analyzers use a SnowballStemFilter instead.

* analyzers/fr
* analyzers/nl
* analyzers/ru

below are excerpts from this code where they proudly proclaim they use the snowball algorithm.
I think we should delete all of this custom stemming code in favor of the actual snowball package.


{noformat}
/**
 * A stemmer for French words. 
 * <p>
 * The algorithm is based on the work of
 * Dr Martin Porter on his snowball project<br>
 * refer to http://snowball.sourceforge.net/french/stemmer.html<br>
 * (French stemming algorithm) for details
 * </p>
 */

public class FrenchStemmer {

/**
 * A stemmer for Dutch words. 
 * <p>
 * The algorithm is an implementation of
 * the <a href=""http://snowball.tartarus.org/algorithms/dutch/stemmer.html"">dutch stemming</a>
 * algorithm in Martin Porter's snowball project.
 * </p>
 */
public class DutchStemmer {

/**
 * Russian stemming algorithm implementation (see http://snowball.sourceforge.net for detailed description).
 */
class RussianStemmer
{noformat}

"
0,"Cutover remaining usage of pre-flex APIsA number of places still use the pre-flex APIs.

This is actually healthy, since it gives us ongoing testing of the back compat emulation layer.

But we should at some point cut them all over to flex.  Latest we can do this is 4.0, but I'm not sure we should do them all for 3.1... still marking this as 3.1 to ""remind us"" :)"
0,"Document order of result nodes should be configurableQueries without an order by clause are performed with document order for the result nodes. This is a quite expensive operation, because the document order is available in the search index itself. The document order is calculated with the help of the ItemStateManager and requires loading of all result node states including their ancestors.

Queries with a lot of result nodes become quite expensive, even though the actual query execution is fast. Because most use cases will not care for the document order, this feature should be made configurable. Some parameter for the QueryHandler that disables the document order on result nodes."
0,"Provide a Method getCredentialsProvider to the SimpleWebdavServletIt will be useful to provide a easy way to change the default CredentialsProvider (BasicCredentialsProvider) when the SessionProvider will be created.
It makes sense to let the SessionProvider return a other CredentialsProvider so that no BasicAuthentication wil be prompt.
thanks
claus"
0,"Add relative path parameter to rep:excerpt()This allows one to create an excerpt not just for the node associated with a result node, but also for a node relative to the result node."
0,"Minor improvement to JavaDoc for ScoreDocComparatorAbout to attach a very small patch for ScoreDocComparator which broadens the contract of compare(ScoreDoc, ScoreDoc) to follow the same semantics as java.util.Comparator.compare() -- allow any integer to be returned, rather than specifically -1/0/-1.

Note that this behaviour must already be acceptable; the anonymous ScoreDocComparators returned by FieldSortedHitQueue.comparatorStringLocale() already return the result of Collator.compare(), which is not tied to this -1/0/1 restriction."
0,"[PATCH] Make a getter for SortField[] fields in org.apache.lucene.search.SortI'm have my own Collector and I would like to use the Sort object within my
collector, but SortField[] fields; is not accessible outside Lucene's package.
Can you please consider making a public getFields() method in the Sort object so
we can use it in our implementation?"
0,"Logging into a repository with a big version history takes a long timeWenn a SessionImpl instance is created, the VersionManager.getVirtualItemStateProvider method is called. This method - amongst other things - loads the complete (!) version history into memory and walks through it to do some mapping.

Besides taking a long time (near 1 minute just to get the version history through PersistentVersionManager.getVersionHistories()) mapping the version histories ultimately results in an ""OutOfMemoryError"".

Currently there are 768 version histories and this is only a very small fraction of the expected final number of version histories in my application"
0,"Changes.html formatting improvementsSome improvements to the Changes.html generated by the changes2html.pl script via the 'changes-to-html' ant task:

# Simplified the Simple stylesheet (removed monospace font specification) and made it the default.  The Fancy stylesheet is really hard for me to look at (yellow text on light blue background may provide high contrast with low eye strain, but IMHO it's ugly).
# Moved the monospace style from the Simple stylesheet to a new stylesheet named ""Fixed Width""
# Fixed syntax errors in the Fancy stylesheet, so that it displays as intended.
# Added <span style=""attrib"">  to change attributions.
# In the Fancy and Simple stylesheets, change attributions are colored dark green.
# Now properly handling change attributions in CHANGES.txt that have trailing periods.
# Clicking on an anchor to expand its children now changes the document location to show the children.
# Hovering over anchors now causes a tooltip to be displayed - either ""Click to expand"" or ""Click to collapse"" - the tooltip changes appropriately after a collapse or expansion."
0,standard codec's terms dict seek should only scan if new term is in same index blockTermInfosReader in trunk already optimizes for this case... just need to do the same on flex.
0,"Confusing Javadoc in Searchable.javaIn Searchable.java, the javadoc for maxdoc() is:

  /** Expert: Returns one greater than the largest possible document number.
   * Called by search code to compute term weights.
   * @see org.apache.lucene.index.IndexReader#maxDoc()

The qualification ""expert"" and the statement ""called by search code to compute term weights"" is a bit confusing, It implies that maxdoc() somehow computes weights, which is obviously not true (what it does is explained in the other sentence). Maybe it is used as one factor of the weight, but do we really need to mention this here? "
0,"Stop using BaseExceptionThe o.a.j.BaseException class is deprecated (since JCR-1169) and not caught anywhere, so there's no need to keep using it."
0,"document LengthFilter wrt Unicode 4.0LengthFilter calculates its min/max length from TermAttribute.termLength()
This is not characters, but instead UTF-16 code units.

In my opinion this should not be changed, merely documented.
If we changed it, it would have an adverse performance impact because we would have to actually calculate Character.codePointCount() on the text.

If you feel strongly otherwise, fixing it to count codepoints would be a trivial patch, but I'd rather not hurt performance.
I admit I don't fully understand all the use cases for this filter.
"
0,"[PATCH] Avoid checking for TermBuffer in SegmentTermEnum#scanToIt seems that SegmentTermEnum#scanTo is a critical method which is called very often, especially whenever we iterate over a sequence of terms in an index.

When that method is called, the first thing happens is that it checks whether a temporary TermBuffer ""scratch"" has already been initialized.

In fact, this is not necessary. We can simply declare and initialize the ""scratch""-Buffer at the class-level (right now, the initial value is _null_). Java's lazy-loading capabilities allow this without adding any memory footprint for cases where we do not need that buffer.

The attached patch takes care of this. We now save one comparison per term.
In addition to that, the patch renames ""scratch"" to ""scanBuffer"", which aligns with the naming of the other two buffers that are declared in the class."
0,"Document number integrity merge policyThis patch allows for document numbers stays the same even after merge of segments with deletions.

Consumer needs to do this:
indexWriter.setSkipMergingDeletedDocuments(false);

The effect will be that deleted documents are replaced by a new Document() in the merged segment, but not marked as deleted. This should probably be some policy thingy that allows for different solutions such as keeping the old document, et c.

Also see http://www.nabble.com/optimization-behaviour-tf3723327.html#a10418880
"
0,"Remove code duplication in MultiReader/DirectoryReader, make everything inside finalAfter making IndexReader readOnly (LUCENE-3606) there is no need to have completely different DirectoryReader and MultiReader, the current code is heavy code duplication and violations against finalness patterns. There are only few differences in reopen and things like isCurrent/getDirectory/...

This issue will clean this up by introducing a hidden package-private base class for both and only handling reopen and incRef/decRef different. DirectoryReader is now final and all fields in BaseMultiReader, MultiReader and DirectoryReader are final now. DirectoryReader has now only static factories, no public ctor anymore."
0,Collection parameter of CompactNodeTypeDefWriter#write should be covariantThe Collection<QNodeTypeDefinition> parameter of the CompactNodeTypeDefWriter#write methods should have type Collection<? extends QNodeTypeDefinition>. 
0,Move to a newer vesion of Commons CollectionsIt would be useful if the developer wants to use jackrabbit in an application that uses a newer version of this library.
0,"Disk full during addIndexes(Directory[]) can corrupt indexThis is a spinoff of LUCENE-555

If the disk fills up during this call then the committed segments file can reference segments that were not written.  Then the whole index becomes unusable.

Does anyone know of any other cases where disk full could corrupt the index?

I think disk full should worse lose the documents that were ""in flight"" at the time.  It shouldn't corrupt the index."
0,"TimeLimitingCollector starts thread in static {} with no way to stop themSee the comment in LuceneTestCase.

If you even do Class.forName(""TimeLimitingCollector"") it starts up a thread in a static method, and there isn't a way to kill it.

This is broken."
0,"Build should enable unchecked warnings in javacJust have to uncomment this:
{code}
        <!-- for generics in Java 1.5: -->
        <!--<compilerarg line=""-Xlint:unchecked""/>-->
{code}
in common-build.xml.  Test & core are clean, but contrib still has many warnings.  Either we fix contrib with this issue, or, conditionalize this (anyone anty who can do this?) so contrib is off until we can fix it."
0,"MergePolicy should require an IndexWriter upon constructionMergePolicy does not require an IW upon construction, but requires one to be passed as method arg to various methods. This gives the impression as if a single MP instance can be shared across various IW instances, which is not true for all MPs (if at all). In addition, LogMergePolicy uses the IW instance passed to these methods incosistently, and is currently exposed to potential NPEs.

This issue will change MP to require an IW instance, however for back-compat reasons the following changes will be made:
# A new MP ctor w/ IW as arg will be introduced. Additionally, for back-compat a default ctor will also be declared which will assign null to the member IW.
# Methods that require IW will be deprecated, and new ones will be declared.
#* For back-compat, the new ones will not be made abstract, but will throw UOE, with a comment that they will become abstract in 3.0.
# All current MP impls will move to use the member instance.
# The code which calls MP methods will continue to use the deprecated methods, passing an IW even that it won't be necessary --> this is strictly for back-compat.

In 3.0, we'll remove the deprecated default ctor and methods, and change the code to not call the IW method variants anymore.

I hope that I didn't leave anything out. I'm sure I'll find out when I work on the patch :)."
0,"Reduce Fieldable, AbstractField and Field complexityIn order to move field type like functionality into its own class, we really need to try to tackle the hierarchy of Fieldable, AbstractField and Field.  Currently AbstractField depends on Field, and does not provide much more functionality that storing fields, most of which are being moved over to FieldType.  Therefore it seems ideal to try to deprecate AbstractField (and possible Fieldable), moving much of the functionality into Field and FieldType."
0,"TopDocsCollector should have bounded generic <T extends ScoreDoc>TopDocsCollector was changed to be TopDocsCollector<T>. However it has methods which specifically assume the PQ stores ScoreDoc. Therefore, if someone extends it and defines a type which is not ScoreDoc, things will break.

We shouldn't put <T> on TopDocsCollector at all, but rather change its ctor to *protected TopDocsCollector(PriorityQueue<? extends ScoreDoc> pq)*. TopDocsCollector should handle ScoreDoc types. If we do this, we'll need to change FieldValueHitQueue's Entry to extend ScoreDoc as well."
0,"Enable bzip compression in benchmarkbzip compression can aid the benchmark package by not requiring extracting bzip files (such as enwiki) in order to index them. The plan is to add a config parameter bzip.compression=true/false and in the relevant tasks either decompress the input file or compress the output file using the bzip streams.
It will add a dependency on ant.jar which contains two classes similar to GZIPOutputStream and GZIPInputStream which compress/decompress files using the bzip algorithm.

bzip is known to be superior in its compression performance to the gzip algorithm (~20% better compression), although it does the compression/decompression a bit slower.

I wil post a patch which adds this parameter and implement it in LineDocMaker, EnwikiDocMaker and WriteLineDoc task. Maybe even add the capability to DocMaker or some of the super classes, so it can be inherited by all sub-classes."
0,"Jcr-server: Report#init limits the Report interface to DeltaV compliant resourcesAlthough the REPORT features is defined by RFC 3253 and required for DeltaV compliant resources it is not limited to deltaV. See RFC 3744 (WebDAV Access Control Protocol) for additional usage of the REPORT functionality.

In order to keep the Report interface open for later usage by resources providing support for RFC 3744,
the init method signature could be modified as follows:

Report#init(DavResource, ReportInfo) instead of
Report#init(DeltaVResource, ReportInfo)"
0,Test must not fail if mixin cannot be addedNearly all mixin types are optional and a test must not fail if a mixin cannot be added. It should rather throw a NotExecutableException. Some tests still try to add a mixin without checking whether an implementation supports it.
0,"Make contrib/collation/(ICU)CollationKeyAnalyzer constructors publicIn contrib/collation, the constructors for CollationKeyAnalyzer and ICUCollationKeyAnalyzer are package private, and so are effectively unusable."
0,"Default namespaces in JackrabbitNodeTypeManager.registerNodetypesIt would be nice if it wasn't necessary to always specify all the namespaces in node type definition files passed to JackrabbitNodeTypeManager.registerNodeTypes(). The node type parsers should by default use the persistent namespace mappings, but allow custom mappigns to be specified in the parsed node type definition files."
0,"Move to jackrabbit.apache.orgJackrabbit will be moving to

   http://jackrabbit.apache.org/

There are a number of infrastructure tasks that will need to be done by Roy.

There will also be a need to change our documentation and site to point to the new URL
and mailing list addresses, which can be done by anyone.

The existing mailing lists will be moved

    jackrabbit-dev at incubator  -->  dev at jackrabbit.apache.org
    jackrabbit-commits at incubator  -->  commits at jackrabbit.apache.org

and I will add

   users at jackrabbit.apache.org
"
0,Factor out SearcherManager from NRTManagerCurrently we have NRTManager and SearcherManager while NRTManager contains a big piece of the code that is already in SearcherManager. Users are kind of forced to use NRTManager if they want to have SearcherManager goodness with NRT. The integration into NRTManager also forces you to maintain two instances even if you know you always want deletes. To me NRTManager tries to do more than necessary and mixes lots of responsibilities ie. handling searchers and handling indexing generations. NRTManager should use a SearcherManager by aggregation rather than duplicate a lot of logic. SearcherManager should have a NRT and Directory based implementation users can simply choose from.
0,"Rename lucene/solr dev jar files to -SNAPSHOT.jarCurrently the lucene dev jar files end with '-dev.jar' this is all fine, but it makes people using maven jump through a few hoops to get the -SNAPSHOT naming convention required by maven.  If we want to publish snapshot builds with hudson, we would need to either write some crazy scripts or run the build twice.

I suggest we switch to -SNAPSHOT.jar.  Hopefully for the 3.x branch and for the /trunk (4.x) branch"
0,"Introduce daily integration test suiteSome time ago we discussed integration tests that would be run on a daily basis. See also comments in issue JCR-1452. It seems we reached consensus that running a daily integration test suite is desirable.

Here's my proposal:

- Introduce a test suite org.apache.jackrabbit.core.integration.daily.DailyIntegrationTest which includes all tests that should be run on a daily basis.
- Configure our continuous integration system to run the test suite on a daily basis. e.g. mvn -Dtest=DailyIntegrationTest package

With this approach we don't need to introduce maven profiles or any other pom magic, yet it's easy for a developer to run the daily tests when needed."
0,Make inspection of BooleanQuery more efficientJust attempting to inspect a BooleanQuery allocates two new arrays.  This could be cheaper.
0,"Add maven-eclipse-plugin properties to project.xml for easier configuration in IDE- add the maven.eclipse.resources.addtoclasspath=true property to project.properties (make the eclipse plugin create source dirs also for resources). 

- add the <eclipse.dependency>true</eclipse.dependency> property to all the jackrabbit internal dependencies in all the POMs (all the dependencies with ""jackrabbit"" groupId) so internal dependencies becomes project dependencies in eclipse."
0,Remove excessive dependencies from jcr-client module
0,"Support easy pre-authenticated loginSome applications authenticate users themselves and just need to access the repository on behalf of these pre-authenticated users.

Examples of such pre-authentications include SSO solutions or web applications using a web-based authentication protocol not easily implementable in a JAAS LoginModule, for example OpenID or similar.

In such situations a password may not be provided in SimpleCredentials and thus regular login with user name and password is not possible.

Therefore I propose the enhancement of the AbstractLoginModule to allow for setting a specific attribute in the SimpleCredentials attribute map. If this attribute is set, authentication and login succeeds and a session for the user named in the SimpleCredentials is created.

As a starter we might just check for the presence of the attribute."
0,"Registering multiple node types with the same name in a single file must failRegistering node types from a file (XML or CND) containing multiple definitions with the same name will succeed and only the last definition will be used.
The right behavior is to throw an exception as this kind of file is well-formed but invalid."
0,"update NOTICE.txtFrom the java-dev discussion, NOTICE.txt should be up-to-date.

One thing I know, is that the persian stopwords file (analyzers/fa) came from the same source as the arabic stopwords file, and is BSD-licensed. 

There might be others (I think ICU has already been added)"
0,"Add AttributeSource.copyTo(AttributeSource)One problem with AttributeSource at the moment is the missing ""insight"" into AttributeSource.State. If you want to create TokenStreams that inspect cpatured states, you have no chance. Making the contents of State public is a bad idea, as it does not help for inspecting (its a linked list, so you have to iterate).

AttributeSource currently contains a cloneAttributes() call, which returns a new AttrubuteSource with all current attributes cloned. This is the (more expensive) captureState. The problem is that you cannot copy back the cloned AS (which is the restoreState). To use this behaviour (by the way, ShingleMatrix can use it), one can alternatively use cloneAttributes and copyTo. You can easily change the cloned attributes and store them in lists and copy them back. The only problem is lower performance of these calls (as State is a very optimized class).

One use case could be:
{code}
AttributeSource state = cloneAttributes();
// .... do something ...
state.getAttribute(TermAttribute.class).setTermBuffer(foobar);
// ... more work
state.copyTo(this);
{code}"
0,"Benchmark contrib should allow multiple locations in ext.classpathWhen {{ant run-task}} is invoked with the  {{-Dbenchmark.ext.classpath=...}} option, only a single location may be specified.  If a classpath with more than one location is specified, none of the locations is put on the classpath for the invoked JVM."
0,"REFERENCE properties produce duplicate strings in memoryWhen reference property is loaded from PM, Serializer.deserialize(NodeReferences, InputStream) is called, which calls PropertyId.valueOf(String), which in turn calls NameFactoryImpl.create(String) which finally splits a full property name to namespace and local name. Namespace is internalized, but local name is not (comments say that this is done to avoid perm space overfilling).
So, in the end, a new String instance is created for local name. This leads to considerable memory waste when repository has a lot of nodes with REFERENCE properties.
It seems that local name part could be internalized here too because in the most repositories it's not allowed to create properties with arbitrary names, so the danger of perm space exhaust does not seem to be an argument.

As for ways to resolve this, maybe a new NameFactory implementation could be created which would be used for properties only (and, possibly, mainly in the PropertyId.valueOf(String)) which would extend an existing NameFactoryImpl overriding its create(String) method.

What do you think about all this?"
0,"[OCM] rename o.a.j.ocm.persistence.PersistenceManager to avoid confusion with core componentPersistenceManager is a well known and established interface in jackrabbit's architecture. the same-named class in the
jcr-mapping contrib project should IMO be renamed in order to avoid confusion in mailing list threads and jira issues,"
0,"Review the use of BaiscHttpParams and HttpProtocolProcessor in HttpClientReview the use of BaiscHttpParams and HttpProtocolProcessor in HttpClient and replace with thread-safe implementations where necessary.

Oleg"
0,"Behavior on hard power shutdownWhen indexing a large number of documents, upon a hard power failure  (e.g. pull the power cord), the index seems to get corrupted. We start a Java application as an Windows Service, and feed it documents. In some cases (after an index size of 1.7GB, with 30-40 index segment .cfs files) , the following is observed.

The 'segments' file contains only zeros. Its size is 265 bytes - all bytes are zeros.
The 'deleted' file also contains only zeros. Its size is 85 bytes - all bytes are zeros.

Before corruption, the segments file and deleted file appear to be correct. After this corruption, the index is corrupted and lost.

This is a problem observed in Lucene 1.4.3. We are not able to upgrade our customer deployments to 1.9 or later version, but would be happy to back-port a patch, if the patch is small enough and if this problem is already solved.
"
0,"Support DateTools in QueryParserThe QueryParser currently uses the deprecated class DateField to create RangeQueries with date values. However, most users probably use DateTools to store date values in their indexes, because this is the recommended way since DateField has been deprecated. In that case RangeQueries with date values produced by the QueryParser won't work with those indexes.

This patch replaces the use of DateField in QueryParser by DateTools. Because DateTools can produce date values with different resolutions, this patch adds the following methods to QueryParser:

  /**
   * Sets the default date resolution used by RangeQueries for fields for which no
   * specific date resolutions has been set. Field specific resolutions can be set
   * with {@link #setDateResolution(String, DateTools.Resolution)}.
   *  
   * @param dateResolution the default date resolution to set
   */
  public void setDateResolution(DateTools.Resolution dateResolution);
  
  /**
   * Sets the date resolution used by RangeQueries for a specific field.
   *  
   * @param field field for which the date resolution is to be set 
   * @param dateResolution date resolution to set
   */
  public void setDateResolution(String fieldName, DateTools.Resolution dateResolution);

(I also added the corresponding getter methods).

Now the user can set a default date resolution used for all fields or, with the second method, field specific date resolutions.
The initial default resolution, which is used if the user does not set a different resolution, is DateTools.Resolution.DAY. 

Please let me know if you think we should use a different resolution as default.

I extended TestQueryParser to test this new feature.

All unit tests pass.
"
0,"RepositoryLock does not work on NFS sometimesThe RepositoryLock mechanism currently used in Jackrabbit uses FileLock. This doesn't work on some NFS file system. It looks like only NFS version 4 and newer supports locking. Older implementations may throw a IOException ""No locks available"", which means the NFS does not support byte-range locking.

I propose to add a second locking mechanism, and add a configuration option to use it. For example: <FileLocking class=""acme"" />. This second locking mechanism is a cooperative locking protocol that uses a background (watchdog) thread and only uses regular file operations.

"
0,Favour QValue.getPath() over getString() where appropriateTo avoid extra conversion round trips QValue.getPath() should be used instead of  QValue.getString() where appropriate.
0,"Exclude the netcdf dependencyAs discussed on the mailing list, the netcdf dependency we get through Tika since version 0.8 is only used in very rare cases and thus does not justify the added size overhead. We should thus exclude it from default installations."
0,"Excessive Arrays.fill(0) in DocumentsWriter drastically slows down small docs (3.9X slowdown!)I've been doing some ""final"" performance testing of 2.3RC1 and
uncovered a fairly serious bug that adds a large fixed CPU cost when
documents have any term vector enabled fields.

The bug does not affect correctness, just performance.

Basically, for every document, we were calling Arrays.fill(0) on a
large (32 KB) byte array when in fact we only needed to zero a small
part of it.  This only happens if term vectors are turned on, and is
especially devastating for small documents."
0,"more performance improvements for snowballi took a more serious look at snowball after LUCENE-2194.

This gives greatly improved performance, but note it has some minor breaks to snowball internals:
* Among.s becomes a char[] instead of a string
* SnowballProgram.current becomes a char[] instead of a StringBuilder
* SnowballProgram.eq_s(int, String) becomes eq_s(int, CharSequence), so that eq_v(StringBuilder) doesnt need to create an extra string.
* same as the above with eq_s_b and eq_v_b
* replace_s(int, int, String) becomes replace_s(int, int, CharSequence), so that StringBuilder-based slice and insertion methods don't need to create an extra string.

all of these ""breaks"" imho are only theoretical, the problem is just that pretty much everything is public or protected in the snowball internals.

the performance improvement here depends heavily upon the snowball language in use, but its way more significant than LUCENE-2194.
"
0,"Logger (Category) names don't follow common patternThe Wire class uses two loggers named unexpected. The ""org.apache.commons.""
prefix is missing - so you can't mute all debug level statements with a
one-liner in you log4j.properties for example:

  log4j.logger.org.apache.commons.httpclient INFO

You have to add this, too:

  log4j.logger.httpclient INFO

Please prepend the ""org.apache.commons."" before both names.

Cheers,
Christian

<code>
class Wire {

    public static Wire HEADER_WIRE = new
Wire(LogFactory.getLog(""httpclient.wire.header""));
    
    public static Wire CONTENT_WIRE = new
Wire(LogFactory.getLog(""httpclient.wire.content""));

</code>

http://svn.apache.org/viewcvs.cgi/jakarta/commons/proper/httpclient/trunk/src/java/org/apache/commons/httpclient/Wire.java?rev=155418&view=markup"
0,"Clone proxStream lazily in SegmentTermPositionsIn SegmentTermPositions the proxStream should be cloned lazily, i. e. at the first time nextPosition() is called. Then the initialization costs of TermPositions are not higher anymore compared to TermDocs and thus there is no reason anymore for Scorers to use TermDocs instead of TermPositions. In fact, all Scorers should use TermPositions, because custom subclasses of existing scorers might want to access payloads, which is only possible via TermPositions. We could further merge SegmentTermDocs and SegmentTermPositions into one class and deprecate the interface TermDocs.

I'm going to attach a patch once the payloads feature (LUCENE-755) is committed."
0, Reduce number of compiler warning by adding @Override and generics where appropriate Same as JCR-2482 for the webdav library and the modules using it.
0,"Improve BufferedIndexInput.readBytes() performanceDuring a profiling session, I discovered that BufferedIndexInput.readBytes(),
the function which reads a bunch of bytes from an index, is very inefficient
in many cases. It is efficient for one or two bytes, and also efficient
for a very large number of bytes (e.g., when the norms are read all at once);
But for anything in between (e.g., 100 bytes), it is a performance disaster.
It can easily be improved, though, and below I include a patch to do that.

The basic problem in the existing code was that if you ask it to read 100
bytes, readBytes() simply calls readByte() 100 times in a loop, which means
we check byte after byte if the buffer has another character, instead of just
checking once how many bytes we have left, and copy them all at once.

My version, attached below, copies these 100 bytes if they are available at
bulk (using System.arraycopy), and if less than 100 are available, whatever
is available gets copied, and then the rest. (as before, when a very large
number of bytes is requested, it is read directly into the final buffer).

In my profiling, this fix caused amazing performance
improvement: previously, BufferedIndexInput.readBytes() took as much as 25%
of the run time, and after the fix, this was down to 1% of the run time! However, my scenario is *not* the typical Lucene code, but rather a version of Lucene with added payloads, and these payloads average at 100 bytes, where the original readBytes() did worst. I expect that my fix will have less of an impact on ""vanilla"" Lucene, but it still can have an impact because it is used for things like reading fields. (I am not aware of a standard Lucene benchmark, so I can't provide benchmarks on a more typical case).

In addition to the change to readBytes(), my attached patch also adds a new
unit test to BufferedIndexInput (which previously did not have a unit test).
This test simulates a ""file"" which contains a predictable series of bytes, and
then tries to read from it with readByte() and readButes() with various
sizes (many thousands of combinations are tried) and see that exactly the
expected bytes are read. This test is independent of my new readBytes()
inplementation, and can be used to check the old implementation as well.

By the way, it's interesting that BufferedIndexOutput.writeBytes was already efficient, and wasn't simply a loop of writeByte(). Only the reading code was inefficient. I wonder why this happened."
0,"codec postings api (finishDoc) is inconsistentfinishDoc says:

{noformat}
  /** Called when we are done adding positions & payloads
   *  for each doc.  Not called  when the field omits term
   *  freq and positions. */
   public abstract void finishDoc() throws IOException;
{noformat}

But this is confusing (because a field can omit just positions, is it called then?!),
and wrong (because merging calls it always, even if freq+positions is omitted).

I think we should fix the javadoc and fix FreqProxTermsWriter to always call finish()
"
0,java.lang.Iterable support for RangeIteratorsMake javax.jcr.RangeIterator extend java.lang.Iterable in order to enable foreach loops on implementations of RangeIterator.
0,"RepositoryStub implementation in jackrabbit-coreCurrently setting up a Jackrabbit repository for use with the TCK is a relatively complex operation with a large repositoryStubImpl.properties file and lots of specially crafted test content and settings to worry about. This makes it hard to set up new TCK test instances with the various JCR and SPI layers we now have.

To simplify things I'd like to introduce a RepositoryStubImpl class and related configuration files inside src/main/java in jackrabbit-core."
0,Land DWPT on trunkWith LUCENE-2956 we have resolved the last remaining issue for LUCENE-2324 so we can proceed landing the DWPT development on trunk soon. I think one of the bigger issues here is to make sure that all JavaDocs for IW etc. are still correct though. I will start going through that first.
0,"DirectoryTaxonomyWriter should throw a proper exception if it was closedDirTaxoWriter may throw random exceptions (NPE, Already Closed - depend on what API you call) after it has been closed/rollback. We should detect up front that it is already closed, and throw AlreadyClosedException.

Also, on LUCENE-3573 Doron pointed out a problem with DTW.rollback -- it should call close() rather than refreshReader. I will fix that as well in this issue."
0,"Set source and output encoding in POMsModification to POM files to explicitly set the source and report encoding. I've set everything to UTF-8, but this may not be appropriate. However, the encoding properties should be set to ensure that source files are compiled correctly, resources are filtered appropriately and that the reports are using a consistent encoding.

Related info
http://docs.codehaus.org/display/MAVENUSER/POM+Element+for+Source+File+Encoding
http://docs.codehaus.org/display/MAVEN/Reporting+Encoding+Configuration"
0,spi2dav: JSR 283 NodeType Management
0,"Add log.step support per taskFollowing LUCENE-1774, this will add support for log.step per task name, rather than a single log.step setting for all tasks. The .alg file will support:
* log.step - for all tasks.
* log.step.<Task Class Name> - for a specific task. For example, log.step.AddDoc, or log.step.DeleteDoc

I will post the patch soon"
0,"TRStringDistance uses way too much memory (with patch)The implementation of TRStringDistance is based on version 2.1 of org.apache.commons.lang.StringUtils#getLevenshteinDistance(String, String), which uses an un-optimized implementation of the Levenshtein Distance algorithm (it uses way too much memory). Please see Bug 38911 (http://issues.apache.org/bugzilla/show_bug.cgi?id=38911) for more information.

The commons-lang implementation has been heavily optimized as of version 2.2 (3x speed-up). I have reported the new implementation to TRStringDistance."
0,Provide additional test coverage for HTTP and HTTPS over proxyHTTP and HTTPS over proxy test coverage is still insufficient
0,"auto close idle connectionsThis has been mentioned several times on the mailing list (most recently here:
http://nagoya.apache.org/eyebrowse/ReadMsg?listName=commons-httpclient-dev@jakarta.apache.org&msgNo=5191
)
It is desirable for the http client to close it's connection after some
configurable idle time. Failing to do so causes the server (and every TCP
resource in between) to keep the socket open and possibly run out of resources
under load.

The HTTP 1.1 RFC has this to say under section 8.1.4:
Servers will usually have some time-out value beyond which they will
   no longer maintain an inactive connection. Proxy servers might make
   this a higher value since it is likely that the client will be making
   more connections through the same server. The use of persistent
   connections places no requirements on the length (or existence) of
   this time-out for either the client or the server.

   When a client or server wishes to time-out it SHOULD issue a graceful
   close on the transport connection. Clients and servers SHOULD both
   constantly watch for the other side of the transport close, and
   respond to it as appropriate. If a client or server does not detect
   the other side's close promptly it could cause unnecessary resource
   drain on the network.

The first sentence of the 2nd paragraph is interesting: how is the client
supposed to do a ""graceful close""? Does it simply mean closing the socket?
One possiblity may be to issue a HTTP/OPTIONS * request with a Connection:close
header."
0,"Update link for javadocs from 1.0 to 1.3On this page: http://jackrabbit.apache.org/doc/arch/overview/jcrlevels.html

You see this link:
Browse current Jackrabbit API: http://jackrabbit.apache.org/api-1/index.html

This should point to the latest javadoc version."
0,"BoostingTermQuery's BoostingSpanScorer class should be protected instead of package accessCurrently, BoostingTermScorer, an inner class of BoostingTermQuery is not accessible from outside the search.payloads
making it difficult to write an extension of BoostingTermQuery. The other inner classes are protected already, as they should be."
0,Improve performance of simple path queriesQueries with simple path constraints can be quite slow because of the way they are implemented. The current implementation basically does a hierarchical join with the context nodes and the set of nodes with the name of the next location step. When the specified path is quite selective the implementation should   rather resolve the path expression using the item state manager (similar to how regular paths are resolved in the JCR API).
0,"There is no way to specify a different auth scheme priority for host and proxyUsing HttpClient 3.0 rc2, you cannot authenticate to a site using Basic
Autentication and to a proxy server using NTLM authentication.

When you indicate a prefference to use NTLM over Basic authentication the
authentication will fail when it tries to authenticate NTML to the Proxy and to
the Site. If you indicate Basic, then NTLM authentication order the Basic
authentication will fail when used for the Proxy (since basic authentication
can't send the domain name it fails).

The email thread from the discussion group is pasted below for refference.

==============================================================
==============================================================

Hi all,
I am trying to authenticate to a server via a proxy which also requires
authentication. It seems that I can get either the proxy authentication to work
OR the site authentication to work, but not both.

Both seem to work independently when I set the credentials (or proxy
credentials) using NTCredentials (e.g. if I connect to the site from a network
not using a proxy I can get it to work, and I can authenticate to the proxy only
to get a 401 authentication failed from the server when using the proxy).

I read in the Authentication tutorial that you can't authenticate using NTLM to
both the proxy and site, so I'm trying various combinations of authentication,
but I can't find any documentation that specifically covers this case and I feel
like I'm just taking stabs in the dark right now.

If anyone can point me in the direction of the light at the end of the tunnel
I'd really appreciate it.

Thanks,
David

----------------

On Wed, Jun 29, 2005 at 09:53:07AM -0700, David Parks wrote:
> Hi all,
> I am trying to authenticate to a server via a proxy which also requires
authentication. It seems that I can get either the proxy authentication to work
OR the site authentication to work, but not both.
> 
> Both seem to work independently when I set the credentials (or proxy
credentials) using NTCredentials (e.g. if I connect to the site from a network
not using a proxy I can get it to work, and I can authenticate to the proxy only
to get a 401 authentication failed from the server when using the proxy).
> 
> I read in the Authentication tutorial that you can't authenticate using NTLM
to both the proxy and site, so I'm trying various combinations of
authentication, but I can't find any documentation that specifically covers this
case and I feel like I'm just taking stabs in the dark right now.

David,

You _really_ can't use NTLM to authenticate with the proxy and the
target host at the same, due to the nature of this authentication
scheme. Really. That was not a joke.

Please consider using one of the following combinations instead:

(1) BASIC proxy + NTLM host if both the clent and the proxy are within a
trusted network segment

(2) NTLM proxy + SSL + BASIC host

Both combinations should provide an adequate (or better in the latter case)
security

Hope this helps

Oleg

> 
> If anyone can point me in the direction of the light at the end of the tunnel
I'd really appreciate it.
> 
> Thanks,
> David
> 
> 

-------------------

Thanks for the reply Oleg. This is what I figured, but I cannot see how to use
different authentication schemes for the Proxy vs. the Site authentication
challenge.

I tried adding the code suggested in the Authentication tutorial:

        List authPrefs = new ArrayList(2);
        authPrefs.add(AuthPolicy.DIGEST);
        authPrefs.add(AuthPolicy.BASIC);
        authPrefs.add(AuthPolicy.NTLM);
         This will exclude the NTLM authentication scheme
        httpclient.getParams().setParameter(AuthPolicy.AUTH_SCHEME_PRIORITY,
authPrefs);

I got a message stating that it was attempting BASIC authentication for the
Proxy and that it failed (probably because the domain doesn't get passed I
guess). So my thought is that I need NTLM for the proxy authentication and Basic
will work for the site authentication.

The question I am then working on is how to direct the HttpClient to select that
order of authentication methods. If I let it take NTLM as the preffered
authentication method then it will try to authenticate both challenges with NTLM.

I sure there is just some little detail I'm missing here somewhere, it's just
hard to find it.

Thanks a lot!
David

------------------

David,

I see the problem. This will require a patch and a new parameter.
Luckily the preference API introduced in HttpClient 3.0 allows up to add
parameters quite easily. Please file a feature request with Bugzilla
ASAP and I'll do my best to hack up a patch before I leave for holidays
(that is Friday, July 1st)

Oleg


--------------

Hi Oleg, thanks, I'll put that request in today.
This helps a lot, at least I know I'm on the right path now.

I am attempting to devise a workaround for this by handling the authentication
manually (setDoAuthentication(false)).

When I see a 401 error I am processing a basic authentication with the site
credentials, when I see a 407 error I want to process an NTLM authentication
with the proxy credentials.

To that end I have the following code that runs after
httpclient.execute(getmethod) executes. The code below works perfectly for the
basic authentication (when the proxy is not in the picture).

In looking up the Handshake of the NTLM authentication I see that I have a
problem with the code below since the handshake includes 2 challenge and
authorization steps before the authentication succeeds. I'm not clear how I
could manually authenticate the NTLM response. I would expect the NTLMScheme
class to contain a Type 1 and Type 3 authenticate() method for processing both
challenge responses. Is there another way of processing the NTLM authentication
after receiving the initial authentication challenge from the server?

        //Check for Proxy or Site authentication
        if(getmethod.getStatusCode() == 401){
            //Authenticate to the site using Basic authentication.
            BasicScheme basicscheme = new BasicScheme();
            String basic_auth_string = basicscheme.authenticate(new
NTCredentials(""cwftp"", ""664A754c"", """", """"), getmethod);
            Header basic_auth_header = new Header(""Authorization"",
basic_auth_string);
            getmethod.addRequestHeader(basic_auth_header);
            try{
                httpclient.executeMethod(getmethod);
            }catch(Exception e){
                logger.log(Level.SEVERE, ""ack!!!!"", e);
            }
            return getmethod;
       }else if(getmethod.getStatusCode() == 407){
            //Authenticate to the site using Basic authentication
            NTLMScheme ntlmscheme = new NTLMScheme();
            String basic_auth_string = ntlmscheme.authenticate(new
NTCredentials(""00mercbac"", ""!@SAMmerc2004"", ""simproxy"", ""CFC""), getmethod);
            Header basic_auth_header = new Header(""Authorization"",
basic_auth_string);
            getmethod.addRequestHeader(basic_auth_header);
            try{
                httpclient.executeMethod(getmethod);
            }catch(Exception e){
                logger.log(Level.SEVERE, ""ack!!!!"", e);
            }
            return getmethod;
       }


Thanks,
David"
0,"allow ResourceType dav property to have multiple valuesattached is a patch that allows the ResourceType dav property to have multiple values (useful for dav protocol extensions such as caldav). 

it is not a perfect patch, in that subclasses of ResourceType do not know about each others' resource types, but it is a decent start. one way to address this issue might be to have subclasses register extended resource types and their corresponding xml representations with ResourceType, removing the need for them to override resourceTypeToXml() and isValidResourceType().
"
0,"Allow setting the IndexWriter docstore to be a different directoryAdd an IndexWriter.setDocStoreDirectory method that allows doc
stores to be placed in a different directory than the IW default
dir."
0,"Eliminate unnecessary uses of Hashtable and VectorLucene uses Vector, Hashtable and Enumeration when it doesn't need to. Changing to ArrayList and HashMap may provide better performance.

There are a few places Vector shows up in the API. IMHO, List should have been used for parameters and return values.

There are a few distinct usages of these classes:
# internal but with ArrayList or HashMap would do as well. These can simply be replaced.
# internal and synchronization is required. Either leave as is or use a collections synchronization wrapper.
# As a parameter to a method where List or Map would do as well. For contrib, just replace. For core, deprecate current and add new method signature.
# Generated by JavaCC. (All *.jj files.) Nothing to be done here.
# As a base class. Not sure what to do here. (Only applies to SegmentInfos extends Vector, but it is not used in a safe manner in all places. Perhaps, implements List would be better.)
# As a return value from a package protected method, but synchronization is not used. Change return type.
# As a return value to a final method. Change to List or Map.

In using a Vector the following iteration pattern is frequently used.
for (int i = 0; i < v.size(); i++) {
  Object o = v.elementAt(i);
}

This is an indication that synchronization is unimportant. The list could change during iteration.

"
0,"Jackrabbit web page scroll is slow with FirefoxWhen I visit http://jackrabbit.apache.org/ from my Firefox in Ubuntu  the browser scroll is very slow and make CPU go to 100%. 

The problem seems to be the ""fixed"" attribute in the css background definition, so it should be removed.

body {
  background:white url(bg.png) repeat-x fixed center bottom;
  font-family:Verdana,Helvetica,Arial,sans-serif;
  font-size:small;
  margin:0pt;
  padding:0pt;
}"
0,"Query scorers should not use MultiFieldsLucene does all searching/filtering per-segment, today, but there are a number of tests that directly invoke Scorer.scorer or Filter.getDocIdSet on a composite reader."
0,"potential memory leak when using ThreadSafeClientConnManagerWhen using ThreadSafeClientConnManager and developing with Jetty using auto-redeploy feature eventually I run into a PermGen out of memory exception.  I investigated with YourKit 8.0.6 and found a class loader circular reference in RefQueueWorker.  Not really sure what I was doing I made the refQueueHandler non-final and nulled it in the shutdown method of RedQueueWorker.  I don't seem to have the problem any longer with circular class loader references.

Here is a diff from 4.0-beta2


--- httpclient/src/main/java/org/apache/http/impl/conn/tsccm/RefQueueWorker.jav(revision 763223)
+++ httpclient/src/main/java/org/apache/http/impl/conn/tsccm/RefQueueWorker.jav(working copy)
@@ -50,7 +50,7 @@
     protected final ReferenceQueue<?> refQueue;
 
     /** The handler for the references found. */
-    protected final RefQueueHandler refHandler;
+    protected RefQueueHandler refHandler;
 
 
     /**
@@ -112,6 +112,8 @@
             this.workerThread = null; // indicate shutdown
             wt.interrupt();
         }
+
+        refHandler = null;
     }
 
 
"
0,"Upgrade benchmark from commons-compress-1.0 to commons-compress-1.1 for 15 times faster gzip decompressionIn LUCENE-1540 TrecContentSource moved from Java's GZipInputStream to common-compress 1.0. 
This slowed down gzip decompression by a factor of 15. 
Upgrading to 1.1 solves this problem.
I verified that the problem is only in GZIP, not in BZIP.
On the way, as 1.1 introduced constants for the compression methods, the code can be made a bit nicer."
0,"Unnecessary assert in org.apache.lucene.index.DocumentsWriterThreadState.trimFields()In org.apache.lucene.index.DocumentsWriterThreadState.trimFields() is the following code:

      if (fp.lastGen == -1) {
        // This field was not seen since the previous
        // flush, so, free up its resources now

        // Unhash
        final int hashPos = fp.fieldInfo.name.hashCode() & fieldDataHashMask;
        DocumentsWriterFieldData last = null;
        DocumentsWriterFieldData fp0 = fieldDataHash[hashPos];
        while(fp0 != fp) {
          last = fp0;
          fp0 = fp0.next;
        }
        assert fp0 != null;

The assert at the end is not necessary as fp0 cannot be null.  The first line in the above code guarantees that fp is not null by the time the while loop is hit.  The while loop is exited when fp0 and fp are equal.  Since fp is not null then fp0 cannot be null when the while loop is exited, thus the assert is guaranteed to never occur.

This was detected by FindBugs."
0,"remove support for event bundle IDsEvent bundle IDs currently are not used. We can re-add them later in case we need them.
"
0,JSR 283: Retention & Hold Management
0,"Create a Size Estimator model for Lucene and SolrIt is often handy to be able to estimate the amount of memory and disk space that both Lucene and Solr use, given certain assumptions.  I intend to check in an Excel spreadsheet that allows people to estimate memory and disk usage for trunk.  I propose to put it under dev-tools, as I don't think it should be official documentation just yet and like the IDE stuff, we'll see how well it gets maintained."
0,"Improve name resolutionAs discussed in JCR-685, the current CachingNamespaceResolver class contains excessive synchronization causing monitor contention that reduces performance.

In JCR-685 there's a proposed patch that replaces synchronization with a read-write lock that would allow concurrent read access to the name cache."
0,"Move MemoryJournal from test to mainRunning our tests with the FileJournal implementation on a windows box can be quite slow because of the many FileDescriptor.sync() calls.

I'd like to move the MemoryJournal in jackrabbit-core test to the main sources. That way we can use it in other test setups."
0,"Database Data StoreWe want to have a database backed data store implementation.
An implementation using files is already available as part of JCR-926.
"
0,"clean up serialization in the codebaseWe removed contrib/remote, but forgot to cleanup serialization hell everywhere.

this is no longer needed, never really worked (e.g. across versions), and slows 
development (e.g. i wasted a long time debugging stupid serialization of 
Similarity.idfExplain when trying to make a patch for the scoring system).
"
0,"UUIDDocId should check IndexReader using equals()The method UUIDDocId.getDocumentNumber(IndexReader) tests the passed index reader using its object identity.

This is a left over when there was one index per workspace and no system index. When the system index was introduced each query execution will create a new CombinedIndexReader covering the workspace index and the system index. The method should now use the equals() method to test the passed IndexReader."
0,jcr:like on node nameUntil now it is only possible to do an exact match on the node name. It would be useful to also use jcr:like on the node name.
0,Reorganize Jackrabbit into 'core' 'api' and 'commons'
0,"DatabaseJournal needs connection reestablishment logicThe DB based file system and persistence manager implementations have logic for connection reestablishment in case the DB server bounces while the repository is running, but the DB based journal implementation doesn't."
0,Consolidate Solr  & Lucene FunctionQuery into modulesSpin-off from the [dev list | http://www.mail-archive.com/dev@lucene.apache.org/msg13261.html]  
0,"Jcr-Server: Usage of Cache-Control headerDeltaV-methods (except for Version-Control and Report) require the Cache-Control header to be present in the response.
In turn, RFC 2518 only requires the Cache-Control header to be present in the request  when dealing with the If header.

Currently the Cache-Control header is always present in the response and not specific for DeltaV methods.
Problems may arise with IE  and a couple of mimetypes such as zip-files"
0,"misleading lack of javadoc in StringRequestEntityWhen using httpclient2, we were doing the following:

	// Add the Content-type header.  This sets the charset to UTF-8.
	method.setRequestHeader( ""Content-type"", ""text/xml; charset=UTF-8"" );
	// The given string is converted internally by the post method into
	// a UTF-8 encoded byte array.
	method.setRequestBody( xmlstring );

The comments show that this was the way we used to obtain a UTF-8 encoded XML
document (if this was wrong, that may be the origin of the problem?).


When upgrading to httpclient3 and killing deprecated code, this was converted to:

	// Add the Content-type header.  This sets the charset to UTF-8.
	method.setRequestHeader( ""Content-type"", ""text/xml; charset=UTF-8"" );
	// The given string is converted internally by the post method into
	// a UTF-8 encoded byte array.
        method.setRequestEntity( new StringRequestEntity( xmlstring ) );

which went without problem during the tests on my machine and on test production
machine.. because platforms charset were UTF-8, which is not the case for
production machines :(

I think the javadoc of the used StringRequestEntity constructor should strongly
state that it uses String#getBytes for the content, which uses the platform
charset. Also, I didn't notice any ""upgrade to 3.x"" documentation which would
have helped me :/"
0,"Add open ended range query syntax to QueryParserThe QueryParser fails to generate open ended range queries.
Parsing e.g. ""date:[1990 TO *]""  gives zero results,
but
ConstantRangeQuery(""date"",""1990"",null,true,true)
does produce the expected results.

""date:[* TO 1990]"" gives the same results as ConstantRangeQuery(""date"",null,""1990"",true,true)."
0,"[PATCH] documentation typoJust a small patch that fixes a typo and changes the first sentence, as that 
one is used by Javadoc as a kind of summary so it should be something more 
useful than ""The Jakarta Lucene API is divided into several packages."""
0,"Remove query handler idleTimeThe changes included in JCR-415 revealed a synchronization issue with the query handler idle timer task.

See thread on dev-list: http://thread.gmane.org/gmane.comp.apache.jackrabbit.devel/10199

We could either fix the synchronization issue in the SearchManager class or remove the functionality all together.

Because the repository also supports a idle time parameter for the whole workspace (maxIdleTime in Workspaces element) the query handler idle time should be removed."
0,Improve multihome supportMultihomePlainSocketFactory is basically broken and should be deprecated. Multihome logic needs to be moved to the DefaultClientConnectionOperator
0,"Deprecate/remove language-specific tokenizers in favor of StandardTokenizerAs of Lucene 3.1, StandardTokenizer implements UAX#29 word boundary rules to provide language-neutral tokenization.  Lucene contains several language-specific tokenizers that should be replaced by UAX#29-based StandardTokenizer (deprecated in 3.1 and removed in 4.0).  The language-specific *analyzers*, by contrast, should remain, because they contain language-specific post-tokenization filters.  The language-specific analyzers should switch to StandardTokenizer in 3.1.

Some usages of language-specific tokenizers will need additional work beyond just replacing the tokenizer in the language-specific analyzer.  

For example, PersianAnalyzer currently uses ArabicLetterTokenizer, and depends on the fact that this tokenizer breaks tokens on the ZWNJ character (zero-width non-joiner; U+200C), but in the UAX#29 word boundary rules, ZWNJ is not a word boundary.  Robert Muir has suggested using a char filter converting ZWNJ to spaces prior to StandardTokenizer in the converted PersianAnalyzer."
0,"Remove oal.util.MapBackedSet (Java 6 offsers Collections.newSetFromMap())Easy search and replace job. In 3.x we still need the class, as Java 5 does not have Collections.newSetFromMap()."
0,"toplevel exception cleanupHttpClient.execute should throw only one exception, for easier general use.
HttpMethod constructors (HttpGet, HttpPut, etc..) should throw IllegalArgumentException in the string constructor (imply the string is pre-checked).  People wanting to see a URIException can use 'new HttpGet(new URI(uri))' and trigger the exception from the explicit URI creation."
0,"Reduce memory usage of ParentAxisScorerThe ParentAxisScorer keeps a map of non-default scores while it calculates the parent matches of the context scorer. In most cases the scores are not equal to the default score, but still may be all the same.

The scorer should therefore use the first score value as the default instead of the currently used 1.0f."
0,"Add test to check maven artifacts and their pomsAs release manager it is hard to find out if the maven artifacts work correct. It would be good to have an ant task that executes maven with a .pom file that requires all contrib/core artifacts (or one for each contrib) that ""downloads"" the artifacts from the local dist/maven folder and builds that test project. This would require maven to execute the build script. Also it should pass the ${version} ANT property to this pom.xml"
0,"Add Thread-Safety note to IndexWriter JavaDocIndexWriter Javadocs should contain a note about thread-safety. This is already mentioned on the wiki FAQ page but such an essential information should be part of the module documentation too.
"
0,"In IndexSearcher class, make subReader and docCount arrays protected so sub classes can access themPlease make these two member variables protected so subclasses can access them, e.g.:

  protected IndexReader[] subReaders;
  protected int[] docStarts;

Thanks"
0,"Add a serializing content handlerBoth JCR-1310 and JCR-1343 need XML serialization functionality and we've also previously (JCR-367, JCR-1086) implemented something similar.

It would be good to centralize such code, and so I'd like to use the already referenced code from Cocoon [1] as the basis for a SerializingContentHandler class in jackrabbit-jcr-commons.

[1] https://svn.apache.org/repos/asf/cocoon/trunk/core/cocoon-pipeline/cocoon-pipeline-impl/src/main/java/org/apache/cocoon/serialization/AbstractTextSerializer.java"
0,"Add support for tablespaces to Oracle related classesWhen a user account for an Oracle database has no or a temporary default tablespace, then the appropriate database schemas cannot be created. This is an issue for at least the following packages:
- o.a.j.core.persistence.bundle
- o.a.j.core.persistence.db
- o.a.j.core.fs.db
"
0,Properly close resourcesJava has exceptions so resources must always be closed on a finally clause
0,"Add support for type whitelist in TypeTokenFilterA usual use case for TypeTokenFilter is allowing only a set of token types. That is, listing allowed types, instead of filtered ones. I'm attaching a patch to add a useWhitelist option for that."
0,"Create UAX29URLEmailAnalyzer: a standard analyzer that recognizes URLs and emailsThis Analyzer should contain the same components as StandardAnalyzer, except for the tokenizer, which should be UAX29URLEmailTokenizer instead of StandardTokenizer."
0,"[PATCH] small fixes to the new scoring.html docThis is an awesome initiative.  We need more docs that cleanly explain the inner workings of Lucene in general... thanks Grant & Steve & others!

I have a few small initial proposed fixes, largely just adding some more description around the components of the formula.  But also a couple typos, another link out to Wikipedia, a missing closing ), etc.  I've only made it through the ""Understanding the Scoring Formula"" section so far."
0,"back-compat tests (""ant test-tag"") should test JAR drop-in-ability
We now test back-compat with ""ant test-tag"", which is very useful for
catching breaks in back compat before committing.

However, that currently checks out ""src/test"" sources and then
compiles them against the trunk JAR, and runs the tests.  Whereas our
back compat policy:

  http://wiki.apache.org/lucene-java/BackwardsCompatibility

states that no recompilation is required on upgrading to a new JAR.
Ie you should be able to drop in the new JAR in place of your old one
and things should work fine.

So... we should fix ""ant test-tag"" to:

  * Do full checkout of core sources & tests from the back-compat-tag

  * Compile the JAR from the back-compat sources

  * Compile the tests against that back-compat JAR

  * Swap in the trunk JAR

  * Run the tests

"
0,"Add method getID to interface ItemInfoItemInfo is the base for NodeInfo and PropertyInfo both of which declare a method getId with return type NodeId and PropertyId, respectively. With Java 1.5. it is now possible to override a method with a covariant return type. I thus propose to introduce a method getId on ItemInfo with return type ItemId which is the common base type of NodeId and PropertyId."
0,Use repository service wide namespace cacheThe jcr2spi layer requests namespaces for each new session that is created. It should rather cache them and make them available to other sessions.
0,"GCJ build fails with JDK 1.5The build.xml doesn't specify a target VM version. Using JDK 1.5, this means the compiled .class files 
are automatically made for 1.5, with java.lang.StringBuilder insidiously used for string concatenation. 
GCJ doesn't seem to include this class yet, so when it gets to the gcj build it dies trying to read the class 
files.

Steps to reproduce:
1. Install Sun JDK 1.5 for a Java compiler
2. Check out Lucene from svn
3. 'ant gcj'

Expected behavior:
Should build Lucene to .class files and .jar with the JDK compiler and then compile an .a with GCJ.

Actual behavior:
The GCJ build fails, complaining of being unable to find java.lang.StringBuilder.

Suggested fix:
Adding source=""1.3"" target=""1.3"" to the <javac> tasks seems to take care of this. Patch to be attached.

Additional notes:
Using Lucene from SVN and GCJ pulled from GCC CVS circa 2005-04-19. Ant 1.6.2."
0,"Collapse Common module into Lucene core utilIt was suggested by Robert in [http://markmail.org/message/wbfuzfamtn2qdvii] that we should try to limit the dependency graph between modules and where there is something 'common' it should probably go into lucene core.  Given that I haven't added anything to this module except the MutableValue classes, I'm going to collapse them into the util package, remove the module, and correct the dependencies."
0,"Enhanced JCR remoting (extending webdav SPI impl, basic remoting servlet)"
0,Test case for RTFTextExtractorThere should be a test case for the RTFTextExtractor.
0,"Use Maven dependency managementMany of the Jackrabbit components have dependencies to each other and to external libraries,
whose versions should ideally be the same for all the Jackrabbit components. To guarantee
the use of same depedency versions and to simplify overall depedency management we should
start using the Maven depedencyManagement feature in the Jackrabbit parent pom."
0,Include the WebDAV litmus tests in the Jackrabbit integration testsIt would be great to integrate the litmus tests (http://www.webdav.org/neon/litmus/) to our integration test suite.
0,how-to deployment modelshow-to with a detailed description of the steps required for each deployment model. 
0,"Sun hotspot compiler bug in 1.6.0_04/05 affects LuceneThis is not a Lucene bug.  It's an as-yet not fully characterized Sun
JRE bug, as best I can tell.  I'm opening this to gather all things we
know, and to work around it in Lucene if possible, and maybe open an
issue with Sun if we can reduce it to a compact test case.

It's hit at least 3 users:

  http://mail-archives.apache.org/mod_mbox/lucene-java-user/200803.mbox/%3c8c4e68610803180438x39737565q9f97b4802ed774a5@mail.gmail.com%3e
  http://mail-archives.apache.org/mod_mbox/lucene-solr-user/200804.mbox/%3c4807654E.7050900@virginia.edu%3e
  http://mail-archives.apache.org/mod_mbox/lucene-java-user/200805.mbox/%3c733777220805060156t7fdb8fectf0bc984fbfe48a22@mail.gmail.com%3e

It's specific to at least JRE 1.6.0_04 and 1.6.0_05, that affects
Lucene.  Whereas 1.6.0_03 works OK and it's unknown whether 1.6.0_06
shows it.

The bug affects bulk merging of stored fields.  When it strikes, the
segment produced by a merge is corrupt because its fdx file (stored
fields index file) is missing one document.  After iterating many
times with the first user that hit this, adding diagnostics &
assertions, its seems that a call to fieldsWriter.addDocument some
either fails to run entirely, or, fails to invoke its call to
indexStream.writeLong.  It's as if when hotspot compiles a method,
there's some sort of race condition in cutting over to the compiled
code whereby a single method call fails to be invoked (speculation).

Unfortunately, this corruption is silent when it occurs and only later
detected when a merge tries to merge the bad segment, or an
IndexReader tries to open it.  Here's a typical merge exception:

{code}
Exception in thread ""Thread-10"" 
org.apache.lucene.index.MergePolicy$MergeException: 
org.apache.lucene.index.CorruptIndexException:
    doc counts differ for segment _3gh: fieldsReader shows 15999 but segmentInfo shows 16000
        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:271)
Caused by: org.apache.lucene.index.CorruptIndexException: doc counts differ for segment _3gh: fieldsReader shows 15999 but segmentInfo shows 16000
        at org.apache.lucene.index.SegmentReader.initialize(SegmentReader.java:313)
        at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:262)
        at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:221)
        at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3099)
        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:2834)
        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:240)
{code}

and here's a typical exception hit when opening a searcher:

{code}
org.apache.lucene.index.CorruptIndexException: doc counts differ for segment _kk: fieldsReader shows 72670 but segmentInfo shows 72671
        at org.apache.lucene.index.SegmentReader.initialize(SegmentReader.java:313)
        at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:262)
        at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:230)
        at org.apache.lucene.index.DirectoryIndexReader$1.doBody(DirectoryIndexReader.java:73)
        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:636)
        at org.apache.lucene.index.DirectoryIndexReader.open(DirectoryIndexReader.java:63)
        at org.apache.lucene.index.IndexReader.open(IndexReader.java:209)
        at org.apache.lucene.index.IndexReader.open(IndexReader.java:173)
        at org.apache.lucene.search.IndexSearcher.<init>(IndexSearcher.java:48)
{code}

Sometimes, adding -Xbatch (forces up front compilation) or -Xint
(disables compilation) to the java command line works around the
issue.

Here are some of the OS's we've seen the failure on:

{code}
SuSE 10.0
Linux phoebe 2.6.13-15-smp #1 SMP Tue Sep 13 14:56:15 UTC 2005 x86_64 
x86_64 x86_64 GNU/Linux 

SuSE 8.2
Linux phobos 2.4.20-64GB-SMP #1 SMP Mon Mar 17 17:56:03 UTC 2003 i686 
unknown unknown GNU/Linux 

Red Hat Enterprise Linux Server release 5.1 (Tikanga)
Linux lab8.betech.virginia.edu 2.6.18-53.1.14.el5 #1 SMP Tue Feb 19 
07:18:21 EST 2008 i686 i686 i386 GNU/Linux
{code}

I've already added assertions to Lucene to detect when this bug
strikes, but since assertions are not usually enabled, I plan to add a
real check to catch when this bug strikes *before* we commit the merge
to the index.  This way we can detect & quarantine the failure and
prevent corruption from entering the index.

"
0,"FieldCache introspection APIFieldCache should expose an Expert level API for runtime introspection of the FieldCache to provide info about what is in the FieldCache at any given moment.  We should also provide utility methods for sanity checking that the FieldCache doesn't contain anything ""odd""...
   * entries for the same reader/field with different types/parsers
   * entries for the same field/type/parser in a reader and it's subreader(s)
   * etc...


"
0,"Add a isDeleted method to IndexCommitI wish to add a IndexCommit.isDeleted() method.

The use-case is that Solr will now support configurable IndexDeletionPolicy (SOLR-617). For the new replication (SOLR-561) to work, we need access to a list of IndexCommit instances which haven't been deleted yet. I can wrap the user specified IndexDeletionPolicy but since the IndexCommit does not have a isDeleted method, I may store a reference to an IndexCommit on which delete() has been called by the deletion policy. I can wrap the IndexCommit objects too just for having a isDeleted() method so a workaround exists. Not a big pain but if it can be managed on the lucene side easily, I'll appreciate it. It would save me from writing some delegate code."
0,"All Tokenizer implementations should have constructors that take AttributeSource and AttributeFactoryI have a TokenStream implementation that joins together multiple sub TokenStreams (i then do additional filtering on top of this, so i can't just have the indexer do the merging)

in 2.4, this worked fine.
once one sub stream was exhausted, i just started using the next stream 

however, in 2.9, this is very difficult, and requires copying Term buffers for every token being aggregated

however, if all the sub TokenStreams share the same AttributeSource, and my ""concat"" TokenStream shares the same AttributeSource, this goes back to being very simple (and very efficient)


So for example, i would like to see the following constructor added to StandardTokenizer:
{code}
  public StandardTokenizer(AttributeSource source, Reader input, boolean replaceInvalidAcronym) {
    super(source);
    ...
  }
{code}

would likewise want similar constructors added to all Tokenizer sub classes provided by lucene
"
0,"Move contribs/modules away from QueryParser dependencySome contribs and modules depend on the core QueryParser just for simplicity in their tests.  We should apply the same process as I did to the core tests, and move them away from using the QueryParser where possible."
0,"bad assumptions on QueryResult.getIterator() semantics in QueryResultNodeIteratorTest.testSkip()testSkip() assumes that calling getIterator() a second time will return a new iterator of the same size. JSR-170 is silent on this. Forcing a server to implement this essantially means that the query result must be cached until there's no reference to QueryResult anymore.

As this is a test of skip(), not getIterator(), the test should really refetch a new QueryResult in order to obtain a new iterator.

(Note: The issue of the semantics of QueryResult.getIterator should be discussed by the JCR EG.)
"
0,"analysis consumers should use reusable tokenstreamsSome analysis consumers (highlighter, more like this, memory index, contrib queryparser, ...) are using Analyzer.tokenStream but should be using Analyzer.reusableTokenStream instead for better performance."
0,"Add more unit on collection fieldscollection fields based on List are  supported not yet tested correctly.
Check if other kind collection are well tested"
0,Only load item definition when requiredSome item definitions are loaded when an item state is constructed. Whenever possible this should be delayed to a time when the definition is actually used.
0,"FSImport.java link on wiki is deadThe link for FSImport.java 

http://svn.apache.org/repos/asf/jackrabbit/trunk/contrib/examples/src/java/org/apache/jackrabbit/examples/FSImport.java

from wiki page


http://wiki.apache.org/jackrabbit/ExamplesPage

is dead, could it be updated please?
"
0,"upgrade icu to 4.8we should upgrade from 4.6 to 4.8.

some internal methods became public, also a package-private reflection hack can be removed."
0,"The token types of the standard tokenizer is not accessibleThe StandardTokenizerImpl not being public, these token types are not accessible :

{code:java}
public static final int ALPHANUM          = 0;
public static final int APOSTROPHE        = 1;
public static final int ACRONYM           = 2;
public static final int COMPANY           = 3;
public static final int EMAIL             = 4;
public static final int HOST              = 5;
public static final int NUM               = 6;
public static final int CJ                = 7;
/**
 * @deprecated this solves a bug where HOSTs that end with '.' are identified
 *             as ACRONYMs. It is deprecated and will be removed in the next
 *             release.
 */
public static final int ACRONYM_DEP       = 8;

public static final String [] TOKEN_TYPES = new String [] {
    ""<ALPHANUM>"",
    ""<APOSTROPHE>"",
    ""<ACRONYM>"",
    ""<COMPANY>"",
    ""<EMAIL>"",
    ""<HOST>"",
    ""<NUM>"",
    ""<CJ>"",
    ""<ACRONYM_DEP>""
};
{code}

So no custom TokenFilter can be based of the token type. Actually even the StandardFilter cannot be writen outside the org.apache.lucene.analysis.standard package.
"
0,"Make cache limits configurableThe cache settings of the CacheManager class (JCR-619) can be adjusted programmatically (JCR-725), but it would be nice if there was also a way to set them with system properties or ideally as a part of the repository configuration."
0,"Support for MaxDB / SapSB DatabasesI admit that MaxDB / SapSB are a bit exotic but support is easy to achieve when providing the correct ddls. 
"
0,"Field Selection and Lazy Field LoadingThe patch to come shortly implements a Field Selection and Lazy Loading mechanism for Document loading on the IndexReader.

It introduces a FieldSelector interface that defines the accept method:
FieldSelectorResult accept(String fieldName);

(Perhaps we want to expand this to take in other parameters such as the field metadata (term vector, etc.))

Anyone can implement a FieldSelector to define how they want to load fields for a Document.  
The FieldSelectorResult can be one of four values: LOAD, LAZY_LOAD, NO_LOAD, LOAD_AND_BREAK.  
The FieldsReader, as it is looping over the FieldsInfo, applies the FieldSelector to determine what should be done with the current field.

I modeled this after the java.io.FileFilter mechanism.  There are two implementations to date: SetBasedFieldSelector and LoadFirstFieldSelector.  The former takes in two sets of field names, one to load immed. and one to load lazily.  The latter returns LOAD_AND_BREAK on the first field encountered.  See TestFieldsReader for examples.

It should support UTF-8 (I borrowed code from Issue 509, thanks!).  See TestFieldsReader for examples

I added an expert method on IndexInput  named skipChars that takes in the number of characters to skip.  This is a compromise on changing the file format of the fields to better support seeking.  It does some of the work of readChars, but not all of it.  It doesn't require buffer storage and it doesn't do the bitwise operations.  It just reads in the appropriate number of bytes and promptly ignores them.  This is useful for skipping non-binary, non-compressed stored fields.

The biggest change is by far the introduction of the Fieldable interface (as per Doug's suggestion from a mailing list email on Lazy Field loading from a while ago).  Field is now a Fieldable.  All uses of Field have been changed to use Fieldable.  FieldsReader.LazyField also implements Fieldable.

Lazy Field loading is now implemented.  It has a major caveat (that is Documented) in that it clones the underlying IndexInput upon lazy access to the Field value.  IT IS UNDEFINED whether a Lazy Field can be loaded after the IndexInput parent has been closed (although, from what I saw, it does work).  I thought about adding a reattach method, but it seems just as easy to reload the document.  See the TestFieldsReader and DocHelper for examples.

I updated a couple of other tests to reflect the new fields that are on the DocHelper document.

All tests pass."
0,"Deprecate and remove ShingleMatrixFilterSpin-off from LUCENE-1391: This filter is unmainatined and no longer up-to-date, has bugs nobody understands and does not work with attributes.

This issue deprecates it as of Lucene 3.1 and removes it from trunk."
0,"Deprecate/remove unused FileSystem features such as RandomAccessOutputStreamCurrently the FileSystem interface includes a method getRandomAccessOutputStream.
It looks like this method and the class RandomAccessOutputStream is no longer used.
If this is the case, I suggest we remove the method everywhere and deprecate the class."
0,"Clean up spi-commons pom.xmlThe pom.xml contains lines that were copied from the jackrabbit-core but are not actually needed. A log4j.properties is also missing in test resources.

See attached patch."
0,[PATCH] don't use the reflective form of {Collection}.toArrayPassing a prototype array into {Collection}.toArray that is too small makes the toArray call expend alot of effort using reflection to do it's job. It is more performant to just pass in a correctly sized prototype. This patch does this.
0,"FuzzyLikeThisQuery should set MaxNonCompetitiveBoost for faster speedFuzzyLikeThisQuery uses FuzzyTermsEnum directly, and maintains 
a priority queue for its purposes.

Just like TopTermsRewrite method, it should set the 
MaxNonCompetitiveBoost attribute, so that FuzzyTermsEnum can
run faster. Its already tracking the minScore, just not updating
the attribute.

This would be especially nice as it appears to have nice defaults
already (pq size of 50)
"
0,"Modify confusing javadoc for queryNormSee http://markmail.org/message/arai6silfiktwcer

The javadoc confuses me as well."
0,EntryCollector may log warning for inexistent itemCurrently the EntryCollector may log a warning when the node reported in the event does not exist. This may happen when the repository runs in a cluster and a node is created and immediately removed again. This issue is related to JCR-3014. The call to Session.nodeExists() should actually return false when the identifier path cannot be resolved. Currently it throws a RepositoryException.
0,"Some improvements to CMSWhile running optimize on a large index, I've noticed several things that got me to read CMS code more carefully, and find these issues:

* CMS may hold onto a merge if maxMergeCount is hit. That results in the MergeThreads taking merges from the IndexWriter until they are exhausted, and only then that blocked merge will run. I think it's unnecessary that that merge will be blocked.

* CMS sorts merges by segments size, doc-based and not bytes-based. Since the default MP is LogByteSizeMP, and I hardly believe people care about doc-based size segments anymore, I think we should switch the default impl. There are two ways to make it extensible, if we want:
** Have an overridable member/method in CMS that you can extend and override - easy.
** Have OneMerge be comparable and let the MP determine the order (e.g. by bytes, docs, calibrate deletes etc.). Better, but will need to tap into several places in the code, so more risky and complicated.

On the go, I'd like to add some documentation to CMS - it's not very easy to read and follow.

I'll work on a patch."
0,"Make DirectoryTaxonomyWriter's indexWriter member privateDirectoryTaxonomyWriter has a protected indexWriter member. As far as I can tell, for two reasons:

# protected openIndexWriter method which lets you open your own IW (e.g. with a custom IndexWriterConfig).
# protected closeIndexWriter which is a hook for letting you close the IW you opened in the previous one.

The fixes are trivial IMO:
# Modify the method to return IW, and have the calling code set DTW's indexWriter member
# Eliminate closeIW. DTW already has a protected closeResources() which lets you clean other resources you've allocated, so I think that's enough.

I'll post a patch shortly."
0,"add an interface for plugable dns clientsCurrently Httpclient implicitly uses InetAddress.getByName() for DNS resolution.
This has some drawbacks. One is that the DNS cache of Java per default caches entries forever.

So I'd like to be able to replace InetAddress.getByName() with another DNS client implementation.

"
0,Index update overhead on cluster slave due to JCR-905JCR-905 is a quick and dirty fix and causes overhead on a cluster slave node when it processes revisions.
0,"Separate javadocs for core and contribsA while ago we had a discussion on java-dev about separating the javadocs
for the contrib modules instead of having only one big javadoc containing 
the core and contrib classes.

This patch:
* Adds new targets to build.xml: 
  ** ""javadocs-all"" Generates Javadocs for the core, demo, and contrib 
    classes
  ** ""javadocs-core"" Generates Javadocs for the core classes
  ** ""javadocs-demo"" Generates Javadocs for the demo classes
  ** ""javadocs-contrib"" Using contrib-crawl it generates the Javadocs for 
    all contrib modules, except ""similarity"" (currently empty) and gdata.
* Adds submenues to the Javadocs link on the Lucene site with links to
  the different javadocs
* Includes the javadocs in the maven artifacts

Remarks:
- I removed the ant target ""javadocs-internal"", because I didn't want to
  add corresponding targets for all new javadocs target. Instead I 
  defined a new property ""javadoc.access"", so now  
  ""ant -Djavadoc.access=package"" can be used in combination with any of
  the javadocs targets. Is this ok?
- I didn't include gdata (yet) because it uses build files that don't 
  extend Lucenes standard build files.
  
Here's a preview:
http://people.apache.org/~buschmi/site-preview/index.html

Please let me know what you think about these changes!"
0,"TokenSources.getTokenStream(Document...) Sometimes, one already has the Document, and just needs to generate a TokenStream from it, so I am going to add a convenience method to TokenSources.  Sometimes, you also already have just the string, so I will add a convenience method for that."
0,"BaseTestRangeFilter can be extremely slowThe tests that extend BaseTestRangeFilter can sometimes be very slow:
TestFieldCacheRangeFilter, TestMultiTermConstantScore, TestTermRangeFilter

for example, TestFieldCacheRangeFilter just ran for 10 minutes on my computer before I killed it,
but i noticed these tests frequently run for over a minute.

I think at the least we should change these to junit4 so the index is built once in @beforeClass"
0,Create enwiki indexable data as line-per-article rather than file-per-articleCreate a line per article rather than a file. Consume with indexLineFile task.
0," RAMDirectory(Directory dir, boolean closeDir)  constructor uses memory inefficiently.recently I found that  RAMDirectory(Directory dir, boolean closeDir)  constructor uses memory inefficiently.
files from source index are read entirely intro memory as single byte array which is after all is thrown away. And if I want to load my 200M optimized, compound format index to memory for faster search I should give JVM at least 400Mb memory limit. For larger indexes this can be an issue.

I've attached patch how to solve this problem."
0,Improve Memory Consumption for merging DocValues SortedBytes variantsCurrently SortedBytes are loaded into memory during merge which could be a potential trap. Instead of loading them into Heap memory we can merge those sorted values with much smaller memory and without loading all values into ram.
0,"spi2davex: InvalidItemStateException not properly extracted from ambiguous response errorNodeTest#testSaveInvalidStateException
SessionTest#testSaveInvalidStateException

fail with PathNotFoundException instead of InvalidItemStateException.

i remember that i already addressed that issue in spi2dav a long time ago. with the batched writing in
spi2davex it is back: the server isn't aware of the distinction and just isn't able to retrieve that removed
item... either the client side finds a way to distinguish between path-not-found and externally modified
or we have to leave this as known issue...

in spi2dav i added add quick hack: if the operation was some write operation the path-not-found is
simply converted into invaliditemstateexception."
0,"Add a MBean method to programatically create a new Workspace.Would be useful to have a mbean method to create a new workspace to use if with a jmx console.

"
0,"[PATCH] usage feedback for IndexFiles demoJust a small patch that adds ""usage"" output if the demo is called without a 
parameter, makes it a little bit friendlier to beginners."
0,"Grouped total countWhen grouping currently you can get two counts:
* Total hit count. Which counts all documents that matched the query.
* Total grouped hit count. Which counts all documents that have been grouped in the top N groups.

Since the end user gets groups in his search result instead of plain documents with grouping. The total number of groups as total count makes more sense in many situations. "
0,"Improve jcr decorator in jcr-extThe jcr decorator in jcr-ext does not cover all the necessary interfaces of the jcr api. It may happen that a client loses the decoration layer when accessing properties.

I've added decoration for several jcr interfaces to ensure that the decoration layer is never left.

The attached patch also removes the classes related to decorator chaining. I found it hard to understand the purpose of those classes and decided to remove them from the default implementation. If we want to keep those classes they should be less intrusive.

I've also noticed that there are class name clashes, specifically the package org.apache.jackrabbit.name contains classes that are also present in the jackrabbit and jackrabbit-commons jar file. I propose to move the respective classes in jcr-ext to a different package or remove them in favor of the jackrabbit-commons classes.

Let me know if I should commit the the patch.

Thanks"
0,"Introduce 'SecurityConfig' for better extensability.the current repository configuration parser parses the security confguration (inluding appName, AccessManagerConfig and LoginModuleconfig) internally and the passes those 3 values to the repository config. i suggest to add a new 'SecurityConfig' object that encapsulates those 3 values and is parsed in a seperate method, in order to allow for better extensability. this also reduces the size of the alredy bloated repository config constructor."
0,Highlighter Documentation updatesVarious places in the Highlighter documentation refer to bytes (i.e. SimpleFragmenter) when it should be chars.  See http://www.gossamer-threads.com/lists/lucene/java-user/56986
0,SerializationTest leaks sessionsThe class TreeComparator extends from AbstractJCRTest and opens a session in its constructor because it calls the setUp() method. The tearDown() method is never called.
0,"Add Japanese filter to replace term attribute with readingsKoji and Robert are working on LUCENE-3888 that allows spell-checkers to do their similarity matching using a different word than its surface form.

This approach is very useful for languages such as Japanese where the surface form and the form we'd like to use for similarity matching is very different.  For Japanese, it's useful to use readings for this -- probably with some normalization."
0,"Add @Override annotationsDuring removal of deprecated APIs, mostly the problem was, to not only remove the method in the (abstract) base class (e.g. Scorer.explain()), but also remove it in sub classes that override it. You can easily forget that (especially, if the method was not marked deprecated in the subclass). By adding @Override annotations everywhere in Lucene, such removals are simple, because the compiler throws out an error message in all subclasses which then no longer override the method.

Also it helps preventing the well-known traps like overriding hashcode() instead of hashCode().

The patch was generated automatically, and is rather large. Should I apply it, or would it break too many patches (but I think, trunk has changed so much, that this is only a minimum of additional work to merge)?"
0,"[API Doc] Improve the description of the preemptive authenticationHttpClient authentication guide does not reflect the fact that preemptive
authentication requires default credentials to be set. It should also mention
the security implications of preemptive authentication (default credentials sent
with EVERY request to ANY target / proxy server)"
0,"Implement a way to override or resolve DNS entries defined in the OSWhen working with HttpClient in restrictive environments, where the user doesn't have the permissisions to edit the local /etc/hosts file or the DNS configuration, can be eased with an DNS Overrider capability. 

This can be useful with JMeter which can follow redirects automatically and resolve some of the redirected hosts against its configuration. Another example is a custom forward proxy, written in Java and based on httpclient, which can be deployed is such a restricted environment that would ease the development of various web solutions for some developers. "
0,"support protected words in Stemming TokenFiltersThis is from LUCENE-1515

I propose that all stemming TokenFilters have an 'exclusion set' that bypasses any stemming for words in this set.
Some stemming tokenfilters have this, some do not.

This would be one way for Karl to implement his new swedish stemmer (as a text file of ignore words).
Additionally, it would remove duplication between lucene and solr, as they reimplement snowballfilter since it does not have this functionality.
Finally, I think this is a pretty common use case, where people want to ignore things like proper nouns in the stemming.

As an alternative design I considered a case where we generalized this to CharArrayMap (and ignoring words would mean mapping them to themselves), which would also provide a mechanism to override the stemming algorithm. But I think this is too expert, could be its own filter, and the only example of this i can find is in the Dutch stemmer.

So I think we should just provide ignore with CharArraySet, but if you feel otherwise please comment.
"
0,"configurable MultiTermQuery TopTermsScoringBooleanRewrite pq sizeMultiTermQuery has a TopTermsScoringBooleanRewrite, that uses a priority queue to expand the query to the top-N terms.

currently N is hardcoded at BooleanQuery.getMaxClauseCount(), but it would be nice to be able to set this for top-N MultiTermQueries: e.g. expand a fuzzy query to at most only the 50 closest terms.

at a glance it seems one way would be to expose TopTermsScoringBooleanRewrite (it is private right now) and add a ctor to it, so a MultiTermQuery can instantiate one with its own limit."
0,"[PATCH] Some Field methods use Classcast check instead of instanceof which is slowI am not sure if this is because Lucene historically needed to work with older
JVM's but with modern JVM's, instanceof is much quicker. 

The Field.stringValue(), .readerValue(), and .binaryValue() methods all use
ClassCastException checking.

Using the following test-bed class, you will see that instanceof is miles quicker:

package com.aconex.index;

public class ClassCastExceptionTest {

    private static final long ITERATIONS = 100000;

    /**
     * @param args
     */
    public static void main(String[] args) {

        runClassCastTest(1); // once for warm up
        runClassCastTest(2);
        
        runInstanceOfCheck(1);
        runInstanceOfCheck(2);

    }
    private static void runInstanceOfCheck(int run) {
        long start = System.currentTimeMillis();

        Object foo = new Foo();
        for (int i = 0; i < ITERATIONS; i++) {
            String test;
            if(foo instanceof String) {
                System.out.println(""Is a string""); // should never print
            }
        }
        long end = System.currentTimeMillis();
        long diff = end - start;
        System.out.println(""InstanceOf checking run #"" + run + "": "" + diff + ""ms"");
        
    }

    private static void runClassCastTest(int run) {
        long start = System.currentTimeMillis();

        Object foo = new Foo();
        for (int i = 0; i < ITERATIONS; i++) {
            String test;
            try {
                test = (String)foo;
            } catch (ClassCastException c) {
                // ignore
            }
        }
        long end = System.currentTimeMillis();
        long diff = end - start;
        System.out.println(""ClassCast checking run #"" + run + "": "" + diff + ""ms"");
    }

    private static final class Foo {
    }

}


Results
=======

Run #1

ClassCast checking run #1: 1660ms
ClassCast checking run #2: 1374ms
InstanceOf checking run #1: 8ms
InstanceOf checking run #2: 4ms


Run #2
ClassCast checking run #1: 1280ms
ClassCast checking run #2: 1344ms
InstanceOf checking run #1: 7ms
InstanceOf checking run #2: 2ms


Run #3
ClassCast checking run #1: 1347ms
ClassCast checking run #2: 1250ms
InstanceOf checking run #1: 7ms
InstanceOf checking run #2: 2ms

This could explain why Documents with more Fields scales worse, as in, for lots
of Documents with lots of Fields, the effect is exacerbated."
0,"cleanup contrib/demoI don't think we should include optimize in the demo; many people start from the demo and may think you must optimize to do searching, and that's clearly not the case.

I think we should also use a buffered reader in FileDocument?

And... I'm tempted to remove IndexHTML (and the html parser) entirely.  It's ancient, and we now have Tika to extract text from many doc formats."
0,"Create new method optimize(int maxNumSegments) in IndexWriterSpinning this out from the discussion in LUCENE-847.

I think having a way to ""slightly optimize"" your index would be useful
for many applications.

The current optimize() call is very expensive for large indices
because it always optimizes fully down to 1 segment.  If we add a new
method which instead is allowed to stop optimizing once it has <=
maxNumSegments segments in the index, this would allow applications to
eg optimize down to say <= 10 segments after doing a bunch of updates.
This should be a nice compromise of gaining good speedups of searching
while not spending the full (and typically very high) cost of
optimizing down to a single segment.

Since LUCENE-847 is now formalizing an API for decoupling merge policy
from IndexWriter, if we want to add this new optimize method we need
to take it into account in LUCENE-847.
"
0,Move ReusableAnalyzerBase into coreIn LUCENE-2309 it was suggested that we should make Analyzer reusability compulsory.  ReusableAnalyzerBase is a fantastic way to drive reusability so lets move it into core (so that we can then change all impls over to using it).
0,"Make Jackrabbit compile on Java 7Compiling on Java 7 fails with the following error:

    jackrabbit-core/src/main/java/org/apache/jackrabbit/core/util/db/DataSourceWrapper.java:[30,7] error:
    DataSourceWrapper is not abstract and does not override abstract method getParentLogger() in CommonDataSource

We should fix that."
0,"extend LevenshteinAutomata to support transpositions as primitive editsThis would be a nice improvement for spell correction: currently a transposition counts as 2 edits,
which means users of DirectSpellChecker must use larger values of n (e.g. 2 instead of 1) and 
larger priority queue sizes, plus some sort of re-ranking with another distance measure for good results.

Instead if we can integrate ""chapter 7"" of http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.16.652 
then you can just build an alternative DFA where a transposition is only a single edit 
(http://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance)

According to the benchmarks in the original paper, the performance for LevT looks to be very similar to Lev.

Support for this is now in moman (https://bitbucket.org/jpbarrette/moman/) thanks to Jean-Philippe 
Barrette-LaPierre.
"
0,"Rework of the TermInfosReader class to remove the Terms[], TermInfos[], and the index pointer long[] and create a more memory efficient data structure.Basically packing those three arrays into a byte array with an int array as an index offset.  

The performance benefits are stagering on my test index (of size 6.2 GB, with ~1,000,000 documents and ~175,000,000 terms), the memory needed to load the terminfos into memory were reduced to 17% of there original size.  From 291.5 MB to 49.7 MB.  The random access speed has been made better by 1-2%, load time of the segments are ~40% faster as well, and full GC's on my JVM were made 7 times faster.

I have already performed the work and am offering this code as a patch.  Currently all test in the trunk pass with this new code enabled.  I did write a system property switch to allow for the original implementation to be used as well.

-Dorg.apache.lucene.index.TermInfosReader=default or small

I have also written a blog about this patch here is the link.

http://www.nearinfinity.com/blogs/aaron_mccurry/my_first_lucene_patch.html



"
0,Discovery of privileges of any set of Principalsjsr 283 defines means to discover the privileges for the editing session. however there is no way to determine the privileges for other principals.
0,"nightly build failedjavadoc tasked failed due to new project structure in contrib/gdata-server
added correct package structure to java/trunk/build.xml

javadoc creation successful.

Patch added as attachment.

regards simon"
0,"please log allocation of new connections to support debugging, testingI'd like to suggest that the MultiThreaded connection manager emit a trace-level log when it 
allocates a new HttpConnection to support debugging and testing.  I added one while working on 
my integration in Apache Axis (see org.apache.axis.transport.http.CommonsHTTPSender) and 
figured this would be of general use.  I'll attach a patch with the oh-so-minor addition after 
submitting this enhancement request."
0,"Correct 2 minor javadoc mistakes in core, javadoc.access=privatePatches Token.java and TermVectorsReader.java"
0,"Jackrabbit performance test suiteI'd like to set up a multi-version performance test suite inside jackrabbit-core/src/test/performance, similar to the compatibility test suite we added in JCR-2631. This performance test suite would produce comparable performance numbers for a number of simple benchmark tests across different Jackrabbit versions, including the latest snapshot.

"
0,"UserImporter should trigger execution AuthorizableActions in case of user/group creationin accordance to the new implementation specific extensions made to user mangement in JCR-3118 the user-importer
should be adjusted as well."
0,Remove Searcher from Weight#explainExplain needs to calculate corpus wide stats in a way that is consistent with MultiSearcher.
0,"Implement RepositoryFactory in jcr2davIt's currently a bit cumbersome to set up a spi2dav instance because of the two levels of factories (RepositoryFactory & RepositoryServiceFactory) involved in the process. It would be easier if spi2dav implemented RepositoryFactory directly, so downstream users would only need to provide the server URI parameter instead of specifying also the RepositoryServiceFactory classname.

To do this, spi2dav would need to depend also on jcr2spi. This change would actually simplify downstream projects, that then wouldn't need to depend also to jcr2spi to get JCR -> DAV connectivity."
0,"Open access modifier for RepositoryImpl.doShutdown()This is required for a subclass of RepositoryImpl that wants to run additional code on shutdown, otherwise a deadlock may occur because the sequence of lock acquisition cannot be ensured.

Jackrabbit requires that the shutdownLock is first acquired and then the actual shutdown code is executed."
0,"Make the Payload Boosting Queries consistentBoostingFunctionTermQuery should be consistent with BoostingNearQuery -

Renaming to PayloadNearQuery and PayloadTermQuery"
0,"LuceneTestCase's uncaught exceptions handler should check for AssumptionViolatedExceptions and then not trigger test failureAs in single-threaded tests, {{LuceneTestCase}} should not trigger test failures for {{AssumptionViolatedException}}'s when they occur in multi-threaded tests."
0,allow AbstractFileSystemTest.getFileSystem to throw an Exception
0,"Code coverage reportsHi all,

We should be able to measure the code coverage of our unit testcases. I believe it would be very helpful for the committers, if they could verify before committing a patch if it does not reduce the coverage. 

Furthermore people could take a look in the code coverage reports to figure out where work needs to be done, i. e. where additional testcases are neccessary. It would be nice if we could add a page to the Lucene website showing the report, generated by the nightly build. Maybe you could add that to your preview page (LUCENE-707), Grant?

I attach a patch here that uses the tool EMMA to generate the code coverage reports. EMMA is a very nice open-source tool released under the CPL (same license as junit). The patch adds three targets to common-build.xml: 
- emma-check: verifys if both emma.jar and emma_ant.jar are in the ant classpath 
- emma-instrument: instruments the compiled code 
- generate-emma-report: generates an html code coverage report 

The following steps are neccessary in order to generate a code coverage report:
- add emma.jar and emma_ant.jar to your ant classpath (download emma from http://emma.sourceforge.net/)
- execute ant target 'emma-instrument' (depends on compile-test, so it will compile all core and test classes)
- execute ant target 'test' to run the unit tests
- execute ant target 'generate-emma-report'

To view the emma report open build/test/emma/index.html"
0,"need to add a default constructor for CookieThe Cookie class doesn't have a default (no argument) constructor. This is 
causing problem for some framework which supports marshalling and unmarshalling 
of data types. e.g. a SOAP implementation may need to do this to transfer it 
between the SOAP server and SOAP client. It would be nice to add a default 
constructor, as it won't break anything, follows JavaBean convention, and 
potentially save user some trouble in the future."
0,"Highlighter should try and use maxDocCharsToAnalyze in WeightedSpanTermExtractor when adding a new field to MemoryIndex as well as when using CachingTokenStreamhuge documents can be drastically slower than need be because the entire field is added to the memory index
this cost can be greatly reduced in many cases if we try and respect maxDocCharsToAnalyze

things can be improved even further by respecting this setting with CachingTokenStream

"
0,"MMapDirectory speedupsMMapDirectory has some performance problems:
# When the file is larger than Integer.MAX_VALUE, we use MultiMMapIndexInput, 
which does a lot of unnecessary bounds-checks for its buffer-switching etc. 
Instead, like MMapIndexInput, it should rely upon the contract of these operations
in ByteBuffer (which will do a bounds check always and throw BufferUnderflowException).
Our 'buffer' is so large (Integer.MAX_VALUE) that its rare this happens and doing
our own bounds checks just slows things down.
# the readInt()/readLong()/readShort() are slow and should just defer to ByteBuffer.readInt(), etc
This isn't very important since we don't much use these, but I think there's no reason
users (e.g. codec writers) should have to readBytes() + wrap as bytebuffer + get an 
IntBuffer view when readInt() can be almost as fast..."
0,"Simultaneous updates by multiple sessions might not appear in the journalIn a clustering environment, simultaneous updates by multiple sessions in the same cluster node might not appear in the journal, because only record at a time can be handled by the cluster's workspace-specific callback method. When such a situtation arises, the following warnings can be found in the log:

*WARN * ClusterNode: No record created.
*WARN * ClusterNode: No record prepared.
"
0,"Invert IR.getDelDocs -> IR.getLiveDocsSpinoff from LUCENE-1536, where we need to fix the low level filtering
we do for deleted docs to ""match"" Filters (ie, a set bit means the doc
is accepted) so that filters can be pushed all the way down to the
enums when possible/appropriate.

This change also inverts the meaning first arg to
TermsEnum.docs/AndPositions (renames from skipDocs to liveDocs).
"
0,"JCR2SPI: remove duplicate item statesthe original approach with duplicate item state objects connected to each is not required any more 
and can be simplified."
0,"avoid converting property values to stringsQValues currently can not expose properties of types LONG and DOUBLE in a parsed format. Thus, setting/retrieving properties of these types requires roundtripping through Strings, which we should avoid.

Proposal:

1) Add ""long getLong()"" and ""double getDouble()"" to QValue.

2) Add matching create methods to QValueFactory.

3) Take advantage of the new methods in JCR2SPI, for instance by allowing it's own Value implementation to internally just hold the QValue.

"
0,"Improved Payloads APIWe want to make some optimizations to the Payloads API.

See following thread for related discussions:
http://www.gossamer-threads.com/lists/lucene/java-dev/54708"
0,"add suggester that uses shortest path/wFST instead of bucketsCurrently the FST suggester (really an FSA) quantizes weights into buckets (e.g. single byte) and puts them in front of the word.
This makes it fast, but you lose granularity in your suggestions.

Lately the question was raised, if you build lucene's FST with positiveintoutputs, does it behave the same as a tropical semiring wFST?

In other words, after completing the word, we instead traverse min(output) at each node to find the 'shortest path' to the 
best suggestion (with the highest score).

This means we wouldnt need to quantize weights at all and it might make some operations (e.g. adding fuzzy matching etc) a lot easier."
0,JSR 283 support
0,"Correct copy-paste victim CommentCorrect the doc-comment of FieldsProducer (being a copy-paste victim of FieldsConsumer).
""consumes"" replaced with ""produces"".

One word change to avoid confusion: safe to commit.
"
0,"add svn ignores for eclipse artifactsBe nice to ignore the files eclipse puts into the project root as we do the .idea file for intellij.

The two files are

.project
.classpath

I'm gonna lie and say there's a patch available for this because an svn diff patch with propery changes can't be applied with patch anyway."
0,"IndexWriter.close(false) does not actually stop background merge threadsRight now when you close(false), IndexWriter marks any running merges
as aborted but then does not wait for these merges to finish.  This
can cause problems because those threads still hold files open, so,
someone might think they can call close(false) and then (say) delete
all files from that directory, which would fail on Windows.

Instead, close(false) should notify each running merge that it has
been aborted, and not return until all running merges are done.  Then,
SegmentMerger should periodically check whether it has been aborted
and stop if so.
"
0,Remove deprecated Field.Store.COMPRESSAlso remove FieldForMerge and related code.
0,"Add automatic default configurationWe should provide a simple way to start a Jackrabbit repository with default configuration. The current First Hops document exposes too much configuration details to be really friendly to first-time users.

I'd like to provide a default TransientRepository constructor that looks for ""repository.xml"" as the configuration file and ""repository"" as the repository home directory. If either of these does not exist, it is automatically created using default settings. This way the repository setup would boil down to:

    Repository repository = new TransientRepository();

As an added feature I'm planning to support system properties ""org.apache.jackrabbit.repository.conf"" and ""org.apache.jackrabbit.repository.home"" for overriding the defaults.

This improvement would make it easier to write and set up ""Hello, World!"" -type applications, thus helping interested people to try out Jackrabbit. This feature will also make it easier to provide a standard template for test classes that exhibit some error condition. Like this:

    import javax.jcr.*;
    import org.apache.jackrabbit.core.TransientRepository;
    public Example {
        public static void main(String[] args) {
            try {
                Repository repository = new TransientRepository();
                Session session = repository.login();
                try {
                    // YOUR CODE HERE
                } finally {
                    session.close();
                }
            } catch (Exception e) {
                e.printStackTRace();
            }
        }
    }

I'm targetting this for inclusion in 1.0 as it affects none of the existing code and it will probably be very helpful for the expected number of new users we are going to see after 1.0 is out."
0,"Consolidate CustomScoreQuery, ValueSourceQuery and BoostedQuery Lucene's CustomScoreQuery and Solr's BoostedQuery do essentially the same thing: they boost the scores of Documents by the value from a ValueSource.  BoostedQuery does this in a direct fashion, by accepting a ValueSource. CustomScoreQuery on the other hand, accepts a series of ValueSourceQuerys.  ValueSourceQuery seems to do exactly the same thing as FunctionQuery.

With Lucene's ValueSource being deprecated / removed, we need to resolve these dependencies and simplify the code.

Therefore I recommend we do the following things:

- Move CustomScoreQuery (and CustomScoreProvider) to the new Queries module and change it over to use FunctionQuerys instead of ValueSourceQuerys.  
- Deprecate Solr's BoostedQuery in favour of the new CustomScoreQuery.  CSQ provides a lot of support for customizing the scoring process.
- Move and consolidate all tests of CSQ and BoostedQuery, to the Queries module and have them test CSQ instead."
0,"Header adding in org.apache.http.client.protocol.RequestAcceptEncoding should be conditionalorg.apache.http.client.protocol.RequestAcceptEncoding adds a header in any case. Any chance to do it conditional (like in RequestClientConnControl)? The code would be something like
if (!request.containsHeader(""Accept-Encoding"")) {
    request.addHeader(""Accept-Encoding"", ""gzip,deflate"");
}

In our app this header may be added before request intercepting, so would be great if this fact is checked.
"
0,"Unit tests for persistence managersCurrently we only test our persistence managers indirectly via JCR-level test cases. The downside of this approach is that we can only test one persistence manager implementation at a time, and need separate build profiles to switch from one implementation to another. To ensure better coverage and consistent behaviour across all our persistence managers I implemented a simple unit test that works directly against the PersistenceManager interface."
0,"CookieIdentityComparator and CookiePathComparator could/should implement SerializableCookieIdentityComparator and CookiePathComparator could/should implement Serializable

As Findbugs suggests:

""Comparator doesn't implement Serializable

This class implements the Comparator interface. You should consider whether or not it should also implement the Serializable interface. If a comparator is used to construct an ordered collection such as a TreeMap, then the TreeMap will be serializable only if the comparator is also serializable. As most comparators have little or no state, making them serializable is generally easy and good defensive programming. ""

Neither class has any state, so implementing Serializable would be trivial.

"
0,"Make MultiThreadedHttpConnectionManager defaults public statics.Could the defaults for MultiThreadedHttpConnectionManager be made public
constants? I would do it my self since I have karma as a contributer to [lang]
and [codec] but I do not want to step on anyones toes. ;-)

Patch attached."
0,"Searchability settings in PropertyDefinitionRelated to JCR-1591, the new JCR 2.0 property definitions contain settings for searchability of properties.

I'm not sure how deeply we want to implement these settings (perhaps we should just hard-code the values), but in any case the relevant methods need to be implemented."
0,Promote ItemInfo builder classes from GetItemsTest to top level classesorg.apache.jackrabbit.jcr2spi.GetItem test contains builders for ItemInfo and NodeInfo instances. These should be generalized and promoted to spi-commons. 
0,"Cleanup use of EncodingUtil and HttpConstantsHttpConstants has become somewhat irrelevant.  Deprecate HttpConstants and move any existing 
functionality to EncodingUtil."
0,"wire logger skips empty lineWhen logging with 
org.apache.commons.logging.simplelog.log.httpclient.wire=debug, HttpConnection 
skips one line of server output in logs -- CRLF line between headers and body."
0,"https should check CN of x509 certhttps should check CN of x509 cert

Since we're essentially rolling our own ""HttpsURLConnection"",  the checking provided by ""javax.net.ssl.HostnameVerifier"" is no longer in place.

I have a patch I'm about to attach which caused both createSocket() methods on o.a.h.conn.ssl.SSLSocketFactory to blowup:

test1: javax.net.ssl.SSLException: hostname in certificate didn't match: <vancity.com> != <www.vancity.com>
test2: javax.net.ssl.SSLException: hostname in certificate didn't match: <vancity.com> != <www.vancity.com>

Hopefully people agree that this is desirable.
"
0,Move hasVectors() & hasProx() responsibility out of SegmentInfo to FieldInfos Spin-off from LUCENE-2881 which had this change already but due to some random failures related to this change I remove this part of the patch to make it more isolated and easier to test. 
0,"Analyzer for LatvianLess aggressive form of Kreslins' phd thesis: A stemming algorithm for Latvian.
"
0,"better handling of files inside/outside CFS by codecSince norms and deletes were moved under Codec (LUCENE-3606, LUCENE-3661),
we never really properly addressed the issue of how Codec.files() should work,
considering these files are always stored outside of CFS.

LUCENE-3606 added a hack, LUCENE-3661 cleaned up the hack a little bit more,
but its still a hack.

Currently the logic in SegmentInfo.files() is:
{code}
clearCache()

if (compoundFile) {
  // don't call Codec.files(), hardcoded CFS extensions, etc
} else {
  Codec.files()
}

// always add files stored outside CFS regardless of CFS setting
Codec.separateFiles()

if (sharedDocStores) {
  // hardcoded shared doc store extensions, etc
}
{code}

Also various codec methods take a Directory parameter, but its inconsistent
what this Directory is in the case of CFS: for some parts of the index its
the CFS directory, for others (deletes, separate norms) its not.

I wonder if instead we could restructure this so that SegmentInfo.files() logic is:
{code}
clearCache()
Codec.files()
{code}

and so that Codec is instead responsible.

instead Codec.files logic by default would do the if (compoundFile) thing, and
Lucene3x codec itself would only have the if (sharedDocStores) thing, and any
part of the codec that wants to put stuff always outside of CFS (e.g. Lucene3x separate norms, deletes) 
could just use SegmentInfo.dir. Directory parameters in the case of CFS would always
consistently be the CFSDirectory.

I haven't totally tested if this will work but there is definitely some cleanups 
we can do either way, and I think it would be a good step to try to clean this up
and simplify it.
"
0,"QDefinitionBuilderFactory should auto-subtype from nt:baseSimilar to JCR-2066, the QNodeTypeDefinitions build by QDefinitionBuilderFactory should auto subtype from nt:base. "
0,"Handle Returning Null consistantlyConsider returning empty arrays instead of null consistantly.  eg:
getResponseBody().  There may be good reason for both null and empty array
depending on the circumstannces."
0,"First Steps document is outdatedAs reported by Manoj Prasad on the development mailing list, the code and configuration shown on the First Steps document [1] is no longer up to date with the latest Jackrabbit sources. The differences are:

   * the Versioning element needs to be added to the repository configuration file
   * the output values printed by the examples have changes (log messages, new node types, etc.)
   * multiple values (especially jcr:mixinTypes properties) are not handled correctly

The document should be updated.

[1] http://incubator.apache.org/jackrabbit/firststeps.html"
0,Jcr2spiRepositoryFactory: make class loading more robustCurrently Jcr2spiRepositoryFactory loads a RepositoryServiceFactory from the context class loader. In an OSGi environment this fails with a ClassNotFoundException. I suggest to fall back to the class loader of a the class (Jcr2spiRepositoryFactory) in this case. 
0,"SnowballAnalyzer has a link to net.sf (a package that is empty and needs to be removed).need to remove net.sf and points to org.tartarus.snowball.ext. Doesn't work as a link though, so I'll also remove the @link to lose the javadoc error and broken link."
0,"Field specified norms in MatchAllDocumentsScorer This patch allows for optionally setting a field to use for norms factoring when scoring a MatchingAllDocumentsQuery.

From the test case:
{code:java}
.
    RAMDirectory dir = new RAMDirectory();
    IndexWriter iw = new IndexWriter(dir, new StandardAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
    iw.setMaxBufferedDocs(2);  // force multi-segment
    addDoc(""one"", iw, 1f);
    addDoc(""two"", iw, 20f);
    addDoc(""three four"", iw, 300f);
    iw.close();

    IndexReader ir = IndexReader.open(dir);
    IndexSearcher is = new IndexSearcher(ir);
    ScoreDoc[] hits;

    // assert with norms scoring turned off

    hits = is.search(new MatchAllDocsQuery(), null, 1000).scoreDocs;
    assertEquals(3, hits.length);
    assertEquals(""one"", ir.document(hits[0].doc).get(""key""));
    assertEquals(""two"", ir.document(hits[1].doc).get(""key""));
    assertEquals(""three four"", ir.document(hits[2].doc).get(""key""));

    // assert with norms scoring turned on

    MatchAllDocsQuery normsQuery = new MatchAllDocsQuery(""key"");
    assertEquals(3, hits.length);
//    is.explain(normsQuery, hits[0].doc);
    hits = is.search(normsQuery, null, 1000).scoreDocs;

    assertEquals(""three four"", ir.document(hits[0].doc).get(""key""));    
    assertEquals(""two"", ir.document(hits[1].doc).get(""key""));
    assertEquals(""one"", ir.document(hits[2].doc).get(""key""));
{code}"
0,"WebdavResponseImpl should cache TransformerFactoryJackrabbitResponeImpl.sendXmlResponse creates an instance of TransformerFactory on each invocation. We see, that this TransformerFactory initialization consumes significant amount of time, because of complex logic inside:

{code}
    at java.lang.String.intern(Native Method)
    at java.util.jar.Attributes$Name.<init>(Attributes.java:449)
    at java.util.jar.Attributes.putValue(Attributes.java:151)
    at java.util.jar.Attributes.read(Attributes.java:404)
    at java.util.jar.Manifest.read(Manifest.java:234)
    at sun.security.util.SignatureFileVerifier.processImpl(SignatureFileVerifier.java:188)
    at sun.security.util.SignatureFileVerifier.process(SignatureFileVerifier.java:176)
    at java.util.jar.JarVerifier.processEntry(JarVerifier.java:277)
    at java.util.jar.JarVerifier.update(JarVerifier.java:188)
    at java.util.jar.JarFile.initializeVerifier(JarFile.java:321)
    at java.util.jar.JarFile.getInputStream(JarFile.java:386)
    at sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:144)
    at java.net.URL.openStream(URL.java:1009)
    at java.lang.ClassLoader.getResourceAsStream(ClassLoader.java:1170)
    at javax.xml.transform.SecuritySupport$4.run(SecuritySupport.java:94)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.xml.transform.SecuritySupport.getResourceAsStream(SecuritySupport.java:87)
    at javax.xml.transform.FactoryFinder.findJarServiceProvider(FactoryFinder.java:250)
    at javax.xml.transform.FactoryFinder.find(FactoryFinder.java:223)
    at javax.xml.transform.TransformerFactory.newInstance(TransformerFactory.java:102)
    at org.apache.jackrabbit.webdav.WebdavResponseImpl.sendXmlResponse(WebdavResponseImpl.java:163)
{code}

TransformerFactory can be cached in static field:

private static final TransofmerFactory transformerFactory = TransformerFactory.newInstance()."
0,serialVersionUID for AuthorizableExistsException
0,"Improve reliability of canAddMixinThe current implementation of canAddMixin in JCR2SPI lacks flexibility. It only consults the (SPI) node type registry, checking for (1) whether the mixin exists, and (2) whether it is already present and (3) whether it's consistent with the node's type.

This is fine for stores where any legal mixin can be added anywhere. It doesn't work well for stores that are limited in what they can do; for instance when nt:file nodes can be made mix:versionable, but nt:folder nodes can't.

Proposal: enhance QNodeTypeDefinition with

  public Name[] getSupportedMixins();

where the return value is either null (no constraints or no constraints known), or a list of mixin types that are supported for this node type."
0,"Provide Programmatic Access to CheckIndexWould be nice to have programmatic access to the CheckIndex tool, so that it can be used in applications like Solr.  

See SOLR-566"
0,Add bundle support for PostgreSQLThe class DbNameIndex does not work with this RDBMS since the RETURN_GENERATED_KEYS JDBC feature is not implemented in current PostgreSQL drivers.
0,"WordListLoader.java should be able to read stopwords from a ReaderWordListLoader should be able to read the stopwords from a Reader.

This would (for example) allow stopword lists to be stored as a resource in the
jar file of a Lucene application.

Diff is attached."
0,"Setting different MAX_HOST_CONNECTION values per host using a single MultiThreadedHttpConnectionManagerRight now, it's not possible to use the
MultiThreadedHttpConnectionManager.setMaxConnectionsPerHost(int) method in a per
HostConfiguration basis. The value applies to every HostConfiguration the
current connection manager is managing.

I would be quite useful to allow the connection manager to set different values
depending on the HostConfiguration."
0,"Remove unnecessary TestAll classes in jcr-commonsThe module jackrabbit-jcr-commons uses the default test configuration, which means the TestAll test suites are not necessary. They actually cause all tests to be executed twice."
0,Replace TrackingInpuStream with Commons IOThe TrackingInputStream class in jackrabbit-core implements essentially the same functionality as the Commons IO class CountingInputStream.
0,"Spellchecker ""Suggest Mode"" SupportThis is a spin-off from SOLR-2585.

Currently o.a.l.s.s.SpellChecker and o.a.l.s.s.DirectSpellChecker support two ""Suggest Modes"":
1. Suggest for terms that are not in the index.
2. Suggest ""more popular"" terms.

This issue is to add a third Suggest Mode:
3. Suggest always.

This will assist users in developing context-sensitive spell suggestions and/or did-you-mean suggestions.  See SOLR-2585 for a full discussion.

Note that o.a.l.s.s.SpellChecker already can support this functionality, if the user passes in a NULL term & IndexReader.  This, however, is not intutive.  o.a.l.s.s.DirectSpellChecker currently has no support for this third Suggest Mode."
0,SimpleTextCodec needs SimpleText DocValues implcurrently SimpleTextCodec uses binary docValues we should move that to a simple text impl.
0,"Introduce cache for frequently used index lookupsSome queries heavily use hierarchy relations to resolve location steps. E.g. ChildAxisQuery or DescendantSelfAxisQuery. Currently those hierarchy relations are looked up from the native lucene index which is not very efficient. The index should maintain a cache of frequently used hierarchy lookups. 
That is, calls like IndexReader.termDocs() on terms with field: UUID or PARENT"
0,"Lock test assumes that changes in one session are immediately visible in different sessionLockTest.testLogout() assumes that a change in one session (logging out, removing a session-scoped lock) is immediately visible in another session.

Proposal: insert a 

 n1.getSession().refresh(true);

call before checking

 assertFalse(""node must not be locked"", n1.isLocked());"
0,"repository.xml DTD doesn't allow <DataStore> elementThe repository.xml DTD at http://jackrabbit.apache.org/dtd/repository-1.4.dtd conflicts with the instructions in the wiki page at http://wiki.apache.org/jackrabbit/DataStore

Adding the <DataStore> element as specified in the wiki page violates the DTD.

"
0,"Optimize TermsEnum.seek when caller doesn't need next termSome codecs are able to save CPU if the caller is only interested in
exact matches.  EG, Memory codec and SimpleText can do more efficient
FSTEnum lookup if they know the caller doesn't need to know the term
following the seek term.

We have cases like this in Lucene, eg when IW deletes documents by
Term, if the term is not found in a given segment then it doesn't need
to know the ceiling term.  Likewise when TermQuery looks up the term
in each segment.

I had done this change as part of LUCENE-3030, which is a new terms
index that's able to save seeking for exact-only lookups, but now that
we have Memory codec that can also save CPU I think we should commit
this today.

The change adds a ""boolean onlyExact"" param to seek(BytesRef).
"
0,"The native FS lock used in test-framework's o.a.l.util.LuceneJUnitResultFormatter prohibits testing on a multi-user system{{LuceneJUnitResultFormatter}} uses a lock to buffer test suites' output, so that when run in parallel, they don't interrupt each other when they are displayed on the console.

The current implementation uses a fixed directory ({{lucene_junit_lock/}} in {{java.io.tmpdir}} (by default {{/tmp/}} on Unix/Linux systems) as the location of this lock.  This functionality was introduced on SOLR-1835.

As Shawn Heisey reported on SOLR-2739, some tests fail when run as root, but succeed when run as a non-root user.  

On #lucene IRC today, Shawn wrote:
{quote}
(2:06:07 PM) elyograg: Now that I know I can't run the tests as root, I have discovered /tmp/lucene_junit_lock.  Once you run the tests as user A, you cannot run them again as user B until that directory is deleted, and only root or the original user can do so.
{quote}
"
0,"fix LowerCaseFilter for unicode 4.0lowercase suppl. characters correctly. 

this only fixes the filter, the LowerCaseTokenizer is part of a more complex issue (CharTokenizer)
"
0,"Remove/deprecate IndexReader.undeleteAllThis API is rather dangerous in that it's ""best effort"" since it can only un-delete docs that have not yet been merged away, or, dropped (as of LUCENE-2010).

Given that it exposes impl details of how Lucene prunes deleted docs, I think we should remove this API.

Are there legitimate use cases....?"
0,Improve Performance of DescendantSelfAxisQueryIn DescendantSelfAxisQuery.DescendantSelfAxisScorer.isValid(int) contextHits is populated with docs that are found on the way down the axis. The current algorithm unfortunately doesn't add any new docs at all because it only adds docs already present in contextHits. This leads to more calls to HierarchyResolver.getParent(int) than necessary.
0,"uploading large streams through rmiwhen I try to upload a file of 35 Meg, I get an out of memory error.

This is caused because the whole file is read into memory instead of buffering"
0,"improved compound file handlingCurrently CompoundFileReader could use some improvements, i see the following problems
* its CSIndexInput extends bufferedindexinput, which is stupid for directories like mmap.
* it seeks on every readInternal
* its not possible for a directory to override or improve the handling of compound files.

for example: it seems if you were impl'ing this thing from scratch, you would just wrap the II directly (not extend BufferedIndexInput,
and add compound file offset X to seek() calls, and override length(). But of course, then you couldnt throw read past EOF always when you should,
as a user could read into the next file and be left unaware.

however, some directories could handle this better. for example MMapDirectory could return an indexinput that simply mmaps the 'slice' of the CFS file.
its underlying bytebuffer etc naturally does bounds checks already etc, so it wouldnt need to be buffered, not even needing to add any offsets to seek(),
as its position would just work.

So I think we should try to refactor this so that a Directory can customize how compound files are handled, the simplest 
case for the least code change would be to add this to Directory.java:

{code}
  public Directory openCompoundInput(String filename) {
    return new CompoundFileReader(this, filename);
  }
{code}

Because most code depends upon the fact compound files are implemented as a Directory and transparent. at least then a subclass could override...
but the 'recursion' is a little ugly... we could still label it expert+internal+experimental or whatever.
"
0,"Improve StandardTokenizer's understanding of non ASCII punctuation and quotesIn the vein of LUCENE-1126 and LUCENE-1390, StandardTokenizerImpl.jflex should do a better job at understanding non-ASCII punctuation characters.

For example, its understanding of the single-quote character ""'"" is currently limited to that character only. It will set a token's type to APOSTROPHE only if the ""'"" was used.
In the patch attached, I added all the characters that ASCIIFoldingFilter would change into ""'"".

I'm not sure that this is the right approach so I didn't write a complete patch for all the other hardcoded characters used in jflex rules such as ""."", ""-"" which have some variants in ASCIIFoldingFilter that could be used as well.

Maybe a better approach would be to make it possible to have an ASCIIFoldingFilter-like reader as a character filter that could be in inserted in front of StandardTokenizer ?"
0,"Create jackrabbit-api(.jar) and the respective jackrabbit-rmi extensionscurrently some of the management functions not covered in jcr, like notetype management and workspace creation, are not exposed via any specific api and therfor not accessible via rmi.

create a jackrabbit api and the respective rmi extension."
0,"Unit tests for HttpConnHttpConn needs more test coverage.
Starting at 0%.
"
0,"Index sorterA tool to sort index according to a float document weight. Documents with high weight are given low document numbers, which means that they will be first evaluated. When using a strategy of ""early termination"" of queries (see TimeLimitedCollector) such sorting significantly improves the quality of partial results.

(Originally this tool was created by Doug Cutting in Nutch, and used norms as document weights - thus the ordering was limited by the limited resolution of norms. This is a pure Lucene version of the tool, and it uses arbitrary floats from a specified stored field)."
0,"RepositoryImpl.activeSessions should use Session instead of SessionImplTurn Map<SessionImpl, SessionImpl> activeSessions into Map<Session, Session> activeSessions as there is not clear need for the use of SessionImpl."
0,"Log / trace wrapper for the JCR APII have implemented the log / trace mechanism for the JCR API. A short summary:

- A wrapper for a Repository. All other objects that where created directly or indirectly (Session, Node and so on) are wrapped as well. 
- The wrappers log all JCR method calls to a file and call the underlying methods. 
- Return values and calling method / line number can be logged as well (optional). 
- The log file itself is mainly Java source code and can be compiled and run.
- Included is a player to re-play log files (for example, if the log file is too big to be compiled).
"
0,"FormBodyPart code does not agree with ContentDescriptor Javadoc wrt nullability of mimeType and transferEncodingThe FormBodyPart does not agree with ContentDescriptor Javadoc wrt nullability of mimeType and transferEncoding:

The code in FormBodyPart explicitly allows mimeType and transferEncoding to be null, in which case the relevant header is not generated.
This is useful behaviour, as the headers are not necessaruly needed.

However the bahaviour disagrees with the Javadoc in the ContentDescriptor interface - null is not allowed.
Also, AbstractContentBody does not allow mime-type to be null."
0,"Add support for Map of referenced beansOCM should support the mapping of maps of referenced beans.

@Collection(collectionConverter= BeanReferenceCollectionConverterImpl.class)
private java.util.Map<String, ReferencedBean> aMap;

BeanReferenceCollectionConverterImpl (mainly the method doGetCollection) needs to be updated to support the interface ManageableMap interface.
"
0,"Supplementary Character Handling in CharTokenizerCharTokenizer is an abstract base class for all Tokenizers operating on a character level. Yet, those tokenizers still use char primitives instead of int codepoints. CharTokenizer should operate on codepoints and preserve bw compatibility. "
0,"Jackrabbit utilitiesAttached are two utilities for Jackrabbit:

The first one is a DataStore implementation that uses Amazon S3 for storage.
This is fairly straightforward. It is configured by adding a DataStore
section to the repository.xml file, e.g.:
   <DataStore class=""org.jcrutil.S3DataStore"">
       <param name=""awsAccessKey"" value="""" />
       <param name=""awsSecretKey"" value="""" />
       <param name=""bucketName"" value="""" />
       <param name=""minModifiedDate"" value=""0"" />
       <param name=""minRecordLength"" value=""0"" />
   </DataStore>

The second utility is a JCR based Commons VFS filesystem provider. This
allows you to access a JCR repository (nt:file and nt:folder nodes) using
the Commons VFS API. I've also used this with MINA FTP Server and Dctm VFS
(http://dctmvfs.sourceforge.net/) to provide FTP access to a Jackrabbit
repository.
"
0,Add memory based bundle store
0,"Make license checking/maintenance easier/automatedInstead of waiting until release to check licenses are valid, we should make it a part of our build process to ensure that all dependencies have proper licenses, etc."
0,"Reusable Repository access and bind servletsAs discussed in http://mail-archives.apache.org/mod_mbox/jackrabbit-dev/200705.mbox/%3C510143ac0705151453t7a0eb4cam859a40fb106e81f5@mail.gmail.com%3E and JCR-955, it would be useful to have a reusable set of servlet components for accessing and exposing repositories in various configurable ways.

My plan is to refactor the current RepositoryAccessServlet from jackrabbit-webapp and place the resulting servlet components in jackrabbit-jcr-commons, with servlet-api as a new optional (or provided) dependency."
0,"[API Doc] Compile performance optimization guidePerformance optimization guide is long overdue and badly needed. The more people
start using HttpClient in all sorts of creative ways the more we are going to
need it.

Oleg"
0,"Remove benchmark/lib/xml-apis.jar - JVM 1.5 already contains the required JAXP 1.3 implementationOn [LUCENE-2957|https://issues.apache.org/jira/browse/LUCENE-2957?focusedCommentId=13004991&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13004991], Uwe wrote:
{quote}
xml-apis.jar is not needed with xerces-2.9 and Java 5, as Java 5 already has these interface classes (JAXP 1.3). Xerces 2.11 needs it, because it ships with Java 6's JAXP release (containing STAX & Co. not available in Java 5).
{quote}

On the #lucene IRC channel, Uwe also wrote:
{noformat}
since we are on java 5 since 3.0
we have the javax APIs already available in the JVM
xerces until 2.9.x only needs JAXP 1.3
so the only thing you need is xercesImpl.jar
and serializer.jar
serializer.jar is shared between all apache xml projects, dont know the exact version number
ok you dont need it whan you only parse xml
as soon as you want to serialize a dom tree or result of an xsl transformation you need it
[...]
but if we upgrade to latest xerces we need it [the xml-apis jar] again unless we are on java 6
so the one shipped with xerces 2.11 is the 1.4 one
because xerces 2.11 supports Stax
{noformat}"
0,"factor out a shared spellchecking moduleIn lucene's contrib we have spellchecking support (index-based spellchecker, directspellchecker, etc). 
we also have some things like pluggable comparators.

In solr we have auto-suggest support (with two implementations it looks like), some good utilities like HighFrequencyDictionary, etc.

I think spellchecking is really important... google has upped the ante to what users expect.
So I propose we combine all this stuff into a shared modules/spellchecker, which will make it easier
to refactor and improve the quality.
"
0,"Swap URL+Email recognizing StandardTokenizer and UAX29TokenizerCurrently, in addition to implementing the UAX#29 word boundary rules, StandardTokenizer recognizes email adresses and URLs, but doesn't provide a way to turn this behavior off and/or provide overlapping tokens with the components (username from email address, hostname from URL, etc.).

UAX29Tokenizer should become StandardTokenizer, and current StandardTokenizer should be renamed to something like UAX29TokenizerPlusPlus (or something like that).

For rationale, see [the discussion at the reopened LUCENE-2167|https://issues.apache.org/jira/browse/LUCENE-2167?focusedCommentId=12929325&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12929325]."
0,"Misleading exception message when re-index failsE.g. the log may say:

19.06.2007 11:25:42 *ERROR* RepositoryImpl: Failed to initialize workspace 'default' (RepositoryImpl.java, line 382)
javax.jcr.RepositoryException: Error indexing root node: 10022d38-c449-4751-b8f0-9d07ac45ead5:
[...]

The mentioned uuid is not the root node and the root cause is missing."
0,"add Galician analyzerAdds analyzer for Galician, based upon [""Regras do lematizador para o galego""|http://bvg.udc.es/recursos_lingua/stemming.jsp] , and a set of stopwords created in the usual fashion.

This is really just an adaptation of the Portuguese [RSLP|http://www.inf.ufrgs.br/~viviane/rslp/index.htm], so I added that too, and modified our existing hand-coded RSLP-S (RSLP's plural-only step) to just be a plural-only flow of RSLP.
"
0,"Flexible query parser does not support open ranges and range queries with mixed inclusive and exclusive rangesFlexible query parser does not support open ranges and range queries with mixed inclusive and exclusive ranges.

These two problems were found while developing LUCENE-1768."
0,"Optimize BlockTermsReader.seekWhen we seek, we first consult the terms index to find the right block
of 32 (default) terms that may hold the target term.  Then, we scan
that block looking for an exact match.

The scanning just uses next() and then compares the full term, but
this is actually rather wasteful.  First off, since all terms in the
block share a common prefix, we should compare the target against that
common prefix once, and then only compare the new suffix of each
term.  Second, since the term suffixes have already been read up front
into a byte[], we should do a no-copy comparison (vs today, where we
first read a copy into the local BytesRef and then compare).

With this opto, I removed the ability for BlockTermsWriter/Reader to
support arbitrary term sort order -- it's now hardwired to
BytesRef.utf8SortedAsUnicode.
"
0,"Remove old hooks for the implementation of hard linksEarly drafts of the JCR specification specified that repositories should support ""hard links"", which woud lead to the situation, that items might have multiple parent nodes. In the meantime hard links have been removed from the spec and are unlikely to be re-added in future revisions.

Nevertheless, Jackrabbit still contains some references to supporting this mechanism (e.g. the NodeState.parentUUIDs field), which should be removed."
0,"Allow query results with unknown sizeTo further optimize certain queries the query implementation should be changed to allow for unknown result sizes. Currently there is only one query ( //* ) where the query result returns an unknown size and a special query result implementation is returned. At the same time, this should be fixed that only one implementation is used."
0,"make collection element names configurable- add jcrElementName to CollectionDescriptor and Collection annotation
- make COLLECTION_ELEMENT_NAME protected instead of private
"
0,"NodeTypeRegistry could auto-subtype from nt:basewhen tying to register a (primary) nodetype that does not extend from nt:base the following error is
thrown:

""all primary node types except nt:base itself must be (directly or indirectly) derived from nt:base""

since the registry is able to detect this error, it would be easy to auto-subtype all nodetypes from nt:base. imo it's pointless to explzitely add the nt:base to every supperclass set. as an analogy, you don't need to 'extend from java.lang.Object' explicitely - the compiler does that automatically for your."
0,"hashCode improvementsIt would be nice for all Query classes to implement hashCode and equals to enable them to be used as keys when caching.
"
0,Provide fail-over for multi-home remote servers (if one server in a farm goes down)The HTTP Client does not provide automatic fail-over for multi-home remote servers (web-farm) if one server in a farm goes down
0,"Directory createOutput and openInput should take an IOContextToday for merging we pass down a larger readBufferSize than for searching because we get better performance.

I think we should generalize this to a class (IOContext), which would hold the buffer size, but then could hold other flags like DIRECT (bypass OS's buffer cache), SEQUENTIAL, etc.

Then, we can make the DirectIOLinuxDirectory fully usable because we would only use DIRECT/SEQUENTIAL during merging.

This will require fixing how IW pools readers, so that a reader opened for merging is not then used for searching, and vice/versa.  Really, it's only all the open file handles that need to be different -- we could in theory share del docs, norms, etc, if that were somehow possible."
0,"Move NoDeletionPolicy from benchmark to coreAs the subject says, but I'll also make it a singleton + add some unit tests, as well as some documentation. I'll post a patch hopefully today."
0,"javacc skeleton files not regeneratedCopies of the the character stream files for javacc are checked into svn. These files were generated under javacc 3.0 (at least that's what they say, though javacc 3.2 says this too). javacc 4 complains that they are out of date but won't replace them; they must be removed before it will regenerate them.

There is one side effect of removing them: local changes are lost.  r387550 removed a couple of deprecated methods. By using the files as generated by javacc, these deprecated  methods will be readded (at least until the javacc team removes them totally). There are other changes being made to the stream files, so I woudl think it's better to live with them unmodified than to keep local versions just for this change.

If we want javacc to recreate the files, the attached patch will remove them before running javacc.

All the tests pass using both javacc3.2 and 4.0.


"
0,"singletermsenumsingletermsenum for flex (like the existing singletermenum, it is a filteredtermSenum that only matches one term, to preserve multitermquery semantics)"
0,"Optimize concurrent queriesThere are a number of bottlenecks that prevent scalability of concurrent queries:

- Fake norms are created repeatedly because a new SearchIndex$CombinedIndexReader is created for each query. This prevents caching of fake norms on the level of the CombinedIndexReader. Creating fake norms for index readers that span multiple sub reader is inefficient and should be avoided. Like with other Jackrabbit specific queries, there should be one for TermQuery, which is aware of sub readers. Its weight should then create one scorer for each sub reader. This effectively reuses the fake norms on the sub reader.

- There should be a  UUID cache that maps document number to UUID. This is basically the inverse of the existing DocNumberCache. UUID lookup is regularly a bottleneck in the SegmentReader where the method document() is synchronized and does I/O.

- Queries often contain constraints that limit the result to nodes with a certain flag set to a literal. These constraints should be cached in the query handler."
0,"StopFilter should have option to incr positionIncrement after stop wordI've seen this come up on the mailing list a few times in the last month, so i'm filing a known bug/improvement arround it...

StopFilter should have an option that if set, records how many stop words are ""skipped"" in a row, and then sets that value as the positionIncrement on the ""next"" token that StopFilter does return."
0,"tests should run checkIndex on indexes they createI think we should add a boolean checkIndexesOnClose (default=true) to MockDirectoryWrapper.

Only a very few tests need to disable this.
"
0,"Make prefixLength accessible to PrefixTermEnum subclassesPrefixTermEnum#difference() offers a way to influence scoring based on the difference between the prefix Term and a term in the enumeration. To effectively use this facility the length of the prefix should be accessible to subclasses. Currently the prefix term is private to PrefixTermEnum. I added a getter for the prefix length and made PrefixTermEnum#endEnum(), PrefixTermEnum#termCompare() final for consistency with other TermEnum subclasses.

Patch is attached.

Simon"
0,Remove @author tags in jackrabbit-jcr-rmiIt is a recommendation within Apache not to use @author tags or other means to identify source code with individual developers.  The @author tags in jackrabbit-jcr-rmi should therefore be removed.
0,"automaton spellcheckerThe current spellchecker makes an n-gram index of your terms, and queries this for spellchecking.
The terms that come back from the n-gram query are then re-ranked by an algorithm such as Levenshtein.

Alternatively, we could just do a levenshtein query directly against the index, then we wouldn't need
a separate index to rebuild.
"
0,Upgrade all default socket factories to use SO_REUSEADDR parameterSee HTTPCORE-209
0,"Merge UUID to NodeIdThe current NodeId class is mostly just a wrapper around UUID, which causes two objects to be instantiated for each node identifier that the system uses. The memory and processing overhead is quite small, but given that there are tons of NodeId instances it would be good to eliminate that overhead.

There is also lots of code that just converts UUIDs to NodeIds and vice versa. We could simplify such code if we just used NodeId everywhere.

Also, we might want to open up the possibility of using non-UUID node identifiers at some point in future, so it would make a lot of sense to remove the NodeId.getUUID method and rely directly on NodeId and it's equals(), hashCode(), and toString() methods in many places where we currently use UUIDs."
0,"upgrade contrib/ant's tidy.jarcontrib/ant uses a Tidy.jar that also includes classes in org.w3c.dom, org.xml.sax, etc.

This is no problem if you are an ant user, but if you are an IDE user you need to carefully configure the order of your classpath or things will not compile, as these will override the ones in the Solr libs, for example.

The solution is to upgrade the tidy.jar to the newest one that only includes org.w3c.tidy and doesn't cause these problems."
0,"Can not subscribeHello, 
I have sent email to lucene-dev-subscribe@jakarta.apache.org and it always 
returns failed: 
 
<lucene-dev-subscribe@jakarta.apache.org>: 
Sorry, no mailbox here by that name. (#5.1.1) 
 
Please help me subscribe."
0,"fileformats.xml doesn't document compound file streamsCurrent versions of Lucene generate segments in compound file stream format
files, but the fileformats documentation does not have any description of the
format for those files."
0,"Optimize usage of normsThere is a very significant potential for optimizing the size of the search index.

We have seen a case where there were multiple segments with about the same number of nodes (roughly 10 million), but the size on disk was very different.
One segment was 19 GB while all others where around 3 GB. The major difference was the number of fields indexed. The large segment had significantly more fields, which resulted in a large norms file.

We should go through our implementation and see where norms are really necessary and disable tracking of norms wherever possible."
0,"Speedup merging of stored fields when field mapping ""matches""Robert Engels suggested the following idea, here:

  http://www.gossamer-threads.com/lists/lucene/java-dev/54217

When merging in the stored fields from a segment, if the field name ->
number mapping is identical then we can simply bulk copy the entire
entry for the document rather than re-interpreting and then re-writing
the actual stored fields.

I've pulled the code from the above thread and got it working on the
current trunk."
0,"Performance improvement for TermInfosReaderCurrently we have a bottleneck for multi-term queries: the dictionary lookup is being done
twice for each term. The first time in Similarity.idf(), where searcher.docFreq() is called.
The second time when the posting list is opened (TermDocs or TermPositions).

The dictionary lookup is not cheap, that's why a significant performance improvement is
possible here if we avoid the second lookup. An easy way to do this is to add a small LRU 
cache to TermInfosReader. 

I ran some performance experiments with an LRU cache size of 20, and an mid-size index of
500,000 documents from wikipedia. Here are some test results:

50,000 AND queries with 3 terms each:
old:                  152 secs
new (with LRU cache): 112 secs (26% faster)

50,000 OR queries with 3 terms each:
old:                  175 secs
new (with LRU cache): 133 secs (24% faster)

For bigger indexes this patch will probably have less impact, for smaller once more.

I will attach a patch soon."
0,"PayloadTermQuery refers to a deprecated documentation for redirection When PayloadTermQuery refers to the function for scoring Similarity - it refers to override the deprecated method - 

Similarity#scorePayload(String, byte[],int,int) . 

That method has been deprecated by  Similarity#scorePayload(int, String, int, int, byte[],int,int) . 


This javadoc patch addresses the class level javadoc for the class to provide the right signature in Similarity to be overridden. "
0,"optimize spanfirstquery, spanpositionrangequerySpanFirstQuery and SpanPositionRangeQuery (SpanFirst is just a special case of this), are currently inefficient.

Take this worst case example: SpanFirstQuery(""the"").
Currently the code reads all the positions for the term ""the"".

But when enumerating spans, once we have passed the allowable range we should move on to the next document (skipTo)
 "
0,Disable consistency check per defaultThere should be a way to disable the consistency check entirely. Currently a consistency check is performed on startup whenever the redo log is applied. For large workspaces this may take a long time and should only be performed when 'requested'.
0,"Optimize ReadOnlyIndexReader.read(int[] docs, int[] freqs)This method is currently implemented trivially using next(), doc() and freq(). It should read in blocks and filter out deleted docs."
0,"don't silently merge session-local transient changes with external changes before save().currently, external changes (i.e. changes committed by other sessions) are silently merged with transient changes. this might potentially cause concurrency issues/inconsistent transient state (see e.g. JCR-2632).

it would probably be better to isolate transient changes from external changes until they're saved (true copy-on-write). "
0,"improve documentation of SPI Batch addPropertyClarify that Batch.addProperty should succeed even though the property already exists.


(See mailing list thread starting with: http://mail-archives.apache.org/mod_mbox/jackrabbit-dev/200801.mbox/%3c47A1E1C1.2050107@gmx.de%3e)"
0,"httpClient does not support installation of different SSLSocketFactoryDescription:

The SSLProtocolSocketFactory class had hard-
coded ""javax.net.ssl.SSLSocketFactory"" as the socket factory.  It does not 
support installation of other socket factory.

Proposed Fix:

We added a setDefaultSSLSocketFactory method to the SSLProtocolSocketFactory 
and modified the code to use the factory it it is set.  The code falls back on 
using ""javax.net.ssl.SSLSocketFactory"" if a default is not set."
0,"Eliminate class HostConfigurationRemove the target host attribute from the HostConfiguration class. This will allow one HostConfiguration object to be used for different targets.
The problem is that currently MultiThreadedHttpConnectionManager uses HostConfiguration objects as cache keys, which needs to be changed.

This is a followup to HTTPCLIENT-615.

cheers,
  Roland
"
0,"Unclosed sessions in test casesSome tests may throw exceptions in the setUp() method and leave the session open that was opened in the super class setUp() method. For jackrabbit-core, this is not really a problem, because the memory footprint of a session is quite small, but in jcr2spi the memory footprint is considerable higher, which may lead to out of memory errors when running the tests."
0,"Lower log level in o.a.j.jcr2spi.query.NodeIteratorImplNodeIteratorImpl.fetchNext() logs an error when it cannot load a node and skips that node. Since this is not an error condition (the node could have been deleted by another session), logging should occur at the warn level."
0,"Implementation of Delete methodThe HTTP request method, Delete, had not been implemented. I needed it and created an HttpDelete class modeled after HttpGet."
0,"add LuceneTestCase.rarely()/LuceneTestCase.atLeast()in LUCENE-3175, the tests were sped up a lot by using reasonable number of iterations normally, but cranking up for NIGHTLY.
we also do crazy things more 'rarely' for normal builds (e.g. simpletext, payloads, crazy merge params, etc)
also, we found some bugs by doing this, because in general our parameters are too fixed.

however, it made the code look messy... I propose some new methods:
instead of some crazy code in your test like:
{code}
int numdocs = (TEST_NIGHTLY ? 1000 : 100) * RANDOM_MULTIPLIER;
{code}

you use:
{code}
int numdocs = atLeast(100);
{code}

this will apply the multiplier, also factor in nightly, and finally add some random fudge... so e.g. in local runs its sometimes 127 docs, sometimes 113 docs, etc.

additionally instead of code like:
{code}
if ((TEST_NIGHTLY && random.nextBoolean()) || (random.nextInt(20) == 17)) {
{code}

you do
{code}
if (rarely()) {
{code}

which applies NIGHTLY and also the multiplier (logarithmic growth).
"
0,"Analysis package calls Java 1.5 APII found compile errors when I tried to compile trunk with 1.4 JVM.
org.apache.lucene.analysis.NormalizeCharMap
org.apache.lucene.analysis.MappingCharFilter

uses Character.valueOf() which has been added in 1.5.
I added a CharacterCache (+ testcase) with a valueOf method as a replacement for that quite useful method.

org.apache.lucene.analysis.BaseTokenTestCase

uses StringBuilder instead of the synchronized version StringBuffer (available in 1.4)

I will attach a patch shortly."
0,"Basic refactoring of DocumentsWriterAs a starting point for making DocumentsWriter more understandable,
I've fixed its inner classes to be static, and then broke the classes
out into separate sources, all in org.apache.lucene.index package.

"
0,"Replace Maven POM templates with full POMs, and change documentation accordinglyThe current Maven POM templates only contain dependency information, the bare bones necessary for uploading artifacts to the Maven repository.

The full Maven POMs in the attached patch include the information necessary to run a multi-module Maven build, in addition to serving the same purpose as the current POM templates.

Several dependencies are not available through public maven repositories.  A profile in the top-level POM can be activated to install these dependencies from the various {{lib/}} directories into your local repository.  From the top-level directory:

{code}
mvn -N -Pbootstrap install
{code}

Once these non-Maven dependencies have been installed, to run all Lucene/Solr tests via Maven's surefire plugin, and populate your local repository with all artifacts, from the top level directory, run:

{code}
mvn install
{code}

When one Lucene/Solr module depends on another, the dependency is declared on the *artifact(s)* produced by the other module and deposited in your local repository, rather than on the other module's un-jarred compiler output in the {{build/}} directory, so you must run {{mvn install}} on the other module before its changes are visible to the module that depends on it.

To create all the artifacts without running tests:

{code}
mvn -DskipTests install
{code}

I almost always include the {{clean}} phase when I do a build, e.g.:

{code}
mvn -DskipTests clean install
{code}
"
0,"Audit logJCR-2031 added the user name and path in debug logs for audit purposes. There are some problems with the fix that I had outlined in the comments for JCR-2031 and provided a patch. Additionally, it would use useful to add an update counter and size information to the debug log as well. Something like this:

17.03.2009 14:43:37 [1] 18216140 admin@/apps/acme/templates/contentpage/thumbnail.png (12343) 
17.03.2009 14:43:37 [2] 18216141 admin@/apps/acme/templates/contentpage/my.png (123) 
17.03.2009 14:43:37 [3] 18216142 admin@/apps/acme/templates/contentpage/blah.png (1423) 
17.03.2009 14:43:37 [4] 18216143 admin@/apps/acme/templates/contentpage/test.png (123423) 
17.03.2009 14:43:37 [5] 18216144 admin@/apps/acme/templates/contentpage/test2.png (123423) 

<date> <time> [<counter>] <txid> <userid>@<path> (<size>)

We should also think about whether we want this log as part of regular jackrabbit log or in a separate audit log. 
"
0,"port url+email tokenizer to standardtokenizerinterface (or similar)We should do this so that we can fix the LUCENE-3358 bug there, and preserve backwards.
We also want this mechanism anyway, for upgrading to new unicode versions in the future.

We can regenerate the new TLD list for 3.4 but, we should ensure the existing one is used for the urlemail33 or whatever,
so that its exactly the same."
0,"Improve performance of MatchAllScorerThe BitSets created in MatchAllScorer should be cached per IndexReader. This enhancement should also take care that the supplied IndexReader may in fact be a CombinedIndexReader or a CachingMultiReader with multiple contained IndexReaders. To achieve a good cache efficiency the BitSets must be cached per contained IndexReader and combined later.

See also thread on dev list: http://thread.gmane.org/gmane.comp.apache.jackrabbit.devel/10976"
0,JSR 283: EventJournalImplement the event journal as specified in JSR 283.
0,"contrib/benchmark tests fail find data dirsThis was exposed by LUCENE-940 - a test was added that uses the Reuters collection. Then tests succeed when ran from contrib/benchmark (e.g. by IDE) but fail when running as part of ""ant test-contrib"" because the test expects to find the Reuters data under trunk/work. 
"
0,"Re-add SorterTemplate and use it to provide fast ArraySorting and replace BytesRefHash sortingThis patch adds back an optimized and rewritten SorterTemplate back to Lucene (removed after release of 3.0). It is of use for several components:

- Automaton: Automaton needs to sort States and other things. Using Arrays.sort() is slow, because it clones internally to ensure stable search. This component is much faster. This patch adds Arrays.sort() replacements in ArrayUtil that work with natural order or using a Comparator<?>. You can choose between quickSort and mergeSort.
- BytesRefHash uses another QuickSort algorithm without insertionSort for very short ord arrays. This class uses SorterTemplate to provide the same with insertionSort fallback in a very elegant way. Ideally this class can be used everywhere, where the sort algorithm needs to be separated from the underlying data and you can implement a swap() and compare() function (that get slot numbers instead of real values). This also applies to Solr (Yonik?).

SorterTemplate provides quickSort and mergeSort algorithms. Internally for short arrays, it automatically chooses insertionSort (like JDK's Arrays). The quickSort algorith was copied modified from old BytesRefHash. This new class only shares MergeSort with the original CGLIB SorterTemplate, which is no longer maintained."
0,"ASCIIFoldingFilter: expose folding logic + small improvements to ISOLatin1AccentFilterThis patch adds a couple of non-ascii chars to ISOLatin1AccentFilter (namely: left & right single quotation marks, en dash, em dash) which we very frequently encounter in our projects. I know that this class is now deprecated; this improvement is for legacy code that hasn't migrated yet.

It also enables easy access to the ascii folding technique use in ASCIIFoldingFilter for potential re-use in non-Lucene-related code."
0,"build.xml's tar task should use longfile=""gnu""The default (used now) is the same, but we get all those nasty false warnings filling the screen."
0,Provide a method for writing name space declarations in CompactNodeTypeDefWriterCurrently CompactNodeTypeDefWriter includes (when configured to do so) only name space declarations from name spaces actually used in the node type definitions written. In some situations it is necessary to write additional name space declarations. I thus propose to add a method writeNamespaceDelclaration.
0,"Permit using different tablespaces for tables and indexes with OracleOracleFileSystem, OraclePersistenceManager and OracleDatabaseJournal already provide a tableSpace parameter to customize the DDL, but the same tablespace is used for both tables and indexes. It is common place to use distinct tablespaces for these. Jackrabbit could provide support for this."
0,"GData - Server wrong commit does not buildThe last GData - Server commit  does not build due to a wrong commit.
Yonik did not commit all the files in the diff file. There are several sources and packages missing.
  
The diff - file with the date of 26.06.06 should be applied.
--> http://issues.apache.org/jira/browse/LUCENE-598
26.06.06.diff (644 kb)

could any of the lucene committers apply this patch. Yonik is on the way to Dublin.

Thanks Simon
"
0,"Fine grained locking in SharedItemStateManagerThe SharedItemStateManager (SISM) currently uses a simple read-write lock to ensure data consistency. Store operations to the PersistenceManager (PM) are effectively serialized.

We should think about more sophisticated locking to allow concurrent writes on the PM.

One possible approach:

If a transaction is currently storing data in a PM a second transaction may check if the set of changes does not intersect with the first transaction. If that is the case it can safely store its data in the PM.

This fine grained locking must also be respected when reading from the SISM. A read request for an item that is currently being stored must be blocked until the store is finished."
0,"JCR Test for Adding Node Type Tests That Abstract Nodes Can Be Added as Children, contrary to JCR 2.0 specificationWhen the TCK test method testLegalAndResidualType in the CanAddChildNodeCallWithNodeTypeTest class picks a node with a residual type, it does not filter out abstract nodes.  For example, in my local test, nt:hierarchyNode is selected for the local variable 'type'.

Since abstract node types ""cannot be directly assigned to a node,""[1] canAddChildNode(anyPropertyName, ""nt:hierarchyNode"") must return false.  However, since the test assumes that a non-abstract node type was chosen, it expects canAddChildNode(String, String) to return true.

This could be fixed if NodeTypeUtil.locateChildNodeDef(...) were extended to add an extra argument allowing or disallowing abstract types and that extra argument was used to filter the type used in testLegalAndResidualType (or if locateChildNodeDef(...) automatically excluded abstract types in the same manner that it automatically excludes protected types).

[1] - Section 3.7.1.3 of the JCR2 specification"
0,switch appendingcodec to use appending blocktreecurrently it still uses block terms + fixed gap index
0,"Add support for Digest authentication to the Authenticator classHere's some code initially whipped up by Geza for Apache Axis, now adapted to
HTTPClient that adds support for Digest authentication to the Authenticator
class. I have tested this code against tomcat 4.0.4 with a sample code that
calls an Apache Axis Web Service. One caveat according to Geza, the code ""Right
now does not support qop-int""."
0,"bulk postings should be codec privateIn LUCENE-2723, a lot of work was done to speed up Lucene's bulk postings read API.

There were some upsides:
* you could specify things like 'i dont care about frequency data up front'.
  This made things like multitermquery->filter and other consumers that don't
  care about freqs faster. But this is unrelated to 'bulkness' and we have a
  separate patch now for this on LUCENE-2929.
* the buffersize for standardcodec was increased to 128, increasing performance
  for TermQueries, but this was unrelated too.

But there were serious downsides/nocommits:
* the API was hairy because it tried to be 'one-size-fits-all'. This made consumer code crazy.
* the API could not really be specialized to your codec: e.g. could never take advantage that e.g. docs and freqs are aligned.
* the API forced codecs to implement delta encoding for things like documents and positions. 
  But this is totally up to the codec how it wants to encode! Some codecs might not use delta encoding.
* using such an API for positions was only theoretical, it would have been super complicated and I doubt ever
  performant or maintainable.
* there was a regression with advance(), probably because the api forced you to do both a linear scan thru
  the remaining buffer, then refill...

I think a cleaner approach is to let codecs do whatever they want to implement the DISI
contract. This lets codecs have the freedom to implement whatever compression/buffering they want
for the best performance, and keeps consumers simple. If a codec uses delta encoding, or if it wants
to defer this to the last possible minute or do it at decode time, thats its own business. Maybe a codec
doesn't want to do any buffering at all.
"
0,"default behaviour of useExpectHeaderI suggest to set the ExpectContinueMethod.setUseExpectHeader per default to
false or to arrange that per default it is not used. We lost an awfull lot of
time in a project in which we used MultipartPostMethod via a proxy. Everthing
worked fine in dev, however as soon as we started to use the proxy in production
or testing environment we had severe problems. We lost several manday looking
for the problem, including sniffing and logging op the proxy. It ended up to be
the useexpectheader which was true per default. Putting in on false ended our
problems...

In my opinion it is a bit hard to make something default behaviour if the
javadoc warns : <snip>
handshake should be used with caution, as it may cause problems with HTTP
servers and proxies that do not support HTTP/1.1 protocol.
</snip>

regards
dirkp"
0,"Put everything in jackrabbit-spi-commons under org.apache.jackrabbit.spi.commonsTo avoid confusion and naming conflicts, we should put all classes and packages in jackrabbit-spi-commons under org.apache.jackrabbit.spi.commons."
0,"request.abort() should interrupt thread waiting for a connectionCalls to HttpRequestBase.abort() will not unblock a thread that is still waiting for a connection and therefore has no ConnectionReleaseTrigger yet.
"
0,"SSL contrib files do not use standard javax.net.ssl package provided from JDK 1.4.2Hi all,

While trying to use ssl on AIX, i found that some of the files contributed in 
src/contrib/org/apache/commons/httpclient/contrib/ssl were making hard 
references to com.sun.net.ssl package. Since JDK 1.4.2, one shall use the 
javax.net.ssl package instead.

I have then:
1/ fixed the source files appropriately
2/ updated the build.xml to also build a commons-http-client-contrib.jar 

I will attached to this bug report the resulting unified diff to include in svn"
0,"IndexingAggregateTest#testNtFileAggregate fails occasionallyIt may happen that the text extraction from a plain/text resource times out due to the tough extractor time out set on the indexing-test workspace.

The test should check if the indexing queue is empty before it executes a query."
0,Refactor RewriteMethods out of MultiTermQueryPoliceman work :-) - as usual
0,"Deprecate / Remove DutchAnalyzer.setStemDictionaryDutchAnalyzer.setStemDictionary(File) prevents reuse of TokenStreams (and also uses a File which isn't ideal).  It should be deprecated in 3x, removed in trunk."
0,"Support for OpenOffice text extractionHi, here is the patch.

>hi nicolas,
>
>thanks for your offer to contribute your openoffice textfilter, that's greatly appreciated!
>
>i suggest you post a jira 'Improvement' or ""New Feature' issue and attach your code as an svn patch. somebody will take care 
>of it (i assume marcel), i.e. review your contribution and provide feedback/further instructions.

>cheers
>stefan

>On 2/2/06, Nicolas Jouanin <nicolas.jouanin@gmail.com> wrote:
>>
>>
>>
>> Hi Stefan,
>>
>>
>>
>> I work with Martin Perez, main developper of jLibrary.
>>
>> Therefore, I developed a class which extracts metadata and text 
>> content from any openoffice file (that was not the hardest job). This 
>> class is already used into jLibrary.
>>
>> As Martin suggested me, I used this class to create a new TextFilter 
>> subclass into textfilters contrib project. I downloaded textfilters 
>> project from svn, created my class into the project tree and tested it 
>> with a test class , just like it was done with the other extractors.
>>
>> I can send you the code if you want to review it, or just tell me how 
>> I can commit it.
>>
>>
>>
>> Regards,
>>
>>
>>
>> Nicolas.
"
0,"Move 'good' contrib/queries classes to Queries moduleWith the Queries module now filled with the FunctionQuery stuff, we should look at closing down contrib/queries.  While not a huge contrib, it contains a number of pretty useful classes and some that should go elsewhere.

Heres my proposed plan:

- similar.* -> suggest module
- regex.* -> queries module
- BooleanFilter -> queries module under .filters package
- BoostingQuery -> queries module
- ChainedFilter -> queries module under .filters package
- DuplicateFilter -> queries module under .filters package
- FieldCacheRewriteMethod -> This doesn't belong in this contrib or the queries module.  I think we should push it to contrib/misc for the time being.  It seems to have quite a few constraints on when its useful.  If indeed CONSTANT_SCORE_AUTO rewrite is better, then I dont see a purpose for it.
- FilterClause -> class inside BooleanFilter
- FuzzyLikeThisQuery -> suggest module. This class seems a mess with its Similarity hardcoded.  With all that said, it does seem to do what it claims and with some cleanup, it could be good.
- TermsFilter -> queries module under .filters package
- SlowCollated* -> They can stay in the module till we have a better place to nuke them.

One of the implications of the above moves, is that the xml-query-parser, which supports many of the queries, will need to have a dependency on the queries module.  But that seems unavoidable at this stage.



"
0,create configuration on InputStreamRepositoryConfig should be possible to create based on InputStreams (in case of URLs) ; right now it's possible only using String and InputSource. Please update also the JCA connector. 
0,"Move common implementations of SPI interfaces to spi-commons moduleSome of the spi modules use nearly duplicate code, which should be moved to the spi-commons module."
0,"Enable access to the freq information in a Query's sub-scorersThe ability to gather more details than just the score, of how a given
doc matches the current query, has come up a number of times on the
user's lists.  (most recently in the thread ""Query Match Count"" by
Ryan McV on java-user).

EG if you have a simple TermQuery ""foo"", on each hit you'd like to
know how many times ""foo"" occurred in that doc; or a BooleanQuery +foo
+bar, being able to separately see the freq of foo and bar for the
current hit.

Lucene doesn't make this possible today, which is a shame because
Lucene in fact does compute exactly this information; it's just not
accessible from the Collector.
"
0,"Better MimeType HandlingAfter saving a Excel File through WebDAV the mimetype will be changed.
The mimetype for a Win2000 Exel File is application/vnd.ms-excel. This will be changed to application/msexcel.
Also problems makes the new office 07 format (docx,xlsx,pptx). They will also be changed to application/octet-stream (default mimetype).
We have a lot of file types that we store in jackrabbit that are not in the properties file (MSInfoPath-, OutlookMsg-, MsAccess-Files, ...)
I think it will be better to let the mimetype property untouched if a mimetype is present so we must not put all the possible mimetypes in the property file.

BR
claus"
0,"TokenFilter should implement reset()TokenFilter maintains a private member of TokenStream.
It should implement reset() and call its member TokenStream's reset() method. Otherwise, that TokenStream never gets reset.
Patch applied."
0,"wrong exception from NativeFSLockFactory (LIA2 test case)As part of integrating Lucene In Action 2 test cases (LUCENE-2661), I found one of the test cases fail

the test is pretty simple, and passes on 3.0. The exception you get instead (LockReleaseFailedException) is 
pretty confusing and I think we should fix it.
"
0,"Create resource sensitive cache for item statesthere is currently a lru-caching strategy for the itemstates in the shared ism, with a hardcoded limit of 1000 entries. the problem is that the size of the states is not respected in the caching strategy; this poses a problem, if the
states are large (i.e. large values in property states, or large number of childnode entries)."
0,"Add a ContextAwareAuthScheme that has access to the HttpContext in the authenticate methodThe interface to be added would be:

/**
 * This interface represents an extended  authentication scheme
 * that requires access to {@link HttpContext} in order to
 * generate an authorization string.
 *
 * @since 4.1
 */

public interface ContextAwareAuthScheme extends AuthScheme {

    /**
     * Produces an authorization string for the given set of
     * {@link Credentials}.
     *
     * @param credentials The set of credentials to be used for athentication
     * @param request The request being authenticated
     * @param context HTTP context
     * @throws AuthenticationException if authorization string cannot
     *   be generated due to an authentication failure
     *
     * @return the authorization string
     */
    Header authenticate(
            Credentials credentials,
            HttpRequest request,
            HttpContext context) throws AuthenticationException;

}

Binary compatibility can be maintained by doing an instanceof check at the location where AuthScheme.authenticate() is called at the moment, and calling the context aware version if available.

This interface is necessary for the NegotiateScheme authentication scheme because the service names for the authentication tickets are based on the hostname of the target host or proxy host, depending on whether it's normal or proxy authentication, and this information is only available from the HttpContext.

Without the HttpContext there is a workaround that works most of the time, which looks like this:

	String host;
	if (isProxy()) {
		// FIXME this should actually taken from the HttpContext.
		HttpHost proxy = ConnRouteParams.getDefaultProxy(request.getParams());
		host = proxy.getHostName();
	} else {
		host = request.getLastHeader(""Host"").getValue();
	}

"
0,"when a test Assume fails, display informationCurrently if a test uses Assume.assumeTrue, it silently passes.

I think we should output something, at *least* if you have VERBOSE set, maybe always.

Here's an example of what the output might look like:
{noformat}
junit-sequential:
    [junit] Testsuite: org.apache.solr.servlet.SolrRequestParserTest
    [junit] Tests run: 4, Failures: 0, Errors: 0, Time elapsed: 1.582 sec
    [junit]
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: testStreamURL Assume failed (ignored):
    [junit] org.junit.internal.AssumptionViolatedException: got: <java.io.FileNotFoundException: http://www.apdfgdfgache
.org/dist/lucene/solr/>, expected: null
    [junit]     at org.junit.Assume.assumeThat(Assume.java:70)
    [junit]     at org.junit.Assume.assumeNoException(Assume.java:92)
    [junit]     at org.apache.solr.servlet.SolrRequestParserTest.testStreamURL(SolrRequestParserTest.java:123)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)
    [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit]     at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:802)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:775)
    [junit]     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    [junit]     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    [junit]     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:768)
    [junit] Caused by: java.io.FileNotFoundException: http://www.apdfgdfgache.org/dist/lucene/solr/
    [junit]     at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1311)
    [junit]     at org.apache.solr.servlet.SolrRequestParserTest.testStreamURL(SolrRequestParserTest.java:120)
    [junit]     ... 26 more
    [junit] ------------- ---------------- ---------------
{noformat}"
0,"Remove SortField.AUTOI'd like to remove SortField.AUTO... it's dangerous for Lucene to
guess the type of your field, based on the first term it encounters.
It can easily be wrong, and, whether it's wrong or right could
suddenly change as you index different documents.

It unexepctedly binds SortField to needing an IndexReader to do the
guessing.

It's caused various problems in the past (most recently, for me on
LUCENE-1656) as we fix other issues/make improvements.

I'd prefer that users of Lucene's field sort be explicit about the
type that Lucene should cast the field to.  Someday, if we have
optional strong[er] typing of Lucene's fields, such type information
would already be known.  But in the meantime, I think users should be
explicit.
"
0,"Add next() and skipTo() variants to DocIdSetIterator that return the current doc, instead of booleanSee http://www.nabble.com/Another-possible-optimization---now-in-DocIdSetIterator-p23223319.html for the full discussion. The basic idea is to add variants to those two methods that return the current doc they are at, to save successive calls to doc(). If there are no more docs, return -1. A summary of what was discussed so far:
# Deprecate those two methods.
# Add nextDoc() and skipToDoc(int) that return doc, with default impl in DISI (calls next() and skipTo() respectively, and will be changed to abstract in 3.0).
#* I actually would like to propose an alternative to the names: advance() and advance(int) - the first advances by one, the second advances to target.
# Wherever these are used, do something like '(doc = advance()) >= 0' instead of comparing to -1 for improved performance.

I will post a patch shortly"
0,"Extend contrib Highlighter to properly support PhraseQuery, SpanQuery,  ConstantScoreRangeQueryThis patch adds a new Scorer class (SpanQueryScorer) to the Highlighter package that scores just like QueryScorer, but scores a 0 for Terms that did not cause the Query hit. This gives 'actual' hit highlighting for the range of SpanQuerys, PhraseQuery, and  ConstantScoreRangeQuery. New Query types are easy to add. There is also a new Fragmenter that attempts to fragment without breaking up Spans.

See http://issues.apache.org/jira/browse/LUCENE-403 for some background.

There is a dependency on MemoryIndex."
0,"CoordConstrainedBooleanQuery + QueryParser supportAttached 2 new classes:

1) CoordConstrainedBooleanQuery
A boolean query that only matches if a specified number of the contained clauses
match. An example use might be a query that returns a list of books where ANY 2
people from a list of people were co-authors, eg:
""Lucene In Action"" would match (""Erik Hatcher"" ""Otis Gospodneti&#263;"" ""Mark Harwood""
""Doug Cutting"") with a minRequiredOverlap of 2 because Otis and Erik wrote that.
The book ""Java Development with Ant"" would not match because only 1 element in
the list (Erik) was selected.

2) CustomQueryParserExample
A customised QueryParser that allows definition of
CoordConstrainedBooleanQueries. The solution (mis)uses fieldnames to pass
parameters to the custom query."
0,"JcrRemotingServlet should interpolate system properties in the home init-paramFor deployment scenarios where the same Jackrabbit WAR file is deployed multiple times on the same server with the same current working directory, it is useful to have the home init-param support system property interpolation."
0,"Jcr-Server: Avoid xml parsing if request body is missingOriginally reported by Brian.

"
0,"Suggestion regarding NodeImpl and PropertyImpl in jackrabbit.coreBoth NodeImpl and PropertyImpl contain in their respective setProperty/setValue (respectively) initial validation checks, that is indentical across the various
variants of each and could possibly in either case be place in a separate method.

in NodeImpl.setProperty (also: addMixin, removeMixin, orderBefore):

- sanityCheck
- is-parent-checked-out
- is-not-protected (missing for setProperty ???)
- is-parent-not-locked

in PropertyImpl.setValue:

- sanityCheck
- is-parent-checked-out
- is-not-protected 
- is-value-compatible-with-multivalue-definition (NOTE: check opposite for setting single
  or multiple values)
- is-parent-not-locked


Second, i'm never sure, in which case ChildNode, ChildProperty is prefered 
over Node/Property (as present in the jcr api)...

regards
angela

"
0,"Document SINGLE_COOKIE_HEADER param in the cookie guideIncluded is some sample code that shows the behaviour when loading pages from a phpBB powered 
site. Here are the results as i see them on my machine:

==== start results

==================================
Policy: rfc2109
==================================


        URL: http://www.sgboards.com/forums/viewtopic.php?t=12&view=next&mforum=str
        Response status code: 200
        Present cookies: 
                ForumSetCookie=str
                phpbb_str_data=a%3A0%3A%7B%7D
                phpbb_str_sid=c8da590cc4b1683b9079da3d82f4efa6

        URL: http://www.sgboards.com/forums/viewtopic.php?p=24&mforum=str
        Response status code: 200
        Present cookies: 
                phpbb_str_data=a%3A0%3A%7B%7D
                phpbb_str_sid=c8da590cc4b1683b9079da3d82f4efa6
                ForumSetCookie=str

        URL: http://www.sgboards.com/forums/posting.php?mode=quote&p=24&mforum=str
        Response status code: 200
        Present cookies: 
                phpbb_str_data=a%3A0%3A%7B%7D
                phpbb_str_sid=c8da590cc4b1683b9079da3d82f4efa6
                ForumSetCookie=str

        URL: http://www.sgboards.com/forums/viewtopic.php?p=25&mforum=str
        Response status code: 200
        Present cookies: 
                phpbb_str_data=a%3A0%3A%7B%7D
                phpbb_str_sid=c8da590cc4b1683b9079da3d82f4efa6
                ForumSetCookie=str

==================================
Policy: netscape
==================================


        URL: http://www.sgboards.com/forums/viewtopic.php?t=12&view=next&mforum=str
        Response status code: 200
        Present cookies: 
                phpbb_str_sid=e2604334a0022283333153f6879feb70

        URL: http://www.sgboards.com/forums/viewtopic.php?p=24&mforum=str
        Response status code: 200
        Present cookies: 
                phpbb_str_sid=e2604334a0022283333153f6879feb70

        URL: http://www.sgboards.com/forums/posting.php?mode=quote&p=24&mforum=str
        Response status code: 200
        Present cookies: 
                phpbb_str_sid=e2604334a0022283333153f6879feb70

        URL: http://www.sgboards.com/forums/viewtopic.php?p=25&mforum=str
        Response status code: 200
        Present cookies: 
                phpbb_str_sid=e2604334a0022283333153f6879feb70

==================================
Policy: compatibility
==================================


        URL: http://www.sgboards.com/forums/viewtopic.php?t=12&view=next&mforum=str
        Response status code: 200
        Present cookies: 
                ForumSetCookie=str
                phpbb_str_data=a%3A0%3A%7B%7D
                phpbb_str_sid=d156f6dbfa605320b5a250129fa0b22e

        URL: http://www.sgboards.com/forums/viewtopic.php?p=24&mforum=str
        Response status code: 200
        Present cookies: 
                ForumSetCookie=str
                phpbb_str_data=a%3A0%3A%7B%7D
                phpbb_str_sid=d5d5a46fd27fd783cdb4e324992bc9d2

        URL: http://www.sgboards.com/forums/posting.php?mode=quote&p=24&mforum=str
        Response status code: 200
        Present cookies: 
                phpbb_str_data=a%3A0%3A%7B%7D
                phpbb_str_sid=b4312fee4250f767cd1b34b11afadb3d
                ForumSetCookie=str

        URL: http://www.sgboards.com/forums/viewtopic.php?p=25&mforum=str
        Response status code: 200
        Present cookies: 
                ForumSetCookie=str
                phpbb_str_data=a%3A0%3A%7B%7D
                phpbb_str_sid=daf72685d35d851c3eec68b6b3bc3705

==== end results

As you can see the only cookie policy that ISN'T successfully tracking sessions is the COMPATIBILITY 
setting. There are a lot of these phpBB sites around, so that's where I've noticed the behaviour most. 
Trying another random php powered site I see that all policies work as expected.

It would be nice to know what's messing up the cookie handing on these phpBB sites. If you can't rely 
on the compatibility setting to reliably maintain session variables (and hence truly imitate a browser) 
then life get's a little complicated.

Both 3.0beta1 and the CVS version show the same behaviour.

Many thanks,

Garry

Example code below.

====== begin code
import org.apache.commons.httpclient.Cookie;
import org.apache.commons.httpclient.HttpClient;
import org.apache.commons.httpclient.HttpState;
import org.apache.commons.httpclient.cookie.CookiePolicy;
import org.apache.commons.httpclient.methods.GetMethod;


public class CookieProbe {
	static final String[] urls = {
		""http://www.sgboards.com/forums/viewtopic.php?t=12&view=next&mforum=str"",
		""http://www.sgboards.com/forums/viewtopic.php?p=24&mforum=str"",
		""http://www.sgboards.com/forums/posting.php?mode=quote&p=24&mforum=str"",
		""http://www.sgboards.com/forums/viewtopic.php?p=25&mforum=str""
	};
	static final String[] urls2 = {
		""http://www.virginmobilelouder.com/live/index.php"",
		""http://www.virginmobilelouder.com/live/index.php?page_id=214"",
		""http://www.virginmobilelouder.com/live/index.php?page_id=3"",
		""http://www.virginmobilelouder.com/live/index.php?page_id=116""
	};
	
	static final String[] policies = {
		CookiePolicy.RFC_2109, 
		CookiePolicy.NETSCAPE, 
		CookiePolicy.BROWSER_COMPATIBILITY, 
	};
	
	
	public static void main(String[] args) {
		try {
			for (int i = 0; i < policies.length; i++) {
				System.out.println(""\n=================================="");
				System.out.println(""Policy: "" + policies[i]);
				System.out.println(""==================================\n"");
				tryPolicy(policies[i]);
			}
		} catch (Exception e) {
			e.printStackTrace(System.err);
		}
	}
	
	public static void tryPolicy(String policy) throws Exception {
		HttpState initialState = new HttpState();
		HttpClient httpclient = new HttpClient();
		httpclient.getHttpConnectionManager().
			getParams().setConnectionTimeout(30000);
		httpclient.setState(initialState);
		
		httpclient.getParams().setCookiePolicy(policy);
		for (int i = 0; i < urls.length; i++) {
			System.out.println(""\n\tURL: "" + urls[i]);
			tryURL(httpclient, urls[i]);
			Thread.sleep(1000); // give server a break
		}
			
	}

	public static void tryURL(HttpClient httpclient, String strURL) throws Exception {
		GetMethod httpget = new GetMethod(strURL);
		int result = httpclient.executeMethod(httpget);
		System.out.println(""\tResponse status code: "" + result);
		// Get all the cookies
		Cookie[] cookies = httpclient.getState().getCookies();
		System.out.println(""\tPresent cookies: "");
		for (int i = 0; i < cookies.length; i++) {
			System.out.println(""\t\t"" + cookies[i].toExternalForm());
		}
		// Release current connection to the connection pool once you are done
		httpget.releaseConnection();
	}
}
====== end code"
0,Best effort merge if concurrent modifications include changes to mixin typescurrently the NodeStateMerger#merge method immediately aborts if the mixin types of the 2 nodes are not the same.
0,"Add simple query method to ObjectContentManagerAs discussed in [1], I suggest a new method 

    ObejctContentManager.getObjectIterator(String query, String language)

to easily query the repository for objects using a predefined query. (I chose getObjectIterator instead of getObjects as I intend the method to return an Iterator and not a Collection)

[1] http://www.mail-archive.com/dev%40jackrabbit.apache.org/msg07475.html"
0,"Allow usage of HyphenationCompoundWordTokenFilter without dictionaryWe should allow to use the HyphenationCompoundWordTokenFilter without a dictionary. This produces a lot of ""nonword"" tokens but might be useful sometimes."
0,"Make BooleanWeight and DisjunctionMaxWeight protectedCurrently, BooleanWeight is private, yet it has 2 protected members (similarity, weights) which are unaccessible from custom code

i have some use cases where it would be very useful to crawl a BooleanWeight to get at the sub Weight objects

however, since BooleanWeight is private, i have no way of doing this

If BooleanWeight is protected, then i can subclass BooleanQuery to hook in and wrap BooleanWeight with a subclass to facilitate this walking of the Weight objects

Would also want DisjunctionMaxWeight to be protected, along with its ""weights"" member

Would be even better if these Weights were made public with accessors to their sub ""weights"" objects (then no subclassing would be necessary on my part)

this should be really trivial and would be great if it can get into 2.9

more generally, it would be nice if all Weight classes were public with nice accessors to relevant ""sub weights""/etc so custom code can get its hooks in where and when desired"
0,"JackrabbitParser and tika 0.7 parserHi,

I was trying to implement a custom parser and found the following problem.
Since tika 0.7 it is possible to implement your custom parser and specify it into a service provider configuration file (META-INF/services/org.apache.tika.parser.Parser). In this way there would be no need to maintain a custom tika-config.xml file if you'd like to implement a custom parser.

The problem that I had was in the JackrabbitParser because I wasn't able to instantiate the AutoDetectParser with the default constructor is will be instantiated using the default TikaConfig constructor.
Basically from tika 0.7, the TikaConfig.getTikaConfig() is instantiating the TikaConfig using the default constructor instead of accessing the tika-config.xml file from withing the package, and reads the service provider configuration files and populate the parsers map.

What I'm proposing is to change the JackrabbitParser to instantiate the AutoDetectParser using the default constructor, in this way the using tika version >= 0.7 we could easily implement our own parsers and there won't be a reason to maintain the tika-config.xml, also a sort of ""backward"" compatibility would be maintained because using the AutoDetectParser default constructor the TikaConfig is instantiated using TikaConfig.getTikaConfig() wich for tika versions < 0.7 calls the TikaConfig(InputStream) constructor whcih reads the configuration directly from the package.

Basically the JackrabbitParser should look like this:

    public JackrabbitParser() {
            	parser = new AutoDetectParser();
    }
 
Thanks,
Dan"
0,"Not getting random-seed/reproduce-with if a test fails from another threadSee https://builds.apache.org/job/Lucene-Solr-tests-only-trunk/12822/console as an example.

This is at least affecting 4.0, maybe 3.x too"
0,"Kerberos Authentication SchemeHttpClient 4.1.2 has a SPNEGO authentication that uses the Negotiate keyword.  But the MS IIS that I must connect to does not send back an WWW-Authenticate: Negotiate, but, instead, does send an WWW-Authenticate: Kerberos

So I used the NegotiateScheme.java and NegotiateSchemeFactory.java as a base to create a ""new"" scheme, called, KerberosScheme.java and KerberosSchemeFactory.java to make it work.

Essentially I replaced every ""Negotiate"" scheme by ""Kerberos"", in the KerberosScheme.java, and removed the part of the code that tried, first, SPNEGO_OID, using KERBEROS_OID directly, instead.

It works fine for me, but took me a while to figure this out.

That why I think it could come on the new versions.

I'll attach my version but it has no package - it was made only for a test project.  It's trivial to put it in the right place/package.
"
0,Add missing license headersThe RAT tool (http://code.google.com/p/arat/) points out a few files within Jackrabbit trunk that are currently missing the correct license header. We should fix those.
0,"[PATCH] Remove Stutter in ItemValidatorItemValidator duplicates code for no reason. Remove the duplication

        if (permissions > Permission.NONE) {
            Path path = item.getPrimaryPath();
            if (!accessMgr.isGranted(item.getPrimaryPath(), permissions)) {
                return false;
            }

to

        if (permissions > Permission.NONE) {
            Path path = item.getPrimaryPath();
            if (!accessMgr.isGranted(path, permissions)) {
                return false;
            }

"
0,"Change defaults in IndexWriter to maximize ""out of the box"" performanceThis is follow-through from LUCENE-845, LUCENE-847 and LUCENE-870;
I'll commit this once those three are committed.

Out of the box performance of IndexWriter is maximized when flushing
by RAM instead of a fixed document count (the default today) because
documents can vary greatly in size.

Likewise, merging performance should be faster when merging by net
segment size since, to minimize the net IO cost of merging segments
over time, you want to merge segments of equal byte size.

Finally, ConcurrentMergeScheduler improves indexing speed
substantially (25% in a simple initial test in LUCENE-870) because it
runs the merges in the backround and doesn't block
add/update/deleteDocument calls.  Most machines have concurrency
between CPU and IO and so it makes sense to default to this
MergeScheduler.

Note that these changes will break users of ParallelReader because the
parallel indices will no longer have matching docIDs.  Such users need
to switch IndexWriter back to flushing by doc count, and switch the
MergePolicy back to LogDocMergePolicy.  It's likely also necessary to
switch the MergeScheduler back to SerialMergeScheduler to ensure
deterministic docID assignment.

I think the combination of these three default changes, plus other
performance improvements for indexing (LUCENE-966, LUCENE-843,
LUCENE-963, LUCENE-969, LUCENE-871, etc.) should make for some sizable
performance gains Lucene 2.3!"
0,"Journal: Use buffered input / output streamsThe journal should use buffered input / output streams wherever possible. Currently there are some places where bytes are directly written to the journal file, which degrades performance."
0,"Re-index fails on corrupt bundleThe re-indexing process should be more resilient, log an error and simply continue with the next node. It doesn't seem useful to refuse repository startup in this case."
0,CanAddChildNodeCallWithNodeTypeTest.testDefinedAndLegalType() may fail if protected child node definition is pickedIf the utility NodeTypeUtil.locateChildNodeDef() picks a protected child node definition the test case will fail because it is not allowed add a protected child node.
0,"need tests to guarantee transparency of caching module on end-to-end headers""A transparent proxy SHOULD NOT modify an end-to-end header unless the definition of that header requires or specifically allows that.""

http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.5.2

This is already true of our implementation, but we should have tests to preserve that behavior.
"
0,"test case (TCK) maintenance for JCR 2.0Umbrella issue for changes/additions to JUnit test cases, setup and config."
0,"New Analysis  ContributionsWith the advent of the new TeeTokenFilter and SinkTokenizer, there now exists some interesting new things that can be done in the analysis phase of indexing.  See LUCENE-1058.

This patch provides some new implementations of SinkTokenizer that may be useful."
0,"Hindi AnalyzerAn analyzer for hindi.

below are MAP values on the FIRE 2008 test collection.
QE means expansion with morelikethis, all defaults, on top 5 docs.

||setup||T||T(QE)||TD||TD(QE)||TDN||TDN(QE)||
|words only|0.1646|0.1979|0.2241|0.2513|0.2468|0.2735|
|HindiAnalyzer|0.2875|0.3071|0.3387|*0.3791**|0.3837|0.3810|
|improvement|74.67%|55.18%|51.14%|50.86%|55.47%|39.31%|

* TD was the official measurement, highest score for this collection in FIRE 2008 was 0.3487: http://www.isical.ac.in/~fire/paper/mcnamee-jhu-fire2008.pdf

needs a bit of cleanup and more tests"
0,"Source release files missing the *.pom.template filesThe source release files should contain the *.pom.template files, otherwise it is not possible to build the maven artifacts using ""ant generate-maven-artifacts"" from official release files."
0,Do not log warning when coercing value in query is not possibleThe LuceneQueryBuilder currently logs a warning when a String literal cannot be coerced into a type derived from information provided by the node type manager. The log level should be lowered to debug.
0,"Change contrib QP API that uses CharSequence as string identifierThere are some API methods on contrib queryparser that expects CharSequence as identifier. This is wrong, since it may lead to incorrect or mislead behavior, as shown on LUCENE-2855. To avoid this problem, these APIs will be changed and enforce the use of String instead of CharSequence on version 4. This patch already deprecate the old API methods and add new substitute methods that uses only String."
0,"Performance of AC Evaluation1. Performance in access control evaluation
=====================================================================

 - main focus on
   > read performance
   > resource-based access control in .a.j.c/s/authorization/acl/*

 - comparison admin vs. anonymous with full permissions
 - comparision between shortcut and ACL-evaluation.
 - comparison JR 1.4 vs JR 2.0 [actually i will compare Day's CRX as it already provided
   some custom AC stuff with JR 1.4]


2. Potential Problems
=====================================================================

   I would expect the most significant problems to be found in

a) ACLProvider#retrieveResultEntries: calculating effective ACEs
     for each session separately.

b) AclPermission:
     Each instance registering an event listener in order to
     keep the result cache up to date

c) AclPermission:
     Resolution form Path to Item or to nearest existing Item "
0,"Make Token.DEFAULT_TYPE publicMake Token.DEFAULT_TYPE public so that TokenFilters using the reusable Token model have a way of setting the type back to the default.

No patch necessary.  I will commit soon."
0,Release the OCM componentThe contrib/jackrabbit-jcr-mapping/jcr-mapping should be promoted from contrib into a jackrabbit-jcr-ocm component.
0,"[PATCH] Remove Stutter in NodeStateCode duplicates code for no reason

Index: src/main/java/org/apache/jackrabbit/jcr2spi/state/NodeState.java
===================================================================
--- src/main/java/org/apache/jackrabbit/jcr2spi/state/NodeState.java	(revision 740824)
+++ src/main/java/org/apache/jackrabbit/jcr2spi/state/NodeState.java	(working copy)
@@ -449,7 +449,7 @@
              */
             NodeState parent = getParent();
             NodeId wspId = (NodeId) getWorkspaceId();
-            def = definitionProvider.getQNodeDefinition(getParent().getNodeTypeNames(), getName(), getNodeTypeName(), wspId);
+            def = definitionProvider.getQNodeDefinition(parent.getNodeTypeNames(), getName(), getNodeTypeName(), wspId);
         }
         return def;
     }
"
0,Make CacheEntry use an immutable object to represent cache content Make CacheEntry use an immutable object to represent cache content similar to HttpEntity
0,"TestNLS fails with ja localeset ANT_ARGS=""-Dargs=-Duser.language=ja -Duser.country=JP""
ant test-core -Dtestcase=TestNLS

The test has 2 sets of message, one fallback, and one ja.
The tests assume if it asks for a non-ja locale, that it will get the fallback message,
but this is not how ResourceBundle.getBundle works:
{noformat}
Otherwise, the following sequence is generated from the attribute values of the specified locale 
(language1, country1, and variant1) and of the default locale (language2, country2, and variant2):

baseName + ""_"" + language1 + ""_"" + country1 + ""_"" + variant1
baseName + ""_"" + language1 + ""_"" + country1
baseName + ""_"" + language1
baseName + ""_"" + language2 + ""_"" + country2 + ""_"" + variant2
baseName + ""_"" + language2 + ""_"" + country2
baseName + ""_"" + language2
baseName
{noformat}

So in the case of ja default locale, you get a japanese message instead from the baseName + ""_"" + language2 match"
0,"refactoring of DavSession acquisition in jcr-serveri'm subclassing WebdavServer, and i want to use my own logic for finding credentials in the request, logging into the repository and instantiating a DavSession.

unfortunately, WebdavServlet.getSession and its friend the inner class DavSessionImpl are declared private. i changed WebdavServlet.getSession to be protected so that i could override it, but even so, i have no access to DavSessionImpl, so for now, i've copied and pasted it as an inner class in my subclass. yuck.

here's a proposal for making this more extensible:

1) create the interface DavSessionProvider in org.apache.jackrabbit.server with these methods:

  public void acquireSession(WebdavRequest request) throws DavException;
  public void releaseSession(WebdavRequest request);

2) make JCRWebdavServer implement DavSessionProvider (it already includes the above methods)

3) move WebdavServlet$DavSessionImpl to DavSessionImpl in org.apache.jackrabbit.server.simple

4) create a DavSessionProviderImpl in org.apache.jackrabbit.server.simple implementing DavSessionProvider which returns instances of DavSessionImpl

5) change WebdavServlet to use a DavSessionProvider rather than its own getSession method, and use a DavSessionProviderImpl by default. subclasses can override with setDavSessionProvider().
"
0,"A handy utility class for tracking deprecated overridden methodsThis issue provides a new handy utility class that keeps track of overridden deprecated methods in non-final sub classes. This class can be used in new deprecations.

See the javadocs for an example."
0,Performance tuning
0,"Add optional state attribute to managed client connectionsProvide an optional state attribute to managed client connections. The connection state can represent a user identify in case of connection based authentication schemes such as NTLM or SSL, thus allowing for connection re-use on a per user identity basis."
0,"[PATCH] Field.toString could be more helpfulorg.apache.lucene.document.Field.toString defaults to using Object.toString
for some sensible fields. e.g. !isStored && isIndexed && !isTokenized
fields. This makes debugging slightly more difficult than is really needed.

Please find pasted below possible alternative:

 /** Prints a Field for human consumption. */
  public final String toString() {
  	StringBuffer result = new StringBuffer();
  	if (isStored) {
  		if (isIndexed) {
  			if (isTokenized) {
  				result.append(""Text"");
  			} else {
  				result.append(""Keyword"");
  			}
  		} else {
			// XXX warn on tokenized not indexed?
  			result.append(""Unindexed"");
  		}
  	} else {
  		if (isIndexed) {
  			if (isTokenized) {
  				result.append(""Unstored"");
  			} else {
  				result.append(""UnstoredUntokenized"");
  			}
  		} else {
			result.append(""Nonsense_UnstoredUnindexed"");
  		}
  	}
  	
  	result.append('<');
  	result.append(name);
  	result.append(':');
  	if (readerValue != null) {
  		result.append(readerValue.toString());
  	} else {
  		result.append(stringValue);
  	}
  	result.append('>');
  	return result.toString();
  }


NB Im working against CVS HEAD"
0,"allow cache to be configured as a non-shared (private) cacheCurrently the CachingHttpClient only behaves as a shared cache, which is a safe and conservative assumption. However, in some settings, it would be appropriate to be able to configure the CachingHttpClient as a non-shared cache, which would make more responses cacheable, including:
* responses to requests with Authorization headers
* responses with 'Cache-Control: private'
* ability to serve stale responses when invalidation fails for 'Cache-Control: proxy-revalidate'
"
0,"Remove redundant RepositoryService.executeQuery() method There are currently two executeQuery() methods on RepositoryService. For simplicity we should remove the one that assumes default values for limit, offset and bind variable values."
0,"Refactor the Mapper & DescriptotReader classesI would like to refactor the mappers and the descriptor readers  in order to : 
* Create an abstract mapper impl because both Mapper classes have a lot of code in common (AnnotedObjectMapper & DigesterMapperImpl). Only the readers are different. The Mappers can make exactly the same process. 
* The Mapper classes should not have the responsibility to create the jcr node types. This can be done outside the mapper and it should be an optional operation. There are certainly some use cases where node type creation is not necessary. Right now, the annotated object mapper creates jcr node types. "
0,"MultiThreadedConnectionManager should provide a shutdownMultiThreadedConnectionManager should provide a shutdown() method to release 
all its resources, it is currently using daemon threads that cannot be stopped 
and HTTP connections that cannot be released.
This is annoying when the pool of connection is created within a web 
application that is undeployed and re-deployed (i.e. the JVM is not restarted) 
consuming resources on local and remote servers."
0,"Add a constructor to org.apache.http.conn.ssl.SSLSocketFactory to allow for directly wrapping a javax.net.ssl.SSLSocketFactory socketfactoryOur application use Java Webstart for deployment.  Amoung other things, Webstart gives us the ability to access the system's (in our case, Windows) certificate system.  For instance, one of our client is using certificate based authentication to their webserver.  This is done through a hardware device they attach to their system.  Window's already has a way to interface with this device, and Webstart has a way to interface with the Windows API.

I don't think we can get by with using any SocketFactory that we create.  (We would have to check with Oracle to be sure.)  I think we need to use the one that is set as the default in HttpsURLConnection.

What I am suggesting is that another constructor be added to allow for just wrapping this one.  I was not planning on putting a dependancy on HttpsURLConnection, but rather just add the ability to wrap any javax.net.ssl.SSLSocketFactory.

This will not be a big change to the API.  I will get a patch ready soon."
0,"Deprecating InstantiatedIndexWriterhttp://markmail.org/message/j6ip266fpzuaibf7

I suppose that should have been suggested before 2.9 rather than  
after...

There are at least three reasons to why I want to do this:

The code is based on the behaviour or the Directory IndexWriter as of  
2.3 and I have not been touching it since then. If there will be  
changes in the future one will have to keep IIW in sync, something  
that's easy to forget.
There is no locking which will cause concurrent modification  
exceptions when accessing the index via searcher/reader while  
committing.
It use the old token stream API so it has to be upgraded in case it  
should stay.

The java- and package level docs have since it was committed been  
suggesting that one should consider using II as if it was immutable  
due to the locklessness. My suggestion is that we make it immutable  
for real.

Since II is ment for small corpora there is very little time lost by  
using the constructor that builts the index from an IndexReader. I.e.  
rather than using InstantiatedIndexWriter one would have to use a  
Directory and an IndexWriter and then pass an IndexReader to a new  
InstantiatedIndex.

Any objections?"
0,"introduce QValue.getCalendar()Introduce QValue.getCalendar() in order to avoid unnecessary conversions from/to string format.
"
0,"Remove SVN.exe and revision numbers from build.xml by svn-copy the backwards branch and linking snowball tests by svn:externalsAs we often need to update backwards tests together with trunk and always have to update the branch first, record rev no, and update build xml, I would simply like to do a svn copy/move of the backwards branch.

After a release, this is simply also done:
{code}
svn rm backwards
svn cp releasebranch backwards
{code}

By this we can simply commit in one pass, create patches in one pass.

The snowball tests are currently downloaded by svn.exe, too. These need a fixed version for checkout. I would like to change this to use svn:externals. Will provide patch, soon."
0,"SPI: improve description of locking methods on RepositoryServicein detail:

1) getLockInfo

- intended behavior if no lock is present?
- intended behavior if locking is not supported?

2) lock

- currently InvalidItemStateException is listed. i don't think this make too much sense.

3) refreshLock

- intended behavior if locking is not supported?

4) unlock

- currently InvalidItemStateException is listed. i don't think this make too much sense."
0,TestIndexwriterWithThreads#testCloseWithThreads hangs if a thread hit an exception before indexing its first documentin TestIndexwriterWithThreads#testCloseWithThreads we loop until all threads have indexed a single document but if one or more threads fail on before they index the first doc the test hangs forever. We should check if the thread is still alive unless it has indexed a document and fail if it already died.
0,[PATCH] add term index interval accessorsIt should be possible for folks to set the index interval used when writing indexes.
0,MultiFields not thread safeMultiFields looks like it has thread safety issues
0,"Realm from authentication challenge unavailableThere is currently no way to extract the authentication realm from HttpClient 
except to extract the authentication challenge header and parse it manually.

Either the realm needs to be available to the client or a method in 
Authenticator should extract the realm from a given authentication header.

The same problems occurs with determining which type of authentication is 
being used and what other options there are (basic, digest, NTLM, others)."
0,Lower log level for index updates from queueThe log level is currently at info and should be lowered to debug.
0,"Duplicate log of HTTP headerThe HTTP header line:

""HTTP/1.1 200 OK[\r][\n]"" 

is duplicated in the wire logs. Seems to be because the line is logged at:

HttpParser [line: 131] - readLine(InputStream, String)

and at:

HttpMethodBase [line: 1980] - readStatusLine(HttpState, HttpConnection)

It looks like the latter log should be removed?"
0,"UserManagement: membership cache default size too smallThe membership cache that has been introduced in JCR-2703 is making use of an LRUMap to cache group memberships (authorizable nodeId -> group nodeIds). In environments where users belong to more than 100 groups, the cache quickly becomes ineffective due to the default maximum size of the LRUMap.

Once the cache limit is hit, the rather expensive Node#getWeakReferences API calls resulting in search queries are executed again, leading to quite noticeable performance drops. Thus I'd suggest to either make the membership cache configurable or introduce some logic to let the cache grow dynamically as needed (still having some kind of hard limit to avoid memory issues)."
0,"Participation of a workspace in a cluster should be configurableCurrently, when clustering is enabled, every workspace participates automatically. This should be configurable in the workspace, by introducing an attribute such as ""clustered=[true|false]""."
0,"Convert PrecedenceQueryParser to new TokenStream APIAdriano Crestani provided a patch, that updates the PQP to use the new TokenStream API...all tests still pass. 
I hope this helps to keep the PQP 
"
0,"BrowserCompatHostnameVerifier and StrictHostnameVerifier should handle wildcards in SSL certificates betterI ran into a problem with SSL wildcard certificates in the class BrowserCompatHostnameVerifier. It handles ""*.example.org"" fine but ""server*.example.org"" fails to work correctly. The javadoc claims that it should behave the same way as curl and FireFox. In Firefox an SSL certificate for ""server*.example.org"" works fine for the host ""server.example.org"", using HttpClient it throws an exception.

Here is an example test (JUnit4):

package org.example.hb;

import javax.net.ssl.SSLException;

import org.apache.http.conn.ssl.BrowserCompatHostnameVerifier;
import org.junit.Test;

public class BrowserCompatHostnameVerifierTest {

	/**
	 * Should not throw an exeption in the verify method.
	 * @throws SSLException
	 */
	@Test
	public void testVerifyStringStringArrayStringArray() throws SSLException
	{
		BrowserCompatHostnameVerifier hv = new BrowserCompatHostnameVerifier();
		String host = ""www.example.org"";
		String[] cns = {""www*.example.org""};
		
		hv.verify(host, cns, cns);
	}

}"
0,"XMLReader logs fatal error to system outSome test cases check if an appropriate exception is thrown when invalid XML is supplied, in that case the build in XMLReader in Java 1.5 logs a fatal error to system out.

This seems to be caused by a missing error handler on the XMLReader."
0,"Index File Format - Example for frequency file .frq is wrongReported by Johan Stuyts - http://www.nabble.com/Possible-documentation-error--p7012445.html - 

Frequency file example says: 

     For example, the TermFreqs for a term which occurs once in document seven and three times in document eleven would be the following sequence of VInts: 
         15, 22, 3 

It should be: 

     For example, the TermFreqs for a term which occurs once in document seven and three times in document eleven would be the following sequence of VInts: 
         15, 8, 3 


"
0,"Small speedups to DocumentsWriter's quickSortIn working on LUCENE-510 I found that DocumentsWriter's quickSort can
be further optimized to handle the common case of sorting only 2
values.

I ran with this alg:

  analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
  
  doc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker
  
  docs.file=/Volumes/External/lucene/wiki.txt
  doc.stored = true
  doc.term.vector = true
  doc.add.log.step=2000
  doc.maker.forever = false
  
  directory=FSDirectory
  autocommit=false
  compound=false
  
  ram.flush.mb=64
  
  { ""Rounds""
    ResetSystemErase
    { ""BuildIndex""
      CreateIndex
      { ""AddDocs"" AddDoc > : 200000
      - CloseIndex
    }
    NewRound
  } : 5
  
  RepSumByPrefRound BuildIndex

Best of 5 was 857.3 docs/sec before the optimization and 881.6 after =
2.8% speedup, on a quad-core Mac Pro with 4-drive RAID 0 array.

The fix is trivial.  I will commit shortly.

"
0,"benchmark tests always fail on windows because directory cannot be removedThis seems to be a bug recently introduced. I have no idea what's wrong. Attached is a log file, reproduces everytime.

"
0,"benchmark pkg: specify trec_eval submission output from the command linethe QueryDriver for the trec benchmark currently requires 4 command line arguments.
the third argument is ignored (i typically populate this with ""bogus"")
Instead, allow the third argument to specify the submission.txt file for trec_eval.

while I am here, add a usage() documenting what the arguments to this driver program do."
0,"Unclosed threads in JackrabbitThe Tomcat integration test added in JCR-2831 shows the following warnings about Jackrabbit threads that remain in place even after the repository has been closed:

08-Dec-2010 12:14:58 org.apache.catalina.loader.WebappClassLoader clearReferencesThreads
SEVERE: The web application [] appears to have started a thread named [Timer-1] but has failed to stop it. This is very likely to create a memory leak.
08-Dec-2010 12:14:58 org.apache.catalina.loader.WebappClassLoader clearReferencesThreads
SEVERE: The web application [] appears to have started a thread named [DynamicPooledExecutor] but has failed to stop it. This is very likely to create a memory leak.
08-Dec-2010 12:14:58 org.apache.catalina.loader.WebappClassLoader clearReferencesThreads
SEVERE: The web application [] appears to have started a thread named [Timer-2] but has failed to stop it. This is very likely to create a memory leak.

It would be best to close all such background threads even if they are singleton daemon threads and thus unlikely to cause trouble when left unattended."
0,"TCK: ImpersonateTest#testImpersonate should allow LoginExceptionJSR-170 allows Session.impersonate to throw LoginException if the session lacks permission to impersonate another user.  Some repositories may not allow any session to impersonate another user, in which case this test would fail.

Proposal: catch and consume LoginException.

--- ImpersonateTest.java        (revision 422074)
+++ ImpersonateTest.java        (working copy)
@@ -17,11 +17,13 @@
 package org.apache.jackrabbit.test.api;
  
 import org.apache.jackrabbit.test.AbstractJCRTest;
+import org.apache.jackrabbit.test.NotExecutableException;
  
 import javax.jcr.Session;
 import javax.jcr.Credentials;
 import javax.jcr.NodeIterator;
 import javax.jcr.Node;
+import javax.jcr.LoginException;
 import java.security.AccessControlException;
  
 /**
@@ -40,7 +42,14 @@
      */
     public void testImpersonate() throws Exception {
         // impersonate to read-only user
-        Session session = superuser.impersonate(helper.getReadOnlyCredentials());
+        Session session = null;
+
+        try {
+            session = superuser.impersonate(helper.getReadOnlyCredentials());
+        }
+        catch (LoginException e) {
+          throw new NotExecutableException(""impersonate threw LoginException"");
+        }
  
         // get a path to test the permissions on
         String thePath = """";
"
0,"More Fine grained Permission FlagsIt would be fine to have one more Permission Flag on node add.
At the moment there are 3 flags. We need to know if a node will be updated or created.
This is not possible with the current implementation because on node add the permission flag 
AccessManager.WRITE will be used. This is a Problem in a  WebDav Scenario with Microsoft-Word because if i open a Node and 
try to save it i need write permissions on the parent node. this is ok. If a user trys to save the file with a other name
he can because the same PermissionFlag will be used.
Maybe there is a other solution for this problem ?
BR,
claus"
0,"Optimization for IndexWriter.addIndexes()One big performance problem with IndexWriter.addIndexes() is that it has to optimize the index both before and after adding the segments.  When you have a very large index, to which you are adding batches of small updates, these calls to optimize make using addIndexes() impossible.  It makes parallel updates very frustrating.

Here is an optimized function that helps out by calling mergeSegments only on the newly added documents.  It will try to avoid calling mergeSegments until the end, unless you're adding a lot of documents at once.

I also have an extensive unit test that verifies that this function works correctly if people are interested.  I gave it a different name because it has very different performance characteristics which can make querying take longer."
0,"move JDK collation to core, ICU collation to ICU contribAs mentioned on the list, I propose we move the JDK-based CollationKeyFilter/CollationKeyAnalyzer, currently located in contrib/collation into core for collation support (language-sensitive sorting)

These are not much code (the heavy duty stuff is already in core, IndexableBinaryString). 

And I would also like to move the ICUCollationKeyFilter/ICUCollationKeyAnalyzer (along with the jar file they depend on) also currently located in contrib/collation into a contrib/icu.

This way, we can start looking at integrating other functionality from ICU into a fully-fleshed out icu contrib.
"
0,"Uploade Lucene 2.1 to ibiblioPlease uploaded Lucene (specifically lucene-core) 2.1.0 to ibiblio. I see 2.0.0 but not 2.1.0.

Thanks!"
0,Fix Hits deprecation noticeJust needs to be committed to 2.9 branch since hits is now removed.
0,"MANIFEST.MF cleanup (main jar and luci customizations)there are several problems with the MANIFEST.MF file used in the core jar, and some inconsistencies in th luci jar:

Lucli's build.xml has an own ""jar"" target and does not use the jar target from common-build.xml. The result
is that the MANIFEST.MF file is not consistent and the META-INF dir does not contain LICENSE.TXT and NOTICE.TXT.

Is there a reason why lucli behaves different in this regard? If not I think we should fix this."
0,"[PATCH] XPathQueryBuilder reports misleading column numbers for faulty queriesXPathQueryBuilder returns an error string with the column offset where a parsing error occurred. Unfortunately this value is difficult to correlate to the users query string, as XPathQueryBuilder embellishes the query by doing

statement = ""for $v in "" + statement + "" return $v"";

This patch appends the modified statement to the query message so that the user can get the real position of the error."
0,"Specialize BooleanQuery if all clauses are TermQueriesDuring work on LUCENE-3319 I ran into issues with BooleanQuery compared to PhraseQuery in the exact case. If I disable scoring on PhraseQuery and bypass the position matching, essentially doing a conjunction match, ExactPhraseScorer beats plain boolean scorer by 40% which is a sizeable gain. I converted a ConjunctionScorer to use DocsEnum directly but still didn't get all the 40% from PhraseQuery. Yet, it turned out with further optimizations this gets very close to PhraseQuery. The biggest gain here came from converting the hand crafted loop in ConjunctionScorer#doNext to a for loop which seems to be less confusing to hotspot. In this particular case I think code specialization makes lots of sense since BQ with TQ is by far one of the most common queries.

I will upload a patch shortly"
0,"Revert subsequent token-node updates (tentatively introduced)i would like to revert this improvement has been tentatively introduced based on the following
thread on the dev list: http://www.mail-archive.com/dev@jackrabbit.apache.org/msg24437.html
as i am still concerned about undesired effects. in addition i still feel that this somehow
violates the basic contract."
0,"Stored-only fields automatically enable norms and tf when added to documentDuring updating my internal components to the new TrieAPI, I have seen the following:

I index a lot of numeric fields with trie encoding omitting norms and term frequency. This works great. Luke shows that both is omitted.

As I sometimes also want to have the components of the field stored and want to use the same field name for it. So I add additionally the field again to the document, but stored only (as the Field c'tor using a TokenStream cannot additionally store the field). As it is stored only, I thought, that I can left out explicit setting of omitNorms and omitTermFreqAndPositions. After adding the stored-only-without-omits field, Luke shows all fields with norms enabled. I am not sure, if the norms/tf were really added to the index, but Luke shows a value for the norms and FieldInfo has it enabled.

In my opinion, this is not intuitive, o.a.l.document.Field  should switch both omit* options on when storing fields only (and also disable other indexing-only options). Alternatively the internal FieldInfo.update(boolean isIndexed, boolean storeTermVector, boolean storePositionWithTermVector, boolean storeOffsetWithTermVector, boolean omitNorms, boolean storePayloads, boolean omitTermFreqAndPositions) should only change the omit* and other options, if the isIndexed parameter (not this.isIndexed) is also true, elsewhere leave it as it is.

In principle, when adding a stored-only field, any indexing-specific options should not be changed in FieldInfo. If the field was indexed with norms before, norms should stay enabled (but this would be the default as it is)."
0,"if the build fails to download JARs for contrib/db, just skip its testsEvery so often our nightly build fails because contrib/db is unable to download the necessary BDB JARs from http://downloads.osafoundation.org.  I think in such cases we should simply skip contrib/db's tests, if it's the nightly build that's running, since it's a false positive failure."
0,"Tests using Version.LUCENE_CURRENT will produce problems in backwards branch, when development for 3.2 startsA lot of tests for the most-recent functionality in Lucene use Version.LUCENE_CURRENT, which is fine in trunk, as we use the most recent version without hassle and changing this in later versions.

The problem is, if we copy these tests to backwards branch after 3.1 is out and then start to improve analyzers, we then will have the maintenance hell for backwards tests. And we loose backward compatibility testing for older versions. If we would specify a specific version like LUCENE_31 in our tests, after moving to backwards they must work without any changes!

To not always modify all tests after a new version comes out (e.g. after switching to 3.2 dev), I propose to do the following:
- declare a static final Version TEST_VERSION = Version.LUCENE_CURRENT (or better) Version.LUCENE_31 in LuceneTestCase(4J).
- change all tests that use Version.LUCENE_CURRENT using eclipse refactor to use this constant and remove unneeded import statements.

When we then move the tests to backward we must only change one line, depending on how we define this constant:
- If in trunk LuceneTestCase it's Version.LUCENE_CURRENT, we just change the backwards branch to use the version numer of the released thing.
- If trunk already uses the LUCENE_31 constant (I prefer this), we do not need to change backwards, but instead when switching version numbers we just move trunk forward to the next major version (after added to Version enum)."
0,"Exclude PrecedenceQueryParser from build or disable failing test casesAs Erik commented in LUCENE-885 the PrecendenceQueryParser is currently
unmaintained. Since some tests are failing we should either exclude PQP from the 
build or simply disable the failing tests."
0,Replace customized QueryParser.jjtWe should rather use the Lucene default and implement a  Jackrabbit  version that extends from it. This eases maintenance when moving from one Lucene version to another.
0,"Need stopwords and stoptags lists for default Japanese configurationStopwords and stoptags lists for Japanese needs to be developed, tested and integrated into Lucene."
0,"Add a OnWorkspaceInconsistency with logging onlyIf a Workspace performs a re-index on startup with  a inconsistency in it the process will fail now.
The new OnWorkspaceInconsistency ""log"" will only log the inconsistency but the reindex-process will not fail"
0,"Provide utility for handling large number of child nodes/propertiesJackrabbit does not cope well with 'flat' hierarchies. That is with hierarchies where a node has many child nodes and/or properties. The current recommendation for such situations is to manually add intermediate nodes. 

It would be nice to have an utility which adds/removes intermediate nodes as needed and expose a 'flat' view to users. Such an utility should:

- expose a large number of nodes/properties as sequence
- parametrize the order of how nodes/properties appear in the sequence
- provide methods to lookup/add/remove nodes/properties by key 
- organize the node/properties in the underlying JCR hierarchy in a way which is both efficient for above operations and easily understandable to users looking at the hierarchy. "
0,"No way to get the requestBody out of a PostMethod or use if extending classAttempting to extend the PostMethod class I discovered that I had no access to 
the requestBody because the member is declared private and there is no get 
method.

I was trying to override the setRequestContentLength() when I discovered the 
problem.

So my enhancement request specifically is:
1. add a get method to be able to get the requestBody (probably a get method to 
the parameters as well)
2. (optionally) make the requestBody and parameters members protected instead 
of private so extending the class is easier.

In case you were wondering, the reason for extending the class was to add the 
ability to set a timeout value on the httpconnection and also the ability to 
set the character encoding of the request body.  I don't know if these are 
worthy of an enhancement request but I require them for what I'm doing.  Nice 
work by the way.

Thank you."
0,Implement Query.getBindVariableNames()
0,"Simplify NRTManagerNRTManager is hairy now, because the applyDeletes is separately passed
to ctor, passed to maybeReopen, passed to getSearcherManager, etc.

I think, instead, you should pass it only to the ctor, and if you have
some cases needing deletes and others not then you can make two
NRTManagers.  This should be no less efficient than we have today,
just simpler.

I think it will also enable NRTManager to subclass ThingyManager
(LUCENE-3761).
"
0,"Making Term Vectors more accessibleOne of the big issues with term vector usage is that the information is loaded into parallel arrays as it is loaded, which are then often times manipulated again to use in the application (for instance, they are sorted by frequency).

Adding a callback mechanism that allows the vector loading to be handled by the application would make this a lot more efficient.

I propose to add to IndexReader:
abstract public void getTermFreqVector(int docNumber, String field, TermVectorMapper mapper) throws IOException;
and a similar one for the all fields version

Where TermVectorMapper is an interface with a single method:
void map(String term, int frequency, int offset, int position);

The TermVectorReader will be modified to just call the TermVectorMapper.  The existing getTermFreqVectors will be reimplemented to use an implementation of TermVectorMapper that creates the parallel arrays.  Additionally, some simple implementations that automatically sort vectors will also be created.

This is my first draft of this API and is subject to change.  I hope to have a patch soon.

See http://www.gossamer-threads.com/lists/lucene/java-user/48003?search_string=get%20the%20total%20term%20frequency;#48003 for related information."
0,"Change default value for respectDocumentOrderThe current default value for the search index configuration parameter respectDocumentOrder is true. Almost all applications are not interested in document order, while this default adds significant overhead to query execution because document order information is present in the index but has to be calculated over the complete result set.

I propose to change the default value to false and document this change in the 1.4 release notes. If an application relies on document order one can still explicitly set the parameter in the configuration to true."
0,"don't spawn thread statically in FSDirectory on Mac OS Xon the Mac, creating the digester starts up a PKCS11 thread.

I do not think threads should be created statically (I have this same issue with TimeLimitedCollector and also FilterManager).

Uwe discussed removing this md5 digester, I don't care if we remove it or not, just as long as it doesn't create a thread,
and just as long as it doesn't use the system default locale."
0,"Add configuration options for search managerRight now, if the search manager is active, everything is indexed, even the system branch of a workspace with the versions.

take parameters / conditions into account whether a node should be indexed:
- path
- node type
- property type
- property name


see also http://thread.gmane.org/gmane.comp.apache.jackrabbit.devel/3343"
0,"Incorporate GeoHash in contrib/spatialBased on comments from Yonik and Ryan in SOLR-773 
GeoHash provides the ability to store latitude / longitude values in a single field consistent hash field.
Which elements the need to maintain 2 field caches for latitude / longitude fields, reducing the size of an index
and the amount of memory needed for a spatial search."
0,CharacterCache - references deleted CharacterCache is deprecated by Character.valueOf(c) . Hence the latter is chosen over the former. 
0,"Enable setting the terms index divisor used by IndexWriter whenever it opens internal readersOpening a place holder issue... if all the refactoring being discussed don't make this possible, then we should add a setting to IWC to do so.

Apps with very large numbers of unique terms must set the terms index divisor to control RAM usage.

(NOTE: flex's RAM terms dict index RAM usage is more efficient, so this will help such apps).

But, when IW resolves deletes internally it always uses default 1 terms index divisor, and the app cannot change that.  Though one workaround is to call getReader(termInfosIndexDivisor) which will pool the reader with the right divisor."
0,"Collapse nested OR expressionsExecuting a query with multiple OR expressions in a predicate leads to score values that depend on the order of the operands.

For example, the following query:

//*[jcr:contains(@prop1, 'foo') or jcr:contains(@prop2, 'foo') or jcr:contains(@prop3, 'foo')] order by @jcr:score descending

will return a slightly different result compared to:

//*[jcr:contains(@prop3, 'foo') or jcr:contains(@prop1, 'foo') or jcr:contains(@prop2, 'foo')] order by @jcr:score descending

Internally jackrabbit parses the predicate of the first query into a tree:

orExpr(orExpr(contains(prop1, 'foo'), contains(prop2, 'foo')), contains(prop3, 'foo'))

Lucene will calculate the score for the inner OR expression first and then for the outer, which is not equivalent with a nested expression that has property names in a different sequence.

The query should be translated internally into a single OR expression with three operands. That way, the score value is always the same, irrespective of the order of the operands."
0,"TermAttribute.termLength() optimization
   public int termLength() {
     initTermBuffer(); // This patch removes this method call 
     return termLength;
   }

I see no reason to initTermBuffer() in termLength()... all tests pass, but I could be wrong?

"
0,"Document is partially indexed on an unhandled exceptionWith LUCENE-843, it's now possible for a subset of a document's
fields/terms to be indexed or stored when an exception is hit.  This
was not the case in the past (it was ""all or none"").

I plan to make it ""all or none"" again by immediately marking a
document as deleted if any exception is hit while indexing it.

Discussion leading up to this:

  http://www.gossamer-threads.com/lists/lucene/java-dev/56103
"
0,"Add support for custom ExecutorServices in ParallelMultiSearcherRight now, the ParallelMultiSearcher uses a cachedThreadPool, which is limitless and a poor choice for a web application, given the threaded nature of the requests (say a webapp with tomcat-default 200 threads and 100 indexes could be looking at 2000 searching threads pretty easily).  Support for adding a custom ExecutorService is pretty trivial.  Patch forthcoming."
1,"NodeReferences are lost when deleting and setting the same reference in the same save() cycleI've written the following snippet to illustrate the issue :

        Node root = session.getRootNode();
        
        Node a = root.addNode(""a"");
        Node b = root.addNode(""b"");
        b.addMixin(""mix:referenceable"");
        
        a.setProperty(""p"", b);
        
        root.save();
        
        System.out.println(b.getReferences().getSize());     // --> correctly returns 1
        
        a.setProperty(""p"", (Node) null);
        a.setProperty(""p"", b);
        
        root.save();
        
        System.out.println(b.getReferences().getSize());    // --> returns 0 !

When the ChangeLog is processed, added references are processed before deleted ones, so the persisted NodeReferences is finally wrong.

I've set the priority of this issue to critical, because the persisted references count is corrupted.

A simple workaround is to first remove the property, then save, then add the property again, but it not satisfying.
"
1,"Bundle persistence name index not case-sensitive in MySQL and MS SQLAs reported by Martijn Hendriks on the dev mailing list (see http://www.nabble.com/Bundle-persistence-managers---db-collation-tf3571522.html), the NAME column of the NAMES table in the bundle persistence manager needs to be case-sensitive."
1,"Preemtive Auth fails whithout credentialsThe preemtive authorization causes a HttpException to be thrown in teh
Authenticator if no credentials were provided at all. This case should be
handled quietly. A test case should be added."
1,"System view export truncates carriage returnIf a string contains a carriage return (\r), this character was truncated on some platforms. "
1,"VersionManagerImplRestore internalRestoreFrozen method has identity versus equals bugIn method protected void internalRestoreFrozen(NodeStateEx state,
                                         InternalFrozenNode freeze,
                                         VersionSelector vsel,
                                         Set<InternalVersion> restored,
                                         boolean removeExisting,
                                         boolean copy)
in the VersionManagerImplRestore class line 557 the code performs an == instead of calling the NodeId.equals() method.  We ran into problems with the code that executes below this (trying to restore a folder node throws an ItemExistsException since same sibling not allowed on folder nodes)"
1,"In JCAConnectionRequestInfo, equals() and hashCode() implementations are inconsistentJCAConnectionRequestInfo behaves differently in its equals() and hashCode() methods. The former is aware about SimpleCredentials structure, so two instances of JCAConnectionRequestInfo were supplied SimpleCredentials instances with same userID, password and attributes, they are considered equal.
But JCAConnectionRequestInfo.hashCode() just delegates to SimpleCredentials.hashCode() which is same as Object's method. This breaks session pooling."
1,"consistency check fails with derbypm if bundle size exceeds 32kdue to a 'problem' in derby DERBY-1486 interleaved reads on a bundle that is larger than about 32k results in an error:
  ERROR XJ073: The data in this BLOB or CLOB is no longer available.  
               The BLOB or CLOBs transaction may be committed, or its 
               connection is closed.

this issue was already addressed in JCR-1039 but not fixed for the consistency check."
1,"Cluster Journal directory should be created automaticallyIf the cluster journal directory does not exist when starting the cluster, an exception is thrown: ERROR org.apache.jackrabbit.core.RepositoryImpl - failed to start Repository: Directory specified does either not exist or is not a directory: ...

As far as I know, this is not consistent with how all other components of Jackrabbit work. I think the directory should be created automatically if it does not exist:

new File(...).mkdirs();

I know you could argue this is not a bug, but in my view it is an important usability issue."
1,Use of Multi-Args URI Causes URI-Rewriting to improperly unescape charactersSee: http://www.nabble.com/unable-to-encode-reserved-characters-using-java.net.URI-multi-arg-constructors-td14954679.html for information from the httpclient-dev thread.  The basic idea is that URI's multi-arg constructors break things.
1,"NodeTypeDef depends on supertype orderingCurrently the NodeTypeDef.setSupertypes() method simply sets the given QName array as the supertype QName array of the defined node type, thus preserving whatever ordering a node type parser or ultimately a node type definition file uses. This causes problems for example in the equals() method that uses the order-sensitive Arrays.equals() method to check for equality of the supertype QName arrays. The current implementation does therefore not consider the node type definitions ""A > B, C"" and ""A > C, B"" as equal even though they really should be so considered.

The same problem affects also child node and property definitions. The proper fix for this issue would probably be to use Sets to store and handle this information."
1,Scorer.skipTo() does not initialize hitsSome of the custom Scorer implementations in Jackrabbit do not initialize the internal hits BitSet if skipTo() is called before next().
1,"new QueryParser fails to set AUTO REWRITE for multi-term queriesThe old QueryParser defaults to constant score rewrite for Prefix,Fuzzy,Wildcard,TermRangeQuery, but the new one seems not to."
1,"Transient Repository cannot be used more than once when configured with DataSourcesThe TransientRepository cannot be used more than once when the repository is configured with the DataSources construct. This has been verified with both Oracle and Derby configurations. Once the TransientRepository closes for the first time, the ConnectionFactory class sets a boolean value named closed to 'true'.  Thereafter, any use of the ConnectionFactory throws a runtime exception.

The following stacktrace is thrown on the second attempt to utilize the repository:

2011-01-25 08:12:14 DatabaseFileSystem [ERROR] failed to initialize file system
java.lang.IllegalStateException: this factory has already been closed
	at org.apache.jackrabbit.core.util.db.ConnectionFactory.sanityCheck(ConnectionFactory.java:213)
	at org.apache.jackrabbit.core.util.db.ConnectionFactory.getDataBaseType(ConnectionFactory.java:134)
	at org.apache.jackrabbit.core.fs.db.DbFileSystem.getDataSource(DbFileSystem.java:228)
	at org.apache.jackrabbit.core.fs.db.DatabaseFileSystem.init(DatabaseFileSystem.java:190)
	at org.apache.jackrabbit.core.config.RepositoryConfigurationParser$6.getFileSystem(RepositoryConfigurationParser.java:1057)
	at org.apache.jackrabbit.core.config.RepositoryConfig.getFileSystem(RepositoryConfig.java:892)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:284)
	at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:602)
	at org.apache.jackrabbit.core.TransientRepository$1.getRepository(TransientRepository.java:179)
	at org.apache.jackrabbit.core.TransientRepository.startRepository(TransientRepository.java:279)
	at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:375)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:123)
	...
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:115)
2011-01-25 08:12:14 RepositoryImpl [ERROR] failed to start Repository: File system initialization failure.
javax.jcr.RepositoryException: File system initialization failure.
	at org.apache.jackrabbit.core.config.RepositoryConfigurationParser$6.getFileSystem(RepositoryConfigurationParser.java:1060)
	at org.apache.jackrabbit.core.config.RepositoryConfig.getFileSystem(RepositoryConfig.java:892)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:284)
	at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:602)
	at org.apache.jackrabbit.core.TransientRepository$1.getRepository(TransientRepository.java:179)
	at org.apache.jackrabbit.core.TransientRepository.startRepository(TransientRepository.java:279)
	at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:375)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:123)
	at TransientRepositoryTest.addNodeToRepository(TransientRepositoryTest.java:32)
	at TransientRepositoryTest.shouldNotFailWhenUsingTransientRepositoryTwice(TransientRepositoryTest.java:26)
	...
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:115)
Caused by: org.apache.jackrabbit.core.fs.FileSystemException: failed to initialize file system
	at org.apache.jackrabbit.core.fs.db.DatabaseFileSystem.init(DatabaseFileSystem.java:210)
	at org.apache.jackrabbit.core.config.RepositoryConfigurationParser$6.getFileSystem(RepositoryConfigurationParser.java:1057)
	... 42 more
Caused by: java.lang.IllegalStateException: this factory has already been closed
	at org.apache.jackrabbit.core.util.db.ConnectionFactory.sanityCheck(ConnectionFactory.java:213)
	at org.apache.jackrabbit.core.util.db.ConnectionFactory.getDataBaseType(ConnectionFactory.java:134)
	at org.apache.jackrabbit.core.fs.db.DbFileSystem.getDataSource(DbFileSystem.java:228)
	at org.apache.jackrabbit.core.fs.db.DatabaseFileSystem.init(DatabaseFileSystem.java:190)
	... 43 more
2011-01-25 08:12:14 RepositoryImpl [ERROR] Error while closing Version Manager.
java.lang.NullPointerException
	at org.apache.jackrabbit.core.RepositoryImpl.doShutdown(RepositoryImpl.java:1117)
	at org.apache.jackrabbit.core.RepositoryImpl.shutdown(RepositoryImpl.java:1063)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:388)
	at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:602)
	at org.apache.jackrabbit.core.TransientRepository$1.getRepository(TransientRepository.java:179)
	at org.apache.jackrabbit.core.TransientRepository.startRepository(TransientRepository.java:279)
	at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:375)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:123)
	at TransientRepositoryTest.addNodeToRepository(TransientRepositoryTest.java:32)
	at TransientRepositoryTest.shouldNotFailWhenUsingTransientRepositoryTwice(TransientRepositoryTest.java:26)
	...
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:115)
2011-01-25 08:12:14 RepositoryImpl [ERROR] In addition to startup fail, another unexpected problem occurred while shutting down the repository again.
java.lang.NullPointerException
	at org.apache.jackrabbit.core.RepositoryImpl.doShutdown(RepositoryImpl.java:1136)
	at org.apache.jackrabbit.core.RepositoryImpl.shutdown(RepositoryImpl.java:1063)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:388)
	at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:602)
	at org.apache.jackrabbit.core.TransientRepository$1.getRepository(TransientRepository.java:179)
	at org.apache.jackrabbit.core.TransientRepository.startRepository(TransientRepository.java:279)
	at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:375)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:123)
	at TransientRepositoryTest.addNodeToRepository(TransientRepositoryTest.java:32)
	at TransientRepositoryTest.shouldNotFailWhenUsingTransientRepositoryTwice(TransientRepositoryTest.java:26)
	...
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:115)

javax.jcr.RepositoryException: File system initialization failure.
	at org.apache.jackrabbit.core.config.RepositoryConfigurationParser$6.getFileSystem(RepositoryConfigurationParser.java:1060)
	at org.apache.jackrabbit.core.config.RepositoryConfig.getFileSystem(RepositoryConfig.java:892)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:284)
	at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:602)
	at org.apache.jackrabbit.core.TransientRepository$1.getRepository(TransientRepository.java:179)
	at org.apache.jackrabbit.core.TransientRepository.startRepository(TransientRepository.java:279)
	at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:375)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:123)
	at TransientRepositoryTest.addNodeToRepository(TransientRepositoryTest.java:32)
	at TransientRepositoryTest.shouldNotFailWhenUsingTransientRepositoryTwice(TransientRepositoryTest.java:26)
	...
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:115)
Caused by: org.apache.jackrabbit.core.fs.FileSystemException: failed to initialize file system
	at org.apache.jackrabbit.core.fs.db.DatabaseFileSystem.init(DatabaseFileSystem.java:210)
	at org.apache.jackrabbit.core.config.RepositoryConfigurationParser$6.getFileSystem(RepositoryConfigurationParser.java:1057)
	... 42 more
Caused by: java.lang.IllegalStateException: this factory has already been closed
	at org.apache.jackrabbit.core.util.db.ConnectionFactory.sanityCheck(ConnectionFactory.java:213)
	at org.apache.jackrabbit.core.util.db.ConnectionFactory.getDataBaseType(ConnectionFactory.java:134)
	at org.apache.jackrabbit.core.fs.db.DbFileSystem.getDataSource(DbFileSystem.java:228)
	at org.apache.jackrabbit.core.fs.db.DatabaseFileSystem.init(DatabaseFileSystem.java:190)
	... 43 more"
1,"addIndexesNoOptimize intermittantly throws incorrect ""segment exists in external directory..."" exceptionSpinoff from here:

    http://mail-archives.apache.org/mod_mbox/lucene-java-user/200809.mbox/%3Cba72f77f0809111418l29cf215dnd45bf679832d7d42%40mail.gmail.com%3E

Here's my response on that thread:

The bug only happens when you call addIndexesNoOptimize, and one
simple workaround would be to use SerialMergeScheduler.

I think this is already fixed in trunk (soonish to be 2.4) as a side
effect of https://issues.apache.org/jira/browse/LUCENE-1335.

In 2.3, merges that involve external segments (which are segments
folded in by addIndexesNoOptimize) are not supposed to run in a BG
thread.  This is to prevent addIndexesNoOptimize from returning until
after all external segments have been carried over (merged or copied)
into the index, so that if there is an exception (eg disk full),
addIndexesNoOptimize is able to rollback to the index to the starting
point.

The primary merge() method of CMS indeed does not BG any external
merges, but the bug is that when a BG merge finishes it then selects a
new merge to kick off and that selection is happy to pick an external
segment."
1,"DocValues merging is not associative, leading to different results depending upon how merges executerecently I cranked up TestDuelingCodecs to actually test docvalues (previously it wasn't testing it at all).

This test is simple, it indexes the same random content with 2 different indexwriters, it just allows them
to use different codecs with different indexwriterconfigs.

then it asserts the indexes are equal.

Sometimes, always on BYTES_FIXED_DEREF type, we end out with one reader that has a zero-filled byte[] for a doc,
but that same document in the other reader has no docvalues at all.
"
1,"FieldCache.getStringIndex should not throw exception if term count exceeds doc countSpinoff of LUCENE-2133/LUCENE-831.

Currently FieldCache cannot handle more than one value per field.
We may someday want to fix that... but until that day:

FieldCache.getStringIndex currently does a simplistic check to try to
catch when you've accidentally allowed more than one term per field,
by testing if the number of unique terms exceeds the number of
documents.

The problem is, this is not a perfect check, in that it allows false
negatives (you could have more than one term per field for some docs
and the check won't catch you).

Further, the exception thrown is the unchecked RuntimeException.

So this means... you could happily think all is good, until some day,
well into production, once you've updated enough docs, suddenly the
check will catch you and throw an unhandled exception, stopping all
searches [that need to sort by this string field] in their tracks.
It's not gracefully degrading.

I think we should simply remove the test, ie, if you have more terms
than docs then the terms simply overwrite one another.
"
1,"Cannot version the root nodeAfter making the root node versionable, the checkin method fails with the following exception. 

java.lang.ArrayIndexOutOfBoundsException: 0
    at org.apache.jackrabbit.core.version.persistence.PersistentNode.copyFrom(PersistentNode.java:589)
    at org.apache.jackrabbit.core.version.persistence.InternalFrozenNodeImpl.checkin(InternalFrozenNodeImpl.java:277)
    at org.apache.jackrabbit.core.version.persistence.InternalFrozenNodeImpl.checkin(InternalFrozenNodeImpl.java:307)
    at org.apache.jackrabbit.core.version.persistence.InternalFrozenNodeImpl.checkin(InternalFrozenNodeImpl.java:307)
    at org.apache.jackrabbit.core.version.persistence.InternalVersionHistoryImpl.checkin(InternalVersionHistoryImpl.java:354)
    at org.apache.jackrabbit.core.version.persistence.NativePVM.checkin(NativePVM.java:506)
    at org.apache.jackrabbit.core.version.VersionManagerImpl.checkin(VersionManagerImpl.java:212)
    at org.apache.jackrabbit.core.NodeImpl.checkin(NodeImpl.java:2184)
    at com.gtnet.jcr.VersionedNodeTest.testVersionRootNode(VersionedNodeTest.java:218)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at com.intellij.rt.execution.junit2.JUnitStarter.main(JUnitStarter.java:31)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:78)
"
1,"LockMethod.getResponseAsLockDiscovery() fails when status==201 RFC 4918 Section 9.10.6 specifies that 201 is a valid response code for LOCK: ""201 (Created) - The LOCK request was to an unmapped URL, the request succeeded and resulted in the creation of a new resource, and the value of the DAV:lockdiscovery property is included in the response body.""

However, LockMethod.getResponseAsLockDiscovery() would fail in that scenario. 
    org.apache.jackrabbit.webdav.DavException: Created
	 at org.apache.jackrabbit.webdav.client.methods.DavMethodBase.getResponseException(DavMethodBase.java:164)
	 at org.apache.jackrabbit.webdav.client.methods.LockMethod.getResponseAsLockDiscovery(LockMethod.java:119)


The reason is in LockMethod:175 
      return statusCode == DavServletResponse.SC_OK;

Should be: 
      return statusCode == DavServletResponse.SC_OK
             || statusCode ==DavServletResponse.SC_CREATED;"
1,"HttpConnection isOpen flag concurrency problemThe HttpConnection.java class contains an isOpen boolean used to track the state
of the connection (opened or closed).  The problem is that in the
closeSocketAndStreams(), the flag is only flipped at the end of the
unsynchronized method (after resources have been released) which causes a
concurrency issue in flushRequestOutputStream() where the flag is checked first
and the the outputStream is accessed.

I'm providing a patch for this problem."
1,"XML import using MacOS X WebDAV client does not workwhen trying to import a xml file via a webdav mount this does not work.

this is mainly because the client first tries to create a 0-sized file, which fails with the xml importer. after the file is created, it will lock it and put the xml body. a second problem might be the ""dot-underscore"" files mac tries to create. "
1,"New LaxRedirectStrategy class should probably call the super method first.LaxRedirectStrategy extends the defaulRedirect class but does not call the super method as one would expect.

Just adding a patch to make sure it gets called."
1,"If indexwriter hits a non-ioexception from indexExists it leaks a write.lockthe rest of IW's ctor is careful about this.

IndexReader.indexExists catches any IOException and returns false, but the problem
occurs if some other exception (in my test, UnsupportedOperationException, but you
can imagine others are possible), when trying to e.g. read in the segments file.

I think we just need to move the IR.exists stuff inside the try / finally"
1,"WorkspaceImporter throws exceptionsessio.getWorkspace().getImportContentHandler() throws 
java.lang.UnsupportedOperationException: Workspace-Import of protected nodes: Not yet implement.

suggest to issue warning instead of throwing."
1,"JNDI Referencable IssuesI'm questioning the use of Referencable in the BindableResource and BindableResourceFactory classes for the JNDI lookup process. Reason for this is because Referencable needs the Addrs to be in the EXACT order in order for it to be considered the same. (see http://java.sun.com/j2se/1.4.2/docs/api/javax/naming/Reference.html#equals(java.lang.Object) )

In order for me to get the JNDI reference to be found correctly I had to change the BindableResource.getReference method to swap the order the StringReferences were added to match up what was being passed in by glassfish. This seems EXTREMELY fragile to me as I don't know what order, say JBoss, would pass the StringRefences in in the Reference object for the Factory method.

Also, another problem is that getReference is binding the class name to BindableRepository class implementation and not javax.jcr.Repository. This again causes them not to match if you follow the example on the wiki on setting up the JNDI reference and use javax.jcr.Repository as the type. This can either be fixed by changing the JNDI reference to use the BindableRepository class or the change the BindableRepository class to set that to the Repository interface. Not sure which would be considered 'better'

I have a patch that fixes the first issue (at least for glassfish), but not the second. Again, this seems like a really 'breakable' setup right now and not sure what would be better to make sure this is avoided."
1,"fix assertions/checks that use File.length() to use getFilePointer()This came up on this thread ""Getting RuntimeException: after flush: fdx size mismatch while Indexing"" 
(http://www.lucidimagination.com/search/document/a8db01a220f0a126)

In trunk, a side effect of the codec refactoring is that these assertions were pushed into codecs as finish() before close().
they check getFilePointer() instead in this computation, which checks that lucene did its part (instead of falsely tripping if directory metadata is stale).

I think we should fix these checks/asserts on 3.x too
"
1,"Database connection leak with DBCP, MySQL, and ObserversWhen using DBCP and MySQL with an observer that modifies the content repository, we are seeing abandoned connections in our connection pool."
1,"ShingleFilter skips over trie-shingles if outputUnigram is set to falseSpinoff from http://lucene.markmail.org/message/uq4xdjk26yduvnpa

{quote}
I noticed that if I set outputUnigrams to false it gives me the same output for
maxShingleSize=2 and maxShingleSize=3.

please divide divide this this sentence

when i set maxShingleSize to 4 output is:

please divide please divide this sentence divide this this sentence

I was expecting the output as follows with maxShingleSize=3 and
outputUnigrams=false :

please divide this divide this sentence 
{quote}


"
1,"Redirect to a relative URL failsRequest the url 
http://commerce1.cera.net/discount-pcbooks/catalog/categories.asp?
search_str=0782128092

On a browser the redirect works, while with HttpClient it doesn't."
1,"The deprecated constructor of BooleanClause does not set new stateNick Burch reported this on lucene-user. 
 
Patch will follow. 
 
Regards, 
Paul Elschot"
1,"BoostingTermQuery's explanation should be marked as Match even if the payload part negated or zero'ed itSince BTQ multiplies the payload on the score it might return a negative score.
The explanation should be marked as ""Match"" otherwise it is not added to container explanations,
See also in LUCENE-1302."
1,"Query index not in sync with workspaceAfter some time the search index is not in sync anymore with the data in the workspace and returns uuids which have no corresponding Node in the workspace. This results in a NodeIterator which throws an ItemNotFoundException on nextNode().

Instructions how to reproduce this error are not yet available.

Possible areas for further investigation are:
- NodeType registry which maps the node types into the workspace with the use of virtual item states
- versioning?
- atomicity of indexing?"
1,"Fix small perf issues with String/TermOrdValComparatorUncovered some silliness when working on LUCENE-2504, eg we are doing unnecessary binarySearch on a single-segment reader."
1,"SpanRegexQuery and SpanNearQuery is not working with MultiSearcherMultiSearcher is using:
queries[i] = searchables[i].rewrite(original);
to rewrite query and then use combine to combine them.

But SpanRegexQuery's rewrite is different from others.
After you call it on the same query, it always return the same rewritten queries.

As a result, only search on the first IndexSearcher work. All others are using the first IndexSearcher's rewrite queries.
So many terms are missing and return unexpected result.

Billow"
1,"/contrib/orm-persistence/ OJBPersistenceManagerOJBPersistenceManager seems to have the following problems

1. OJBPersistenceBroker inherites from AbstractPersistenceBroker. There's no 
need of using a non transactional implementation as the feature is available in 
jdbc. 

2. A single broker is used and It's not thread-safe. This is not a problem now 
because it inherits from AbstractPersistenceManager, and the store(ChangeLog ) 
method is synchronized.

3. The broker is never closed so it leaves an open connection.

4. There's no pooling with only one broker.

5 Each write method (e.g. store(NodeState state)) starts its own transaction 
but the transaction should start and end in store(ChangeLog log).

6. It never rollbacks, even when an item in the changelog can't be persisted.

7. The mysql example create MyISAM tables which don't support transactions. 
Innodb tables would be more appropriate.

8. jdbc to java type mapping is wrong for 
class: org.apache.jackrabbit.core.state.orm.ORMBlobValue
field: size
Changed from INTEGER to BIGINT

9. When a Blob value is loaded a ArrayStoreException is thrown because in 
load(PropertyId id) BlobFileValues are added to internalValueList instead of 
InternalValue instances.

10. in store(NodeReferences). When storing a NodeReferences which have some (but not all) the references deleted the OJB persistence Manager doesn't delete any one.

Some of this problems are present in the Hibernate implementation."
1,"Mixin type lossWhen using a bundle persistence manager, the mixin type information may be corrupted in the lucene index, causing queries like '//element(*, my:mixin)' to fail.



The problem is that the 'jcr:mixinTypes' may be stored in the bundle. Here is how this could happen :


First step: Create a node and add a mixin 'A'.

Everything's fine. The query '//element(*, 'A')' works.


Second step : Select the node and add a second mixin 'B'.

When the second mixin is added, the AbstractBundlePersistenceManager#load(PropertyId) is called to get the current mixins for the node. This method will store the PropertyState for 'jcr:mixinTypes' in the bundle (containing only the mixin 'A'). Then the NodeImpl#setMixinTypesProperty() will set the PropertyState for 'jcr:mixinTypes' in the node state (containing the mixins 'A' and 'B').
When the session is saved, the ChangeLog in AbstractBundlePersistenceManager#store() contains a modification for the 'jcr:mixinTypes' but it's being ignored, leaving the bundle with only mixin 'A'. The NodeIndexer looks into the node state to get the mixin types and indexes the node correctly. The queries '//element(*, 'A')' and '//element(*, 'B')' work.


Thrid step : Select the node and update a property. 

When the session is saved, the NodeIndexer asks again for the 'jcr:mixinTypes' property, by calling the AbstractBundlePersistenceManager#load(PropertyId) to load it. The bundle contains this property and returns only mixin 'A' (as it was stored in the second step), causing the index to use only mixin 'A'. The query '//element(*, 'A')' still works but '//element(*, 'B') doesn't work anymore.



A simple solution to this would be to not store the PropertyState for the 'jcr:mixinTypes' (and 'jcr:uuid' and 'jcr:primaryType', as the class description states) in the bundle when the PropertyState is loaded. It would fix the issue but not the contents on existing repositories. One way to allow the repositories to fix themselves is to not read or write these 3 properties in the BundleBinding#readBundle and BundleBinding#writeBundle methods, but I'm not sure wether or not it would have a performance impact.


"
1,Lucene queries are not properly rewrittenSome of the jackrabbit internal lucene queries are not properly rewritten and may lead to UnsupportedOperationException when terms are extracted from the lucene query.
1,"Deleting docs of all returned Hits during search causes ArrayIndexOutOfBoundsExceptionFor background user discussion:
http://www.nabble.com/document-deletion-problem-to14414351.html

{code}
Hits h = m_indexSearcher.search(q); // Returns 11475 documents 
for(int i = 0; i < h.length(); i++) 
{ 
  int doc = h.id(i); 
  m_indexSearcher.getIndexReader().deleteDocument(doc);  <-- causes ArrayIndexOutOfBoundsException when i = 6400
} 
{code}
"
1,"SSLSocketFactory.createSSLContext does not process trust storeorg.apache.http.conn.ssl.SSLSocketFactory.createSSLContext() does not process a provided trust store.
Only the default (cacerts) is processed. An additional provided trust store is ignored.
Adding the ""trusted"" certificate to the keystore, the peer is authenticated.

Eventually
        tmfactory.init(keystore);
needs to be
        tmfactory.init(truststore);

"
1,"TestStressNRT failures (reproducible)Build server logs. Reproduces on at least two machines.

{noformat}
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestStressNRT -Dtestmethod=test -Dtests.seed=69468941c1bbf693:19e66d58475da929:69e9d2f81769b6d0 -Dargs=""-Dfile.encoding=UTF-8""
    [junit] NOTE: test params are: codec=Lucene3x, sim=RandomSimilarityProvider(queryNorm=true,coord=false): {}, locale=ro, timezone=Etc/GMT+1
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestStressNRT]
    [junit] NOTE: Linux 3.0.0-16-generic amd64/Sun Microsystems Inc. 1.6.0_27 (64-bit)/cpus=2,threads=1,free=74960064,total=135987200
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: test(org.apache.lucene.index.TestStressNRT):	Caused an ERROR
    [junit] MockDirectoryWrapper: cannot close: there are still open files: {_ng.cfs=8}
    [junit] java.lang.RuntimeException: MockDirectoryWrapper: cannot close: there are still open files: {_ng.cfs=8}
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:555)
    [junit] 	at org.apache.lucene.index.TestStressNRT.test(TestStressNRT.java:385)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$SubclassSetupTeardownRule$1.evaluate(LuceneTestCase.java:743)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$InternalSetupTeardownRule$1.evaluate(LuceneTestCase.java:639)
    [junit] 	at org.apache.lucene.util.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:22)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$TestResultInterceptorRule$1.evaluate(LuceneTestCase.java:538)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$RememberThreadRule$1.evaluate(LuceneTestCase.java:600)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:164)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)
    [junit] 	at org.apache.lucene.util.StoreClassNameRule$1.evaluate(StoreClassNameRule.java:21)
    [junit] 	at org.apache.lucene.util.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:22)
    [junit] Caused by: java.lang.RuntimeException: unclosed IndexInput: _ng.cfs
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.addFileHandle(MockDirectoryWrapper.java:479)
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper$1.openSlice(MockDirectoryWrapper.java:777)
    [junit] 	at org.apache.lucene.store.CompoundFileDirectory.openInput(CompoundFileDirectory.java:221)
    [junit] 	at org.apache.lucene.codecs.lucene3x.TermInfosReader.<init>(TermInfosReader.java:112)
    [junit] 	at org.apache.lucene.codecs.lucene3x.Lucene3xFields.<init>(Lucene3xFields.java:84)
    [junit] 	at org.apache.lucene.codecs.lucene3x.PreFlexRWPostingsFormat$1.<init>(PreFlexRWPostingsFormat.java:51)
    [junit] 	at org.apache.lucene.codecs.lucene3x.PreFlexRWPostingsFormat.fieldsProducer(PreFlexRWPostingsFormat.java:51)
    [junit] 	at org.apache.lucene.index.SegmentCoreReaders.<init>(SegmentCoreReaders.java:108)
    [junit] 	at org.apache.lucene.index.SegmentReader.<init>(SegmentReader.java:51)
    [junit] 	at org.apache.lucene.index.IndexWriter$ReadersAndLiveDocs.getMergeReader(IndexWriter.java:521)
    [junit] 	at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3587)
    [junit] 	at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3257)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:382)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:451)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.index.TestStressNRT FAILED
{noformat}"
1,"ConcurrentModificationException during registration of nodetypesDuring the registration of a set of nodetypes this exception may be encountered:

java.util.ConcurrentModificationException
        at org.apache.commons.collections.map.AbstractReferenceMap$ReferenceEntrySetIterator.checkMod(AbstractReferenceMap.java:761)
        at org.apache.commons.collections.map.AbstractReferenceMap$ReferenceEntrySetIterator.hasNext(AbstractReferenceMap.java:735)
        at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.notifyRegistered(NodeTypeRegistry.java:1750)
        at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.registerNodeTypes(NodeTypeRegistry.java:223)

It seems that the copying of the listeners triggered this exception:

    private void notifyRegistered(QName ntName) {
        // copy listeners to array to avoid ConcurrentModificationException
        NodeTypeRegistryListener[] la =
                new NodeTypeRegistryListener[listeners.size()];
        Iterator iter = listeners.values().iterator();
        int cnt = 0;
1750:   while (iter.hasNext()) {
            la[cnt++] = (NodeTypeRegistryListener) iter.next();
        }
        for (int i = 0; i < la.length; i++) {
            if (la[i] != null) {
                la[i].nodeTypeRegistered(ntName);
            }
        }
    }

The methods ""notifyReRegistered"" and ""notifyUnregistered"" will probably suffer from the same problem.

Reproduction of this exception may be tricky; it only occurred once in our application. It is probably a race condition: another thread might access the listeners during the copy. It may be helpful to use a debugger and set a breakpoint in the middle of the copy giving other threads the opportunity to access the listeners...

We think that a possible solution is the following:

    /**
     * Notify the listeners that a node type <code>ntName</code> has been registered.
     */
    private void notifyRegistered(QName ntName) {
        // copy listeners to array to avoid ConcurrentModificationException
    	NodeTypeRegistryListener[] la;
    	synchronized (listeners) {
            la = (NodeTypeRegistryListener[]) listeners.values().toArray(new NodeTypeRegistryListener[listeners.size()]);
		}

        for (int i = 0; i < la.length; i++) {
            if (la[i] != null) {
                la[i].nodeTypeRegistered(ntName);
            }
        }
    }



"
1,"SearchWithSortTask ignores sorting by DocDuring my work in LUCENE-3912, I found the following code:

{code}
if (field.equals(""doc"")) {
    sortField0 = SortField.FIELD_DOC;
} if (field.equals(""score"")) {
    sortField0 = SortField.FIELD_SCORE;
} ...
{code}

This means the setting of SortField.FIELD_DOC is ignored.  While I don't know much about this code, this seems like a valid setting and obviously just a bug."
1,"TestIndexWriter.testBackgroundOptimize fails with too many open filesRecreate with this line:

ant test -Dtestcase=TestIndexWriter -Dtestmethod=testBackgroundOptimize -Dtests.seed=-3981504507637360146:51354004663342240

Might be related to LUCENE-2873 ?"
1,"AbstractJournal doesn't create deep paths for revision filesAbstractJournal throws when trying to create the revision file if the directory the revision file is in doesn't already exist. When initializing a repository during its startup, the create fails is you use a revision param like <param name=""revision"" value=""${rep.home}/repository/revision"" /> because the repository directory hasn't been created yet. Attached is a repository.xml that demonstrates. It uses Oracle for FS and PMs."
1,"DocumentViewExportVisitor class incorrectly handles XML escaping for element namesThe method private static String escapeName(String name) should have the following test:

 if ((i == 0) ? XMLChar.isNCNameStart(ch) : XMLChar.isNCName(ch)) {

changed into

 if ((i == 0) ? !XMLChar.isNCNameStart(ch) :! XMLChar.isNCName(ch)) {

in order to properly escape the text (those two methods in XMLChar return true when the character is valid, not the other way around."
1,"SQL parser chokes on prefixes containing a ""-"" characterSQL parser chokes on prefixes containing a ""-"" character, such as in

  SELECT a-b:c FROM nt:resource

"
1,"NullpointerException in SessionItemStateManagerI got the following exception which is not reproducible and occured during a large batch of write operations. Unfortunately I got no idea how this happened. May be someone has an idea?

[2006-11-27 21:43:53,065, WARN ] {} support.RemoteInvocationTraceInterceptor:80: Processing of RmiServiceExporter remote call resulted in fatal exception: com.subshell.sophora.content.server.IContentManager.importDocument
org.springframework.transaction.TransactionSystemException: Could not commit JCR transaction; nested exception is java.lang.NullPointerException Caused by: 
java.lang.NullPointerException
        at org.apache.jackrabbit.core.state.SessionItemStateManager.nodeModified(SessionItemStateManager.java:878)
        at org.apache.jackrabbit.core.state.StateChangeDispatcher.notifyNodeModified(StateChangeDispatcher.java:143)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.stateModified(LocalItemStateManager.java:426)
        at org.apache.jackrabbit.core.state.StateChangeDispatcher.notifyStateModified(StateChangeDispatcher.java:85)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.stateModified(SharedItemStateManager.java:388)
        at org.apache.jackrabbit.core.state.StateChangeDispatcher.notifyStateModified(StateChangeDispatcher.java:85)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.stateModified(SharedItemStateManager.java:388)
        at org.apache.jackrabbit.core.state.ItemState.notifyStateUpdated(ItemState.java:241)
        at org.apache.jackrabbit.core.state.ChangeLog.persisted(ChangeLog.java:271)
        at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:691)
        at org.apache.jackrabbit.core.state.XAItemStateManager.commit(XAItemStateManager.java:169)
        at org.apache.jackrabbit.core.version.XAVersionManager.commit(XAVersionManager.java:478)
        at org.apache.jackrabbit.core.TransactionContext.commit(TransactionContext.java:172)
        at org.apache.jackrabbit.core.XASessionImpl.commit(XASessionImpl.java:315)
        at org.springmodules.jcr.jackrabbit.support.JackRabbitUserTransaction.commit(JackRabbitUserTransaction.java:104)
        at org.springmodules.jcr.jackrabbit.LocalTransactionManager.doCommit(LocalTransactionManager.java:192)
        at org.springframework.transaction.support.AbstractPlatformTransactionManager.processCommit(AbstractPlatformTransactionManager.java:540)
        at org.springframework.transaction.support.AbstractPlatformTransactionManager.commit(AbstractPlatformTransactionManager.java:510)
        at org.springframework.transaction.interceptor.TransactionAspectSupport.commitTransactionAfterReturning(TransactionAspectSupport.java:310)
        at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:117)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:185)
        at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:89)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:185)
        at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:209)
        at $Proxy14.importDocument(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:318)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:203)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:162)
        at org.springframework.remoting.support.RemoteInvocationTraceInterceptor.invoke(RemoteInvocationTraceInterceptor.java:70)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:185)
        at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:209)
        at $Proxy15.importDocument(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor21.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.springframework.remoting.support.RemoteInvocation.invoke(RemoteInvocation.java:181)
        at org.springframework.remoting.support.DefaultRemoteInvocationExecutor.invoke(DefaultRemoteInvocationExecutor.java:38)
        at org.springframework.remoting.support.RemoteInvocationBasedExporter.invoke(RemoteInvocationBasedExporter.java:76)
        at org.springframework.remoting.rmi.RmiBasedExporter.invoke(RmiBasedExporter.java:72)
        at org.springframework.remoting.rmi.RmiInvocationWrapper.invoke(RmiInvocationWrapper.java:62)
        at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:294)
        at sun.rmi.transport.Transport$1.run(Transport.java:153)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:149)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:460)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:701)
        at java.lang.Thread.run(Thread.java:595)
"
1,"Preemptive Authorization parameter initialization incorrect, causes preemptive auth not to workPreemptive authorization is defeated by an incorrect initialization. Patch 
follows:
--- DefaultHttpParamsFactory.java       2005-10-10 19:09:10.000000000 -0700
+++ DefaultHttpParamsFactory.java.fixed 2005-10-17 17:00:10.259174920 -0700
@@ -118,9 +118,9 @@
         if (preemptiveDefault != null) {
             preemptiveDefault = preemptiveDefault.trim().toLowerCase();
             if (preemptiveDefault.equals(""true"")) {
-                params.setParameter
(HttpClientParams.PREEMPTIVE_AUTHENTICATION, ""on"");
+                params.setParameter
(HttpClientParams.PREEMPTIVE_AUTHENTICATION, Boolean.TRUE);
             } else if (preemptiveDefault.equals(""false"")) {
-                params.setParameter
(HttpClientParams.PREEMPTIVE_AUTHENTICATION, ""off"");
+                params.setParameter
(HttpClientParams.PREEMPTIVE_AUTHENTICATION, Boolean.FALSE);
             }
         }"
1,"remove IndexInput.copyBufthis looks really broken/dangerous as an instance variable.

what happens on clone() ?! copyBytes can instead make its own array inside the method.

its protected, so ill list in the 3.x backwards breaks section since its technically a backwards break."
1,"Error while restoring OPV=Version childnodes (Restore of root version not allowed)when restoring a version of a node (by name) that has opv=version childnodes, the following error is thrown, if such a version does not exist in the child nodes versionhistory:

Error while restoring nodes: javax.jcr.version.VersionException: Restore of root version not allowed."
1,"SSL connections cannot be established using the IP addressHttpClient 4.x introduced a regression in establishing SSL connections to remote peers. The AbstractVerifier class only checks for matches in CN and SubjectAlternative->DNSName. But, when an IP (instead of a hostname) is used, the check should be done on CN and SubjectAlternative->IPAddress."
1,"Destination URI should be normalizedWebdavRequestImpl.getHrefLocator tests if the URI passed as parameter starts with the context path, and passes the next segments to the locator factory.
 
There is a potential hole if the parameter contains "".."", because ""http://example.com/dav/../foo"" starts with the context path ""http://example.com/dav"" but represents to ""http://example.com/foo"". Currently, it is up to the locator factory to detect this situation, meaning that every locator factory should implement this check. Additionally, DavLocatorFactory.createResourceLocator cannot throw exceptions, hence it would not fail cleanly (RuntimeException causing a 500 INTERNAL SERVER ERROR response, when a 403 FORBIDDEN status code would have been apropriate)

Note that the Request-URI should have already been normalized by the servlet container, but in COPY/MOVE operations, the Destination-URI is not normalized.

Conformant clients MUST NOT use dot-segments (""."" or "".."")  [RFC 4918, Section 8.3] in Simple-Ref constructions such as the Destination header [RFC 4918, Section 10.3]), but the server should be able to detect this error.

Proposed change in WebdavRequestImpl:193 (in package org.apache.jackrabbit.webdav from webdav/java)
- ref = uri.getRawPath();
+ ref = uri.normalize().getRawPath();

(This causes /dav/../foo to be rejected because it doesn't start with the context path, and accepts dav/foo/../bar because it starts with the context path)"
1,SpanScorer does not respect ConstantScoreRangeQuery settingConstantScoreRangeQuery is actually on and can't be disabled when it should default to off with the option to turn it on.
1,"close() throws incorrect IllegalStateEx after IndexWriter hit an OOME when autoCommit is trueSpinoff from http://www.nabble.com/IllegalStateEx-thrown-when-calling-close-to20201825.html

When IndexWriter hits an OOME, it records this and then if close() is
called it calls rollback() instead.  This is a defensive measure, in
case the OOME corrupted the internal buffered state (added/deleted
docs).

But there's a bug: if you opened IndexWriter with autoCommit true,
close() then incorrectly throws an IllegalStatException.

This fix is simple: allow rollback to be called even if autoCommit is
true, internally during close.  (External calls to rollback with
autoCommmit true is still not allowed).
"
1,"Item.isSame() may return true for 2 nodes from different workspaces.the code in ItemImpl.isSame() only compares the item id, but not the source workspace."
1,"OracleFileSystem can't handle empty filesthe following exception is thrown when trying to access a 0-length file
in an OracleFileSystem:
java.sql.SQLException: ORA-22275: invalid LOB locator specified

issue reported on the users list, 
see http://www.nabble.com/problems-with-Oracle-tf2483987.html#a6926522

"
1,"FixedIntBlockIndexInput.Reader does not initialise 'pending' int arrayThe FixedIntBlockIndexInput.Reader.pending int array is not initialised. As a consequence, the FixedIntBlockIndexInput.Reader#next() method returns always 0.

A call to FixedIntBlockIndexInput.Reader#blockReader.readBlock() during the Reader initialisation may solve the issue (to be tested)."
1,".war distribution should be configurable, prompting you to setup JNDI with the Repository Home and Config locations.The Embedded Deployment Model documentation (http://jackrabbit.apache.org/doc/deploy/howto-model1.html) on the jackrabbit page describes how to package up a .war file so that you can use JNDI Resource settings to change the location of the repository home and the repository configuration xml file.

Unfortunately, the .war file that is provided as part of the Jackrabbit distribution doesn't behave like this. Instead, it has an inbuilt repository.xml file and settings in web.xml that act as defaults. These defaults are not useful and force a user to act like a developer and modify the files within the .war file.

The current situation is that we have a .war that's not going to be useful to anyone without modification. The repository.xml file that is contained within the .war makes the repository home to be the Tomcat/bin/repository directory. This is not a useful default. It's better to have no default setup and a clear error message that JNDI needs to be setup. It would be even better if the web application could recognise when the JNDI wasn't configured and could prompt the user with an instructional webpage, describing how to setup the required JNDI settings on Tomcat, JBoss etc.

----

The .war distribution for Jackrabbit ignores the JNDI settings that are described in the documentation. I am using this Tomcat config.xml snippet to configure Tomcat 5.5:

{{{
<?xml version='1.0' encoding='utf-8'?>
<Context displayName=""Ark"" docBase=""c:\dev\ark\jackrabbit-server-1.1.1.war"" path=""/ark"" 
         useNaming=""false"" workDir=""work\Catalina\localhost\ark"" unpackWAR=""false"">

<Resource name=""jcr/repository""
          auth=""Container""
          type=""javax.jcr.Repository""
          factory=""org.apache.jackrabbit.core.jndi.BindableRepositoryFactory""
          configFilePath=""c:/dev/ark/src/main/resources/repository.xml""
          repHomeDir=""c:/jackrabbitrepo""/>

</Context>
}}}

Jackrabbit loads fine. However, the logs show:

{{{
02.01.2007 10:33:00 *INFO * RepositoryStartupServlet: RepositoryStartupServlet initializing... (RepositoryStartupServlet.java, line 190)
02.01.2007 10:33:00 *INFO * RepositoryStartupServlet:   repository-home = C:\Program Files\Apache Software Foundation\Tomcat 5.5\bin\jackrabbit\repository (RepositoryStartupServlet.java, line 242)

...
...

02.01.2007 10:33:00 *INFO * LocalFileSystem: LocalFileSystem initialized at path C:\Program Files\Apache Software Foundation\Tomcat 5.5\bin\jackrabbit\repository\repository (LocalFileSystem.java, line 166)
}}}






----

My use case is that I want to use Jackrabbit to host a Maven 2 repository within my company. So, ideally I want to:
   * Download the Jackrabbit .war file and mount it on my Tomcat server as context ""/maven2"".
   * Configure Tomcat to use LDAP authentication and point it at my company's LDAP server. This is a standard J2EE feature, of course.
   * Create my own repository.xml file which points to my AccessManager implementation (which goes to my company's SingleSignOn service for authorization). My AccessManager implementation will be placed on the Tomcat shared classpath.
   * Set the repository home directory, where all the working files will be placed and the location of the repository.xml file. Ideally, this would be done in JNDI.

If I have to put together my own Jackrabbit .war file, I consider that I have my ""developer"" hat on when I only really want to have my ""Jackrabbit user"" hat on.
"
1,"TCK: Test root path not escaped when used in XPath queriesA repository implementation might use a test root path that contains names that need _xXXXX_ escaping when used in XPath queries. Currently the TCK just uses the test path as-is when constructing queries. Even though this only affects few repositories (I've heard of one legacy connector to run into this problem), it would be good to add the proper escaping."
1,"Multithreading issue with versioningIn a multithreading environment with two or more threads accessing the same version history, inconsistent state may be encountered. Concretely, the first thread is currently checking in the node to which the version history is attached while the second thread walks this same version history by means of a ""self-built"" iterator, which just accesses the successors of each version to get the ""next"" to visit.

At a certain point the second point may encounter an ItemNotFoundException with a stack trace similar to this:

javax.jcr.ItemNotFoundException: c9bd405b-dff4-46ef-845c-d98e073e473a
        at org.apache.jackrabbit.core.ItemManager.createItemInstance(ItemManager.java:354)
        at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:230)
        at org.apache.jackrabbit.core.SessionImpl.getNodeByUUID(SessionImpl.java:494)
        at org.apache.jackrabbit.core.version.VersionImpl.getSuccessors(VersionImpl.java:86)
        ....

It seems that the first thread has already filled the successor of the version, while the node is not yet accessible by the createItemInstance method.

This bug seems to not be enforcible, but it is easily reproducible."
1,"NPE w/ AbstractPoolEntry.openjava.lang.NullPointerException
    at org.apache.http.impl.conn.AbstractPoolEntry.open(AbstractPoolEntry.java:171)
    at org.apache.http.impl.conn.AbstractPooledConnAdapter.open(AbstractPooledConnAdapter.java:119)
    at org.apache.http.impl.client.DefaultClientRequestDirector.execute(DefaultClientRequestDirector.java:309)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:501)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:456)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:422)
    at com.limegroup.gnutella.http.DefaultHttpExecutor.performRequest(DefaultHttpExecutor.java:97)
    at com.limegroup.gnutella.http.DefaultHttpExecutor.access$000(DefaultHttpExecutor.java:26)
    at com.limegroup.gnutella.http.DefaultHttpExecutor$MultiRequestor.run(DefaultHttpExecutor.java:135)
    at org.limewire.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1006)
    at org.limewire.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:549)
    at java.lang.Thread.run(Unknown Source)

Seeing a lot of these against Alpha4.  Also seeing still the occassional IllegalStateException of:

java.lang.IllegalStateException: Connection already open.
    at org.apache.http.impl.conn.AbstractPoolEntry.open(AbstractPoolEntry.java:150)
    at org.apache.http.impl.conn.AbstractPooledConnAdapter.open(AbstractPooledConnAdapter.java:119)
    at org.apache.http.impl.client.DefaultClientRequestDirector.execute(DefaultClientRequestDirector.java:309)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:501)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:456)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:422)
    at com.limegroup.gnutella.http.DefaultHttpExecutor.performRequest(DefaultHttpExecutor.java:97)
    at com.limegroup.gnutella.http.DefaultHttpExecutor.access$000(DefaultHttpExecutor.java:26)
    at com.limegroup.gnutella.http.DefaultHttpExecutor$MultiRequestor.run(DefaultHttpExecutor.java:135)
    at org.limewire.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1006)
    at org.limewire.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:549)
    at java.lang.Thread.run(Unknown Source)
"
1,"Merging between workspaces failsI have setup 2 workspaces in Jackrabbit.  I have a preview and a production
workspace.  These workspaces keep a tree of menu nodes that can have content
associated to those menus.  Each node is of type nt:unstructured and has
mixin types of versionable, lockable, and referenceable.

In our system you are only allowed to edit nodes in the preview workspace.
So what I do is when you go to edit a node we check it out, allow for edits,
then check it in.  This creates a new version on the node.  Then we merge
the node up to the production workspace.  All nodes in the production
workspace are always checked in and not locked.

When I go to do a merge I run into problems when I try to merge a node that
has children.  Lets say I have node A with children B and C.  These all have
the same node types as stated above.  I make a change to a property in Node
A in the preview workspace and now want to merge it into the production
workspace (where it exists already).  Here is the code that is run:

Node destNode = destSession.getNodeByUUID(getUUID());
NodeIterator ni = destNode.merge(""preview"", true);

Now this fails in the ItemImpl.internalRemove() method with a
VersionException of cannot remove a child of a checked-in node.  Here is the
trace for the error:
at org.apache.jackrabbit.core.ItemImpl.internalRemove(ItemImpl.java:848)
at org.apache.jackrabbit.core.NodeImpl.internalMerge(NodeImpl.java:3693)
at org.apache.jackrabbit.core.NodeImpl.internalMerge(NodeImpl.java:3587)
at org.apache.jackrabbit.core.NodeImpl.merge(NodeImpl.java:3003)

Now if I understand correctly when doing a merge the node that you are
trying to merge to needs to be older then the source node and the
destination node cannot be checked out (NodeImpl.doMergeTest() is where I
figured that out).  But then when I step through further in the merge in
NodeImpl it gets all the nodes of the src node and retrieves the same
children in the destination workspace and then tries to remove those
destination children but it can't remove those children b/c the parent node
(which is node A in the production workspace) is not checked out, but
according to the mergeTest it can't be checked out or the merge won't even
begin."
1,"HttpConnection.isResponseAvailable() calls setSoTimeout() but does not catch IOExceptionHttpConnection.isResponseAvailable() can throw an IOException when setting the
soTimeout but should probably just return false in this case.

<http://marc.theaimsgroup.com/?t=106268485100002&r=1&w=2>"
1,"Bundle persistence managers node id key store/load is not symertric on MySql causing NoSuchItemState Exceptions It looks like the binary values read back from MySql where the UUID contains 0's is not the same as that generated from the UUID getRawBytes() call. As result, you can store a node with the UUID that has 0's but its never found when read back. This therefore causes corruption in random places when certain UUIDs are generated.

Test Case: 

I've attached 2 files. One causes node corruption when imported, the other does not.
The only difference is that I removed any 0 values from the problem UUID in the file that causes corruption.

As Stefan pointed out, I had manipulated the test case to use standard nt types when in fact I should have provided the following info (sorry Stefan) e.g. the test folder types are referencable hence the jcr:uuid allocation

[acme:Folder] > nt:folder, mix:referenceable

If I import causes_corruption.xml and then attempt to ""ls"" AclObjectIdentities then loadBundle() returns null for the UUID 

a55f3f6b-a909-4e8d-b65a-93002ced0920 which in bytes is [-91, 95, 63, 107, -87, 9, 78, -115, -74, 90, -109, 0, 44, -19, 9, 32]

If I import works.xml then ""ls"" works fine for the same node as I've manually changed the UUID to replace 0s with 1s in the last section.

a55f3f6b-a909-4e8d-b65a-93112ced1921 [-91, 95, 63, 107, -87, 9, 78, -115, -74, 90, -109, 17, 44, -19, 25, 33]


Testing shows this issue highlights a problem with the Bundle persistence manager and MySqls method of handling BINARY columns.
The solution looks to be to replace BINARY(16) with VARBINARY(16). Quoting from http://dev.mysql.com/doc/refman/5.0/en/binary-varbinary.html...
""If the value retrieved must be the same as the value specified for storage with no padding, it might be preferable to use VARBINARY or one of the BLOB data types instead.""
A review of our logs shows that all of the corruption we've seen has related to nodes with UUIDs including 0's.

* Shall I log a JIRA ticket for this?
* Anyone see any issues with this fix?


In the following example you can see I'm showing all bundles in the ""test1"" workspace.

mysql> select hex(node_id) from test1_bundle;
+----------------------------------+
| hex(node_id)                     |
+----------------------------------+
| 28126C3E36A0471D9CDC5AC423BAC9C5 |
| A55F3F6BA9094E8DB65A93002CED0920 |
| CAFEBABECAFEBABECAFEBABECAFEBABE |
| D638EACCDEB641FD8868804C8ECEFFFD |
| DEADBEEFCAFEBABECAFEBABECAFEBABE |
+----------------------------------+
5 rows in set (0.00 sec)

...but a select using the same UUID hex value returns no rows.

mysql>  select node_id from test1_bundle where 
mysql> unhex('A55F3F6BA9094E8DB65A93002CED0920') = node_id;
Empty set (0.00 sec)

I've then created a new ""test3"" workspace which I modified to use varbinary instead of binary with:

alter table test3_bundle modify NODE_ID varbinary(16); alter table test3_refs modify NODE_ID varbinary(16);

My import test case now no longer fails and the following query proves that query operations, after a store, return rows as expected.

mysql>  select node_id from test3_bundle where 
mysql> unhex('A55F3F6BA9094E8DB65A93002CED0920') = node_id;
+------------------+
| node_id          |
+--------Z ,--  |
+------------------+
1 row in set (0.00 sec)

mysql> desc test3_bundle;
ERROR 2006 (HY000): MySQL server has gone away No connection. Trying to reconnect...
Connection id:    7116
Current database: mmptest

+-------------+---------------+------+-----+---------+-------+
| Field       | Type          | Null | Key | Default | Extra |
+-------------+---------------+------+-----+---------+-------+
| NODE_ID     | varbinary(16) | YES  | UNI | NULL    |       |
| BUNDLE_DATA | longblob      | NO   |     |         |       |
+-------------+---------------+------+-----+---------+-------+
2 rows in set (0.00 sec)


mysql>  alter table test3_bundle modify NODE_ID varbinary(16);
Query OK, 2 rows affected (0.00 sec)
Records: 2  Duplicates: 0  Warnings: 0

"
1,"AttributeSource can have an invalid computed stateIf you work a tokenstream, consume it, then reuse it and add an attribute to it, the computed state is wrong.
thus for example, clearAttributes() will not actually clear the attribute added.

So in some situations, addAttribute is not actually clearing the computed state when it should.
"
1,"Intermittent failure in TestFieldCacheTermsFilter.testMissingTermsRunning tests in while(1) I hit this:

{noformat}

NOTE: reproduce with: ant test -Dtestcase=TestFieldCacheTermsFilter -Dtestmethod=testMissingTerms -Dtests.seed=-1046382732738729184:5855929314778232889

1) testMissingTerms(org.apache.lucene.search.TestFieldCacheTermsFilter)
java.lang.AssertionError: Must match 1 expected:<1> but was:<0>
	at org.junit.Assert.fail(Assert.java:91)
	at org.junit.Assert.failNotEquals(Assert.java:645)
	at org.junit.Assert.assertEquals(Assert.java:126)
	at org.junit.Assert.assertEquals(Assert.java:470)
	at org.apache.lucene.search.TestFieldCacheTermsFilter.testMissingTerms(TestFieldCacheTermsFilter.java:63)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1214)
	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1146)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:24)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:157)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:136)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at org.junit.runner.JUnitCore.runMain(JUnitCore.java:98)
	at org.junit.runner.JUnitCore.runMainAndExit(JUnitCore.java:53)
	at org.junit.runner.JUnitCore.main(JUnitCore.java:45)
{noformat}

Unfortunately the seed doesn't [consistently] repro for me..."
1,"BooleanQuery.hashCode and equals ignore isCoordDisabledBooleanQuery.isCoordDisabled() is not considered by BooleanQuery's hashCode() or equals() methods ... this can cause serious badness to happen when caching BooleanQueries.

bug traces back to at least 1.9"
1,"some valid email address characters not correctly recognizedthe EMAIL expression in StandardTokenizerImpl.jflex misses some unusual but valid characters in the left-hand-side of the email address. This causes an address to be broken into several tokens, for example:

somename+site@gmail.com gets broken into ""somename"" and ""site@gmail.com""
husband&wife@talktalk.net gets broken into ""husband"" and ""wife@talktalk.net""

These seem to be occurring more often. The first seems to be because of an anti-spam trick you can use with google (see: http://labnol.blogspot.com/2007/08/gmail-plus-smart-trick-to-find-block.html). I see the second in several domains but a disproportionate amount are from talktalk.net, so I expect it's a signup suggestion from the service.

Perhaps a fix would be to change line 102 of StandardTokenizerImpl.jflex from:
EMAIL      =  {ALPHANUM} (("".""|""-""|""_"") {ALPHANUM})* ""@"" {ALPHANUM} (("".""|""-"") {ALPHANUM})+

to 

EMAIL      =  {ALPHANUM} (("".""|""-""|""_""|""+""|""&"") {ALPHANUM})* ""@"" {ALPHANUM} (("".""|""-"") {ALPHANUM})+

I'm aware that the StandardTokenizer is meant to be more of a basic implementation rather than an implementation the full standard, but it is quite useful in places and hopefully this would improve it slightly."
1,SPI: Description of Path.isDescendantOf(Path) Description of Path.isDescendantOf lists different reasons for IllegalArgumentException and RepositoryException than isAncestorOf... this is obviously a mistake.
1,"NullPointerException on DelegatingObservationDispatcher cause by parameter null on call : createEventStateCollection(null)There is a NullPointerException when jackrabbit try to synchronise its indexes :
22 janv. 2009 09:53:56 INFO  [ClusterNode] - Processing revision: 4485
22 janv. 2009 09:53:56 ERROR [ClusterNode] - Unexpected error while syncing of journal: null
java.lang.NullPointerException
        at org.apache.jackrabbit.core.observation.DelegatingObservationDispatcher.createEventStateCollection(DelegatingObservationDispatcher.java:80)
        at org.apache.jackrabbit.core.version.VersionManagerImpl$DynamicESCFactory.createEventStateCollection(VersionManagerImpl.java:556)
        at org.apache.jackrabbit.core.version.VersionManagerImpl.externalUpdate(VersionManagerImpl.java:500)
        at org.apache.jackrabbit.core.cluster.ClusterNode.process(ClusterNode.java:853)
        at org.apache.jackrabbit.core.cluster.ChangeLogRecord.process(ChangeLogRecord.java:457)
        at org.apache.jackrabbit.core.cluster.ClusterNode.consume(ClusterNode.java:799)
        at org.apache.jackrabbit.core.journal.AbstractJournal.doSync(AbstractJournal.java:213)
        at org.apache.jackrabbit.core.journal.AbstractJournal.sync(AbstractJournal.java:188)
        at org.apache.jackrabbit.core.cluster.ClusterNode.sync(ClusterNode.java:315)
        at org.apache.jackrabbit.core.cluster.ClusterNode.run(ClusterNode.java:286)
        at java.lang.Thread.run(Thread.java:595)

In fact the method createEventStateCollection() of DelegatingObservationDispatcher is called by the VersionManagerImpl with session parameter as null...

DelegatingObservationDispatcher:

 public EventStateCollection createEventStateCollection(SessionImpl session,
                                                           Path pathPrefix) {
        String userData = null;
        try {
            userData = ((ObservationManagerImpl) session.getWorkspace().getObservationManager()).getUserData();
        } catch (RepositoryException e) {
            // should never happen because this
            // implementation supports observation
        }
        return new EventStateCollection(this, session, pathPrefix, userData);
    }

VersionManagerImpl$DynamicESCFactory :
 public EventStateCollection createEventStateCollection(SessionImpl source) {
            return obsMgr.createEventStateCollection(source, VERSION_STORAGE_PATH);
        }

VersionManagerImpl :
public void externalUpdate(ChangeLog changes, List events,
                               long timestamp, String userData)
            throws RepositoryException {
        EventStateCollection esc = getEscFactory().createEventStateCollection(null);
        esc.addAll(events);
        esc.setTimestamp(timestamp);
        esc.setUserData(userData);

        sharedStateMgr.externalUpdate(changes, esc);
    }"
1,"method.getURI()  returns escaped URIs but it shouldn'tHi guys,

Please, consider the following imaginary and simplified code:


URI u = new URI(""http://some.host.com/%41.html"", true);
HttpClient httpClient = new HttpClient();
GetMethod method = new GetMethod();
method.setURI(u);
URI u2 = method.getURI();

System.out.println(""1. "" + u);
System.out.println(""2. "" + new String(u.getRawURI()));
System.out.println(""3. "" + u.getURI());
System.out.println(""4. "" + u2);
System.out.println(""5. "" + new String(u2.getRawURI()));
System.out.println(""6. "" + u2.getURI());


The result that you'll get is:

1. http://some.host.com/%41.html
2. http://some.host.com/%41.html
3. http://some.host.com/A.html
4. http://some.host.com/%2541.html
5. http://some.host.com/%2541.html
6. http://some.host.com/%41.html


You can see that for lines 4, 5, and 6, the URI suddenly gets escaped (the 
percent sign gets converted to %25).

Why is that? Am I doing something wrong? Is this the desired behaviour? I would 
have expected to get the SAME URI back, without any escaping.

Besides, I have another question:

After executing a method -- httpClient.executeMethod(method) -- what will 
method.getURI() return? The URI *after* all redirections or the original URI? 
It seems I get the URI *after* the redirections, which is fine, but the 
documentation doesn't say that. It only explicitly says that the getPath() 
method has that behaviour.

Best regards and thanks,
Bisser"
1,"RepositoryImpl.acquireRepositoryLock() fails to detect that the file lock is already held by the current processwith java 1.4 and 1.5 on a *nix-based platform it is possible to (concurrently) instantiate 
more than one repository instance in the same jvm based on same/identical configurations.

this is a critical issue since it might lead to data corruption.

the issue only exists with java versions prior to 1.6 and *nix-based platforms (only verified
on mac os-x 10.4).

note that the issue does not exist when the file lock is held by another jvm.

 code snippet to reproduce the issue:

            Repository rep1 = new TransientRepository();
            Session s1 = rep1.login(new SimpleCredentials(""johndoe"", """".toCharArray()));
            Repository rep2 = new TransientRepository();
            Session s2 = rep2.login(new SimpleCredentials(""johndoe"", """".toCharArray()));


the root problem is the incorrect behavior of java.nio.channels.FileChannel#tryLock()
which is demonstrated by the following code snippet:

            try {
                FileLock fl1 = new FileOutputStream(""foo"").getChannel().tryLock();
                System.out.println(""1st lock: "" + fl1);
                FileLock fl2 = new FileOutputStream(""foo"").getChannel().tryLock();
                System.out.println(""2nd lock: "" + fl2);
            } catch (Throwable t) {
                t.printStackTrace();
            }

"
1,"HttpClient loops endlessly while trying to retrieve status lineWhen fed with the wrong URL, for example http://localhost:19/ (chargen port),
HttpClient will loop endlessly while attempting to read the status line.

This is caused by a bug in HttpMethodBase.readStatusLine(HttpState, HttpConnection)

(while loop without any exceptional abort condition).

wire log excerpt:

2003/11/10 12:33:04:085 CET [DEBUG] HttpMethodDirector - -Execute loop try 1
2003/11/10 12:33:04:312 CET [DEBUG] wire - ->> ""GET / HTTP/1.1[\r][\n]""
2003/11/10 12:33:04:351 CET [DEBUG] HttpMethodBase - -Adding Host request header
2003/11/10 12:33:04:532 CET [DEBUG] wire - ->> ""User-Agent: Jakarta
Commons-HttpClient[\r][\n]""
2003/11/10 12:33:04:554 CET [DEBUG] wire - ->> ""Host: localhost:19[\r][\n]""
2003/11/10 12:33:04:559 CET [DEBUG] wire - ->> ""[\r][\n]""
2003/11/10 12:33:04:639 CET [DEBUG] wire - -<<
""!""#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefgh[\r][\n]""
2003/11/10 12:33:04:669 CET [DEBUG] wire - -<<
""""#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghi[\r][\n]""
2003/11/10 12:33:04:673 CET [DEBUG] wire - -<<
""#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghij[\r][\n]""
2003/11/10 12:33:04:692 CET [DEBUG] wire - -<<
""$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijk[\r][\n]""
2003/11/10 12:33:04:698 CET [DEBUG] wire - -<<
""%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijkl[\r][\n]""
2003/11/10 12:33:04:703 CET [DEBUG] wire - -<<
""&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklm[\r][\n]""
<snip>"
1,"Host request header does not contain portThe Host request header is always added with just the hostname used for the 
connection.  If the port is different than 80 it needs to be included as well, 
with a colon separating it from the hostname.  This problem is especially 
apparent when you use the httpclient to connect to tomcat 4 and then use 
HttpUtils to create a full URL representing the request.  HttpUtils pulls the 
host and port from the Host header.  When commons-httpclient is used HttpUtils 
never includes the port since it was never in the Host header."
1,"RMI published Repository using the jcr-rmi library gets lost over timeThe jcr-server/webapp project contains a servlet - RepositoryStartupServlet - which may be used in a web app to start a repository and optionally register the repository with JNDI and RMI. To register the repository with JNDI, the jcr-rmi library is used to create a Remote repository instance, which is registered with the RMI registry. Inside the RMI implementation mechanisms based on stub classes created by the RMI compiler are created to make the remote repository available remotely. This includes creating a object table to map remote references to local objects. This table stores references to the local object as weak references to support distributed garbage collection.

Over time, it may now be that this remote repository instance is actually collected and the object table cannot access it anymore thus preventing the repository from being accessed remotely. To prevent this from happening, the RepositoryStartupServlet must keep a strong reference to the remote repository and drop this reference when the servlet is destroyed and the repository unregistered.

*NOTE:* This is an issue to all long running applications which publish repository instances over RMI using the jcr-rmi library."
1,"jcr:baseVersion is not updated when the base version is removed from the version history
        Session s1 = repo.login(new SimpleCredentials(""user1"", ""pwd1"".toCharArray()));
        Node root1 = s1.getRootNode() ;
        Node test1 = root1.addNode(""test"") ;
        test1.addMixin(""mix:versionable"");
        s1.save() ;
        System.out.println(test1.getProperty(""jcr:baseVersion"").getValue().getString()) ;
        test1.checkin() ;
        System.out.println(test1.getProperty(""jcr:baseVersion"").getValue().getString()) ;
        test1.getVersionHistory().removeVersion(""1.0"") ;
        // the base version wasn't updated :(
        System.out.println(test1.getProperty(""jcr:baseVersion"").getValue().getString()) ;
        // the next line throws ItemNotFoundException :(
        test1.getBaseVersion() ;

javax.jcr.ItemNotFoundException: c33bf049-c7e1-4b34-968a-63ff1b1113b0
	at org.apache.jackrabbit.core.ItemManager.createItemInstance(ItemManager.java:498)
	at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:349)
	at org.apache.jackrabbit.core.PropertyImpl.getNode(PropertyImpl.java:642)
	at org.apache.jackrabbit.core.NodeImpl.getBaseVersion(NodeImpl.java:2960)
	at org.apache.jackrabbit.core.RemoveVersionTest.main(RemoveVersionTest.java:56)


"
1,"cache does not honor must-revalidate or proxy-revalidate Cache-Control directiveshttp://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.4

There are a couple of missed requirements here regarding must-revalidate and proxy-revalidate (which applies only to shared caches).
1. When a cache entry with this directive is revalidated, it must be an end-to-end revalidation (meaning it must include 'max-age=0' on the request).
2. If the revalidation with the origin fails, the cache MUST NOT return a stale entry and MUST return a 504 response.
"
1,"NPE in RequestProxyAuthentication on AndroidGot a NPE backtrace in RequestProxyAuthentication.process(). 

HttpRoute route = conn.getRoute();
        if (route.isTunnelled()) {      <= line 88, NPE here
            return;
        }

There's no null check on the returned route although getRoute() can return null.
I guess it's not supposed to happen.

In the httpclient code, there's a few more calls to getRoute() without a null check on the returned route.


java.lang.NullPointerException
at com.bubblesoft.org.apache.http.client.protocol.RequestProxyAuthentication.process(SourceFile:88)
at com.bubblesoft.org.apache.http.protocol.ImmutableHttpProcessor.process(SourceFile:108)
at com.bubblesoft.org.apache.http.protocol.HttpRequestExecutor.preProcess(SourceFile:174)
at com.bubblesoft.org.apache.http.impl.client.DefaultRequestDirector.execute(SourceFile:457)
at com.bubblesoft.org.apache.http.impl.client.AbstractHttpClient.createHttpProcessor(SourceFile:821)
                                                                 execute
at com.bubblesoft.org.apache.http.impl.client.AbstractHttpClient.createHttpProcessor(SourceFile:755)
                                                                 execute
at com.bubblesoft.org.apache.http.impl.client.AbstractHttpClient.createHttpProcessor(SourceFile:733)
                                                                 execute
"
1,"VarDerefBytesImpl doc values prefix length may fall across two pagesThe VarDerefBytesImpl doc values encodes the unique byte[] with prefix (1 or 2 bytes) first, followed by bytes, so that it can use PagedBytes.fillSliceWithPrefix.

It does this itself rather than using PagedBytes.copyUsingLengthPrefix...

The problem is, it can write an invalid 2 byte prefix spanning two blocks (ie, last byte of block N and first byte of block N+1), which fillSliceWithPrefix won't decode correctly.

"
1,"Clustering is broken due to duplicated CachingPersistenceManager interfaceThere are now two interfaces CachingPersistenceManager in the packages org.apache.jackrabbit.core.persistence.bundle and org.apache.jackrabbit.core.persistence.pool. A persistence manager that implements the ..bundle... interface doesn't receive the onExternalUpdate events that are required for clustering to work.

I will move this interface to the package org.apache.jackrabbit.core.persistence and remove the second implementation.

This change has no affect to backward compatibility, because anyway there were many breaking changes in the past (NodeId / UUID for example).
"
1,"sysview export/import of multivalue properties seems not to workthe sysview export of multivalue properties does not differ from the export of singlevalue properties.

if a mv-property contains only 1 value, how can the import find the correct property-def?

currently, it throws: 
  javax.jcr.nodetype.ConstraintViolationException: no matching property definition found for <propertyname>"
1,"SO_TIMEOUT is not set on a request levelThe scenario is as follows: I'm doing two consecutive requests to the same host, using a multi-threaded (or thread safe) connection pool manager. The first invocation has a timeout of 10s and the second has a timeout of 30s. 

In version 3.1 of HttpClient all works well, but in 4.0 I get a timeout exception in the second request, after ~10 seconds, which means the first timeout is used.

Looking at the code, I see that in version 3.1, the HttpMethodDirector.executeWithRetry() method invokes a method named applyConnectionParams() that took care of setting the timeout taken from the request on the socket. 

But in version 4.0, the only place I see the timeout is set on the socket is when DefaultRequestDirector.execute(HttpHost, HttpRequest, HttpContext) opens a connection using the managedConn.open() method. Since the connection is reused between the requests, the second request uses a socket with a timeout of the first request.
"
1,"StaleItemStateException with distributed transactionsThere seams to be a serious bug in jackrabbit when used in distributed transactions. It does not occur with local transactions! And it seams to be related to JCR-566.

There are 2 scenarios where a StaleItemStateException occurs reproducible that causes transactions to fail. All my operations (implemented in a custom ServiceBean) such as setProperty() or deleteNode() run in separate transactions. The transactions are configured through Spring Annotations (@Transactional).

Scenario A (setProperty):
(1) multiple setProperty() with same property name on the same node (newly created or already existent)
=> With the 3. setProperty() (and sometimes also the 5.), a StaleItemStateException for the property state is raised when the transaction is commited. Following setProperty invocations will not fail!

Scenario B (deleteNode):
(1) iterate 10 times:
(1.1) create new node n and a subnode for n
(1.2) delete node n
=> Deletion of node n raises a StaleItemStateException for node n in iteration 1, 3 and (6 or 7), when the related transaction is commited. Following deletions of node n will also fail with a predictable pattern.

The Exception trace for scenario A (it's the same for scenario B, with one difference: StaleItemStateException is raised for the node and not for the property):

org.springframework.transaction.UnexpectedRollbackException: JTA transaction unexpectedly rolled back (maybe due to a timeout); nested exception is javax.transaction.RollbackException: Error during one-phase commit
	at org.springframework.transaction.jta.JtaTransactionManager.doCommit(JtaTransactionManager.java:1031)
	at org.springframework.transaction.support.AbstractPlatformTransactionManager.processCommit(AbstractPlatformTransactionManager.java:709)
	at org.springframework.transaction.support.AbstractPlatformTransactionManager.commit(AbstractPlatformTransactionManager.java:678)
	at org.springframework.transaction.interceptor.TransactionAspectSupport.commitTransactionAfterReturning(TransactionAspectSupport.java:321)
	at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:116)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:204)
	at $Proxy9.setNodeProperty(Unknown Source)
	at de.zeb.control.prototype.jrTxBug.test.TestJackrabbitTxBug.testTransactionBug001(TestJackrabbitTxBug.java:97)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at org.testng.internal.MethodHelper.invokeMethod(MethodHelper.java:580)
	at org.testng.internal.Invoker.invokeMethod(Invoker.java:478)
	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:607)
	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:874)
	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:125)
	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109)
	at org.testng.TestRunner.runWorkers(TestRunner.java:689)
	at org.testng.TestRunner.privateRun(TestRunner.java:566)
	at org.testng.TestRunner.run(TestRunner.java:466)
	at org.testng.SuiteRunner.runTest(SuiteRunner.java:301)
	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:296)
	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:276)
	at org.testng.SuiteRunner.run(SuiteRunner.java:191)
	at org.testng.TestNG.createAndRunSuiteRunners(TestNG.java:808)
	at org.testng.TestNG.runSuitesLocally(TestNG.java:776)
	at org.testng.TestNG.run(TestNG.java:701)
	at org.testng.remote.RemoteTestNG.run(RemoteTestNG.java:73)
	at org.testng.remote.RemoteTestNG.main(RemoteTestNG.java:124)
Caused by: javax.transaction.RollbackException: Error during one-phase commit
	at org.apache.geronimo.transaction.manager.TransactionImpl.commit(TransactionImpl.java:281)
	at org.apache.geronimo.transaction.manager.TransactionManagerImpl.commit(TransactionManagerImpl.java:143)
	at org.apache.geronimo.transaction.context.InheritableTransactionContext.complete(InheritableTransactionContext.java:196)
	at org.apache.geronimo.transaction.context.InheritableTransactionContext.commit(InheritableTransactionContext.java:146)
	at org.apache.geronimo.transaction.context.OnlineUserTransaction.commit(OnlineUserTransaction.java:80)
	at org.jencks.factory.UserTransactionFactoryBean$GeronimoUserTransaction.commit(UserTransactionFactoryBean.java:118)
	at org.springframework.transaction.jta.JtaTransactionManager.doCommit(JtaTransactionManager.java:1028)
	... 30 more
Caused by: javax.transaction.xa.XAException
	at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:155)
	at org.apache.jackrabbit.core.XASessionImpl.commit(XASessionImpl.java:337)
	at org.apache.jackrabbit.jca.TransactionBoundXAResource.commit(TransactionBoundXAResource.java:39)
	at org.apache.geronimo.transaction.manager.WrapperNamedXAResource.commit(WrapperNamedXAResource.java:47)
	at org.apache.geronimo.transaction.manager.TransactionImpl.commit(TransactionImpl.java:272)
	... 36 more
Caused by: org.apache.jackrabbit.core.TransactionException: Unable to prepare transaction.
	at org.apache.jackrabbit.core.state.XAItemStateManager.prepare(XAItemStateManager.java:150)
	at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:138)
	... 40 more
Caused by: org.apache.jackrabbit.core.state.StaleItemStateException: bef3c056-bc91-4195-a35c-aa184182b5ad/{}TEST_PROPERTY has been modified externally
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:620)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:843)
	at org.apache.jackrabbit.core.state.XAItemStateManager.prepare(XAItemStateManager.java:144)
	... 41 more


When debugging into jackrabbit you will see, that the cause of the StaleItemStateException is, that the local state und the overlayed state differ in the value of the 'modCount' attribute: modCount of local state is lower than modCount of overlayed state. Perhaps its a state caching problem...
	
I'm attaching a simple java application configured with maven and ready to run standalone. The JCA container of JBoss is therefore replaced with jencks in order to support distributed transactions. The configured repository uses the InMemPersistenceManager. Both scenarios are implemented in a TestNG - test, that catches the occuring TransactionExceptions and prints out the stacktrace. Therefore you will see the exceptions, but the tests will not fail."
1,BitVector never skips fully populated bytes when writing ClearedDgapsWhen writing cleared DGaps in BitVector we compare a byte against 0xFF (255) yet the byte is casted into an int (-1) and the comparison will never succeed. We should mask the byte with 0xFF before comparing or compare against -1
1,"invalid groupid for tm-extractors in textfilters projectgroupid for tm-extractors should be ""org.textmining"" and not ""textmining"".
The dependency with the correct groupid is available on ibiblio"
1,"DocViewSaxEventGenerator may generate non-NS-wellformed XMLThe XML serialization code relies on the fact that all required prefix-to-uri mappings are known beforehand (actually, when serializing the root node). So there's an assumption that the permanent namespace registry will never change during serialization, which may be incorrect when another client adds namespace registrations while the XML export is in progress.

To fix this, ""addNamespacePrefixes"" should ensure that namespace declarations have been written for all prefixes used on the current node (node name + properties), potentially going back to the namespace resolver when needed.

(Should there be consensus for that change I'm happy to give it a try)"
1,"Proxy NTLM Authentication  Redirecting to different address fails saying Proxy Auth Required.The issue has been discussed in,
http://www.nabble.com/redirect-fails-when-NTLM-authentication-is-used-for-proxy-tt23867531.html

This was found in http client 3.1 release,  where NTLM proxy authentication is must and the server ask the redirect to a new url, in this case, when redirecting, the earlier proxy auth status is not cleared, so, it does not do proxy authentication for the new URL and hence fails.

Target Host Authenticaiton NTLM authentication - redirect also had problem and fixed as said,
http://issues.apache.org/jira/browse/HTTPCLIENT-211
Proxy Authentication - redirect has to be fixed, 

The wire logs for the release https://repository.apache.org/content/repositories/snapshots/org/apache/httpcomponents/httpclient/4.0-beta3-SNAPSHOT/
is given below,

[DEBUG] wire - >> ""GET http://verisign.com HTTP/1.1[EOL]""
[DEBUG] wire - >> ""Host: verisign.com[EOL]""
[DEBUG] wire - >> ""Proxy-Connection: Keep-Alive[EOL]""
[DEBUG] wire - >> ""User-Agent: Apache-HttpClient/UNAVAILABLE (java 1.5)[EOL]""
[DEBUG] wire - >> ""[EOL]""
[DEBUG] wire - << ""HTTP/1.1 407 Proxy Authentication Required ( The ISA Server requires authorization to fulfill the request. Access to the Web Proxy filter is denied.  )[EOL]""
[DEBUG] wire - << ""Via: 1.1 lab1[EOL]""
[DEBUG] wire - << ""Proxy-Authenticate: Negotiate[EOL]""
[DEBUG] wire - << ""Proxy-Authenticate: Kerberos[EOL]""
[DEBUG] wire - << ""Proxy-Authenticate: NTLM[EOL]""
[DEBUG] wire - << ""Proxy-Authenticate: Basic realm=""lab1.""[EOL]""
[DEBUG] wire - << ""Connection: Keep-Alive[EOL]""
[DEBUG] wire - << ""Proxy-Connection: Keep-Alive[EOL]""
[DEBUG] wire - << ""Pragma: no-cache[EOL]""
[DEBUG] wire - << ""Cache-Control: no-cache[EOL]""
[DEBUG] wire - << ""Content-Type: text/html[EOL]""
[DEBUG] wire - << ""Content-Length: 4107  [EOL]""
[DEBUG] wire - << ""[EOL]""
[DEBUG] wire - << ""<!DOCTYPE HTML PUBLIC ""-//W3C//DTD HTML 4.0 Transitional//EN"">[\r][\n]""
[DEBUG] wire - << ""<HTML><HEAD><TITLE>Error Message</TITLE>[\r][\n]""
[DEBUG] wire - << ""<META http-equiv=Content-Type content=""text/html; charset=UTF-8"">[\r][\n]""
[DEBUG] wire - << ""<STYLE id=L_default_1>A {[\r][\n]""
[DEBUG] wire - << ""[0x9]FONT-WEIGHT: bold; FONT-SIZE: 10pt; COLOR: #005a80; FONT-FAMILY: tahoma[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << ""A:hover {[\r][\n]""
[DEBUG] wire - << ""[0x9]FONT-WEIGHT: bold; FONT-SIZE: 10pt; COLOR: #0d3372; FONT-FAMILY: tahoma[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << ""TD {[\r][\n]""
[DEBUG] wire - << ""[0x9]FONT-SIZE: 8pt; FONT-FAMILY: tahoma[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << ""TD.titleBorder {[\r][\n]""
[DEBUG] wire - << ""[0x9]BORDER-RIGHT: #955319 1px solid; BORDER-TOP: #955319 1px solid; PADDING-LEFT: 8px; FONT-WEIGHT: bold; FONT-SIZE: 12pt; VERTICAL-ALIGN: middle; BORDER-LEFT: #955319 0px solid; COLOR: #955319; BORDER-BOTTOM: #955319 1px solid; FONT-FAMILY: tahoma; HEIGHT: 35px; BACKGROUND-COLOR: #d2b87a; TEXT-ALIGN: left[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << ""TD.titleBorder_x {[\r][\n]""
[DEBUG] wire - << ""[0x9]BORDER-RIGHT: #955319 0px solid; BORDER-TOP: #955319 1px solid; PADDING-LEFT: 8px; FONT-WEIGHT: bold; FONT-SIZE: 12pt; VERTICAL-ALIGN: middle; BORDER-LEFT: #955319 1px solid; COLOR: #978c79; BORDER-BOTTOM: #955319 1px solid; FONT-FAMILY: tahoma; HEIGHT: 35px; BACKGROUND-COLOR: #d2b87a; TEXT-ALIGN: left[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << "".TitleDescription {[\r][\n]""
[DEBUG] wire - << ""[0x9]FONT-WEIGHT: bold; FONT-SIZE: 12pt; COLOR: black; FONT-FAMILY: tahoma[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << ""SPAN.explain {[\r][\n]""
[DEBUG] wire - << ""[0x9]FONT-WEIGHT: normal; FONT-SIZE: 10pt; COLOR: #934225[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << ""SPAN.TryThings {[\r][\n]""
[DEBUG] wire - << ""[0x9]FONT-WEIGHT: normal; FONT-SIZE: 10pt; COLOR: #934225[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << "".TryList {[\r][\n]""
[DEBUG] wire - << ""[0x9]MARGIN-TOP: 5px; FONT-WEIGHT: normal; FONT-SIZE: 8pt; COLOR: black; FONT-FAMILY: tahoma[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << "".X {[\r][\n]""
[DEBUG] wire - << ""[0x9]BORDER-RIGHT: #955319 1px solid; BORDER-TOP: #955319 1px solid; FONT-WEIGHT: normal; FONT-SIZE: 12pt; BORDER-LEFT: #955319 1px solid; COLOR: #7b3807; BORDER-BOTTOM: #955319 1px solid; FONT-FAMILY: verdana; BACKGROUND-COLOR: #d1c2b4[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << "".adminList {[\r][\n]""
[DEBUG] wire - << ""[0x9]MARGIN-TOP: 2px[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << ""</STYLE>[\r][\n]""
[DEBUG] wire - << ""<META content=""MSHTML 6.00.2800.1170"" name=GENERATOR></HEAD>[\r][\n]""
[DEBUG] wire - << ""<BODY bgColor=#f3f3ed>[\r][\n]""
[DEBUG] wire - << ""<TABLE cellSpacing=0 cellPadding=0 width=""100%"">[\r][\n]""
[DEBUG] wire - << ""  <TBODY>[\r][\n]""
[DEBUG] wire - << ""  <TR>[\r][\n]""
[DEBUG] wire - << ""    <TD class=titleborder_x width=30>[\r][\n]""
[DEBUG] wire - << ""      <TABLE height=25 cellSpacing=2 cellPadding=0 width=25 bgColor=black>[\r][\n]""
[DEBUG] wire - << ""        <TBODY>[\r][\n]""
[DEBUG] wire - << ""        <TR>[\r][\n]""
[DEBUG] wire - << ""          <TD class=x vAlign=center alig""
[DEBUG] wire - << ""n=middle>X</TD>[\r][\n]""
[DEBUG] wire - << ""        </TR>[\r][\n]""
[DEBUG] wire - << ""        </TBODY>[\r][\n]""
[DEBUG] wire - << ""      </TABLE>[\r][\n]""
[DEBUG] wire - << ""    </TD>[\r][\n]""
[DEBUG] wire - << ""    <TD class=titleBorder id=L_default_2>Network Access Message:<SPAN class=TitleDescription> The page cannot be displayed</SPAN> </TD>[\r][\n]""
[DEBUG] wire - << ""  </TR>[\r][\n]""
[DEBUG] wire - << ""  </TBODY>[\r][\n]""
[DEBUG] wire - << ""</TABLE>[\r][\n]""
[DEBUG] wire - << ""[\r][\n]""
[DEBUG] wire - << ""<TABLE id=spacer>[\r][\n]""
[DEBUG] wire - << ""  <TBODY>[\r][\n]""
[DEBUG] wire - << ""  <TR>[\r][\n]""
[DEBUG] wire - << ""    <TD height=10></TD></TR></TBODY></TABLE>[\r][\n]""
[DEBUG] wire - << ""<TABLE width=400>[\r][\n]""
[DEBUG] wire - << ""  <TBODY>[\r][\n]""
[DEBUG] wire - << ""  <TR>[\r][\n]""
[DEBUG] wire - << ""    <TD noWrap width=25></TD>[\r][\n]""
[DEBUG] wire - << ""    <TD width=400><SPAN class=explain><ID id=L_default_3><B>Explanation:</B></ID></SPAN><ID id=L_default_4> There is a problem with the page you are trying to reach and it cannot be displayed. </ID><BR><BR>[\r][\n]""
[DEBUG] wire - << ""    <B><SPAN class=tryThings><ID id=L_default_5><B>Try the following:</B></ID></SPAN></B> [\r][\n]""
[DEBUG] wire - << ""      <UL class=TryList>[\r][\n]""
[DEBUG] wire - << ""        <LI id=L_default_6><B>Refresh page:</B> Search for the page again by clicking the Refresh button. The timeout may have occurred due to Internet congestion.[\r][\n]""
[DEBUG] wire - << ""<LI id=L_default_7><B>Check spelling:</B> Check that you typed the Web page address correctly. The address may have been mistyped.[\r][\n]""
[DEBUG] wire - << ""<LI id=L_default_8><B>Access from a link:</B> If there is a link to the page you are looking for, try accessing the page from that link.[\r][\n]""
[DEBUG] wire - << ""[\r][\n]""
[DEBUG] wire - << ""      </UL>[\r][\n]""
[DEBUG] wire - << ""<ID id=L_default_9>If you are still not able to view the requested page, try contacting your administrator or Helpdesk.</ID> <BR><BR>[\r][\n]""
[DEBUG] wire - << ""    </TD>[\r][\n]""
[DEBUG] wire - << ""  </TR>[\r][\n]""
[DEBUG] wire - << ""  </TBODY>[\r][\n]""
[DEBUG] wire - << ""</TABLE>[\r][\n]""
[DEBUG] wire - << ""[\r][\n]""
[DEBUG] wire - << ""<TABLE id=spacer><TBODY><TR><TD height=15></TD></TR></TBODY></TABLE>[\r][\n]""
[DEBUG] wire - << ""[\r][\n]""
[DEBUG] wire - << ""<TABLE width=400>[\r][\n]""
[DEBUG] wire - << ""  <TBODY>[\r][\n]""
[DEBUG] wire - << ""  <TR>[\r][\n]""
[DEBUG] wire - << ""    <TD noWrap width=25></TD>[\r][\n]""
[DEBUG] wire - << ""    <TD width=400 id=L_default_10><B>Technical Information (for support personnel)</B> [\r][\n]""
[DEBUG] wire - << ""      <UL class=adminList>[\r][\n]""
[DEBUG] wire - << ""        <LI id=L_default_11>Error Code: 407 Proxy Authentication Required. The ISA Server requires authorization to fulfill the request. Access to the Web Proxy filter is denied. (12209)[\r][\n]""
[DEBUG] wire - << ""<LI id=L_default_12>IP Address: x.x.x.x[\r][\n]""
[DEBUG] wire - << ""<LI id=L_default_13>Date: 6/29/2009 11:15:15 AM [GMT][\r][\n]""
[DEBUG] wire - << ""<LI id=L_default_14>Server: lab1[\r][\n]""
[DEBUG] wire - << ""<LI id=L_default_15>Source: proxy[\r][\n]""
[DEBUG] wire - << ""[\r][\n]""
[DEBUG] wire - << ""      </UL>[\r][\n]""
[DEBUG] wire - << ""    </TD>[\r][\n]""
[DEBUG] wire - << ""  </TR>[\r][\n]""
[DEBUG] wire - << ""  </TBODY>[\r][\n]""
[DEBUG] wire - << ""</TABLE>[\r][\n]""
[DEBUG] wire - << ""[\r][\n]""
[DEBUG] wire - << ""</BODY>[\r][\n]""
[DEBUG] wire - << ""</HTML>[\r][\n]""
[DEBUG] wire - << ""[\r][\n]""
[DEBUG] wire - >> ""GET http://verisign.com HTTP/1.1[EOL]""
[DEBUG] wire - >> ""Host: verisign.com[EOL]""
[DEBUG] wire - >> ""Proxy-Connection: Keep-Alive[EOL]""
[DEBUG] wire - >> ""User-Agent: Apache-HttpClient/UNAVAILABLE (java 1.5)[EOL]""
[DEBUG] wire - >> ""Proxy-Authorization: NTLM TlRMTVNTUAABAAAAATIAAAgACAAgAAAADgAOACgAAABNWURPTUFJTkpDSUZTMjMwXzg2Xzkx[EOL]""
[DEBUG] wire - >> ""[EOL]""
[DEBUG] wire - << ""HTTP/1.1 407 Proxy Authentication Required ( Access is denied.  )[EOL]""
[DEBUG] wire - << ""Via: 1.1 lab1[EOL]""
[DEBUG] wire - << ""Proxy-Authenticate: NTLM TlRMTVNTUAACAAAAAAAAADgAAAABAgACqbXrIWnZ3i4AAAAAAAAAAAAAAAA4AAAABQLODgAAAA8=[EOL]""
[DEBUG] wire - << ""Connection: Keep-Alive[EOL]""
[DEBUG] wire - << ""Proxy-Connection: Keep-Alive[EOL]""
[DEBUG] wire - << ""Pragma: no-cache[EOL]""
[DEBUG] wire - << ""Cache-Control: no-cache[EOL]""
[DEBUG] wire - << ""Content-Type: text/html[EOL]""
[DEBUG] wire - << ""Content-Length: 0     [EOL]""
[DEBUG] wire - << ""[EOL]""
[DEBUG] wire - >> ""GET http://verisign.com HTTP/1.1[EOL]""
[DEBUG] wire - >> ""Host: verisign.com[EOL]""
[DEBUG] wire - >> ""Proxy-Connection: Keep-Alive[EOL]""
[DEBUG] wire - >> ""User-Agent: Apache-HttpClient/UNAVAILABLE (java 1.5)[EOL]""
[DEBUG] wire - >> ""Proxy-Authorization: NTLM TlRMTVNTUAADAAAAGAAYAEAAAAAwADAAWAAAABAAEACIAAAAGgAaAJgAAAAcABwAsgAAAAAAAAAAAAAAAQIAAAXLpW40q7jqh7E6FgFnJqy9529ANaSLqfTiwjyF2BrUP9F8ObYOyYsBAQAAAAAAACDgxRg9+skBRt4mUOFFCs0AAAAAAAAAAE0AWQBEAE8ATQBBAEkATgBBAGQAbQBpAG4AaQBzAHQAcgBhAHQAbwByAEoAQwBJAEYAUwAyADMAMABfADgANgBfADkAMQA=[EOL]""
[DEBUG] wire - >> ""[EOL]""
[DEBUG] wire - << ""HTTP/1.1 301 Unknown reason[EOL]""
[DEBUG] wire - << ""Via: 1.1 lab1[EOL]""
[DEBUG] wire - << ""Connection: Keep-Alive[EOL]""
[DEBUG] wire - << ""Proxy-Connection: Keep-Alive[EOL]""
[DEBUG] wire - << ""Content-length: 0[EOL]""
[DEBUG] wire - << ""Date: Mon, 29 Jun 2009 11:16:50 GMT[EOL]""
[DEBUG] wire - << ""Location: http://www.verisign.com/[EOL]""
[DEBUG] wire - << ""Content-type: text/html[EOL]""
[DEBUG] wire - << ""Server: Netscape-Enterprise/4.1[EOL]""
[DEBUG] wire - << ""[EOL]""
[ERROR] RequestProxyAuthentication - Proxy authentication error: Unexpected state: MSG_TYPE3_GENERATED
[DEBUG] wire - >> ""GET http://www.verisign.com/ HTTP/1.1[EOL]""
[DEBUG] wire - >> ""Host: www.verisign.com[EOL]""
[DEBUG] wire - >> ""Proxy-Connection: Keep-Alive[EOL]""
[DEBUG] wire - >> ""User-Agent: Apache-HttpClient/UNAVAILABLE (java 1.5)[EOL]""
[DEBUG] wire - >> ""[EOL]""
[DEBUG] wire - << ""HTTP/1.1 407 Proxy Authentication Required ( The ISA Server requires authorization to fulfill the request. Access to the Web Proxy filter is denied.  )[EOL]""
[DEBUG] wire - << ""Via: 1.1 lab1[EOL]""
[DEBUG] wire - << ""Proxy-Authenticate: Negotiate[EOL]""
[DEBUG] wire - << ""Proxy-Authenticate: Kerberos[EOL]""
[DEBUG] wire - << ""Proxy-Authenticate: NTLM[EOL]""
[DEBUG] wire - << ""Proxy-Authenticate: Basic realm=""lab1.""[EOL]""
[DEBUG] wire - << ""Connection: Keep-Alive[EOL]""
[DEBUG] wire - << ""Proxy-Connection: Keep-Alive[EOL]""
[DEBUG] wire - << ""Pragma: no-cache[EOL]""
[DEBUG] wire - << ""Cache-Control: no-cache[EOL]""
[DEBUG] wire - << ""Content-Type: text/html[EOL]""
[DEBUG] wire - << ""Content-Length: 4107  [EOL]""
[DEBUG] wire - << ""[EOL]""
----------------------------------------
HTTP/1.1 407 Proxy Authentication Required ( The ISA Server requires authorization to fulfill the request. Access to the Web Proxy filter is denied.  )

Thanks,
Raj





"
1,"Impossible comparison in NodeTypeImplorg.apache.jackrabbit.jcr2spi.nodetype.NodeTypeImpl does

    public boolean isNodeType(Name nodeTypeName) {
        return getName().equals(nodeTypeName) ||  ent.includesNodeType(nodeTypeName);
    }


as getName() is a string and nodeTypeName is a Name this will always be false. Perhaps you meant

    public boolean isNodeType(Name nodeTypeName) {
        return getName().equals(nodeTypeName.getLocalName()) ||  ent.includesNodeType(nodeTypeName);
    }

"
1,"FNFE hit when creating an empty index and infoStream is onShai just reported this on the dev list.  Simple test:
{code}
Directory dir = new RAMDirectory();
IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), MaxFieldLength.UNLIMITED);
writer.setInfoStream(System.out);
writer.addDocument(new Document());
writer.commit();
writer.close();
{code}

hits this:

{code}
Exception in thread ""main"" java.io.FileNotFoundException: _0.prx
    at org.apache.lucene.store.RAMDirectory.fileLength(RAMDirectory.java:149)
    at org.apache.lucene.index.DocumentsWriter.segmentSize(DocumentsWriter.java:1150)
    at org.apache.lucene.index.DocumentsWriter.flush(DocumentsWriter.java:587)
    at org.apache.lucene.index.IndexWriter.doFlushInternal(IndexWriter.java:3572)
    at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:3483)
    at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3474)
    at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1940)
    at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1894)
{code}

Turns out it's just silly -- this is actually an issue I've already fixed on the flex (LUCENE-1458) branch.  DocumentsWriter has its own method to enumerate the flushed files and compute their size, but really it shouldn't do that -- it should use SegmentInfo's method, instead."
1,Aggregate include ignored if no primaryType setIf the include element of an aggregate definition does not have a primaryType attribute then the include is never matched.
1,"Proxy authentication error: Unexpected state: MSG_TYPE3_GENERATED when using NTLM authenticationTrying to connect to a website that requires basic authentication through a proxy that requires NTLM authentication.

Proxy authentication fails with ""Proxy authentication error: Unexpected state: MSG_TYPE3_GENERATED"".

Full wire log attached.  Code to replicate problem follows:

    private void execute() throws HttpException, IOException {
    	
    	URL targetUrl = new URL(TARGET_URL);
    	
        DefaultHttpClient httpclient = new DefaultHttpClient();

        HttpHost targetHost = new HttpHost(targetUrl.getHost()); 
        HttpHost proxyHost = new HttpHost(PROXY_HOST, PROXY_PORT); 
        
        httpclient.getParams().setParameter(ConnRoutePNames.DEFAULT_PROXY, 
        		proxyHost);

        CredentialsProvider credProvider = httpclient.getCredentialsProvider();
        
        Credentials proxyCredentials = new NTCredentials(PROXY_USER, 
        		PROXY_PASSWORD, PROXY_MACHINE, PROXY_DOMAIN);
        AuthScope proxyAuthScope = new AuthScope(proxyHost.getHostName(),
        		proxyHost.getPort());
        
        credProvider.setCredentials(proxyAuthScope, proxyCredentials);
        
        Credentials targetCredentials = new UsernamePasswordCredentials(
        		TARGET_USER, TARGET_PASSWORD);
        AuthScope targetAuthScope = new AuthScope(targetHost.getHostName(),
        		targetHost.getPort());
        
        credProvider.setCredentials(targetAuthScope, targetCredentials);
      
        HttpGet httpget = new HttpGet(targetUrl.getPath());

        HttpResponse response = httpclient.execute(targetHost, httpget);
        
        System.out.println(""response = "" + response);
        
       
    }
"
1,"spi2dav: EventFilters not respectedi have the impression that the event filter passed to the event subscription in spi2dav is not (or not properly) respected.

marcel, is there a specific reason that you always pass the static SubscriptionInfo constant (no node type filter, noLocal false) to the SubscribeMethod
in spi2dav/RepositoryServiceImpl#createSubscription ?

i guess this is the reason for the failure of
  testNodeType(org.apache.jackrabbit.test.api.observation.AddEventListenerTest)
  testNoLocalTrue(org.apache.jackrabbit.test.api.observation.AddEventListenerTest)
"
1,"FileDataStore garbage collection can throw a NullPointerException if there is I/O problemThe FileDataStore can throw a NPE when doing garbage collection, if there is file I/O problem (for example an access rights problem). The reason is that it doesn't check if File.list / listFiles returns null. Stack trace:

Exception in thread ""Thread-461"" java.lang.NullPointerException
	at org.apache.jackrabbit.core.data.FileDataStore.deleteOlderRecursive(FileDataStore.java:334)
	at org.apache.jackrabbit.core.data.FileDataStore.deleteOlderRecursive(FileDataStore.java:328)
	at org.apache.jackrabbit.core.data.FileDataStore.deleteOlderRecursive(FileDataStore.java:328)
	at org.apache.jackrabbit.core.data.FileDataStore.deleteOlderRecursive(FileDataStore.java:328)

"
1,"jcr2spi: Item.isSame may return wrong result if any ancestor is invalidatedjulian detected an issue with jcr2spi that was previously shadowed due to heavy reloading of items upon save.
with the most recent changes however reloading of items is postponed until the next access. this will cause the following test to fail:

        Node n = testRootNode.addNode(""aFile"", ""nt:file"");
        n = n.addNode(""jcr:content"", ""nt:resource"");
        n.setProperty(""jcr:lastModified"", Calendar.getInstance());
        n.setProperty(""jcr:mimeType"", ""text/plain"");
        Property jcrData = n.setProperty(""jcr:data"", ""abc"", PropertyType.BINARY);
        testRootNode.save();

        // access same property through different session
        Session otherSession = helper.getReadOnlySession();
        try {
            Property otherProperty = (Property) otherSession.getItem(jcrData.getPath());
            assertTrue(jcrData.isSame(otherProperty));
        } finally {
            otherSession.logout();
        }

while 
     
       assertTrue(n.isSame(otherSession.getItem(n.getPath()));

would be successful.

the reason: the jcrData property is not reloaded and it's parent is still _invalidated_. consequently the property isn't aware of it's id having changed due to the fact that nt:resource is a node type extending from mix:referenceable.

possible fixes:

1) mark all items _invalid_ after save 
    instead of setting status non-protected/autocreated properties to EXISTING.
    -> forcing jcrData to be reloaded before isSame can be called.
    -> drawback: much more round trip(s) to the server just to make sure the id is up to date.

2) change Item#isSame to make sure the workspaceId is up to date (walking up the
     hierarchy and force reloading of the first invalidated ancestor).
     -> drawback: if referenceable nodes are rare or missing at all, this causes some
          extra round trips.
 
3) change Item.isSame to compare the 'workspacePath' instead of the 'workspaceId'.
     -> drawback: upon persisted move of a referenceable node Item#isSame will return false


after taking a closer look at the code and at some additional tests i would opt for 2).
"
1,"TestNRTThreads hangs in nightly 3.x buildsMaybe we have a problem, maybe its a bug in the test.

But its strange that lately the 3.x nightlies have been hanging here."
1,"PerFieldCodecWrapper.loadTermsIndex concurrency problemSelckin's while(1) testing on RT branch hit another error:
{noformat}
    [junit] Testsuite: org.apache.lucene.TestExternalCodecs
    [junit] Testcase: testPerFieldCodec(org.apache.lucene.TestExternalCodecs):	Caused an ERROR
    [junit] (null)
    [junit] java.lang.NullPointerException
    [junit] 	at org.apache.lucene.index.PerFieldCodecWrapper$FieldsReader.loadTermsIndex(PerFieldCodecWrapper.java:202)
    [junit] 	at org.apache.lucene.index.SegmentReader.loadTermsIndex(SegmentReader.java:1005)
    [junit] 	at org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:652)
    [junit] 	at org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:609)
    [junit] 	at org.apache.lucene.index.BufferedDeletesStream.applyDeletes(BufferedDeletesStream.java:276)
    [junit] 	at org.apache.lucene.index.IndexWriter.applyAllDeletes(IndexWriter.java:2660)
    [junit] 	at org.apache.lucene.index.IndexWriter.maybeApplyDeletes(IndexWriter.java:2651)
    [junit] 	at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:381)
    [junit] 	at org.apache.lucene.index.IndexReader.open(IndexReader.java:316)
    [junit] 	at org.apache.lucene.TestExternalCodecs.testPerFieldCodec(TestExternalCodecs.java:541)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1246)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1175)
    [junit] 
    [junit] 
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.909 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestExternalCodecs -Dtestmethod=testPerFieldCodec -Dtests.seed=-7296204858082494534:5010909751437000758
    [junit] WARNING: test method: 'testPerFieldCodec' left thread running: merge thread: _i(4.0):Cv130 _m(4.0):Cv30 _n(4.0):cv10 into _o
    [junit] RESOURCE LEAK: test method: 'testPerFieldCodec' left 1 thread(s) running
    [junit] NOTE: test params are: codec=PreFlex, locale=zh_TW, timezone=America/Santo_Domingo
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestDemo, TestExternalCodecs]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=2,free=104153512,total=125632512
    [junit] ------------- ---------------- ---------------
    [junit] TEST org.apache.lucene.TestExternalCodecs FAILED
    [junit] Exception in thread ""Lucene Merge Thread #5"" org.apache.lucene.util.ThreadInterruptedException: java.lang.InterruptedException: sleep interrupted
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:505)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:472)
    [junit] Caused by: java.lang.InterruptedException: sleep interrupted
    [junit] 	at java.lang.Thread.sleep(Native Method)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:503)
    [junit] 	... 1 more
{noformat}

I suspect this is also a trunk issue, but I can't reproduce it yet.

I think this is happening because the codecs HashMap is changing (via another thread), while .loadTermsIndex is called."
1,"DbDataStore connection does not always reconnectIf a DbDataStore connection is closed due to an error all subsequent addRecord calls will fail with 'connection has been closed and autoReconnect == false'
 after getRecord is called and the connection is reconnected addRecord will succeed.

the connection should be validated before setting autoReconnect = false or on retrieval from the pool."
1,"intermittant exceptions in TestConcurrentMergeScheduler
The TestConcurrentMergeScheduler throws intermittant exceptions that
do not result in a test failure.

The exception happens in the ""testNoWaitClose()"" test, which repeated
tests closing an IndexWriter with ""false"", meaning abort any
still-running merges.  When a merge is aborted it can hit various
exceptions because the files it is reading and/or writing have been
deleted, so we ignore these exceptions.

The bug was just that we were failing to properly check whether the
running merge was actually aborted because of a scoping issue of the
""merge"" variable in ConcurrentMergeScheduler.  So the exceptions are
actually ""harmless"".  Thanks to Ning for spotting it!

"
1,"SpellChecker min score is increased by timeThe minimum score, an instance variable, is modified in a search. That is wrong, since it makes it 1. thread unsafe and 2. not working. 

Lucky enought it is only used from the one and same method call, so I simply compied the instance variable to a local method variable.

        float min = this.min; 
"
1,"Problem with redirect on HEAD when (bad, naughty) server returns body contentI've been testing/using HttpClient 2.0a3 with Resin 2.1.9. I've found that when
using a HEAD request on a JSP, Resin returns the body content along with the
headers.

In this case, something in the HttpClient breaks. Looking at the httpclient
logs, it looks like:

1) HttpClient does a HEAD against the original URL
2) Resin returns valid status line and headers
3) HttpClient parses the headers and recognizes the redirect header
4) HttpClient does a HEAD against the new URL (from the Location header)
5) HttpMethodBase calls readStatusLine, which (eventually) calles readRawLine in
HttpConnection (which reads from the internal inputStream)
6) readRawLine returns the first line in the body from the original HEAD request
in (1).

It looks like the original body content (in response to the first HEAD) is being
buffered somewhere, but I can't figure out where.

I know that this is invalid behavior on the server's part, but I would like to
be able to recover from it.



---- redir_test.jsp ----
<?xml version=""1.0""?>
<% 
  response.setStatus(response.SC_MOVED_TEMPORARILY);
  response.setHeader(""Location"", ""redirect_pass.xml"");
%>
<some>
  <dummy>
    <data attr=""yea, well""/>
  </dummy>
</some>"
1,"NTLM Authentication FailsNTLM Authentication requires multiple request/responses for the authentication
to succeed.  Since HttpMethodBase is now using just the host, port and realm to
identify whether or not authentication has been attempted the second pass for
NTLM authentication is never performed."
1,Deleted documents are visible across reopened MSRs
1,"NoSuchItemStateException on checkin after removeVersion in XA EnvironmentAfter removing a version, a checkin on the same node in a different transaction (with a different session) fails.
The NoSuchItemStateException refer to the uuid of the previously removed version. 
I'll attach a test demonstrating the problem. "
1,"BindableRepositoryFactory doesn't handle repository shutdownThe BindableRepositoryFactory class keeps a cached reference to a repository even after the repository has been shut down.

This causes the following code snippet to fail with an IllegalStateException:

        Hashtable environment = new Hashtable();
        environment.put(
                Context.INITIAL_CONTEXT_FACTORY,
                DummyInitialContextFactory.class.getName());
        environment.put(Context.PROVIDER_URL, ""http://jackrabbit.apache.org/"");
        Context context = new InitialContext(environment);

        JackrabbitRepository repository;
        String xml = ""src/test/repository/repository.xml"";
        String dir = ""target/repository"";
        String key = ""repository"";

        // Create first repository
        RegistryHelper.registerRepository(context, key, xml, dir, true);
        repository = (JackrabbitRepository) context.lookup(key);
        repository.login().logout();
        repository.shutdown();

        // Create second repository with the same configuration
        RegistryHelper.registerRepository(context, key, xml, dir, true);
        repository = (JackrabbitRepository) context.lookup(key);
        repository.login().logout(); // throws an IllegalStateException!
        repository.shutdown();
"
1,"Internal error in WorkspaceItemStateFactory#createDeepNodeState When WorkspaceItemStateFactory#createDeepNodeState receives the current entry as argument for anyParent, it throws RepositoryException with the message ""Internal error while getting deep itemState"". This is incorrect (probably a leftover from JCR-1797) since any entry is valid as argument for anyParent. "
1,"SQL Parser fails with SQL 92 timestamp formatThe SQL query parser fails with an exception if the SQL 92 timestamp format is used.

E.g:
... WHERE my:date > TIMESTAMP '1976-01-01 00:00:00.000+01:00'

does not work, but the following will succeed using ISO8601:

... WHERE my:date > TIMESTAMP '1976-01-01T00:00:00.000+01:00'"
1,"Automatic type conversion no longer worksString values are no longer converted to binary when required. Example:

Node n = testRootNode.addNode(""testConvert"", ""nt:file"");
Node content = n.addNode(""jcr:content"", ""nt:resource"");
content.setProperty(""jcr:lastModified"", Calendar.getInstance());
content.setProperty(""jcr:mimeType"", ""text/html"");
content.setProperty(""jcr:data"", ""Hello"");
n.getSession().save();

This used to work in a previous 2.0 build, but now throws:

javax.jcr.nodetype.ConstraintViolationException: no matching property definition found for {http://www.jcp.org/jcr/1.0}data
at org.apache.jackrabbit.core.nodetype.EffectiveNodeType.getApplicablePropertyDef(EffectiveNodeType.java:782)
at org.apache.jackrabbit.core.NodeImpl.getApplicablePropertyDefinition(NodeImpl.java:747)
at org.apache.jackrabbit.core.ItemManager.getDefinition(ItemManager.java:241)
at org.apache.jackrabbit.core.ItemData.getDefinition(ItemData.java:101)
at org.apache.jackrabbit.core.ItemManager.canRead(ItemManager.java:409)
at org.apache.jackrabbit.core.ItemManager.getItemData(ItemManager.java:383)
at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:316)
at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:595)
at org.apache.jackrabbit.core.NodeImpl.removeChildProperty(NodeImpl.java:554)
at org.apache.jackrabbit.core.NodeImpl.removeChildProperty(NodeImpl.java:534)
at org.apache.jackrabbit.core.NodeImpl.setProperty(NodeImpl.java:2303)
at org.apache.jackrabbit.core.nodetype.ConvertDataTypeTest.testStringToBinary(ConvertDataTypeTest.java:36)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)"
1,"IndexWriter.addIndexes(IndexReader[] readers) doesn't correctly handle exception success flag.After this bit of code in addIndexes(IndexReader[] readers)

 try {
        flush(true, false, true);
        optimize();					  // start with zero or 1 seg
        success = true;
      } finally {
        // Take care to release the write lock if we hit an
        // exception before starting the transaction
        if (!success)
          releaseWrite();
      }

The success flag should be reset to ""false"" because it's used again in another try/catch/finally block.  

TestIndexWriter.testAddIndexOnDiskFull() sometimes will hit this bug; but it's infrequent.


"
1,"importXML prepending line feeds to tag valuesImporting using Session.importXML(...) results in new line characters being inserted at the beginning of tag
values:

<?xml version=""1.0"" encoding=""UTF-8""?>
<Policy xmlns=""urn:oasis:names:tc:xacml:1.0:policy""  xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" PolicyId=""test:policy-one"" RuleCombiningAlgId=""urn:oasis:names:tc:xacml:1.0:rule-combining-algorithm:ordered-permit-overrides"">
  <Description>policy-description</Description>
  <Target>
...

Becomes

/test/policies/Policy/jcr:primaryType=nt:unstructured
/test/policies/Policy/PolicyId=test:policy-one
/test/policies/Policy/RuleCombiningAlgId=urn:oasis:names:tc:xacml:1.0:rule-combining-algorithm:ordered-permit-overrides
/test/policies/Policy/Description/jcr:primaryType=nt:unstructured
/test/policies/Policy/Description/jcr:xmltext/jcr:primaryType=nt:unstructured
/test/policies/Policy/Description/jcr:xmltext/jcr:xmlcharacters=
policy-description
/test/policies/Policy/Target/jcr:primaryType=nt:unstructured

(in other cases, many LFs are inserted)

FULL EXAMPLE XML FILE:

<?xml version=""1.0"" encoding=""UTF-8""?>
<Policy xmlns=""urn:oasis:names:tc:xacml:1.0:policy"" 
  xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" 
  PolicyId=""test:policy-one"" 
  RuleCombiningAlgId=""urn:oasis:names:tc:xacml:1.0:rule-combining-algorithm:ordered-permit-overrides"">
  <Description>policy-description</Description>
  <Target>
    <Resources>
      <Resource>
        <ResourceMatch 
          MatchId=""urn:oasis:names:tc:xacml:1.0:function:string-equal"">
          <AttributeValue DataType=""http://www.w3.org/2001/XMLSchema#string"">test/12345-resource-67890</AttributeValue>
          <ResourceAttributeDesignator 
            DataType=""http://www.w3.org/2001/XMLSchema#string"" 
            AttributeId=""urn:oasis:names:tc:xacml:1.0:resource:resource-id""/>
        </ResourceMatch>
      </Resource>
    </Resources>
    <Actions>
      <AnyAction/>
    </Actions>
  </Target>
  <Rule RuleId=""PermitRule"" Effect=""Permit"">
    <Target>
      <Subjects>
        <Subject>
          <SubjectMatch 
            MatchId=""urn:oasis:names:tc:xacml:1.0:function:string-equal"">
            <AttributeValue DataType=""http://www.w3.org/2001/XMLSchema#string"">alice</AttributeValue>
            <SubjectAttributeDesignator 
              DataType=""http://www.w3.org/2001/XMLSchema#string"" 
              AttributeId=""urn:oasis:names:tc:xacml:1.0:subject:subject-id""/>
          </SubjectMatch>
        </Subject>
      </Subjects>
      <Actions>
        <Action>
          <ActionMatch 
            MatchId=""urn:oasis:names:tc:xacml:1.0:function:string-equal"">
            <AttributeValue DataType=""http://www.w3.org/2001/XMLSchema#string"">read</AttributeValue>
            <ActionAttributeDesignator 
              DataType=""http://www.w3.org/2001/XMLSchema#string"" 
              AttributeId=""urn:oasis:names:tc:xacml:1.0:action:action-id""/>
          </ActionMatch>
        </Action>
        <Action>
          <ActionMatch 
            MatchId=""urn:oasis:names:tc:xacml:1.0:function:string-equal"">
            <AttributeValue DataType=""http://www.w3.org/2001/XMLSchema#string"">write</AttributeValue>
            <ActionAttributeDesignator 
              DataType=""http://www.w3.org/2001/XMLSchema#string"" 
              AttributeId=""urn:oasis:names:tc:xacml:1.0:action:action-id""/>
          </ActionMatch>
        </Action>
        <Action>
          <ActionMatch 
            MatchId=""urn:oasis:names:tc:xacml:1.0:function:string-equal"">
            <AttributeValue DataType=""http://www.w3.org/2001/XMLSchema#string"">delete</AttributeValue>
            <ActionAttributeDesignator 
              DataType=""http://www.w3.org/2001/XMLSchema#string"" AttributeId=""urn:oasis:names:tc:xacml:1.0:action:action-id""/>
          </ActionMatch>
        </Action>
      </Actions>
    </Target>
  </Rule>
</Policy>"
1,"org.apache.lucene.ant.HtmlDocument creates a FileInputStream in its constructor that it doesn't closeA look through the jtidy source code doesn't show a close that i can find in parse (seems to be standard that you close your own streams anyway), so this looks like a small descriptor leak to me."
1,"jcr:mixinTypes property inconsitent, if addMixin() throws exceptionIf Node.addMixin() throws an exception, that was not due to validation checks but rather internal, the jcr:mixinTypes still contains the newly added nodetype. a subsequent call to Item.save() will store that property. The effective nodetype is not affected though."
1,"TokenSources.getTokenStream() does not assign positionIncrementTokenSources.StoredTokenStream does not assign positionIncrement information. This means that all tokens in the stream are considered adjacent. This has implications for the phrase highlighting in QueryScorer when using non-contiguous tokens.

For example:
Consider  a token stream that creates tokens for both the stemmed and unstemmed version of each word - the fox (jump|jumped)
When retrieved from the index using TokenSources.getTokenStream(tpv,false), the token stream will be - the fox jump jumped

Now try a search and highlight for the phrase query ""fox jumped"". The search will correctly find the document; the highlighter will fail to highlight the phrase because it thinks that there is an additional word between ""fox"" and ""jumped"". If we use the original (from the analyzer) token stream then the highlighter works.

Also, consider the converse - the fox did not jump
""not"" is a stop word and there is an option to increment the position to account for stop words - (the,0) (fox,1) (did,2) (jump,4)
When retrieved from the index using TokenSources.getTokenStream(tpv,false), the token stream will be - (the,0) (fox,1) (did,2) (jump,3).

So the phrase query ""did jump"" will cause the ""did"" and ""jump"" terms in the text ""did not jump"" to be highlighted. If we use the original (from the analyzer) token stream then the highlighter works correctly."
1,"SSL + proxy + Host auth + Keep Alive off causes an infinite loop in HttpMethodDirectorThe combination of SSL tunnelling, host authentication, and disabled persistent
connection support (HTTPD KeepAlive off) causes an infinite loop in
HttpMethodDirector. 

The problem has been reported on the httpclient-dev list by Rindress MacDonald
<RMacDona at enterasys.com>"
1,"o.a.j.core.state.ChildNodeEntries does not override equals(Object) and hashCode() methodso.a.j.c.state.NodeStateMerger calls ChildNodeEntries.equals(ChildNodeEntries) to compare two child node entries collections.
ChildNodeEntries however doesn't override the equals(Object) method."
1,"Docview import fails, if attribute and childelem have same namedocimport fails, if element has same name as one of the attributes of its parent element.

example:

<?xml version=""1.0"" encoding=""UTF-8""?>
<feature plugin=""foobar"">
    <plugin>test</plugin>
</feature>

importing this results in a ItemExistsException: 'plugin'"
1,"Missing skip()-Method in ContentLengthInputStreamContentLengthInputStream is missing the skip()-Method.

This causes the internal pos variable to get out of sync with the content 
length. 
We oberseved that closing the stream caused a wait time of about 15 sec in 
routines which use the skip()-method of InputStream.

Here's a possible implementation which should solve the problem:

    public long skip(long len) throws IOException {
        long count = super.skip(len);
        pos += count;
        return count;
    }"
1,"PostgreSQL: Failed to guess validation queryWhen using PostgreSQL, the following warning appears in the log file:

> *WARN  [org.apache.jackrabbit.core.util.db.ConnectionFactory] (main)
> Failed to guess validation query for URL
> jdbc:postgresql:..."
1,"Support updateDocument() with DWPTsWith separate DocumentsWriterPerThreads (DWPT) it can currently happen that the delete part of an updateDocument() is flushed and committed separately from the corresponding new document.

We need to make sure that updateDocument() is always an atomic operation from a IW.commit() and IW.getReader() perspective.  See LUCENE-2324 for more details."
1,"QueryParser doesn't accept empty stringfoo:"""" currently throws a parse exception
foo: bar is also parsed as foo:bar (not serious since it's arguably illegal syntax)"
1,"UsernamePasswordCredentials.equals(null) throws NPESteps to reproduce:
1. new UsernamePasswordCredentials().equals(null);

Observed:
NullPointerException is thrown

Expected:
equals() returns false"
1,"NullPointerException when deleting a property of type REFERENCEIn method org.apache.jackrabbit.rmi.value.SerialValueFactory#createValue a NPE is thrown when parameter value is null.

Solution:

Change:

   public final Value createValue(Node value) throws RepositoryException {
        return createValue(value.getUUID(), PropertyType.REFERENCE);
    }

to

   public final Value createValue(Node value) throws RepositoryException {
        if (value == null) {
           return null;
        }
        
        return createValue(value.getUUID(), PropertyType.REFERENCE);
    }"
1,"Setting a property which has been transiently removed fails with a PathNotFoundExceptionThe following tests currently all fail with a PathNotFoundException

org.apache.jackrabbit.jcr2spi.AddPropertyTest#testReplacingProperty
org.apache.jackrabbit.jcr2spi.AddPropertyTest#testReplacingProperty2
org.apache.jackrabbit.jcr2spi.AddPropertyTest#testAddingProperty
org.apache.jackrabbit.jcr2spi.AddPropertyTest#testAddingProperty2

"
1,"Document View Import: ISO 9075-encoded element/attribute names may lead to illegal node/property names reported by sridhar raman on the users-list:

importing the following xml document leads to a node of name ""abc [1]"" which is illegal:

<?xml version=""1.0""?>
<abc_x0020__x005B_1_x005D_ foo=""bar""/>
"
1,"New versions added after a restore have bad version nameI add several versions to a node (1.0, 1.1, 1.2, 1.3, 1.4). Perform a restore to version 1.2 and add more versions. After that VersionHistory is like this:

- 1.0
- 1.1
- 1.2
- 1.3
- 1.4
- 1.3.1
- 1.3.2
- 1.3.3
- 1.3.4
- 1.3.5

New versions should be 1.2.x no 1.3.x, isn't it?"
1,"VolatileIndex not closed properlyThe MultiIndex.resetVolatileIndex() method doesn't properly close the existing VolatileIndex instance before creating a new one. This can confuse the DynamicPooledExecutor reference count added in JCR-2836, leading to a background thread leak."
1,"SQLException with OracleBundle PM in name indexThe oracle bundle pm shows errors like:

java.lang.IllegalStateException: Unable to insert index: java.sql.SQLException:  
  ORA-01400: cannot insert NULL into  (""MARTIJNH"".""WM9_VERSIONING_PM_NAMES"".""ID"")

this is due to the fact that oracle treats empty strings as NULL values which does the schema not allow."
1,"SortField.AUTO doesn't work with longThis is actually the same as LUCENE-463 but I cannot find a way to re-open that issue. I'm attaching a test case by dragon-fly999 at hotmail com that shows the problem and a patch that seems to fix it.

The problem is that a long (as used for dates) cannot be parsed as an integer, and the next step is then to parse it as a float, which works but which is not correct. With the patch the following parsers are used in this order: int, long, float.
"
1,"Repository is corrupt after concurrent changes with the same sessionAfter concurrent write operations using the same session, the repository can get corrupt, meaning a ItemNotFoundException is thrown when trying to remove a node.

Concurrent write operations are not supported, however I believe the persistent state of the repository should not be get corrupt.

One way to solve this problem is to synchronize on the session internally."
1,"Index recovery may fail with IllegalArgumentExceptionWhen repeatedly killed and started up again, jackrabbit may throw an IllegalArgumentException on index recovery:

Caused by: java.lang.IllegalArgumentException: already contains: _c
   at org.apache.jackrabbit.core.query.lucene.IndexInfos.addName(IndexInfos.java:170)
   at org.apache.jackrabbit.core.query.lucene.MultiIndex.deleteIndex(MultiIndex.java:716)
   at org.apache.jackrabbit.core.query.lucene.MultiIndex$DeleteIndex.execute(MultiIndex.java:1553)
   at org.apache.jackrabbit.core.query.lucene.MultiIndex.executeAndLog(MultiIndex.java:809)
   at org.apache.jackrabbit.core.query.lucene.MultiIndex.flush(MultiIndex.java:740)
   at org.apache.jackrabbit.core.query.lucene.Recovery.run(Recovery.java:160)
"
1,http client cache: SizeLimitedResponseReader is not setting content type for InputStreamEntity in constructResponse()the newly created InputStreamEntity should be populated with content-encoding and content-type.
1,"Xpath query parser accepts ""/a | /b"" and treats it as ""/a/b""The XPath query parser accepts the query

  /a | /b

and parses it into a query tree corresponging the Xpath query

  /a/b

It should be rejected instead."
1,"MMapDirectory can't create new index on WindowsWhen I set the system property to request the use of the mmap directory, and start building a large index, the process dies with an IOException trying to delete a file. Apparently, Lucene isn't closing down the memory map before deleting the file.
"
1,"Null Pointer Exception while looking for a DavProperty that hasn't been setNull pointer exception.
Exception occurs because the DavPropertySet.map does not contain an expected entry: ItemResourceConstants.JCR_NAME

Suggested fix: add the constant to the nameSet in RepositoryServiceImpl.java:760
 nameSet.add(ItemResourceConstants.JCR_NAME);

I tried that and it works. See stack trace at below.

Exception in thread ""main"" java.lang.NullPointerException
  at org.apache.jackrabbit.spi2dav.URIResolverImpl.buildPropertyId(URIResolverImpl.java:201)
  at org.apache.jackrabbit.spi2dav.RepositoryServiceImpl.getNodeInfo(RepositoryServiceImpl.java:808)
  at org.apache.jackrabbit.spi2dav.RepositoryServiceImpl.getItemInfos(RepositoryServiceImpl.java:834)
  at org.apache.jackrabbit.jcr2spi.state.WorkspaceItemStateFactory.createNodeState(WorkspaceItemStateFactory.java:88)
  at org.apache.jackrabbit.jcr2spi.state.TransientISFactory.createNodeState(TransientISFactory.java:99)
  at org.apache.jackrabbit.jcr2spi.hierarchy.NodeEntryImpl.doResolve(NodeEntryImpl.java:959)
  at org.apache.jackrabbit.jcr2spi.hierarchy.HierarchyEntryImpl.resolve(HierarchyEntryImpl.java:95)
  at org.apache.jackrabbit.jcr2spi.hierarchy.HierarchyEntryImpl.getItemState(HierarchyEntryImpl.java:212)
  at org.apache.jackrabbit.jcr2spi.ItemManagerImpl.getItem(ItemManagerImpl.java:157)
  at org.apache.jackrabbit.jcr2spi.SessionImpl.getRootNode(SessionImpl.java:225)
"
1,"JackrabbitIndexReader prevents use of DocNumberCacheThe JackrabbitIndexReader was introduced in 1.5 by JCR-1363. Unfortunately it does not overwrite the method termDocs(Term), which means the default implementation in IndexReader is used. This bypasses the DocNumberCache built into CachingIndexReader, which is used for UUID terms that look up individual documents."
1,"TestIndexWriter.testCommitThreadSafety fails on realtime_search branchHudson failed on RT with this error - I wasn't able to reproduce yet....

{noformat}
NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testCommitThreadSafety -Dtests.seed=410261592077577885:-4099127561715488589 -Dtests.multiplier=3
NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testCommitThreadSafety -Dtests.seed=410261592077577885:-4099127561715488589 -Dtests.multiplier=3
The following exceptions were thrown by threads:
*** Thread: Thread-331 ***
java.lang.RuntimeException: java.lang.AssertionError: term=f:2_0; r=DirectoryReader(segments_6 _8(4.0):Cv7) expected:<1> but was:<0>
	at org.apache.lucene.index.TestIndexWriter$5.run(TestIndexWriter.java:2416)
Caused by: java.lang.AssertionError: term=f:2_0; r=DirectoryReader(segments_6 _8(4.0):Cv7) expected:<1> but was:<0>
	at org.junit.Assert.fail(Assert.java:91)
	at org.junit.Assert.failNotEquals(Assert.java:645)
	at org.junit.Assert.assertEquals(Assert.java:126)
	at org.junit.Assert.assertEquals(Assert.java:470)
	at org.apache.lucene.index.TestIndexWriter$5.run(TestIndexWriter.java:2410)
NOTE: test params are: codec=RandomCodecProvider: {=SimpleText, f6=MockVariableIntBlock(baseBlockSize=91), f7=MockFixedIntBlock(blockSize=1289), f8=Standard, f9=MockRandom, f1=MockSep, f0=Pulsing(freqCutoff=15), f3=Pulsing(freqCutoff=15), f2=MockFixedIntBlock(blockSize=1289), f5=MockVariableIntBlock(baseBlockSize=91), f4=MockRandom, f=MockSep, c=MockVariableIntBlock(baseBlockSize=91), termVector=SimpleText, d9=SimpleText, d8=MockSep, d5=MockVariableIntBlock(baseBlockSize=91), d4=MockRandom, d7=Standard, d6=SimpleText, d25=Standard, d0=MockVariableIntBlock(baseBlockSize=91), c29=Standard, d24=SimpleText, d1=MockFixedIntBlock(blockSize=1289), c28=MockFixedIntBlock(blockSize=1289), d23=MockVariableIntBlock(baseBlockSize=91), d2=Standard, c27=MockVariableIntBlock(baseBlockSize=91), d22=MockRandom, d3=MockRandom, d21=MockFixedIntBlock(blockSize=1289), d20=MockVariableIntBlock(baseBlockSize=91), c22=MockVariableIntBlock(baseBlockSize=91), c21=MockRandom, c20=Pulsing(freqCutoff=15), d29=MockVariableIntBlock(baseBlockSize=91), c26=SimpleText, d28=MockRandom, c25=MockSep, d27=Pulsing(freqCutoff=15), c24=MockRandom, d26=MockFixedIntBlock(blockSize=1289), c23=Standard, e9=MockRandom, e8=MockFixedIntBlock(blockSize=1289), e7=MockVariableIntBlock(baseBlockSize=91), e6=MockSep, e5=Pulsing(freqCutoff=15), c17=Standard, e3=MockFixedIntBlock(blockSize=1289), d12=SimpleText, c16=SimpleText, e4=Pulsing(freqCutoff=15), d11=MockSep, c19=MockSep, e1=MockSep, d14=Pulsing(freqCutoff=15), c18=Pulsing(freqCutoff=15), e2=SimpleText, d13=MockFixedIntBlock(blockSize=1289), e0=Standard, d10=Standard, d19=Pulsing(freqCutoff=15), c11=SimpleText, c10=MockSep, d16=MockRandom, c13=MockSep, c12=Pulsing(freqCutoff=15), d15=Standard, d18=SimpleText, c15=MockFixedIntBlock(blockSize=1289), d17=MockSep, c14=MockVariableIntBlock(baseBlockSize=91), b3=MockRandom, b2=Standard, b5=SimpleText, b4=MockSep, b7=MockSep, b6=Pulsing(freqCutoff=15), d50=MockVariableIntBlock(baseBlockSize=91), b9=MockFixedIntBlock(blockSize=1289), b8=MockVariableIntBlock(baseBlockSize=91), d43=Pulsing(freqCutoff=15), d42=MockFixedIntBlock(blockSize=1289), d41=SimpleText, d40=MockSep, d47=MockRandom, d46=Standard, b0=SimpleText, d45=MockFixedIntBlock(blockSize=1289), b1=Standard, d44=MockVariableIntBlock(baseBlockSize=91), d49=MockSep, d48=Pulsing(freqCutoff=15), c6=MockVariableIntBlock(baseBlockSize=91), c5=MockRandom, c4=Pulsing(freqCutoff=15), c3=MockFixedIntBlock(blockSize=1289), c9=MockSep, c8=MockRandom, c7=Standard, d30=MockFixedIntBlock(blockSize=1289), d32=MockRandom, d31=Standard, c1=MockVariableIntBlock(baseBlockSize=91), d34=Standard, c2=MockFixedIntBlock(blockSize=1289), d33=SimpleText, d36=MockSep, c0=MockSep, d35=Pulsing(freqCutoff=15), d38=MockVariableIntBlock(baseBlockSize=91), d37=MockRandom, d39=SimpleText, e92=MockFixedIntBlock(blockSize=1289), e93=Pulsing(freqCutoff=15), e90=MockSep, e91=SimpleText, e89=MockVariableIntBlock(baseBlockSize=91), e88=MockSep, e87=Pulsing(freqCutoff=15), e86=SimpleText, e85=MockSep, e84=MockRandom, e83=Standard, e80=MockFixedIntBlock(blockSize=1289), e81=Standard, e82=MockRandom, e77=MockVariableIntBlock(baseBlockSize=91), e76=MockRandom, e79=Standard, e78=SimpleText, e73=MockSep, e72=Pulsing(freqCutoff=15), e75=MockFixedIntBlock(blockSize=1289), e74=MockVariableIntBlock(baseBlockSize=91), binary=MockVariableIntBlock(baseBlockSize=91), f98=Pulsing(freqCutoff=15), f97=MockFixedIntBlock(blockSize=1289), f99=MockRandom, f94=Standard, f93=SimpleText, f96=MockSep, f95=Pulsing(freqCutoff=15), e95=SimpleText, e94=MockSep, e97=Pulsing(freqCutoff=15), e96=MockFixedIntBlock(blockSize=1289), e99=MockFixedIntBlock(blockSize=1289), e98=MockVariableIntBlock(baseBlockSize=91), id=MockRandom, f34=Standard, f33=SimpleText, f32=MockVariableIntBlock(baseBlockSize=91), f31=MockRandom, f30=MockFixedIntBlock(blockSize=1289), f39=Standard, f38=MockVariableIntBlock(baseBlockSize=91), f37=MockRandom, f36=Pulsing(freqCutoff=15), f35=MockFixedIntBlock(blockSize=1289), f43=MockSep, f42=Pulsing(freqCutoff=15), f45=MockFixedIntBlock(blockSize=1289), f44=MockVariableIntBlock(baseBlockSize=91), f41=SimpleText, f40=MockSep, f47=Standard, f46=SimpleText, f49=MockSep, f48=Pulsing(freqCutoff=15), content=MockSep, e19=SimpleText, e18=MockSep, e17=Standard, f12=SimpleText, e16=SimpleText, f11=MockSep, f10=MockRandom, e15=MockVariableIntBlock(baseBlockSize=91), e14=MockRandom, f16=MockRandom, e13=MockSep, f15=Standard, e12=Pulsing(freqCutoff=15), e11=Standard, f14=MockFixedIntBlock(blockSize=1289), e10=SimpleText, f13=MockVariableIntBlock(baseBlockSize=91), f19=Pulsing(freqCutoff=15), f18=Standard, f17=SimpleText, e29=MockRandom, e26=MockSep, f21=Pulsing(freqCutoff=15), e25=Pulsing(freqCutoff=15), f20=MockFixedIntBlock(blockSize=1289), e28=MockFixedIntBlock(blockSize=1289), f23=MockVariableIntBlock(baseBlockSize=91), e27=MockVariableIntBlock(baseBlockSize=91), f22=MockRandom, f25=SimpleText, e22=MockFixedIntBlock(blockSize=1289), f24=MockSep, e21=MockVariableIntBlock(baseBlockSize=91), f27=Pulsing(freqCutoff=15), e24=MockRandom, f26=MockFixedIntBlock(blockSize=1289), e23=Standard, f29=MockFixedIntBlock(blockSize=1289), f28=MockVariableIntBlock(baseBlockSize=91), e20=Pulsing(freqCutoff=15), field=MockRandom, string=Standard, e30=MockRandom, e31=MockVariableIntBlock(baseBlockSize=91), a98=Standard, e34=MockSep, a99=MockRandom, e35=SimpleText, f79=MockSep, e32=Standard, e33=MockRandom, b97=MockRandom, f77=MockRandom, e38=Standard, b98=MockVariableIntBlock(baseBlockSize=91), f78=MockVariableIntBlock(baseBlockSize=91), e39=MockRandom, b99=SimpleText, f75=MockFixedIntBlock(blockSize=1289), e36=MockVariableIntBlock(baseBlockSize=91), f76=Pulsing(freqCutoff=15), e37=MockFixedIntBlock(blockSize=1289), f73=Pulsing(freqCutoff=15), f74=MockSep, f71=SimpleText, f72=Standard, f81=MockFixedIntBlock(blockSize=1289), f80=MockVariableIntBlock(baseBlockSize=91), e40=Standard, e41=Pulsing(freqCutoff=15), e42=MockSep, e43=MockFixedIntBlock(blockSize=1289), e44=Pulsing(freqCutoff=15), e45=MockRandom, e46=MockVariableIntBlock(baseBlockSize=91), f86=SimpleText, e47=MockSep, f87=Standard, e48=SimpleText, f88=Pulsing(freqCutoff=15), e49=MockFixedIntBlock(blockSize=1289), f89=MockSep, f82=MockVariableIntBlock(baseBlockSize=91), f83=MockFixedIntBlock(blockSize=1289), f84=Standard, f85=MockRandom, f90=MockRandom, f92=SimpleText, f91=MockSep, str=MockSep, a76=SimpleText, e56=SimpleText, f59=MockVariableIntBlock(baseBlockSize=91), a77=Standard, e57=Standard, a78=Pulsing(freqCutoff=15), e54=MockRandom, f57=Pulsing(freqCutoff=15), a79=MockSep, e55=MockVariableIntBlock(baseBlockSize=91), f58=MockSep, e52=MockVariableIntBlock(baseBlockSize=91), e53=MockFixedIntBlock(blockSize=1289), e50=Pulsing(freqCutoff=15), e51=MockSep, f51=MockFixedIntBlock(blockSize=1289), f52=Pulsing(freqCutoff=15), f50=SimpleText, f55=Standard, f56=MockRandom, f53=MockVariableIntBlock(baseBlockSize=91), e58=MockFixedIntBlock(blockSize=1289), f54=MockFixedIntBlock(blockSize=1289), e59=Pulsing(freqCutoff=15), a80=MockRandom, e60=MockRandom, a82=SimpleText, a81=MockSep, a84=MockSep, a83=Pulsing(freqCutoff=15), a86=MockFixedIntBlock(blockSize=1289), a85=MockVariableIntBlock(baseBlockSize=91), a89=Standard, f68=Standard, e65=Pulsing(freqCutoff=15), f69=MockRandom, e66=MockSep, a87=MockVariableIntBlock(baseBlockSize=91), e67=MockVariableIntBlock(baseBlockSize=91), a88=MockFixedIntBlock(blockSize=1289), e68=MockFixedIntBlock(blockSize=1289), e61=Standard, e62=MockRandom, e63=MockSep, e64=SimpleText, f60=MockRandom, f61=MockVariableIntBlock(baseBlockSize=91), f62=SimpleText, f63=Standard, e69=SimpleText, f64=MockSep, f65=SimpleText, f66=MockFixedIntBlock(blockSize=1289), f67=Pulsing(freqCutoff=15), f70=MockSep, a93=MockVariableIntBlock(baseBlockSize=91), a92=MockRandom, a91=Pulsing(freqCutoff=15), e71=Pulsing(freqCutoff=15), a90=MockFixedIntBlock(blockSize=1289), e70=MockFixedIntBlock(blockSize=1289), a97=SimpleText, a96=MockSep, a95=MockRandom, a94=Standard, c58=MockFixedIntBlock(blockSize=1289), a63=MockVariableIntBlock(baseBlockSize=91), a64=MockFixedIntBlock(blockSize=1289), c59=Pulsing(freqCutoff=15), c56=MockSep, d59=MockRandom, a61=Pulsing(freqCutoff=15), c57=SimpleText, a62=MockSep, c54=SimpleText, c55=Standard, a60=SimpleText, c52=MockRandom, c53=MockVariableIntBlock(baseBlockSize=91), d53=Standard, d54=MockRandom, d51=MockVariableIntBlock(baseBlockSize=91), d52=MockFixedIntBlock(blockSize=1289), d57=Pulsing(freqCutoff=15), b62=MockFixedIntBlock(blockSize=1289), d58=MockSep, b63=Pulsing(freqCutoff=15), d55=SimpleText, b60=MockSep, d56=Standard, b61=SimpleText, b56=SimpleText, b55=MockSep, b54=MockRandom, b53=Standard, d61=SimpleText, b59=MockVariableIntBlock(baseBlockSize=91), d60=MockSep, b58=MockSep, b57=Pulsing(freqCutoff=15), c62=MockSep, c61=Pulsing(freqCutoff=15), a59=Pulsing(freqCutoff=15), c60=Standard, a58=MockFixedIntBlock(blockSize=1289), a57=MockSep, a56=Pulsing(freqCutoff=15), a55=Standard, a54=SimpleText, a72=Standard, c67=MockRandom, a73=MockRandom, c68=MockVariableIntBlock(baseBlockSize=91), a74=MockSep, c69=SimpleText, a75=SimpleText, c63=Pulsing(freqCutoff=15), c64=MockSep, a70=MockRandom, c65=MockVariableIntBlock(baseBlockSize=91), a71=MockVariableIntBlock(baseBlockSize=91), c66=MockFixedIntBlock(blockSize=1289), d62=MockSep, d63=SimpleText, d64=MockFixedIntBlock(blockSize=1289), b70=MockFixedIntBlock(blockSize=1289), d65=Pulsing(freqCutoff=15), b71=MockRandom, d66=MockVariableIntBlock(baseBlockSize=91), b72=MockVariableIntBlock(baseBlockSize=91), d67=MockFixedIntBlock(blockSize=1289), b73=SimpleText, d68=Standard, b74=Standard, d69=MockRandom, b65=Pulsing(freqCutoff=15), b64=MockFixedIntBlock(blockSize=1289), b67=MockVariableIntBlock(baseBlockSize=91), b66=MockRandom, d70=Pulsing(freqCutoff=15), b69=MockRandom, b68=Standard, d72=MockVariableIntBlock(baseBlockSize=91), d71=MockRandom, c71=MockFixedIntBlock(blockSize=1289), c70=MockVariableIntBlock(baseBlockSize=91), a69=SimpleText, c73=MockRandom, c72=Standard, a66=MockFixedIntBlock(blockSize=1289), a65=MockVariableIntBlock(baseBlockSize=91), a68=MockRandom, a67=Standard, c32=MockSep, c33=SimpleText, c30=Standard, c31=MockRandom, c36=MockVariableIntBlock(baseBlockSize=91), a41=MockRandom, c37=MockFixedIntBlock(blockSize=1289), a42=MockVariableIntBlock(baseBlockSize=91), a0=SimpleText, c34=Pulsing(freqCutoff=15), c35=MockSep, a40=Pulsing(freqCutoff=15), b84=Pulsing(freqCutoff=15), d79=MockSep, b85=MockSep, b82=SimpleText, d77=Standard, c38=SimpleText, b83=Standard, d78=MockRandom, c39=Standard, b80=Standard, d75=MockRandom, b81=MockRandom, d76=MockVariableIntBlock(baseBlockSize=91), d73=MockFixedIntBlock(blockSize=1289), d74=Pulsing(freqCutoff=15), d83=Standard, a9=MockSep, d82=SimpleText, d81=MockVariableIntBlock(baseBlockSize=91), d80=MockRandom, b79=MockSep, b78=Standard, b77=SimpleText, b76=MockVariableIntBlock(baseBlockSize=91), b75=MockRandom, a1=SimpleText, a35=Pulsing(freqCutoff=15), a2=Standard, a34=MockFixedIntBlock(blockSize=1289), a3=Pulsing(freqCutoff=15), a33=SimpleText, a4=MockSep, a32=MockSep, a5=MockFixedIntBlock(blockSize=1289), a39=MockRandom, c40=Pulsing(freqCutoff=15), a6=Pulsing(freqCutoff=15), a38=Standard, a7=MockRandom, a37=MockFixedIntBlock(blockSize=1289), a8=MockVariableIntBlock(baseBlockSize=91), a36=MockVariableIntBlock(baseBlockSize=91), c41=MockFixedIntBlock(blockSize=1289), c42=Pulsing(freqCutoff=15), c43=MockRandom, c44=MockVariableIntBlock(baseBlockSize=91), c45=Standard, a50=SimpleText, c46=MockRandom, a51=Standard, c47=MockSep, a52=Pulsing(freqCutoff=15), c48=SimpleText, a53=MockSep, b93=MockVariableIntBlock(baseBlockSize=91), d88=MockFixedIntBlock(blockSize=1289), c49=MockVariableIntBlock(baseBlockSize=91), b94=MockFixedIntBlock(blockSize=1289), d89=Pulsing(freqCutoff=15), b95=Standard, b96=MockRandom, d84=SimpleText, b90=SimpleText, d85=Standard, b91=MockFixedIntBlock(blockSize=1289), d86=Pulsing(freqCutoff=15), b92=Pulsing(freqCutoff=15), d87=MockSep, d92=MockSep, d91=Pulsing(freqCutoff=15), d94=MockFixedIntBlock(blockSize=1289), d93=MockVariableIntBlock(baseBlockSize=91), b87=MockSep, b86=Pulsing(freqCutoff=15), d90=SimpleText, b89=MockFixedIntBlock(blockSize=1289), b88=MockVariableIntBlock(baseBlockSize=91), a44=MockVariableIntBlock(baseBlockSize=91), a43=MockRandom, a46=Standard, a45=SimpleText, a48=SimpleText, a47=MockSep, c51=Standard, a49=MockFixedIntBlock(blockSize=1289), c50=SimpleText, d98=MockFixedIntBlock(blockSize=1289), d97=MockVariableIntBlock(baseBlockSize=91), d96=MockSep, d95=Pulsing(freqCutoff=15), d99=MockRandom, a20=MockRandom, c99=MockVariableIntBlock(baseBlockSize=91), c98=MockRandom, c97=Pulsing(freqCutoff=15), c96=MockFixedIntBlock(blockSize=1289), b19=MockVariableIntBlock(baseBlockSize=91), a16=SimpleText, a17=Standard, b17=Pulsing(freqCutoff=15), a14=MockRandom, b18=MockSep, a15=MockVariableIntBlock(baseBlockSize=91), a12=MockVariableIntBlock(baseBlockSize=91), a13=MockFixedIntBlock(blockSize=1289), a10=Pulsing(freqCutoff=15), a11=MockSep, b11=MockFixedIntBlock(blockSize=1289), b12=Pulsing(freqCutoff=15), b10=SimpleText, b15=Standard, b16=MockRandom, a18=MockFixedIntBlock(blockSize=1289), b13=MockVariableIntBlock(baseBlockSize=91), a19=Pulsing(freqCutoff=15), b14=MockFixedIntBlock(blockSize=1289), b30=MockSep, a31=Pulsing(freqCutoff=15), a30=MockFixedIntBlock(blockSize=1289), b28=Standard, a25=Pulsing(freqCutoff=15), b29=MockRandom, a26=MockSep, a27=MockVariableIntBlock(baseBlockSize=91), a28=MockFixedIntBlock(blockSize=1289), a21=Standard, a22=MockRandom, a23=MockSep, a24=SimpleText, b20=MockRandom, b21=MockVariableIntBlock(baseBlockSize=91), b22=SimpleText, b23=Standard, a29=SimpleText, b24=MockSep, b25=SimpleText, b26=MockFixedIntBlock(blockSize=1289), b27=Pulsing(freqCutoff=15), b41=MockFixedIntBlock(blockSize=1289), b40=MockVariableIntBlock(baseBlockSize=91), c77=MockRandom, c76=Standard, c75=MockFixedIntBlock(blockSize=1289), c74=MockVariableIntBlock(baseBlockSize=91), c79=Standard, c78=SimpleText, c80=MockVariableIntBlock(baseBlockSize=91), c83=MockSep, c84=SimpleText, c81=Standard, b39=MockSep, c82=MockRandom, b37=MockRandom, b38=MockVariableIntBlock(baseBlockSize=91), b35=MockFixedIntBlock(blockSize=1289), b36=Pulsing(freqCutoff=15), b33=Pulsing(freqCutoff=15), b34=MockSep, b31=SimpleText, b32=Standard, str2=Pulsing(freqCutoff=15), b50=MockRandom, b52=SimpleText, str3=MockRandom, b51=MockSep, c86=SimpleText, tvtest=MockSep, c85=MockSep, c88=Pulsing(freqCutoff=15), c87=MockFixedIntBlock(blockSize=1289), c89=MockVariableIntBlock(baseBlockSize=91), c90=Pulsing(freqCutoff=15), c91=MockSep, c92=MockFixedIntBlock(blockSize=1289), c93=Pulsing(freqCutoff=15), c94=MockRandom, c95=MockVariableIntBlock(baseBlockSize=91), content1=MockSep, b46=SimpleText, b47=Standard, content3=Standard, b48=Pulsing(freqCutoff=15), content4=SimpleText, b49=MockSep, content5=MockRandom, b42=MockVariableIntBlock(baseBlockSize=91), b43=MockFixedIntBlock(blockSize=1289), b44=Standard, b45=MockRandom}, locale=lv_LV, timezone=Australia/Lindeman
NOTE: all tests run in this JVM:
[TestNumericTokenStream, TestIndexFileDeleter, TestIndexInput, TestIndexReaderCloneNorms, TestIndexReaderReopen, TestIndexWriter]
NOTE: FreeBSD 8.2-RELEASE amd64/Sun Microsystems Inc. 1.6.0 (64-bit)/cpus=16,threads=1,free=44228576,total=213778432
{noformat}"
1,"SloppyPhraseScorer returns non-deterministic results for queries with many repeatsProximity queries with many repeats (four or more, based on my testing) return non-deterministic results. I run the same query multiple times with the same data set and get different results.

So far I've reproduced this with Solr 1.4.1, 3.1, 3.2, 3.3, and latest 4.0 trunk.

Steps to reproduce (using the Solr example):
1) In solrconfig.xml, set queryResultCache size to 0.
2) Add some documents with text ""dog dog dog"" and ""dog dog dog dog"". http://localhost:8983/solr/update?stream.body=%3Cadd%3E%3Cdoc%3E%3Cfield%20name=%22id%22%3E1%3C/field%3E%3Cfield%20name=%22text%22%3Edog%20dog%20dog%3C/field%3E%3C/doc%3E%3Cdoc%3E%3Cfield%20name=%22id%22%3E2%3C/field%3E%3Cfield%20name=%22text%22%3Edog%20dog%20dog%20dog%3C/field%3E%3C/doc%3E%3C/add%3E&commit=true
3) Do a ""dog dog dog dog""~1 query. http://localhost:8983/solr/select?q=%22dog%20dog%20dog%20dog%22~1
4) Repeat step 3 many times.

Expected results: The document with id 2 should be returned.

Actual results: The document with id 2 is always returned. The document with id 1 is sometimes returned.

Different proximity values show the same bug - ""dog dog dog dog""~5, ""dog dog dog dog""~100, etc show the same behavior.

So far I've traced it down to the ""repeats"" array in SloppyPhraseScorer.initPhrasePositions() - depending on the order of the elements in this array, the document may or may not match. I think the HashSet may be to blame, but I'm not sure - that at least seems to be where the non-determinism is coming from."
1,"ObjectIterator may return null, which is not readily expected from an IteratorThe ObjectIterator class implements an Iterator of objects mapped from an underlying NodeIterator. This ObjectIterator may return null from next() if no mapping for a node in the iterator exists. Rather than returning null, the iterator should probably just ignore the unmappable node and return an object from the next node in the underlying iterator which is mappable."
1,Jcr-Server: ItemDefinitionImpl.toXml throws NPE for the root node.ItemDefinitionImpl.toXml throws NPE for the root node due to a missing assertion regarding the declaring nodetype.
1,"DataStore: garbage collection can fail when using workspace maxIdleTimeThe GarbageCollectorTest fails because some workspaces have an idle timeout. The data store garbage collector should prevent workspace close-on-idle.

Proposed solution: instead of using the 'regular' system sessions in the garbage collector, use special 'registered system sessions'. The sessions get garbage collected when no longer used, that means this patch requires that JCR-1216 ""Unreferenced sessions should get garbage collected"" is applied. So for each workspace, the code is:

// this will initialize the workspace if required
wspInfo.getSystemSession();

SessionImpl session = SystemSession.create(rep, wspInfo.getConfig());
// mark this session as 'active' for so the workspace does
// not get disposed by workspace-janitor until the garbage collector is done
rep.onSessionCreated(session);            
"
1,"RepositoryCopier does not copy open-scoped LocksIf you use the RepositoryCopier to make a backup of your repository and you have open-scoped (not session scoped) locks, these locks will not be copied. If you try to restore your copy of the repository all locks are gone."
1,"problem with edgengramtokenfilter and highlighteri ran into a problem while using the edgengramtokenfilter, it seems to report incorrect offsets when generating tokens, more specifically all the tokens have offset 0 and term length as start and end, this leads to goofy highlighting behavior when creating edge grams for tokens beyond the first one, i created a small patch that takes into account the start of the original token and adds that to the reported start/end offsets.

"
1,"SSL connections cannot be established using resolvable IP addressHttpClient 4.1 introduced a regression in establishing SSL connections to remote peers (it seems this is a common regression for major httpclient updates, see HTTPCLIENT-803).
The new SSLSocketFactory.connectSocket method calls the X509HostnameVerifier with InetSocketAddress.getHostName() parameter. When the selected IP address has a reverse lookup name, the verifier is called with the resolved name, and so the IP check fails.
4.0 release checked for original ip/hostname, but this cannot be done with the new connectSocket() method. 
The TestHostnameVerifier.java only checks 127.0.0.1/.2 and so masked the issue, because the matching certificate has both ""localhost"" and ""127.0.0.1"", but actually only ""localhost"" is matched. A test case with 8.8.8.8 would be better."
1,Extra </div> in populate.jspThe populate.jsp page in jackrabbit-webapp has an extra </div> that causes minor breakage to the page layout.
1,"Request with DIGEST authentication fails when redirectedRequest with DIGEST authentication fails when redirected due to invalid URI
parameter.

-- Client side log ----------------------------------------------------------

[DEBUG] HttpClient - -Java version: 1.2.2
[DEBUG] HttpClient - -Java vendor: Sun Microsystems Inc.
[DEBUG] HttpClient - -Operating system name: Linux
[DEBUG] HttpClient - -Operating system architecture: i386
[DEBUG] HttpClient - -Operating system version: 2.4.20-13.9-ok
[DEBUG] HttpClient - -SUN 1.2: SUN (DSA key/parameter generation; DSA signing;
SHA-1, MD5 digests; SecureRandom; X.509 certificates; JKS keystore)
[DEBUG] HttpClient - -SunJSSE 1.0301: Sun JSSE provider(implements RSA
Signatures, PKCS12, SunX509 key/trust factories, SSLv3, TLSv1)
[DEBUG] HttpConnection - -Creating connection for localhost using protocol http:80
[DEBUG] HttpConnection - -HttpConnection.setSoTimeout(0)
[DEBUG] HttpMethod - -Execute loop try 1
[DEBUG] wire - ->> ""GET /transfer HTTP/1.1[\r][\n]""
[DEBUG] HttpMethod - -Adding Host request header
[DEBUG] wire - ->> ""User-Agent: Jakarta Commons-HttpClient/2.0beta1[\r][\n]""
[DEBUG] wire - ->> ""Host: localhost[\r][\n]""
[DEBUG] wire - ->> ""[\r][\n]""
[DEBUG] wire - -<< ""HTTP/1.1 401 Authorization Required[\r][\n]""
[DEBUG] wire - -<< ""Date: Fri, 20 Jun 2003 08:30:06 GMT[\r][\n]""
[DEBUG] wire - -<< ""Server: Apache/2.0.40 (Red Hat Linux)[\r][\n]""
[DEBUG] wire - -<< ""WWW-Authenticate: Digest realm=""guest realm"",
nonce=""ei+T7oPAAwA=53c8e6d609ff81a8dcbc370b51f8aadec565009a"", algorithm=MD5,
domain=""/transfer"", qop=""auth""[\r][\n]""
[DEBUG] wire - -<< ""Vary: accept-language[\r][\n]""
[DEBUG] wire - -<< ""Accept-Ranges: bytes[\r][\n]""
[DEBUG] wire - -<< ""Content-Length: 1285[\r][\n]""
[DEBUG] wire - -<< ""Content-Type: text/html; charset=ISO-8859-1[\r][\n]""
[DEBUG] HttpMethod - -Authorization required
[DEBUG] HttpAuthenticator - -Using 'guest realm' authentication realm
[DEBUG] HttpMethod - -HttpMethodBase.execute(): Server demanded authentication
credentials, will try again.
...
[DEBUG] HttpMethod - -Resorting to protocol version default close connection policy
[DEBUG] HttpMethod - -Should NOT close connection, using HTTP/1.1.
[DEBUG] HttpMethod - -Execute loop try 2
[DEBUG] wire - ->> ""GET /transfer HTTP/1.1[\r][\n]""
[DEBUG] HttpMethod - -Request to add Host header ignored: header already added
[DEBUG] wire - ->> ""User-Agent: Jakarta Commons-HttpClient/2.0beta1[\r][\n]""
[DEBUG] wire - ->> ""Host: localhost[\r][\n]""
[DEBUG] wire - ->> ""Authorization: Digest username=""guest"", realm=""guest realm"",
nonce=""ei+T7oPAAwA=53c8e6d609ff81a8dcbc370b51f8aadec565009a"", uri=""/transfer"",
qop=""auth"", algorithm=""MD5"", nc=00000001,
cnonce=""81d4b905a4e9def944beaed8daf79283"",
response=""71394edcddf4bcee6237ea4bb50cfaa5""[\r][\n]""
[DEBUG] wire - ->> ""[\r][\n]""
[DEBUG] wire - -<< ""HTTP/1.1 301 Moved Permanently[\r][\n]""
[DEBUG] wire - -<< ""Date: Fri, 20 Jun 2003 08:30:06 GMT[\r][\n]""
[DEBUG] wire - -<< ""Server: Apache/2.0.40 (Red Hat Linux)[\r][\n]""
[DEBUG] wire - -<< ""Location: http://localhost/transfer/[\r][\n]""
[DEBUG] wire - -<< ""Content-Length: 302[\r][\n]""
[DEBUG] wire - -<< ""Content-Type: text/html; charset=iso-8859-1[\r][\n]""
[DEBUG] HttpMethod - -Redirect required
[DEBUG] HttpMethod - -Redirect requested to location 'http://localhost/transfer/'
[DEBUG] HttpMethod - -Redirecting from 'http://localhost:80/transfer' to
'http://localhost/transfer/
...
[DEBUG] HttpMethod - -Resorting to protocol version default close connection policy
[DEBUG] HttpMethod - -Should NOT close connection, using HTTP/1.1.
[DEBUG] HttpMethod - -Execute loop try 3
[DEBUG] wire - ->> ""GET /transfer/ HTTP/1.1[\r][\n]""
[DEBUG] HttpMethod - -Request to add Host header ignored: header already added
[DEBUG] wire - ->> ""User-Agent: Jakarta Commons-HttpClient/2.0beta1[\r][\n]""
[DEBUG] wire - ->> ""Host: localhost[\r][\n]""
[DEBUG] wire - ->> ""Authorization: Digest username=""guest"", realm=""guest realm"",
nonce=""ei+T7oPAAwA=53c8e6d609ff81a8dcbc370b51f8aadec565009a"", uri=""/transfer"",
qop=""auth"", algorithm=""MD5"", nc=00000001,
cnonce=""81d4b905a4e9def944beaed8daf79283"",
response=""71394edcddf4bcee6237ea4bb50cfaa5""[\r][\n]""
[DEBUG] wire - ->> ""[\r][\n]""
[DEBUG] wire - -<< ""HTTP/1.1 400 Bad Request[\r][\n]""
[DEBUG] wire - -<< ""Date: Fri, 20 Jun 2003 08:30:06 GMT[\r][\n]""
[DEBUG] wire - -<< ""Server: Apache/2.0.40 (Red Hat Linux)[\r][\n]""
[DEBUG] wire - -<< ""Vary: accept-language[\r][\n]""
[DEBUG] wire - -<< ""Accept-Ranges: bytes[\r][\n]""
[DEBUG] wire - -<< ""Content-Length: 973[\r][\n]""
[DEBUG] wire - -<< ""Connection: close[\r][\n]""
[DEBUG] wire - -<< ""Content-Type: text/html; charset=ISO-8859-1[\r][\n]""

-- End of client side log -----------------------------------------------------


-- Server side log ------------------------------------------------------------

[Fri Jun 20 10:30:06 2003] [error] [client 127.0.0.1] Digest: uri mismatch -
</transfer> does not match request-uri </transfer/>

-- End of server side log -----------------------------------------------------"
1,"MultiThreadedHttpConnectionManager does not properly respond to thread interruptsMultiThreadedHttpConnectionManager uses interrupts to notify waiting threads when a connection is ready for them. Issues arise if the threads are interrupted by someone else while they are still waiting on a thread, because doGetConnection does not remove the threads from the queue of waiting threads when they are interrupted:

                        connectionPool.wait(timeToWait);

                        // we have not been interrupted so we need to remove ourselves from the 
                        // wait queue
                        hostPool.waitingThreads.remove(waitingThread);                        connectionPool.waitingThreads.remove(waitingThread);
                    } catch (InterruptedException e) {
                        // do nothing                    } finally {
                        if (useTimeout) {
                            endWait = System.currentTimeMillis();
                            timeToWait -= (endWait - startWait);                        }                    }

Under ordinary circumstances, the queue maintenance is done by the notifyWaitingThread method. However, if the thread is interrupted by any other part of the system, it will (1) not actually be released, since the loop in doGetConnection will force it back to the wait, and (2) will be added the waiting thread to the queue repeatedly, which basically means that the thread will eventually receive the interrupt from notifyWaitingThread at some later point, when it is no longer actually waiting for a connection.

This code could probably be re-architected to make it less error-prone, but the fundamental issue seems to be the use of interrupts to signal waiting threads, as opposed to something like a notify. "
1,"Value#getBinary() and #getStream() return internal representation for type PATH and NAMEjust found a path-related spi2dav test failing that passed some time before jackrabbit 2.0 (BatchTest#testSetPathValue).

i had a quick look at it and it seems to me that the reasons is the internal (Path, Name) value representation 
being exposed when calling Value#getBinary(), Value#getStream() and the corresponding shortcuts on Property.

from my understanding of the specification these methods should always return the standard JCR path (or name) representation as it
is exposed by Value#getString() and Property#getString() as it used to be in previous versions.



"
1,"[PATCH] Javadoc improvements and minor fixesJavadoc improvements for Scorer.java and Weight.java. 
This also fixes some recent changes introduced minor warnings when building 
the javadocs and adds a small comment in Similarity.java. 
The individual patches will be attached."
1,"Handling of multiple residual prop defs in EffectiveNodeTypeImplorg.apache.jackrabbit.jcr2spi.nodetype.EffectiveNodeTypeImpl currently rejects multiple residual property definitions, if they do not differ in getMultiple(). In fact, it should accept all combinations, so differing values for getOnParentVersionAction and other aspects should be accepted as well.

See JSR 170, 6.7.8:

""For purposes of the above, the notion of two definitions having the same name does not apply to two residual definitions. Two (or more) residual property or child node definitions with differing subattributes must be permitted to co-exist in the same effective node type. They are interpreted as disjunctive (ORed) options."""
1,"Session.save() and Session.refresh(boolean) rely on accessibility of the root nodefollow-up issue to JCR-2418:

an editing session that is only allowed to write in a subtree but isn't allowed to access the root node will not be
able to save or revert changes made in the transient space within that subtree.

the reason for this is, that both SessionImpl.save() and SessionImpl.refresh(boolean) access the root node
in order to execute the call. since it's the regular call READ permissions are checked, although the user
made no attempt to *look* at the root.

A workaround would be to call Item.save() on the modified tree itself that obviously was visible for the 
user... unfortunately that method is deprecated as of JCR 2.0. Therefore, I have the impression that we
should fix the methods mentioned above.

"
1,"incorrect HTML excerpt generation for queries on japanese text content The generated excerpt highlights single characters instead of full words. Test case (to be added to FullTextQueryTest):

     public void testJapaneseAndHighlight() throws RepositoryException {
        // http://translate.google.com/#auto|en|%E3%82%B3%E3%83%B3%E3%83%86%E3%83%B3%E3%83%88
        String jContent = ""\u30b3\u30fe\u30c6\u30f3\u30c8"";
        // http://translate.google.com/#auto|en|%E3%83%86%E3%82%B9%E3%83%88
        String jTest = ""\u30c6\u30b9\u30c8"";
        
        String content = ""some text with japanese: "" + jContent
                + "" ('content')"" + "" and "" + jTest + "" ('test')."";

        // expected excerpt; note this may change if excerpt providers change
        String expectedExcerpt = ""<div><span>some text with japanese: "" + jContent
                + "" ('content') and <strong>"" + jTest
                + ""</strong> ('test').</span></div>"";
        
        Node n = testRootNode.addNode(""node1"");
        n.setProperty(""title"", content);
        testRootNode.getSession().save();
        
        String xpath = ""/jcr:root"" + testRoot + ""/element(*, nt:unstructured)""
                + ""[jcr:contains(., '"" + jTest + ""')]/rep:excerpt(.)"";
        Query q = superuser.getWorkspace().getQueryManager()
                .createQuery(xpath, Query.XPATH);
        
        QueryResult qr = q.execute();
        RowIterator it = qr.getRows();
        int cnt = 0;
        while (it.hasNext()) {
            cnt++;
            Row found = it.nextRow();
            assertEquals(n.getPath(), found.getPath());
            String excerpt = found.getValue(""rep:excerpt(.)"").getString();
            assertEquals(expectedExcerpt, excerpt);
        }
        
        assertEquals(1, cnt);
    }
"
1,"(Parallel-)MultiSearcher: using Sort object changes the scoresExample: 
Hits hits=multiSearcher.search(query);
returns different scores for some documents than
Hits hits=multiSearcher.search(query, Sort.RELEVANCE);
(both for MultiSearcher and ParallelMultiSearcher)

The documents returned will be the same and in the same order, but the scores in the second case will seem out of order.

Inspecting the Explanation objects shows that the scores themselves are ok, but there's a bug in the normalization of the scores.

The document with the highest score should have score 1.0, so all document scores are divided by the highest score.  (Assuming the highest score was>1.0)

However, for MultiSearcher and ParallelMultiSearcher, this normalization factor is applied *per index*, before merging the results together (the merge itself is ok though).

An example: if you use
Hits hits=multiSearcher.search(query, Sort.RELEVANCE);
for a MultiSearcher with two subsearchers, the first document will have score 1.0.
The next documents from the same subsearcher will have decreasing scores.
The first document from the other subsearcher will however have score 1.0 again !

The same applies for other Sort objects, but it is less visible.

I will post a TestCase demonstrating the problem and suggested patches to solve it in a moment..."
1,"RMIRemoteBindingServlet fails to initialize if the RMI registry is not availableIf the RMI registry is not available, the RMIRemoteBindingServlet in jcr-rmi will throw an exception in the init() method and prevent the servlet from being loaded.

The same servlet can however also be mapped to the normal HTTP URL space as an alternative mechanism of making the RMI endpoint available to clients. Thus it would be better if the init() method just logged a warning instead of failing completely."
1,"VirtualItemStates of node types definitions not accessible with uuidThe VirtualNodeTypeStateProvider that maps node type definitions into the workspace under /jcr:system/jcr:nodeTypes does not implement the methods:

- internalGetNodeState(NodeId id)
- internalHasNodeState(NodeId id)

This has the effect that ItemStates that reflect node type definitions are not accessible directly with their uuid."
1,"Deadlock during checkinUnder a load of 3 threads performing checkin and restore operations it's possible for all to become deadlocked in AbstractVersionManager.checkin(). This method attempts to upgrade a read lock to a write lock with the following code

    aquireReadLock();
    ....

    try {
        aquireWriteLock();
        releaseReadLock();
        ...

If 2 or more threads acquire the read lock then neither can acquire the write lock resulting in the deadlock, and after that any other thread that calls this method will block waiting for the write lock. The release of the read lock needs to be done before acquiring the write lock, this is documented Concurrent library javadoc.

There is another area where there is an attempt to upgrade a read lock to write lock, RepositoryImpl.WorkspaceInfo.disposeIfIdle() acquires a read lock and calls dispose() which then acquires a write lock, this maybe ok, as I assume there is only 1 thread that will attempt to dispose of idle workspaces.
"
1,"When BG merge hits an exception, optimize sometimes throws an IOException missing the root cause
When IndexWriter.optimize() is called, ConcurrentMergeScheduler will
run the requested merges with background threads and optimize() will
wait for these merges to complete.

If a merge hits an exception, it records the root cause exception such
that optimize can then retrieve this root cause and throw its own
exception, with the root cause.

But there is a bug: sometimes, the fact that an exception occurred on
a merge is recorded, but the root cause is missing.  In this cause,
optimize() still throws an exception (correctly indicating that the
optimize() has not finished successfully), but it's not helpful
because it's missing the root cause.  You must then go find the root
cause in the JRE's stderr logs.

This has hit a few users on this lists, most recently:

  http://www.nabble.com/Background-merge-hit-exception-td19540409.html#a19540409

I found the isssue, and finally got a unit test to intermittently show
it.  It's a simple thread safety issue: in a finally clause in
IndexWriter.merge we record the fact that the merge hit an exception
before actually setting the root cause, and then only in
ConcurrentMergeScheduler's exception handler do we set the root
cause.  If the optimize thread is scheduled in between these two, it
can throw an exception missing its root cause.

The fix is straightforward.  I plan to commit to 2.4 & 2.9.
"
1,"Saving a node deletion that has been modified externally throws a ConstraintViolationExceptionDeleting a node ""a"" and saving its parent might result in a ConstraintViolationException if node ""a"" has been modified externally, where an InvalidItemStateException with message ""item x has been modified externally"" would be more intuitive.
"
1,"Transient states should be persisted in depth-first traversal orderInside Node.save(), when filling the list of transient (modified) items, the node itself is added first (if transient) and all transient descendant nodes in depth-first order. This can lead to the following problem with shareable nodes and path-based access management: 

1) assume a node N has a shared child S, which is shared with at least one other node N'
2) S.removeShare is invoked: this removes S from the list of child nodes in N
3) N.save is invoked

N is persisted first, then S. If a path-based access manager tries to build the path of S after N has been persisted, S will no longer be returned in the list of removed child node entries, and an exception will be thrown. This can be circumvented by adding N last."
1,"SQL2 query - supplying column selector fails with NPE on getColumnName()I am preparing and executing an SQL2 query (JCR 2.0) as follows:

QueryManager qm = jcrSession.getWorkspace().getQueryManager();
String queryString = ""select order.[customerAccountUUID] as cust from [atl:order] as order"";
Query query = qm.createQuery(queryString, Query.JCR_SQL2);
QueryResult queryResult = query.execute();

The following query fails:

select order.[customerAccountUUID] from [atl:order] as order

java.lang.NullPointerException
        org.apache.jackrabbit.commons.query.sql2.QOMFormatter.isSimpleName(QOMFormatter.java:577)
        org.apache.jackrabbit.commons.query.sql2.QOMFormatter.formatName(QOMFormatter.java:567)
        org.apache.jackrabbit.commons.query.sql2.QOMFormatter.format(QOMFormatter.java:452)
        org.apache.jackrabbit.commons.query.sql2.QOMFormatter.format(QOMFormatter.java:123)
        org.apache.jackrabbit.commons.query.sql2.QOMFormatter.format(QOMFormatter.java:117)

line 452: c.getColumnName() returns null.

The following query is fine:

select order.[customerAccountUUID] as cust from [atl:order] as order

I have been using the test case (here: http://svn.apache.org/viewvc/jackrabbit/trunk/jackrabbit-spi-commons/src/test/resources/org/apache/jackrabbit/spi/commons/query/sql2/test.sql2.txt?view=markup) as a guideline.

Cheers,

James "
1,"Empty response body is not properly handled when chunked encoding is usedIIS 5.0 server, when returning no content in response to an HTTP/1.1 request,
still includes ""Transfer-Encoding: chunked"" response header. As HttpClient
always expects chunk-encoded stream to be properly terminated, an
HttpRecoverableException exception results, when no content is sent back

=====================================================================

POST /someurl.aspx HTTP/1.1
Content-Length: 1132
Host: xxx.xxx.xxx.xxx
User-Agent: Jakarta Commons-HttpClient/2.0alpha2
Content-Type: multipart/form-data; boundary=----------------314159265358979323846

------------------314159265358979323846
Content-Disposition: form-data; name=""nmFile""; filename=""xxxxxxxxx.xml""
Content-Type: application/octet-stream

<... content removed ...>

------------------314159265358979323846--

HTTP/1.1 200 OK
Server: Microsoft-IIS/5.0
Date: Sat, 08 Feb 2003 15:22:26 GMT
Transfer-Encoding: chunked
Cache-Control: private
Content-Type: text/html

=====================================================================

Bug reported by Jim Crossley"
1,"coord should still apply to missing terms/clausesMissing terms in a boolean query ""disappear"" (i.e. they don't even affect the coord factor)."
1,"Phrase query with term repeated 3 times requires more slop than expectedConsider a document with the text ""A A A"".
The phrase query ""A A A"" (exact match) succeeds.
The query ""A A A""~1 (same document and query, just increasing the slop value by one) fails.
""A A A""~2 succeeds again.

If the exact match succeeds, I wouldn't expect the same query but with more slop to fail.  The fault seems to require some term to be repeated at least three times in the query, but the three occurrences do not need to be adjacent.  I will attach a file that contains a set of JUnit tests that demonstrate what I mean."
1,"Lucene is not fsync'ing files on commitThanks to hurricane Irene, when Mark's electricity became unreliable, he discovered that on power loss Lucene could easily corrumpt the index, which of course should never happen...

I was able to easily repro, by pulling the plug on an Ubuntu box during indexing.  On digging, I discovered, to my horror, that Lucene is failing to fsync any files, ever!

This bug was unfortunately created when we committed LUCENE-2328... that issue added tracking, in FSDir, of which files have been closed but not sync'd, so that when sync is called during IW.commit we only sync those files that haven't already been sync'd.

That tracking is done via the FSDir.onIndexOutputClosed callback, called when an FSIndexOutput is closed.  The bug is that we only call it on exception during close:

{noformat}

    @Override
    public void close() throws IOException {
      // only close the file if it has not been closed yet
      if (isOpen) {
        boolean success = false;
        try {
          super.close();
          success = true;
        } finally {
          isOpen = false;
          if (!success) {
            try {
              file.close();
              parent.onIndexOutputClosed(this);
            } catch (Throwable t) {
              // Suppress so we don't mask original exception
            }
          } else
            file.close();
        }
      }
    }
{noformat}

And so FSDir thinks no files need syncing when its sync method is called....

I think instead we should call it up-front; better to over-sync than under-sync.

The fix is trivial (move the callback up-front), but I'd love to somehow have a test that can catch such a bad regression in the future.... still I think we can do that test separately and commit this fix first.

Note that even though LUCENE-2328 was backported to 2.9.x and 3.0.x, this bug wasn't, ie the backport was a much simpler fix (to just address the original memory leak); it's 3.1, 3.2, 3.3 and trunk when this bug is present."
1,"Bug in UtilDateTypeConverterImplIn this converter following line is used:
return this.getValueFactory().createValue(((java.util.Date) propValue).getTime());
but propValue must be converted to java.util.Calendar, not into long! ValueFactory than converts to LongValue not DateValue as expected.

Following code works OK:
final long timeInMilis = ((java.util.Date) propValue).getTime();
final Calendar calendar = Calendar.getInstance();
calendar.setTimeInMillis(timeInMilis);
return this.getValueFactory().createValue( calendar );

but I dont know better Date-> Calendar conversion.
"
1,"PathFactoryImpl creates illegal Path objectsit is currently possible to create illegal/inconsistent paths using the default path factory.
Path objects are expected to represent syntactically correct paths.

some examples:

            PathFactory pf = PathFactoryImpl.getInstance();
            Path.Element re = pf.getRootElement();
            Path illegalPath = pf.create(new Path.Element[]{re, re});
            
            Path.Element pe = pf.getParentElement();
            Path nonNormalizedPath = pf.create(new Path.Element[]{pe, pe});    // ""../..""
            assertFalse(nonNormalizedPath.isNormalized());

"
1,"FSDirectory.getDirectory always creates index pathThis was reported to me as a Luke bug, but going deeper it proved to be a non-intuitive (broken?) behavior of FSDirectory.

If you use FSDirectory.getDirectory(File nonexistent) on a nonexistent path, but one that is located under some existing parent path, then FSDirectory:174 uses file.mkdirs() to create this directory. One would expect a variant of the method with a boolean flag to decide whether or not to create the output path. However, the API with ""create"" flag is now deprecated, with a comment that points to IndexWriter's ""create"" flag. This comment is misleading, because the indicated path is created anyway in the file system just by calling FSDirectory.getDirectory().

I propose to do one of the following:

* reinstate the variant of the method with ""create"" flag. In case if this flag is false, and the index directory is missing, either return null or throw an IOException,

* keep the API as it is now, but either return null or throw IOException if the index dir is missing. This breaks the backwards compatibility, because now users are required to do file.mkdirs() themselves prior to calling FSDirectory.getDirectory()."
1,"TCK: Incorrect check of namespace mappings in System View XML exportorg.apache.jackrabbit.test.api.SysViewContentHandler. In endDocument(), two issues:

1. line 351: tries to go through a table of prefixes but uses a fixed index inside the loop;
2. The mapping for the 'xml' prefix should be skipped (it must be registered in the Session but must not be registered during export since this is a built-in XML mapping."
1,"ParameterParser parse method for authentication headers does not appear to deal with empty value stringsHi, I have found an issue with HTTPClient due to the way it parses parameter 
strings.

In particular, consider the following WWW-Authenticate header:

WWW-Authenticate: Digest realm="""", algorithm=MD5, qop=""auth"", 
domain=""/content"", nonce=""0e11dcf146563c3a89e5327f0c5f5bad""
 
The realm is definitely specified, but is equal to the empty string.  It is not 
a null value.

However, the extractParams method of AuthChallengeParser which in turn calls 
ParameterParser will actually parse the value as Null  instead of an empty 
string.

This is due to parseQuotedToken getToken(true) call which essentially returns a 
null String result  as the condition i2>i1 fails :-

        String result = null;
        if (i2 > i1) {
            result = new String(chars, i1, i2 - i1);
        }
        return result;

As the processChallenge method of DigestScheme throws an exception when 
getParameter(""realm"") == null, HTTPClient is not able to process the digest 
request when an empty string realm value is present."
1,"HttpMethod#getResponseBody throws NPEHttpMethod#getResponseBody throws an NPE if the response from the server was 
204.  Shouldn't getResponseBody return null by contract instead of throwing 
NPE?"
1,"Incorrect node position after importI have found a behavior that does not seem to be consistent with the
spec:

After replacing a node with importXML using IMPORT_UUID_COLLISION_REPLACE_EXISTING the new node is not at the position of the replaced node (talking about the position among the siblings).

The origininal node is removed, but the new node is created as the last child of the parent node, and not spec-compliant at the position of the replaced node.

Here how I use it:

// assume Session s, Node n, String text (holding XML data)

s.importXML(
	n.getPath(), 
	new ByteArrayInputStream (text.getBytes(""UTF-8"")),
	ImportUUIDBehavior.IMPORT_UUID_COLLISION_REPLACE_EXISTING
);
s.save();
 

And here a quote from the spec section 7.3.6

ImportUUIDBehavior.IMPORT_UUID_COLLISION_REPLACE_EXISTING: 
If an incoming referenceable node has the same UUID as a node already existing in the workspace then the already existing node is replaced by the incoming node in the same position as the existing node.
[note ""same position""]
"
1,Query may throw ArrayIndexOutOfBoundsExceptionThere's a bug in DescendantSelfAxisQuery.DescendantSelfAxisScorer.skipTo() that causes the exception.
1,NearSpansOrdered.getPayload does not return the payload from the minimum match span
1,"CharFilters not being invoked in Solr
On Solr trunk, *all* CharFilters have been non-functional since LUCENE-3396 was committed in r1175297 on 25 Sept 2011, until Yonik's fix today in r1235810; Solr 3.x was not affected - CharFilters have been working there all along."
1,"SpanMultiTermQueryWrapper with Prefix Query issueIf we try to do a search with SpanQuery and a PrefixQuery this message is returned:

""You can only use SpanMultiTermQueryWrapper with a suitable SpanRewriteMethod.""

The problem is in the WildcardQuery rewrite function.

If the wildcard query is a prefix, a new prefix query is created, the rewrite method is set with the SpanRewriteMethod and the prefix query is returned.

But, that's the rewritten prefix query which should be returned:

-      return rewritten;
+      return rewritten.rewrite(reader);

I will attach a patch with a unit test included.



"
1,"DateTools needs to use UTC for correct collation,If your local timezone is Europe/London then the times Sun, 30 Oct 2005 00:00:00 +0000 and exactly one hour later are both converted to 200530010000 by DateTools.dateToString() with minute resolution.   The Linux date command is useful in seeing why:

    $ date --date ""Sun, 30 Oct 2005 00:00:00 +0000""
    Sun Oct 30 01:00:00 BST 2005

    $ date --date ""Sun, 30 Oct 2005 01:00:00 +0000""
    Sun Oct 30 01:00:00 GMT 2005

Both times are 1am in the morning, but one is when DST is in force, the other isn't.   Of course, these are actually different times!

Of course, if dates are stored in the index with implicit timezone information then not only do we get problems when the clocks go back at the end of summer, but we also have problems crossing timezones.   If a database is created in California and used in Paris then the times are going to be badly skewed (there's a nine hour time difference most of the year).
"
1,"ChunkedInputStream incorrectly handles chunksize without semicolonChunkedInputStream does not correctly read the chunk size when a semicolon does
not appear in the first line of the chunk.  If whitespace exists between the
chunk size value and the end of line and no semicolon is present, the whitespace
is not removed before parseInt is called resulting in an IOException ""Bad chunk
size""

I can not tell from RFC2616 if whitespace is legal here, but I have received it
from at least one web server.  The relevant section is 3.6.1.

A small patch repairs the problem.  I will attach it immediately."
1,"""overwriting cached entry"" warningswhen using multiple concurrent sessions you'll find *lots* of log entries like:

    03.11.2010 21:17:03 *WARN * ItemStateReferenceCache: overwriting cached entry ad79ca57-5eb1-4b7d-a439-a9fd73cc8c5a (ItemStateReferenceCache.java, line 176)

those are actually legitimate warnings since there's a siginificant risk of data loss/inconsistency involved.

this is apparently a regression of changes introduced by JCR-2699, specifically svn r1004223"
1,"Moved node disappearsMoving a node and then refreshing it can make it disappear.

deleteDirectory(new File(""repository""));
Repository rep = new TransientRepository();
Session session = rep.login(new SimpleCredentials("""", new char[0]));
Node root = session.getRootNode();
Node a = root.addNode(""a"");
Node b = a.addNode(""b"");
session.save();
session.move(""/a/b"", ""/b"");
b.refresh(false);
// session.save(); // no effect
for (NodeIterator it = root.getNodes(); it.hasNext();) {
    Node n = it.nextNode();
    System.out.println(n.getName());
    for (NodeIterator it2 = n.getNodes(); it2.hasNext();) {
        System.out.println(""  "" + it2.nextNode().getName());
    }
}

In the trunk, the node 'b' is not listed after the refresh (not under the root page, and not under a). The output is:
jcr:system
  jcr:versionStorage
  jcr:nodeTypes
a


Jackrabbit 1.4.x throws an exception:

jcr:system
  jcr:versionStorage
  jcr:nodeTypes
a
Exception in thread ""main"" javax.jcr.RepositoryException: failed to resolve name of acee31c4-c33b-4ed4-b1b5-39db6f17fb09
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getName(HierarchyManagerImpl.java:451)
	at org.apache.jackrabbit.core.CachingHierarchyManager.getName(CachingHierarchyManager.java:287)
	at org.apache.jackrabbit.core.NodeImpl.getName(NodeImpl.java:1931)
	at org.apache.jackrabbit.core.fuzz.TestMoveRemoveRefresh.test(TestMoveRemoveRefresh.java:33)
	at org.apache.jackrabbit.core.fuzz.TestMoveRemoveRefresh.main(TestMoveRemoveRefresh.java:15)


void deleteDirectory(File file) {
    if (file.isDirectory()) {
        File[] list = file.listFiles();
        for(int i=0; i<list.length; i++) {
            deleteDirectory(list[i]);
        }
    }
    file.delete();
}
"
1,"JCR2SPI; setProperty(name, date-string) fails when property is added and property type is PropertyType.DATE.Example code:

        Node l_parent = (Node)session.getItem(this.m_path);
        
        Node l_test = l_parent.addNode(""createcontenttest"", ""nt:file"");
        Node l_content = l_test.addNode(""jcr:content"", ""nt:resource"");
        
        l_content.setProperty(""jcr:encoding"", ""UTF-8"");
        l_content.setProperty(""jcr:mimeType"", ""text/plain"");
        l_content.setProperty(""jcr:data"", new ByteArrayInputStream(""foobar"".getBytes()));
        l_content.setProperty(""jcr:lastModified"", ""2007-07-25T17:04:00.000Z""); // TODO: this should work as well, bug in JCR2SPI?
        session.save();

This will fail when the property is defined as DATE, what should happen is that a value comparison is attempted (note that it works when the property already exists and just is overwritten).

The exception is:

javax.jcr.nodetype.ConstraintViolationException: no matching property definition found for {http://www.jcp.org/jcr/1.0}lastModified
        at org.apache.jackrabbit.jcr2spi.nodetype.ItemDefinitionProviderImpl.getQPropertyDefinition(ItemDefinitionProviderImpl.java:269)
        at org.apache.jackrabbit.jcr2spi.nodetype.ItemDefinitionProviderImpl.getQPropertyDefinition(ItemDefinitionProviderImpl.java:159)
        at org.apache.jackrabbit.jcr2spi.NodeImpl.getApplicablePropertyDefinition(NodeImpl.java:1672)
        at org.apache.jackrabbit.jcr2spi.NodeImpl.createProperty(NodeImpl.java:1369)
        at org.apache.jackrabbit.jcr2spi.NodeImpl.setProperty(NodeImpl.java:264)
        at org.apache.jackrabbit.jcr2spi.NodeImpl.setProperty(NodeImpl.java:345)
        at org.apache.jackrabbit.jcr2spi.NodeImpl.setProperty(NodeImpl.java:336)
"
1,"UserManager throws javax.jcr.query.InvalidQueryException on createUserThe UserManager method createUser(String userID, String password) throws an exception (javax.jcr.query.InvalidQueryException) if the user name contains a '@' character.

Stack trace:
Exception in thread ""main"" javax.jcr.query.InvalidQueryException: Lexical error at line 1, column 76.  Encountered: ""@"" (64), after : """" for statement: for $v in /jcr:root/rep:security/rep:authorizables/rep:groups//element(test@example.com,rep:Group) return $v: Lexical error at line 1, column 76.  Encountered: ""@"" (64), after : """": Lexical error at line 1, column 76.  Encountered: ""@"" (64), after : """"
        at org.apache.jackrabbit.spi.commons.query.xpath.XPathQueryBuilder.<init>(XPathQueryBuilder.java:302)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPathQueryBuilder.createQuery(XPathQueryBuilder.java:331)
        at org.apache.jackrabbit.spi.commons.query.xpath.QueryBuilder.createQueryTree(QueryBuilder.java:39)
        at org.apache.jackrabbit.spi.commons.query.QueryParser.parse(QueryParser.java:57)
        at org.apache.jackrabbit.core.query.lucene.QueryImpl.<init>(QueryImpl.java:91)
        at org.apache.jackrabbit.core.query.lucene.SearchIndex.createExecutableQuery(SearchIndex.java:615)
        at org.apache.jackrabbit.core.query.QueryImpl.init(QueryImpl.java:128)
        at org.apache.jackrabbit.core.SearchManager.createQuery(SearchManager.java:282)
        at org.apache.jackrabbit.core.query.QueryManagerImpl.createQuery(QueryManagerImpl.java:102)
        at org.apache.jackrabbit.core.security.user.IndexNodeResolver.buildQuery(IndexNodeResolver.java:105)
        at org.apache.jackrabbit.core.security.user.IndexNodeResolver.findNode(IndexNodeResolver.java:50)
        at org.apache.jackrabbit.core.security.user.UserManagerImpl.getAuthorizable(UserManagerImpl.java:93)
        at org.apache.jackrabbit.core.security.user.UserManagerImpl.createUser(UserManagerImpl.java:177)
        at org.apache.jackrabbit.core.security.user.UserManagerImpl.createUser(UserManagerImpl.java:158)
        at FirstHop.main(FirstHop.java:20)
Caused by: org.apache.jackrabbit.spi.commons.query.xpath.TokenMgrError: Lexical error at line 1, column 76.  Encountered: ""@"" (64), after : """"
        at org.apache.jackrabbit.spi.commons.query.xpath.XPathTokenManager.getNextToken(XPathTokenManager.java:13263)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.jj_ntk(XPath.java:9187)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ElementTest(XPath.java:8745)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.KindTest(XPath.java:8120)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.NodeTest(XPath.java:5041)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.AbbrevForwardStep(XPath.java:4891)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ForwardStep(XPath.java:4747)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.AxisStep(XPath.java:4692)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.StepExpr(XPath.java:4597)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.RelativePathExpr(XPath.java:4547)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.PathExpr(XPath.java:4396)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ValueExpr(XPath.java:4125)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.UnaryExpr(XPath.java:4032)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.CastExpr(XPath.java:3935)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.CastableExpr(XPath.java:3898)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.TreatExpr(XPath.java:3861)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.InstanceofExpr(XPath.java:3824)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.IntersectExceptExpr(XPath.java:3748)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.UnionExpr(XPath.java:3672)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.MultiplicativeExpr(XPath.java:3586)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.AdditiveExpr(XPath.java:3510)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.RangeExpr(XPath.java:3451)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ComparisonExpr(XPath.java:3353)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.AndExpr(XPath.java:3290)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.OrExpr(XPath.java:3227)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ExprSingle(XPath.java:2214)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ForClause(XPath.java:2337)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.FLWORExpr(XPath.java:2233)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ExprSingle(XPath.java:2133)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.Expr(XPath.java:2094)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.QueryBody(XPath.java:2066)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.MainModule(XPath.java:512)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.Module(XPath.java:387)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.QueryList(XPath.java:151)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.XPath2(XPath.java:118)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPathQueryBuilder.<init>(XPathQueryBuilder.java:295)
        ... 14 more
org.apache.jackrabbit.spi.commons.query.xpath.TokenMgrError: Lexical error at line 1, column 76.  Encountered: ""@"" (64), after : """"
        at org.apache.jackrabbit.spi.commons.query.xpath.XPathTokenManager.getNextToken(XPathTokenManager.java:13263)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.jj_ntk(XPath.java:9187)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ElementTest(XPath.java:8745)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.KindTest(XPath.java:8120)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.NodeTest(XPath.java:5041)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.AbbrevForwardStep(XPath.java:4891)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ForwardStep(XPath.java:4747)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.AxisStep(XPath.java:4692)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.StepExpr(XPath.java:4597)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.RelativePathExpr(XPath.java:4547)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.PathExpr(XPath.java:4396)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ValueExpr(XPath.java:4125)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.UnaryExpr(XPath.java:4032)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.CastExpr(XPath.java:3935)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.CastableExpr(XPath.java:3898)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.TreatExpr(XPath.java:3861)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.InstanceofExpr(XPath.java:3824)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.IntersectExceptExpr(XPath.java:3748)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.UnionExpr(XPath.java:3672)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.MultiplicativeExpr(XPath.java:3586)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.AdditiveExpr(XPath.java:3510)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.RangeExpr(XPath.java:3451)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ComparisonExpr(XPath.java:3353)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.AndExpr(XPath.java:3290)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.OrExpr(XPath.java:3227)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ExprSingle(XPath.java:2214)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ForClause(XPath.java:2337)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.FLWORExpr(XPath.java:2233)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ExprSingle(XPath.java:2133)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.Expr(XPath.java:2094)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.QueryBody(XPath.java:2066)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.MainModule(XPath.java:512)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.Module(XPath.java:387)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.QueryList(XPath.java:151)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.XPath2(XPath.java:118)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPathQueryBuilder.<init>(XPathQueryBuilder.java:295)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPathQueryBuilder.createQuery(XPathQueryBuilder.java:331)
        at org.apache.jackrabbit.spi.commons.query.xpath.QueryBuilder.createQueryTree(QueryBuilder.java:39)
        at org.apache.jackrabbit.spi.commons.query.QueryParser.parse(QueryParser.java:57)
        at org.apache.jackrabbit.core.query.lucene.QueryImpl.<init>(QueryImpl.java:91)
        at org.apache.jackrabbit.core.query.lucene.SearchIndex.createExecutableQuery(SearchIndex.java:615)
        at org.apache.jackrabbit.core.query.QueryImpl.init(QueryImpl.java:128)
        at org.apache.jackrabbit.core.SearchManager.createQuery(SearchManager.java:282)
        at org.apache.jackrabbit.core.query.QueryManagerImpl.createQuery(QueryManagerImpl.java:102)
        at org.apache.jackrabbit.core.security.user.IndexNodeResolver.buildQuery(IndexNodeResolver.java:105)
        at org.apache.jackrabbit.core.security.user.IndexNodeResolver.findNode(IndexNodeResolver.java:50)
        at org.apache.jackrabbit.core.security.user.UserManagerImpl.getAuthorizable(UserManagerImpl.java:93)
        at org.apache.jackrabbit.core.security.user.UserManagerImpl.createUser(UserManagerImpl.java:177)
        at org.apache.jackrabbit.core.security.user.UserManagerImpl.createUser(UserManagerImpl.java:158)
        at FirstHop.main(FirstHop.java:20)

"
1,"Revision 949509 (LUCENE-2480) causes IOE ""read past EOF"" when processing older format SegmentInfo data when JVM assertion processing is disabled.At revision 949509 in org.apache.lucene.index.SegmentInfo at line 155, there is the following code:
{noformat} 
    if (format > SegmentInfos.FORMAT_4_0) {
      // pre-4.0 indexes write a byte if there is a single norms file
      assert 1 == input.readByte();
    }
{noformat} 
Note that the assert statement invokes input.readByte().
If asserts are disabled for the JVM, input.readByte() will not be invoked, causing the following readInt() to return a bogus value, and then causing an IOE during the (mistakenly entered) loop at line 165.
This can occur when processing old format (format ""-9"") index data under Tomcat (whose startup scripts by default do not turn on asserts).

Full stacktrace:
{noformat} 
SEVERE: java.lang.RuntimeException: java.io.IOException: read past EOF
	at org.apache.solr.core.SolrCore.getSearcher(SolrCore.java:1066)
	at org.apache.solr.core.SolrCore.<init>(SolrCore.java:581)
	at org.apache.solr.core.CoreContainer.create(CoreContainer.java:431)
	at org.apache.solr.core.CoreContainer.load(CoreContainer.java:286)
	at org.apache.solr.core.CoreContainer$Initializer.initialize(CoreContainer.java:125)
	at org.apache.solr.servlet.SolrDispatchFilter.init(SolrDispatchFilter.java:86)
	at org.apache.catalina.core.ApplicationFilterConfig.getFilter(ApplicationFilterConfig.java:275)
	at org.apache.catalina.core.ApplicationFilterConfig.setFilterDef(ApplicationFilterConfig.java:397)
	at org.apache.catalina.core.ApplicationFilterConfig.<init>(ApplicationFilterConfig.java:108)
	at org.apache.catalina.core.StandardContext.filterStart(StandardContext.java:3800)
	at org.apache.catalina.core.StandardContext.start(StandardContext.java:4450)
	at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:791)
	at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:771)
	at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:526)
	at org.apache.catalina.startup.HostConfig.deployWAR(HostConfig.java:850)
	at org.apache.catalina.startup.HostConfig.deployWARs(HostConfig.java:724)
	at org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:493)
	at org.apache.catalina.startup.HostConfig.start(HostConfig.java:1206)
	at org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:314)
	at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:119)
	at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1053)
	at org.apache.catalina.core.StandardHost.start(StandardHost.java:722)
	at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1045)
	at org.apache.catalina.core.StandardEngine.start(StandardEngine.java:443)
	at org.apache.catalina.core.StandardService.start(StandardService.java:516)
	at org.apache.catalina.core.StandardServer.start(StandardServer.java:710)
	at org.apache.catalina.startup.Catalina.start(Catalina.java:583)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:288)
	at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:413)
Caused by: java.io.IOException: read past EOF
	at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:154)
	at org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:39)
	at org.apache.lucene.store.ChecksumIndexInput.readByte(ChecksumIndexInput.java:40)
	at org.apache.lucene.store.DataInput.readInt(DataInput.java:76)
	at org.apache.lucene.store.DataInput.readLong(DataInput.java:99)
	at org.apache.lucene.index.SegmentInfo.<init>(SegmentInfo.java:165)
	at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:230)
	at org.apache.lucene.index.DirectoryReader$1.doBody(DirectoryReader.java:91)
	at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:649)
	at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:87)
	at org.apache.lucene.index.IndexReader.open(IndexReader.java:415)
	at org.apache.lucene.index.IndexReader.open(IndexReader.java:294)
	at org.apache.solr.core.StandardIndexReaderFactory.newReader(StandardIndexReaderFactory.java:38)
	at org.apache.solr.core.SolrCore.getSearcher(SolrCore.java:1055)
	... 32 more
{noformat} "
1,"intermittent failure in TestIndexWriter. testExceptionDuringSync {code}
common.test:

    [mkdir] Created dir: C:\Projects\lucene\trunk-full1\build\test

    [junit] Testsuite: org.apache.lucene.index.TestIndexWriter

    [junit] Tests run: 102, Failures: 0, Errors: 1, Time elapsed: 100,297sec

    [junit]

    [junit] Testcase: testExceptionDuringSync(org.apache.lucene.index.TestIndexWriter): Caused an ERROR

    [junit] _a.fnm

    [junit] java.io.FileNotFoundException: _a.fnm

    [junit]     at org.apache.lucene.store.MockRAMDirectory.openInput(MockRAMDirectory.java:226)

    [junit]     at org.apache.lucene.index.FieldInfos.<init>(FieldInfos.java:68)

    [junit]     at org.apache.lucene.index.SegmentReader$CoreReaders.<init>(SegmentReader.java:116)

    [junit]     at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:620)

    [junit]     at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:590)

    [junit]     at org.apache.lucene.index.DirectoryReader.<init>(DirectoryReader.java:104)

    [junit]     at org.apache.lucene.index.ReadOnlyDirectoryReader.<init>(ReadOnlyDirectoryReader.java:27)

    [junit]     at org.apache.lucene.index.DirectoryReader$1.doBody(DirectoryReader.java:74)

    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:704)

    [junit]     at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:69)

    [junit]     at org.apache.lucene.index.IndexReader.open(IndexReader.java:307)

    [junit]     at org.apache.lucene.index.IndexReader.open(IndexReader.java:193)

    [junit]     at org.apache.lucene.index.TestIndexWriter.testExceptionDuringSync(TestIndexWriter.java:2723)

    [junit]     at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:206)

    [junit]

    [junit]

    [junit] Test org.apache.lucene.index.TestIndexWriter FAILED
{code}"
1,"UserManagement: IndexNodeResolver.findNodes(..... , nonExact) fails to find values containing backslash"
1,"DatabaseJournal.checkSchema generates ""Cannot call commit when autocommit=true"" ExceptionWhen I tried to activate clustering with adding the following XML to repository.xml, 

    <Cluster id=""node_1"" syncDelay=""5"">
		<Journal class=""org.apache.jackrabbit.core.journal.DatabaseJournal"">
			<param name=""revision"" value=""${rep.home}/revision""/>
			<param name=""driver"" value=""com.mysql.jdbc.Driver""/>
			<param name=""url"" value=""jdbc:mysql://localhost/jcr""/>
			<param name=""user"" value=""userX""/>
			<param name=""password"" value=""passWordC""/>
			<param name=""schema"" value=""mysql""/>
			<param name=""schemaObjectPrefix"" value=""J_C_""/>
		</Journal>
    </Cluster>

Databse Journal threw the following exception:

....
Caused by: javax.jcr.RepositoryException: Unable to initialize connection.: Unable to initialize connection.
        at org.apache.jackrabbit.core.RepositoryImpl.createClusterNode(RepositoryImpl.java:677)
        at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:276)
        at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:584)
        at org.apache.jackrabbit.core.TransientRepository$2.getRepository(TransientRepository.java:245)
        at org.apache.jackrabbit.core.TransientRepository.startRepository(TransientRepository.java:265)
        at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:333)
        at com.liferay.portal.jcr.jackrabbit.JCRFactoryImpl.createSession(JCRFactoryImpl.java:71)
        at com.liferay.portal.jcr.JCRFactoryUtil.createSession(JCRFactoryUtil.java:53)
        at com.liferay.portal.jcr.JCRFactoryUtil.createSession(JCRFactoryUtil.java:57)
        at com.liferay.documentlibrary.util.IndexerImpl.reIndex(IndexerImpl.java:258)
        ... 17 more
Caused by: org.apache.jackrabbit.core.cluster.ClusterException: Unable to initialize connection.
        at org.apache.jackrabbit.core.cluster.ClusterNode.init(ClusterNode.java:218)
        at org.apache.jackrabbit.core.cluster.ClusterNode.init(ClusterNode.java:189)
        at org.apache.jackrabbit.core.RepositoryImpl.createClusterNode(RepositoryImpl.java:674)
        ... 26 more
Caused by: java.sql.SQLException: Can't call commit when autocommit=true
        at com.mysql.jdbc.Connection.commit(Connection.java:2161)
        at org.apache.jackrabbit.core.journal.DatabaseJournal.checkSchema(DatabaseJournal.java:437)
        at org.apache.jackrabbit.core.journal.DatabaseJournal.init(DatabaseJournal.java:168)
        at org.apache.jackrabbit.core.cluster.ClusterNode.init(ClusterNode.java:213)
        ... 28 more

When I examined the source code of the release jackrabbit 1.3, I saw that the init() method for DatabaseJournal class is:

			...
			Class.forName(driver);
			con = DriverManager.getConnection(url, user, password);
			con.setAutoCommit(true);

			checkSchema();
			prepareStatements();
			...

and just before checkSchema() method's finally block:

			...
			// commit the changes
			con.commit();
			...

So, it seemed normal to see the mentioned exception. I just commented out the commit expression and continued my development. Am I missing something?"
1,"'100-continue' response times out unexpectedlyEntity encosing methods time out (3 seconds) rather than getting the
100-continue response. Then, after it has send the body, the 100-continue
response is received and returned.

Adding

  method.setUseExpectHeader(false);

seems to fix it.

Platform 1: Jetty server on Windows XP, Sun JDK 1.4.1_01, 
Platform 2: Tomcat-4.1.18 + Turbine on Windows 2000 Pro, Sun JDK 1.3.1
Platform 3: Tomcat-4.1.18 on Linux if the connection is running over stunnel-4.00

Reported by: 
 Simon Roberts <simon.roberts@fifthweb.net>
 Aurelien Pernoud <apernoud@sopragroup.com>
 Ingo Brunberg <ib@fiz-chemie.de>"
1,"Node's childNodes out of sync after unsuccessful save()If node.save() failes due to an exception in PersistanceManager.store(ChangeLog) a successive call to node.getNodes() will still contain a reference to the node which failed to be persisted.
This is the case even after a call to refresh(false).
You have to restart Jackrabbit in order to get rid of this reference


UseCase in kind of dummy-code:

node.addNode(""new"", ""nt:unstructured"");
node.save();
=> ItemStateException from persistance

node.refresh(false);

Iterator itr = node.getNodes();
while(itr.hasNext()) {
   child = itr.nextNode();
}
=> Exception: ""Failed to build path to ""new""

"
1,"Jackrabbit fails to shutdown properly when tomcat is shutting downThis is the same issue already discudded in http://issues.apache.org/jira/browse/JCR-57

The problem only occurs when Jackrabbit is deployed in the WEB-INF/lib directory of a web application in Tomcat.
During dispose() jackrabbit tries to instantiate a few objects from classes which were not previously loaded by the webapp classloader, but tomcat doesn't allow to load new classes while shutting down.
This causes the repository not to be closed properly, and an annoying set of stack traces are written to the log.

It seems that there are only two classes which are loaded in this situation: org.apache.jackrabbit.core.observation.EventListenerIteratorImpl and org.apache.jackrabbit.core.fs.FileSystemPathUtil. This is the log from the server standard output:

org.apache.catalina.loader.WebappClassLoader loadClass
INFO: Illegal access: this web application instance has been stopped already.  Could not load org.apache.jackrabbit.core.observation.EventListenerIteratorImpl.  The eventual following stack trace is caused by an error thrown for debugging purposes as well as to attempt to terminate the thread which caused the illegal access, and has no functional impact.
[repeaded more times at each shutdown]

org.apache.catalina.loader.WebappClassLoader loadClass
INFO: Illegal access: this web application instance has been stopped already.  Could not load org.apache.jackrabbit.core.fs.FileSystemPathUtil.  The eventual following stack trace is caused by an error thrown for debugging purposes as well as to attempt to terminate the thread which caused the illegal access, and has no functional impact.


A quick fix is to force preloading of classes normally needed only during shutdown, simply adding a static block to caller classes. The following patch makes tomcat happy, causing classes to be loaded by the webapp classloaded when still allowed  (probably not really elegant, but perfectly working...)




Index: org/apache/jackrabbit/core/fs/FileSystemResource.java
===================================================================
--- src\java\org\apache\jackrabbit\core\fs\FileSystemResource.java	(revision 169503)
+++ src\java\org\apache\jackrabbit\core\fs\FileSystemResource.java	(working copy)
@@ -30,6 +30,11 @@
 
     protected final String path;
 
+    static {
+        // preload FileSystemPathUtil to prevent classloader issues during shutdown
+        FileSystemPathUtil.class.hashCode();
+    }
+
     /**
      * Creates a new <code>FileSystemResource</code>
      *
Index: org/apache/jackrabbit/core/observation/ObservationManagerImpl.java
===================================================================
--- src\java\org\apache\jackrabbit\core\observation\ObservationManagerImpl.java	(revision 169503)
+++ src\java\org\apache\jackrabbit\core\observation\ObservationManagerImpl.java	(working copy)
@@ -54,6 +54,11 @@
      */
     private final ObservationManagerFactory obsMgrFactory;
 
+    static {
+        // preload EventListenerIteratorImpl to prevent classloader issues during shutdown
+        EventListenerIteratorImpl.class.hashCode();
+    }
+
     /**
      * Creates an <code>ObservationManager</code> instance.
      *


"
1,"JNDI data sources with various PersistenceManager: wrong default valuesWith JCR-1305 Jackrabbit supports creating a connection throug a JNDI Datasource and without configuring user and password. This works for some but not all provided PersistenceManagers. Some of them - like the Oracle-specific BundleDBPersistenceManager - sets default values for user and password if none are provided in the jackrabbit config. This way its impossible to use such PersistenceManagers with the plain JNDI DS.

This concerns the following BundleDbPersistenceManagers: OraclePersistenceManager, DerbyPersistenceManager, H2PersistenceManager.

There also might be other PMs (perhaps some special SimpleDbPersistenceManagers) with similar behaviour."
1,New BufferedIndexOutput optimization fails to update bufferStartNew BufferIndexOutput optimization of writeBytes fails to update bufferStart under some conditions. Test case and fix attached.
1,"search.jsp doesn't handle utf-8 parameters correctly
1.  I  cannot use WebDav client to uploaded a file whose name is in Chinese.  The file name I had is ''.txt'  and the uploaded command by the WebDav client did something like:

  ========= Outbound Message =========
PUT /op/%ED%EF%3A.txt HTTP/1.1
Host: localhost:8080
-----

 The server didn't decode it correctly -- the result is the file name got screwed and the file content was not uploaded.

2. In the default web.war module,  there is search.jsp for rendering the search page. If I type Chinese text in the search box,  search.jsp does not decode the input parameter from ISO-8859-1 to utf-8 and in turn the search engine searches wrong string.

3. The search engine does do search correctly if I hardcode the query  variable in search.jsp or do decoding the query parameter from ISO-885901 to utf-8.

"
1,"SSL verification occurs before setSoTimeout, which can lead to hangspartial thread dump:

       at java.net.SocketInputStream.socketRead0(Native Method)
       at java.net.SocketInputStream.read(SocketInputStream.java:129)
       at com.sun.net.ssl.internal.ssl.InputRecord.readFully(InputRecord.java:293)
       at com.sun.net.ssl.internal.ssl.InputRecord.read(InputRecord.java:331)
       at com.sun.net.ssl.internal.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:723)
       - locked <0x00002aaab87d9de0> (a java.lang.Object)
       at com.sun.net.ssl.internal.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1030)
       - locked <0x00002aaab87d9dc0> (a java.lang.Object)
       at com.sun.net.ssl.internal.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1057)
       at com.sun.net.ssl.internal.ssl.SSLSocketImpl.getSession(SSLSocketImpl.java:1757)
       at org.apache.http.conn.ssl.AbstractVerifier.verify(AbstractVerifier.java:87)
       at org.apache.http.conn.ssl.SSLSocketFactory.connectSocket(SSLSocketFactory.java:295)
       at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:131)
       at org.apache.http.impl.conn.AbstractPoolEntry.open(AbstractPoolEntry.java:143)
       at org.apache.http.impl.conn.AbstractPooledConnAdapter.open(AbstractPooledConnAdapter.java:120)
       at org.apache.http.impl.client.DefaultClientRequestDirector.execute(DefaultClientRequestDirector.java:286)
       at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:452)
       at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:406)
       at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:365)


... this is because in DefaultClientConnectionOperator, prepareSocket (which sets any configured timeouts) isn't called until after SocketFactory.connectSocket. When using SSLSocketFactory, the default behavior is to verify the hostname, which opens a connection, and can block indefinitely.

Simple workaround is to use the AllowAllHostnameVerifier which doesn't do any verification."
1,UserManager.getAuthorizable() may fail with InvalidQueryExceptionHappens when the principal name contains an apostrophe.
1,"Read permission on parent node required to access an item's definitionIf a session is granted all permissions on a given item B but lacks permission to read it's parent node A an attempt to
access the definition of B by means of Node.getDefinition or Property.getDefinition will fail with AccessDeniedException.

Similarly, the same session will not be able to modify that item B - e.g. add a child node in case it was a node - since implementation e.g. checks of that
item B isn't protected, which is determined by looking at the definition.

My feeling is, that the item definition should be accessible even if the parent node cannot be read."
1,"Importer drops jcr:xmlcharacters fields after a large jcr:xmlcharacters entryIf you have an XML node as follows:

<a>
[lots and lots of data]
</a>

This should translate into

Node: a
 +- Node: jcr:xmltext
   +- Property: jcr:xmlcharacters = [lots and lots of data]

Instead, the following things happen:
- There is no node jcr:xmltext
- If the node a has child nodes, they also lose the jcr:xmltext node
- Any nodes on the same level after node a also lose the jcr:xmltext node
- Nodes that come after a, but are on a higher level, have correct jcr:xmltext 
nodes

(I've used some 100+k of data, namely a base64 encoded picture)"
1,"UUID compareTo and hashCodeThe current UUID.compareTo implementation is not correct. Usually, 'equals' is used so this is not a big problem, but I need to create an ordered list of UUIDs and for this I need compareTo. The current implementation is based on subtraction, but this doesn't always work. Example:

//long a = 10, b = 20, c = 0;
long a = Long.MAX_VALUE, b = Long.MIN_VALUE, c = 0;
System.out.println((a - b) < 0 ? ""a < b"" : ""a >= b"");
System.out.println((c - a) < 0 ? ""c < a"" : ""c >= a"");
System.out.println((b - c) < 0 ? ""b < c"" : ""b >= c"");

The hashCode implementation is OK, but the multiplication is not required."
1,"jcr2spi spi2dav getProperties returns only cached propertiesI'm using JCR through webdav (contrib jcr2spi and spi2dav libraries).
Server is default jcr server (jackrabbit-webapp-1.3.1 on tomcat),
client is 2007/09/28 svn snapshot

I've noticed that Node.getProperty returns only cached properties. 

Sample test case can be found at http://kplab.tuke.sk/svn/kplab/ctm/trunk/test/org/kplab/tsf/CTMTest.java

Note that sometimes properties do get printed, so it may not be so straightforwarding to reproduce this bug.


Simple Example:

// I have nt:file node at kplab/jojo in my repository
...
Session session = repository.login();
Node root  = session.getRootNode();
Node node = root.getNode(""kplab/jojo"");
// node.getProperty(""jcr:content/jcr:data""); // force to load property
from server
dump(node); // simple dump method from (
http://jackrabbit.apache.org/doc/firststeps.html )


dump prints:
/kplab/jojo
/kplab/jojo/jcr:content

But when I uncomment the getProperty line above, it prints:
/kplab/jojo
/kplab/jojo/jcr:content
/kplab/jojo/jcr:content/jcr:lastModified = 2007-09-27T15:52:27.312+02:00
/kplab/jojo/jcr:content/jcr:uuid = 4421ed5a-6200-4918-864e-c58643bc8d4e
/kplab/jojo/jcr:content/jcr:mimeType = text/plain
/kplab/jojo/jcr:content/jcr:data = hura hura
/kplab/jojo/jcr:content/jcr:primaryType = nt:resource

--
Jozef Wagner"
1,"TestIndexWriterExceptions random failure: AIOOBE in ByteBlockPool.allocSliceTestIndexWriterExceptions threw this today, and its reproducable"
1,"ItemStateMap warnings during node type changesAs reported already in JCR-1105, the ItemStateMap logs warnings when a cached item state is being overwritten. This shouldn't normally happen, but it turns out that virtual item state providers do this when the root of the virtual tree is modified. Probably the most common such situation is when node types are being modified. This case is luckily not troublesome for the virtual tree functionality, but the logged warnings are annoying.

Here's a relevant part of a stack trace where this problem occurs:

        at org.apache.jackrabbit.core.state.ItemStateMap.put(ItemStateMap.java:72)
        at org.apache.jackrabbit.core.virtual.AbstractVISProvider.cache(AbstractVISProvider.java:324)
        at org.apache.jackrabbit.core.virtual.AbstractVISProvider.createNodeState(AbstractVISProvider.java:284)
        at org.apache.jackrabbit.core.nodetype.virtual.VirtualNodeTypeStateProvider.createNodeTypeState(VirtualNodeTypeStateProvider.java:157)
        at org.apache.jackrabbit.core.nodetype.virtual.VirtualNodeTypeStateProvider.createRootNodeState(VirtualNodeTypeStateProvider.java:80)
        at org.apache.jackrabbit.core.virtual.AbstractVISProvider.stateDiscarded(AbstractVISProvider.java:470)
        at org.apache.jackrabbit.core.state.ItemState.notifyStateDiscarded(ItemState.java:226)
        at org.apache.jackrabbit.core.state.ItemState.discard(ItemState.java:370)
        at org.apache.jackrabbit.core.nodetype.virtual.VirtualNodeTypeStateProvider.onNodeTypesRemoved(VirtualNodeTypeStateProvider.java:139)
        at org.apache.jackrabbit.core.nodetype.virtual.VirtualNodeTypeStateManager.nodeTypesUnregistered(VirtualNodeTypeStateManager.java:199)
        at org.apache.jackrabbit.core.nodetype.virtual.VirtualNodeTypeStateManager.nodeTypeReRegistered(VirtualNodeTypeStateManager.java:174)
        at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.notifyReRegistered(NodeTypeRegistry.java:1821)
        at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.reregisterNodeType(NodeTypeRegistry.java:433)
        at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.reregisterNodeType(NodeTypeRegistry.java:364)
        at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.registerNodeTypes(NodeTypeManagerImpl.java:591)
        at org.apache.jackrabbit.commons.cnd.CndImporter.registerNodeTypes(CndImporter.java:118)"
1,"Exception shouldn't be thrown for unsupported authentication methodCurrently, Authenticator will throw an UnsupportedOperationException for 
unsupported authentication method (like NTLM). This is correct. However,  
HttpMethodBase.execute only catches HttpException, so this 
UnsupportedOperationException is leaked to the user. This is undesirable, 
because user may want a chance to handle such such authentication themself. The 
correct way is to pass the http status code to the user, just like how it 
treats redirect to a different host.

The simple fix is to catch all exceptions in HttpMethodBase.execute when 
calling Authenticator.authenticate."
1,"ArrayIndexOutOfBoundsException: ConcurrentCacheArrayIndexOutOfBoundsException after several days of uptime.

I'm experiencing some strange ArrayIndexOutOfBoundsExceptions on
 accessing the jackrabbit ConcurrentCache in 2.2.5. in Line 241 during
 shrinkIfNeeded check.

 Caused by: java.lang.ArrayIndexOutOfBoundsException: -14
        at
 org.apache.jackrabbit.core.cache.ConcurrentCache.shrinkIfNeeded(ConcurrentCache.java:241)


I reviewed jackrabbit-code and I'm sure it's caused by that
 AtomicInteger for realizing accessCounter in AbstractCache, which will
 have become negative during increasing over the Integer.MAX_VALUE constant.

         // Semi-random start index to prevent bias against the first
 segments
         int start = (int) getAccessCount() % segments.length;
         for (int i = start; isTooBig(); i = (i + 1) % segments.length) {
             synchronized (segments[i]) {

 ___________________________

 Uncaught Throwable java.lang.ArrayIndexOutOfBoundsException: -7
         at
 org.apache.jackrabbit.core.cache.ConcurrentCache.shrinkIfNeeded(ConcurrentCache.java:241)
         at
 org.apache.jackrabbit.core.cache.ConcurrentCache.put(ConcurrentCache.java:176)
         at
 org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.getBundle(AbstractBundlePersistenceManager.java:657)
         at
 org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.load(AbstractBundlePersistenceManager.java:400)
         at
 org.apache.jackrabbit.core.state.SharedItemStateManager.loadItemState(SharedItemStateManager.java:1819)
         at
 org.apache.jackrabbit.core.state.SharedItemStateManager.getNonVirtualItemState(SharedItemStateManager.java:1739)
         at
 org.apache.jackrabbit.core.state.SharedItemStateManager.getItemState(SharedItemStateManager.java:261)
         at
 org.apache.jackrabbit.core.state.LocalItemStateManager.getNodeState(LocalItemStateManager.java:107)
         at
 org.apache.jackrabbit.core.state.LocalItemStateManager.getItemState(LocalItemStateManager.java:172)
         at
 org.apache.jackrabbit.core.state.XAItemStateManager.getItemState(XAItemStateManager.java:260)
         at
 org.apache.jackrabbit.core.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:161)
         at
 org.apache.jackrabbit.core.ItemManager.getItemData(ItemManager.java:370)
         at
 org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:316)
         at
 org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:610)
         at
 org.apache.jackrabbit.core.SessionImpl.getNodeById(SessionImpl.java:493)
         at
 org.apache.jackrabbit.core.SessionImpl.getNodeByIdentifier(SessionImpl.java:1045)
         at sun.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)
         at
 sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
         at java.lang.reflect.Method.invoke(Method.java:597)
         at
 org.apache.sling.jcr.base.SessionProxyHandler$SessionProxyInvocationHandler.invoke(SessionProxyHandler.java:109)
         at $Proxy2.getNodeByIdentifier(Unknown Source)
         at
 de.dig.cms.frontend.servlet.helper.ResourceUtil.findResourceById(ResourceUtil.java:44)
         at
 de.dig.cms.frontend.servlet.CMSContentEnrichServletFilter.doFilter(CMSContentEnrichServletFilter.java:194)
         at
 org.apache.sling.engine.impl.filter.AbstractSlingFilterChain.doFilter(AbstractSlingFilterChain.java:60)
         at
 de.dig.cms.frontend.servlet.CacheControlFilter.doFilter(CacheControlFilter.java:120)
         at
 org.apache.sling.engine.impl.filter.AbstractSlingFilterChain.doFilter(AbstractSlingFilterChain.java:60)
         at
 de.dig.cms.cache.impl.WallCacheServletFilter.processCacheableRequest(WallCacheServletFilter.java:244)
         at
 de.dig.cms.cache.impl.WallCacheServletFilter.processCacheableRequestWithLatch(WallCacheServletFilter.java:185)
         at
 de.dig.cms.cache.impl.WallCacheServletFilter.doFilter(WallCacheServletFilter.java:154)
         at
 org.apache.sling.engine.impl.filter.AbstractSlingFilterChain.doFilter(AbstractSlingFilterChain.java:60)
         at
 de.dig.cms.frontend.servletapi.CMSSlingHttpServletRequestFilter.doFilter(CMSSlingHttpServletRequestFilter.java:52)
         at
 org.apache.sling.engine.impl.filter.AbstractSlingFilterChain.doFilter(AbstractSlingFilterChain.java:60)
         at
 org.apache.sling.engine.impl.SlingMainServlet.service(SlingMainServlet.java:313)
         at
 org.apache.sling.engine.impl.SlingMainServlet.service(SlingMainServlet.java:207)
         at
 org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:502)
         at
 org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:389)
         at
 org.ops4j.pax.web.service.internal.HttpServiceServletHandler.handle(HttpServiceServletHandler.java:64)
         at
 org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)
         at
 org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)
         at
 org.ops4j.pax.web.service.internal.HttpServiceContext.handle(HttpServiceContext.java:111)
         at
 org.ops4j.pax.web.service.internal.JettyServerHandlerCollection.handle(JettyServerHandlerCollection.java:64)
         at
 org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
         at org.mortbay.jetty.Server.handle(Server.java:324)
         at
 org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:535)
         at
 org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:865)
         at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:539)
         at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
         at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
         at
 org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:409)
         at
 org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:520)"
1,"DBFileSystem MySQL DDL not compatible with pre-5.0 versionsThe packaged ddl for mysql index sizes is too large for 4.x versions of MySQL. As the sum-total of the index sizes may only reach 500.

So, 

create unique index ${schemaObjectPrefix}FSENTRY_IDX on ${schemaObjectPrefix}FSENTRY (FSENTRY_PATH(745), FSENTRY_NAME)

will not work. I would suggest shortening the FSENTRY_PATH index value to 245, as FSENTRY_NAME is already set to 255.

create unique index ${schemaObjectPrefix}FSENTRY_IDX on ${schemaObjectPrefix}FSENTRY (FSENTRY_PATH(245), FSENTRY_NAME)"
1,"adding docs with large (binary) fields of 5mb causes OOM regardless of heap sizeas reported by George Washington in a message to java-user@lucene.apache.org with subect ""Storing large text or binary source documents in the index and memory usage"" arround 2006-01-21 there seems to be a problem with adding docs containing really large fields.

I'll attach a test case in a moment, note that (for me) regardless of how big i make my heap size, and regardless of what value I set  MIN_MB to, once it starts trying to make documents of containing 5mb of data, it can only add 9 before it rolls over and dies.

here's the output from the code as i will attach in a moment...

    [junit] Testsuite: org.apache.lucene.document.TestBigBinary
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 78.656 sec

    [junit] ------------- Standard Output ---------------
    [junit] NOTE: directory will not be cleaned up automatically...
    [junit] Dir: /tmp/org.apache.lucene.document.TestBigBinary.97856146.100iters.4mb
    [junit] iters completed: 100
    [junit] totalBytes Allocated: 419430400
    [junit] NOTE: directory will not be cleaned up automatically...
    [junit] Dir: /tmp/org.apache.lucene.document.TestBigBinary.97856146.100iters.5mb
    [junit] iters completed: 9
    [junit] totalBytes Allocated: 52428800
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testBigBinaryFields(org.apache.lucene.document.TestBigBinary):    Caused an ERROR
    [junit] Java heap space
    [junit] java.lang.OutOfMemoryError: Java heap space


    [junit] Test org.apache.lucene.document.TestBigBinary FAILED
"
1,"ConcurrentModificationException in WebDAV UPDATEAfter fixing JCR-2750, I started seeing the following exception in the jcr2dav integration tests:

java.util.ConcurrentModificationException: null
	at java.util.LinkedHashMap$LinkedHashIterator.nextEntry(LinkedHashMap.java:365) ~[na:1.5.0_22]
	at java.util.LinkedHashMap$ValueIterator.next(LinkedHashMap.java:380) ~[na:1.5.0_22]
	at java.util.AbstractCollection.toArray(AbstractCollection.java:176) ~[na:1.5.0_22]
	at org.apache.jackrabbit.webdav.MultiStatus.getResponses(MultiStatus.java:122) ~[jackrabbit-webdav-2.2-SNAPSHOT.jar:2.2-SNAPSHOT]
	at org.apache.jackrabbit.webdav.MultiStatus.toXml(MultiStatus.java:151) ~[jackrabbit-webdav-2.2-SNAPSHOT.jar:2.2-SNAPSHOT]
	at org.apache.jackrabbit.webdav.WebdavResponseImpl.sendXmlResponse(WebdavResponseImpl.java:145) ~[jackrabbit-webdav-2.2-SNAPSHOT.jar:2.2-SNAPSHOT]
	at org.apache.jackrabbit.webdav.WebdavResponseImpl.sendMultiStatus(WebdavResponseImpl.java:113) ~[jackrabbit-webdav-2.2-SNAPSHOT.jar:2.2-SNAPSHOT]
	at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.doUpdate(AbstractWebdavServlet.java:1117) ~[jackrabbit-webdav-2.2-SNAPSHOT.jar:2.2-SNAPSHOT]
	at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.execute(AbstractWebdavServlet.java:327) ~[jackrabbit-webdav-2.2-SNAPSHOT.jar:2.2-SNAPSHOT]
	at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.service(AbstractWebdavServlet.java:201) ~[jackrabbit-webdav-2.2-SNAPSHOT.jar:2.2-SNAPSHOT]
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820) ~[servlet-api-2.5-20081211.jar:na]
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511) ~[jetty-6.1.22.jar:6.1.22]
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:390) ~[jetty-6.1.22.jar:6.1.22]
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765) [jetty-6.1.22.jar:6.1.22]
	at org.mortbay.jetty.handler.HandlerCollection.handle(HandlerCollection.java:114) [jetty-6.1.22.jar:6.1.22]
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152) [jetty-6.1.22.jar:6.1.22]
	at org.mortbay.jetty.Server.handle(Server.java:326) [jetty-6.1.22.jar:6.1.22]
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542) [jetty-6.1.22.jar:6.1.22]
	at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:938) [jetty-6.1.22.jar:6.1.22]
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:755) [jetty-6.1.22.jar:6.1.22]
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218) [jetty-6.1.22.jar:6.1.22]
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404) [jetty-6.1.22.jar:6.1.22]
	at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228) [jetty-6.1.22.jar:6.1.22]
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582) [jetty-util-6.1.22.jar:6.1.22]

Instead of something caused by JCR-2750, it looks like a deeper problem that the JCR_2750 fix just uncovered. As far as I can tell, the ConcurrentModificationException is coming from the AbstractResource.EListener class that may end up concurrently modifying the MultiStatus response while it's being serialized."
1,"Jcr-server: Parsing NodeTypeProperty not compliant with definitionCreating a new NodeTypeProperty from an existing DavProperty fails, since assumptions made are not compliant with definition:

a) nodetype name is always enclosed in a 'nodetypename' element
b) nodetype property may be empty, thus contain no 'nodetype' element."
1,"Exception executing SQL2/JQOM with non-admin sessionConstraints are correctly handled when session does not have access to a node:

Caused by: javax.jcr.ItemNotFoundException: 21232f29-7a57-35a7-8389-4a0e4a801fc3
        at org.apache.jackrabbit.core.SessionImpl.getNodeById(SessionImpl.java:545)
        at org.apache.jackrabbit.core.query.lucene.constraint.NodeLocalNameOperand.getValues(NodeLocalNameOperand.java:44)
        at org.apache.jackrabbit.core.query.lucene.constraint.ComparisonConstraint.evaluate(ComparisonConstraint.java:80)"
1,"after enabling access manager, I can't createNode and setProperty without a node.save in the middleI added my own access manager. after that I can't get the following code working 

Node n = createNewNode(parentNode);
n.setProperty();
parentNode.save();

It seems that setProperty will invoke access control check, but since the new node is not in the repository yet, my access manager implementation won't be able to grant permission. I also tried to use hierachyManager to get the path of the new node, it also returned null. 


"
1,"NullPointerException in ClassDescriptorIndex: /Users/cziegeler/Developer/workspaces/default/jackrabbit/contrib/jackrabbit-jcr-mapping/jcr-mapping/src/main/java/org/apache/jackrabbit/ocm/mapper/model/ClassDescriptor.java
===================================================================
--- /Users/cziegeler/Developer/workspaces/default/jackrabbit/contrib/jackrabbit-jcr-mapping/jcr-mapping/src/main/java/org/apache/jackrabbit/ocm/mapper/model/ClassDescriptor.java	(revision 579109)
If a class descriptor (for whatever reason) does not have a jcr type, a npe is thrown in ClassDescriptor.
The following patch solves this issue:

+++ contrib/jackrabbit-jcr-mapping/jcr-mapping/src/main/java/org/apache/jackrabbit/ocm/mapper/model/ClassDescriptor.java	(working copy)
@@ -468,7 +468,7 @@
         while (iterator.hasNext()) {
             ClassDescriptor descendantClassDescriptor = (ClassDescriptor) iterator.next();
   
-            if (descendantClassDescriptor.getJcrType().equals(nodeType)) {
+            if (nodeType.equals(descendantClassDescriptor.getJcrType())) {
                 return descendantClassDescriptor;
             }
   
"
1,"Identifier paths for inexistent items throw exceptionThe following fails with a RepositoryException but it should rather return false:

session.itemExists(""["" + UUID.randomUUID() + ""]"")"
1,"FastVectorHighlighter: small fragCharSize can cause StringIndexOutOfBoundsException If fragCharSize is smaller than Query string, StringIndexOutOfBoundsException is thrown."
1,"InstantiatedTermEnum#skipTo(Term) throws ArrayIndexOutOfBoundsException on empty index{code}
java.lang.ArrayIndexOutOfBoundsException: 0
	at org.apache.lucene.store.instantiated.InstantiatedTermEnum.skipTo(InstantiatedTermEnum.java:105)
	at org.apache.lucene.store.instantiated.TestEmptyIndex.termEnumTest(TestEmptyIndex.java:73)
	at org.apache.lucene.store.instantiated.TestEmptyIndex.testTermEnum(TestEmptyIndex.java:54)
{code}"
1,"SegmentMerger doesn't set payload bit in new optimized codeIn the new optimized code in SegmentMerger the payload bit is not set correctly
in the merged segment. This means that we loose all payloads during a merge!

The Payloads unit test doesn't catch this. Now that we have the new
DocumentsWriter we buffer much more docs by default then before. This means
that the test cases can't assume anymore that the DocsWriter flushes after 10
docs by default. TestPayloads however falsely assumed this, which means that no
merges happen anymore in TestPayloads. We should check whether there are
other testcases that rely on this.

The fixes for TestPayloads and SegmentMerger are very simple, I'll attach a patch
soon."
1,"Cookie.java blowing up on cookies from ""country code"" domainsThe following exception is thrown from Cookie.java when receiving a cookie from
a ""country code"" domain such as amazon.ca.

     [java] INFO: Cookie.parse(): Rejecting set cookie header
""session-id=702-1613649-9326458; path=/; domain=
.amazon.ca; expires=Tuesday, 29-Oct-2002 08:00:00 GMT,
session-id-time=1035878400; path=/; domain=.amazon.ca;
expires=Tuesday, 29-Oct-2002 08:00:00 GMT"" because ""session-id"" has an illegal
domain attribute ("".amazon.ca"")
 for the given domain ""www.amazon.ca"".  It violoates the Netscape cookie
specification for non-special TLDs.
     [java] Oct 22, 2002 9:32:37 AM org.apache.commons.httpclient.HttpMethodBase
processResponseHeaders
     [java] SEVERE: Exception processing response headers
     [java] org.apache.commons.httpclient.HttpException: Bad Set-Cookie header:
session-id=702-1613649-9326458
; path=/; domain=.amazon.ca; expires=Tuesday, 29-Oct-2002 08:00:00 GMT,
session-id-time=1035878400; path=/; do
main=.amazon.ca; expires=Tuesday, 29-Oct-2002 08:00:00 GMT Illegal domain
attribute .amazon.ca
     [java]     at org.apache.commons.httpclient.Cookie.parse(Cookie.java:944)
     [java]     at
org.apache.commons.httpclient.HttpMethodBase.processResponseHeaders(HttpMethodBase.java:141
9)
     [java]     at
org.apache.commons.httpclient.HttpMethodBase.readResponse(HttpMethodBase.java:1504)
     [java]     at
org.apache.commons.httpclient.HttpMethodBase.processRequest(HttpMethodBase.java:2128)
     [java]     at
org.apache.commons.httpclient.HttpMethodBase.execute(HttpMethodBase.java:790)
     [java]     at
org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:442)


The http response that caused this exception is below.

HTTP/1.1 302 Found
Date: Tue, 22 Oct 2002 13:30:11 GMT
Server: Stronghold/2.4.2 Apache/1.3.6 C2NetEU/2412 (Unix)
Set-Cookie: session-id=702-8591055-5561622; path=/; domain=.amazon.ca;
expires=Tuesday, 29-Oct-2002 08:00:00 GMT
Set-Cookie: session-id-time=1035878400; path=/; domain=.amazon.ca;
expires=Tuesday, 29-Oct-2002 08:00:00 GMT
Location: http://www.amazon.ca/exec/obidos/tg/browse/-/915398/702-8591055-5561622
Connection: close
Transfer-Encoding: chunked
Content-Type: text/html

I've seen this problem with other .ca domains so this isn't a problem unique to
amazon.ca.

My guess would be that the problem is on line 929 of Cookie.java:

int domainParts = new StringTokenizer(cookie.getDomain(), ""."").countTokens();

Where domainParts would be 2 for a domain like "".amazon.ca"" instead of the 3
that the code is expecting.  I'm not that familiar with the cookie spec so I
could be completely wrong ;-)

The results above were done with the Oct 20/2002 gump build."
1,"setProperty(""name"", new Value[0], PropertyType.LONG) loses property typeAdding an empty multivalued property with a specific non-STRING type to an unstructured node (i.e. one with an UNDEFINED multivalued property definition) creates an empty multivalued property of type STRING.

In some cases keeping the explicit type information is important, so we should avoid losing it."
1,"Change value of 'Expect' header in org.apache.http.client.methods.HttpPostsee original report at http://code.google.com/p/android/issues/detail?id=7208.

i'm going to apply the obvious patch to Android:

diff --git a/src/org/apache/http/params/CoreProtocolPNames.java b/src/org/apache/http/params/CoreProtocolPNames.java
index a42c5de..a0a726d 100644
--- a/src/org/apache/http/params/CoreProtocolPNames.java
+++ b/src/org/apache/http/params/CoreProtocolPNames.java
@@ -94,8 +94,8 @@ public interface CoreProtocolPNames {
 
     /**
      * <p>
-     * Activates 'Expect: 100-Continue' handshake for the 
-     * entity enclosing methods. The purpose of the 'Expect: 100-Continue'
+     * Activates 'Expect: 100-continue' handshake for the
+     * entity enclosing methods. The purpose of the 'Expect: 100-continue'
      * handshake to allow a client that is sending a request message with 
      * a request body to determine if the origin server is willing to 
      * accept the request (based on the request headers) before the client
diff --git a/src/org/apache/http/protocol/HTTP.java b/src/org/apache/http/protocol/HTTP.java
index de76ca6..9223955 100644
--- a/src/org/apache/http/protocol/HTTP.java
+++ b/src/org/apache/http/protocol/HTTP.java
@@ -60,7 +60,7 @@ public final class HTTP {
     public static final String SERVER_HEADER = ""Server"";
     
     /** HTTP expectations */
-    public static final String EXPECT_CONTINUE = ""100-Continue"";
+    public static final String EXPECT_CONTINUE = ""100-continue"";
 
     /** HTTP connection control */
     public static final String CONN_CLOSE = ""Close"";
"
1,"MatchAllScorer calculateDocFilter() bugIn MatchAllScorer.calculateDocFilter(), When you have just two nodes, with different properties, like ""myprop"" and ""myprop2"", and you have an xpath String xpath = ""//*[@myprop], you get both nodes back (to be precise, you'll get every node that has a property that startswith ""myprop"")


You can reproduce it by changing the SimpleQueryTest.testIsNotNull() a little:

Change 

bar.setProperty(""text"", ""the quick brown fox jumps over the lazy dog.""); 

to

bar.setProperty(""mytextwhichstartswithmytext"", ""the quick brown fox jumps over the lazy dog."");

Now the test with xpath = ""//*[@jcr:primaryType='nt:unstructured' and @mytext]""; fails because 2 results. I did test for the trunk and tag 1.3.1 and both have the same problem. I have attached MatchAllScorer.java.patch in this mail, or should I create a JIRA issue for this? 

Furthermore I would like to discuss a different implementation for the MatchAllScorer, because IMHO the current calculateDocFilter() becomes slow pretty fast (see bottom email the code part i am referring to: if you have 100.000 docs with ""mytext"" property, and you query  [@mytext] the loop below is executed at least 100.000 times). I think it might be out of scope for the user-list, or is the user-list the place to discuss something like this? 

-----------------------------------------------------------------------

TermEnum terms = reader.terms(new Term(FieldNames.PROPERTIES, field));
        try {
            TermDocs docs = reader.termDocs();
            try {
                while (terms.term() != null
                        && terms.term().field() == FieldNames.PROPERTIES
                        && terms.term().text().startsWith(field)) {
                    docs.seek(terms);
                    while (docs.next()) {
                        docFilter.set(docs.doc());
                    }
                    terms.next();
                }
            } finally {
                docs.close();
            }
        } finally {
            terms.close();
        }

-----------------------------------------------------------------------

"
1,"JCR-Server: IllegalArgumentException when retrieving DateHeaderissue reported by martin perez:

Here is one exception. If I access to any repository through WebDAV using a
web browser (http://localhost:8080/webapp/repository/default the first time
goes well, but if I refresh the page then I get the following exception:

GRAVE: Servlet.service() para servlet Webdav lanz excepcin
java.lang.IllegalArgumentException: mar, 29 nov 2005 22:45:48 CET
    at org.apache.catalina.connector.Request.getDateHeader(Request.java
:1791)
    at org.apache.catalina.connector.RequestFacade.getDateHeader(
RequestFacade.java:630)
    at org.apache.jackrabbit.webdav.WebdavRequestImpl.getDateHeader(
WebdavRequestImpl.java:724)
    at org.apache.jackrabbit.server.AbstractWebdavServlet.spoolResource(
AbstractWebdavServlet.java:363)
    at org.apache.jackrabbit.server.AbstractWebdavServlet.doGet(
AbstractWebdavServlet.java:344)
    at org.apache.jackrabbit.j2ee.SimpleWebdavServlet.execute(
SimpleWebdavServlet.java:191)
    at org.apache.jackrabbit.server.AbstractWebdavServlet.service(
AbstractWebdavServlet.java:170)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(
ApplicationFilterChain.java:252)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(
ApplicationFilterChain.java:173)
    at org.apache.catalina.core.StandardWrapperValve.invoke(
StandardWrapperValve.java:213)
    at org.apache.catalina.core.StandardContextValve.invoke(
StandardContextValve.java:178)
    at org.apache.catalina.core.StandardHostValve.invoke(
StandardHostValve.java:126)
    at org.apache.catalina.valves.ErrorReportValve.invoke(
ErrorReportValve.java:105)
    at org.apache.catalina.core.StandardEngineValve.invoke(
StandardEngineValve.java:107)
    at org.apache.catalina.connector.CoyoteAdapter.service(
CoyoteAdapter.java:148)
    at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java
:868)
    at
org.apache.coyote.http11.Http11BaseProtocol$Http11ConnectionHandler.processConnection
(Http11BaseProtocol.java:663)
    at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(
PoolTcpEndpoint.java:527)
    at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt(
LeaderFollowerWorkerThread.java:80)
    at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(
ThreadPool.java:684)
    at java.lang.Thread.run(Thread.java:595)"
1,[PATCH] Fix possible Null Ptr exception in ConnectionFactorycode will throw npe if driver string is null - patch fixes this.
1,"Decompounders based on CompoundWordTokenFilterBase cannot be used with custom attributesThe CompoundWordTokenFilterBase.setToken method will call clearAttributes() and then will reset only the default Token attributes (term, position, flags, etc) resulting in any custom attributes losing their value. Commenting out clearAttributes() seems to do the trick, but will fail the TestCompoundWordTokenFilter tests.."
1,"QueryImpl result offSet must be considered after security class grant the item.ackrabbit version is 1.4 (jackrabbit-core - 1.4.5).
I use searches with result limit and offset but it is working some wrong for my case.
Lets suppose the total of nodes that will return with the search:

NAME      - GRANTACCESS -   OFFSET
node1     -     true                    - 0
node2     -     false                  -  1
node3     -     true                    - 2
node4     -     true                    - 3
node5     -     false                  -  4

My page must have 2 records, so first I do a count for the search and get size of 3 records (after filtered by my security class invoked automatically by jackrabbit), so I have 2 pages to show to the user. The first page must return 2 records, of course, and the second must return 1 record.

In the first search I do set:
QueryImp.setOffset(0);
QueryImpl.setLimit(2);

So, I get the nodes 1 and 3, thats correct.

In the second same search (for second page), I do set:
QueryImp.setOffset(2);
QueryImpl.setLimit(2);

This way I pretend to get two records, starting from the record nro 3, which would be only the node4.
But, the result I got is node3 (again) and node4, because the offset worked not according to the grantacess (provided by the security class), but according to the sequence of the raw result.

This offset have to start in the correct position, counting only the granted nodes returned by the security class.
Hope this make sense for you.

Thanks.
Helio."
1,"ClassCastException when registering custom node by XML fileWhen trying to register node type from XML file using following code:

		JackrabbitNodeTypeManager nodeTypeManager = (JackrabbitNodeTypeManager)workspace.getNodeTypeManager();
		for(Resource resource : nodeDefinitions){
			System.out.println(""** registering node:""+resource);
			nodeTypeManager.registerNodeTypes(resource.getInputStream(), JackrabbitNodeTypeManager.TEXT_XML);
		}

we receive such surprise:

Caused by: java.lang.ClassCastException: com.sun.org.apache.xerces.internal.dom.DeferredDocumentImpl
	at org.apache.jackrabbit.core.util.DOMWalker.iterateElements(DOMWalker.java:215)
	at org.apache.jackrabbit.core.nodetype.xml.NodeTypeReader.getNodeTypeDefs(NodeTypeReader.java:121)
	at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.registerNodeTypes(NodeTypeManagerImpl.java:257)
	at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.registerNodeTypes(NodeTypeManagerImpl.java:499)
	at pl.codeservice.jcr.JcrCustomNodeRegister.registerNodes(JcrCustomNodeRegister.java:41)
	at pl.codeservice.jcr.JcrCustomNodeRegister.init(JcrCustomNodeRegister.java:27)
	...


Registering nodes by .cnd files works fine."
1,"Infinite loop on NTLM authenticationI got an infinite loop on NTLM authentication if the authentication failed (bad credentials).

The state FAILED of the NTLM sheme is never catched in the method authenticate of the class HttpAuthenticator (line 123).
I fix temporatily this bug by adding a case for the protocol state HANDSHAKE."
1,"make spell checker test case work againSee attached path which makes the spellchecker test case work again. The problem without the patch is that consecutive calls to indexDictionary() will create a spelling index with duplicate words. Does anybody see a problem with this patch? I see that the spellchecker code is now used in Solr, isn't it? I didn't have time to test this patch inside Solr.

Also see http://issues.apache.org/jira/browse/LUCENE-632, but the null check is included in this patch so the NPE described there cannot happen anymore.
"
1,"AbstractHttpClient.determineTarget does not recognize target host correctlyI am trying to execute an HttpGet with the following URI:
""http://www.foo.foo/doSomething.html?url=http://www.bar.bar/doSomethingElse.html""

This leads to UnknownHostException

Going through the internal code, the problem seems to be in the AbstractHttpClient.determineTarget method:
            String ssp = requestURI.getSchemeSpecificPart();
            ssp = ssp.substring(2, ssp.length()); //remove ""//"" prefix
            int end = ssp.indexOf(':') > 0 ? ssp.indexOf(':') :
                    ssp.indexOf('/') > 0 ? ssp.indexOf('/') :
                    ssp.indexOf('?') > 0 ? ssp.indexOf('?') : ssp.length();
            String host = ssp.substring(0, end);

This code sets the target host to ""www.foo.foo/doSomething.html?url=http"" instead of ""www.foo.foo"". This obviously breaks the execution not far down the line... DefaultClientConnectionOperator.resolveHostname throws an UnknownHostException.

FWIW the AbstractHttpClient.determineTarget method actually has access to the request URI object, which correctly states that the host is ""www.foo.foo"".

So why does it try to extract the host from the scheme specific part anyway?

I hope this is useful... and if there is any workaround please let me know, as I'm stuck on this one.

Marco"
1,"Error on query initialization - intermittentAbout 1 in ten times, I get the error as shown in the stack trace below. This happens when I run test, or when I start the app. The only way to resolve (when testing) seems to be to blow away the repository. 

It always happens at the point the query manager is accessed (triggering the query subsystem to start up). It DOES NOT cause an exception to be thrown back to the caller, I just noticed it in the logs. Basically the queries return NO data at all (and show up as test failures of course). 

In each case when I startup the system/test, if the repository exists I use it, and (for tests) clean it by deleting the root node of the user content, and then starting again, otherwise there is nothing that exciting.

Please let me know if more info is needed.


ERROR 05-03 15:54:39,386 (LazyQueryResultImpl.java:getResults:266)  -Exception while executing query:
java.io.IOException : No such file or directory
    at java.io.UnixFileSystem.createFileExclusively(Native Method)
    at java.io.File.createNewFile(File.java:850)
    at org.apache.jackrabbit.core.query.lucene.FSDirectory$1.obtain( FSDirectory.java:119)
    at org.apache.lucene.store.Lock.obtain(Lock.java:51)
    at org.apache.lucene.store.Lock$With.run(Lock.java:98)
    at org.apache.lucene.index.IndexReader.open(IndexReader.java:141)
    at org.apache.lucene.index.IndexReader.open(IndexReader.java:136)
    at org.apache.jackrabbit.core.query.lucene.AbstractIndex.getReadOnlyIndexReader(AbstractIndex.java:191)
    at org.apache.jackrabbit.core.query.lucene.MultiIndex.getIndexReader (MultiIndex.java:616)
    at org.apache.jackrabbit.core.query.lucene.SearchIndex.executeQuery(SearchIndex.java:384)
    at org.apache.jackrabbit.core.query.lucene.LazyQueryResultImpl.executeQuery(LazyQueryResultImpl.java :204)
    at org.apache.jackrabbit.core.query.lucene.LazyQueryResultImpl.getResults(LazyQueryResultImpl.java:244)
    at org.apache.jackrabbit.core.query.lucene.LazyQueryResultImpl.<init>(LazyQueryResultImpl.java :161)
    at org.apache.jackrabbit.core.query.lucene.QueryImpl.execute(QueryImpl.java:164)
    at org.apache.jackrabbit.core.query.QueryImpl.execute(QueryImpl.java:142)"
1,"Possible rare thread hazard in IW.commitI was seeing a very rare intermittent failure in TestIndexWriter.testCommitThreadSafety.

The issue happens if one thread calls commit while another is flushing, and is exacerbated at high flush rates (eg maxBufferedDocs=2).  The thread doing commit will first flush, and then it syncs the files.  However in between those two, if other threads manage to add enough docs and trigger another flush, a 2nd new segment can sneak into the SegmentInfos before we sync.

This is normally harmless, in that it just means the commit includes a few more docs that had been added by other threads, so it's fine. But, it can mean that a committed segment references the still-open doc store files.  Our tests now catch this (I changed MockDirWrapper to throw an exception in this case), and so testCommitThreadSafety can fail with this exception.  If you hardwire the maxBufferedDocs to 2 it happens quite often.

It's not clear this is really a problem in real apps vs just our anal MockDirWrapper but I think we should fix it..."
1,"DocMakers setup for the ""docs.dir"" property fails when passing an absolute path.setConfig in TrecDocMaker assumes docs.dir is a relative path. Therefore it create new File(workDir, docs.dir). However, if docs.dir is an absolute path, this works incorrectly and results in No txt files in dataDir exception."
1,"BeanConfig may incorrectly throw ConfigurationExceptionWith the changes from JCR-1462 the BeanConfig.newInstance() may throw a ConfigurationException if the bean does not support a configuration parameter that is configured.

There may be cases where the check in newInstance() yields an unsupported property even though there is a bean property present with the given key. Because the implementation uses 'map.get(key) == null'  as a check for a property name the method will throw if the key exists but the value is null.

The implementation should rather use 'map.containsKey(key)'."
1,"getScheme() and getPort() return wrong defaults for HttpsURLgetScheme(), if called on an instance of HttpsURL, wrongly returns http instead
of https. That's because dynamic data binding doesn't work for final static
fields (see DEFAULT_SCHEME)."
1,Deadlock in IndexWriterIf autoCommit == true a merge usually triggers a commit. A commit (prepareCommit) can trigger a merge vi the flush method. There is a synchronization mechanism for commit (commitLock) and a separate synchronization mechanism for merging (ConcurrentMergeScheduler.wait). If one thread holds the commitLock monitor and another one holds the ConcurrentMergeScheduler monitor we have a deadlock.
1,"QueryStat getPopularQueries doesn't set the proper positionEmbarrassing copy/paste error. I was updating the wrong array and the position info was never returned. 

This made any jmx client to fail with: 
at javax.management.openmbean.TabularDataSupport.checkValueAndIndex(TabularDataSupport.java:871) 
at javax.management.openmbean.TabularDataSupport.internalPut(TabularDataSupport.java:331) 
at javax.management.openmbean.TabularDataSupport.put(TabularDataSupport.java:323) 
at org.apache.jackrabbit.core.jmx.QueryStatManager.asTabularData(QueryStatManager.java:103)"
1,"NPE Thrown when two Cluster Nodes are hitting the same underlying database.I've created a test that creates two repositories with clustering enabled that are backed by the same database.  Using the following workflow causes a NullPointerException to be thrown.

The workflow I'm using is:
The root node is versioned.
ClusterNode1 creates a versioned child node named ""foo"".
The test waits to make sure the syncDelay has passed so ClusterNode2 will notice the newly created node.
ClusterNode2 retrieves the ""foo"" child node and removes it.
The test waits for the change ClusterNode1 to sync with that change.
ClusterNode1 tries to create another new node however a NullPointerException is thrown when the it tries to checkout the rootNode.

java.lang.NullPointerException: null values not allowed
	at org.apache.commons.collections.map.AbstractReferenceMap.put(AbstractReferenceMap.java:251)
	at org.apache.jackrabbit.core.version.VersionManagerImpl.getItem(VersionManagerImpl.java:280)
	at org.apache.jackrabbit.core.version.XAVersionManager.getItem(XAVersionManager.java:334)
	at org.apache.jackrabbit.core.version.AbstractVersionManager.getVersion(AbstractVersionManager.java:87)
	at org.apache.jackrabbit.core.NodeImpl.getBaseVersion(NodeImpl.java:3198)
	at org.apache.jackrabbit.core.NodeImpl.checkout(NodeImpl.java:2991)
	at com.cerner.system.configuration.repository.jcr.SimpleJackrabbitConflictTest.testNullPointerExceptionThrown(SimpleJackrabbitConflictTest.java:96)"
1,"CartesianPolyFilterBuilder doesn't properly account for which tiers actually exist in the index In the CartesianShapeFilterBuilder, there is logic that determines the ""best fit"" tier to create the Filter against.  However, it does not account for which fields actually exist in the index when doing so.  For instance, if you index tiers 1 through 10, but then choose a very small radius to restrict the space to, it will likely choose a tier like 15 or 16, which of course does not exist."
1,"TestBytesRefHash#testCompact is brokenTestBytesRefHash#testCompact fails when run with ant test -Dtestcase=TestBytesRefHash -Dtestmethod=testCompact -Dtests.seed=-7961072421643387492:5612141247152835360
{noformat}

    [junit] Testsuite: org.apache.lucene.util.TestBytesRefHash
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.454 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestBytesRefHash -Dtestmethod=testCompact -Dtests.seed=-7961072421643387492:5612141247152835360
    [junit] NOTE: test params are: codec=PreFlex, locale=et, timezone=Pacific/Tahiti
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestBytesRefHash]
    [junit] NOTE: Linux 2.6.35-28-generic amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=12,threads=1,free=363421800,total=379322368
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testCompact(org.apache.lucene.util.TestBytesRefHash):	Caused an ERROR
    [junit] bitIndex < 0: -27
    [junit] java.lang.IndexOutOfBoundsException: bitIndex < 0: -27
    [junit] 	at java.util.BitSet.set(BitSet.java:262)
    [junit] 	at org.apache.lucene.util.TestBytesRefHash.testCompact(TestBytesRefHash.java:146)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1260)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1189)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.util.TestBytesRefHash FAILED
{noformat}

the test expects that _TestUtil.randomRealisticUnicodeString(random, 1000); will never return the same string.

I will upload a patch soon."
1,"IdleConnectionHandler can leave closed connections in a inconsistent stateIdleConnectionHandler when shutting down 'stale' connection does not update the state of AbstractPoolEntry thus causing an inconsistency between the state of the connection (closed) and that of the pool entry (still assumed open). The problem is mitigated by the fact that the pooling manager usually evicts closed connections almost immediately. There is a small window of time in the ThreadSafeClientConnManager#closeIdleConnection method, at which a connection can be closed by the IdleConnectionHandler and immediately leased from the pool by another thread in an inconsistent state before the main thread gets a chance to re-acquire the pool lock and clean out closed connections.

For 4.0.x the problem can be worked around by retaining the pool lock for the entire span of the #closeIdleConnection. For the 4.1 branch a better solution should be devised. This probably means a complete rewrite or deprecation of IdleConnectionHandler."
1,"iterative removal of same-name sibling nodes might under certain circumstances throw unexpected exceptionscode fragment to reproduce the issue:

            // setup test
            if (root.hasNode(""tmp"")) {
                root.getNode(""tmp"").remove();
                session.save();
            }
            Node tmp = root.addNode(""tmp"");
            for (int i = 0; i < 4; i++) {
                Node a = tmp.addNode(""a"");
                System.out.println(""added "" + a.getPath());
            }
            session.save();

            // iterative removal of same name sibling child nodes
            NodeIterator ni = tmp.getNodes();
            while (ni.hasNext()) {
                Node n = ni.nextNode();
                System.out.println(""removing "" + n.getPath());
                n.remove();
                tmp.save();
            }

console output:

added /tmp/a
added /tmp/a[2]
added /tmp/a[3]
added /tmp/a[4]
removing /tmp/a
removing /tmp/a
removing /
javax.jcr.RepositoryException: /: cannot remove root node
	at org.apache.jackrabbit.core.ItemImpl.internalRemove(ItemImpl.java:766)
	at org.apache.jackrabbit.core.ItemImpl.remove(ItemImpl.java:997)
	at org.apache.jackrabbit.core.Test.main(Test.java:141)


note that the msg of the exception is misleading: the above code did never try to remove
the root node. 

the exception is caused by a bug in CachingHierarchyManager which fails to update
the cache correctly.

btw: if you comment the first logging stmt, i.e. 

                //System.out.println(""added "" + a.getPath());

the problem doesn't occur anymore."
1,"PdfTextFilter may leave parsed document open in case of errorsIn case of errors in a parsed PDF document jackrabbit may fail to properly close the parsed document. PDFBox will write a stack trace to system out at finalize to warn agains this.

this is the resulting log:

WARN org.apache.jackrabbit.core.query.LazyReader LazyReader.java(read:82) 20.02.2007 15:42:50 exception initializing reader org.apache.jackrabbit.core.query.PdfTextFilter$1: java.io.IOException: Error: Expected hex number, actual=' 2'
java.lang.Throwable: Warning: You did not close the PDF Document
   at org.pdfbox.cos.COSDocument.finalize(COSDocument.java:384)
   at java.lang.ref.Finalizer.invokeFinalizeMethod(Native Method)
   at java.lang.ref.Finalizer.runFinalizer(Finalizer.java:83)
   at java.lang.ref.Finalizer.access$100(Finalizer.java:14)
   at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:160)


this may happens because the parse() method at

parser = new PDFParser(new BufferedInputStream(in));
parser.parse();

immediately creates a document, but it can throw an exception while processing the file.
PdfTextFilter should check if parser still holds a document and close it appropriately.

"
1,"Errors during concurrent session import of nodes with same UUIDs21.08.2009 16:22:14 *ERROR* [Executor 0] ConnectionRecoveryManager: could not execute statement, reason: The statement was aborted because it would have caused a duplicate key value in a unique or primary key constraint or unique index identified by 'SQL090821042140130' defined on 'DEFAULT_BUNDLE'., state/code: 23505/20000 (ConnectionRecoveryManager.java, line 453)
21.08.2009 16:22:14 *ERROR* [Executor 0] BundleDbPersistenceManager: failed to write bundle: 6c292772-349e-42b3-8255-7729615c67de (BundleDbPersistenceManager.java, line 1212)
ERROR 23505: The statement was aborted because it would have caused a duplicate key value in a unique or primary key constraint or unique index identified by 'SQL090821042140130' defined on 'DEFAULT_BUNDLE'.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.IndexChanger.insertAndCheckDups(Unknown Source)
	at org.apache.derby.impl.sql.execute.IndexChanger.doInsert(Unknown Source)
	at org.apache.derby.impl.sql.execute.IndexChanger.insert(Unknown Source)
	at org.apache.derby.impl.sql.execute.IndexSetChanger.insert(Unknown Source)
	at org.apache.derby.impl.sql.execute.RowChangerImpl.insertRow(Unknown Source)
	at org.apache.derby.impl.sql.execute.InsertResultSet.normalInsertCore(Unknown Source)
	at org.apache.derby.impl.sql.execute.InsertResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.execute(Unknown Source)
	at org.apache.jackrabbit.core.persistence.bundle.util.ConnectionRecoveryManager.executeStmtInternal(ConnectionRecoveryManager.java:371)
	at org.apache.jackrabbit.core.persistence.bundle.util.ConnectionRecoveryManager.executeStmtInternal(ConnectionRecoveryManager.java:298)
	at org.apache.jackrabbit.core.persistence.bundle.util.ConnectionRecoveryManager.executeStmt(ConnectionRecoveryManager.java:261)
	at org.apache.jackrabbit.core.persistence.bundle.util.ConnectionRecoveryManager.executeStmt(ConnectionRecoveryManager.java:239)
	at org.apache.jackrabbit.core.persistence.bundle.BundleDbPersistenceManager.storeBundle(BundleDbPersistenceManager.java:1209)
	at org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.putBundle(AbstractBundlePersistenceManager.java:709)
	at org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.storeInternal(AbstractBundlePersistenceManager.java:651)
	at org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.store(AbstractBundlePersistenceManager.java:527)
	at org.apache.jackrabbit.core.persistence.bundle.BundleDbPersistenceManager.store(BundleDbPersistenceManager.java:563)
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:724)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:1101)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:351)
	at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:354)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:326)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:326)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1098)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:925)
	at org.apache.jackrabbit.core.ConcurrentImportTest$1.execute(ConcurrentImportTest.java:73)
	at org.apache.jackrabbit.core.AbstractConcurrencyTest$Executor.run(AbstractConcurrencyTest.java:209)
	at java.lang.Thread.run(Thread.java:637)
"
1,"RTF text extractor fails on Java 1.4 in some environmentsI've seen the RTF text extractor fail with the following errors with Java 1.4 on Unix platforms. Both are platform issues, but Jackrabbit should be prepared for such cases and for example just log a warning and return an empty text stream when encountering these errors.

java.lang.UnsatisfiedLinkError: /home/jukka/bin/java/j2sdk1.4.2_18/jre/lib/i386/libawt.so: libXp.so.6: cannot open shared object file: No such file or directory
        at java.lang.ClassLoader$NativeLibrary.load(Native Method)
        at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1586)
        at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1503)
        at java.lang.Runtime.loadLibrary0(Runtime.java:788)
        at java.lang.System.loadLibrary(System.java:834)
        at sun.security.action.LoadLibraryAction.run(LoadLibraryAction.java:50)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.awt.NativeLibLoader.loadLibraries(NativeLibLoader.java:38)
        at sun.awt.DebugHelper.<clinit>(DebugHelper.java:29)
        at java.awt.EventQueue.<clinit>(EventQueue.java:83)
        at javax.swing.SwingUtilities.isEventDispatchThread(SwingUtilities.java:1238)
        at javax.swing.text.StyleContext.reclaim(StyleContext.java:419)
        at javax.swing.text.StyleContext.addAttribute(StyleContext.java:276)
        at javax.swing.text.StyleContext$NamedStyle.addAttribute(StyleContext.java:1468)
        at javax.swing.text.StyleContext$NamedStyle.setName(StyleContext.java:1278)
        at javax.swing.text.StyleContext$NamedStyle.<init>(StyleContext.java:1226)
        at javax.swing.text.StyleContext.addStyle(StyleContext.java:88)
        at javax.swing.text.StyleContext.<init>(StyleContext.java:68)
        at javax.swing.text.DefaultStyledDocument.<init>(DefaultStyledDocument.java:88)
        at org.apache.jackrabbit.extractor.RTFTextExtractor.extractText(RTFTextExtractor.java:60)
        at org.apache.jackrabbit.extractor.RTFTextExtractorTest.testExtractor(RTFTextExtractorTest.java:35)

java.lang.InternalError: Can't connect to X11 window server using ':0.0' as the value of the DISPLAY variable.
	at sun.awt.X11GraphicsEnvironment.initDisplay(Native Method)
	at sun.awt.X11GraphicsEnvironment.<clinit>(X11GraphicsEnvironment.java:134)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:141)
	at java.awt.GraphicsEnvironment.getLocalGraphicsEnvironment(GraphicsEnvironment.java:62)
	at sun.awt.motif.MToolkit.<clinit>(MToolkit.java:81)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:141)
	at java.awt.Toolkit$2.run(Toolkit.java:748)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.awt.Toolkit.getDefaultToolkit(Toolkit.java:739)
	at java.awt.Toolkit.getEventQueue(Toolkit.java:1519)
	at java.awt.EventQueue.isDispatchThread(EventQueue.java:676)
	at javax.swing.SwingUtilities.isEventDispatchThread(SwingUtilities.java:1238)
	at javax.swing.text.StyleContext.reclaim(StyleContext.java:419)
	at javax.swing.text.StyleContext.addAttribute(StyleContext.java:276)
	at javax.swing.text.StyleContext$NamedStyle.addAttribute(StyleContext.java:1468)
	at javax.swing.text.StyleContext$NamedStyle.setName(StyleContext.java:1278)
	at javax.swing.text.StyleContext$NamedStyle.<init>(StyleContext.java:1226)
	at javax.swing.text.StyleContext.addStyle(StyleContext.java:88)
	at javax.swing.text.StyleContext.<init>(StyleContext.java:68)
	at javax.swing.text.DefaultStyledDocument.<init>(DefaultStyledDocument.java:88)
	at org.apache.jackrabbit.extractor.RTFTextExtractor.extractText(RTFTextExtractor.java:60)
	at org.apache.jackrabbit.extractor.RTFTextExtractorTest.testExtractor(RTFTextExtractorTest.java:35)

"
1,"WriteLineDocTask incorrectly normalizes fieldsWriteLineDocTask normalizes the body, title and date fields by replacing any ""\t"" with a space. However, if any one of them contains newlines, LineDocMaker will fail, since the first line read will include some of the text, however the second line, which it now expects to be a new document, will include other parts of the text.

I don't know how we didn't hit it so far. Maybe the wikipedia text doesn't have such lines, however when I ran over the TREC collection I hit a lot of those.

I will attach a patch shortly."
1,"MultipartPost closes input streamThis is something of a collection of issues that are all interrelated.

1. MultipartPost calls close on the outputstream it retrieved from 
HttpConnection which causes an exception to be thrown later on.  This call 
should be replaced with a call to flush().

2. The MultipartPost classes do not have any logging in them.  We should add 
trace statements at a minimum.

3. new FilePart(String, File) throws a null pointer exception.

4. The tests in TestPartsNoHost are broken.

I'll attach patches for these fixes in a moment, broken down as much as 
possible."
1,"CookieSpecBase.domainMatch() leaks cookies to 3rd party domainsThe change committed for #32833
<http://issues.apache.org/bugzilla/show_bug.cgi?id=32833> is buggy; it doesn't
match browser behavior and in fact leaks cookies to third party domains. 

To see, try the following:

CookieSpecBase cspec = new CookieSpecBase();
Cookie cookie = new Cookie("".hotmail.com"",""foo"",""bar"",""/"",Integer.MAX_VALUE,false);
cspec.match(""iwanttostealcookiesfromhotmail.com"",80,""/"",false,cookie);

It will return true. Testing in Firefox1.0.4 and IE6 show no such similar
leakage for similar cases. (Indeed, it'd be a headline-making privacy bug if
they were to do this.)

Those browsers do, in my limited testing, behave as desired by the filer of
#32833: a cookie of domain value '.mydomain.com' will be returned to exact host
'mydomain.com' (. However, the fix that was suggested was overbroad.

I suggest instead for CookieSpecBase.domainMatch():

    public boolean domainMatch(final String host, final String domain) {
// BUGGY: matches a '.service.com' cookie to hosts like 'enemyofservice.com'
//        return host.endsWith(domain)
//            || (domain.startsWith(""."") && host.endsWith(domain.substring(1)));
// BETTER: RFC2109, plus matches a '.service.com' cookie to exact host 'service.com'
        return host.equals(domain)
            || (domain.startsWith(""."") 
                    && (host.endsWith(domain)
                            || host.equals(domain.substring(1))));
    }"
1,"Jackrabbit's lucene based query implementation does not check property constraints on the root node.XPath queries of the kind ""/jcr:root[<any property constraint>]"" apparently always match."
1,"impl.conn.Wire uses String.getBytes() which depends on the default charsetimpl.conn.Wire uses String.getBytes() which depends on the default charset

The methods 
public void output(final String s)
and
public void input(final String s)

could probably be recoded to avoid this problem, as the output routine uses a StringBuilder."
1,"BasicOperations.concatenate creates invariantsI started writing a test for LUCENE-2716, and i found a problem with BasicOperations.concatenate(Automaton, Automaton):
it creates automata with invariant representation (which should never happen, unless you manipulate states/transitions manually).

strangely enough the BasicOperations.concatenate(List<Automaton>) does not have this problem.
"
1,"AccessControlProvider#getEffectivePolicies for a set of principals does not include repo-level acas of JCR-2774 the resource based ac implementation allows to edit permissions for repository level operations.
however, ACLProvider#getEffectivePolicies(Set<Principal>, CompiledPermissions) does not include the repo level AC
in the result set due to a missing test for regular acl OR repo-level acl."
1,"Repository holds onto Session instance after logout
After a call to Session.logout the Repository instance's activeSession map still holds a reference to the session. This causes a problem when trying to unlock nodes locked by another session, the addLockToken method rejects the lock token.

Looking at the code in Session.logout, it tries to notify SessionListeners about the logout but Repository, which implements the SessionListener interface and will remove a session on logout, doesn't register with the Session to receive the logout notification.
"
1,"Cannot move a first-level nodeGiven /nodeA,
session.move(""/nodeA"", ""/nodeB"")

throws this exception:

javax.jcr.PathNotFoundException: no ancestor at degree 1: {}
	at org.apache.jackrabbit.spi.commons.name.PathFactoryImpl$PathImpl.getAncestor(PathFactoryImpl.java:481)
	at org.apache.jackrabbit.core.retention.RetentionRegistryImpl.hasEffectiveRetention(RetentionRegistryImpl.java:291)
	at org.apache.jackrabbit.core.ItemValidator.hasRetention(ItemValidator.java:426)
	at org.apache.jackrabbit.core.ItemValidator.checkCondition(ItemValidator.java:328)
	at org.apache.jackrabbit.core.ItemValidator.checkRemove(ItemValidator.java:281)
	at org.apache.jackrabbit.core.SessionImpl.move(SessionImpl.java:1075)
	at org.apache.jackrabbit.core.MoveAtRootTest.testMoveAtRoot(MoveAtRootTest.java:54)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at org.apache.jackrabbit.test.AbstractJCRTest.run(AbstractJCRTest.java:456)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:130)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)

"
1,"jackrabbit wrongly think nodetype is changed on nodetype re-registrationWhen trying node type re-registration with jackrabbit 2.0, it wrongly detects a nodetype as having changed, with non-trivial changes. Example nodetype definition;

[nen:profile] > mix:referenceable mixin orderable
- nen:dn (string)
- nen:cn (string)
- * (string)
+ * multiple

Exception on nodetype re-registration;

javax.jcr.RepositoryException: The following nodetype change contains
non-trivial changes.Up until now only trivial changes are supported.
(see javadoc for org.apache.jackrabbit.core.nodetype.NodeTypeDefDiff):
org.apache.jackrabbit.core.nodetype.NodeTypeDefDiff[
       nodeTypeName={http://netenviron.com/nen/1.0}profile,
       mixinFlagDiff=NONE,
       supertypesDiff=NONE,
       propertyDifferences=[
               org.apache.jackrabbit.core.nodetype.NodeTypeDefDiff$PropDefDiff[itemName={http://netenviron.com/nen/1.0}dn,
type=TRIVIAL, operation=MODIFIED],
               org.apache.jackrabbit.core.nodetype.NodeTypeDefDiff$PropDefDiff[itemName={http://netenviron.com/nen/1.0}cn,
type=TRIVIAL, operation=MODIFIED],
               org.apache.jackrabbit.core.nodetype.NodeTypeDefDiff$PropDefDiff[itemName={}*,
type=TRIVIAL, operation=MODIFIED]
       ],
       childNodeDifferences=[
               org.apache.jackrabbit.core.nodetype.NodeTypeDefDiff$ChildNodeDefDiff[itemName={}*,
type=MAJOR, operation=REMOVED],
               org.apache.jackrabbit.core.nodetype.NodeTypeDefDiff$ChildNodeDefDiff[itemName={}*,
type=TRIVIAL, operation=ADDED]
       ]
]

       at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.reregisterNodeType(NodeTypeRegistry.java:442)
       at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.reregisterNodeType(NodeTypeRegistry.java:363)
       at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.registerNodeTypes(NodeTypeManagerImpl.java:589)
       at org.apache.jackrabbit.commons.cnd.CndImporter.registerNodeTypes(CndImporter.java:118)
       at com.netenviron.content.manager.SessionManager.checkRepositorySchema(SessionManager.java:355)
       at com.netenviron.content.manager.SessionManager.afterPropertiesSet(SessionManager.java:199)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1288)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1257)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:438)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory$1.run(AbstractAutowireCapableBeanFactory.java:383)
       at java.security.AccessController.doPrivileged(Native Method)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:353)
       at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:245)
       at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:169)
       at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:242)
       at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:164)
       at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:269)
       at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:104)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1172)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:940)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:437)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory$1.run(AbstractAutowireCapableBeanFactory.java:383)
       at java.security.AccessController.doPrivileged(Native Method)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:353)
       at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:245)
       at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:169)
       at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:242)
       at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:164)
       at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:269)
       at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:104)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1172)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:940)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:437)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory$1.run(AbstractAutowireCapableBeanFactory.java:383)
       at java.security.AccessController.doPrivileged(Native Method)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:353)
       at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:245)
       at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:169)
       at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:242)
       at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:164)
       at org.springframework.beans.factory.support.AbstractBeanFactory.getTypeForFactoryBean(AbstractBeanFactory.java:1223)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.getTypeForFactoryBean(AbstractAutowireCapableBeanFactory.java:582)
       at org.springframework.beans.factory.support.AbstractBeanFactory.isTypeMatch(AbstractBeanFactory.java:438)
       at org.springframework.beans.factory.support.DefaultListableBeanFactory.getBeanNamesForType(DefaultListableBeanFactory.java:214)
       at org.springframework.beans.factory.support.DefaultListableBeanFactory.getBeanNamesForType(DefaultListableBeanFactory.java:189)
       at org.springframework.beans.factory.BeanFactoryUtils.beanNamesForTypeIncludingAncestors(BeanFactoryUtils.java:143)
       at org.springframework.beans.factory.support.DefaultListableBeanFactory.findAutowireCandidates(DefaultListableBeanFactory.java:614)
       at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:572)
       at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredMethodElement.inject(AutowiredAnnotationBeanPostProcessor.java:496)
       at org.springframework.beans.factory.annotation.InjectionMetadata.injectMethods(InjectionMetadata.java:87)
       at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessPropertyValues(AutowiredAnnotationBeanPostProcessor.java:250)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:928)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:437)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory$1.run(AbstractAutowireCapableBeanFactory.java:383)
       at java.security.AccessController.doPrivileged(Native Method)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:353)
       at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:245)
       at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:169)
       at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:242)
       at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:164)
       at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:269)
       at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:104)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1172)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:940)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:437)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory$1.run(AbstractAutowireCapableBeanFactory.java:383)
       at java.security.AccessController.doPrivileged(Native Method)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:353)
       at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:245)
       at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:169)
       at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:242)
       at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:164)
       at org.springframework.beans.factory.support.DefaultListableBeanFactory.getBeansOfType(DefaultListableBeanFactory.java:299)
       at org.springframework.context.support.AbstractApplicationContext.getBeansOfType(AbstractApplicationContext.java:955)
       at org.springframework.context.support.AbstractApplicationContext.registerListeners(AbstractApplicationContext.java:712)
       at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:366)
       at org.springframework.web.context.ContextLoader.createWebApplicationContext(ContextLoader.java:261)
       at org.springframework.web.context.ContextLoader.initWebApplicationContext(ContextLoader.java:199)
       at org.springframework.web.context.ContextLoaderListener.contextInitialized(ContextLoaderListener.java:45)
       at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:3972)
       at org.apache.catalina.core.StandardContext.start(StandardContext.java:4467)
       at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:791)
       at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:771)
       at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:526)
       at org.apache.catalina.startup.HostConfig.deployWAR(HostConfig.java:905)
       at org.apache.catalina.startup.HostConfig.deployWARs(HostConfig.java:740)
       at org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:500)
       at org.apache.catalina.startup.HostConfig.check(HostConfig.java:1345)
       at org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:303)
       at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:119)
       at org.apache.catalina.core.ContainerBase.backgroundProcess(ContainerBase.java:1337)
       at org.apache.catalina.core.ContainerBase$ContainerBackgroundProcessor.processChildren(ContainerBase.java:1601)
       at org.apache.catalina.core.ContainerBase$ContainerBackgroundProcessor.processChildren(ContainerBase.java:1610)
       at org.apache.catalina.core.ContainerBase$ContainerBackgroundProcessor.run(ContainerBase.java:1590)
       at java.lang.Thread.run(Thread.java:637)
"
1,"CachingHttpClient returns a 411 respones when executing a POST (HttpPost) request The CachingHttpClient validates requests prior executing them, by calling RequestProtocolCompliance.requestIsFatallyNonCompliant(..).

When executing an HttpPost, this method considers the request is invalid because it does not contain (yet) a content-length header. Indeed, I observed that this header is generated at the time the DefaultHttpClient fires the request.

NB: i'm using the Cache 4.1-alpha2 plugged over the HttpClient 4.0.1-final. I can't use the latest version for both because I need to rely on a stable version if there's any. I would be curious to know if we get the same behaviour in 4.1...

Anyway, I would see two fixes for that issue:
- make HttpPost set the content-length at the time the entity is set,
- or remove the validation step on the CachingHttpClient side.
"
1,NPE in ConsolidatingChangeLog for id base NodeIdConsolidatingChangeLog does not guard guard against null in the path value of a NodeId. 
1,"Suggested Patches to MultiPhraseQuery and QueryTermExtractor (for use with HighLighter)I encountered a problem with the Highlighter, where it was not recognizing MultiPhraseQuery.
To fix this, I developed the following two patches:

=====================================================
1. Addition to org.apache.lucene.search.MultiPhraseQuery:

Add the following method:

/** Returns the set of terms in this phrase. */
public Term[] getTerms() {
  ArrayList allTerms = new ArrayList();
  Iterator iterator = termArrays.iterator();
  while (iterator.hasNext()) {
    Term[] terms = (Term[])iterator.next();
    for (int i = 0, n = terms.length; i < n; ++i) {
      allTerms.add(terms[i]);
    }
  }
  return (Term[])allTerms.toArray(new Term[0]);
}

=====================================================
2. Patch to org.apache.lucene.search.highlight.QueryTermExtractor:

a) Add the following import:
import org.apache.lucene.search.MultiPhraseQuery;

b) Add the following code to the end of the getTerms(...) method:
      else  if(query instanceof MultiPhraseQuery)
              getTermsFromMultiPhraseQuery((MultiPhraseQuery) query, terms, fieldName);
  }

c) Add the following method:
 private static final void getTermsFromMultiPhraseQuery(MultiPhraseQuery query, HashSet terms, String fieldName)
 {
   Term[] queryTerms = query.getTerms();
   int i;

   for (i = 0; i < queryTerms.length; i++)
   {
       if((fieldName==null)||(queryTerms[i].field()==fieldName))
       {
           terms.add(new WeightedTerm(query.getBoost(),queryTerms[i].text()));
       }
   }
 }


=====================================================

Can the team update the repository?

Thanks
Michael Harhen "
1,"Group#getMembers may list inherited members multiple timesi just happen to detect the following regression that seems to be introduces quite a while ago:

Group#getMembers is defined to return all group members including those inherited by another group being member of that group.

Example:
User t
Group a : t is declared member
Group b : t is declared member
Group c : a, b are declared members

The expected result of Group.getMembers was: a, b and t.

What is currently happening is that t is included twice in the returned iterator.
Quickly testing on jackrabbit 2.0 revealed that this used to work before...

I didn't carefully check when that bug has been introduced but the the refactoring of the membership
collections seems to be a possible culprit.

"
1,"Handle URIs with path component nullHttpClient does not handle URIs with path component null (e.g. http://google.com) the same as path component '/'. This results e.g. in a ProtocolException ""The server failed to respond with a valid HTTP response""."
1,Session holds LockToken after removeLockToken in XA Environment
1,"HttpOptions.getAllowedMethods expects single Allow headerIn client.methods.HttpOptions.getAllowMethods(), a single Allow header is parsed to obtain the result. Since the value is a comma-separated list, servers can optionally return the values in multiple headers. HttpMethod.getHeaders(name) should be used instead of .getFirstHeader(name).
"
1,"Node.getPath() will corrupt the sessionWhen calling Node.getPath() anytime, no mather if its before or after save, and when deleting nodes, the internal reference points to the wrong nodes. 
The attached test will always fail with a javax.jcr.RepositoryException: /: cannot remove root node. 
We have seen other configurations where a node suddenly behaves as the another node that has references and throw a reference exception, and yet other configurations where the node we though we deleted still exists, and another node has now disappeared.

I do not know what causes the bug,a good bet is perhaps the CachingHierarchyManager?. It was not present in Jackrabbit 1.0.1, but was introduced in 1.1.

Have also tested the latest release: 1.2.2, and the bug is still present there.
"
1,"BundleFsPersistenceManager has no property called: minBlobSize2008-04-03 16:48:51,ERROR,org.apache.jackrabbit.core.RepositoryImpl,Thread-237 failed to start Repository: Cannot instantiate persistence manager org.apache.jackrabbit.core.persistence.bundle.BundleFsPersistenceManager: The bean of type: org.apache.jackrabbit.core.persistence.bundle.BundleFsPersistenceManager has no property called: minBlobSize
javax.jcr.RepositoryException: Cannot instantiate persistence manager org.apache.jackrabbit.core.persistence.bundle.BundleFsPersistenceManager: The bean of type: org.apache.jackrabbit.core.persistence.bundle.BundleFsPersistenceManager has no property called: minBlobSize: The bean of type: org.apache.jackrabbit.core.persistence.bundle.BundleFsPersistenceManager has no property called: minBlobSize
	at org.apache.jackrabbit.core.RepositoryImpl.createPersistenceManager(RepositoryImpl.java:1176)
	at org.apache.jackrabbit.core.RepositoryImpl.createVersionManager(RepositoryImpl.java:390)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:294)
	at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:557)
	at pps.jcr.util.RepositoryManager.createRepository(RepositoryManager.java:117)
	at pps.jcr.util.RepositoryManager.startRepository(RepositoryManager.java:43)
	at pps.jcr.ejb.session.JcrUtilFacade.startRepository(JcrUtilFacade.java:62)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.sun.enterprise.security.application.EJBSecurityManager.runMethod(EJBSecurityManager.java:1067)
	at com.sun.enterprise.security.SecurityUtil.invoke(SecurityUtil.java:176)
	at com.sun.ejb.containers.BaseContainer.invokeTargetBeanMethod(BaseContainer.java:2895)
	at com.sun.ejb.containers.BaseContainer.intercept(BaseContainer.java:3986)
	at com.sun.ejb.containers.EJBLocalObjectInvocationHandler.invoke(EJBLocalObjectInvocationHandler.java:197)
	at com.sun.ejb.containers.EJBLocalObjectInvocationHandlerDelegate.invoke(EJBLocalObjectInvocationHandlerDelegate.java:127)
	at $Proxy181.startRepository(Unknown Source)
	at pps.jcr.web.JcrLifecycleListener.contextInitialized(JcrLifecycleListener.java:55)
	at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:4523)
	at org.apache.catalina.core.StandardContext.start(StandardContext.java:5184)
	at com.sun.enterprise.web.WebModule.start(WebModule.java:326)
	at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:973)
	at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:957)
	at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:688)
	at com.sun.enterprise.web.WebContainer.loadWebModule(WebContainer.java:1584)
	at com.sun.enterprise.web.WebContainer.loadWebModule(WebContainer.java:1222)
	at com.sun.enterprise.web.WebContainer.loadJ2EEApplicationWebModules(WebContainer.java:1147)
	at com.sun.enterprise.server.TomcatApplicationLoader.doLoad(TomcatApplicationLoader.java:141)
	at com.sun.enterprise.server.AbstractLoader.load(AbstractLoader.java:244)
	at com.sun.enterprise.server.ApplicationManager.applicationDeployed(ApplicationManager.java:336)
	at com.sun.enterprise.server.ApplicationManager.applicationDeployed(ApplicationManager.java:210)
	at com.sun.enterprise.server.ApplicationManager.applicationDeployed(ApplicationManager.java:645)
	at com.sun.enterprise.admin.event.AdminEventMulticaster.invokeApplicationDeployEventListener(AdminEventMulticaster.java:928)
	at com.sun.enterprise.admin.event.AdminEventMulticaster.handleApplicationDeployEvent(AdminEventMulticaster.java:912)
	at com.sun.enterprise.admin.event.AdminEventMulticaster.processEvent(AdminEventMulticaster.java:461)
	at com.sun.enterprise.admin.event.AdminEventMulticaster.multicastEvent(AdminEventMulticaster.java:176)
	at com.sun.enterprise.admin.server.core.DeploymentNotificationHelper.multicastEvent(DeploymentNotificationHelper.java:308)
	at com.sun.enterprise.deployment.phasing.DeploymentServiceUtils.multicastEvent(DeploymentServiceUtils.java:226)
	at com.sun.enterprise.deployment.phasing.ServerDeploymentTarget.sendStartEvent(ServerDeploymentTarget.java:298)
	at com.sun.enterprise.deployment.phasing.ApplicationStartPhase.runPhase(ApplicationStartPhase.java:132)
	at com.sun.enterprise.deployment.phasing.DeploymentPhase.executePhase(DeploymentPhase.java:108)
	at com.sun.enterprise.deployment.phasing.PEDeploymentService.executePhases(PEDeploymentService.java:919)
	at com.sun.enterprise.deployment.phasing.PEDeploymentService.start(PEDeploymentService.java:591)
	at com.sun.enterprise.deployment.phasing.PEDeploymentService.start(PEDeploymentService.java:635)
	at com.sun.enterprise.admin.mbeans.ApplicationsConfigMBean.start(ApplicationsConfigMBean.java:744)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.sun.enterprise.admin.MBeanHelper.invokeOperationInBean(MBeanHelper.java:375)
	at com.sun.enterprise.admin.MBeanHelper.invokeOperationInBean(MBeanHelper.java:358)
	at com.sun.enterprise.admin.config.BaseConfigMBean.invoke(BaseConfigMBean.java:464)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
	at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.sun.enterprise.admin.util.proxy.ProxyClass.invoke(ProxyClass.java:90)
	at $Proxy1.invoke(Unknown Source)
	at com.sun.enterprise.admin.server.core.jmx.SunoneInterceptor.invoke(SunoneInterceptor.java:304)
	at com.sun.enterprise.interceptor.DynamicInterceptor.invoke(DynamicInterceptor.java:174)
	at com.sun.enterprise.deployment.client.DeploymentClientUtils.startApplication(DeploymentClientUtils.java:145)
	at com.sun.enterprise.deployment.client.DeployAction.run(DeployAction.java:537)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.IllegalArgumentException: The bean of type: org.apache.jackrabbit.core.persistence.bundle.BundleFsPersistenceManager has no property called: minBlobSize
	at org.apache.commons.collections.BeanMap.put(BeanMap.java:367)
	at org.apache.jackrabbit.core.config.BeanConfig.newInstance(BeanConfig.java:109)
	at org.apache.jackrabbit.core.RepositoryImpl.createPersistenceManager(RepositoryImpl.java:1171)
	... 64 more

<?xml version=""1.0""?>
<Repository>
    <DataStore class=""org.apache.jackrabbit.core.data.FileDataStore"">
        <param name=""path"" value=""${rep.home}/datastore""/>
        <param name=""minRecordLength"" value=""100""/>
    </DataStore>    
    <FileSystem class=""org.apache.jackrabbit.core.fs.local.LocalFileSystem"">
        <param name=""path"" value=""${rep.home}""/>
    </FileSystem>    
    <Security appName=""Jackrabbit"">
        <AccessManager class=""org.apache.jackrabbit.core.security.SimpleAccessManager"" />
        <LoginModule class=""org.apache.jackrabbit.core.security.SimpleLoginModule"">
            <param name=""anonymousId"" value=""anonymous"" />
        </LoginModule>
    </Security>
    <Workspaces rootPath=""${rep.home}/workspaces"" defaultWorkspace=""default"" />
    <Workspace name=""${wsp.name}"">
        <FileSystem class=""org.apache.jackrabbit.core.fs.local.LocalFileSystem"">
            <param name=""path"" value=""${rep.home}/${wsp.name}""/>
        </FileSystem>            
        <!--  <PersistenceManager class=""org.apache.jackrabbit.core.persistence.obj.ObjectPersistenceManager""/>  -->
        <PersistenceManager class=""org.apache.jackrabbit.core.persistence.bundle.BundleFsPersistenceManager"">
            <param name=""bundleCacheSize"" value=""8""/> 
            <param name=""blobFSBlockSize"" value=""0""/> 
            <param name=""minBlobSize"" value=""4096""/> 
            <param name=""errorHandling"" value=""""/>             
        </PersistenceManager>        
        <SearchIndex class=""org.apache.jackrabbit.core.query.lucene.SearchIndex"">
            <param name=""path"" value=""${wsp.home}/index""/>    
            <param name=""textFilterClasses"" value=""
                   org.apache.jackrabbit.extractor.MsExcelTextExtractor,
                   org.apache.jackrabbit.extractor.MsPowerPointTextExtractor,
                   org.apache.jackrabbit.extractor.MsWordTextExtractor,
                   org.apache.jackrabbit.extractor.PdfTextExtractor,
                   org.apache.jackrabbit.extractor.PlainTextExtractor,
                   org.apache.jackrabbit.extractor.HTMLTextExtractor,
                   org.apache.jackrabbit.extractor.XMLTextExtractor,
                   org.apache.jackrabbit.extractor.RTFTextExtractor,
            org.apache.jackrabbit.extractor.OpenOfficeTextExtractor""/>
        </SearchIndex>
    </Workspace>
    <Versioning rootPath=""${rep.home}/version"">
        <FileSystem class=""org.apache.jackrabbit.core.fs.local.LocalFileSystem"">
            <param name=""path"" value=""${rep.home}/version""/>
        </FileSystem>                
        <!-- <PersistenceManager class=""org.apache.jackrabbit.core.persistence.obj.ObjectPersistenceManager""/> -->
        <PersistenceManager class=""org.apache.jackrabbit.core.persistence.bundle.BundleFsPersistenceManager"">
            <param name=""bundleCacheSize"" value=""8""/> 
            <param name=""blobFSBlockSize"" value=""0""/> 
            <param name=""minBlobSize"" value=""4096""/> 
            <param name=""errorHandling"" value=""""/>             
        </PersistenceManager>            
    </Versioning>
    <SearchIndex class=""org.apache.jackrabbit.core.query.lucene.SearchIndex"">
        <param name=""path"" value=""${rep.home}/index""/>    
        <param name=""textFilterClasses"" value=""
               org.apache.jackrabbit.extractor.MsExcelTextExtractor,
               org.apache.jackrabbit.extractor.MsPowerPointTextExtractor,
               org.apache.jackrabbit.extractor.MsWordTextExtractor,
               org.apache.jackrabbit.extractor.PdfTextExtractor,
               org.apache.jackrabbit.extractor.PlainTextExtractor,
               org.apache.jackrabbit.extractor.HTMLTextExtractor,
               org.apache.jackrabbit.extractor.XMLTextExtractor,
               org.apache.jackrabbit.extractor.RTFTextExtractor,
        org.apache.jackrabbit.extractor.OpenOfficeTextExtractor""/>
    </SearchIndex>    
</Repository>
"
1,"Token authentication parameters are not loaded from JAAS configuration.token based authentication can be disabled and expiration time set in the login module config.
however, this only works with local auth context but  not when using a jaas configuration."
1,"Session.import() failes to resolve propert property definition in some casesSome Properties get assigned the wrong definiton when imported via SysView XML.

The selecteion of the definition failes under the following condition:
The nodetype contains a multi-valued property and a single-valued
residual property.
If the data to be imported than contains only one value for the multivalued property, it will be created with the residual definition.
A later access to this propertie's values will fail with an ValueFormatException.

Example:
Node-Type
 - Property
  - name: myapp:name
  - mulitple: true
 - Property
  - name: *
  - multible: false

Sysview:
<sv:node sv:name=""somenode"">
  <sv:property sv:name=""jcr:primaryType"" sv:type=""Name"">
   <sv:value>myapp:sampleNt</sv:value> 
  </sv:property>
 
  <sv:property sv:name=""myapp:name"" sv:type=""String"">
   <sv:value>At least I could have multi values</sv:value> 
  </sv:property>
</sv:node>

=> The ""mayapp:name"" will be imported into the residule property."
1,"jcr2spi NodeEntryImpl.getPath() blows stack due to getIndex() calling itselfThe jcr2spi NodeEntryImpl class contains logic that causes getIndex() to call itself.

Calling code:

    Session sess = repo.login(creds);
    Node inboxNode = sess.getRootNode().getNode(""Inbox"");
    inboxNode.getPath(); <== blows stack

Tracing reveals:

    1. NodeEntryImpl.getPath() ultimately calls getIndex()
    2. getIndex() calls NodeState.getDefinition()
    3. which calls ItemDefinitionProviderImpl.getQNodeDefinition(...)
    4. which catches a RepositoryException then calls NodeEntryImpl.getWorkspaceId()
    5. which calls NodeEntryImpl.getWorkspaceIndex()
    6. which calls getIndex() (back to step 2, ad infinitum)

Configuration:
    1. A configuration is loaded specifying in-memory persist manager
    2. Config is wrapped in TransientRepository
    3. that's wrapped in spi2jcr's RepositoryService using default BatchReadConfig
    4. a jcr2spi provider is instantiated that directly couples to spi2jcr
    5. Node in question is created as follows:

    Session sess = repo.login(creds);
    sess.getRootNode().addNode(""Inbox"", ""nt:folder"");
    sess.save();

I guess that's about it.
David"
1,"CMS fails to cleanly stop threadsWhen you close IW, it waits for (or aborts and then waits for) all running merges.

However, it's wait criteria is wrong -- it waits for the threads to be done w/ their merges, not for the threads to actually die.

CMS already has a sync() method, to wait for running threads, which we can call from CMS.close.  However it has a thread hazard because a MergeThread removes itself from mergeThreads before it actually exits.  So sync() is able to return even while a merge thread is still running.

This was uncovered by LUCENE-2819 on the test case TestCustomScoreQuery.testCustomExternalQuery, though I expect other test cases would show it."
1,"ArrayIndexOutOfBounds Exception on invalid content-lengthIf the server returns an invalid (not parsable to int) content legnth the method
protected int getResponseContentLength() in HttpMethodBase walks off the
end of the Header[] array and throws the ArrayIndexOutOfBoundsException.

The loop at line 687 in HttpMethodBase.java:

   for (int i = headers.length - 1; i >= 0; i++) {

starts at the end of the array, but uses ++ intead of -- and so walks off the
end of the array on the next line if the header is invalid.  If the header is
valid the return statement in the try block succeeds so there is no error.

The fix is simply to change the line to be

   for (int i = headers.length -1; i>=0; i--) {"
1,"Inconsistencies if ""everyone"" Group is created by User Managementcurrently the 'everyone' principal used to define ACEs that apply for all regular users in the repository is hardcoded in the
principal management. this leads to inconsistencies if a group (or user) is created within the user management that has a principal 
name 'everyone'.


"
1,"Test case failure for testConnTimeout[java] There was 1 failure:
     [java] 1)
testConnTimeout(org.apache.commons.httpclient.TestHttpConnection)junit.framework.AssertionFailedError:
Should have timed out
     [java]     at
org.apache.commons.httpclient.TestHttpConnection.testConnTimeout(TestHttpConnection.java:118)
     [java]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
     [java]     at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
     [java]     at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)

This test has been failing for some time.  It is run with the test-local ant target."
1,SearcherManager misses to close IR if manager is closed during reopenif we close SM while there is a thread calling maybReopen() and swapSearcher throws already closed exception we miss to close the searcher / reader.
1,"Mixins as supertypes do not appear to be queryableWhen creating custom nodetypes that contain mixins as the supertype, nodes of the custom type do not appear to be queryable when using statements of the form: //element(*, mixin). Attached are a relatively simple JUnit test and compact type definition that seem to illustrate the problem."
1,"Evict fixed NodePropBundle from cacheThe BundleDbPersistenceManager only stores back fixed NodePropBundles in checkConsistency() but does not invalidate the cache, which may potentially contain a cached version of a NodePropBundle."
1,"SystemSessions created for GarbageCollector are not logged out ofI have a simple garbage collection task that runs periodically. After upgrading to 1.5.5 it started logging a warning shortly after each run:

2009-05-09 03:44:45,480 WARN [org.apache.jackrabbit.core.SessionImpl] - <Unclosed session detected. The session was opened here: >
java.lang.Exception: Stack Trace
	at org.apache.jackrabbit.core.SessionImpl.<init>(SessionImpl.java:239)
	at org.apache.jackrabbit.core.SystemSession.<init>(SystemSession.java:76)
	at org.apache.jackrabbit.core.SystemSession.create(SystemSession.java:64)
	at org.apache.jackrabbit.core.SessionImpl.createDataStoreGarbageCollector(SessionImpl.java:649)

So it's not my session, but an internally created SystemSession.


Code I'm using:
            getTemplate().execute(new JcrCallback()
            {
                public Object doInJcr(Session session)
                    throws IOException, RepositoryException {
                    SessionImpl sessionImpl = (SessionImpl)session;
                    GarbageCollector gc = sessionImpl.createDataStoreGarbageCollector();
                    gc.scan();
                    gc.stopScan();
                    gc.deleteUnused();
                    return null;
                }
            }, true);
"
1,"spi2dav: Observation's user data not property handledorg.apache.jackrabbit.test.api.observation#GetUserDataTest still fail in the setup jcr2spi - spi2dav(ex) - jcr-server.

"
1,"add workaround for jre breakiterator bugson some inputs, the java breakiterator support will internally crash.

for example: ant test -Dtestcase=TestThaiAnalyzer -Dtestmethod=testRandomStrings -Dtests.seed=-8005471002120855329:-2517344653287596566 -Dtests.multiplier=3"
1,"SimpleSpanFragmenter can create very short fragmentsLine 74 of SimpleSpanFragmenter returns true when the current token is the start of a hit on a span or phrase, thus starting a new fragment. Two problems occur:

- The previous fragment may be very short, but if it contains a hit it will be combined with the new fragment later so this disappears.
- If the token is close to a natural fragment boundary the new fragment will end up very short; possibly even as short as just the span or phrase itself. This is the result of creating a new fragment without incrementing currentNumFrags.

To fix, remove or comment out line 74. The result is that fragments average to the fragment size unless a span or phrase hit is towards the end of the fragment - that fragment is made larger and the following fragment shorter to accommodate the hit."
1,"IndexReader.setNorms is no op if one of the field instances omits normsIf I add two documents to an index w/ same field, and one of them omit norms, then IndexReader.setNorms is no-op. I'll attach a patch w/ test case"
1,"maxFieldLength actual limit is 1 greater than expected value.
// Prepare document.
Document document = new Document();
document.add(new Field(""name"",
            ""pattern oriented software architecture"", Store.NO,
            Index.TOKENIZED, TermVector.WITH_POSITIONS_OFFSETS));

// Set max field length to 2.
indexWriter.setMaxFieldLength(2);

// Add document into index.
indexWriter.addDocument(document, new StandardAnalyzer());

// Create a query.
QueryParser queryParser = new QueryParser(""name"", new StandardAnalyzer());
Query query = queryParser.parse(""software"");

// Search the 3rd term.
Hits hits = indexSearcher.search(query);

Assert.assertEquals(0, hits.length());
// failed. Actual hits.length() == 1, but expect 0."
1,"Trouble undeploying jackrabbit-webapp from TomcatWhen testing jackrabbit-webapp for the 1.4 release, I again came across this issue that I've occasionally seen also before, but never qualified enough for a bug report.

The Jackrabbit webapp would deploy without problems, but when I undeploy the webapp Tomcat fails to remove the Derby jar in WEB-INF/lib (I have unpackWARs enabled). This causes problems especially when I have autoDeploy enabled, as Tomcat then deploys the skeleton webapp right after undeployment, and the only way to really get rid of the webapp is to shutdown Tomcat and to manually remove the webapp on the file system.

I suspect that this problem is related to Derby jar being somehow referenced even after the webapp is undeployed, causing Windows to prevent the jar file from being removed.

Unless someone has some bright idea on how to resolve this, I'll consider this a known issue in Jackrabbit 1.4."
1,"Cluster: Node type register/unregister deadlockA deadlock can occur when two cluster nodes concurrently register or unregister node types.

Reason: 

NodeTypeRegistry.registerNodeTypes is synchronized, and calls eventChannel.registered(ntDefs), which calls AbstractJournal.lockAndSync(), which tries to lock AbstractJournal.rwLock.

On the other hand, AbstractJournal.sync() locks AbstractJournal.rwLock, then calls NodeTypeRecord.process, which calls NodeTypeRegistry.unregisterNodeTypes, which is also synchronized.

Possible solutions: Either 

- NodeTypeRegistry doesn't synchronize on the object when calling a eventChannel method,

- or NodeTypeRegistry locks AbstractJournal.rwLock before synchronizing.

There might be other solutions."
1,"insufficient privilegesHI,
In Jackrabbit  DBStore, On the fly its creating some tables in DB .  But, In our Dev environment we do not have permission for creating tables on the fly. So, I manually inserted all the dll (tables & indexes) before the application start. Although I'm getting the following exception while running application. 

Attached repository.xml.

Below the log.

[11/23/09 11:32:04:405 EST] 0000003a SystemOut     O WARN > org.apache.jackrabbit.core.config.ConfigurationErrorHandler[WebContainer : 3]: Warning parsing the configuration at line 4 using system id file:/usr/local/web/fda/WAS/61x/svdw0047v61fda/installedApps/afda21Network001/osa_registry.ear/osa_registry.war/WEB-INF/cfg/repository.xml: org.xml.sax.SAXParseException: Document root element ""Repository"", must match DOCTYPE root ""null"".
[11/23/09 11:32:04:407 EST] 0000003a SystemOut     O WARN > org.apache.jackrabbit.core.config.ConfigurationErrorHandler[WebContainer : 3]: Warning parsing the configuration at line 4 using system id file:/usr/local/web/fda/WAS/61x/svdw0047v61fda/installedApps/afda21Network001/osa_registry.ear/osa_registry.war/WEB-INF/cfg/repository.xml: org.xml.sax.SAXParseException: Document is invalid: no grammar found.
[11/23/09 11:32:04:977 EST] 0000003a SystemOut     O INFO > org.apache.jackrabbit.core.RepositoryImpl[WebContainer : 3]: Starting repository...
[11/23/09 11:32:05:474 EST] 0000003a SystemOut     O ERROR> org.apache.jackrabbit.core.fs.db.DatabaseFileSystem[WebContainer : 3]: failed to initialize file system
java.sql.SQLException: ORA-01031: insufficient privileges

	at oracle.jdbc.driver.DatabaseError.throwSqlException(DatabaseError.java:112)
	at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:331)
	at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:288)
	at oracle.jdbc.driver.T4C8Oall.receive(T4C8Oall.java:745)
	at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:210)
	at oracle.jdbc.driver.T4CStatement.executeForRows(T4CStatement.java:961)
	at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1190)
	at oracle.jdbc.driver.OracleStatement.executeUpdateInternal(OracleStatement.java:1657)
	at oracle.jdbc.driver.OracleStatement.executeUpdate(OracleStatement.java:1626)
	at org.apache.jackrabbit.core.fs.db.OracleFileSystem.checkSchema(OracleFileSystem.java:211)
	at org.apache.jackrabbit.core.fs.db.DatabaseFileSystem.init(DatabaseFileSystem.java:190)
	at org.apache.jackrabbit.core.fs.db.OracleFileSystem.init(OracleFileSystem.java:137)
	at org.apache.jackrabbit.core.config.RepositoryConfigurationParser$2.getFileSystem(RepositoryConfigurationParser.java:762)
	at org.apache.jackrabbit.core.config.RepositoryConfig.getFileSystem(RepositoryConfig.java:666)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:262)
	at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:621)
	at org.apache.jackrabbit.core.jndi.BindableRepository.createRepository(BindableRepository.java:140)
	at org.apache.jackrabbit.core.jndi.BindableRepository.init(BindableRepository.java:116)
	at org.apache.jackrabbit.core.jndi.BindableRepository.<init>(BindableRepository.java:105)
	at org.apache.jackrabbit.core.jndi.BindableRepositoryFactory.getObjectInstance(BindableRepositoryFactory.java:51)
	at org.apache.jackrabbit.core.jndi.RegistryHelper.registerRepository(RegistryHelper.java:74)
	at com.ssc.soareg.jackrabbit.ContentRepository.<clinit>(ContentRepository.java:71)
	at com.ssc.soareg.jaxr.registry.client.infomodel.ServiceImpl.<init>(ServiceImpl.java:177)
	at com.ssc.soareg.governance.client.SOALifeCycleManagerImpl.saveBusinessServices(SOALifeCycleManagerImpl.java:259)
	at com.ssc.soareg.registry.server.UploadServlet.service(UploadServlet.java:473)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:856)
	at com.ibm.ws.webcontainer.servlet.ServletWrapper.service(ServletWrapper.java:1068)
	at com.ibm.ws.webcontainer.servlet.ServletWrapper.handleRequest(ServletWrapper.java:543)
	at com.ibm.ws.wswebcontainer.servlet.ServletWrapper.handleRequest(ServletWrapper.java:478)
	at com.ibm.ws.webcontainer.webapp.WebApp.handleRequest(WebApp.java:3357)
	at com.ibm.ws.webcontainer.webapp.WebGroup.handleRequest(WebGroup.java:267)
	at com.ibm.ws.webcontainer.WebContainer.handleRequest(WebContainer.java:811)
	at com.ibm.ws.wswebcontainer.WebContainer.handleRequest(WebContainer.java:1455)
	at com.ibm.ws.webcontainer.channel.WCChannelLink.ready(WCChannelLink.java:115)
	at com.ibm.ws.http.channel.inbound.impl.HttpInboundLink.handleDiscrimination(HttpInboundLink.java:454)
	at com.ibm.ws.http.channel.inbound.impl.HttpInboundLink.handleNewInformation(HttpInboundLink.java:383)
	at com.ibm.ws.http.channel.inbound.impl.HttpICLReadCallback.complete(HttpICLReadCallback.java:102)
	at com.ibm.ws.tcp.channel.impl.AioReadCompletionListener.futureCompleted(AioReadCompletionListener.java:165)
	at com.ibm.io.async.AbstractAsyncFuture.invokeCallback(AbstractAsyncFuture.java:217)
	at com.ibm.io.async.AsyncChannelFuture.fireCompletionActions(AsyncChannelFuture.java:161)
	at com.ibm.io.async.AsyncFuture.completed(AsyncFuture.java:136)
	at com.ibm.io.async.ResultHandler.complete(ResultHandler.java:195)
	at com.ibm.io.async.ResultHandler.runEventProcessingLoop(ResultHandler.java:784)
	at com.ibm.io.async.ResultHandler$2.run(ResultHandler.java:873)
	at com.ibm.ws.util.ThreadPool$Worker.run(ThreadPool.java:1473)
"
1,"problems with IR's readerFinishedListenerThere are two major problems:
1. The listener api does not really apply all indexreaders. for example segmentreaders dont fire it on close, only segmentcorereaders. this is wrong, a segmentcorereader is *not* an indexreader. Furthermore, if you register it on a top-level reader you get events for anything under the reader tree (sometimes, unless they are segmentreaders as mentioned above, where it doesnt work correctly at all).
2. Furthermore your listener is 'passed along' in a viral fashion from clone() and reopen(). This means for example, if you are trying to listen to readers in NRT search you are just accumulating reader listeners, all potentially keeping references to old indexreaders (because, in order to deal with #1 your listener must 'keep' a reference to the IR it was registered on, so it can check if thats *really* the one).

We should discuss how to fix #1. 

I will create a patch for #2 shortly and commit it, its just plain wrong.
"
1,"httpClient failed to reconnect after keep-alive connection timed outDescription:

When using httpClient with https tunnelling througha proxy server, after keep-
alive connection timed out on server side.  The httpClient code was unable to 
establish the connection again.

Cause:

The HttpMethodBase.processRequest's retry loop retries the connection without 
going through the ""CONNECT"" request to the proxy server.  Our proxy server 
returns 407 error code.  In case of tunnelling connection, proper reconnect 
should be done by first doing the ""CONNECT"" sequence to get authenticated 
throught the proxy.

Temp fix and Work around:

We implemented some work around to do the retry from the application layer.  In 
order to detect the situation, we have to rely on the error message contained 
in the HttpRecoverableException.  We are checking the text ""Connection aborted 
by peer: socket write error"".  We also have to modify the HttpMethodBase code 
to throw the HttpRecoverableException out to the application."
1,"ConstraintSplitter.getSelectorNames doesn't support FullTextSearch constraintsThe constraint type FullTextSearch is missing in the tested types in org.apache.jackrabbit.core.query.lucene.join.ConstraintSplitter.getSelectorNames(Constraint) method. Submitting a QOM query with a full-text constraint throws a javax.jcr.UnsupportedRepositoryOperationException, while the repository reports supporting such queries : session.getRepository().getDescriptorValue(Repository.QUERY_FULL_TEXT_SEARCH_SUPPORTED).getBoolean() returns TRUE.

Typical stack trace :

javax.jcr.UnsupportedRepositoryOperationException: Unknown constraint type: CONTAINS(r.[jcr:title], 'REGA -APA')
	org.apache.jackrabbit.core.query.lucene.join.ConstraintSplitter.getSelectorNames(ConstraintSplitter.java:177)
	org.apache.jackrabbit.core.query.lucene.join.ConstraintSplitter.getSelectorNames(ConstraintSplitter.java:195)
	org.apache.jackrabbit.core.query.lucene.join.ConstraintSplitter.getSelectorNames(ConstraintSplitter.java:157)
	org.apache.jackrabbit.core.query.lucene.join.ConstraintSplitter.split(ConstraintSplitter.java:106)
	org.apache.jackrabbit.core.query.lucene.join.ConstraintSplitter.split(ConstraintSplitter.java:104)
	org.apache.jackrabbit.core.query.lucene.join.ConstraintSplitter.<init>(ConstraintSplitter.java:80)
	org.apache.jackrabbit.core.query.lucene.join.QueryEngine.execute(QueryEngine.java:162)
	org.apache.jackrabbit.core.query.lucene.join.QueryEngine.execute(QueryEngine.java:147)
	org.apache.jackrabbit.core.query.QueryObjectModelImpl.execute(QueryObjectModelImpl.java:114)"
1,"Startup fails if clustered jackrabbit is upgrade from 1.4.4 to 1.5This is closely related to JCR-1087

The call to checkLocalRevisionSchema() is too late because preapreStatements() already uses the LOCAL_REVISIONS table.

checkLocalRevisionSchema() should be called in checkSchema()"
1,"NullPointerException in CompoundFileReaderHello,

we have got a NullPointerException in the Lucene-class CompoundFileReader:

java.lang.NullPointerException
        at
org.apache.lucene.index.CompoundFileReader.<init>(CompoundFileReader.java:94)
        at org.apache.lucene.index.SegmentReader.<init>(SegmentReader.java:97)
        at org.apache.lucene.index.IndexWriter.mergeSegments(IndexWriter.java:466)
        at
org.apache.lucene.index.IndexWriter.flushRamSegments(IndexWriter.java:426)
        at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:236)

Lucene has been working fine for some days, until this NullPointerException
has occured which has corrupted the complete index.

The reason for this NullPointerException is the following Code 
in Lucenes source file CompoundFileReader.java:

    public CompoundFileReader(Directory dir, String name)
    throws IOException
    {
        boolean success = false;
        ...

        try {
            stream = dir.openFile(name);

            // read the directory and init files
            ...

            success = true;

        } finally {
            if (! success) {
                try {
                    stream.close();
                } catch (IOException e) { }
            }
        }
    }

If the IO-method-call ""dir.openFile()"" throws an IOExeption,
then the variable ""stream"" remains its null value.
The statement ""stream.close()"" in the finally clause will then cause a
NullPointerException.

I would suggest that you change the code from:
    stream.close();
to:
    if ( stream != null ) {
        stream.close();
    }

There are a lot of reasons why an IO-operation like ""dir.openFile()""
could throw an IOException.
I cannot guarantee that such an IO exception will never occur again.
Therefore it is better to handle such an IO exception correctly.

This issue is similar to bug# 29774, except that I recommand an easy way
to solve this problem."
1,"DocViewSAXEventGenerator produces invalid SAX streamISO9075.encode() is called twice in DocViewSAXEventGenerator.leaving(), which produces invalid endElement events.

Faulty block of code (note the encode method called twice):

        // encode node name to make sure it's a valid xml name
        name = ISO9075.encode(name);
        // element name
        String elemName;
        if (node.getDepth() == 0) {
            // root node needs a name
            elemName = jcrRoot;
        } else {
            // encode node name to make sure it's a valid xml name
            elemName = ISO9075.encode(name);
        }"
1,QPropertyDefinitionImpl.equals() is implemented incorrectly 
1,"CachingIndexReader: NullPointerException initializing parents cacheUsing the jackrabbit-core-1.4.9 (after upgrading from jackrabbot-core-1.4.6), the following exception is logged. The code where the exception happens was introduced in JCR-1884 and is first included in the 1.4.9 core release.

10.03.2009 18:56:25 *WARN * CachingIndexReader: Error initializing parents cache. (CachingIndexReader.java, line 310)
java.lang.NullPointerException
    at org.apache.jackrabbit.core.query.lucene.CachingIndexReader$CacheInitializer$2.collect(CachingIndexReader.java:362)
    at org.apache.jackrabbit.core.query.lucene.CachingIndexReader$CacheInitializer.collectTermDocs(CachingIndexReader.java:426)
    at org.apache.jackrabbit.core.query.lucene.CachingIndexReader$CacheInitializer.initializeParents(CachingIndexReader.java:356)
    at org.apache.jackrabbit.core.query.lucene.CachingIndexReader$CacheInitializer.run(CachingIndexReader.java:306)
    at org.apache.jackrabbit.core.query.lucene.CachingIndexReader.<init>(CachingIndexReader.java:109)
    at org.apache.jackrabbit.core.query.lucene.AbstractIndex.getReadOnlyIndexReader(AbstractIndex.java:276)
    at org.apache.jackrabbit.core.query.lucene.MultiIndex.getIndexReader(MultiIndex.java:731)
    at org.apache.jackrabbit.core.query.lucene.MultiIndex.<init>(MultiIndex.java:303)
    at org.apache.jackrabbit.core.query.lucene.SearchIndex.doInit(SearchIndex.java:454)
    at com.day.crx.query.lucene.LuceneHandler.doInit(LuceneHandler.java:93)
    at org.apache.jackrabbit.core.query.AbstractQueryHandler.init(AbstractQueryHandler.java:53)
    at org.apache.jackrabbit.core.SearchManager.initializeQueryHandler(SearchManager.java:583)
    at org.apache.jackrabbit.core.SearchManager.<init>(SearchManager.java:265)
    at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.getSearchManager(RepositoryImpl.java:1600)
    at org.apache.jackrabbit.core.RepositoryImpl.initWorkspace(RepositoryImpl.java:606)
    at org.apache.jackrabbit.core.RepositoryImpl.getWorkspaceInfo(RepositoryImpl.java:718)
    at com.day.crx.core.CRXRepositoryImpl.login(CRXRepositoryImpl.java:964)
    at org.apache.sling.jcr.base.internal.SessionPool.acquireSession(SessionPool.java:268)
    at org.apache.sling.jcr.base.internal.SessionPoolManager.login(SessionPoolManager.java:99)
    at org.apache.sling.jcr.base.AbstractSlingRepository.login(AbstractSlingRepository.java:240)
    at org.apache.sling.jcr.base.AbstractSlingRepository.loginAdministrative(AbstractSlingRepository.java:206)
    at org.apache.sling.jcr.base.AbstractSlingRepository.pingAndCheck(AbstractSlingRepository.java:506)
    at org.apache.sling.jcr.base.AbstractSlingRepository.startRepository(AbstractSlingRepository.java:810)
    at org.apache.sling.jcr.base.AbstractSlingRepository.activate(AbstractSlingRepository.java:629)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.felix.scr.impl.ImmediateComponentManager.createImplementationObject(ImmediateComponentManager.java:226)
    at org.apache.felix.scr.impl.ImmediateComponentManager.createComponent(ImmediateComponentManager.java:133)
    at org.apache.felix.scr.impl.AbstractComponentManager.activateInternal(AbstractComponentManager.java:476)
    at org.apache.felix.scr.impl.AbstractComponentManager.enableInternal(AbstractComponentManager.java:398)
    at org.apache.felix.scr.impl.AbstractComponentManager.access$000(AbstractComponentManager.java:36)
    at org.apache.felix.scr.impl.AbstractComponentManager$1.run(AbstractComponentManager.java:99)
    at org.apache.felix.scr.impl.ComponentActorThread.run(ComponentActorThread.java:85)
10.03.2009 18:56:31 *INFO * SearchIndex: Index initialized: /u01/media/u01/crxlocal/workspaces/dailymail-prod/index Version: 2 (SearchIndex.java, line 492)
"
1,"ArrayIndexOutOfBoundsException when using MultiFieldQueryParserWe get the following exception:

Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: -1
        at java.util.Vector.elementAt(Vector.java:434)
        at org.apache.lucene.queryParser.QueryParser.addClause(QueryParser.java:181)
        at org.apache.lucene.queryParser.QueryParser.Query(QueryParser.java:529)
        at org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:108)
        at org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:87)
        at
org.apache.lucene.queryParser.MultiFieldQueryParser.parse(MultiFieldQueryParser.java:77)
        at idx.Mquery.main(Mquery.java:64)


We are using a query with 'AND' like 'bla AND blo' on 5 fields.
One of the fields has a Tokenizer which returns no token
at all on this query, and this together with the AND
triggers the exception."
1,"If you ""flush by RAM usage"" then IndexWriter may over-mergeI think a good way to maximize performance of Lucene's indexing for a
given amount of RAM is to flush (writer.flush()) the added documents
whenever the RAM usage (writer.ramSizeInBytes()) has crossed the max
RAM you can afford.

But, this can confuse the merge policy and cause over-merging, unless
you set maxBufferedDocs properly.

This is because the merge policy looks at the current maxBufferedDocs
to figure out which segments are level 0 (first flushed) or level 1
(merged from <mergeFactor> level 0 segments).

I'm not sure how to fix this.  Maybe we can look at net size (bytes)
of a segment and ""infer"" level from this?  Still we would have to be
resilient to the application suddenly increasing the RAM allowed.

The good news is to workaround this bug I think you just need to
ensure that your maxBufferedDocs is less than mergeFactor *
typical-number-of-docs-flushed.
"
1,"Contrib RMI: NotSerializableExceptionorg.apache.jackrabbit.rmi.client.RemoteRepositoryException:

error unmarshalling return; nested exception is:.java.io.WriteAbortedException: writing aborted; java.io.NotSerializableException: javax.jcr.NameValue

"
1,"Optimize runs forever if you keep deleting docs at the same timeBecause we ""cascade"" merges for an optimize... if you also delete documents while the merges are running, then the merge policy will see the resulting single segment as still not optimized (since it has pending deletes) and do a single-segment merge, and will repeat indefinitely (as long as your app keeps deleting docs)."
1,"finalize()-methods of FSDirectory.FSIndexInput and FSDirectory.FSIndexOutput try to close already closed fileHi all,

I found a small problem in FSDirectory: The finalize()-methods of FSDirectory.FSIndexInput and FSDirectory.FSIndexOutput try to close the underlying file. This is not a problem unless the file has been closed before by calling the close() method. If it has been closed before, the finalize method throws an IOException saying that the file is already closed. Usually this IOException would go unnoticed, because the GarbageCollector, which calls finalize(), just eats it. However, if I use the Eclipse debugger the execution of my code will always be suspended when this exception is thrown.

Even though this exception probably won't cause problems during normal execution of Lucene, the code becomes cleaner if we apply this small patch. Might this IOException also have a performance impact, if it is thrown very frequently?

I attached the patch which applies cleanly on the current svn HEAD. All testcases pass and I verfied with the Eclipse debugger that the IOException is not longer thrown."
1,"Query for name literal without namespace failsQuery for a name literal without a namespace fails. 

Example:
//*[@foo = 'bla']

should return nodes with foo property that contain the String value 'bla' or the Name value 'bla' (no namespace). Only nodes with String value 'bla' are returned."
1,"Query dump failed with deep query treeWith a big query (more than 400 OR operands) the query dump failed.
The query dump is made at QueryImpl.execute (line 136)

It failed because of the constant PADDING at QueryTreeDump.visit(line 85).
The constant PADDING is a 255 character array, but in my program it would need it to be bigger.
I think putting it to 65535 would not be a problem : it would only take a little bit more memory.

This is the top of the stacktract for info:
java.lang.ArrayIndexOutOfBoundsException
	at java.lang.System.arraycopy(Native Method)
	at java.lang.StringBuffer.append(StringBuffer.java:499)
	at org.apache.jackrabbit.core.query.QueryTreeDump.visit(QueryTreeDump.java:85)
	at org.apache.jackrabbit.core.query.OrQueryNode.accept(OrQueryNode.java:50)
	at org.apache.jackrabbit.core.query.QueryTreeDump.traverse(QueryTreeDump.java:263)
                     ...

This is not critical because I can avoid the dump by unactivating debug logs.
"
1,"ConcurrentModificationException during logoutWe regularly get the following exception:

java.util.ConcurrentModificationException
        at org.apache.commons.collections.map.AbstractReferenceMap$ReferenceEntrySetIterator.checkMod(AbstractReferenceMap.java:761)
        at org.apache.commons.collections.map.AbstractReferenceMap$ReferenceEntrySetIterator.hasNext(AbstractReferenceMap.java:735)
        at java.util.Collections$UnmodifiableCollection$1.hasNext(Collections.java:1009)
        at java.util.Collections$UnmodifiableCollection$1.hasNext(Collections.java:1009)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.dispose(LocalItemStateManager.java:341)
        at org.apache.jackrabbit.core.WorkspaceImpl.dispose(WorkspaceImpl.java:170)
        at org.apache.jackrabbit.core.SessionImpl.logout(SessionImpl.java:1225)
        at org.apache.jackrabbit.core.XASessionImpl.logout(XASessionImpl.java:379)

Two causes for this exception have been identified:

 (Taken from an email to the dev-list from Marcel Reutegger):
> - session A reads some items I
> - session B transiently removes items in I
> - session A logs out and starts to iterate over I in  LocalItemStateManager (LISM)
> - session B saves changes and removed items are evicted from A's LISM
> - session A gets concurrent modification exception

Another scenario is the following:
- Session A gets the iterator of the values of (the primary cache of) an ItemStateReferenceCache in LocalItemStateManager.dispose.
- Session B then does something that triggers the CacheManager.
- The CacheManager then calls resizeAll, and evicts some items from the secondary cache of the ItemStateReferenceCache of which the LocalItemStateManager has a values iterator.
- The garbage collector then runs and evicts the removed items also from the primary cache, which effectively modifies the set over which is iterated.

Regards,

Martijn Hendriks"
1,NodeDefinitionTemplateImpl.setDefaultPrimaryTypeName(null) throws exceptionexpect to clear the name.
1,"handling of expanded-form jcr names by node type *Template classes ItemDefinitionTemplate treats the name as opque string, instead of a JCR Name.

Example: when setting the name to

  ""{http://example.org/}foo""

then getName() needs to return

  ""bar:foo""

which the prefix ""bar"" being mapped to the namesapce ""http://example.org/""."
1,"MultipartPostMethod Holding File Stream Open?From: ""Daniel Walsh"" <daniel.walsh13@verizon.net>
Date: Tue Feb 25, 2003  8:05:49 PM US/Eastern
To: ""Commons HttpClient Project"" <commons-httpclient-dev@jakarta.apache.org>
Subject: MultipartPostMethod Holding File Stream Open?
Reply-To: ""Commons HttpClient Project"" <commons-httpclient-dev@jakarta.apache.org>

I'm using a MultipartPostMethod to upload a file to a servlet:

File file = new File(strUrl);

HttpClient client = new HttpClient();
HostConfiguration hostConfig = new HostConfiguration();
MultipartPostMethod mpPost = new MultipartPostMethod();

 hostConfig.setHost(someURL.getHost(), someURL.getPort(), someURL.getProtocol());
client.setConnectionTimeout(30000);
client.setHostConfiguration(hostConfig);

mpPost.addParameter(""someName"", ""someValue"");
mpPost.addParameter(file.getName(), file);

mpPost.setPath(strPath);
client.executeMethod(mpPost);

String confirmUpload = tpPost.getResponseBodyAsString();
mpPost.releaseConnection();

file.delete();  // this is being blocked.

After the upload, I would like to delete the file off of my disk.  Using other
methods of uploading the file (in particular a PutMethod), I was able to then
delete the file after the upload.  Now that I am using the MultipartPostMethod
obj for the upload, I am unable to delete the file (the return value is false,
and there is no SecurityException being thrown - no SecurityManager even set as
of this point either).

So, I guess my question is whether there is a call to the MultipartPostMethod
obj that I'm overlooking that would release it's connection (I'm sure that it is
opening an InputStream of some sort to read the file contents, in order to form
the HTTP message) to the file - so that I can then have unimpeded access to it
for other operations?"
1,TCK: check for wrong repository descriptor. should be versioning instead of locking... at least according to the comment.
1,"SQL2 Join with OR clause still has some issuesThere are still some issues with Joins that have OR clauses in them. I changed the test, so that it reflects the changes"
1,"SetCookie / DateParser failing to parse non-standard date formatI'm receiving the following expiration date in SetCookie which DateParser 
doesn't handle:

expires=Sat,19-Apr-03 04:28:07 GMT

The lack of a space between ',' and '19' is causing the problem. Is it possible 
to add the following lines to DatePattern?

""EEE,dd-MMM-yy HH:mm:ss z""
""EEE,dd-MMM-yyyy HH:mm:ss z"""
1,"Stale connection check does not work with IBM JSSE/JREOS: Windows/AIX
JRE: IBM JRE 1.4.1
JSSE: IBM's implementation (SSLite?)
HttpClient Library: 2.0.2 release

My code enabled connection pooling feature to gain performance improvement in 
the SSL Handshake area. The code works perfectly on Sun JRE 1.4.2 with a think 
time of 60seconds between requests, but the same code fails on IBM JRE. On IBM 
JRE, the code fails to detech stale connections, thus causing down the stream 
setSoTimeout() call to fail.

Further debugging into the library code revealed difference in the way the 
HTTPConnection.isStale() behaves. With in that method, particularly, the 
inputStream.isAvailable() method returns 0 with Sun JRE but -1 with IBM JRE.

I made a small code change to HttpConnection.isStale() method by moving the try
{}finally{} block outside of the if(inputStream.isAvailable()==0) check in the 
following code and BINGO, everything started working on IBM JVMs. It did not 
break anything on Suns JVM.

============== CODE BEGIN
    protected boolean isStale() {
    	LOG.debug(""##SUBBA## HttpConnection.isStale() got called. soTimeout="" 
+ soTimeout);
        boolean isStale = true;
        if (isOpen) {
        	LOG.debug(""##SUBBA## HttpConnection.isStale() got called. 
isOpen="" + isOpen);        	
            // the connection is open, but now we have to see if we can read it
            // assume the connection is not stale.
            isStale = false;

                try {         
                    if (inputStream.available() == 0) {		// ALWAYS 
RETURNS -1 on IBM JVM  0 on SUN
                    	
		  // try {		// SUBBA  MOVED OUTSIDE IF
	                	socket.setSoTimeout(1);
	                  	LOG.debug(""##SUBBA## HttpConnection.isStale() 
got called. setSoTimeout(1)"");                    	
	                    
	                    inputStream.mark(1);
	                    int byteRead = inputStream.read();
	                	LOG.debug(""##SUBBA## HttpConnection.isStale() 
got called. bytesRead="" + byteRead);                    	
	                    
	                    if (byteRead == -1) {
	                    	LOG.debug(""##SUBBA## HttpConnection.isStale() 
got called. SETTING isStale to TRUE HERE"");                    	
	                    	
	                        // again - if the socket is reporting all data 
read,
	                        // probably stale
	                        isStale = true;
	                    } else {
	                        inputStream.reset();
	                    }
		    // SUBBA  MOVED OUTSIDE IF
                //} finally {
                //	LOG.debug(""##SUBBA## HttpConnection.isStale() got 
called. finally block - BEGIN "" + soTimeout);                    	
                //    socket.setSoTimeout(soTimeout);
                //	LOG.debug(""##SUBBA## HttpConnection.isStale() got 
called. finally block - DONE"");                        
               // }

	
                    }                        
                } finally {
                	LOG.debug(""##SUBBA## HttpConnection.isStale() got 
called. finally block - BEGIN "" + soTimeout);                    	
                    socket.setSoTimeout(soTimeout);
                	LOG.debug(""##SUBBA## HttpConnection.isStale() got 
called. finally block - DONE"");                        
                }
.....
.....
.....
========================== CODE END


I've attached logs captured before and after the change on both the JRE's for 
your review:

==================================
IBMs LOG (after change):
==================================
<Jun 10, 2005 1:26:55 PM EDT> <DEBUG> 
<apache.commons.httpclient.MultiThreadedHttpConnectionManager:700> <Getting 
free connection, hostConfig=HostConfiguration
[host=uatservices30.ilab.fnfismd.com, protocol=https:443, port=443]>
<Jun 10, 2005 1:26:55 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:492> <##SUBBA## 
HttpConnection.isStale() got called. soTimeout=0>
<Jun 10, 2005 1:26:55 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:495> <##SUBBA## 
HttpConnection.isStale() got called. isOpen=true>
<Jun 10, 2005 1:26:55 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:500> <##SUBBA## 
HttpConnection.isStale() got called. [class 
java.io.BufferedInputStream].available=-1>
<Jun 10, 2005 1:26:55 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:523> <##SUBBA## 
HttpConnection.isStale() got called. finally block - BEGIN 0>
<Jun 10, 2005 1:26:55 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:532> <An error occurred while 
reading from the socket, is appears to be stale>
java.net.SocketException: Socket is closed
	at java.net.Socket.setSoTimeout(Socket.java:927)
	at com.ibm.sslite.bf.setSoTimeout(Unknown Source)
	at com.ibm.jsse.bg.setSoTimeout(Unknown Source)
	at org.apache.commons.httpclient.HttpConnection.isStale
(HttpConnection.java:524)
	at org.apache.commons.httpclient.HttpConnection.isOpen
(HttpConnection.java:436)
	at 
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpConnection
Adapter.isOpen(MultiThreadedHttpConnectionManager.java:1122)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:626)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:497)
	at 
com.touchpoint.pia.services.transactions.msp.ApacheHttpClient.invokeRequest
(ApacheHttpClient.java:69)
	at 
com.touchpoint.pia.services.transactions.msp.MsWSManager.invokeRequest
(MsWSManager.java:86)
	at 
com.touchpoint.pia.services.transactions.msp.MsWSManager.invokeRequestWithPaylo
ad(MsWSManager.java:114)
	at com.touchpoint.pia.services.transactions.msp.MsWSManager.main
(MsWSManager.java:179)
<Jun 10, 2005 1:26:55 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:540> <##SUBBA## 
HttpConnection.isStale() return=true>
<Jun 10, 2005 1:26:55 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:437> <Connection is stale, 
closing...>

==================================
IBMs LOG (before change):
==================================
<Jun 10, 2005 1:07:29 PM EDT> <DEBUG> 
<apache.commons.httpclient.MultiThreadedHttpConnectionManager:666> <enter 
HttpConnectionManager.ConnectionPool.getHostPool(HostConfiguration)>
<Jun 10, 2005 1:07:29 PM EDT> <DEBUG> 
<apache.commons.httpclient.MultiThreadedHttpConnectionManager:700> <Getting 
free connection, hostConfig=HostConfiguration
[host=uatservices30.ilab.fnfismd.com, protocol=https:443, port=443]>
<Jun 10, 2005 1:07:29 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:492> <##SUBBA## 
HttpConnection.isStale() got called.>
<Jun 10, 2005 1:07:29 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:495> <##SUBBA## 
HttpConnection.isStale() got called. isOpen=true>
<Jun 10, 2005 1:07:29 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:500> <##SUBBA## 
HttpConnection.isStale() got called. [class 
java.io.BufferedInputStream].available=-1>
<Jun 10, 2005 1:07:29 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:538> <##SUBBA## 
HttpConnection.isStale() return=false>
<Jun 10, 2005 1:07:29 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:599> <HttpConnection.setSoTimeout(0)>
<Jun 10, 2005 1:07:29 PM EDT> <WARN> 
<apache.commons.httpclient.HttpConnection:607> <##SUBBA## Socket Exception>
java.net.SocketException: Socket is closed
	at java.net.Socket.setSoTimeout(Socket.java:927)
	at com.ibm.sslite.bf.setSoTimeout(Unknown Source)
	at com.ibm.jsse.bg.setSoTimeout(Unknown Source)
	at org.apache.commons.httpclient.HttpConnection.setSoTimeout
(HttpConnection.java:603)
	at 
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpConnection
Adapter.setSoTimeout(MultiThreadedHttpConnectionManager.java:1296)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:633)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:497)
	at 
com.touchpoint.pia.services.transactions.msp.ApacheHttpClient.invokeRequest
(ApacheHttpClient.java:69)
	at 
com.touchpoint.pia.services.transactions.msp.MsWSManager.invokeRequest
(MsWSManager.java:86)
	at 
com.touchpoint.pia.services.transactions.msp.MsWSManager.invokeRequestWithPaylo
ad(MsWSManager.java:114)
	at com.touchpoint.pia.services.transactions.msp.MsWSManager.main
(MsWSManager.java:179)
<Jun 10, 2005 1:07:29 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:1151> <enter 
HttpConnection.releaseConnection()>
<Jun 10, 2005 1:07:29 PM EDT> <DEBUG> 
<apache.commons.httpclient.MultiThreadedHttpConnectionManager:513> <enter 
HttpConnectionManager.releaseConnection(HttpConnection)>
<Jun 10, 2005 1:07:29 PM EDT> <DEBUG> 
<apache.commons.httpclient.MultiThreadedHttpConnectionManager:791> <Freeing 
connection, hostConfig=HostConfiguration[host=uatservices30.ilab.fnfismd.com, 
protocol=https:443, port=443]>
<Jun 10, 2005 1:07:29 PM EDT> <DEBUG> 
<apache.commons.httpclient.MultiThreadedHttpConnectionManager:666> <enter 
HttpConnectionManager.ConnectionPool.getHostPool(HostConfiguration)>
<Jun 10, 2005 1:07:29 PM EDT> <DEBUG> 
<apache.commons.httpclient.MultiThreadedHttpConnectionManager:774> <Notifying 
no-one, there are no waiting threads>
java.net.SocketException: Socket is closed
	at java.net.Socket.setSoTimeout(Socket.java:927)
	at com.ibm.sslite.bf.setSoTimeout(Unknown Source)
	at com.ibm.jsse.bg.setSoTimeout(Unknown Source)
	at org.apache.commons.httpclient.HttpConnection.setSoTimeout
(HttpConnection.java:603)
	at 
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpConnection
Adapter.setSoTimeout(MultiThreadedHttpConnectionManager.java:1296)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:633)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:497)
	at 
com.touchpoint.pia.services.transactions.msp.ApacheHttpClient.invokeRequest
(ApacheHttpClient.java:69)
	at 
com.touchpoint.pia.services.transactions.msp.MsWSManager.invokeRequest
(MsWSManager.java:86)
	at 
com.touchpoint.pia.services.transactions.msp.MsWSManager.invokeRequestWithPaylo
ad(MsWSManager.java:114)
	at com.touchpoint.pia.services.transactions.msp.MsWSManager.main
(MsWSManager.java:179)
java.net.SocketException: Socket is closed
	at java.net.Socket.setSoTimeout(Socket.java:927)
	at com.ibm.sslite.bf.setSoTimeout(Unknown Source)
	at com.ibm.jsse.bg.setSoTimeout(Unknown Source)
	at org.apache.commons.httpclient.HttpConnection.setSoTimeout
(HttpConnection.java:603)
	at 
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpConnection
Adapter.setSoTimeout(MultiThreadedHttpConnectionManager.java:1296)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:633)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:497)
	at 
com.touchpoint.pia.services.transactions.msp.ApacheHttpClient.invokeRequest
(ApacheHttpClient.java:69)
	at 
com.touchpoint.pia.services.transactions.msp.MsWSManager.invokeRequest
(MsWSManager.java:86)
	at 
com.touchpoint.pia.services.transactions.msp.MsWSManager.invokeRequestWithPaylo
ad(MsWSManager.java:114)
	at com.touchpoint.pia.services.transactions.msp.MsWSManager.main
(MsWSManager.java:179)
Exception in thread ""main"" java.net.SocketException: Socket is closed
	at java.net.Socket.setSoTimeout(Socket.java:927)
	at com.ibm.sslite.bf.setSoTimeout(Unknown Source)
	at com.ibm.jsse.bg.setSoTimeout(Unknown Source)
	at org.apache.commons.httpclient.HttpConnection.setSoTimeout
(HttpConnection.java:603)
	at 
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpConnection
Adapter.setSoTimeout(MultiThreadedHttpConnectionManager.java:1296)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:633)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:497)
	at 
com.touchpoint.pia.services.transactions.msp.ApacheHttpClient.invokeRequest
(ApacheHttpClient.java:69)
	at 
com.touchpoint.pia.services.transactions.msp.MsWSManager.invokeRequest
(MsWSManager.java:86)
	at 
com.touchpoint.pia.services.transactions.msp.MsWSManager.invokeRequestWithPaylo
ad(MsWSManager.java:114)
	at com.touchpoint.pia.services.transactions.msp.MsWSManager.main
(MsWSManager.java:179)


============================================
**SUNs LOG (after change = before change):
============================================
<Jun 10, 2005 1:25:12 PM EDT> <DEBUG> 
<apache.commons.httpclient.MultiThreadedHttpConnectionManager:700> <Getting 
free connection, hostConfig=HostConfiguration
[host=uatservices30.ilab.fnfismd.com, protocol=https:443, port=443]>
<Jun 10, 2005 1:25:12 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:492> <##SUBBA## 
HttpConnection.isStale() got called. soTimeout=0>
<Jun 10, 2005 1:25:12 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:495> <##SUBBA## 
HttpConnection.isStale() got called. isOpen=true>
<Jun 10, 2005 1:25:12 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:500> <##SUBBA## 
HttpConnection.isStale() got called. [class 
java.io.BufferedInputStream].available=0>
<Jun 10, 2005 1:25:12 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:506> <##SUBBA## 
HttpConnection.isStale() got called. setSoTimeout(1)>
<Jun 10, 2005 1:25:12 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:510> <##SUBBA## 
HttpConnection.isStale() got called. bytesRead=-1>
<Jun 10, 2005 1:25:12 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:513> <##SUBBA## 
HttpConnection.isStale() got called. SETTING isStale to TRUE HERE>
<Jun 10, 2005 1:25:12 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:523> <##SUBBA## 
HttpConnection.isStale() got called. finally block - BEGIN 0>
<Jun 10, 2005 1:25:12 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:532> <An error occurred while 
reading from the socket, is appears to be stale>
java.net.SocketException: Socket Closed
	at java.net.PlainSocketImpl.setOption(PlainSocketImpl.java:177)
	at java.net.Socket.setSoTimeout(Socket.java:924)
	at com.sun.net.ssl.internal.ssl.SSLSocketImpl.setSoTimeout(DashoA12275)
	at org.apache.commons.httpclient.HttpConnection.isStale
(HttpConnection.java:524)
	at org.apache.commons.httpclient.HttpConnection.isOpen
(HttpConnection.java:436)
	at 
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpConnection
Adapter.isOpen(MultiThreadedHttpConnectionManager.java:1122)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:626)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:497)
	at 
com.touchpoint.pia.services.transactions.msp.ApacheHttpClient.invokeRequest
(ApacheHttpClient.java:69)
	at 
com.touchpoint.pia.services.transactions.msp.MsWSManager.invokeRequest
(MsWSManager.java:86)
	at 
com.touchpoint.pia.services.transactions.msp.MsWSManager.invokeRequestWithPaylo
ad(MsWSManager.java:114)
	at com.touchpoint.pia.services.transactions.msp.MsWSManager.main
(MsWSManager.java:179)
<Jun 10, 2005 1:25:12 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:540> <##SUBBA## 
HttpConnection.isStale() return=true>"
1,MemoryFileSystem.deleteFolder deletes all folders starting with this name MemoryFileSystem.deleteFolder deletes not only the folder requested but all folders that start with the given name.
1,"highlighter problems with overlapping tokensThe lucene highlighter has problems when tokens that overlap are generated.

For example, if analysis of iPod generates the tokens ""i"", ""pod"", ""ipod"" (with pod and ipod in the same position),
then the highlighter will output this as iipod, regardless of if any of those tokens are highlighted.

Discovered via http://issues.apache.org/jira/browse/SOLR-24
"
1,"Duplicate attribute in BeanDescriptor and CollectionDescriptor2 different attributes are used in BeanDescriptor and CollectionDescriptor to store the jcr type (jcrType and jcrNodeType). JcrNodeType can be removed. 
This imply modifications in  the DTD, the pm implementation and the different mapping xml files used for the unit tests. 

Furthermore, it should be nice to use the same name (jcrType)  in all descriptors.  There is a lot of confusion across the different descriptors. sometime jcrType is used, sometime jcrNodeType is used. I propose to use only jcrType with the following purpose : 

* In the ClassDescriptor, it  is used to store the primary node type of the class. 
* In the FieldDescriptor, it  is used to store the property type. 
* In the BeanDescriptor  it is used to defined the child node type. 
* In the CollectionDescriptor , it is used to defined the child node type. "
1,"DecompressingEntity not calling close on InputStream retrieved by getContentThe method DecompressingEntity.writeTo(OutputStream outstream) does not close the InputStream retrieved by getContent().
According to the documentation of HttpEntity.writeTo:
IMPORTANT: Please note all entity implementations must ensure that
all allocated resources are properly deallocated when this method
returns.

-> imho this is not satisfied in DecompressingEntity.writeTo "
1,"Deadlock on version operations in a clustered environmentVersion operations in a cluster may end up in a deadlock: a write operation in the version store will acquire the version manager's write lock (N1.VW) and subsequently the cluster journal's write lock (N1.JW). Another cluster node's write operation in some workspace will acquire the journal's write lock (N2.JW) and first process the journal record log: if some of these changes concern the version store, the version manager's read lock (N2.VR) has to be acquired in order to deliver them. If the first cluster node reaches N1.VW, and the second reaches N2.JW, we have a deadlock. The same scenario takes place when the second cluster node synchronizes to the latest journal changes and reaches N2.JR, when the first cluster node is in N1.VW."
1,"InvalidItemStateException when attempting concurrent, non conflicting writesI'm having some problems doing concurrent addition of nodes to a parent node in Jackrabbit.  I've attached a simple test which starts up a bunch of threads which add nodes to a parent node concurrently. If I add in locks I can get this to work, however according to the mailing list this should work without locks. However, the test always fails with this:

javax.jcr.InvalidItemStateException: Item cannot be saved because it has been modified externally: node /testParent
	at org.apache.jackrabbit.core.ItemImpl.getTransientStates(ItemImpl.java:281)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:939)
	at org.mule.galaxy.impl.JackrabbitConcurrentWriteTest$1.run(JackrabbitConcurrentWriteTest.java:71)

I'm using Jackrabbit 1.6.1. Here is my (verbose) node type:

  <nodeType name=""galaxy:noSiblings"" 
    isMixin=""false"" 
    hasOrderableChildNodes=""false""
    primaryItemName="""">
    <propertyDefinition name=""*"" requiredType=""undefined"" onParentVersion=""COPY"" />
    <propertyDefinition name=""*"" requiredType=""undefined"" onParentVersion=""COPY"" multiple=""true""/>
    <childNodeDefinition name=""*"" defaultPrimaryType=""nt:unstructured"" onParentVersion=""COPY"" sameNameSiblings=""false"" />
    <supertypes>
        <supertype>nt:base</supertype>
        <supertype>mix:referenceable</supertype>
        <supertype>mix:lockable</supertype>
    </supertypes>
  </nodeType>

And my test:    

package org.mule.galaxy.impl;

import java.io.File;
import java.io.IOException;
import java.io.InputStream;
import java.util.ArrayList;
import java.util.List;
import java.util.UUID;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.TimeUnit;

import javax.jcr.LoginException;
import javax.jcr.Node;
import javax.jcr.Repository;
import javax.jcr.RepositoryException;
import javax.jcr.Session;
import javax.jcr.SimpleCredentials;

import junit.framework.TestCase;

import org.apache.commons.io.FileUtils;
import org.apache.jackrabbit.api.JackrabbitNodeTypeManager;
import org.apache.jackrabbit.core.RepositoryImpl;
import org.apache.jackrabbit.core.TransientRepository;
import org.apache.jackrabbit.core.config.RepositoryConfig;

public class JackrabbitConcurrentWriteTest extends TestCase {
    
    private Repository repository;
    private Session session;
    private String parentUUID;
    private boolean continueLoop = true;
    
    public void setUp() throws Exception {
        FileUtils.deleteDirectory(new File(""repository""));
        File repoDir = new File(""repository"");
        repoDir.mkdirs();
        RepositoryConfig config = RepositoryConfig.create(new File(""src/test/resources/META-INF/jackrabbit-repo-test.xml""), repoDir);
        repository = RepositoryImpl.create(config);
        session = createSession();
        
        createCustomNodeTypes(session);
        
        parentUUID = session.getRootNode().addNode(""testParent"", ""galaxy:noSiblings"").getUUID();
        session.save();
        session.logout();
    }

    private Session createSession() throws LoginException, RepositoryException {
        return repository.login(new SimpleCredentials(""username"", ""password"".toCharArray()));
    }
    
    public void testConcurrency() throws Exception {
        final List<Exception> exceptions = new ArrayList<Exception>();
        int threadCount = 20;
        final CountDownLatch latch = new CountDownLatch(threadCount);
        
        for (int i = 0; i < threadCount; i++) {
            Thread thread = new Thread() {

                @Override
                public void run() {
                    try {
                        while (continueLoop) {
                            Session session = createSession();
                            try {
                                Node node = session.getNodeByUUID(parentUUID);
                                node.addNode(UUID.randomUUID().toString());
                                node.save();
                                session.save();
                            } finally {
                                session.logout();
                            }   
                        }
                    } catch (RepositoryException e) {
                        exceptions.add(e);
                        continueLoop = false;
                    }
                    latch.countDown();
                }
                
            };
            thread.start();
        }
        
        latch.await(10, TimeUnit.SECONDS);
        continueLoop = false;
        
        for (Exception e : exceptions) {
            e.printStackTrace();
        }
        assertEquals(0, exceptions.size());
    }
    
    public void createCustomNodeTypes(Session session) throws RepositoryException, IOException {
        // Get the JackrabbitNodeTypeManager from the Workspace.
        // Note that it must be cast from the generic JCR NodeTypeManager to
        // the Jackrabbit-specific implementation.
        // (see: http://jackrabbit.apache.org/node-types.html)
        JackrabbitNodeTypeManager manager = (JackrabbitNodeTypeManager) session.getWorkspace().getNodeTypeManager();

        // Register the custom node types defined in the CND file
        InputStream is = Thread.currentThread().getContextClassLoader()
                .getResourceAsStream(""org/mule/galaxy/impl/jcr/nodeTypes.xml"");
        manager.registerNodeTypes(is, JackrabbitNodeTypeManager.TEXT_XML);
    }
}"
1,"WebDAV/DaveX Servlets susceptible to CSRF AttacksBoth the WebDAV and the remoting (DaveX) servlets are susceptible to CSRF attacks.

"
1,"ALLOW_CIRCULAR_REDIRECTS has no effect if references include query stringALLOW_CIRCULAR_REDIRECTS parameter in HttpClientParameters has no effect if
circular reference contains in URL parameters."
1,"RamUsageEstimator.NUM_BYTES_ARRAY_HEADER and other constants are incorrectRamUsageEstimator.NUM_BYTES_ARRAY_HEADER is computed like that: NUM_BYTES_OBJECT_HEADER + NUM_BYTES_INT + NUM_BYTES_OBJECT_REF. The NUM_BYTES_OBJECT_REF part should not be included, at least not according to this page: http://www.javamex.com/tutorials/memory/array_memory_usage.shtml

{quote}
A single-dimension array is a single object. As expected, the array has the usual object header. However, this object head is 12 bytes to accommodate a four-byte array length. Then comes the actual array data which, as you might expect, consists of the number of elements multiplied by the number of bytes required for one element, depending on its type. The memory usage for one element is 4 bytes for an object reference ...
{quote}

While on it, I wrote a sizeOf(String) impl, and I wonder how do people feel about including such helper methods in RUE, as static, stateless, methods? It's not perfect, there's some room for improvement I'm sure, here it is:

{code}
	/**
	 * Computes the approximate size of a String object. Note that if this object
	 * is also referenced by another object, you should add
	 * {@link RamUsageEstimator#NUM_BYTES_OBJECT_REF} to the result of this
	 * method.
	 */
	public static int sizeOf(String str) {
		return 2 * str.length() + 6 // chars + additional safeness for arrays alignment
				+ 3 * RamUsageEstimator.NUM_BYTES_INT // String maintains 3 integers
				+ RamUsageEstimator.NUM_BYTES_ARRAY_HEADER // char[] array
				+ RamUsageEstimator.NUM_BYTES_OBJECT_HEADER; // String object
	}
{code}

If people are not against it, I'd like to also add sizeOf(int[] / byte[] / long[] / double[] ... and String[])."
1,"Do not increment revison while target workspace is not initializedIt can happen that a workspace is not initialized at all during a syncronize should be performed.
This can happen when a cluster member performs a re-index of a workspace. The changelog should not be ignored it
should be processed after the workspace is initailized."
1,"NPE in TestNRTThreadsI hit this when while(1)ing this test... I think it's because the logic on when we ask the SegmentReader to load stored fields is off...

{noformat}
*** Thread: Lucene Merge Thread #1 ***
org.apache.lucene.index.MergePolicy$MergeException:
java.lang.NullPointerException
       at org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:507)
       at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:472)
Caused by: java.lang.NullPointerException
       at org.apache.lucene.index.SegmentReader$FieldsReaderLocal.initialValue(SegmentReader.java:245)
       at org.apache.lucene.index.SegmentReader$FieldsReaderLocal.initialValue(SegmentReader.java:242)
       at org.apache.lucene.util.CloseableThreadLocal.get(CloseableThreadLocal.java:68)
       at org.apache.lucene.index.SegmentReader.getFieldsReader(SegmentReader.java:749)
       at org.apache.lucene.index.SegmentReader.document(SegmentReader.java:838)
       at org.apache.lucene.index.IndexReader.document(IndexReader.java:951)
       at org.apache.lucene.index.TestNRTThreads$1.warm(TestNRTThreads.java:86)
       at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3311)
       at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:2875)
       at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:379)
       at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:447)
{noformat}
"
1,"Lazy Field Loading has edge case bug causing read past EOFWhile trying to run some benchmarking of Lazy filed loading, i discovered there seems to be an edge case when accessing the last field of the last doc of an index.

the problem seems to only happen when the doc has been accessed after at least one other doc.

i have not tried to dig into the code to find the root cause, testcase to follow..."
1,"BQ provides an explanation on a non-matchPlug in seed -6336594106867842617L into TestExplanations then run TestSimpleExplanationsOfNonMatches and you'll hit this:
{noformat}
    [junit] Testsuite: org.apache.lucene.search.TestSimpleExplanationsOfNonMatches
    [junit] Testcase: testBQ2(org.apache.lucene.search.TestSimpleExplanationsOfNonMatches):	FAILED
    [junit] Explanation of [[+yy +w3]] for #0 doesn't indicate non-match: 0.08778467 = (MATCH) product of:
    [junit]   0.17556934 = (MATCH) sum of:
    [junit]     0.17556934 = (MATCH) weight(field:w3 in 0), product of:
    [junit]       0.5165708 = queryWeight(field:w3), product of:
    [junit]         0.7768564 = idf(docFreq=4, maxDocs=4)
    [junit]         0.6649502 = queryNorm
    [junit]       0.33987468 = (MATCH) fieldWeight(field:w3 in 0), product of:
    [junit]         1.0 = tf(termFreq(field:w3)=1)
    [junit]         0.7768564 = idf(docFreq=4, maxDocs=4)
    [junit]         0.4375 = fieldNorm(field=field, doc=0)
    [junit]   0.5 = coord(1/2)
    [junit]  expected:<0.0> but was:<0.08778467>
    [junit] junit.framework.AssertionFailedError: Explanation of [[+yy +w3]] for #0 doesn't indicate non-match: 0.08778467 = (MATCH) product of:
    [junit]   0.17556934 = (MATCH) sum of:
    [junit]     0.17556934 = (MATCH) weight(field:w3 in 0), product of:
    [junit]       0.5165708 = queryWeight(field:w3), product of:
    [junit]         0.7768564 = idf(docFreq=4, maxDocs=4)
    [junit]         0.6649502 = queryNorm
    [junit]       0.33987468 = (MATCH) fieldWeight(field:w3 in 0), product of:
    [junit]         1.0 = tf(termFreq(field:w3)=1)
    [junit]         0.7768564 = idf(docFreq=4, maxDocs=4)
    [junit]         0.4375 = fieldNorm(field=field, doc=0)
    [junit]   0.5 = coord(1/2)
    [junit]  expected:<0.0> but was:<0.08778467>
    [junit] 	at org.apache.lucene.search.CheckHits.checkNoMatchExplanations(CheckHits.java:60)
    [junit] 	at org.apache.lucene.search.TestSimpleExplanationsOfNonMatches.qtest(TestSimpleExplanationsOfNonMatches.java:36)
    [junit] 	at org.apache.lucene.search.TestExplanations.qtest(TestExplanations.java:101)
    [junit] 	at org.apache.lucene.search.TestSimpleExplanations.testBQ2(TestSimpleExplanations.java:235)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:397)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.run(LuceneTestCase.java:389)
{noformat}

The bug is real -- BQ's explain method fails to properly enforce required clauses when the sub-scorer is null.  Thank you random testing!
"
1,"ArrayHits does not end properly when skipTo doesn't find documentIf skipTo(target) does not find a document that that has a higher value than the target, it falls out of the loop and calls next() possibly returning a previously found document. The patch makes sure that -1 is returned in this case, otherwise confusing results might occur.

Index: src/main/java/org/apache/jackrabbit/core/query/lucene/hits/ArrayHits.java
===================================================================
--- src/main/java/org/apache/jackrabbit/core/query/lucene/hits/ArrayHits.java	(revision 608900)
+++ src/main/java/org/apache/jackrabbit/core/query/lucene/hits/ArrayHits.java	(working copy)
@@ -87,9 +87,9 @@
             int nextDocValue = hits[i];
             if (nextDocValue >= target) {
                 index = i;
-                break;
+                return next();
             }
         }
-        return next();
+        return -1;
     }
 }
"
1,"Node.addNode(String, String) doesn't prevent use of mixin types as primary type"
1,"The class from cotrub directory org.apache.lucene.index.IndexSplitter creates a non correct indexWhen using the method IndexSplitter.split(File destDir, String[] segs) from the Lucene cotrib directory (contrib/misc/src/java/org/apache/lucene/index) it creates an index with segments descriptor file with wrong data. Namely wrong is the number representing the name of segment that would be created next in this index.
If some of the segments of the index already has this name this results either to impossibility to create new segment or in crating of an corrupted segment."
1,"registration of new namespace does not respect existing session mappingsconsider the following (starting with a default namespace registry):

// remap nt namespace
Session.setNamespacePrefix(""foobar"", ""http://www.jcp.org/jcr/nt/1.0"");

// create new namespace
NamespaceRegistry.registerNamespace(""foobar"", ""http://www.foo.org/bar/1.0"");

now the session used above that remapped the nt namespace has an ambigous namespace mapping:
foobar --> ""http://www.jcp.org/jcr/nt/1.0""
""http://www.jcp.org/jcr/nt/1.0"" --> foobar
""http://www.foo.org/bar/1.0"" --> foobar

i.e. the new foobar namespace is hidden for this session. either the registration should not work, or an automatic prefix is to be defined in all local session mappings.

"
1,"BooleanScorer2 does not compile with ecjBooleanScorer2, derived from scorer, has two inner classes both derived, ultimately, from Scorer.
As such they all define doc() or inherit it.
ecj produces an error when doc() is called from score in the inner classes in the methods
        countingDisjunctionSumScorer
    and
        countingConjunctionSumScorer

The error message is:
    The method doc is defined in an inherited type and in an enclosing scope.

The affected lines are: 160, 161, 178, and 179


I have run the junit test TestBoolean2 (as well as all the others) with
        doc()
    changed to
        BooleanScorer2.this.doc()
    and also to:
        this.doc();
The result was that the tests passed for both.

I added debug statements to all the doc methods and the score methods in the affected classes, but I could not determine what it should be.
"
1,3.1 fileformats out of dateThe 3.1 fileformats is missing the change from LUCENE-2811
1,"session.exportDocumentView() does not work with jaxb 2.1.x  UnmarshallerHandlerI tried to update my project from Jackrabbit 1.4 to 1.5 and found following error, that is critical for my app.
Project uses Import/Export features of JCR and JAXB to map XML from JCR to java objects.

exportDocumentView() works with streams when I call it like this:

              Unmarshaller umr = getUnmarshaller();
        ...
                fo = new FileOutputStream(""/tmp/export-node.xml"");
                jcrs.exportDocumentView(path,fo , false, false);
                fi = new FileInputStream(""/tmp/export-node.xml"");
                umr.unmarshal(new InputSource(fi));    

But it does not work when I call it using SAX event handler:

            UnmarshallerHandler ctxh = umr.getUnmarshallerHandler();
             jcrs.exportDocumentView(path, ctxh, false, false);

giving following exception:

java.lang.NullPointerException
        at org.xml.sax.helpers.AttributesImpl.getIndex(AttributesImpl.java:203)
        at com.sun.xml.bind.v2.runtime.unmarshaller.InterningXmlVisitor$AttributesImpl.getIndex(InterningXmlVisitor.java:112)
        at com.sun.xml.bind.v2.runtime.unmarshaller.XsiNilLoader.selectLoader(XsiNilLoader.java:62)
        at com.sun.xml.bind.v2.runtime.unmarshaller.ProxyLoader.startElement(ProxyLoader.java:53)
        at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallingContext._startElement(UnmarshallingContext.java:449)
        at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallingContext.startElement(UnmarshallingContext.java:427)
        at com.sun.xml.bind.v2.runtime.unmarshaller.InterningXmlVisitor.startElement(InterningXmlVisitor.java:71)
        at com.sun.xml.bind.v2.runtime.unmarshaller.SAXConnector.startElement(SAXConnector.java:137)
        at org.apache.jackrabbit.commons.xml.Exporter.startElement(Exporter.java:438)
        at org.apache.jackrabbit.commons.xml.DocumentViewExporter.exportNode(DocumentViewExporter.java:76)
        at org.apache.jackrabbit.commons.xml.Exporter.exportNode(Exporter.java:298)
        at org.apache.jackrabbit.commons.xml.Exporter.exportNodes(Exporter.java:214)
        at org.apache.jackrabbit.commons.xml.DocumentViewExporter.exportNode(DocumentViewExporter.java:77)
        at org.apache.jackrabbit.commons.xml.Exporter.exportNode(Exporter.java:298)
        at org.apache.jackrabbit.commons.xml.Exporter.exportNodes(Exporter.java:214)
        at org.apache.jackrabbit.commons.xml.DocumentViewExporter.exportNode(DocumentViewExporter.java:77)
        at org.apache.jackrabbit.commons.xml.Exporter.exportNode(Exporter.java:298)
        at org.apache.jackrabbit.commons.xml.Exporter.export(Exporter.java:144)
        at org.apache.jackrabbit.commons.AbstractSession.export(AbstractSession.java:461)
        at org.apache.jackrabbit.commons.AbstractSession.exportDocumentView(AbstractSession.java:241)
        at ua.org.dg.semaril.helpers.AbstractTypeResolver.getContent(AbstractTypeResolver.java:31

Version 1.4. works fine.

Jukka, please check your changes to  org.apache.jackrabbit.commons.xml.Exporter.
"
1,"EdgeNGramTokenFilter stops on tokens smaller then minimum gram size.If a token is encountered in the stream that is shorter in length than the min gram size, the filter will stop processing the token stream.

Working up a unit test now, but may be a few days before I can provide it. Wanted to get it in the system."
1,"Removal of first version throws javax.jcr.ReferentialIntegrityExceptionA ReferentialIntegrityException occurs when I delete the first version succeeding the root version. Deleting other versions works fine. Here is the stack:

javax.jcr.ReferentialIntegrityException: Unable to remove version. At least once referenced.
        at org.apache.jackrabbit.core.version.InternalVersionHistoryImpl.removeVersion(InternalVersionHistoryImpl.java:379)
        at org.apache.jackrabbit.core.version.InternalVersionManagerBase.internalRemoveVersion(InternalVersionManagerBase.java:684)
        at org.apache.jackrabbit.core.version.InternalVersionManagerImpl$5.run(InternalVersionManagerImpl.java:495)
        at org.apache.jackrabbit.core.version.InternalVersionManagerImpl$DynamicESCFactory.doSourced(InternalVersionManagerImpl.java:760)
        at org.apache.jackrabbit.core.version.InternalVersionManagerImpl.removeVersion(InternalVersionManagerImpl.java:493)
        at org.apache.jackrabbit.core.version.InternalXAVersionManager.removeVersion(InternalXAVersionManager.java:264)
        at org.apache.jackrabbit.core.version.VersionHistoryImpl.removeVersion(VersionHistoryImpl.java:253)

The code is simple:

VersionHistory vh = session.getWorkspace().getVersionManager().getVersionHistory(path);
vh.removeVersion(version); // where version is the first version succeeding the root version

"
1,"XMLTextExtractor returns an empty reader when encoding is unsupportedXMLTextExtractor is failing to index xml files.  Searching for content in xml files is not coming back with results.

On the extractText(InputStream stream, String type, String encoding) method, the encoding is coming in as an empty string, and it throws an exception at line 62 (reader.parse(source)).

modifying the following statement fixes the problem:
before:  if (encoding != null) {
after:  if (encoding != null && !encoding.equals("""")) {"
1,occasional MergeException while indexingTestStressIndexing2.testMultiConfig occasionally hits merge exceptions
1,"Deadlock: IndexWriter.addIndexes(IndexReader[])A deadlock issue occurs under the following circumstances
- IndexWriter.autoCommit == true
- IndexWriter.directory contains multiple segments
- IndexWriter.AddIndex(IndexReader[]) is invoked

I put together a JUnit test that recreates the deadlock, which I've attached.  It is the first test method, 'testAddIndexByIndexReader()'.

In a nutshell, here is what happens:

        // 1) AddIndexes(IndexReader[]) acquires the write lock,
        // then begins optimization of destination index (this is
        // prior to adding any external segments).
        //
        // 2) Main thread starts a ConcurrentMergeScheduler.MergeThread
        // to merge the 2 segments.
        //
        // 3) Merging thread tries to acquire the read lock at
        // IndexWriter.blockAddIndexes(boolean) in
        // IndexWriter.StartCommit(), but cannot as...
        //
        // 4) Main thread still holds the write lock, and is
        // waiting for the IndexWriter.runningMerges data structure
        // to be devoid of merges with their optimize flag
        // set (IndexWriter.optimizeMergesPending()).
"
1,"SegmentTermEnum.next() doesn't maintain prevBuffer at endWhen you're iterating a SegmentTermEnum and you go past the end of the docs, you end up with a state where the nextBuffer = null and the prevBuffer is the penultimate term, not the last term.  This patch fixes it.  (It's also required for my Prefetching bug [LUCENE-506])

Index: java/org/apache/lucene/index/SegmentTermEnum.java
===================================================================
--- java/org/apache/lucene/index/SegmentTermEnum.java	(revision 382121)
+++ java/org/apache/lucene/index/SegmentTermEnum.java	(working copy)
@@ -109,6 +109,7 @@
   /** Increments the enumeration to the next element.  True if one exists.*/
   public final boolean next() throws IOException {
     if (position++ >= size - 1) {
+      prevBuffer.set(termBuffer);
       termBuffer.reset();
       return false;
     }
"
1,"ClusterNode not properly shutdown when repository has shutdownSometimes when the repository is shutdown the ClusterNode is not shutdown and it therefore tries to update records or access a closed Journal file.  The setup that generated the exception is I have 3 VMs each with a Repository that are all connected to the same database.  In the below stack trace one of the repositories is being shutdown however the ClusterNode thread is also trying to update the repository journal at the same time.  Below is a copy of the stack trace.

[4/23/08 9:58:52:496 CDT] 00000061 SystemOut     O 89811653 [WebContainer : 2] INFO  org.apache.jackrabbit.core.RepositoryImpl  - Shutting down repository...
[4/23/08 9:58:52:511 CDT] 0000054c SystemOut     O 89811621 [ClusterNode-b06e4fe7-a602-4a93-b106-e0834046ae0f] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7174
[4/23/08 9:58:52:527 CDT] 00000061 SystemOut     O 89811684 [WebContainer : 2] INFO  org.apache.jackrabbit.core.RepositoryImpl  - shutting down workspace 'default'...
[4/23/08 9:58:52:574 CDT] 00000061 SystemOut     O 89811715 [WebContainer : 2] INFO  org.apache.jackrabbit.core.observation.ObservationDispatcher  - Notification of EventListeners stopped.
[4/23/08 9:58:53:058 CDT] 00000061 SystemOut     O 89812215 [WebContainer : 2] INFO  org.apache.jackrabbit.core.RepositoryImpl  - workspace 'default' has been shutdown
[4/23/08 9:58:53:308 CDT] 00000308 SystemOut     O 91641048 [ClusterNode-e609e8a6-320e-44ea-be0f-ab8c5cb89662] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7165
[4/23/08 9:58:53:324 CDT] 00000308 SystemOut     O 91641064 [ClusterNode-e609e8a6-320e-44ea-be0f-ab8c5cb89662] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7166
[4/23/08 9:58:53:324 CDT] 00000308 SystemOut     O 91641064 [ClusterNode-e609e8a6-320e-44ea-be0f-ab8c5cb89662] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7167
[4/23/08 9:58:53:339 CDT] 00000308 SystemOut     O 91641079 [ClusterNode-e609e8a6-320e-44ea-be0f-ab8c5cb89662] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7168
[4/23/08 9:58:53:339 CDT] 00000308 SystemOut     O 91641079 [ClusterNode-e609e8a6-320e-44ea-be0f-ab8c5cb89662] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7169
[4/23/08 9:58:53:355 CDT] 00000308 SystemOut     O 91641095 [ClusterNode-e609e8a6-320e-44ea-be0f-ab8c5cb89662] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7170
[4/23/08 9:58:53:371 CDT] 00000308 SystemOut     O 91641111 [ClusterNode-e609e8a6-320e-44ea-be0f-ab8c5cb89662] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7171
[4/23/08 9:58:53:386 CDT] 00000308 SystemOut     O 91641126 [ClusterNode-e609e8a6-320e-44ea-be0f-ab8c5cb89662] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7172
[4/23/08 9:58:53:417 CDT] 00000308 SystemOut     O 91641157 [ClusterNode-e609e8a6-320e-44ea-be0f-ab8c5cb89662] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7173
[4/23/08 9:58:53:433 CDT] 00000308 SystemOut     O 91641173 [ClusterNode-e609e8a6-320e-44ea-be0f-ab8c5cb89662] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7174
[4/23/08 9:58:53:433 CDT] 00000308 SystemOut     O 91641173 [ClusterNode-e609e8a6-320e-44ea-be0f-ab8c5cb89662] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7175
[4/23/08 9:58:53:496 CDT] 00000308 SystemOut     O 91641236 [ClusterNode-e609e8a6-320e-44ea-be0f-ab8c5cb89662] INFO  org.apache.jackrabbit.core.journal.AbstractJournal  - Synchronized to revision: 7175
[4/23/08 9:58:54:292 CDT] 00000131 SystemOut     O 89171473 [ClusterNode-4930503b-ab33-4444-999e-c87fb3681bf7] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7173
[4/23/08 9:58:54:308 CDT] 00000131 SystemOut     O 89171504 [ClusterNode-4930503b-ab33-4444-999e-c87fb3681bf7] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7174
[4/23/08 9:58:54:308 CDT] 00000131 SystemOut     O 89171504 [ClusterNode-4930503b-ab33-4444-999e-c87fb3681bf7] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7175
[4/23/08 9:58:54:386 CDT] 00000131 SystemOut     O 89171582 [ClusterNode-4930503b-ab33-4444-999e-c87fb3681bf7] INFO  org.apache.jackrabbit.core.journal.AbstractJournal  - Synchronized to revision: 7175
[4/23/08 9:58:55:417 CDT] 00000061 SystemOut     O 89814574 [WebContainer : 2] INFO  org.apache.jackrabbit.core.RepositoryImpl  - Repository has been shutdown
[4/23/08 9:58:56:089 CDT] 0000054c SystemOut     O 89815199 [ClusterNode-b06e4fe7-a602-4a93-b106-e0834046ae0f] ERROR org.apache.jackrabbit.core.cluster.ClusterNode  - Unable to read revision '7174'.
org.apache.jackrabbit.core.journal.JournalException: I/O error while reading string.
	at org.apache.jackrabbit.core.journal.ReadRecord.readString(ReadRecord.java:169)
	at org.apache.jackrabbit.core.cluster.ClusterNode.consume(ClusterNode.java:979)
	at org.apache.jackrabbit.core.journal.AbstractJournal.doSync(AbstractJournal.java:198)
	at org.apache.jackrabbit.core.journal.AbstractJournal.sync(AbstractJournal.java:173)
	at org.apache.jackrabbit.core.cluster.ClusterNode.sync(ClusterNode.java:303)
	at org.apache.jackrabbit.core.cluster.ClusterNode.run(ClusterNode.java:274)
	at java.lang.Thread.run(Thread.java:797)
Caused by: 
java.io.IOException: Closed Connection
	at oracle.jdbc.driver.DatabaseError.SQLToIOException(DatabaseError.java:517)
	at oracle.jdbc.driver.OracleBlobInputStream.needBytes(OracleBlobInputStream.java:187)
	at oracle.jdbc.driver.OracleBufferedStream.readInternal(OracleBufferedStream.java:130)
	at oracle.jdbc.driver.OracleBufferedStream.read(OracleBufferedStream.java:108)
	at java.io.DataInputStream.readBoolean(DataInputStream.java:246)
	at org.apache.jackrabbit.core.journal.ReadRecord.readString(ReadRecord.java:161)
	... 6 more
[4/23/08 9:58:56:261 CDT] 0000054c SystemOut     O 89815355 [ClusterNode-b06e4fe7-a602-4a93-b106-e0834046ae0f] ERROR org.apache.jackrabbit.core.journal.DatabaseJournal  - Error while moving to next record.
java.sql.SQLException: Closed Connection: next
	at oracle.jdbc.driver.DatabaseError.throwSqlException(DatabaseError.java:112)
	at oracle.jdbc.driver.DatabaseError.throwSqlException(DatabaseError.java:146)
	at oracle.jdbc.driver.OracleResultSetImpl.next(OracleResultSetImpl.java:181)
	at org.apache.jackrabbit.core.journal.DatabaseRecordIterator.fetchRecord(DatabaseRecordIterator.java:136)
	at org.apache.jackrabbit.core.journal.DatabaseRecordIterator.hasNext(DatabaseRecordIterator.java:85)
	at org.apache.jackrabbit.core.journal.AbstractJournal.doSync(AbstractJournal.java:190)
	at org.apache.jackrabbit.core.journal.AbstractJournal.sync(AbstractJournal.java:173)
	at org.apache.jackrabbit.core.cluster.ClusterNode.sync(ClusterNode.java:303)
	at org.apache.jackrabbit.core.cluster.ClusterNode.run(ClusterNode.java:274)
	at java.lang.Thread.run(Thread.java:797)
[4/23/08 9:58:56:402 CDT] 0000054c SystemOut     O 89815418 [ClusterNode-b06e4fe7-a602-4a93-b106-e0834046ae0f] WARN  org.apache.jackrabbit.core.cluster.ClusterNode  - Unable to set current revision to 7174.
org.apache.jackrabbit.core.journal.JournalException: Revision file closed.
	at org.apache.jackrabbit.core.journal.FileRevision.set(FileRevision.java:100)
	at org.apache.jackrabbit.core.cluster.ClusterNode.setRevision(ClusterNode.java:1073)
	at org.apache.jackrabbit.core.journal.AbstractJournal.doSync(AbstractJournal.java:211)
	at org.apache.jackrabbit.core.journal.AbstractJournal.sync(AbstractJournal.java:173)
	at org.apache.jackrabbit.core.cluster.ClusterNode.sync(ClusterNode.java:303)
	at org.apache.jackrabbit.core.cluster.ClusterNode.run(ClusterNode.java:274)
	at java.lang.Thread.run(Thread.java:797)

"
1,"DirectoryTaxonomyWriter can lose the INDEX_CREATE_TIME property, causing DirTaxoReader.refresh() to falsely succeed (or fail)DirTaxoWriter sets createTime to null after it put it in the commit data once. But that's wrong because if one calls commit(Map<>) twice, the second time doesn't record the creation time. Also, in the ctor, if an index exists and OpenMode is not CREATE, the creation time property is not read.

I wrote a couple of unit tests that assert this, and modified DirTaxoWriter to always record the creation time (in every commit) -- that's the only safe way.

Will upload a patch shortly."
1,"When node is created and locked in same transaction, exception is thrownFollowing code fails when executed inside an XA transaction:

Node n = session.getRootNode().addNode(""n"");
n.addMixin(""mix:lockable"");
session.save();
Lock lock = n.lock(false, false);

Stacktrace is

Caused by: javax.transaction.xa.XAException
	at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:155)
	at org.apache.jackrabbit.core.XASessionImpl.commit(XASessionImpl.java:337)
	at org.apache.jackrabbit.jca.TransactionBoundXAResource.commit(TransactionBoundXAResource.java:39)
	at org.apache.geronimo.transaction.manager.WrapperNamedXAResource.commit(WrapperNamedXAResource.java:47)
	at org.apache.geronimo.transaction.manager.TransactionImpl.commit(TransactionImpl.java:301)
	... 32 more
Caused by: org.apache.jackrabbit.core.TransactionException: Unable to update.
	at org.apache.jackrabbit.core.lock.XAEnvironment.prepare(XAEnvironment.java:275)
	at org.apache.jackrabbit.core.lock.XALockManager.prepare(XALockManager.java:245)
	at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:138)
	... 36 more
Caused by: javax.jcr.ItemNotFoundException: failed to build path of 48fb59d8-ac77-4b9f-8b53-9f2492dca5e5: 48fb59d8-ac77-4b9f-8b53-9f2492dca5e5: 48fb59d8-ac77-4b9f-8b53-9f2492dca5e5
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getPath(HierarchyManagerImpl.java:407)
	at org.apache.jackrabbit.core.CachingHierarchyManager.getPath(CachingHierarchyManager.java:272)
	at org.apache.jackrabbit.core.lock.LockManagerImpl.getPath(LockManagerImpl.java:651)
	at org.apache.jackrabbit.core.lock.LockManagerImpl.internalLock(LockManagerImpl.java:276)
	at org.apache.jackrabbit.core.lock.XAEnvironment$LockInfo.update(XAEnvironment.java:409)
	at org.apache.jackrabbit.core.lock.XAEnvironment.prepare(XAEnvironment.java:273)
	... 38 more
Caused by: org.apache.jackrabbit.core.state.NoSuchItemStateException: 48fb59d8-ac77-4b9f-8b53-9f2492dca5e5
	at org.apache.jackrabbit.core.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:189)
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getItemState(HierarchyManagerImpl.java:188)
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getPath(HierarchyManagerImpl.java:402)
	... 43 more

"
1,"[PATCH] ClassDescriptor.hasIdField uses faulty logichasIdField tries to compare a FieldDescriptor to an empty string, which doesn't make sense, here:

     public boolean hasIdField() {
        return (this.idFieldDescriptor != null && ! this.idFieldDescriptor.equals(""""));
     }


i'm assuming it should be

       return (this.idFieldDescriptor != null && this.idFieldDescriptor.isId());

patch does this

"
1,"IndexDeletionPolicy.delete behaves incorrectly when deleting latest generation I have been looking to provide the ability to rollback committed transactions and encountered some issues.
I appreciate IndexDeletionPolicy's main motivation is to handle cleaning away OLD commit points but it does not explicitly state that it can or cannot be used to clean NEW commit points.

If this is not supported then the documentation should ideally state this. If the intention is to support this behaviour then read on .......

There seem to be 2 issues so far:
1) The first attempt to call IndexCommit.delete on the latest commit point fails to remove any contents. The subsequent call succeeds however
2) Deleting the latest commit point fails to update the segments.gen file to point to segments_N-1. New IndexReaders that are opened are then misdirected to open segments_N which has been deleted

Junit test to follow...

"
1,"SSLSocketFactory.connectSocket(...) possible NPE    public Socket connectSocket(
            final Socket sock,
            final InetSocketAddress remoteAddress,
            final InetSocketAddress localAddress,
            final HttpParams params) throws IOException, UnknownHostException, ConnectTimeoutException {
...

        SSLSocket sslsock = (SSLSocket) (sock != null ? sock : createSocket()); // ==> sock may be null
        if (localAddress != null) {
            sock.setReuseAddress(HttpConnectionParams.getSoReuseaddr(params)); // ==> NPE if sock is null
            sslsock.bind(localAddress);
        }

Should sock.setReuseAddress be sslsock.setReuseAddress?
"
1,"SegmentMerger should assert .del and .s* files are not passed to createCompoundFileSpinoff from LUCENE-3126. SegmentMerger.createCompoundFile does not document that it should not receive files that are not included in the .cfs, such as .del and .s* (separate norms). Today, that method is called from code which ensures that, but we should:
# Add some documentation to clarify that.
# Add some asserts so that if a test (or other code, running w/ -ea) does that, we catch it.

Will post a patch soon"
1,"NativeFSLockFactory.makeLock(...).isLocked() does not workIndexWriter.isLocked() or IndexReader.isLocked() do not work with NativeFSLockFactory.

The problem is, that the method NativeFSLock.isLocked() just checks if the same lock instance was locked before (lock != null). If the LockFactory created a new lock instance, this always returns false, even if its locked."
1,"DateValue.equals() relies on Calendar.equals()JSR170 states regarding Date values:
""The text format of dates must follow the following ISO 8601:2000-compliant format"".

While DateValue.valueOf(String) and DateValue.getString() both rely on the functionality provided by the org.apache.jackrabbit.util.ISO8601, DateValue.equals() compares the equality of the internal Calendar object (DateValue line 89). This may return false even if the Iso-format of both values are equal.

In other words: Creating a new DateValue using the ValueFactory from the String representation of an existing DateValue will return an object, that is not equal to the original DateValue. The reason for this is, that the String does not contain all infomation, that is used during Calendar.equals.

regards
angela

"
1,"FastVectorHighlighter: AIOOBE occurs if one PhraseQuery is contained by another PhraseQueryI'm very sorry but this is another one. If q=""a b c d"" OR ""b c"", then ArrayIndexOutOfBoundsException occurs in FieldQuery.checkOverlap(). I'm working on this and fix with test case soon to be posted.
Thank you for your patient!
"
1,"InstantiatedIndexReader throws NullPointerException in norms() when used with a MultiReader
When using InstantiatedIndexReader under a MultiReader where the other Reader contains documents, a NullPointerException is thrown here;

 public void norms(String field, byte[] bytes, int offset) throws IOException {
    byte[] norms = getIndex().getNormsByFieldNameAndDocumentNumber().get(field);
    System.arraycopy(norms, 0, bytes, offset, norms.length);
  }

the 'norms' variable is null. Performing the copy only when norms is not null does work, though I'm sure it's not the right fix.

java.lang.NullPointerException
	at org.apache.lucene.store.instantiated.InstantiatedIndexReader.norms(InstantiatedIndexReader.java:297)
	at org.apache.lucene.index.MultiReader.norms(MultiReader.java:273)
	at org.apache.lucene.search.TermQuery$TermWeight.scorer(TermQuery.java:70)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:131)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:112)
	at org.apache.lucene.search.Searcher.search(Searcher.java:136)
	at org.apache.lucene.search.Searcher.search(Searcher.java:146)
	at org.apache.lucene.store.instantiated.TestWithMultiReader.test(TestWithMultiReader.java:41)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at junit.framework.TestCase.runTest(TestCase.java:164)
	at junit.framework.TestCase.runBare(TestCase.java:130)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:120)
	at junit.framework.TestSuite.runTest(TestSuite.java:230)
	at junit.framework.TestSuite.run(TestSuite.java:225)
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:130)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)

"
1,Null paths break the compare methodThe compare method cannot handle the path being null
1,"Syns2Index failsRunning Syns2Index fails with a
java.lang.IllegalArgumentException: maxBufferedDocs must at least be 2 when enabled exception.
at org.apache.lucene.index.IndexWriter.setMaxBufferedDocs(IndexWriter.java:883)
at org.apache.lucene.wordnet.Syns2Index.index(Syns2Index.java:249)
at org.apache.lucene.wordnet.Syns2Index.main(Syns2Index.java:208)

The code is here
		// blindly up these parameters for speed
		writer.setMergeFactor( writer.getMergeFactor() * 2);
		writer.setMaxBufferedDocs( writer.getMaxBufferedDocs() * 2);

It looks like getMaxBufferedDocs used to return 10, and now it returns -1, not sure when that started happening.

My suggestion would be to just remove these three lines.  Since speed has already improved vastly, there isn't a need to speed things up.

To run this, Syns2Index requires two args.  The first is the location of the wn_s.pl file, and the second is the directory to create the index in."
1,"IndexWriter.addIndexes results in java.lang.OutOfMemoryErrorI'm re-opening a bug I logged previously. My previous bug report has 
disappeared. 

Issue: IndexWriter.addIndexes results in java.lang.OutOfMemoryError for large 
merges.

Until this writing, I've been merging successfully only through repetition, 
i.e. I keep repeating merges until a success. As my index size has grown, my 
success rate has steadily declined. I've reached the point where merges now 
fail 100% of the time. I can't merge.

My tests indicate the threshold is ~30GB on P4/800MB VM with 6 indexes. I have 
repeated my tests on many different machines (not machine dependent). I have 
repeated my test using local and attached storage devices (not storage 
dependent).

For what its worth, I believe the exception occurs entirely during the optimize 
process which is called implicitly after the merge. I say this because each 
time it appears the correct amount of bytes are written to the new index. Is it 
possible to decouple the merge and optimize processes?


The code snippet follows. I can send you the class file and 120GB data set. Let 
me know how you want it.

>>>>> code sample >>>>>

Directory[] sources = new Directory[paths.length];
...

Directory dest = FSDirectory.getDirectory( path, true);
IndexWriter writer = new IndexWriter( dest, new TermAnalyzer( 
StopWords.SEARCH_MAP), true);

writer.addIndexes( sources);
writer.close();"
1,"VirtualNodeTypeStateProvider creates PropertyState with type != value(s).getTypeVirtualNodeTypeStateProvider creates the item states for the in content representation of the node type definitions and in case of jcr:defaultValues hard codes the type of the property state (thus the jcr property) to PropertyType.STRING.

the nt-definition of nt:propertyDefinition however states that jcr:defaultValues doesn't have a required type, thus the type should rather be determined based on the values themselves.

the current behaviour leads situations where

Property.getType != Property.getValues()[0].getType()

which from my point of view is a bug."
1,"Proxy-Authorization header received on server side
 
 I'm following example
 http://hc.apache.org/httpcomponents-client-ga/examples.html
 Proxy authentication
 
 but it seems that not only proxy is receiving credentials for proxy.
 In log, which is generated at target.host I can see header
 Proxy-Authorization: Basic ....

--------- HEADER
Host:target.host:443
Connection:Keep-Alive
User-Agent:Apache-HttpClient/4.1 (java 1.5)
Proxy-Authorization:Basic Z
--------- POST


Dusan"
1,"DirectoryTaxonomyReader.refresh misbehaves with ref countsDirectoryTaxonomyReader uses the internal IndexReader in order to track its own reference counting. However, when you call refresh(), it reopens the internal IndexReader, and from that point, all previous reference counting gets lost (since the new IndexReader's refCount is 1).

The solution is to track reference counting in DTR itself. I wrote a simple unit test which exposes the bug (will be attached with the patch shortly)."
1,"QueryParser does not correctly handle escaped characters within quoted stringsThe Lucene query parser incorrectly handles escaped characters inside quoted strings; specifically, a quoted string that ends with an (escaped) backslash followed by any additional quoted string will not be properly tokenized. Consider the following example:

bq. {{(name:""///mike\\\\\\"") or (name:""alphonse"")}}

This is not a contrived example -- it derives from an actual bug we've encountered in our system. Running this query will throw an exception, but removing the second clause resolves the problem. After some digging I've found that the problem is with the way quoted strings are processed by the lexer: you'll notice that Mike's name is followed by three escaped backslashes right before the ending quote; looking at the JavaCC code for the query parser highlights the problem:

{code:title=QueryParser.jj|borderStyle=solid}
<DEFAULT> TOKEN : {
  <AND:       (""AND"" | ""&&"") >
| <OR:        (""OR"" | ""||"") >
| <NOT:       (""NOT"" | ""!"") >
| <PLUS:      ""+"" >
| <MINUS:     ""-"" >
| <LPAREN:    ""("" >
| <RPAREN:    "")"" >
| <COLON:     "":"" >
| <STAR:      ""*"" >
| <CARAT:     ""^"" > : Boost
| <QUOTED:     ""\"""" (~[""\""""] | ""\\\"""")* ""\"""">
...
{code}

Take a look at the way the QUOTED token is constructed -- there is no lexical processing of the escaped characters within the quoted string itself. In the above query the lexer matches everything from the first quote through all the backslashes, _treating the end quote as an escaped character_, thus also matching the starting quote of the second term. This causes a lexer error, because the last quote is then considered the start of a new match.

I've come to understand that the Lucene query handler is supposed to be able to handle unsanitized human input; indeed the lexer above would handle a query like {{""blah\""}} without complaining, but that's a ""best-guess"" approach that results in bugs with legal, automatically generated queries. I've attached a patch that fixes the erroneous behavior but does not maintain leniency with malformed queries; I believe this is the correct approach because the two design goals are fundamentally at odds. I'd appreciate any comments."
1,"StandardTokenizer splitting all of Korean words into separate charactersStandardTokenizer splits all those Korean words inth separate character tokens. For example, ""?????"" is one Korean word that means ""Hello"", but StandardAnalyzer separates it into five tokens of ""?"", ""?"", ""?"", ""?"", ""?""."
1,"CachingHieraarchyManager may serve moved itemsThere is a problem with weak referenced item states and event notification in the
LocalItemStateManager.

consider the following:
- Session A traverses some nodes and fills-up the cache of the ChachingHierarchyManager
- This also fills the weak-ref cache in Session A LocalItemStateManager.
- Session B does some operations
- At some point, GC decides to remove the weakly refferenced ItemStates in Session As
  LocalItemStateManager
- Session B moves a node and saves the changes.
- The SharedItemStateManager notifies all listeners that a node was modified
- The LocalItemStateManager of Session A receives the event, but does not bubble it,
  because it does not have the item anymore in its cache
- The CachingHierarchyManager of Session A never receives the modification event and still
  servers the items at the old location.

Solution A:
reconnect missing states in the LocalItemStateManager when an event is received. this has
the drawback that a lot of state would be generated that are not needed.

Solution B:
add a new event 'nodeModified' that is only sent by the LocalItemStateManager if a
'stateModified' was received for which it does not have the item aymore. this has the
drawback that alot more events are generated.

Will implement solution B
"
1,"NullPointerException during indexing in DocumentsWriter$ThreadState$FieldData.addPositionIn my case during indexing sometimes appear documents with unusually large ""words"" - text-encoded images in fact.
Attempt to add document that contains field with such token produces java.lang.IllegalArgumentException:
java.lang.IllegalArgumentException: term length 37944 exceeds max term length 16383
        at org.apache.lucene.index.DocumentsWriter$ThreadState$FieldData.addPosition(DocumentsWriter.java:1492)
        at org.apache.lucene.index.DocumentsWriter$ThreadState$FieldData.invertField(DocumentsWriter.java:1321)
        at org.apache.lucene.index.DocumentsWriter$ThreadState$FieldData.processField(DocumentsWriter.java:1247)
        at org.apache.lucene.index.DocumentsWriter$ThreadState.processDocument(DocumentsWriter.java:972)
        at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:2202)
        at org.apache.lucene.index.DocumentsWriter.addDocument(DocumentsWriter.java:2186)
        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1432)
        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1411)

This is expected, exception is caught and ignored. The problem is that after this IndexWriter becomes somewhat corrupted and subsequent attempts to add documents to the index fail as well, this time with NPE:
java.lang.NullPointerException
        at org.apache.lucene.index.DocumentsWriter$ThreadState$FieldData.addPosition(DocumentsWriter.java:1497)
        at org.apache.lucene.index.DocumentsWriter$ThreadState$FieldData.invertField(DocumentsWriter.java:1321)
        at org.apache.lucene.index.DocumentsWriter$ThreadState$FieldData.processField(DocumentsWriter.java:1247)
        at org.apache.lucene.index.DocumentsWriter$ThreadState.processDocument(DocumentsWriter.java:972)
        at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:2202)
        at org.apache.lucene.index.DocumentsWriter.addDocument(DocumentsWriter.java:2186)
        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1432)
        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1411)

This is 100% reproducible."
1,ItemManager.toString() causes StackOverflowError
1,"Contrib queries package Query implementations do not override equals()Query implementations should override equals() so that Query instances can be cached and that Filters can know if a Query has been used before.  See the discussion in this thread.

http://www.mail-archive.com/java-user@lucene.apache.org/msg13061.html

Following 3 contrib Query implementations do no override equals()

org.apache.lucene.search.BoostingQuery;
org.apache.lucene.search.FuzzyLikeThisQuery;
org.apache.lucene.search.similar.MoreLikeThisQuery;

Test cases below show the problem.

package com.teamware.office.lucene.search;

import static org.junit.Assert.*;

import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.index.Term;
import org.apache.lucene.search.BoostingQuery;
import org.apache.lucene.search.FuzzyLikeThisQuery;
import org.apache.lucene.search.TermQuery;
import org.apache.lucene.search.similar.MoreLikeThisQuery;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;
public class ContribQueriesEqualsTest
{
    /**
     * @throws java.lang.Exception
     */
    @Before
    public void setUp() throws Exception
    {
    }

    /**
     * @throws java.lang.Exception
     */
    @After
    public void tearDown() throws Exception
    {
    }
    
    /**
     *  Show that the BoostingQuery in the queries contrib package 
     *  does not implement equals() correctly.
     */
    @Test
    public void testBoostingQueryEquals()
    {
        TermQuery q1 = new TermQuery(new Term(""subject:"", ""java""));
        TermQuery q2 = new TermQuery(new Term(""subject:"", ""java""));
        assertEquals(""Two TermQueries with same attributes should be equal"", q1, q2);
        BoostingQuery bq1 = new BoostingQuery(q1, q2, 0.1f);
        BoostingQuery bq2 = new BoostingQuery(q1, q2, 0.1f);
        assertEquals(""BoostingQuery with same attributes is not equal"", bq1, bq2);
    }

    /**
     *  Show that the MoreLikeThisQuery in the queries contrib package 
     *  does not implement equals() correctly.
     */
    @Test
    public void testMoreLikeThisQueryEquals()
    {
        String moreLikeFields[] = new String[] {""subject"", ""body""};
        
        MoreLikeThisQuery mltq1 = new MoreLikeThisQuery(""java"", moreLikeFields, new StandardAnalyzer());
        MoreLikeThisQuery mltq2 = new MoreLikeThisQuery(""java"", moreLikeFields, new StandardAnalyzer());
        assertEquals(""MoreLikeThisQuery with same attributes is not equal"", mltq1, mltq2);
    }
    /**
     *  Show that the FuzzyLikeThisQuery in the queries contrib package 
     *  does not implement equals() correctly.
     */
    @Test
    public void testFuzzyLikeThisQueryEquals()
    {
        FuzzyLikeThisQuery fltq1 = new FuzzyLikeThisQuery(10, new StandardAnalyzer());
        fltq1.addTerms(""javi"", ""subject"", 0.5f, 2);
        FuzzyLikeThisQuery fltq2 = new FuzzyLikeThisQuery(10, new StandardAnalyzer());
        fltq2.addTerms(""javi"", ""subject"", 0.5f, 2);
        assertEquals(""FuzzyLikeThisQuery with same attributes is not equal"", fltq1, fltq2);
    }
}
"
1,"Denying a primaryType does not work in XPathThe following query does not work:

//element(*, my:type)[jcr:contains(.,'foo') and @jcr:primaryType != 'nt:frozenNode')

The jcr:primaryType predicate does not respect the 'not equal' operation."
1,"unable to workspace import XML.tika detects xml as ""application/xml"" thus breaking the org.apache.jackrabbit.server.io.XmlHandler
which just checks for ""text/xml""."
1,FastVectorHighlighter: out of alignment when the first value is empty in multiValued field
1,"Jcr-Server: registration of ReportTypes failsRegistration of ReportType(s) using 

ReportType.register(String localName, Namespace namespace, Class reportClass)  [ReportType]

fails due to wrong evaluation of interfaces implemented by the given class object.
"
1,"Infinite loop on basic authenticationClass org.apache.http.impl.client.DefaultRequestDirector has a bug whereby when Authentication fails if the log is not warnEnabled then you will receive a retry request and end up in an infinite loop retrying requests.. This occurred for me when SL4J was being picked up as the implementation but not properly configured.

In 4.1.2 the line number of the offending code is in the handleResponse method, line 1126, the return null statement requires moving outside of the if statement that checkes if the log is warn enabled."
1,"DatabaseJournal commits twice inside a transaction, causing an error with MySQLWhen committing a transaction in a clustered setup, multiple records may be appended to the DatabaseJournal. After having appended a record, commit() is called on the connection and auto-commit mode is again enabled. Apart from not being semantically correct, committing a connection that is already in auto-commit mode throws an error when using MySQL as backend."
1,Repository does not release all resources on shutdownWhen Jackrabbit is shutdown some java.util.Timer threads are still running in the background even though no tasks are scheduled. This prevents the GC from collecting the classes when Jackrabbit is redeployed within a web application.
1,"LuceneQueryBuilder assumes readability of root-Node to be granted in any case.Have a User U. 
Have the User U denied to read ""/"".
Have the User U allowed to read ""/home/u"".

Any query of User U on this workspace fails with an AccessDeniedException.

The exception is caused by a call insided LuceneQueryBuilder on ln212:
NodeId id = ((NodeImpl) session.getRootNode()).getNodeId(); 

I couldn't find a specification that imposes the readability of root-node as a precondtion for query.
Therefore I consider this behavior as a bug."
1,"ProxyCredentials disclosed to remote hostI'm using httpclient (svn-trunk of today) to connect to a remote SSL-Host 
via a proxy. The proxy requires authorization (basic) and I want to use 
preemptive authorization. 
 
Since HTTPCLIENT-514 is fixed the preemptive authorization works, but my traces 
show that the proxy credentials are also transmitted to the remote host 
through the CONNECT-tunnel, thus disclosing sensitive information to the 
remote host. 
 
My code looks like this: 
 
HttpClient client = new HttpClient(); 
HttpMethod method = new GetMethod(""https://test""); 
 
client.getHostConfiguration().setProxy(""127.0.0.1"",3128); 
client.getState().setProxyCredentials( 
                new AuthScope(""127.0.0.1"", 3128), 
                new UsernamePasswordCredentials(""proxy"", ""test"")); 
client.getState().setAuthenticationPreemptive(true); 
client.executeMethod(method); 
 
The trace: 
 
2005/11/03 13:53:13:244 CET [DEBUG] HttpMethodDirector - Preemptively 
sending default basic credentials 
2005/11/03 13:53:13:261 CET [DEBUG] HttpMethodDirector - Authenticating 
with BASIC <any realm>@127.0.0.1:3128 
2005/11/03 13:53:13:262 CET [DEBUG] HttpMethodParams - Credential charset 
not configured, using HTTP element charset 
2005/11/03 13:53:13:266 CET [DEBUG] HttpMethodDirector - Authenticating 
with BASIC <any realm>@test:443 
2005/11/03 13:53:13:267 CET [WARN] HttpMethodDirector - Required 
credentials not available for BASIC <any realm>@test:443 
2005/11/03 13:53:13:267 CET [WARN] HttpMethodDirector - Preemptive 
authentication requested but no default credentials available 
2005/11/03 13:53:13:268 CET [DEBUG] HttpConnection - Open connection to 
127.0.0.1:3128 
2005/11/03 13:53:13:279 CET [DEBUG] HttpMethodDirector - Preemptively 
sending default basic credentials 
2005/11/03 13:53:13:280 CET [DEBUG] HttpMethodDirector - Authenticating 
with BASIC <any realm>@127.0.0.1:3128 
2005/11/03 13:53:13:280 CET [DEBUG] HttpMethodParams - Credential charset 
not configured, using HTTP element charset 
2005/11/03 13:53:13:283 CET [DEBUG] header - >> ""CONNECT test:443 HTTP/1.1"" 
2005/11/03 13:53:13:284 CET [DEBUG] HttpMethodBase - Adding Host request 
header 
2005/11/03 13:53:13:284 CET [DEBUG] header - >> ""Proxy-Authorization: 
Basic cHJveHk6dGVzdA==[\r][\n]"" 
2005/11/03 13:53:13:285 CET [DEBUG] header - >> ""User-Agent: Jakarta 
Commons-HttpClient/3.0-rc4[\r][\n]"" 
2005/11/03 13:53:13:285 CET [DEBUG] header - >> ""Host: test[\r][\n]""       
                                                                           
2005/11/03 13:53:13:286 CET [DEBUG] header - >> ""Proxy-Connection: 
Keep-Alive[\r][\n]"" 
2005/11/03 13:53:13:286 CET [DEBUG] header - >> ""[\r][\n]""                 
                                                                         
2005/11/03 13:53:13:311 CET [DEBUG] header - << ""HTTP/1.0 200 
Connection established[\r][\n]""                                            
2005/11/03 13:53:13:326 CET [DEBUG] ConnectMethod - CONNECT status code 200 
2005/11/03 13:53:13:327 CET [DEBUG] HttpConnection - Secure tunnel to 
test:443 
2005/11/03 13:53:13:418 CET [DEBUG] header - >> ""GET / HTTP/1.1[\r][\n]"" 
2005/11/03 13:53:13:420 CET [DEBUG] HttpMethodBase - Adding Host request 
header 
2005/11/03 13:53:13:423 CET [DEBUG] header - >> ""Proxy-Authorization: 
Basic cHJveHk6dGVzdA==[\r][\n]"" 
2005/11/03 13:53:13:424 CET [DEBUG] header - >> ""User-Agent: Jakarta 
Commons-HttpClient/3.0-rc4[\r][\n]"" 
2005/11/03 13:53:13:425 CET [DEBUG] header - >> ""Host: test[\r][\n]"" 
2005/11/03 13:53:13:425 CET [DEBUG] header - >> ""[\r][\n]"" 
2005/11/03 13:53:14:391 CET [DEBUG] header - << ""HTTP/1.1 200 OK[\r][\n]"" 
 
As you can see the proxy credentials are also transmitted through the 
SSL-tunnel to the remote host which is a security risk."
1,TieredMergePolicy expungeDeletes should not enforce maxMergedSegmentMB
1,"NPE in classes of OJB-PMNPE occurs while accessing the Id of the parent of the root node which is null.

Patch follows"
1,"TaxonomyWriter parents array creation is not thread safe, can cause NPEFollowing user list thread [TaxWriter leakage? | http://markmail.org/thread/jkkhemfzpnbdzoft] it appears that if two threads or more are asking for the parent array for the first time, a context switch after the first thread created the empty parents array but before it initialized it would cause the other array to use an uninitialized array, causing an NPE. Fix is simple: synchronize the method getParentArray()"
1,"BLOBFileValue() might be discarded to earlySituation:

if the internal value of a property of type binary is created by the constructor BLOBFileValue(InputStream in) and the content is not stored in an temp-file, then calling the methods 

a) #setProperty(InputStream in) on this node and then
b) #refresh(false) on the node of this property 

on the node of this property leads to an internal value of this property with an erased byte[].

Solution:

Only if the spoolFile is created the field 'temp' should be set to true.
If the InputStream is stored in the byte[] the field 'temp' should be set to false.

Patch:

Index: BLOBFileValue.java
===================================================================
retrieving revision 1.1
diff -u -r1.1 BLOBFileValue.java
--- BLOBFileValue.java	8 May 2006 13:57:49 -0000	1.1
+++ BLOBFileValue.java	8 May 2006 15:19:54 -0000
@@ -142,6 +142,7 @@
                     len += read;
                 }
             }
+            in.close();
         } finally {
             if (out != null) {
                 out.close();
@@ -151,8 +152,15 @@
         // init vars
         file = spoolFile;
         fsResource = null;
-        // this instance is backed by a temporarily allocated resource/buffer
-        temp = true;
+        if (file != null)
+        {
+            // this instance is backed by a temporarily allocated resource
+            temp = true;
+        }
+        else
+        {
+            temp = true;
+        }
     }
 
     /**


"
1,"NPE when calling isCurrent() on a ParallellReaderAs demonstrated by the test case below, if you call isCurrent() on a ParallelReader it causes an NPE. Fix appears to be to add an isCurrent() to ParallelReader which calls it on the underlying indexes but I'm not sure what other problems may be lurking here. Do methods such as getVersion(), lastModified(), isOptimized() also have to be rewritten or is this a use case where ParallelReader will never mimic IndexReader perfectly? At the very least this behavior should be documented so others know what to expect.


    [junit] Testcase: testIsCurrent(org.apache.lucene.index.TestParallelReader):        Caused an ERROR
    [junit] null
    [junit] java.lang.NullPointerException
    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:502)
    [junit]     at org.apache.lucene.index.SegmentInfos.readCurrentVersion(SegmentInfos.java:336)
    [junit]     at org.apache.lucene.index.IndexReader.isCurrent(IndexReader.java:316)
    [junit]     at org.apache.lucene.index.TestParallelReader.testIsCurrent(TestParallelReader.java:146)



Index: src/test/org/apache/lucene/index/TestParallelReader.java
===================================================================
--- src/test/org/apache/lucene/index/TestParallelReader.java    (revision 518122)
+++ src/test/org/apache/lucene/index/TestParallelReader.java    (working copy)
@@ -135,6 +135,15 @@
       assertEquals(docParallel.get(""f4""), docSingle.get(""f4""));
     }
   }
+  
+  public void testIsCurrent() throws IOException {
+    Directory dir1 = getDir1();
+    Directory dir2 = getDir2();
+    ParallelReader pr = new ParallelReader();
+    pr.add(IndexReader.open(dir1));
+    pr.add(IndexReader.open(dir2));
+    assertTrue(pr.isCurrent());
+  }
 
   // Fiels 1-4 indexed together:
   private Searcher single() throws IOException {
"
1,"DescendantSelfAxisQuery may fail with IOException when session has limited accessThe DescendantSelfAxisQuery uses the current session to look up nodes by id. When the session does not have access to a node the exception is incorrectly re-thrown an IOException. Instead, any ItemNotFoundException should be caught and ignored. This is probably a regression caused by JCR-1365 introduced with Jackrabbit 1.5."
1,"CookieSpec.formatCookie(Cookie) produces an incorrect cookie header valueConsider the following:
----------------------------------------------------------------------
Cookie cookie = new Cookie("".foo.com"", ""name"", ""value"");
cookie.setVersion(1);
cookie.setPath(""/"");
CookieSpec spec = CookiePolicy.getSpecByPolicy(CookiePolicy.RFC2109);
System.out.println(spec.formatCookie(cookie));                
----------------------------------------------------------------------

When calling CookieSpec.formatCookie(Cookie) the resulting output is:

   name=""value""

The Version attribute is not present as required by RFC2109, nor is the path or
domain information included.

It seems that in this case, only Cookie type 0 output is produced."
1,"Requests are retried 3 times unconditionalyUsing the 20020811 tarball and jdk1.4.0, a get or post will retry as soon
as it finishes sending the request. I turned on logging and verified that
as soon as the last \r\n hits the wire, it starts on the next retry. For
example:

08-10 09:53:12 [main] httpclient.wire: >> [\r\n]
08-10 09:53:12 [main] httpclient.methods.PostMethod: enter
PostMethod.writeRequestBody(HttpState, HttpConnection)
08-10 09:53:12 [main] commons.httpclient.HttpConnection: enter
HttpConnection.write(byte[], int, int)
08-10 09:53:12 [main] commons.httpclient.HttpMethod: Attempt number 3 to write
request
08-10 09:53:12 [main] commons.httpclient.HttpMethod: enter
HttpMethodBase.writeRequest(HttpState, HttpConnection)
08-10 09:53:12 [main] commons.httpclient.HttpMethod: enter
HttpMethodBase.writeRequestLine(HttpState, HttpConnection)
08-10 09:53:12 [main] commons.httpclient.HttpMethod: enter
HttpMethodBase.generateRequestLine(HttpConnection, String, String, String,
String)
08-10 09:53:12 [main] commons.httpclient.HttpConnection: enter
HttpConnection.print(String)
08-10 09:53:12 [main] commons.httpclient.HttpConnection: enter
HttpConnection.write(byte[])
08-10 09:53:12 [main] httpclient.wire: >> ""POST /lookup.jsp HTTP/1.1"" [\r\n]

The top line is the end of the second post and the last line is the start
of the third post.

To make sure the server really wasn't sending something back, I wrote a
quick server that would listen for a request and send a 404 as soon as it
read a post or get line (but would keep reading and dumping info). In the
httpclient log, it still shoots off 3 requests before it receives the
response and the server got all three requests. (client and server are
running on the same machine)

So why is httpclient sending three requests without waiting for a
response?"
1,JCARepositoryManager does not close InputStream used to obtain repository config from classpath
1,"CLONE -ManageableCollectionUtil doesn't support MapsManageableCollectionUtil has two getManageableCollection methods, which do not currently return a ManageableCollection which wraps Maps. 

ManagedHashMap already exists in the codebase which I assume was created for this purpose, so both getManageableCollection methods could be modified so that they do something like:

            if (object instanceof Map){
                return new ManagedHashMap((Map)object);
            }


An alternative solution might be to modify the JCR mapping to support explicitly defining the 'ManagedXXX' class."
1,"Incorrect usage of AttributeSource.addAttribute/getAttribute leads to failures when onlyUseNewAPI=truewhen seting ""use only new API"" for TokenStream, i received the following exception:

{code}
   [junit] Caused by: java.lang.IllegalArgumentException: This AttributeSource does not have the attribute 'interface org.apache.lucene.analysis.tokenattributes.TermAttribute'.
    [junit] 	at org.apache.lucene.util.AttributeSource.getAttribute(AttributeSource.java:249)
    [junit] 	at org.apache.lucene.index.TermsHashPerField.start(TermsHashPerField.java:252)
    [junit] 	at org.apache.lucene.index.DocInverterPerField.processFields(DocInverterPerField.java:145)
    [junit] 	at org.apache.lucene.index.DocFieldProcessorPerThread.processDocument(DocFieldProcessorPerThread.java:244)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:772)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:755)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:2613)
{code}

However, i can't actually see the culprit that caused this exception

suggest that the IllegalArgumentException include ""getClass().getName()"" in order to be able to identify which TokenStream implementation actually caused this
"
1,"PostMethod#setParameter[HttpClient2.0-rc1]

-------- code fragment 1 -------------------------
PostMethod method = new PostMethod(uriString);
method.addParameter(""tel"", ""1111-1111"");
method.addParameter(""tel"", ""2222-2222"");
method.setParameter(""tel"", ""3333-3333"");

(post data sent)
tel=1111-1111&tel=2222-2222&tel=3333-3333

(post data i hope)
tel=3333-3333
-------------------------------------------------

---------------- code fragment 2 -----------------
PostMethod method = new PostMethod(uriString);
method.addParameter(""tel"", ""1111-1111"");
method.addParameter(""tel"", ""2222-2222"");
method.addParameter(""tel"", ""3333-3333"");

(post data sent)
tel=1111-1111&tel=2222-2222&tel=3333-3333
--------------------------------------------------

what difference between code 1 and code2 ?

sorry for my poor english."
1,"Exception in DocumentsWriter.ThreadState.init leads to corruptionIf an exception is hit in the init method, DocumentsWriter incorrectly
increments numDocsInRAM when in fact the document is not added.

Spinoff of this thread:

  http://markmail.org/message/e76hgkgldxhakuaa

The root cause that led to the exception in init was actually due to
incorrect use of Lucene's APIs (one thread still modifying the
Document while IndexWriter.addDocument is adding it) but still we
should protect against any exceptions coming out of init.

"
1,"inconsistent repository after overlapping node add operationsIt seems I can reproduce a sequence of operations that cause the repository to be inconsistent.

The short version: 2 sessions add a same-named child node to the same parent folder (not allowing same-name-siblings). Session 1's save() succeeds. Session 2's save() fails, but succeeds on retry (!).

After the operation, the child node created by session 1 is still present, but the parent doesn't list it as child node anymore.

(will add test case)"
1,"[PATCH] Ordered spanquery with slop can failIn CVS of 7 April 2004. 
An ordered SpanQuery with slop 1 querying: w1 w2 w3 
in document: w1 w3 w2 w3 
fails. It should match as: w1 . w2 w3"
1,"NodeTypeRegistry.registerNodeType(NodeTypeDef) does not verify that the referenced namespaces are registeredcurrently it's possible to register a node type using a defintion that contains references to unregistered namespaces.

using such a  node type in content would lead to unpredictable results."
1,"jcr2spi:  Remove sanityCheck() from ItemImpl.getSession()same as JCR-911 for jcr2spi.

the check was responsible for the failure of ActivitiesTest#testActivitiesRelation"
1,"StandardBenchmarker#makeDocument does not explicitly close opened filesStandardBenchmarker#makeDocument(File in, String[] tags, boolean stored, boolean tokenized, boolean tfv)

        BufferedReader reader = new BufferedReader(new FileReader(in));

Above reader is not closed until GC hits it. Can cause problems in cases where ulimit is set too low.

I did this:

        while ((line = reader.readLine()) != null)
        {
            body.append(line).append(' ');
        }
+        reader.close();"
1,"NullPointerException in AbstractVersionManager.createVersionHistory()Running ConcurrentCheckinMixedTransactionTest with 200 threads results in NullPointerExceptions in AbstractVersionManager.

Exception in thread ""Thread-16"" java.lang.NullPointerException
	at org.apache.jackrabbit.core.version.AbstractVersionManager.createVersionHistory(AbstractVersionManager.java:309)
	at org.apache.jackrabbit.core.version.XAVersionManager.createVersionHistory(XAVersionManager.java:145)
	at org.apache.jackrabbit.core.ItemImpl.initVersionHistories(ItemImpl.java:785)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1221)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:897)
	at org.apache.jackrabbit.core.ConcurrentCheckinMixedTransactionTest$1$1.execute(ConcurrentCheckinMixedTransactionTest.java:66)
	at org.apache.jackrabbit.core.AbstractConcurrencyTest$Executor.run(AbstractConcurrencyTest.java:110)
	at java.lang.Thread.run(Thread.java:619)

I'm not sure why the node that is created by the current thread is not available. I assume that some other thread using XA transactions is committing changes while the current thread creates the node. The changes from the committing thread then overwrite the node that has been modified by the current thread. The write lock is somewhat bypassed in that case."
1,"QValueFactoryImpl$BinaryQValue must not return 'this' on getBinaryThis is basically the same as JCR-2238, but for the spi-commons module. BinaryQValue returns 'this' on getBinary(), which will lead to a file not found exception because Binary.dispose() will delete the the underlying temp file.

The issue is partially hidden by the presence of a bug in BinaryQValue.read(): the RandomAccessFile is not closed after reading, which might prevent deleting of the temp file."
1,"""Directory was previously created with a different LockFactory"" when open, close, delete a repository in a loopOpening a TransientRepository in a loop throws the exception ""Directory was previously created with a different LockFactory instance"".

Test case:

for (int i = 0; i < 3; i++) {
	FileUtils.deleteDirectory(new File(""repository""));
	Repository rep = new TransientRepository();
	Session session = rep.login(new SimpleCredentials("""", new char[0]));
	session.logout();
}

The problem seems to be that org.apache.lucene.store.FSDirectory.DIRECTORIES is not cleared (FSDirectory.close() is not called?).

Stack trace:

Exception in thread ""main"" javax.jcr.RepositoryException: Directory was previously created with a different LockFactory instance; please pass null as the lockFactory instance and use setLockFactory to change it: Directory was previously created with a different LockFactory instance; please pass null as the lockFactory instance and use setLockFactory to change it: Directory was previously created with a different LockFactory instance; please pass null as the lockFactory instance and use setLockFactory to change it
	at org.apache.jackrabbit.core.SearchManager.initializeQueryHandler(SearchManager.java:555)
	at org.apache.jackrabbit.core.SearchManager.<init>(SearchManager.java:239)
	at org.apache.jackrabbit.core.RepositoryImpl.getSystemSearchManager(RepositoryImpl.java:688)
	at org.apache.jackrabbit.core.RepositoryImpl.access$3(RepositoryImpl.java:681)
	at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.getSearchManager(RepositoryImpl.java:1780)
	at org.apache.jackrabbit.core.RepositoryImpl.initWorkspace(RepositoryImpl.java:667)
	at org.apache.jackrabbit.core.RepositoryImpl.initStartupWorkspaces(RepositoryImpl.java:480)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:321)
	at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:618)
	at org.apache.jackrabbit.core.TransientRepository$2.getRepository(TransientRepository.java:241)
	at org.apache.jackrabbit.core.TransientRepository.startRepository(TransientRepository.java:261)
Caused by: java.io.IOException: Directory was previously created with a different LockFactory instance; please pass null as the lockFactory instance and use setLockFactory to change it
	at org.apache.lucene.store.FSDirectory.getDirectory(FSDirectory.java:192)
	at org.apache.jackrabbit.core.query.lucene.directory.FSDirectoryManager.getDirectory(FSDirectoryManager.java:64)
	at org.apache.jackrabbit.core.query.lucene.MultiIndex.<init>(MultiIndex.java:227)
	at org.apache.jackrabbit.core.query.lucene.SearchIndex.doInit(SearchIndex.java:477)
	at org.apache.jackrabbit.core.query.AbstractQueryHandler.init(AbstractQueryHandler.java:59)
"
1,"cluster synchronization NPEwe have a 4 machines setup and encountered the following NPE in one of the nodes. After restarting tomcat, the problem seems to go away. But it would be nice to find out why.


java.lang.NullPointerException
        at org.apache.jackrabbit.core.query.lucene.NodeIndexer.createDoc(NodeInd
exer.java:146)
        at org.apache.jackrabbit.core.query.lucene.SearchIndex.createDocument(Se
archIndex.java:566)
        at org.apache.jackrabbit.core.query.lucene.SearchIndex$2.next(SearchInde
x.java:368)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.update(MultiIndex.
java:354)
        at org.apache.jackrabbit.core.query.lucene.SearchIndex.updateNodes(Searc
hIndex.java:356)
        at org.apache.jackrabbit.core.SearchManager.onEvent(SearchManager.java:4
23)
        at org.apache.jackrabbit.core.observation.EventConsumer.consumeEvents(Ev
entConsumer.java:231)
        at org.apache.jackrabbit.core.observation.ObservationDispatcher.dispatch
Events(ObservationDispatcher.java:201)
        at org.apache.jackrabbit.core.observation.EventStateCollection.dispatch(
EventStateCollection.java:424)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.externalUpdat
e(SharedItemStateManager.java:882)
        at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.externalUpdat
e(RepositoryImpl.java:1957)
        at org.apache.jackrabbit.core.cluster.ClusterNode.end(ClusterNode.java:8
34)
        at org.apache.jackrabbit.core.cluster.ClusterNode.consume(ClusterNode.ja
va:929)
        at org.apache.jackrabbit.core.journal.AbstractJournal.doSync(AbstractJou
rnal.java:191)
        at org.apache.jackrabbit.core.journal.AbstractJournal.sync(AbstractJourn
al.java:166)
        at org.apache.jackrabbit.core.cluster.ClusterNode.sync(ClusterNode.java:
283)
        at org.apache.jackrabbit.core.cluster.ClusterNode.start(ClusterNode.java
:229)
        at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:
308)
        at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:
584)
        at org.apache.jackrabbit.core.jndi.BindableRepository.createRepository(B
indableRepository.java:174)"
1,"DefaultAccessManager#hasPrivileges(String,Set,Privilege[]) doesn't close compiled permissionsDefaultAccessManager#hasPrivileges(String,Set,Privilege[]) retrieves the compiled permissions for the specified set of principals
from the ac provider but omit the CompiledPermissions#close() call before returning."
1,"Config incorrectly handles Windows absolute pathnamesI have no idea how no one ran into this so far, but I tried to execute an .alg file which used ReutersContentSource and referenced both docs.dir and work.dir as Windows absolute pathnames (e.g. d:\something). Surprisingly, the run reported an error of missing content under benchmark\work\something.

I've traced the problem back to Config, where get(String, String) includes the following code:
{code}
    if (sval.indexOf("":"") < 0) {
      return sval;
    }
    // first time this prop is extracted by round
    int k = sval.indexOf("":"");
    String colName = sval.substring(0, k);
    sval = sval.substring(k + 1);
    ...
{code}

It detects "":"" in the value and so it thinks it's a per-round property, thus stripping ""d:"" from the value ... fix is very simple:
{code}
    if (sval.indexOf("":"") < 0) {
      return sval;
    } else if (sval.indexOf("":\\"") >= 0) {
      // this previously messed up absolute path names on Windows. Assuming
      // there is no real value that starts with \\
      return sval;
    }
    // first time this prop is extracted by round
    int k = sval.indexOf("":"");
    String colName = sval.substring(0, k);
    sval = sval.substring(k + 1);
{code}

I'll post a patch w/ the above fix + test shortly."
1,"redirect not handled correctly if location header doesn't have a protocolHttp redirect is not handled correctly if the location header doesn't have a 
protocol, e.g.:

Location: web/tbghome.nsf/pages/index

a java.net.MalformedURLException is throw in this case. The correct behavior is 
to inherit the protocol from current URL.

The relevant code is in HttpMethodBase.execute()"
1,"IndexWriter.addIndexesNoOptimize ignores the compound file setting of the destination indexIndexWriter.addIndexesNoOptimize(Directory[]) ignores the compound file setting of the destination index. It is using the compound file flags of segments in the source indexes.
This sometimes causes undesired increase of the number of files in the destination index when non-compound file indexes are added until merge kicks in."
1,"KuromojiTokenizer fails with large docsjust shoving largeish random docs triggers asserts like:

{noformat}
    [junit] Caused by: java.lang.AssertionError: backPos=4100 vs lastBackTracePos=5120
    [junit] 	at org.apache.lucene.analysis.kuromoji.KuromojiTokenizer.backtrace(KuromojiTokenizer.java:907)
    [junit] 	at org.apache.lucene.analysis.kuromoji.KuromojiTokenizer.parse(KuromojiTokenizer.java:756)
    [junit] 	at org.apache.lucene.analysis.kuromoji.KuromojiTokenizer.incrementToken(KuromojiTokenizer.java:403)
    [junit] 	at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:404)
{noformat}

But, you get no seed...

I'll commit the test case and @Ignore it."
1,"Benchmark's ExtractReuters creates its temp dir wrongly if provided out-dir param ends by slashSee LUCENE-929 for context.
As result, it might fail to create the temp dir at all."
1,"XPathQueryBuilder may not handle multiple jcr:deref correctlyIf you have the following tree (inspired from DerefTest) :
+ people
   + carl (worksfor -> company/microsoft)
   + frank (worksfor -> company/microsoft)
+ company
    + microsoft (eotm -> carl)

The following queries will be translated to :

testroot/people/frank/jcr:deref(@worksfor, '*')/jcr:deref(@eotm, '*')
+ Root node
+ Select properties: *
  + PathQueryNode
    + LocationStepQueryNode:  NodeTest={}testroot Descendants=false Index=NONE
    + LocationStepQueryNode:  NodeTest={}people Descendants=false Index=NONE
    + LocationStepQueryNode:  NodeTest={}frank Descendants=false Index=NONE
    + DerefQueryNode:  NodeTest=* Descendants=false Index=NONE
    + DerefQueryNode:  NodeTest=* Descendants=false Index=NONE
=> Matching carl node

testroot/people/frank/jcr:deref(@worksfor, '*')/jcr:deref(@eotm, '*')[@jcr:uuid]
+ Root node
+ Select properties: *
  + PathQueryNode
    + LocationStepQueryNode:  NodeTest={}testroot Descendants=false Index=NONE
    + LocationStepQueryNode:  NodeTest={}people Descendants=false Index=NONE
    + LocationStepQueryNode:  NodeTest={}frank Descendants=false Index=NONE
    + LocationStepQueryNode:  NodeTest=* Descendants=false Index=NONE
    + DerefQueryNode:  NodeTest=* Descendants=false Index=NONE
      + RelationQueryNode: Op: NOT NULL Prop=@{http://www.jcp.org/jcr/1.0}uuid
=> Not matching carl node

testroot/people/frank/jcr:deref(@worksfor, '*')[@jcr:uuid]/jcr:deref(@eotm, '*')[@jcr:uuid]
+ Root node
+ Select properties: *
  + PathQueryNode
    + LocationStepQueryNode:  NodeTest={}testroot Descendants=false Index=NONE
    + LocationStepQueryNode:  NodeTest={}people Descendants=false Index=NONE
    + LocationStepQueryNode:  NodeTest={}frank Descendants=false Index=NONE
    + DerefQueryNode:  NodeTest=* Descendants=false Index=NONE
      + RelationQueryNode: Op: NOT NULL Prop=@{http://www.jcp.org/jcr/1.0}uuid
    + DerefQueryNode:  NodeTest=* Descendants=false Index=NONE
      + RelationQueryNode: Op: NOT NULL Prop=@{http://www.jcp.org/jcr/1.0}uuid
=> Matching carl node

testroot/people/frank/jcr:deref(@worksfor, '*')[@jcr:uuid]/jcr:deref(@eotm, '*')
+ Root node
+ Select properties: *
  + PathQueryNode
    + LocationStepQueryNode:  NodeTest={}testroot Descendants=false Index=NONE
    + LocationStepQueryNode:  NodeTest={}people Descendants=false Index=NONE
    + LocationStepQueryNode:  NodeTest={}frank Descendants=false Index=NONE
    + DerefQueryNode:  NodeTest=* Descendants=false Index=NONE
      + RelationQueryNode: Op: NOT NULL Prop=@{http://www.jcp.org/jcr/1.0}uuid
    + DerefQueryNode:  NodeTest=* Descendants=false Index=NONE
=> Matching carl node

This is because XPathQueryBuilder calls NAryQueryNode#removeOperand(QueryNode)
in order to replace current LocationStepQueryNode with a DerefQueryNode.

NAryQueryNode#removeOperand(QueryNode) uses internally a List and thus
relies on Object#equals(Object) for retrieving the object to remove.

But the equals method is redefined for every QueryNode with a different semantic.

Then, the call to NAryQueryNode#removeOperand(QueryNode) will not remove the
wanted operand but the first operand returning true after calling equals in
ArrayList#remove(Object)."
1,"Jcr-Server: Invalid value for HTTP auth headerAt present, DAV Explorer won't log in to the JCR WebDav servlet - it doesn't even ask for a username & password.  (Neither the Microsoft WinXP WebDAV & Novell's NetDrive were as fussy and were happy to log in)
Using Ethereal, I compared the traffic for a valid Slide WebDav login compared to a JCR WebDav login.

I've now found and fixed the problem on my local build, and I've now got DAV Explorer to work with JCR Webdav.  Here's a description of the bugfix:

In jackrabbit/contrib/jcr-server/server/src/java/org/apache/jackrabbit/server/AbstractWebdavServlet.java, there is a public static final String DEFAULT_AUTHENTICATE_HEADER.
This is currently set to ""Basic Realm=Jackrabbit Webdav Server"".

This is not a valid string for use in this context as it is in breach of RFC2617 for 2 reasons:
1) ""Realm"" should be ""realm""
2) ""Jackrabbit Webdav Server"" should be in quotes, i.e. ""\""Jackrabbit Webdav Server\""""

According to http://www.ietf.org/rfc/rfc2617.txt, a valid challenge would be:
   WWW-Authenticate: Basic realm=""WallyWorld""
Note that ""realm"" is not capitalised and ""WallyWorld"" has been enclosed in quotes (the ""WWW-Authenticate: "" string is held elsewhere in the Java code and is correct)


In other words, AbstractWebdavServlet.java line 82, which currently reads:
    public static final String DEFAULT_AUTHENTICATE_HEADER = ""Basic Realm=Jackrabbit Webdav Server"";
should be changed to read
    public static final String DEFAULT_AUTHENTICATE_HEADER = ""Basic realm=\""Jackrabbit Webdav Server\"""";

"
1,"If you pass Integer.MAX_VALUE as 2nd param to search(Query, int) you hit unexpected NegativeArraySizeExceptionNote that this is a nonsense value to pass in, since our PQ impl allocates the array up front.

It's because PQ takes 1+ this value (which wraps to -1), and attempts to allocate that.  We should bounds check it, and drop PQ size by one in this case.

Better, maybe: in IndexSearcher, if that n is ever > maxDoc(), set it to maxDoc().

This trips users up fairly often because they assume our PQ doesn't statically pre-allocate (a reasonable assumption...)."
1,"XPath query with negative numbers incorrectA XPath query that contains a negative number does not return correct results.

E.g. the query:

//*[@number = -10]

does not return nodes with number properties containing a value of -10 but will return nodes with number values equal to 10. Similarly the query returns wrong results for double values."
1,"No entry created for this pool.Followup to https://issues.apache.org/jira/browse/HTTPCLIENT-741, as reported by Sam Berlin:

java.lang.IllegalStateException: No entry created for this pool. HttpRoute[{}->http://74.160.66.42:14561]
    at org.apache.http.impl.conn.tsccm.RouteSpecificPool.freeEntry(RouteSpecificPool.java:137)
    at org.apache.http.impl.conn.tsccm.ConnPoolByRoute.freeEntry(ConnPoolByRoute.java:337)
    at org.apache.http.impl.conn.tsccm.ThreadSafeClientConnManager.releaseConnection(ThreadSafeClientConnManager.java:230)
    at org.apache.http.impl.client.DefaultClientRequestDirector.execute(DefaultClientRequestDirector.java:427)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:500)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:455)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:421)
    at com.limegroup.gnutella.http.DefaultHttpExecutor.performRequest(DefaultHttpExecutor.java:97)
    at com.limegroup.gnutella.http.DefaultHttpExecutor.access$000(DefaultHttpExecutor.java:26)
    at com.limegroup.gnutella.http.DefaultHttpExecutor$MultiRequestor.run(DefaultHttpExecutor.java:139)
    at org.limewire.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1006)
    at org.limewire.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:549)
    at java.lang.Thread.run(Thread.java:613)
---

DefaultHttpExecutor$MultiRequestor basically is just a Runnable / Cancellable [exposes a cancel() method] that can be cancelled from any thread. cancel just calls abort() on the current AbortableHttpRequest, but is called on a thread other than the one that's doing the client.execute(request).

The last one is the most common exception, and seems to happen with some regularity. The other two we've only seen once, so may just be a memory quirk (we've seen some crazy bugs, including recursive NPEs while constructing an NPE.)
"
1,"JCRUrlConnection relies on nt:file/nt:resourceThe JCRUrlConnection class implementing the jcr: URL handler for the JCR class loader relies on the fact that the intended primary type of the jcr:content child node of an nt:file node is of type nt:resource. When writing files with the Jackrabbit WebDAV server this is not the case as the jcr:content child node is of type nt:unstructured.

As a result the JCRUrlConnection.connect method fails with an ItemNotFoundException in the Util.getProperty(Item)  method because the primary item of the nt:unstructured node type is not defined."
1,SPI: Helper does not properly retrieve org.apache.jackrabbit.spi.workspacename param.consequently the Helper always obtains SessionInfo with null workspace name.
1,"StaleItemStateException during distributed transactionWe use the Jackrabbit JCA Component within a Weblogic 10.3 Application Server with distributed transactions between an Oracle Database an the Jackrabbit JCA.

Updating a node property multiple times in a transaction results in a XAException. Root cause seems to be a StaleItemStateException (see Stack-Trace).
Googling revealed, that a similar bug was fixed for Jackrabbit 1.5.3. Looking through the code showed, that the proposed fix in JCR-1554 seems not to be applied on Jackrabbit 2.0 (tag and trunk).

I tried to apply the proposed fix on the trunk code base, but this seemed not to help.

Stack-Trace:
javax.ejb.TransactionRolledbackLocalException: Error committing transaction:; nested exception is: javax.transaction.xa.XAException                                                                                             
        at weblogic.ejb.container.internal.EJBRuntimeUtils.throwTransactionRolledbackLocal(EJBRuntimeUtils.java:238)                                                                                                            
        at weblogic.ejb.container.internal.EJBRuntimeUtils.throwEJBException(EJBRuntimeUtils.java:133)                                                                                                                          
        at weblogic.ejb.container.internal.BaseLocalObject.postInvoke1(BaseLocalObject.java:623)                                                                                                                                
        at weblogic.ejb.container.internal.BaseLocalObject.postInvokeTxRetry(BaseLocalObject.java:424)                                                                                                                          
        at ch.ejpd.sireneit.facade.ejb.ablage.DokumentFacadeBean_7xdnsq_DokumentFacadeImpl.updateStructuredDokument(DokumentFacadeBean_7xdnsq_DokumentFacadeImpl.java:340)                                                      
        at ch.ejpd.sireneit.access.rest.ablage.DokumentResource.update(DokumentResource.java:453)                                                                                                                               
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)                                                                                                                                                          
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)                                                                                                                                        
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)                                                                                                                                
        at java.lang.reflect.Method.invoke(Method.java:597)                                                                                                                                                                     
        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:175)                                                
        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:67)                                                                                         
        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:208)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:109)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:109)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:109)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:75)                                                                                                                             
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:67)                                                                                                                 
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:724)                                                                                                                
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:689)                                                                                                                 
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:680)                                                                                                                 
        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:324)                                                                                                                                     
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:425)                                                                                                                             
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:604)                                                                                                                             
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)                                                                                                                                                         
        at weblogic.servlet.internal.StubSecurityHelper$ServletServiceAction.run(StubSecurityHelper.java:227)                                                                                                                   
        at weblogic.servlet.internal.StubSecurityHelper.invokeServlet(StubSecurityHelper.java:125)                                                                                                                              
        at weblogic.servlet.internal.ServletStubImpl.execute(ServletStubImpl.java:292)                                                                                                                                          
        at weblogic.servlet.internal.TailFilter.doFilter(TailFilter.java:26)                                                                                                                                                    
        at weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:42)                                                                                                                                          
        at ch.ejpd.lib.webclient.jfa.JfaTokenServletFilter.doFilter(JfaTokenServletFilter.java:108)                                                                                                                             
        at weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:42)                                                                                                                                          
        at weblogic.servlet.internal.WebAppServletContext$ServletInvocationAction.run(WebAppServletContext.java:3496)                                                                                                           
        at weblogic.security.acl.internal.AuthenticatedSubject.doAs(AuthenticatedSubject.java:321)                                                                                                                              
        at weblogic.security.service.SecurityManager.runAs(Unknown Source)                                                                                                                                                      
        at weblogic.servlet.internal.WebAppServletContext.securedExecute(WebAppServletContext.java:2180)                                                                                                                        
        at weblogic.servlet.internal.WebAppServletContext.execute(WebAppServletContext.java:2086)                                                                                                                               
        at weblogic.servlet.internal.ServletRequestImpl.run(ServletRequestImpl.java:1406)                                                                                                                                       
        at weblogic.work.ExecuteThread.execute(ExecuteThread.java:201)                                                                                                                                                          
        at weblogic.work.ExecuteThread.run(ExecuteThread.java:173)                                                                                                                                                              
javax.transaction.xa.XAException                                                                                                                                                                                                
        at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:171)                                                                                                                                   
        at org.apache.jackrabbit.core.XASessionImpl.commit(XASessionImpl.java:346)                                                                                                                                              
        at org.apache.jackrabbit.jca.TransactionBoundXAResource.commit(TransactionBoundXAResource.java:39)                                                                                                                      
        at weblogic.connector.security.layer.AdapterLayer.commit(AdapterLayer.java:252)                                                                                                                                         
        at weblogic.connector.transaction.outbound.XAWrapper.commit(XAWrapper.java:113)                                                                                                                                         
        at weblogic.transaction.internal.XAServerResourceInfo.commit(XAServerResourceInfo.java:1334)                                                                                                                            
        at weblogic.transaction.internal.XAServerResourceInfo.commit(XAServerResourceInfo.java:578)                                                                                                                             
        at weblogic.transaction.internal.ServerSCInfo.startCommit(ServerSCInfo.java:547)                                                                                                                                        
        at weblogic.transaction.internal.ServerTransactionImpl.localCommit(ServerTransactionImpl.java:2006)                                                                                                                     
        at weblogic.transaction.internal.ServerTransactionImpl.globalRetryCommit(ServerTransactionImpl.java:2723)                                                                                                               
        at weblogic.transaction.internal.ServerTransactionImpl.globalCommit(ServerTransactionImpl.java:2645)                                                                                                                    
        at weblogic.transaction.internal.ServerTransactionImpl.internalCommit(ServerTransactionImpl.java:282)                                                                                                                   
        at weblogic.transaction.internal.ServerTransactionImpl.commit(ServerTransactionImpl.java:230)                                                                                                                           
        at weblogic.ejb.container.internal.BaseLocalObject.postInvoke1(BaseLocalObject.java:591)                                                                                                                                
        at weblogic.ejb.container.internal.BaseLocalObject.postInvokeTxRetry(BaseLocalObject.java:424)                                                                                                                          
        at ch.ejpd.sireneit.facade.ejb.ablage.DokumentFacadeBean_7xdnsq_DokumentFacadeImpl.updateStructuredDokument(DokumentFacadeBean_7xdnsq_DokumentFacadeImpl.java:340)                                                      
        at ch.ejpd.sireneit.access.rest.ablage.DokumentResource.update(DokumentResource.java:453)                                                                                                                               
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)                                                                                                                                                          
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)                                                                                                                                        
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)                                                                                                                                
        at java.lang.reflect.Method.invoke(Method.java:597)                                                                                                                                                                     
        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:175)                                                
        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:67)                                                                                         
        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:208)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:109)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:109)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:109)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:75)                                                                                                                             
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:67)                                                                                                                 
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:724)                                                                                                                
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:689)                                                                                                                 
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:680)                                                                                                                 
        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:324)                                                                                                                                     
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:425)                                                                                                                             
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:604)                                                                                                                             
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)                                                                                                                                                         
        at weblogic.servlet.internal.StubSecurityHelper$ServletServiceAction.run(StubSecurityHelper.java:227)                                                                                                                   
        at weblogic.servlet.internal.StubSecurityHelper.invokeServlet(StubSecurityHelper.java:125)                                                                                                                              
        at weblogic.servlet.internal.ServletStubImpl.execute(ServletStubImpl.java:292)                                                                                                                                          
        at weblogic.servlet.internal.TailFilter.doFilter(TailFilter.java:26)                                                                                                                                                    
        at weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:42)                                                                                                                                          
        at ch.ejpd.lib.webclient.jfa.JfaTokenServletFilter.doFilter(JfaTokenServletFilter.java:108)                                                                                                                             
        at weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:42)                                                                                                                                          
        at weblogic.servlet.internal.WebAppServletContext$ServletInvocationAction.run(WebAppServletContext.java:3496)                                                                                                           
        at weblogic.security.acl.internal.AuthenticatedSubject.doAs(AuthenticatedSubject.java:321)                                                                                                                              
        at weblogic.security.service.SecurityManager.runAs(Unknown Source)                                                                                                                                                      
        at weblogic.servlet.internal.WebAppServletContext.securedExecute(WebAppServletContext.java:2180)                                                                                                                        
        at weblogic.servlet.internal.WebAppServletContext.execute(WebAppServletContext.java:2086)                                                                                                                               
        at weblogic.servlet.internal.ServletRequestImpl.run(ServletRequestImpl.java:1406)                                                                                                                                       
        at weblogic.work.ExecuteThread.execute(ExecuteThread.java:201)                                                                                                                                                          
        at weblogic.work.ExecuteThread.run(ExecuteThread.java:173)                                                                                                                                                              
org.apache.jackrabbit.core.TransactionException: Unable to prepare transaction.                                                                                                                                                 
        at org.apache.jackrabbit.core.state.XAItemStateManager.prepare(XAItemStateManager.java:169)                                                                                                                             
        at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:154)                                                                                                                                   
        at org.apache.jackrabbit.core.XASessionImpl.commit(XASessionImpl.java:346)                                                                                                                                              
        at org.apache.jackrabbit.jca.TransactionBoundXAResource.commit(TransactionBoundXAResource.java:39)                                                                                                                      
        at weblogic.connector.security.layer.AdapterLayer.commit(AdapterLayer.java:252)                                                                                                                                         
        at weblogic.connector.transaction.outbound.XAWrapper.commit(XAWrapper.java:113)                                                                                                                                         
        at weblogic.transaction.internal.XAServerResourceInfo.commit(XAServerResourceInfo.java:1334)                                                                                                                            
        at weblogic.transaction.internal.XAServerResourceInfo.commit(XAServerResourceInfo.java:578)                                                                                                                             
        at weblogic.transaction.internal.ServerSCInfo.startCommit(ServerSCInfo.java:547)                                                                                                                                        
        at weblogic.transaction.internal.ServerTransactionImpl.localCommit(ServerTransactionImpl.java:2006)                                                                                                                     
        at weblogic.transaction.internal.ServerTransactionImpl.globalRetryCommit(ServerTransactionImpl.java:2723)                                                                                                               
        at weblogic.transaction.internal.ServerTransactionImpl.globalCommit(ServerTransactionImpl.java:2645)                                                                                                                    
        at weblogic.transaction.internal.ServerTransactionImpl.internalCommit(ServerTransactionImpl.java:282)                                                                                                                   
        at weblogic.transaction.internal.ServerTransactionImpl.commit(ServerTransactionImpl.java:230)                                                                                                                           
        at weblogic.ejb.container.internal.BaseLocalObject.postInvoke1(BaseLocalObject.java:591)                                                                                                                                
        at weblogic.ejb.container.internal.BaseLocalObject.postInvokeTxRetry(BaseLocalObject.java:424)                                                                                                                          
        at ch.ejpd.sireneit.facade.ejb.ablage.DokumentFacadeBean_7xdnsq_DokumentFacadeImpl.updateStructuredDokument(DokumentFacadeBean_7xdnsq_DokumentFacadeImpl.java:340)                                                      
        at ch.ejpd.sireneit.access.rest.ablage.DokumentResource.update(DokumentResource.java:453)                                                                                                                               
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)                                                                                                                                                          
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)                                                                                                                                        
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)                                                                                                                                
        at java.lang.reflect.Method.invoke(Method.java:597)                                                                                                                                                                     
        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:175)                                                
        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:67)                                                                                         
        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:208)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:109)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:109)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:109)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:75)                                                                                                                             
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:67)                                                                                                                 
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:724)                                                                                                                
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:689)                                                                                                                 
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:680)                                                                                                                 
        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:324)                                                                                                                                     
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:425)                                                                                                                             
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:604)                                                                                                                             
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)                                                                                                                                                         
        at weblogic.servlet.internal.StubSecurityHelper$ServletServiceAction.run(StubSecurityHelper.java:227)                                                                                                                   
        at weblogic.servlet.internal.StubSecurityHelper.invokeServlet(StubSecurityHelper.java:125)                                                                                                                              
        at weblogic.servlet.internal.ServletStubImpl.execute(ServletStubImpl.java:292)                                                                                                                                          
        at weblogic.servlet.internal.TailFilter.doFilter(TailFilter.java:26)                                                                                                                                                    
        at weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:42)                                                                                                                                          
        at ch.ejpd.lib.webclient.jfa.JfaTokenServletFilter.doFilter(JfaTokenServletFilter.java:108)                                                                                                                             
        at weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:42)                                                                                                                                          
        at weblogic.servlet.internal.WebAppServletContext$ServletInvocationAction.run(WebAppServletContext.java:3496)                                                                                                           
        at weblogic.security.acl.internal.AuthenticatedSubject.doAs(AuthenticatedSubject.java:321)                                                                                                                              
        at weblogic.security.service.SecurityManager.runAs(Unknown Source)                                                                                                                                                      
        at weblogic.servlet.internal.WebAppServletContext.securedExecute(WebAppServletContext.java:2180)                                                                                                                        
        at weblogic.servlet.internal.WebAppServletContext.execute(WebAppServletContext.java:2086)                                                                                                                               
        at weblogic.servlet.internal.ServletRequestImpl.run(ServletRequestImpl.java:1406)                                                                                                                                       
        at weblogic.work.ExecuteThread.execute(ExecuteThread.java:201)                                                                                                                                                          
        at weblogic.work.ExecuteThread.run(ExecuteThread.java:173)                                                                                                                                                              
org.apache.jackrabbit.core.state.StaleItemStateException: e1863ec3-4eb7-483b-b1db-7586c089bc64/{}To has been modified externally                                                                                                
        at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:653)                                                                                                                
        at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:1110)                                                                                                                
        at org.apache.jackrabbit.core.state.XAItemStateManager.prepare(XAItemStateManager.java:163)                                                                                                                             
        at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:154)                                                                                                                                   
        at org.apache.jackrabbit.core.XASessionImpl.commit(XASessionImpl.java:346)                                                                                                                                              
        at org.apache.jackrabbit.jca.TransactionBoundXAResource.commit(TransactionBoundXAResource.java:39)                                                                                                                      
        at weblogic.connector.security.layer.AdapterLayer.commit(AdapterLayer.java:252)                                                                                                                                         
        at weblogic.connector.transaction.outbound.XAWrapper.commit(XAWrapper.java:113)                                                                                                                                         
        at weblogic.transaction.internal.XAServerResourceInfo.commit(XAServerResourceInfo.java:1334)                                                                                                                            
        at weblogic.transaction.internal.XAServerResourceInfo.commit(XAServerResourceInfo.java:578)                                                                                                                             
        at weblogic.transaction.internal.ServerSCInfo.startCommit(ServerSCInfo.java:547)                                                                                                                                        
        at weblogic.transaction.internal.ServerTransactionImpl.localCommit(ServerTransactionImpl.java:2006)                                                                                                                     
        at weblogic.transaction.internal.ServerTransactionImpl.globalRetryCommit(ServerTransactionImpl.java:2723)                                                                                                               
        at weblogic.transaction.internal.ServerTransactionImpl.globalCommit(ServerTransactionImpl.java:2645)                                                                                                                    
        at weblogic.transaction.internal.ServerTransactionImpl.internalCommit(ServerTransactionImpl.java:282)                                                                                                                   
        at weblogic.transaction.internal.ServerTransactionImpl.commit(ServerTransactionImpl.java:230)                                                                                                                           
        at weblogic.ejb.container.internal.BaseLocalObject.postInvoke1(BaseLocalObject.java:591)                                                                                                                                
        at weblogic.ejb.container.internal.BaseLocalObject.postInvokeTxRetry(BaseLocalObject.java:424)                                                                                                                          
        at ch.ejpd.sireneit.facade.ejb.ablage.DokumentFacadeBean_7xdnsq_DokumentFacadeImpl.updateStructuredDokument(DokumentFacadeBean_7xdnsq_DokumentFacadeImpl.java:340)                                                      
        at ch.ejpd.sireneit.access.rest.ablage.DokumentResource.update(DokumentResource.java:453)                                                                                                                               
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)                                                                                                                                                          
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)                                                                                                                                        
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)                                                                                                                                
        at java.lang.reflect.Method.invoke(Method.java:597)                                                                                                                                                                     
        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:175)                                                
        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:67)                                                                                         
        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:208)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:109)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:109)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:109)                                                                                                                                  
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:75)                                                                                                                             
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:115)                                                                                                                            
        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:67)                                                                                                                 
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:724)                                                                                                                
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:689)                                                                                                                 
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:680)                                                                                                                 
        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:324)                                                                                                                                     
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:425)                                                                                                                             
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:604)                                                                                                                             
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)                                                                                                                                                         
        at weblogic.servlet.internal.StubSecurityHelper$ServletServiceAction.run(StubSecurityHelper.java:227)                                                                                                                   
        at weblogic.servlet.internal.StubSecurityHelper.invokeServlet(StubSecurityHelper.java:125)                                                                                                                              
        at weblogic.servlet.internal.ServletStubImpl.execute(ServletStubImpl.java:292)                                                                                                                                          
        at weblogic.servlet.internal.TailFilter.doFilter(TailFilter.java:26)                                                                                                                                                    
        at weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:42)                                                                                                                                          
        at ch.ejpd.lib.webclient.jfa.JfaTokenServletFilter.doFilter(JfaTokenServletFilter.java:108)                                                                                                                             
        at weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:42)                                                                                                                                          
        at weblogic.servlet.internal.WebAppServletContext$ServletInvocationAction.run(WebAppServletContext.java:3496)                                                                                                           
        at weblogic.security.acl.internal.AuthenticatedSubject.doAs(AuthenticatedSubject.java:321)                                                                                                                              
        at weblogic.security.service.SecurityManager.runAs(Unknown Source)                                                                                                                                                      
        at weblogic.servlet.internal.WebAppServletContext.securedExecute(WebAppServletContext.java:2180)                                                                                                                        
        at weblogic.servlet.internal.WebAppServletContext.execute(WebAppServletContext.java:2086)                                                                                                                               
        at weblogic.servlet.internal.ServletRequestImpl.run(ServletRequestImpl.java:1406)                                                                                                                                       
        at weblogic.work.ExecuteThread.execute(ExecuteThread.java:201)                                                                                                                                                          
        at weblogic.work.ExecuteThread.run(ExecuteThread.java:173)                                                                                                                                                              
>                                                                                                                                                                                                                               "
1,Node.canAddMixin(String)after the spec this method must return false if the node is locked.
1,"TrecDocMaker skips over documents when ""Date"" is missing from documentsTrecDocMaker skips over Trec documents if they do not have a ""Date"" line. When such a document is encountered, the code may skip over several documents until the next tag that is searched for is found.
The result is, instead of reading ~25M documents from the GOV2 collection, the code reads only ~23M (don't remember the actual numbers).

The fix adds a terminatingTag to read() such that the code looks for prefix, but only until terminatingTag is found. Appropriate changes were made in getNextDocData().

Patch to follow"
1,"IndexWriter.addIndexes* can deadlock in rare casesIn somewhat rare cases it's possible for addIndexes to deadlock
because it is a synchronized method.

Normally the merges that are necessary for addIndexes are done
serially (with the primary thread) because they involve segments from
an external directory.  However, if mergeFactor of these merges
complete then a merge becomes necessary for the merged segments, which
are not external, and so it can run in the background.  If too many BG
threads need to run (currently > 4) then the ""pause primary thread""
approach adopted in LUCENE-1164 will deadlock, because the addIndexes
method is holding a lock on IndexWriter.

This was appearing as a intermittant deadlock in the
TestIndexWriterMerging test case.

This issue is not present in 2.3 (it was caused by LUCENE-1164).

The solution is to shrink the scope of synchronization: don't
synchronize on the whole method & wrap synchronized(this) in the right
places inside the methods."
1,"Error creating DB2 table for JournalingHi guys,

First of all, congratulations for the fantastic job.

I'm deploying a Jackrattbit-JCA resource adapter to a clustered Websphere environment and using a DB2 for storing data and realized some missing code to add DB2 support.

Here is:

JACKRABBIT-CORE, @ org.apache.jackrabbit.core.util.db, method guessValidationQuery (the last one), I adjusted as follows to add a DB2 validation query:

----------------------------------------------------------------------------------------------------------
    private String guessValidationQuery(String url) {
        if (url.contains(""derby"")) {
            return ""values(1)"";
        } else if (url.contains(""mysql"")) {
            return ""select 1"";
        } else if (url.contains(""sqlserver"") || url.contains(""jtds"")) {
            return ""select 1"";
        } else if (url.contains(""oracle"")) {
            return ""select 'validationQuery' from dual"";
        } else if (url.contains(""h2"")) {
            return ""select 1"";
        } else if (url.contains(""db2"")) {
        	return ""values(1)"";
        }
        log.warn(""Failed to guess validation query for URL "" + url);
        return null;
----------------------------------------------------------------------------------------------------------

And as a final touch, a DDL to build the tables:

JACKRABBIT-CORE, @ src/main/resources/org/apache/jackrabbit/core/journal, added a file named db2.dll as follows (actually I copied this from derby.dll)

----------------------------------------------------------------------------------------------------------
#  Licensed to the Apache Software Foundation (ASF) under one or more
#  contributor license agreements.  See the NOTICE file distributed with
#  this work for additional information regarding copyright ownership.
#  The ASF licenses this file to You under the Apache License, Version 2.0
#  (the ""License""); you may not use this file except in compliance with
#  the License.  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an ""AS IS"" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
create table ${schemaObjectPrefix}JOURNAL (REVISION_ID BIGINT NOT NULL, JOURNAL_ID varchar(255), PRODUCER_ID varchar(255), REVISION_DATA blob)
create unique index ${schemaObjectPrefix}JOURNAL_IDX on ${schemaObjectPrefix}JOURNAL (REVISION_ID)
create table ${schemaObjectPrefix}GLOBAL_REVISION (REVISION_ID BIGINT NOT NULL)
create unique index ${schemaObjectPrefix}GLOBAL_REVISION_IDX on ${schemaObjectPrefix}GLOBAL_REVISION (REVISION_ID)
create table ${schemaObjectPrefix}LOCAL_REVISIONS (JOURNAL_ID varchar(255) NOT NULL, REVISION_ID BIGINT NOT NULL)

# Inserting the one and only revision counter record now helps avoiding race conditions
insert into ${schemaObjectPrefix}GLOBAL_REVISION VALUES(0)
----------------------------------------------------------------------------------------------------------


Best regards,


J Marcos"
1,"testIWondiskfull checkindex failurelooks like charlie cron created a corrupt index on disk full.. can't reproduce with the seed on this machine, i can try on that VM with the same environment and see if i have better luck."
1,"multitermquery scoring differences between 3x and trunktry this patch with a test, that applies clean to both 3x and trunk, but fails on trunk.

if you modify the test-data-generator to use TopTerms*BoostOnly* rewrite, then it acts like TestFuzzyQuery2, and passes.

So the problem is in TopTermsScoringBooleanRewrite, or BooleanQuery, or somewhere else.
"
1,"In NRT mode, and CFS enabled, IndexWriter incorrectly ties up disk spaceSpinoff of java-user thread titled ""searching while optimize""...

If IndexWriter is in NRT mode (you've called getReader() at least
once), and CFS is enabled, then internally the writer pools readers.
However, after a merge completes, it opens the reader against het
non-CFS segment files, and pools that.  It then builds the CFS file,
as well, thus tying up the storage for that segment twice.

Functionally the bug is harmless (it's only a disk space issue).
Also, when the segment is merged, the disk space is released again
(though the newly merged segment will also be double-tied-up).

Simple workaround is to use non-CFS mode, or, don't use getReader."
1,"cache getting out of sync with transientstore causes pathnotfoundexceptionDone some further debugging and think the problem is in the synchronization between cache and transientstore. When I retrieve a childnode when I just made its parent node transient (by removing a prop or something), it will not be added to the cache. When I then remove this node, its nodeid is not removed from cache since its stateId wasn't saved in the cache.  After that I add the same node node again with the same name. When I now try to retrieve this node, I get a path not found exception. I see that by retrieving it, its nodeit is resolved from the cache using its path. Only since the removed node was not removed from cache it returns the nodeid of the already removed node. There is no node present with this id in the transientstore and therefor it throws a pathnotfoundexception.

provided a failing junit test and repository.xml"
1,"JCR2SPI: test regression for WorkspaceMoveReferenceableTest.testMoveNodesReferenceableNodesNewUUIDThe latest changes (up to 581637) seems to have broken TCK tests:


-------------------------------------------------------------------------------
Test set: org.apache.jackrabbit.test.TestAll
-------------------------------------------------------------------------------
Tests run: 1037, Failures: 6, Errors: 2, Skipped: 0, Time elapsed: 102.644 sec <<< FAILURE!
testMoveNodesReferenceableNodesNewUUID(org.apache.jackrabbit.test.api.WorkspaceMoveReferenceableTest)  Time elapsed: 0.03 sec  <<< ERROR!
javax.jcr.InvalidItemStateException: Item 'org.apache.jackrabbit.jcr2spi.NodeImpl@13ef9df' doesn't exist anymore
    at org.apache.jackrabbit.jcr2spi.ItemImpl.checkStatus(ItemImpl.java:428)
    at org.apache.jackrabbit.jcr2spi.NodeImpl.getName(NodeImpl.java:120)
    at org.apache.jackrabbit.test.api.WorkspaceMoveReferenceableTest.testMoveNodesReferenceableNodesNewUUID(WorkspaceMoveReferenceableTest.java:57) "
1,"FSDirectory.list() is inconsistentLUCENE-638 added a check to the FSDirectory.list() method to only return files that are Lucene related. I think this change made the FSDirectory implementation inconsistent with all other methods in Directory. E.g. you can create a file with an arbitrary name using FSDirectory, fileExists() will report that it is there, deleteFile() will remove it, but the array returned by list() will not contain the file.

The actual issue that was reported in LUCENE-638 was about sub directories. Those should clearly not be listed, but IMO it is not the responsibility of a Directory implementation to decide what kind of files can be created or listed. The Directory class is an abstraction of a directory and it should't to more than that.
"
1,"ConcurrentModificationException in FineGrainedISMLockingWe have a report where the FineGrainedISMLocking throws a ConcurrentModificationException (stacktrace
from a Jackrabbit 2.2.x):

java.util.ConcurrentModificationException
	at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
	at java.util.HashMap$KeyIterator.next(HashMap.java:828)
	at org.apache.jackrabbit.core.state.FineGrainedISMLocking$LockMap.hasDependency(FineGrainedISMLocking.java:388)
	at org.apache.jackrabbit.core.state.FineGrainedISMLocking.acquireWriteLock(FineGrainedISMLocking.java:138)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.acquireWriteLock(SharedItemStateManager.java:1848)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.access$200(SharedItemStateManager.java:113)
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:563)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:1457)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:1487)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:351)
	at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:354)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:326)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:289)
	at org.apache.jackrabbit.core.ItemSaveOperation.perform(ItemSaveOperation.java:258)
	at org.apache.jackrabbit.core.session.SessionState.perform(SessionState.java:200)
	at org.apache.jackrabbit.core.ItemImpl.perform(ItemImpl.java:91)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:329)
	at org.apache.jackrabbit.core.session.SessionSaveOperation.perform(SessionSaveOperation.java:42)
	at org.apache.jackrabbit.core.session.SessionState.perform(SessionState.java:200)
	at org.apache.jackrabbit.core.SessionImpl.perform(SessionImpl.java:355)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:758)"
1,"NPE Exception Thrown By AbstractJournal During Commit OperationThis seems related to JCR-712 (which was apparently fixed in 1.2.2), but I see the following error now-and-then on JR 1.2.2 (I'm using the DB based journal implementation with Oracle 10g):

java.lang.NullPointerException
        at org.apache.jackrabbit.core.cluster.AbstractJournal.commit(AbstractJournal.java:525)
        at org.apache.jackrabbit.core.cluster.ClusterNode.updateCommitted(ClusterNode.java:424)
        at org.apache.jackrabbit.core.cluster.ClusterNode$WorkspaceUpdateChannel.updateCommitted(ClusterNode.java:565)
        at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:712)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:808)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:326)
        at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:313)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:302)
        at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:308)
        at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1204)
        at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:823)
"
1,"setProperty access control evaluation does not properly cope with XA transactionsThis is another instance of the problems with ACL evaluation within transactions described in https://issues.apache.org/jira/browse/JCR-2999.
In this case PropertyImpl#getParent() called from PropertyImpl#checkSetValue() is trying to check read permissions of the yet uncommited parent and thus fails with an ItemNotFound exception.

The problem is reproducible with the following test:

public void testTransaction() throws Exception {

        // make sure testUser has all privileges
        Privilege[] privileges = privilegesFromName(Privilege.JCR_ALL);
        givePrivileges(path, privileges, getRestrictions(superuser, path));

        // create new node and lock it
        Session s = getTestSession();
        UserTransaction utx = new UserTransactionImpl(s);
        utx.begin();

        // add node and save it
        Node n = s.getNode(childNPath);
        if (n.hasNode(nodeName1)) {
            Node c = n.getNode(nodeName1);
            c.remove();
            s.save();
        }

        // create node and save
        Node n2 = n.addNode(nodeName1);
        s.save(); // -> node is NEW -> no failure

        // set a property on a child node of an uncommited parent
        n2.setProperty(propertyName1, ""testSetProperty"");
        s.save();  // -> fail because PropertyImpl#getParent called from PropertyImpl#checkSetValue
                       //    was checking read permission on the not yet commited parent

        // commit
        utx.commit();
    }"
1,"IndexWriterConfig does not allow readerTermsIndexDivisor to be -1, while the latest indicates the terms index should not be loadedWhile you can pass -1 to IR.open(), and it's documented, you cannot do the same for IndexWriter's readers (b/c IWC blocks it). Need to allow this setting as well as add support for it in our tests, e.g. we should randomly set it to -1. Robert also suggested RandomIW use -1 randomly when it opens readers.

I'll work on a patch"
1,"Digest auth uses wrong uri in proxy authenticationI'm having a problem getting httpclient-rc1 to authenticate using
digest to our IAS server.  I've tried upgrading to rc3 without any
effect.  I also got our IT guys to upgrade IAS without luck.  I was
also able to have the GET method work under IAS and CONNECT to work
with a couple other proxy servers.  After examining ethereal logs for
my (commons) code and firefox to the same URLs I noticed that the
value for the ""uri"" setting in the ""Proxy-Authorization"" header was
the only significant difference.  After looking at RFC 2617 I noticed
that in section 3.2.2 (The Authorization Request Header) it states:

digest-uri
The URI from Request-URI of the Request-Line; duplicated here because
proxies are allowed to change the Request-Line in transit.

A re-examination of the headers showed that firefox was matching the Request-URI
with the digest-uri but that httpclient was not.  I reproduced partial headers
below.  I tried modifying the RC3 source to produce a hard-coded value for ""uri""
and demonstrated that it would successfully authenticate to that URI.  I also
checked that authentication would fail to any other URI and it did.

partial httpclient header (fails with 407):
CONNECT gmail.google.com:443 HTTP/1.1
Proxy-Authorization: Digest username=""proxytest"", realm=""Digest"",
nonce=""503902c343c8c501057a85cea6bad2734378fb44b4cbd1970bf320637871dae85373082cf70ac254"",
uri=""/"", response=""7717d0738332a3d8e83e9102b5ead6b9"", qop=""auth"", nc=00000001,
cnonce=""583aa0469b31290dc2acd7ec6cfc98f1"", algorithm=""MD5-sess"",
opaque=""bb319760fce84856e5648d3536502d81""

partial firefox header (succeeds with 200):
CONNECT mail1.combrio.local:443 HTTP/1.1
Proxy-Authorization: Digest username=""proxytest"", realm=""Digest"",
nonce=""0e61fe645ec8c5015aa3afe8cfe5219488ed473e277a8cddf8225ad66e74fd214f97d9d96ac99991"",
uri=""mail1.combrio.local:443"", algorithm=MD5-sess,
response=""bfac109287273e867531170475172ccf"",
opaque=""70cb2a1533b85882d0f1aa1e2ad1fbae"", qop=auth, nc=00000001,
cnonce=""b41aecd6e527e774"""
1,"same named child nodes disappear on restoreWhen restoring a versionable node which has several (non-versionable) child nodes with the same name, some child nodes disappear. 

            Node node = session.getRootNode().addNode(""myNode"");
            node.addMixin(""mix:versionable"");
            for (int i = 1; i < 6; i++) {
                Node child = node.addNode(""child"");
                child.setProperty(""name"", ""child_""+i);
            }
            session.save();
            VersionManager versionManager = session.getWorkspace().getVersionManager();
            versionManager.checkin(node.getPath());
            System.out.println(""number of child nodes: "" + node.getNodes().getSize());

            versionManager.checkout(node.getPath());
            node.getNode(""child"").setProperty(""name"", ""modified"");
            session.save();
            Version baseVersion = versionManager.getBaseVersion(node.getPath());
            versionManager.restore(baseVersion, true);
            System.out.println(""number of child nodes in restored node: ""+node.getNodes().getSize());


produces the following output:

number of child nodes: 5
number of child nodes in restored node: 3

Giving unique names or adding the mixin versionable to the child nodes solves the problem.
"
1,"HttpMethodBase: Port mismatch in URL for redirect to absolute locationThe CVS version of latka was failing a test ( $ ant test, not maven ), when
using the CVS version of httpclient. The message was

 [java] WARN  [main] org.apache.commons.httpclient.HttpMethod
      - Redirect from port 80 to -1 is not supported: 21 Sep 2002 00:54:32,098

The request preceding this was:

 [java] DEBUG [main] httpclient.wire - >> ""GET /commons HTTP/1.1
     [java] "" [\r\n]: 21 Sep 2002 00:54:31,262
     [java] DEBUG [main] org.apache.commons.httpclient.HttpMethod - enter
[java] DEBUG [main] httpclient.wire - >> ""Host: jakarta.apache.org
     [java] "" [\r\n]: 21 Sep 2002 00:54:31,441
     [java] DEBUG [main] httpclient.wire - >> ""User-Agent: Jakarta
Commons-HttpClient/2.0M1
 [java] "" [\r\n]: 21 Sep 2002 00:54:31,442

And the response was:

 [java] DEBUG [main] org.apache.commons.httpclient.HttpMethod - enter
HttpMethodBase.readStatusLine(HttpState, HttpConnection): 21 Sep 2002 00:54:31,444
     [java] DEBUG [main] org.apache.commons.httpclient.HttpConnection - enter
HttpConnection.readLine(): 21 Sep 2002 00:54:31,444
[java] DEBUG [main] httpclient.wire - << ""HTTP/1.1 301 Moved Permanently""
[\r\n]: 21 Sep 2002 00:54:32,080
     [java] DEBUG [main] org.apache.commons.httpclient.HttpMethod - enter
HttpMethodBase.readResponseHeaders(HttpState,HttpConnection): 21 Sep 2002
00:54:32,086
     [java] DEBUG [main] org.apache.commons.httpclient.HttpConnection - enter
HttpConnection.readLine(): 21 Sep 2002 00:54:32,087
[java] DEBUG [main] httpclient.wire - << ""Date: Fri, 20 Sep 2002 23:54:30 GMT""
[\r\n]: 21 Sep 2002 00:54:32,088
     [java] DEBUG [main] org.apache.commons.httpclient.HttpConnection - enter
HttpConnection.readLine(): 21 Sep 2002 00:54:32,089
[java] DEBUG [main] httpclient.wire - << ""Server: Apache/2.0.42 (Unix)"" [\r\n]:
21 Sep 2002 00:54:32,090
     [java] DEBUG [main] org.apache.commons.httpclient.HttpConnection - enter
HttpConnection.readLine(): 21 Sep 2002 00:54:32,090
[java] DEBUG [main] httpclient.wire - << ""Location:
http://jakarta.apache.org/commons/"" [\r\n]: 21 Sep 2002 00:54:32,091
     [java] DEBUG [main] org.apache.commons.httpclient.HttpConnection - enter
HttpConnection.readLine(): 21 Sep 2002 00:54:32,091
[java] DEBUG [main] httpclient.wire - << ""Content-Length: 319"" [\r\n]: 21 Sep
2002 00:54:32,091
     [java] DEBUG [main] org.apache.commons.httpclient.HttpConnection - enter
HttpConnection.readLine(): 21 Sep 2002 00:54:32,092
[java] DEBUG [main] httpclient.wire - << ""Content-Type: text/html;
charset=iso-8859-1"" [\r\n]: 21 Sep 2002 00:54:32,092
     [java] DEBUG [main] org.apache.commons.httpclient.HttpConnection - enter
HttpConnection.readLine(): 21 Sep 2002 00:54:32,092
[java] DEBUG [main] org.apache.commons.httpclient.HttpMethod - enter
HttpMethodBase.processResponseHeaders(HttpState, HttpConnection): 21 Sep 2002
00:54:32,093
     [java] DEBUG [main] org.apache.commons.httpclient.methods.GetMethod - enter
GetMethod.readResponseBody(HttpState, HttpConnection): 21 Sep 2002 00:54:32,093
     [java] DEBUG [main] org.apache.commons.httpclient.HttpMethod - enter
HttpMethodBase.readResponseBody(HttpState, HttpConnection): 21 Sep 2002 00:54:32,093
     [java] DEBUG [main] org.apache.commons.httpclient.HttpMethod - enter
HttpMethodBase.readResponseBody(HttpState, HttpConnection): 21 Sep 2002 00:54:32,094
     [java] DEBUG [main] org.apache.commons.httpclient.HttpConnection - enter
HttpConnection.getRequestOutputStream(HttpMethod): 21 Sep 2002 00:54:32,094
     [java] DEBUG [main] org.apache.commons.httpclient.HttpMethod - enter
writeRemainingRequestBody(HttpState, HttpConnection): 21 Sep 2002 00:54:32,096
     [java] DEBUG [main] org.apache.commons.httpclient.HttpMethod - Redirect
required: 21 Sep 2002 00:54:32,097
     [java] DEBUG [main] org.apache.commons.httpclient.HttpMethod - Redirect
requested to location 'http://jakarta.apache.org/commons/': 21 Sep 2002 00:54:32,097
     [java] WARN  [main] org.apache.commons.httpclient.HttpMethod - Redirect
from port 80 to -1 is not supported: 21 Sep 2002 00:54:32,098


The problem appears to be in this section of code from HttpMethodBase

if (url == null) {
    //try to construct the new url based on the current url
    try {
        URL currentUrl = new URL(conn.getProtocol(),
                                 conn.getHost(),
                                 conn.getPort(), getPath());
        url = new URL(currentUrl, location);   <--- is this inheriting the port?
    } catch (Exception ex) {
        log.error(""Redirected location '""
                  + locationHeader.getValue()
                  + ""' is malformed"");
        return statusCode;
    }
}"
1,"Property.setValue(InputStream) closes streamCurrently the Property.setValue(InputStream) - actually all methods setting a property value from an InputStream - method closes the stream when it has completely been read. While this might be a nice-to-have in some situations, it is IMHO not standard behaviour for stream consumers to close the stream when done.

My special use case is unpacking the contents of a ZIP file (ZIPInputStream). After streaming the contents of the first ZIP file entry into a property, the ZIPInputStream is closed by Jackrabbit and the rest of the file cannot be read.

Workaround: Instead of giving the original InputStream to the method, create a FileInputStream wrapper overwriting the close method to do nothing."
1,Add delete term and query need to more precisely record the bytes usedDocumentsWriter's add delete query and add delete term add to the number of bytes used regardless of the query or term already existing in the respective map.
1,"NullPointerException on startup if IndexingQueue has pending nodesThis happens because of the newly introduced index version, which is not yet set when the IndexingQueue is instanciated."
1,Connections are not release when a recoverable exception occurs.Please see the url for discussion details.
1,"Text.isDescendant returns false if parent is '/'the method isDescendant(String, String) of the Text utility class returns false if the 
passed potential parent-path represents the root node (""/"").

"
1,"PlainTextExtractor returns an empty reader when encoding is unsupportedPlainTextExtractor is failing to index text files.  Searching for content in text files is not coming back with results.

On the extractText(InputStream stream, String type, String encoding) method, the encoding is coming in as an empty string, and it throws the java.io.UnsupportedEncodingException at line 40 ( return new InputStreamReader(stream, encoding); ).

modifying the following statement fixes the problem:
before:  if (encoding != null) {
after:  if (encoding != null && !encoding.equals("""")) {"
1,"Can no longer set a Date property using a Long valueAttempting to set a Date property with a Long value throws a javax.jcr.nodetype.ConstraintViolationException. This worked in Jackrabbit 1.6.2.

To reproduce:
  Node node = session.getItem(""/"");
  node = node.addNode(""dummy"", ""nt:resource"");
  ValueFactory vf = session.getValueFactory();
  Value = vf.createValue(""1234"", 3); // Create a LongValue
  node.setProperty(""jcr:lastModified"", value);
  System.out.println(node.getProperty(""jcr:lastModified""));

Expected result:
- A date around 1970 is printed to System.out

Actual result:
  javax.jcr.nodetype.ConstraintViolationException: no matching property definition found for {http://www.jcp.org/jcr/1.0}lastModified
       at org.apache.jackrabbit.core.nodetype.EffectiveNodeType.getApplicablePropertyDef(EffectiveNodeType.java:770)
       at org.apache.jackrabbit.core.NodeImpl.getApplicablePropertyDefinition(NodeImpl.java:911)
       at org.apache.jackrabbit.core.ItemManager.getDefinition(ItemManager.java:224)
       at org.apache.jackrabbit.core.ItemData.getDefinition(ItemData.java:97)
       at org.apache.jackrabbit.core.PropertyData.getPropertyDefinition(PropertyData.java:53)
       at org.apache.jackrabbit.core.PropertyImpl.getDefinition(PropertyImpl.java:729)
       at org.apache.jackrabbit.core.NodeImpl.setProperty(NodeImpl.java:2512)

According to Jukka Zitting [1], this might be a side-effect of JCR-2170.

[1] Mail thread from dev@jackrabbit.apache.org: http://markmail.org/message/hn3snufsogjvldad"
1,"HttpClient incorrectly handles Transfer-Encoding headerRFC2616, section 4.4 item 3 states:
     If a Content-Length header field (section 14.13) is present, its
     decimal value in OCTETs represents both the entity-length and the
     transfer-length. The Content-Length header field MUST NOT be sent
     if these two lengths are different (i.e., if a Transfer-Encoding
     header field is present). If a message is received with both a
     Transfer-Encoding header field and a Content-Length header field,
     the latter MUST be ignored.

This is not handled correctly in the case that a noncompliant HTTP server
returns both a Transfer-Encoding header and a Content-Length header.

I gave up on writing a TestCase for this as it would require a reliably
noncompliant HTTP Server."
1,"Problem with child order after restoring of parentThe following sequence leads to swapped child nodes in the mentioned trunk version (this worked in 1.0.1).
Specifically:

Add nodes:
 parent
   childA
   childB

Remove childA:
 parent
   childB

Restore initial version:
 parent
   childB
   childA

The parent node is of type nt:unstructured which carries an
OrderableChildNodes=true so I would assume that upon restoring the order
would be preserved.

Cheers,
Tanju

--------------

TESTCASE used with both Derby & InMemPMs (boiled down beyond sense :)

// Add parent & childA, childB
Node parent = session.getRootNode().addNode(""parent"", ""nt:unstructured"");
parent.addMixin(""mix:versionable"");
Node c1 = parent.addNode(""childA"", ""nt:unstructured"");
c1.addMixin(""mix:versionable"");
Node c2 = parent.addNode(""childB"", ""nt:unstructured"");
c2.addMixin(""mix:versionable"");
session.save();
c1.checkin();
c2.checkin();
Version v1 = parent.checkin();
// OK : parent.getNodes() -> childA, childB

// Remove childA
parent = session.getRootNode().getNode(""parent"");
parent.checkout();
c1 = parent.getNodes().nextNode();
c1.checkout();
c1.remove();
session.save();
Version v2 = parent.checkin();
// OK : parent.getNodes() -> childA, childB

// Remove childA
parent = session.getRootNode().getNode(""parent"");
parent.restore(v1, true);
// Not OK : parent.getNodes() -> childB, childA"
1,"Repository lock keeps file openThe RepositoryLock opens a RandomAccessFile, but does not close it. The problematic line is:

lock = new RandomAccessFile(file, ""rw"").getChannel().tryLock();

This is usually not a problem as the file will be closed when the RandomAccessFile object is garbage collected. However, if called a lot in a short time frame, this results in 'too many open files' in some environments (for example Linux). "
1,"Freezes w/ MultiThreadedHttpConnectionManagerMy single threaded user of VFS (an HttpClient user, that uses
MultiThreadedHttpConnectionManager) hangs [I suspect indefinitely] on minor
activity.

I've turned on HttpClient debug and I see this, the last line
being the last thing I get...

2003/10/09 09:34:26:482 MDT [DEBUG] wire - -<< ""Content-Type:
text/html[\r][\n]""
2003/10/09 09:34:26:482 MDT [DEBUG] HttpMethodBase - -Resorting to protocol
version default close co
nnection policy
2003/10/09 09:34:26:492 MDT [DEBUG] HttpMethodBase - -Should NOT close
connection, using HTTP/1.1
2003/10/09 09:34:26:502 MDT [DEBUG] HttpMethodDirector - -Execute loop try 1
2003/10/09 09:34:26:512 MDT [DEBUG]
MultiThreadedHttpConnectionManager - -HttpConnectionManager.getC
onnection:  config = HostConfiguration[host=www.ibiblio.org,
protocol=http:80, port=80], timeout = 0

2003/10/09 09:34:26:522 MDT [DEBUG]
MultiThreadedHttpConnectionManager - -Unable to get a connection
, waiting..., hostConfig=HostConfiguration[host=www.ibiblio.org,
protocol=http:80, port=80]

This is pretty reproducible. When I hack VFS not to use the
MultiThreadedHttpConnectionManager I don't get the problem."
1,"MockRandomMergePolicy optimizes segments not in the Set passed inThe test class MockRandomMergePolicy shuffles the whole SegmentInfos passed to the optimize callback and returns random segments for optimizing. This is fine, but it also returns segments, that are not listed in the Set<SegmentInfo> that is also passed in, containing the subset of segments to optimize.

This bug was found when writing a testcase for LUCENE-3082: The wrapper MergePolicy (when wrapped around MockRandomMergePolicy) only passes a subset of the segments to the delegate (the ones that are in old index format). But MockRandom created OneMerge in its return MergeSpecification having segments outside this set."
1,"Incorrect parsing by QueryParser.parse() when it encounters backslashes (always eats one backslash.)Test code and output follow. Tested  Lucene 1.9 version only. Affects hose who would index/search for Lucene's reserved characters.

Description: When an input search string has a sequence of N (java-escaped) backslashes, where N >= 2, the QueryParser will produce a query in which that sequence has N-1 backslashes.

TEST CODE:
    Analyzer analyzer = new WhitespaceAnalyzer();
    String[] queryStrs = {""item:\\\\"",
                          ""item:\\\\*"",
                          ""(item:\\\\ item:ABCD\\\\))"",
                          ""(item:\\\\ item:ABCD\\\\)""};
    for (String queryStr : queryStrs) {
      System.out.println(""--------------------------------------"");
      System.out.println(""String queryStr = "" + queryStr);
      Query luceneQuery = null;
      try {
        luceneQuery = new QueryParser(""_default_"", analyzer).parse(queryStr);
        System.out.println(""luceneQuery.toString() = "" + luceneQuery.toString());
      } catch (Exception e) {
        System.out.println(e.getClass().toString());
      }
    }

OUTPUT (with remarks in comment notation:) 
--------------------------------------
String queryStr = item:\\
luceneQuery.toString() = item:\             //One backslash has disappeared. Searcher will fail on this query.
--------------------------------------
String queryStr = item:\\*
luceneQuery.toString() = item:\*           //One backslash has disappeared. This query will search for something unintended.
--------------------------------------
String queryStr = (item:\\ item:ABCD\\))
luceneQuery.toString() = item:\ item:ABCD\)     //This should have thrown a ParseException because of an unescaped ')'. It did not.
--------------------------------------
String queryStr = (item:\\ item:ABCD\\)
class org.apache.lucene.queryParser.ParseException        //...and this one should not have, but it did.

"
1,"Nullpointer when creating URI from scheme specific part with null fragmentin org.apache.commons.httpclient.URI class constructor:

public URI(String scheme, String schemeSpecificPart, String fragment)
        throws URIException {
....
_fragment = fragment.toCharArray(); 

should be 

_fragment = fragment==null ? null : fragment.toCharArray();"
1,LevenshteinDistance code normalization is incorrectThe normalization of the edit distance should use the maximum of the two string being compared instead of the minimum.  Otherwise negative distances are possible.  The spell checker filters out edits below a certain threshold so this hasn't been a problem in practice.
1,"MockRAMDirectory (used only by unit tests) has some synchronization problemsComing out of a failure that Earwin noted on java-dev this morning, I reworked the synchronization on MockRAMDirectory."
1,"Query throws UnsupportedOperationExceptionWhen executing an absolute XPath statement where the first location step is not jcr:root the Query may throw an UnsupportedOperationException:

Query: /foo//element(*, nt:unstructured)[@prop = 'bar']

Stacktrace:
java.lang.UnsupportedOperationException
        at org.apache.jackrabbit.core.query.lucene.CachingMultiReader$MultiTermDocs.skipTo(CachingMultiReader.java:281)
        at org.apache.lucene.search.TermScorer.skipTo(TermScorer.java:88)
        at org.apache.lucene.search.ConjunctionScorer.doNext(ConjunctionScorer.java:53)
        at org.apache.lucene.search.ConjunctionScorer.next(ConjunctionScorer.java:48)
        at org.apache.lucene.search.Scorer.score(Scorer.java:37)
        at org.apache.jackrabbit.core.query.lucene.ChildAxisQuery$ChildAxisScorer.calculateChildren(ChildAxisQuery.java:291)
        at org.apache.jackrabbit.core.query.lucene.ChildAxisQuery$ChildAxisScorer.next(ChildAxisQuery.java:251)
        at org.apache.lucene.search.Scorer.score(Scorer.java:37)
        at org.apache.jackrabbit.core.query.lucene.DescendantSelfAxisQuery$DescendantSelfAxisScorer.calculateSubHits(DescendantSelfAxisQuery.java:302)
        at org.apache.jackrabbit.core.query.lucene.DescendantSelfAxisQuery$DescendantSelfAxisScorer.next(DescendantSelfAxisQuery.java:237)
        at org.apache.lucene.search.Scorer.score(Scorer.java:37)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:92)
        at org.apache.lucene.search.Hits.getMoreDocs(Hits.java:64)
        at org.apache.lucene.search.Hits.<init>(Hits.java:43)
        at org.apache.lucene.search.Searcher.search(Searcher.java:33)
        at org.apache.lucene.search.Searcher.search(Searcher.java:27)
        at org.apache.jackrabbit.core.query.lucene.SearchIndex.executeQuery(SearchIndex.java:287)
        at org.apache.jackrabbit.core.query.lucene.QueryImpl.execute(QueryImpl.java:179)
        at org.apache.jackrabbit.core.query.QueryImpl.execute(QueryImpl.java:132)"
1,"Escaped quotes inside a phrase cause a ParseExceptionQueryParser cannot handle escaped quotes when inside a phrase. Escaped quotes not in a phrase are not a problem. This can be added to TestQueryParser.testEscaped() to demonstrate the issue - the second assert throws an exception:

assertQueryEquals(""a \\\""b c\\\"" d"", a, ""a \""b c\"" d"");
assertQueryEquals(""\""a \\\""b c\\\"" d\"""", a, ""\""a \""b c\"" d\"""");

See also this thread:
http://www.nabble.com/ParseException-with-escaped-quotes-in-a-phrase-t1647115.html
"
1,"Change Primitive Data Types from int to long in class SegmentMerger.javaHi

We are getting an exception while optimize. We are getting this exception ""mergeFields produced an invalid result: docCount is 385282378 but fdx file size is 3082259028; now aborting this merge to prevent index corruption""
 
I have  checked the code for class SegmentMerger.java and found this check 

***********************************************************************************************************************************************************************
if (4+docCount*8 != fdxFileLength)
        // This is most likely a bug in Sun JRE 1.6.0_04/_05;
        // we detect that the bug has struck, here, and
        // throw an exception to prevent the corruption from
        // entering the index.  See LUCENE-1282 for
        // details.
        throw new RuntimeException(""mergeFields produced an invalid result: docCount is "" + docCount + "" but fdx file size is "" + fdxFileLength + ""; now aborting this merge to prevent index corruption"");
}
***********************************************************************************************************************************************************************

In our case docCount is 385282378 and fdxFileLength size is 3082259028, even though 4+385282378*8 is equal to 3082259028, the above code will not work because number 3082259028 is out of int range. So type of variable docCount needs to be changed to long

I have written a small test for this 

************************************************************************************************************************************************************************

public class SegmentMergerTest {
public static void main(String[] args) {
int docCount = 385282378; 
long fdxFileLength = 3082259028L; 
if(4+docCount*8 != fdxFileLength) 
System.out.println(""No Match"" + (4+docCount*8));
else 
System.out.println(""Match"" + (4+docCount*8));
}
}

************************************************************************************************************************************************************************

Above test will print No Match but if you change the data type of docCount to long, it will print Match

Can you please advise us if this issue will be fixed in next release?

Regards
Deepak







 



"
1,"SamplingWrapperTest failure with certain test seedBuild: https://builds.apache.org/job/Lucene-Solr-tests-only-trunk/12231/

1 tests failed.
REGRESSION:  org.apache.lucene.facet.search.SamplingWrapperTest.testCountUsingSamping

Error Message:
Results are not the same!

Stack Trace:
org.apache.lucene.facet.FacetTestBase$NotSameResultError: Results are not the same!
       at org.apache.lucene.facet.FacetTestBase.assertSameResults(FacetTestBase.java:333)
       at org.apache.lucene.facet.search.sampling.BaseSampleTestTopK.assertSampling(BaseSampleTestTopK.java:104)
       at org.apache.lucene.facet.search.sampling.BaseSampleTestTopK.testCountUsingSamping(BaseSampleTestTopK.java:82)
       at org.apache.lucene.util.LuceneTestCase$3$1.evaluate(LuceneTestCase.java:529)
       at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:165)
       at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)

NOTE: reproduce with: ant test -Dtestcase=SamplingWrapperTest -Dtestmethod=testCountUsingSamping -Dtests.seed=4a5994491f79fc80:-18509d134c89c159:-34f6ecbb32e930f7 -Dtests.multiplier=3 -Dargs=""-Dfile.encoding=UTF-8""
NOTE: test params are: codec=Lucene40: {$facets=PostingsFormat(name=MockRandom), $full_path$=PostingsFormat(name=MockSep), content=Pulsing40(freqCutoff=19 minBlockSize=65 maxBlockSize=209), $payloads$=PostingsFormat(name=Lucene40WithOrds)}, sim=RandomSimilarityProvider(queryNorm=true,coord=true): {$facets=LM Jelinek-Mercer(0.700000), content=DFR I(n)B3(800.0)}, locale=bg, timezone=Asia/Manila
"
1,"PathNotFoundException but item existsThe following test case (for jcr2spi) throws a PathNotFoundException for an item which exists. It does not throw if the marked line below is commented out. 

public void testBug24687() throws RepositoryException {
    String parentPath = testNode.getPath();
    String folderName = ""folder_"" + System.currentTimeMillis();
    Session session = getHelper().getReadWriteSession();

    Session session2 = getHelper().getReadOnlySession();
    session2.getItem(parentPath);  // removing this line makes the failure go away

    Node parent = (Node) session.getItem(parentPath);
    Node toDelete = parent.addNode(folderName, ""nt:folder"");
    parent.save();

    try {
        Item item2 = session2.getItem(parentPath + ""/"" + folderName);  // wrongly throws PathNotFoundException
        assertEquals(parentPath + ""/"" + folderName, item2.getPath());
    }
    finally {
        toDelete.remove();
        parent.save();
        assertFalse(parent.hasNode(folderName));
    }
}
"
1,"builtin_nodetypes.cnd contains non-ascii chars and is not correctly decoded using webspherejackrabbit can't be started within websphere with the following error:

Caused by: 
org.apache.jackrabbit.spi.commons.nodetype.compact.ParseException: IOException
while attempting to read input stream
(org/apache/jackrabbit/core/nodetype/builtin_nodetypes.cnd, line 272)
.
.
Caused by: 
sun.io.MalformedInputException
	at sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:262)
	at sun.nio.cs.StreamDecoder$ConverterSD.convertInto(StreamDecoder.java:314)
	at sun.nio.cs.StreamDecoder$ConverterSD.implRead(StreamDecoder.java:345)


I quickly checked the file and it contains some non ascii-7 characters. actually it should be stored, packaged and read as utf8. somthing was list in translation.
the simplest fix would be to remove all non-ascii7 characters from the file."
1,"Registering NodeType from templates throws exception about invalid decl. node type.when adding PropertyDefinitionTemplates to NodeTypeTemplates, the internal declaredNodeType field is not set and causes the registration fail with:

org.apache.jackrabbit.api.jsr283.nodetype.InvalidNodeTypeDefinitionException: [{}foo#{}test] invalid declaring node type specified
	at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.registerNodeTypes(NodeTypeManagerImpl.java:695)
	at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.registerNodeType(NodeTypeManagerImpl.java:615)"
1,Fix for NPE's in Spatial Lucene for searching bounding box onlyNPE occurs when using DistanceQueryBuilder for minimal bounding box search without the distance filter.
1,"Superfluous AndQueryNode  in query tree built by SQL parserTest query (tested with <http://people.apache.org/~mreutegg/jcr-query-translator/translator.html>):

  SELECT * FROM nt:folder WHERE x = 1 

generates the following query tree:

+ Root node
+ Select properties: *
  + PathQueryNode
    + LocationStepQueryNode:  NodeTest=* Descendants=true Index=NONE
      + AndQueryNode
        + RelationQueryNode: Op: =  Prop=@{}x Type=LONG Value=1
      + NodeTypeQueryNode:  Prop={http://www.jcp.org/jcr/1.0}primaryType Value={http://www.jcp.org/jcr/nt/1.0}folder

It seems the AndQueryNode is superfluous.
"
1,"rep:excerpt() not working for attribute searchesexample: //element(*, nt:unstructured)[jcr:contains(@textProp, 'foobar')]/(rep:excerpt())

produces empty excerpts."
1,"Open-scoped locks may be lost on restart and might not be transferrableTwo issues with open-scoped locks were reported by Cdric Damioli:

(1) When open-scoped locks are being reapplied on repository startup, locks on non-referenceable nodes are lost.
(2) When a session holding an open-scoped lock logs out, the lock token is not automatically removed from the session and other sessions are not able to take responsibility for the lock, even when having the correct lock token.
"
1,"[contrib-bdb] initialization fails if directory doesn't existBerkeleyDBPersistenceManager initialization fails if the directory configured doesn't exist (this doesn't happen with other PMs).
This can easily be fixed in the persistence manager, by making it create all the directories in the path (actually it only creates the last -db- directory).

The trivial patch is to replace envDir.mkdir() to envDir.mkdirs() (note the final ""s"") at BerkeleyDBPersistenceManager  line 73:
        if (!envDir.exists())
            envDir.mkdir();
should be:
        if (!envDir.exists())
            envDir.mkdirs();

(I am not submitting any svn diff since the manual fix sounds so trivial, it's easier to change it manually)
"
1,"HttpMultiClient reuses closed connectionsIf a socket times out while sitting in the connection pool, 
HttpConnectionManager still attempts to reuse it resulting in an IOException 
being thrown when writing to the socket.  I believe this is a problem with both 
server side and client side timeouts (ie: we try to reuse a connection that we 
timed out) though am not certain of that.  At the very least server side 
timeouts cause the issue.

As yet I can't see how to fix this.  With the current code there doesn't even 
appear to be a suitable workaround because when the exception is thrown, the 
connection is added back into the pool to be reused (even though it is closed) 
which causes the next attempt to fail as well.

I can't see any reliable way to tell whether or not a connection is open, so 
would suggest the following as a fix:

1. In HttpMultiClient.executeMethod, close the connection if an exception is 
thrown (optionally, only if an IOException is thrown instead of an 
HttpException, but generally exceptions tend to leave things in an unknown 
state).

2. (optional) Add a retry loop to executeMethod to retry if an exception occurs 
(possibly only if an IOException is thrown, depending on exactly when a 
HttpException is thrown).

I'll attach a patch which does both of this to help clarify."
1,"UserInfo disapears after creating URII tested this using firefox (Where I have configured our proxy server)
I run the following URI: ftp://username:password@ftp.mytest.test/testdir/

I use a sniffer to look at the GET commond send to the proxy server. It looks as
follows:

GET ftp://username:password@ftp.mytest.test/testdir/ HTTP/1.1
Host: ftp.mytest.test
User-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.0; en-US; rv:1.7.12)
Gecko/20050915 Firefox/1.0.7
Accept:
text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain;q=0.8,image/png,*/*;q=0.5
Accept-Language: en-us,en;q=0.5
Accept-Encoding: gzip,deflate
Accept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7
Keep-Alive: 300
Proxy-Connection: keep-alive

Using this request we get access to the directory and see the contents displayed.
However, when I try the same in Java (using HttpClient) I get the following GET
request (Java code included below):

GET ftp://ftp.mytest.test/testdir/ HTTP/1.1
User-Agent: Jakarta Commons-HttpClient/3.0-rc3
Host: ftp.mytest.test
Proxy-Connection: Keep-Alive

Finally I get a ACCESS DENIED error. 
This seems to be because the GET request does not contain the USER / PASSWORD
info in the URL.

/// JAVA CODE:

package nl.essent.test.ftp.httptest;

import java.io.IOException;

import org.apache.commons.httpclient.Credentials;
import org.apache.commons.httpclient.DefaultHttpMethodRetryHandler;
import org.apache.commons.httpclient.HostConfiguration;
import org.apache.commons.httpclient.HttpClient;
import org.apache.commons.httpclient.HttpException;
import org.apache.commons.httpclient.HttpMethod;
import org.apache.commons.httpclient.HttpStatus;
import org.apache.commons.httpclient.NTCredentials;
import org.apache.commons.httpclient.UsernamePasswordCredentials;
import org.apache.commons.httpclient.auth.AuthScope;
import org.apache.commons.httpclient.methods.GetMethod;
import org.apache.commons.httpclient.params.HttpMethodParams;
import org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory;
import org.apache.commons.httpclient.protocol.Protocol;

public class TestClient {

    public static void main(String[] args) {
        new TestClient().testFtpViaHttp();
    }
    
    public void testFtpViaHttp() {
        
        HttpClient client = new HttpClient();
        
        HostConfiguration hostConfig = client.getHostConfiguration();
        hostConfig.setProxy(""proxy"", 8080);
        client.setHostConfiguration(hostConfig);
        
        Protocol protol = new Protocol(""ftp"", new
DefaultProtocolSocketFactory(), 21);
        Protocol.registerProtocol(""ftp"", protol);
        
        Credentials proxyCreds = new NTCredentials(""xxxx"", ""xxxxx"","""", ""xxxx"" );
        client.getState().setProxyCredentials(AuthScope.ANY, proxyCreds);
        
        GetMethod gmethod = new
GetMethod(""ftp://username:password@ftp.mytest.test/testdir/"");
        
        gmethod.getParams().setParameter(HttpMethodParams.RETRY_HANDLER, 
                new DefaultHttpMethodRetryHandler(3, false));
       
        try {
            // Execute the method.
            int statusCode = client.executeMethod(gmethod);

            if (statusCode != HttpStatus.SC_OK) {
              System.err.println(""Method failed: "" + gmethod.getStatusLine());
            }

            // Read the response body.
            byte[] responseBody = gmethod.getResponseBody();

            // Deal with the response.
            // Use caution: ensure correct character encoding and is not binary data
            System.out.println(new String(responseBody));

          } catch (HttpException e) {
            System.err.println(""Fatal protocol violation: "" + e.getMessage());
            e.printStackTrace();
          } catch (IOException e) {
            System.err.println(""Fatal transport error: "" + e.getMessage());
            e.printStackTrace();
          } finally {
            // Release the connection.
            gmethod.releaseConnection();
          } 

    }

}

//// END JAVA CODE"
1,"SnapshotDeletionPolicy.snapshot() throws NPE if no commits happenedSDP throws NPE if no commits occurred and snapshot() was called. I will replace it w/ throwing IllegalStateException. I'll also move TestSDP from o.a.l to o.a.l,index. I'll post a patch soon"
1,"Remove FieldMaskingSpanQuery (or fix its scoring)In Lucene 4.0 we added new scoring mechanisms, but FieldMaskingSpanQuery is a serious problem:

Because it lies about the fields of its terms, this sometimes results in totally bogus
statistics, cases where a single terms totalTermFreq exceeds sumTotalTermFreq for the entire field (since its lying about it).

Such lying could result in NaN/Inf/Negative scores, exceptions, divide by zero, and other problems,
because the statistics are impossibly bogus."
1,"JCR-Server: respect maximal value for timeoutRFC 2518 states:

""The timeout value for TimeType ""Second"" MUST NOT be greater than 2^32-1.""

->> adjust constant according.

BTW: sending 'Infinite' timeout in case of maximal value causes problems with microsoft builtin client, that will never unlock that resource."
1,Using the WeightedTerms option in the Highlighter can cause fragments to be supressed for indexes with deletesAn index with a few documents and many deletes can report a lower total docs than docFreq for a term - total docs will account for deletes while docFreq will not - this causes the idf to be negative and the fragment to score < 0.
1,"QueryResult's RowIterator.getSize returned the wrong size of the results after I implemented my own AccessManagerThe background is I have implemented my own AccessManager. After executing a query and get back the RowIterator from the result, if I call rowiterator.getSize, it will return the size of all nodes matching my query (without honoring the access control) . But if I iterate through the result, I find lots of duplicates in the results; and if I filter out those duplicate, the final result is quite off the original number from RowIteartor.getSize()

BTW, I also disabled Doc Order sorting.

 "
1,"EnhancementsPayloadIterator.getCategoryData(CategoryEnhancement) problematic usage of Object.equals()EnhancementsPayloadIterator has an internal list of category enhancemnets, and in getCategoryData(CategoryEnhancement) there is a lookup of the given CategoryEnhancement in the list. In order to make sure this lookup works, CategoryEnhancement must override Object.equals(Object)."
1,"HttpClient does not correctly handle escaped characters in HTTP header elementsAn excerpt from Microsoft's ""How Digest Authentication Works"":
http://www.microsoft.com/technet/prodtechnol/windowsserver2003/library/TechRef/717b450c-f4a0-4cc9-86f4-cc0633aae5f9.mspx

<quote>
* RFC 2617-compliant Digest Authentication challenges and responses must also
comply with RFC 2616: Hypertext Transfer Protocol -- HTTP/1.1 quoted string
requirements. This requirement particularly affects the use of backslash (\) and
embedded double quotes. Both must be preceded (escaped) with a backslash.

* For example, domain\username according to RFC 2616 is read as domainusername.
This reading is important because if an application sends information in this
format rather than as domain\\username, authentication fails.

* However, because this is a known issue with domain\username , if
authenticating with backslash encoding fails, Digest SSP attempts to
authenticate the response and assumes that the backslash is part of the string.
This behavior can be turned off by setting the ServerCompat registry key.
</quote>

Review and fix the ParameterParser class and classes implemeting CookieSpec or
AuthScheme interfaces

See also PR #34909"
1,"Nodes that have properties marked for async extraction should be available for queryingThe problems only appears when dealing with nodes that have async extractors. In this case we return a lightweight copy of the node (without the property that will be processed in the background).

The copy algorithm ignores certain field types (that have been probably introduced during the Lucene 3 upgrade, not sure) such as SingletonTokenStream(s).
So the lightweight copy does not include all the existing properties, therefore the node will not appear in queries during the extraction time."
1,"WriteLineDocTask should keep docs w/ just title and no bodyWriteLineDocTask throws away a document if it does not have a body element. However, if the document has a title, then it should be kept. Some documents, such as emails, may not have a body which is legitimate. I'll post a patch + a test case."
1,"JcrParser: use of bitwise instead of logical AND operatorJcrParser, line 134:

            if ((!insideSingleQuote & !insideDoubleQuote & Character
"
1,"Incorrect handling of InputStreams when connecting to a server that requires authenticationI'm trying to upload a file to a WebDav server (mod_dav on Apache Web Server 2.2.14) that has basic (or digest, the result is the same) authentication enabled.
I'm using the following code:
        String url = ""http://myserver/dir/test2.gif"";
        File file = new File(""d:/test2.gif"");
        DefaultHttpClient httpClient = new DefaultHttpClient();
        HttpPut put = new HttpPut(url);
        put.setEntity(new InputStreamEntity(new FileInputStream(file), file.length()));
        
        URI uri = put.getURI();
        httpClient.getCredentialsProvider().setCredentials(new AuthScope(uri.getHost(), uri.getPort()),
                getCredentials());
        put.getParams().setBooleanParameter(CoreProtocolPNames.USE_EXPECT_CONTINUE, true);
        HttpResponse response = httpClient.execute(put);
        System.out.println(response.getStatusLine());

When running the above code, I'm getting a org.apache.http.client.NonRepeatableRequestException: Cannot retry request with a non-repeatable request entity. I tested both the latest alpha & the svn head. Doing the same thing in HttpClient 3.1 worked as expected. 

This could be normal, as I'm using an InputStream that is indeed not repeatable, but as I'm also using Expect: 100-Continue, the stream shouldn't have been consumed with the first connection (the one that gets a code 401 from the WebDav server), and only in the second one, when the credentials are provided.

The problem is that DefaultRequestDirector.execute doesn't take this into account and assumes that if a request has been tried once, its associated entity (if any) has been consumed.
Here's the fix that I came up with:
Change DefaultRequestDirector.execute so that if the wrapper is an EntityEnclosingRequestWrapper, it checks if the entity has actually been consumed before throwing a NonRepeatableRequestException. I'm using the method isStreaming() from HttpEntity, as it's the closest thing to what I was looking for. Reading the JavaDoc, it could lead to the situation where an entity has started streaming but has not yet finished, and so is not in a state where it can be used. However I don't think that's a problem as the javadoc for HttpEntity.getContent() states that it can't be called two times on a non-repeatable entity, so it's just a matter of when the request will fail.
This lead me to also modify InputStreamEntity (from the httpCore project) as it didn't comply with the javadoc. With these two modifications, The file upload completes successfully.

I also modified:
 * TestInputStreamEntity.testBasics() (from the httpCore project) test so that it complies with getContent()'s Javadoc.
 * TestDefaultClientRequestDirector.FaultyHttpRequestExecutor because it didn't consume the entity's content.
All the tests from both httpCore and httpClient pass.
I tested both InputStreamEntity and BasicHttpEntity.
 
Please keep in mind that I am by no means an httpClient (or http, for that matter) expert, and these modifications may have some unexpected side-effects that I did not foresee, contain plain dumb code, or whatever, so it would be great if someone could review my changes and give their opinion.
"
1,"MultiIndexDocValues pretends it can merge sorted sourcesNightly build hit this failure:

{noformat}
ant test-core -Dtestcase=TestSort -Dtestmethod=testReverseSort -Dtests.seed=791b126576b0cfab:-48895c7243ecc5d0:743c683d1c9f7768 -Dtests.multiplier=3 -Dargs=""-Dfile.encoding=ISO8859-1""

    [junit] Testcase: testReverseSort(org.apache.lucene.search.TestSort):	Caused an ERROR
    [junit] expected:<[CEGIA]> but was:<[ACEGI]>
    [junit] 	at org.apache.lucene.search.TestSort.assertMatches(TestSort.java:1248)
    [junit] 	at org.apache.lucene.search.TestSort.assertMatches(TestSort.java:1216)
    [junit] 	at org.apache.lucene.search.TestSort.testReverseSort(TestSort.java:759)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$3$1.evaluate(LuceneTestCase.java:523)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:149)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:51)
{noformat}

It's happening in the test for reverse-sort of a string field with DocValues, when the test had gotten SlowMultiReaderWrapper.

I committed a fix to the test to avoid testing this case, but we need a better fix to the underlying bug.

MultiIndexDocValues cannot merge sorted sources (I think?), yet somehow it's pretending it can (in the above test, the three subs had BYTES_FIXED_SORTED type, and the TypePromoter happily claims to merge these to BYTES_FIXED_SORTED; I think MultiIndexDocValues should return null for the sorted source in this case?"
1,"IndexCommit.equals() bugIndexCommit.equals() checks for equality of Directories and versions, but it doesn't check IMHO the more important generation numbers. It looks like commits are really identified by a combination of directory and segments_XXX, which means the generation number, because that's what the DirectoryReader.open() checks for.

This bug leads to an unexpected behavior when the only change to be committed is in userData - we get two commits then that are declared equal, they have the same version but they have different generation numbers. I have no idea how this situation is treated in a few dozen references to IndexCommit.equals() across Lucene...

On the surface the fix is trivial - either add the gen number to equals(), or use gen number instead of version. However, it's puzzling why these two would ever get out of sync??? and if they are always supposed to be in sync then maybe we don't need both of them at all, maybe just generation or version is sufficient?"
1,"BlockJoinQuery doesn't implement boostAfter reviewing LUCENE-3494, i checked other queries and noticed that BlockJoinQuery currently throws UOE for getBoost and setBoost:
{noformat}
throw new UnsupportedOperationException(""this query cannot support boosting; please use childQuery.setBoost instead"");
{noformat}

I don't think we can safely do that in queries, because other parts of lucene rely upon this working... for example BQs rewrite when
it has a single clause and erases itself.

So I think we should just pass down the boost to the inner weight.
"
1,"Retry on ConnectionException does not workI noticed that the Retry handler mechanism does not work when the client cannot
initiate a connection (which throws java.net.ConnectionException). This happens
for me for instance when there is proxy and a tunneling in the picture and
sometimes there are connectivity problems.

I had my own RetryHandler, however, the Connection Timeout exception never falls
in it. I took a look at the source code and noticed that the open() method and
any thrown exception at this level occurs outside the control of the Retry
Handler (which seems to be involved only after open() succeeds).

In fact, if the open() throws ConnectionException (as is my case), since the
try/catch wrapping the open() is not inside the while() but on top of it, it
stops the loop and the retry handler does not get a chance to be invoked .

riad"
1,"Some small fixes after the flex merge...Changes:

  * Re-introduced specialization optimization to FieldCacheRangeQuery;
    also fixed bug (was failing to check deletions in advance)

  * Changes 2 checkIndex methods from protected -> public

  * Add some missing null checks when calling MultiFields.getFields or
    IndexReader.fields()

  * Tweak'd CHANGES a bit

  * Removed some small dead code
"
1,"ICU collator thread-safety issuesThe ICU Collators (unlike the JDK ones) aren't thread safe: http://userguide.icu-project.org/collation/architecture , a little non-obvious since its not mentioned
in the javadocs, and its not clear if the docs apply to only the C code, but i looked
at the source and there is all kinds of internal state.

So in my opinion, we should clone the icu collators (which are passed in from the outside) 
when creating a new TokenStream/AttributeImpl to prevent problems. This shouldn't be a big
deal since everything uses reusableTokenStream anyway.
"
1,"NamespaceRegistry.registerNamespace(pre, uri)  might accidentally remove namespace in certain situationsassume the following mappings exist in the global NamespaceRegistry:

pre1 <-> uri1
pre2 <-> uri2

the following stmt correclty throws a NamespaceException, complaining that an existing prefix can not be remapped:

nsReg.registerNamespace(""pre2"", ""uri1"")

but, as a sideeffect, it has also removed the mapping pre1 <-> uri1."
1,"ConcurrentModificationException in SessionItemStateManager.getIdOfRootTransientNodeState()SessionItemStateManager.getIdOfRootTransientNodeState() is throwing a ConcurrentModificationException on line 607:

Here's a snippet of the code:
{code}
                    for (NodeId id : candidateIds) {
                        if (nodeId.equals(id) || hierMgr.isAncestor(id, nodeId)) {
                            // already a candidate or a descendant thereof
                            // => skip
                            skip = true;
                            break;
                        }
                        if (hierMgr.isAncestor(nodeId, id)) {
                            // candidate is a descendant => remove
                            candidateIds.remove(id);
                        }
                    }
{code}

Can't use Collection.remove(Object) in the middle of iterating. It should probably be changed to use Iterator.remove():
{code}
                    Iterator<NodeId> nodeIdItor = candidateIds.iterator();
                    while (nodeIdItor.hasNext()) {
                        NodeId id = nodeIdItor.next();
                        if (nodeId.equals(id) || hierMgr.isAncestor(id, nodeId)) {
                            // already a candidate or a descendant thereof
                            // => skip
                            skip = true;
                            break;
                        }
                        if (hierMgr.isAncestor(nodeId, id)) {
                            // candidate is a descendant => remove
                            nodeIdItor.remove();
                        }
                    }
{code}

Any idea what I could do differently to workaround the issue?"
1,"RangeQuery equals method does not compare collator property fullyThe equals method in the range query has the collator comparison implemented as:
(this.collator != null && ! this.collator.equals(other.collator))

When _this.collator = null_ and _other.collator = someCollator_  this method will incorrectly assume they are equal. 

So adding something like
|| (this.collator == null && other.collator != null)
would fix the problem
"
1,"FilterIndexReader in trunk does not implement getSequentialSubReaders() correctlySince LUCENE-2459, getSequentialSubReaders() in FilterIndexReader returns null, so it returns an atomic reader. But If you call then any of the enum methods, it throws Exception because the underlying reader is not atomic.

We should move the null-returning method to SlowMultiReaderWrapper and fix FilterIndexReader's default to return in.getSequentialSubReaders(). Ideally an implementation must of course also wrap the sub-readers.

If we change this we have to look into other Impls like the MultiPassIndexSplitter if we need to add atomicity."
1,"DefaultHttpParamsFactory.getDefaultParams() is not thread safeThe method getDefaultParams() in 
org.apache.commons.httpclient.params.DefaultHttpParamsFactory is not thread 
safe.  In this code:

    public HttpParams getDefaultParams() {
        if (httpParams == null) {
            httpParams = createParams();
        }

        return httpParams;
    }

it is possible that httpParams will be called by one thread which will set 
httpParams, then a second thread may call it and may find httpParams is 
non-null.  However, under both the old (Java Language Spec chapter 17) and 
new Java Memory Models, the second thread won't necessarily see the values 
the first thread has set in the referenced HttpParams object.

The easiest way to fix this for all JVMs and memory models is by declaring 
getDefaultParams() to be synchronized."
1,"Field.setValue(...) doesn't properly handle switching between byte[] and other typesThis came up in PyLucene testing, based on Lucene 2.4.1.  Thread here:

  http://pylucene.markmail.org/message/75jzxzqi3smp2s4z

The problem is that Field.setValue does not fix up the isBinary
boolean, so if you create a String field, and then do
setValue(byte[]), you'll get an exception when adding a document
containing that field to the index."
1,"unsynchronized access on 'itemCache' map in ItemManager the access 'itemCache' map in ItemManager is mostly synchronized by not via the ItemStateListener methods:
[...]
    public void stateCreated(ItemState created) {
        ItemImpl item = retrieveItem(created.getId());
        if (item != null) {
            item.stateCreated(created);
        }
    }
[...]
    private ItemImpl retrieveItem(ItemId id) {
        return (ItemImpl) itemCache.get(id);
    }
[...]

this can result in a corruption of a map (eg subsequent accesses may result in a endless loop)."
1,"ClassCastException when registering new node typejava.lang.ClassCastException: org.apache.jackrabbit.core.nodetype.NodeTypeImpl
	at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.registerNodeTypes(NodeTypeManagerImpl.java:708)
	at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.registerNodeType(NodeTypeManagerImpl.java:637)

"
1,"IOExeception can cause loss of data due to premature segment deletionIf you hit an IOException, e.g., disk full, while making a cfs from its constituent parts, you may not be able to rollback to the before-merge process. This happens via addIndexes.

I don't have a nice easy test for this; generating IOEs ain't so easy. But it does happen in the patch for the factored merge policy with the existing tests because the pseudo-randomly generated IOEs fall in a different place."
1,"NoSuchItemStateException if Node.checkin() is invoked within a transactionWhen you run a code that takes versionning outside transactions - everything goes ok. But when you run it inside transaction, it fails:
Here's the stacktrace:

15:41:14,434 ERROR (TransactionalItemStateManager.java:114) -
java.lang.Exception: Cannot commit transaction.
[...]
Caused by: org.apache.jackrabbit.core.state.TransactionException: Unable
to commit transaction.:
31f78b39-6422-4ec8-b41e-2571b6807b05/{http://www.jcp.org/jcr/1.0}isCheckedOut
[...]
Caused by: org.apache.jackrabbit.core.state.NoSuchItemStateException:
31f78b39-6422-4ec8-b41e-2571b6807b05/{http://www.jcp.org/jcr/1.0}isCheckedOut
[...]

When you dont checkin the node transaction commits well, but the node is left checked out..."
1,"DefaultHighlighter.java does not encode illegal XML charactersWhen merging excerpts (method protected String mergeFragments(...) in DefaultHighlighter.java), illegal XML characters are not encoded in all places."
1,"DefaultRedirectHandler not resolving relative location URI wrt the request URIThe adjustment of a relative URI in the Location header value does not take the request URI into account. So you may want to replace ...
------------------------------
try {
    uri = new URI(
            target.getSchemeName(),
            null,
            target.getHostName(),
            target.getPort(),
            uri.getPath(),
            uri.getQuery(),
            uri.getFragment());
------------------------------
... with ...
------------------------------
HttpRequest request = (HttpRequest) context.getAttribute(ExecutionContext.HTTP_REQUEST);
try {
    URI requestURI = new URI(request.getRequestLine().getUri());
    URI absoluteRequestURI = new URI(
            target.getSchemeName(),
            null,
            target.getHostName(),
            target.getPort(),
            requestURI.getPath(),
            requestURI.getQuery(),
            requestURI.getFragment());
    uri = absoluteRequestURI.resolve(uri);
------------------------------
... or get the request URI from somewhere else."
1,"System properties does not get replaced in a Cluster configurationSince JCR-1304 has been added to jackrabbit 1.4 I guess this should be reported as a bug...

Still not debugged deeply, but if I try to configure a Cluster using:
<Cluster id=""${server}"" syncDelay=""10"">

after setting a ""server"" system property I expect to have the cluster initialized properly using the value of such property... I just realized that my cluster node gets initialized with the final value of ""${server}"" instead :(

Cluster config is a very good place where to use system properties, since all the configuration is usually identical between cluster nodes while the ""id"" property must be different...

Is there anything I missed/did wrong in my configuration?
"
1,"QueryParser escaping/parsin issue with strings starting/ending with ||There is a problem with query parser when search string starts/ends with ||.  When string contains || in the middle like 'something || something' everything runs without a problem.

Part of code: 
  searchText = QueryParser.escape(searchText);
  QueryParser parser = null;
  parser = new QueryParser(fieldName, new CustomAnalyser());
  parser.parse(searchText);

CustomAnalyser class extends Analyser. Here is the only redefined method: 

    @Override
    public TokenStream tokenStream(String fieldName, Reader reader) {
      return new PorterStemFilter( (new StopAnalyzer()).tokenStream(fieldName, reader));
    }

I have tested this on Lucene 2.1 and latest source I have checked-out from SVN (Revision 538867) and in both cases parsing exception was thrown.

Part of Stack Trace (Lucene - SVN checkout - Revision 538867):
Cannot parse 'someting ||': Encountered ""<EOF>"" at line 1, column 11.
Was expecting one of:
    <NOT> ...
    ""+"" ...
    ""-"" ...
    ""("" ...
    ""*"" ...
    <QUOTED> ...
    <TERM> ...
    <PREFIXTERM> ...
    <WILDTERM> ...
    ""["" ...
    ""{"" ...
    <NUMBER> ...
    
 org.apache.lucene.queryParser.ParseException: Cannot parse 'someting ||': Encountered ""<EOF>"" at line 1, column 11.
Was expecting one of:
    <NOT> ...
    ""+"" ...
    ""-"" ...
    ""("" ...
    ""*"" ...
    <QUOTED> ...
    <TERM> ...
    <PREFIXTERM> ...
    <WILDTERM> ...
    ""["" ...
    ""{"" ...
    <NUMBER> ...
    
        at org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:150)


Part of Stack Trace (Lucene 2.1):
Cannot parse 'something ||': Encountered ""<EOF>"" at line 1, column 12.
Was expecting one of:
    <NOT> ...
    ""+"" ...
    ""-"" ...
    ""("" ...
    ""*"" ...
    <QUOTED> ...
    <TERM> ...
    <PREFIXTERM> ...
    <WILDTERM> ...
    ""["" ...
    ""{"" ...
    <NUMBER> ...
    
 org.apache.lucene.queryParser.ParseException: Cannot parse 'something ||': Encountered ""<EOF>"" at line 1, column 12.
Was expecting one of:
    <NOT> ...
    ""+"" ...
    ""-"" ...
    ""("" ...
    ""*"" ...
    <QUOTED> ...
    <TERM> ...
    <PREFIXTERM> ...
    <WILDTERM> ...
    ""["" ...
    ""{"" ...
    <NUMBER> ...
    
        at org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:149)


"
1,"HttpState#matchCredentials is brokenCredentials matching algorithm is flawed, generates unnecessary garbage by
instantiating intermediate object during lookup"
1,"AbstractClientConnAdapter#abortConnection() does not release the connection if called from the main execution thread while there is no blocking I/O operation #abortConnection() is usually expected to be  called from a helper thread in order to unblock the main execution thread blocked in an I/O operation. It may be unsafe to call #releaseConnection() from the helper thread, so we have to rely on an IOException thrown by the closed socket on the main thread to trigger the release of the connection back to the connection manager. However, if this method is called from the main execution thread it should be safe to release the connection immediately. Besides, this also helps ensure the connection gets released back to the manager if #abortConnection() is called from the main execution thread while there is no blocking I/O operation."
1,"Connection.setAutoCommit(...) fails if connection is managed for JNDIDatabasePersistenceManagerInvoking setAutoCommit() on a db connection fails if the connection is managed.

I propose as a workaround to check if the auto commit must be set previous to setting it (a trivial patch will be provided).

This can happen eg. if you use JNDI (eg JNDIDatabasePersistenceManager) to fetch the connection on JBoss, and the persistent manager tries to reconnect (see stack trace below).

05 Jul 09:54:24 ERROR sePersistenceManager| failed to re-establish connection
java.sql.SQLException: You cannot set autocommit during a managed transaction!
        at org.jboss.resource.adapter.jdbc.BaseWrapperManagedConnection.setJdbcAutoCommit(BaseWrapperManagedConnection.java:482)
        at org.jboss.resource.adapter.jdbc.WrappedConnection.setAutoCommit(WrappedConnection.java:322)
        at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.initConnection(DatabasePersistenceManager.java:731)
        at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.reestablishConnection(DatabasePersistenceManager.java:806)
        at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.executeStmt(DatabasePersistenceManager.java:852)
        at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.exists(DatabasePersistenceManager.java:647)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.hasNonVirtualItemState(SharedItemStateManager.java:1102)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.hasItemState(SharedItemStateManager.java:289)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.hasItemState(LocalItemStateManager.java:180)
        at org.apache.jackrabbit.core.state.XAItemStateManager.hasItemState(XAItemStateManager.java:252)
        at org.apache.jackrabbit.core.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:174)"
1,"TestStressIndexing2 testMultiConfig failuretrunk: r1134311

reproducible

{code}
    [junit] Testsuite: org.apache.lucene.index.TestStressIndexing2
    [junit] Tests run: 1, Failures: 2, Errors: 0, Time elapsed: 0.882 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] java.lang.AssertionError: ram was 460908 expected: 408216 flush mem: 395100 active: 65808
    [junit]     at org.apache.lucene.index.DocumentsWriterFlushControl.assertMemory(DocumentsWriterFlushControl.java:102)
    [junit]     at org.apache.lucene.index.DocumentsWriterFlushControl.doAfterDocument(DocumentsWriterFlushControl.java:164)
    [junit]     at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:380)
    [junit]     at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1473)
    [junit]     at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1445)
    [junit]     at org.apache.lucene.index.TestStressIndexing2$IndexingThread.indexDoc(TestStressIndexing2.java:723)
    [junit]     at org.apache.lucene.index.TestStressIndexing2$IndexingThread.run(TestStressIndexing2.java:757)
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestStressIndexing2 -Dtestmethod=testMultiConfig -Dtests.seed=2571834029692482827:-8116419692655152763
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestStressIndexing2 -Dtestmethod=testMultiConfig -Dtests.seed=2571834029692482827:-8116419692655152763
    [junit] The following exceptions were thrown by threads:
    [junit] *** Thread: Thread-0 ***
    [junit] junit.framework.AssertionFailedError: java.lang.AssertionError: ram was 460908 expected: 408216 flush mem: 395100 active: 65808
    [junit]     at junit.framework.Assert.fail(Assert.java:47)
    [junit]     at org.apache.lucene.index.TestStressIndexing2$IndexingThread.run(TestStressIndexing2.java:762)
    [junit] NOTE: test params are: codec=RandomCodecProvider: {f33=Standard, f57=MockFixedIntBlock(blockSize=649), f11=Standard, f41=MockRandom, f40=Standard, f62=MockRandom, f75=Standard, f73=MockSep, f29=MockFixedIntBlock(blockSize=649), f83=MockRandom, f66=MockSep, f49=MockVariableIntBlock(baseBlockSize=9), f72=Pulsing(freqCutoff=7), f54=Standard, id=MockFixedIntBlock(blockSize=649), f80=MockRandom, f94=MockSep, f93=Pulsing(freqCutoff=7), f95=Standard}, locale=en_SG, timezone=Pacific/Palau
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestStressIndexing2]
    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=133324528,total=158400512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testMultiConfig(org.apache.lucene.index.TestStressIndexing2):     FAILED
    [junit] r1.numDocs()=17 vs r2.numDocs()=16
    [junit] junit.framework.AssertionFailedError: r1.numDocs()=17 vs r2.numDocs()=16
    [junit]     at org.apache.lucene.index.TestStressIndexing2.verifyEquals(TestStressIndexing2.java:308)
    [junit]     at org.apache.lucene.index.TestStressIndexing2.verifyEquals(TestStressIndexing2.java:278)
    [junit]     at org.apache.lucene.index.TestStressIndexing2.testMultiConfig(TestStressIndexing2.java:124)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1403)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1321)
    [junit] 
    [junit] 
    [junit] Testcase: testMultiConfig(org.apache.lucene.index.TestStressIndexing2):     FAILED
    [junit] Some threads threw uncaught exceptions!
    [junit] junit.framework.AssertionFailedError: Some threads threw uncaught exceptions!
    [junit]     at org.apache.lucene.util.LuceneTestCase.tearDown(LuceneTestCase.java:603)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1403)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1321)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.index.TestStressIndexing2 FAILED
{code}"
1,small SentinelIntSet can cause infinite loop on resizeA small initial size of <=4 can cause the set to not rehash soon enough and thus go into an infinite loop searching the table for an open space.
1,PrivilegeDefinition should implement equals and hashcode
1,"CLONE -Merge error during add to index (IndexOutOfBoundsException)I've been batch-building indexes, and I've build a couple hundred indexes with 
a total of around 150 million records.  This only happened once, so it's 
probably impossible to reproduce, but anyway... I was building an index with 
around 9.6 million records, and towards the end I got this:

java.lang.IndexOutOfBoundsException: Index: 54, Size: 24
        at java.util.ArrayList.RangeCheck(ArrayList.java:547)
        at java.util.ArrayList.get(ArrayList.java:322)
        at org.apache.lucene.index.FieldInfos.fieldInfo(FieldInfos.java:155)
        at org.apache.lucene.index.FieldInfos.fieldName(FieldInfos.java:151)
        at org.apache.lucene.index.SegmentTermEnum.readTerm(SegmentTermEnum.java
:149)
        at org.apache.lucene.index.SegmentTermEnum.next
(SegmentTermEnum.java:115)
        at org.apache.lucene.index.SegmentMergeInfo.next
(SegmentMergeInfo.java:52)
        at org.apache.lucene.index.SegmentMerger.mergeTermInfos
(SegmentMerger.java:294)
        at org.apache.lucene.index.SegmentMerger.mergeTerms
(SegmentMerger.java:254)
        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:93)
        at org.apache.lucene.index.IndexWriter.mergeSegments
(IndexWriter.java:487)
        at org.apache.lucene.index.IndexWriter.maybeMergeSegments
(IndexWriter.java:458)
        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:310)
        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:294)"
1,"IndexMerger: Synchronization issue on repository shutdownAfter inserting a large number of nodes (~200000) into a repository and then closing the session, I get the following exception:

19:42:40.556 [jackrabbit-pool-5] DEBUG o.a.j.core.query.lucene.IndexMerger - # of busy merge workers: 2
19:42:40.556 [jackrabbit-pool-3] DEBUG o.a.j.core.query.lucene.IndexMerger - accepted merge request
19:42:40.556 [jackrabbit-pool-5] DEBUG o.a.j.core.query.lucene.IndexMerger - Worker finished
19:42:40.556 [jackrabbit-pool-3] DEBUG o.a.j.core.query.lucene.IndexMerger - create new index
19:42:40.557 [jackrabbit-pool-3] DEBUG o.a.j.core.query.lucene.IndexMerger - get index readers from MultiIndex
19:42:40.640 [main] INFO  c.a.kmp.generator.JpaToJcrImporter - end JCR save
19:42:40.849 [main] INFO  o.a.j.core.TransientRepository - Session closed
19:42:40.849 [main] INFO  o.a.jackrabbit.core.RepositoryImpl - Shutting down repository...
19:42:40.849 [main] DEBUG o.a.j.core.query.lucene.IndexMerger - dispose IndexMerger
19:42:40.849 [main] DEBUG o.a.j.core.query.lucene.IndexMerger - quit flag set
19:42:40.849 [main] INFO  o.a.j.core.query.lucene.SearchIndex - Index closed: repository/repository/index
19:42:40.850 [main] INFO  o.a.jackrabbit.core.RepositoryImpl - shutting down workspace 'default'...
19:42:40.850 [main] INFO  o.a.j.c.o.ObservationDispatcher - Notification of EventListeners stopped.
19:42:40.850 [main] DEBUG o.a.j.core.query.lucene.IndexMerger - dispose IndexMerger
19:42:40.850 [main] DEBUG o.a.j.core.query.lucene.IndexMerger - quit flag set
19:42:40.850 [main] DEBUG o.a.j.core.query.lucene.IndexMerger - IndexMerger.Worker thread stopped
19:42:40.855 [main] DEBUG o.a.j.core.query.lucene.IndexMerger - index added: name=_6h, numDocs=890
19:42:41.367 [jackrabbit-pool-3] DEBUG o.a.j.core.query.lucene.IndexMerger - deleting index _6g
19:42:41.393 [main] INFO  o.a.j.core.query.lucene.SearchIndex - Index closed: repository/workspaces/default/index
19:42:41.410 [jackrabbit-pool-3] ERROR o.a.j.core.query.lucene.IndexMerger - Error while merging indexes: 
org.apache.lucene.store.AlreadyClosedException: this IndexWriter is closed
	at org.apache.lucene.index.IndexWriter.ensureOpen(IndexWriter.java:412) ~[lucene-core-2.4.1.jar:2.4.1 750176 - 2009-03-04 21:56:52]
	at org.apache.lucene.index.IndexWriter.ensureOpen(IndexWriter.java:417) ~[lucene-core-2.4.1.jar:2.4.1 750176 - 2009-03-04 21:56:52]
	at org.apache.lucene.index.IndexWriter.startTransaction(IndexWriter.java:2511) ~[lucene-core-2.4.1.jar:2.4.1 750176 - 2009-03-04 21:56:52]
	at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:3273) ~[lucene-core-2.4.1.jar:2.4.1 750176 - 2009-03-04 21:56:52]
	at org.apache.jackrabbit.core.query.lucene.PersistentIndex.addIndexes(PersistentIndex.java:114) ~[jackrabbit-core-2.1.1.jar:2.1.1]
	at org.apache.jackrabbit.core.query.lucene.IndexMerger$Worker.run(IndexMerger.java:525) ~[jackrabbit-core-2.1.1.jar:2.1.1]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441) [na:1.6.0_20]
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) [na:1.6.0_20]
	at java.util.concurrent.FutureTask.run(FutureTask.java:138) [na:1.6.0_20]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98) [na:1.6.0_20]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:207) [na:1.6.0_20]
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) [na:1.6.0_20]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) [na:1.6.0_20]
	at java.lang.Thread.run(Thread.java:619) [na:1.6.0_20]
19:42:41.420 [jackrabbit-pool-3] DEBUG o.a.j.core.query.lucene.IndexMerger - Worker finished
19:42:41.839 [main] INFO  o.a.j.c.p.b.DerbyPersistenceManager - Database 'repository/workspaces/default/db' shutdown.


The problem is reproducible. Apparently, the Lucene index is closed before all IndexMerger worker threads are terminated. The root cause seems to be the AtomicBoolean IndexMerger.Worker.terminated which is always true. The enclosed patch solves the problem in my use case.

"
1,"java.lang.IllegalStateException: Connection already open.I am seeing many of the same problems noted in HTTPCLIENT-741 using the latest builds from the maven repo.

java.lang.IllegalStateException: Connection already open.
        at org.apache.http.impl.conn.AbstractPoolEntry.open(AbstractPoolEntry.java:150)
        at org.apache.http.impl.conn.AbstractPooledConnAdapter.open(AbstractPooledConnAdapter.java:119)
        at org.apache.http.impl.client.DefaultClientRequestDirector.execute(DefaultClientRequestDirector.java:308)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:501)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:456)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:422)
        at com.hi5.os.Hi5RemoteContentFetcher.fetch(Hi5RemoteContentFetcher.java:279)
"
1,"ParallelTermEnum is BROKENParallelTermEnum.next() fails to advance properly to new fields.  This is a serious bug. 

Christian Kohlschuetter diagnosed this as the root problem underlying LUCENE-398 and posted a first patch there.

I've addressed a couple issues in the patch (close skipped field TermEnum's, generate field iterator only once, integrated Christian's test case as a Lucene test) and packaged in all the revised patch here.

All Lucene tests pass, and I've further tested in this in my app, which makes extensive use of ParallelReader.
"
1,"JCARepositoryHandle.login(...) methods never throw NoSuchWorkspaceExceptionCall sequence:
  JCARepositoryHandle.login(Credentials, String)      // (here non-existent workspace is specified for login)
    JCARepositoryHandle.login(JCAConnectionRequestInfo)
      ConnectionManager.allocateConnection(ManagedConnectionFactory, ConnectionRequestInfo)
        ...
          JCAManagedConnection.openSession(JCAConnectionRequestInfo)
            Repository.login(Credentials, String)        // here NoSuchWorkspaceException is thrown, catched by JCAManagedConnection.openSession(JCAConnectionRequestInfo), _set as linkedException_ to ResourceException, which is thrown
        ...
     Here (in JCARepositoryHandle.login(JCAConnectionRequestInfo)) ResourceException is caught, its _cause_ is retreived, and, if cause is NoSuchWorkspaceException, it's thrown, else another exception is thrown.

Note, that when exception occures on lower level, it's wrapped in ResourceException using setLinkedException(), but on upper level it's unwrapped using getCause(). But cause is not set by anyone, it's null, so NoSuchWorkspaceException is never thrown here.

Suggested fix is to use same mechanism on both ends: either change wrapping mechanism to exception chaining (new ResourceException(msg, cause)), or unwrap using ResourceException.getLinkedException()."
1,"Registering node type names with spaces fails in clustered environmentRegistering a node type name that contains at least one space in a clustered environment will cause a JournalException in cluster nodes trying to read that change back from the journal. The stack trace observed is:

JournalException: Parse error while reading node type definition.
       at AbstractRecord.readNodeTypeDef(AbstractRecord.java:245)
       ...
Caused by: ParseException: Missing '[' delimiter for beginning of node type name ((internal), line 47)
       at Lexer.fail(Lexer.java:148)
       ...

(package names and intermediate frames omitted for brevity)."
1,"ResidualProperties Converter uses wrong AtomicType Converter on updateWhen writing back data, the ResidualPropertiesCollectionConverterImpl.internalSetProperties method looks at the type of the Java object
to find the atomic type converter instead of getting the converter according to the collection descriptor.

This may lead to NullPointerExceptions in case the concrete type is an extension (or implementation) of the declared type.

I am currently working on a patch to attache to this bug."
1,"principalbased ACL editing fails if principalName differs from the authorizableIDthis issue has been reported by alexK:

editing the permissions for a principal whose name differs from the id of the corresponding user/group fails with AccessControlException.

i quickly had a look at it and the main problem is caused by the ACEditor that assumes that the last segment of the 
path corresponds to the principal name. this isn't true if the principalName differs from the id.



"
1,"Binding repository to a nameserver with RegistryHelper causes failure on lookup.Binding a repository to a nameserver using RegistryHelper causes the next subsequent lookup to fail.  This is what I observerd:

1. RegistryHelper.registerRepository creates a new BindableRepository and initializes it.  This, in turn, initializes the ""real"" repository (i.e. delagtee).  It then binds this reference with the nameserver.

2. On the next lookup, BindableRepositoryFactory.getObjectInstance is invoked.  Thie method checks it's cache for a repository.  Since one does not exist yet, it creates a new BindableRepository and tries to initialize it.  This fails since the call to RegistryHelper.registerRepository already initialized the repository.

The error message basically says the repository is already in use by another process because the .lock file is present.  To fix this, I modified RegistryHelper.registerRepository to NOT initialize the repository and simply bind the ""Reference""."
1,"org.apache.http.impl.client.cache.memcached.MemcachedHttpCacheStorage should verify class of returned object before castingorg.apache.http.impl.client.cache.memcached.MemcachedHttpCacheStorage

Original (in getEntry function): 
  byte[] data = (byte[]) client.get(url);

Should be:
  Object obj= client.get(url);
  if (null == obj || !(objinstanceof byte[])) {
    return null;
  }
  byte[] data = (byte[])obj;


Original (in updateEntry function):
  byte[] oldBytes = (v != null) ? (byte[]) v.getValue() : null;

Should be:
  byte[] oldBytes = (v != null && (v.getValue() instanceof byte[])) ? (byte[]) v.getValue() : null;



  
"
1,"HttpClient throws NPE on Invalid Port when used with MultiThreadedHttpConnectionManagerThe HttpClient throws NullPointerException in the main thread when an invalid port (like 80001) is used in the URL. An IllegalArgumentException is thrown in TimeoutGuard thread.
 
Exception in thread ""Timeout guard"" java.lang.IllegalArgumentException: port out of range:80001
	at java.net.InetSocketAddress.<init>(InetSocketAddress.java:118)
	at java.net.Socket.<init>(Socket.java:240)
	at org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory.createSocket(DefaultProtocolSocketFactory.java:80)
	at org.apache.commons.httpclient.protocol.ControllerThreadSocketFactory$1.doit(ControllerThreadSocketFactory.java:91)
	at org.apache.commons.httpclient.protocol.ControllerThreadSocketFactory$SocketTask.run(ControllerThreadSocketFactory.java:158)
	at java.lang.Thread.run(Thread.java:613)
Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.commons.httpclient.HttpConnection.open(HttpConnection.java:721)
	at org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpConnectionAdapter.open(MultiThreadedHttpConnectionManager.java:1361)
	at org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:387)
	at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:171)
	at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397)
	at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:323)
	at com.aol.test.HttpTest$PoolingHttpConnector.doGet(HttpTest.java:47)
	at com.aol.test.HttpTest.main(HttpTest.java:17)

It should throw a checked exception in main thread so caller can handle the error condition more gracefully.

The test program is attached. This is caused by a race condition and it's not always reproducible. Running in debugger shows a different behavior.

package com.aol.test;

import java.io.IOException;

import org.apache.commons.httpclient.HttpClient;
import org.apache.commons.httpclient.HttpStatus;
import org.apache.commons.httpclient.MultiThreadedHttpConnectionManager;
import org.apache.commons.httpclient.methods.GetMethod;
import org.apache.commons.httpclient.params.HttpConnectionManagerParams;

public class HttpTest {
	
	public static void main(String[] args) {
		PoolingHttpConnector conn = new PoolingHttpConnector();
		
		try {
			String response = conn.doGet(""http://www.aol.com:80001"");
			System.out.println(""Response='"" + response + ""'"");
		} catch (IOException e) {
			e.printStackTrace();
		}
	}


	static class PoolingHttpConnector {
		
		public static final int MAX_TOTAL_CONNECTIONS = 16;
		public static final int MAX_CONNECTIONS_PER_HOST = 8;
		public static final int CONNECT_TIMEOUT = 5000;
		public static final int SOCKET_TIMEOUT = 5000;
		public static final boolean TCP_NO_DELAY = true;
		
	    private static MultiThreadedHttpConnectionManager poolManager;
	    private static HttpConnectionManagerParams httpParams;
	    private static HttpClient httpClient;
	    private static boolean initialized = false;
	    
		public PoolingHttpConnector() 
		{
			initialize();
		}

		public String doGet(String url) throws IOException {
			GetMethod method = new GetMethod(url);
					
			try {
	            int status = httpClient.executeMethod(method);	            
		        String response = new String(method.getResponseBody());
	            
	            if (status != HttpStatus.SC_OK)
	            	throw new IOException(""HTTP error: "" + response);
	            
	            return response;
	            
			} finally {
	            method.releaseConnection();
			}
	 	} 	
	
		private synchronized void initialize() {	
			if (initialized)
				return;
			
	        poolManager = new MultiThreadedHttpConnectionManager();
	        httpParams = new HttpConnectionManagerParams();
	        
	        httpParams.setMaxTotalConnections(MAX_TOTAL_CONNECTIONS);
	        httpParams.setDefaultMaxConnectionsPerHost(MAX_CONNECTIONS_PER_HOST);
	        httpParams.setTcpNoDelay(TCP_NO_DELAY);
	        httpParams.setSoTimeout(SOCKET_TIMEOUT);
	        httpParams.setConnectionTimeout(CONNECT_TIMEOUT);
	        
	        poolManager.setParams(httpParams);
	        httpClient = new HttpClient(poolManager);

			initialized = true;
		}
		
	}
}



"
1,"DefaultClientRequestDirector doesn't release connections back to ClientConnectionManager on exceptionsSee HTTPCLIENT-747 for more info.  Basically the deal is that an entry is always allocated, but currently it's only released if execute(..) completes normally."
1,"DEFAULT_ATTRIBUTE_FACTORY faills to load implementation class when iterface comes from different classloaderThis is a followup for [http://www.lucidimagination.com/search/document/1724fcb3712bafba/using_the_new_tokenizer_api_from_a_jar_file]:

The DEFAULT_ATTRIBUTE_FACTORY should load the implementation class for a given attribute interface from the same classloader like the attribute interface. The current code loads it from the classloader of the lucene-core.jar file. In solr this fails when the interface is in a JAR file coming from the plugins folder. 

The interface is loaded correctly, because the addAttribute(FooAttribute.class) loads the FooAttribute.class from the plugin code and this with success. But as addAttribute tries to load the class from its local lucene-core.jar classloader it will not find the attribute.

The fix is to tell Class.forName to use the classloader of the corresponding interface, which is the correct way to handle it, as the impl and the attribute should always be in the same classloader and file.

I hope I can somehow add a test for that."
1,Node deleted while query is executed should not affect result sizeCurrently the QueryResultImpl counts result nodes as invalid when the access check throws a ItemNotFoundException (line 311). This leads to inconsistent total size. IMO it is sufficient to count them as invalid when the client iterates over the nodes (line 555).
1,"ClassCastException in ParallelReader classClassCastException in ParalleReader when calling getTermFreqVectors on line 153

Reason : 

 cast of key and value is swapped

Fixed with : 

      IndexReader reader = (IndexReader)e.getValue();
      String field = (String)e.getKey();
"
1,"intermittent deadlock in TestAtomicUpdate,TestIndexWriterExceptionsWhile backporting issues for 2.9.x/3.0.x release I hit deadlocks in these two tests, under both test-core and test-tag."
1,"AssertionError on creating doc containing field with empty string as field nameSpinoff from here:

  http://www.gossamer-threads.com/lists/lucene/java-user/58496

Pre-2.3 you were allowed to add Fields to a Document where the field name is the empty string.  In 2.3.0 it broke: you hit this during flush:

{code}
java.lang.AssertionError
    at org.apache.lucene.index.TermInfosWriter.add(TermInfosWriter.java:143)
    at org.apache.lucene.index.DocumentsWriter.appendPostings(DocumentsWriter.java:2290)
    at org.apache.lucene.index.DocumentsWriter.writeSegment(DocumentsWriter.java:1985)
    at org.apache.lucene.index.DocumentsWriter.flush(DocumentsWriter.java:539)
    at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:2497)
    at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:2397)
    at org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:1204)
    at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1178)
    at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1153) 
{code}

The bug is just an over-aggressive assert statement.  I'll commit a fix shortly & port to 2.3 branch for 2.3.1 release."
1,"cache does not validate multiple cached variantsThere is a bug in CachingHttpClient, where when we attempt to collect all the etags for existing cached variants so we can send a conditional request to the origin, we accidentally don't find any, and send an unconditional request instead."
1,"encode/decodeAs I mention in my email executing <code>ISO9075.decode(""StringWith$inside"")</code> leads to exception:
java.lang.StringIndexOutOfBoundsException: String index out of range: 1
	at java.lang.String.charAt(String.java:444)
	at java.util.regex.Matcher.appendReplacement(Matcher.java:559)
	at com.day.crx.domino.util.NameEncoderDecoder.decode(NameEncoderDecoder.java:117)
	at integration.query.QueryTest.testQuery(QueryTest.java:49)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:324)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:128)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)

The problem is in Matcher.appendReplacement() method, because it didn't correctly interpret '$' and '\' sign. Both have to be escaped with '\' sign."
1,"IndexWriter & ConcurrentMergeScheduler deadlock case if starting a merge hits an exceptionIf you're using CMS (the default) and mergeInit hits an exception (eg
OOME), we are not properly clearing IndexWriter's internal tracking of
running merges.  This causes IW.close() to hang while it incorrectly
waits for these non-started merges to finish.

"
1,"IndexWriter commits update documents without corresponding deletewhile backporting the testcase from LUCENE-3348 I ran into this thread hazard in the 3.x branch. We actually fixed this issue in LUCENE-3348 for Lucene 4.0 but since DWPT has a slightly different behavior when committing segments I create a new issue to track this down in 3.x. when we prepare a commit we sync on IW flush the DW and apply all deletes then release the lock, maybeMerge and start the commit (IW#startCommit(userdata)). Yet, a new segment could be flushed via getReader and sneak into the SegementInfos which are cloned in IW#startCommit instead of in prepareCommit right after the flush. "
1,"Unable to login with two different Credentials to same workspace in one TransactionI'm using the Jackrabbit 1.2.1 JCA adapter and trying to access in a SessionBean-Method with Container Transaction a Workspace with 2 different Credentials. 
The Method takes about 400ms to finish but no commit on TransactionContextr occurs (Debugging ..) only the prepare was called 2 times .
The Container hangs on the PostInvoke Method about 5 seconds and then i get a ""javax.transaction.xa.XAException"" 
with the Warn Message: Transaction rolled back because timeout expired

The code ..
Context ctx = new InitialContext(); 
Repository repository = (Repository) ctx.lookup(""java:comp/env/jackrabbit""); 
Credentials credentials = new SimpleCredentials(""user1"", ""password1"".toCharArray()); 
Credentials credentials2 = new SimpleCredentials(""user2"", ""password2"".toCharArray()); 
Session session1 = repository.login(credentials, ""default""); 
Session session2 = repository.login(credentials2, ""default""); 

Session1 adds a node to the workspace .. and with the session2 i do nothing except the login !
If i make no second login the Method works fine."
1,SimpleText has a bulk enum buffer reuse bugtestBulkPostingsBufferReuse fails with SimpleText codec.
1,"cache module does not completely handle upstream Warning headers correctlyThere are a couple of MUST requirements from the RFC for Warning headers that aren't correctly handled by the current implementation:

http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.46

1. ""However, if a cache successfully validates a cache entry, it SHOULD remove any Warning headers previously attached to that entry except as specified for specific Warning codes. It MUST then add any Warning headers received in the validating response.""

2. ""If an implementation receives a message with a warning-value that includes a warn-date, and that warn-date is different from the Date value in the response, then that warning-value MUST be deleted from the message before storing, forwarding, or using it. (This prevents bad consequences of naive caching of Warning header fields.) If all of the warning-values are deleted for this reason, the Warning header MUST be deleted as well."" "
1,"RMI: Property.getValue() fails with EOFException after many readsWhen reading binary properties via RMI it can happen that it fails throwing an EOFException. This is caused by a server sided ""Too many open files"" bacause BinaryValue.writeObject() does not close the underlying value InputStream."
1,"CompactNodeTypeDefReader adds nt:base as declared supertype even if already extending(reported to the list by michael singer)

I wrote a simple program which uses the nt-ns-util contribution to
register custom node types written in CND language.

I defined the following (very simple) custom node types:

<test = 'http://foo.bar/test'>
[test:firstnodetype]
+ test:secondnodetype mandatory

<test = 'http://foo.bar/test'>
[test:secondnodetype] > test:firstnodetype
+ test:thirdnodetype

<test = 'http://foo.bar/test'>
[test:thirdnodetype] > test:secondnodetype
- test:catalog (string)  < 'URI', 'URN', 'DOI', 'ISBN', 'ISSN'
- test:entry (string) m


In the resulting custom_nodetypes.xml each of the custom nodes has a
supertype of ""nt:base"" but I didn't explicitely define a supertype of
""nt:base"" for [test:secondnodetype] and [test:thirdnodetype].

I think this behavior is wrong since the method getDeclaredSupertypes()
of class NodeType always returns ""nt:base"" plus the explicitely declared
Supertype (which it e.g. does not for ""nt:folder"").
"
1,HTMLStripCharFilter produces invalid final offsetNightly build found this... I boiled it down to a small test case that doesn't require the big line file docs.
1,"FileNotFoundException thrown by Directory.copy()java.io.FileNotFoundException: segments_bu
        at org.apache.lucene.store.RAMDirectory.openInput(RAMDirectory.java:234)
        at org.apache.lucene.store.Directory.copy(Directory.java:190)

"
1,Session logout doesn't release locks acquired using addLockTokenSession.addLockToken doesn't register locks with the session so when logout is called they are not released. Locks acquired this way maintain a reference to the Session after logout and new sessions attempting to acquire the locks will fail.
1,"NameSet does not implement equals(Object) and hashCode() methodsThe merge context uses the NameSet.equals(NameSet) method to compare two sets; however, the NameSet class does not override the default Object.equals(Object) method, and does not inherit from AbstractSet<E>.  Therefore, the merge check fails, even though the mixin sets are the same.  Object instance equivalence is being performed as opposed to set equivalence.  Behavior is observed when more than one thread is checking the ISM at a given time.  Demonstration code available upon request.

From NodeStateMerger, line 83:
                // mixin types
                if (!state.getMixinTypeNames().equals(overlayedState.getMixinTypeNames())) {
                    // the mixins have been modified but by just looking at the diff we
                    // can't determine where the change happened since the diffs of either
                    // removing a mixin from the overlayed or adding a mixin to the
                    // transient state would look identical...
                    return false;
                }

Proposed solution:
- Implement NameSet.equals(...) method:
	public boolean equals(Object obj) {
		if (obj != null && obj instanceof NameSet) {
			NameSet oo = (NameSet) obj;
			return oo.names.equals(this.names);
		}
		return false;
	}"
1,"InstantiatedIndexReader does not handle #termDocs(null) correct (AllTermDocs)This patch contains core changes so someone else needs to commit it.

Due to the incompatible #termDocs(null) behaviour at least MatchAllDocsQuery, FieldCacheRangeFilter and ValueSourceQuery fails using II since 2.9.

AllTermDocs now has a superclass, AbstractAllTermDocs that also InstantiatedAllTermDocs extend.

Also:

 * II-tests made less plausable to pass on future incompatible changes to TermDocs and TermEnum
 * IITermDocs#skipTo and #next mimics the behaviour of document posisioning from SegmentTermDocs#dito when returning false
 * II now uses BitVector rather than sets for deleted documents
"
1,"Large Lucene index can hit false OOM due to Sun JRE issueThis is not a Lucene issue, but I want to open this so future google
diggers can more easily find it.

There's this nasty bug in Sun's JRE:

  http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6478546

The gist seems to be, if you try to read a large (eg 200 MB) number of
bytes during a single RandomAccessFile.read call, you can incorrectly
hit OOM.  Lucene does this, with norms, since we read in one byte per
doc per field with norms, as a contiguous array of length maxDoc().

The workaround was a custom patch to do large file reads as several
smaller reads.

Background here:

  http://www.nabble.com/problems-with-large-Lucene-index-td22347854.html
"
1,"DataStore: changing the modified date fails if the file is open for reading (Windows only)If the file is open for reading, Windows doesn't allow to change the last modified time using File.setLastModified():

org.apache.jackrabbit.core.data.DataStoreException: Failed to update record
modified date: 2ac72495fd1e270777821b8a872903c79c84a8d9
        at org.apache.jackrabbit.core.data.FileDataStore.addRecord(FileDataStore.java:250)
        at org.apache.jackrabbit.core.value.BLOBInDataStore.getInstance(BLOBInDataStore.java:119)
        at org.apache.jackrabbit.core.value.InternalValue.getBLOBFileValue(InternalValue.java:619)
        at org.apache.jackrabbit.core.value.InternalValue.create(InternalValue.java:369)
        at org.apache.jackrabbit.core.value.InternalValueFactory.create(InternalValueFactory.java:94)
        at org.apache.jackrabbit.core.value.ValueFactoryImpl.createBinary(ValueFactoryImpl.java:74)

Test case and possible workaround:

import java.io.File;
import java.io.FileInputStream;
import java.io.RandomAccessFile;
public class Test {
    public static void main(String... args) throws Exception {
        String name = ""test.txt"";
        File test = new File(name);
        RandomAccessFile r = new RandomAccessFile(name, ""rw"");
        r.write(0);
        r.close();
        long mod = test.lastModified();
        Thread.sleep(3000);
        FileInputStream in = new FileInputStream(name);
        if (!test.setLastModified(test.lastModified()+1)) {
        	if (!test.canWrite()) {
        		System.out.println(""Can't write to "" + name);
        	} else {
            	System.out.println(""canWrite ok"");
            	r = new RandomAccessFile(name, ""rw"");
            	int old = r.read();
            	r.seek(0);
            	r.write(old);
            	r.close();
        	}
        } else {
        	System.out.println(""setLastModified ok"");
        }
        System.out.println(""Modified old: "" + mod);
        System.out.println(""Modified now: "" + test.lastModified());
        in.close();
        System.out.println(""input closed"");
        if (!test.setLastModified(test.lastModified()+1)) {
        	if (!test.canWrite()) {
        		System.out.println(""Can't write to "" + name);
        	} else {
            	System.out.println(""canWrite ok"");
        	}
        } else {
        	System.out.println(""setLastModified ok"");
        }
        new File(name).delete();
    }
}
"
1,"Adding nodes from concurrently running sessions cause exceptionsExceptions are thrown when trying to add child nodes to one parent node from different sessions running concurrently. One of the following exceptions is always thrown:

 * Exception in thread ""Thread-8"" java.lang.RuntimeException: javax.jcr.nodetype.ConstraintViolationException: /A/7 needs to be saved as well.
 * Exception in thread ""Thread-8"" java.lang.RuntimeException: javax.jcr.RepositoryException: /A: unable to update item.: Unable to
resolve path for item: 016b885a-64aa-45b9-a990-05cbabb4586f/{http://www.jcp.org/jcr/1.0}primaryType: Unable to resolve path for item: 016b885a-64aa-45b9-a990-05cbabb4586f/{http://www.jcp.org/jcr/1.0}primaryType

According to JCR-584 ""Improve handling of concurrent node modifications"" the following scenario ""session 1 adds or removes child node 'x', session 2 adds or removes child node 'y'"" should run gracefully, but the following test constantly fails:

public void testSync() throws Exception
{
       Node rootNode = getSession ().getRootNode ();
       rootNode.addNode (""A"");
       rootNode.save();

       final Session session1 = getRepository().login (new SimpleCredentials (""userName"", ""password"".toCharArray()));
       final Session session2 = getRepository().login (new SimpleCredentials (""userName"", ""password"".toCharArray()));

       Thread thread1 = new Thread (new Runnable()
       {
               public void run()
               {
                       try
                       {
                               addNodes (""A"", session1, 0);
                       }
                       catch (RepositoryException ex)
                       {
                               throw new RuntimeException (ex);
                       }
               }
       });

       Thread thread2 = new Thread (new Runnable()
       {
               public void run()
               {
                       try
                       {
                               addNodes (""A"", session2, 1001);
                       }
                       catch (RepositoryException ex)
                       {
                               throw new RuntimeException (ex);
                       }
               }
       });

       thread1.start();
       thread2.start();

       thread1.join();
       thread2.join();
}

private void addNodes (String parentName, Session session, int startIndex)
       throws RepositoryException
{
       Node parentNode = session.getRootNode().getNode (parentName);
       for (int i = startIndex; i < startIndex + 100; i++)
       {
               String name = Integer.toString (i);
               parentNode.addNode (name);
               parentNode.save();
       }
}

BTW: exceptions were also thrown when I tried to add nodes from one thread and remove some of them from another one. Each thread used it's own session, each node had unique name.

"
1,Exception not caugh in DefaultResponseParserThe method hasProtocolVersion in o.a.h.message.BaseLineParser (httpcore-alpha6) throws an IndexOutOfBoundsException which is not caught by the parseHead method in the o.a.h.impl.conn.DefaultResponseParser.
1,"Redirect and Kerberos authentication in conflictWe are using the HttpClient to connect to a Website that uses Kerberos-Authentication.

Beware this trigger word: Kerberos! I think this is *not* the problem, but please read on.

Here is the sequence of events:

Client: GET /
Server: Unauthorized.
Client: GET / and includes authentication.
Server: 302 to /something on the same host (this shows that in principle authentication works)
Client: GET /something,  does not include authentication
Server: Unauthorized

Client quits with 401-Unauthorized.

I would have expected one of the following instead:

1) Client immediately sends authorization information with the redirected GET /something
2) Client re-requests the /something with authorization after 401-Unauthorized.

We could get around the problem by setting the ConnectionReuseStrategy to a constant false.

It would be great if someone could tell me if HttpClient works as expected or whether there is a bug or misconfiguration lurking.

Thanks,
Harald.
"
1,"Document with no term vectors mixed with ones that have term vectors cause EOFException during mergeAnother spinoff from here:

  http://www.gossamer-threads.com/lists/lucene/java-dev/53306

Thank you to Andi Vajda for capturing the issue in a compact test!

This is the same logical error from LUCENE-1008, but in this case the
bug is in TermVectorsWriter: we are failing to write the ""0"" field
count to the tvd file when the document has no vectors.  I have a unit
test showing the issue & simple fix.
"
1,"JNDI data sources with BundleDbPersistenceManager: UnsupportedOperationExceptionWhen using the org.apache.tomcat.dbcp.dbcp.BasicDataSourceFactory, the BundleDbPersistenceManager can not open a database connection via JNDI because the method DataSource.getConnection(user, password) is not supported. Instead, DataSource.getConnection() must be used for this to work.

ConnectionFactory.getConnection should be changed to call this method if user name and password are empty."
1,"BundlePersistenceManager.externalBLOBs can not be configuredIf you try to configure the property externalBLOBs through the workspace.xml it does not work.
The BundlePersistenceManager has not Method setExternalBLOBs(boolean externalBLOBs) so it can not be configured
because its not bean conform. See the DatabasePersistenceManager which has such a Method
"
1,"no-cache directives with field names are transmitted downstream""Field names MUST NOT be included with the no-cache directive in a request.""

http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.4

Currently, the cache implementation allows a request containing something like:
    Cache-Control: no-cache=""Content-Location""
to be passed downstream towards the origin.

This is another one of those tricky situations where our client has passed us a non-compliant request.
"
1,"Merging of compressed string Fields may hit NPEThis bug was introduced with LUCENE-1219 (only present on 2.4).

The bug happens when merging compressed string fields, but only if bulk-merging code does not apply because the FieldInfos for the segment being merged are not congruent.  This test shows the bug:

{code}
  public void testMergeCompressedFields() throws IOException {
    File indexDir = new File(System.getProperty(""tempDir""), ""mergecompressedfields"");
    Directory dir = FSDirectory.getDirectory(indexDir);
    try {
      for(int i=0;i<5;i++) {
        // Must make a new writer & doc each time, w/
        // different fields, so bulk merge of stored fields
        // cannot run:
        IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), i==0, IndexWriter.MaxFieldLength.UNLIMITED);
        w.setMergeFactor(5);
        w.setMergeScheduler(new SerialMergeScheduler());
        Document doc = new Document();
        doc.add(new Field(""test1"", ""this is some data that will be compressed this this this"", Field.Store.COMPRESS, Field.Index.NO));
        doc.add(new Field(""test2"", new byte[20], Field.Store.COMPRESS));
        doc.add(new Field(""field"" + i, ""random field"", Field.Store.NO, Field.Index.TOKENIZED));
        w.addDocument(doc);
        w.close();
      }

      byte[] cmp = new byte[20];

      IndexReader r = IndexReader.open(dir);
      for(int i=0;i<5;i++) {
        Document doc = r.document(i);
        assertEquals(""this is some data that will be compressed this this this"", doc.getField(""test1"").stringValue());
        byte[] b = doc.getField(""test2"").binaryValue();
        assertTrue(Arrays.equals(b, cmp));
      }
    } finally {
      dir.close();
      _TestUtil.rmDir(indexDir);
    }
  }
{code}

It's because in FieldsReader, when we load a field ""for merge"" we create a FieldForMerge instance which subsequently does not return the right values for getBinary{Value,Length,Offset}."
1,"VersionIteratorImpl problem?I meet with problem in VersionIterator:
Classic nextVersion()/hasNext() loop  for VersionIterator become endless. 

I think problem with peek/pop misprint:

    public Version nextVersion() {
.......
        InternalVersion ret = (InternalVersion) successors.<b>peek</b>();
......
    }

I change to
InternalVersion ret = (InternalVersion) successors.<b>pop</b>();


"
1,"Uncommitted changes or connection leak with Container Managed TransactionsApparently the connector doesn't support CMT (container managed transactions). if the jcr session is closed inside a CMT the AS (application server) throws an exception on commit. And if the jcr session is leaved open, the AS commits the TX successfully but it causes a connection leak by leaving the session open."
1,"TestBooleanMinShouldMatch test failureant test -Dtestcase=TestBooleanMinShouldMatch -Dtestmethod=testRandomQueries -Dtests.seed=505d62a62e9f90d0:-60daa428161b404b:-406411290a98f416

I think its an absolute/relative epsilon issue"
1,"Node.restore() may throw InvalidItemStateExceptionIt seems that ItemManager cache is not maintained correctly. I'm getting InvalidItemStateException: 'propertyId' has been modified externally tryin restore/checkout versionable nodes in single thread.

ItemState should be evicted from ItemStateManager cache when modified, it seems that status of ItemState is changed to MODIFIED, but itemState remains in the cache."
1,"Creating a node of type nt:hierarchyNode (or derived) on a JCR 1.0 compliant repository failsWhen creating a node of type nt:hierarchyNode (or derived) on a JCR 1.0 compliant repository, the auto-created property named jcr:created does not get a default value and an exception:

javax.jcr.RepositoryException: createFromDefinition not implemented for: {http://www.jcp.org/jcr/1.0}created

The code in SessionItemStateManager#computeSystemGeneratedPropertyValues handles jcr:created only when found in node type mix:created. In JCR 1.0, however, this property was declared in nt:hierarchyNode. Adding this extra case would allow interoperation with such a repository.



"
1,"Inconsistent state when removing mix:lockable from a locked Nodewhen the lock holder removes mix:lockable from a locked node, the lock related properties get removed.
however, the lock still is live and present on the node.

i would have expected that either

- removing mix:lockable was not allowed or
- the lock was automatically released

test code:

    public void testRemoveMixLockableFromLockedNode() throws RepositoryException {

        Node n = testRootNode.addNode(nodeName1);
        n.addMixin(mixLockable);
        testRootNode.save();

        Lock l = n.lock(true, true);
        n.removeMixin(mixLockable);
        n.save();

        assertFalse(n.isNodeType(mixLockable));                                <===== ok
        assertFalse(l.isLive());                                                                    <===== lock is still live
        assertFalse(n.isLocked());                                                            <=====  node is still locked
        List tokens = Arrays.asList(superuser.getLockTokens());
        assertFalse(tokens.contains(l.getLockToken()));                    <=====  session contains the token

        assertFalse(n.hasProperty(jcrLockOwner));                             <=====  ok. prop got removed.
        assertFalse(n.hasProperty(jcrlockIsDeep));                             <=====  ok. prop got removed.
        n.unlock();                                                                                         <===== LockException (node not lockable)
    }"
1,"JCR2SPI: updating events swallowed (CacheBehavior.OBSERVATION)with CacheBehavior.OBSERVATION the hierarchy held within jcr2spi is updated based on events.

if Session-A persistently adds a mix:referenceable to a Node that is already loaded in Session-B, the latter will not be informed about this change.

Reason: upon processing the SPI Event (-> HierarchyEventListener#onEvent) the parent is retrieved by the Event ItemId, which in the former case contains a uniqueID part, which is not known yet to the listening Session-B.
Consequently the NodeEntry affected by the event is not updated.

Possible fix: If looking up the parent entry of the event doesn't succeed, a 2nd lookup using the Event path should be performed."
1,"Session.impersonate non-functionalCurrently SessionImpl.impersonate simply calls Repository.login with the given credentials and the workspace name of the session. If the credentials are incomplete in that the password is missing, the method throws a ""LoginException: Failed to authenticate userID"", which is actually a misleading text, as the reason is not failure to authenticate userID but at best that the session has not enough access rights to impersonate as userID.

For my application, it is crucial, that Session.impersonate is implemented in the sense that this method allows creation of a session with password-less credentials. I accept this method to fail, but it should fail with a correct message."
1,"Avoid premature publication of XAItemStateManagerThe XAItemStateManager constructor calls the super constructor (LocalItemStateManager)  which registers the instance as a listener with the SharedItemStateManager. The construction of the instance has not yet been finished, but it is accessible from the SharedItemStateManager. This can result in strange exceptions like the following:

java.lang.NullPointerException
        at org.apache.jackrabbit.core.state.XAItemStateManager.stateModified(XAItemStateManager.java:580)
        at org.apache.jackrabbit.core.state.StateChangeDispatcher.notifyStateModified(StateChangeDispatcher.java:111)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.stateModified(SharedItemStateManager.java:400)
        at org.apache.jackrabbit.core.virtual.AbstractVISProvider.stateModified(AbstractVISProvider.java:445)
        at org.apache.jackrabbit.core.state.ItemState.notifyStateUpdated(ItemState.java:244) 

The NPE is caused by the commitLogs field being null (it has not yet been initialized to its final value)."
1,"Possible HttpClient codepage issue (ascii/ebcdic) on WebSphere z/OSI am working with Cactus 1.4.1 on WebSphere NT and also WebSphere z/OS
(mainframe). The problem seems to be with HTTPClient. I have tried with the
nuild on the 7th December.

I am trying to get basic cactus servlet tests working on WebSphere z/OS.
Everything works fine through WebSphere NT, however, when the same application
is deployed to WebSphere z/OS then we get the following error:

<?xml version=""1.0"" encoding=""UTF-8"" ?><?xml-stylesheet type=""text/xsl""
href=""junit-noframes.xsl""?><testsuites><testsuite name=""TestCactusServlet""
tests=""1"" failures=""0"" errors=""1"" time=""10.184""><testcase name=""testNeal""
time=""10.182""><error message=""Error in parsing the status  line from the
response: unable to find line starting with &quot;HTTP/&quot;""
type=""org.apache.commons.httpclient.HttpRecoverableException"">org.apache.commons.httpclient.HttpRecoverableException:
Error in parsing the status  line from the response: unable to find line
starting with &quot;HTTP/&quot;
	at
org.apache.commons.httpclient.HttpMethodBase.readStatusLine(HttpMethodBase.java:1791)
	at
org.apache.commons.httpclient.HttpMethodBase.readResponse(HttpMethodBase.java:1559)
	at
org.apache.commons.httpclient.HttpMethodBase.processRequest(HttpMethodBase.java:2219)
	... etc ...

I have verified that the Application Server on WebSPhere z/OS is working fine. I
set the logging on the cactus to DEBUG and noticed that the data that the
HTTPClient is retrieving from the connection is scrambled in some way. For example:

16:06:20,213 [WebSphere t=009d7920] DEBUG ent.HttpClientConnectionHelper  -
>getCookieString = [null] 
16:06:20,317 [WebSphere t=009d7920] DEBUG httpclient.wire                 - >> ""@a??? etc...

On WebSphere NT the data at this point looks OK.

What springs to mind is maybe ascii/ebcdic conversion problem. z/OS uses unicode
 for java, as it should. However, the HTTPClient creates it own socket
connection to the app server and therefore it is connecting to non java code. In
such a situation codepage conversion is necessary.

Could anybody adsvise on how to get this to work?

Regards,

Neal Johnston-Ward"
1,"empty host header with ip addressfile: HttpMethodBase.java method: addHostRequestHeader

HttpClient writes an empty Host header if the host is referred using IP address.
HTTP 1.1 RFC is not too clear what should be used in this case. However, other
HTTP 1.1 implementations (e.g. Java 1.4.0) uses IP address instead of dns name
in the header.

Furthermore, some HTTP server implementations (e.g. Jetty) will return ""400 bad
request"" if it encounters an empty Host header. That may be a bug in Jetty, but
it might be a good idea to use IP address in Host header to increase compability."
1,"addIndexesNoOptimize should not enforce maxMergeDocs/maxMergeSize limitIf you pass an index that has a segment > maxMergeDocs or maxMergeSize
to addIndexesNoOptimize, it throws an IllegalArgumentException.

But this check isn't reasonable because segment merging can easily
produce segments over these sizes since those limits apply to each
segment being merged, not to the final size of the segment produced.

So if you set maxMergeDocs to X, build up and index, then try to add
that index to another index that also has maxMergeDocs X, you can
easily hit the exception.

I think it's being too pedantic; I plan to just remove the checks for
sizes."
1,"Duplicate hits and missing hits in sorted searchIf using a searcher that subclasses from IndexSearcher I get different result sets (besides the ordering of course). The problem only occurrs if the searcher is wrapped by (Parallel)MultiSearcher and the index is not too small. The number of hits returned by un unsorted and a sorted search are identical but the hits are referencing different documents. A closer look at the result sets revealed that the sorted search returns duplicate hits.

I created test cases for Lucene 1.4.3 as well as for the head release. The problem showed up for both, the number of duplicates beeing bigger for the head realease. The test cases are written for package org.apache.lucene.search. There are messages describing the problem written to the console. In order to see all those hints the asserts are commented out. So dont't be confused if junit reports no errors. (Sorry, beeing a novice user of the bug tracker I don't see any means to attach the test cases on this screen. Let's see.)"
1,"WorkspaceInfo.dispose() does not deregister SharedItemStateManager from virtual item state providersAutomatic disposal of idle workspaces frees unused workspaces but corresponding SharedItemStateManager (and releated PersistenceManager) is still kept in memory referenced by virtual item state providers,  this can lead to memory leaks."
1,"Crashes when it gets a redirectI get the following crash when VFS (not my code) calls HttpClient. This code 
worked with some older version of HttpClient (is my belief) but doesn't appear 
to work with CVS HEAD, hence this posting.

Note: I'm sorry, but I don't know which Method it was calling, but hopefully a 
redirect is a redirect and the bug stands irrespective of that.

This is major to me (and Ruper) 'cos it is the first thing it does before 
attempting to read the contents of that location.

regards,

Adam

java.lang.NullPointerException
	at 
org.apache.commons.httpclient.HttpMethodDirector.processRedirectResponse
(HttpMethodDirector.java:454)
	at org.apache.commons.httpclient.HttpMethodDirector.isRetryNeeded
(HttpMethodDirector.java:639)
	at org.apache.commons.httpclient.HttpMethodDirector.executeMethod
(HttpMethodDirector.java:145)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:378)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:268"
1,"Deadlock with MultiThreadedHttpConnectionManagerI'm getting a dealock with the MultiThreadedHttpConnectionManager. Usually, it
works fine, but when a web page is redirected, it blocks. 

Ludovic.

[ERROR] Redirect to http://sourceforge.net/
Full thread dump Java HotSpot(TM) Client VM (1.4.2_03-b02 mixed mode):

""MultiThreadedHttpConnectionManager cleanup"" daemon prio=5 tid=0x02d566f0
nid=0xe14 in Object.wait() [2e9f000..2e9fd8c]
        at java.lang.Object.wait(Native Method)
        - waiting on <0x10513be8> (a java.lang.ref.ReferenceQueue$Lock)
        at java.lang.ref.ReferenceQueue.remove(Unknown Source)
        - locked <0x10513be8> (a java.lang.ref.ReferenceQueue$Lock)
        at java.lang.ref.ReferenceQueue.remove(Unknown Source)
        at
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$ReferenceQueueThread.run(MultiThreadedHttpConnectionManager.java:805)

""Signal Dispatcher"" daemon prio=10 tid=0x0003da00 nid=0xd44 waiting on condition
[0..0]

""Finalizer"" daemon prio=9 tid=0x009bca30 nid=0xce8 in Object.wait()
[2b5f000..2b5fd8c]
        at java.lang.Object.wait(Native Method)
        - waiting on <0x10504b80> (a java.lang.ref.ReferenceQueue$Lock)
        at java.lang.ref.ReferenceQueue.remove(Unknown Source)
        - locked <0x10504b80> (a java.lang.ref.ReferenceQueue$Lock)
        at java.lang.ref.ReferenceQueue.remove(Unknown Source)
        at java.lang.ref.Finalizer$FinalizerThread.run(Unknown Source)

""Reference Handler"" daemon prio=10 tid=0x009bb600 nid=0xfa4 in Object.wait()
[2b1f000..2b1fd8c]
        at java.lang.Object.wait(Native Method)
        - waiting on <0x10504be8> (a java.lang.ref.Reference$Lock)
        at java.lang.Object.wait(Unknown Source)
        at java.lang.ref.Reference$ReferenceHandler.run(Unknown Source)
        - locked <0x10504be8> (a java.lang.ref.Reference$Lock)

""main"" prio=5 tid=0x00035e28 nid=0xf68 in Object.wait() [7f000..7fc3c]
        at java.lang.Object.wait(Native Method)
        - waiting on <0x105170e8> (a
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$ConnectionPool)
        at
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager.doGetConnection(MultiThreadedHttpConnectionManager.java:388)
        - locked <0x105170e8> (a
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$ConnectionPool)
        at
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager.getConnection(MultiThreadedHttpConnectionManager.java:296)
        at
org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:645)
        at
org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:529)
        at net.sourceforge.cvsgrab.WebBrowser.executeMethod(WebBrowser.java:201)
        at net.sourceforge.cvsgrab.WebBrowser.getResponse(WebBrowser.java:257)
        at net.sourceforge.cvsgrab.WebBrowser.getDocument(WebBrowser.java:295)
        at
net.sourceforge.cvsgrab.CvsWebInterface.loadDocument(CvsWebInterface.java:111)
        at
net.sourceforge.cvsgrab.CvsWebInterface.getDocumentForDetect(CvsWebInterface.java:216)
        at
net.sourceforge.cvsgrab.CvsWebInterface.findInterface(CvsWebInterface.java:86)
        at net.sourceforge.cvsgrab.CVSGrab.detectWebInterface(CVSGrab.java:688)
        at net.sourceforge.cvsgrab.CVSGrab.grabCVSRepository(CVSGrab.java:616)
        at net.sourceforge.cvsgrab.CVSGrab.run(CVSGrab.java:317)
        at net.sourceforge.cvsgrab.CVSGrab.main(CVSGrab.java:206)

""VM Thread"" prio=5 tid=0x009f76d0 nid=0x550 runnable

""VM Periodic Task Thread"" prio=10 tid=0x009f8208 nid=0x560 waiting on condition
""Suspend Checker Thread"" prio=10 tid=0x009bed88 nid=0xe84 runnable"
1,"Session.getUserID returns first principal in the set obtained from Subject.getPrincipals()this may lead to a wrong value for the UserID (e.g. the name of a Group principal).

jsr 170 defines the getUserID() to return "" the user ID associated with this Session."" and implies (javadoc) that the method has a relation to the login.

This issues has already been partially addressed while working on jsr 283 access control (trunk)."
1,DbDataStore keeps ResultSets openThe DbDataStore does not always close the ResultSet which can lead to memory leaks and/or large memory usage. It seems that  this already has been fixed in trunk and 1.5.
1,"AlreadyClosedException on initial index creationHappens when the indexing queue is checked while creating an initial index. This is probably a regression caused by JCR-2035.

Caused by: org.apache.lucene.store.AlreadyClosedException: this Directory is closed
        at org.apache.lucene.store.Directory.ensureOpen(Directory.java:220)
        at org.apache.lucene.store.FSDirectory.getFile(FSDirectory.java:533)
        at org.apache.jackrabbit.core.query.lucene.directory.FSDirectoryManager$FSDir.list(FSDirectoryManager.java:149)
        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:533)
        at org.apache.lucene.index.DirectoryIndexReader.open(DirectoryIndexReader.java:115)
        at org.apache.lucene.index.IndexReader.open(IndexReader.java:316)
        at org.apache.lucene.index.IndexReader.open(IndexReader.java:263)
        at org.apache.jackrabbit.core.query.lucene.AbstractIndex.getIndexReader(AbstractIndex.java:245)
        at org.apache.jackrabbit.core.query.lucene.AbstractIndex.removeDocument(AbstractIndex.java:225)
        at org.apache.jackrabbit.core.query.lucene.PersistentIndex.removeDocument(PersistentIndex.java:90)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex$DeleteNode.execute(MultiIndex.java:1952)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.executeAndLog(MultiIndex.java:1085)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.checkIndexingQueue(MultiIndex.java:1308)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.createIndex(MultiIndex.java:1177)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.createIndex(MultiIndex.java:1191)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.createIndex(MultiIndex.java:1191)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.createIndex(MultiIndex.java:1191)
[...]

I assume there is an index merge happening at the same time that closes index segments."
1,"Exception in deleteDocument, undeleteAll or setNorm in IndexReader can fail to release write lock on closeI hit this while working on LUCENE-140

We have 3 cases in the IndexReader methods above where we have this pattern:

  if (directoryOwner) acquireWriteLock();
  doSomething();
  hasChanges = true;

The problem is if you hit an exception in doSomething(), and hasChanges was not already true, then hasChanges will not have been set to true yet the write lock is held.  If you then try to close the reader without making any other changes, then the write lock is not released because in IndexReader.close() (well, in commit()) we only release write lock if hasChanges is true.

I think the simple fix is to swap the order of hasChanges = true and doSomething().  I already fixed one case of this under LUCENE-140 commit yesterday; I will fix the other two under this issue."
1,"AbstractLoginModule must not call abort() in commit()AbstractLoginModule.commit() currently may call abort() when it detects that the login did not succeed. abort() will reset any state in the login module, including state shared between multiple login modules like Principals in the Subject. When there actually are multiple module, this will delete shared state that was set by other login modules. Moreover, the method commit() is only called when the overall authentication succeeded. Thus, it seems strange to call abort() from within commit().
"
1,"Item.remove fails if a child-item is not visible to the editing sessionthe following test setup fails:

- a given session is allowed to remove a node
- the node has a policy child node which is not visible to the editing session (missing ac-read permission)
  OR the node has another invisible child item which could - based on the permissions above - be removed by that session.

calling Node.remove however fails with accessdeniedexception because the internal remove
mechanism accesses all child items to mark them removed. however, the access is executed
using the regular itemmgr calls that are used to retrieve the items using the JCR API which
results in accessdenied exception as those child items are not visible to the session.
since the items can be removed i would argue that this is a bug in the internal remove process.
"
1,"Response Folded Headers throws HttpExceptionAs of 4/4/02 CVS repository the HttpMethodBase class
doesn't handle folded headers in the 
readResponseHeaders method

HTTP/1.1 and HTTP/1.0 descriptions of folded headers (see 
section 2.2 Basic Rules)
http://www.ietf.org/rfc/rfc2616.txt
http://www-
old.ics.uci.edu/pub/ietf/http/rfc1945.html#Basic-Rules

I've prepared a patch and was 
emailed to jakarta-commons@jakarta.apache.org"
1,"InstantiatedIndexReader.norms called from MultiReader bugSmall bug in InstantiatedIndexReader.norms(String field, byte[] bytes, int offset) where the offset is not applied properly in the System.arraycopy"
1,"setFetchSize() fails in getAllNodeIds()I get the following exception from the PersistenceManagerIteratorTest on Windows:

org.apache.jackrabbit.core.state.ItemStateException: getAllNodeIds failed.
        at org.apache.jackrabbit.core.persistence.pool.BundleDbPersistenceManager.getAllNodeIds(BundleDbPersistenceManager.java:1043)
        at org.apache.jackrabbit.core.data.PersistenceManagerIteratorTest.testGetAllNodeIds(PersistenceManagerIteratorTest.java:106)
Caused by: java.sql.SQLException: Invalid parameter value '10'000' for Statement.setFetchSize(int rows).
        at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
        at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
        at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
        at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
        at org.apache.derby.impl.jdbc.EmbedConnection.newSQLException(Unknown Source)
        at org.apache.derby.impl.jdbc.ConnectionChild.newSQLException(Unknown Source)
        at org.apache.derby.impl.jdbc.EmbedStatement.setFetchSize(Unknown Source)
        at org.apache.commons.dbcp.DelegatingStatement.setFetchSize(DelegatingStatement.java:279)
        at org.apache.commons.dbcp.DelegatingStatement.setFetchSize(DelegatingStatement.java:279)
        at org.apache.jackrabbit.core.util.db.ConnectionHelper.reallyExec(ConnectionHelper.java:372)
        at org.apache.jackrabbit.core.util.db.ConnectionHelper$3.call(ConnectionHelper.java:353)
        at org.apache.jackrabbit.core.util.db.ConnectionHelper$3.call(ConnectionHelper.java:349)
        at org.apache.jackrabbit.core.util.db.ConnectionHelper$RetryManager.doTry(ConnectionHelper.java:472)
        at org.apache.jackrabbit.core.util.db.ConnectionHelper.exec(ConnectionHelper.java:349)
        at org.apache.jackrabbit.core.persistence.pool.BundleDbPersistenceManager.getAllNodeIds(BundleDbPersistenceManager.java:1020)

It's caused by the following code in ConnectionHelper when 0 < maxRows < 10000:

            stmt.setMaxRows(maxRows);
            stmt.setFetchSize(10000);

A simple fix would be:


            stmt.setMaxRows(maxRows);
            stmt.setFetchSize(Math.min(10000, maxRows));
"
1,"Concurrency issues in SegmentInfo.files() could lead to ConcurrentModificationExceptionThe multi-threaded call of the files() in SegmentInfo could lead to the ConcurrentModificationException if one thread is not finished additions to the ArrayList (files) yet while the other thread already obtained it as cached (see below). This is a rare exception, but it would be nice to fix. I see the code is no longer problematic in the trunk (and others ported from flex_1458), looks it was fixed while implementing post 3.x features. The fix to 3.x and 2.9.x branches could be the same - create the files set first and populate it, and then assign to the member variable at the end of the method. This will resolve the issue. I could prepare the patch for 2.9.4 and 3.x, if needed.

--

INFO: [19] webapp= path=/replication params={command=fetchindex&wt=javabin} status=0 QTime=1
Jul 30, 2010 9:13:05 AM org.apache.solr.core.SolrCore execute
INFO: [19] webapp= path=/replication params={command=details&wt=javabin} status=0 QTime=24
Jul 30, 2010 9:13:05 AM org.apache.solr.handler.ReplicationHandler doFetch
SEVERE: SnapPull failed
java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
        at java.util.AbstractList$Itr.next(AbstractList.java:343)
        at java.util.AbstractCollection.addAll(AbstractCollection.java:305)
        at org.apache.lucene.index.SegmentInfos.files(SegmentInfos.java:826)
        at org.apache.lucene.index.DirectoryReader$ReaderCommit.<init>(DirectoryReader.java:916)
        at org.apache.lucene.index.DirectoryReader.getIndexCommit(DirectoryReader.java:856)
        at org.apache.solr.search.SolrIndexReader.getIndexCommit(SolrIndexReader.java:454)
        at org.apache.solr.handler.SnapPuller.fetchLatestIndex(SnapPuller.java:261)
        at org.apache.solr.handler.ReplicationHandler.doFetch(ReplicationHandler.java:264)
        at org.apache.solr.handler.ReplicationHandler$1.run(ReplicationHandler.java:146)
"
1,"BLOBFileValue.read(byte[] b, long pos) ignores return value of InputStream.skipInputStream.skip(long n) returns a long, which may be different from the parameter n (possibly lower).
Currently in BLOBFileValue.read(byte[] b, long pos) the return value is ignored."
1,"Several bugs in last SVN commitJust a few bugs in the last SVN commit, but since I work with it, i thought useful to mention them :
1) org.apache.jackrabbit.ocm.reflection.ReflectionUtils should handle Set --> just add it in defaultImplementation
2) in org.apache.jackrabbit.ocm.manager.collectionconverter.ManageableObjectsUtil.getManageableObjects, correct defaultImplementation test :
        		if (defaultImplementation == null)
        		{
        			throw new JcrMappingException(""No default implementation for the interface "" + manageableObjectsClass);

Thank you and keep up the good work!

Sincerely yours,

Stephane"
1,"NTCollectionConverterImpl throws a null pointer exception on updateWhen calling update on a node which has no child nodes stored (but which can have child nodes) the code can generate a null pointer exception. In the case where one goes to remove JCR nodes which are not present in the current objects collection of child objects the code is calling getCollectionNodes().iterator(). However, since is not checking for the case where getCollectionNodes() returns null if there are no child nodes present a null pointer exception will be generated. "
1,"[PATCH] RussianAnalyzer's tokenizer skips numbers from input text,RussianAnalyzer's tokenizer skips numbers from input text, so that resulting token stream miss numbers. Problem can be solved by adding numbers to RussianCharsets.UnicodeRussian. See test case below  for details.

{code:title=TestRussianAnalyzer.java|borderStyle=solid}

public class TestRussianAnalyzer extends TestCase {

  Reader reader = new StringReader(""text 1000"");

  // test FAILS
  public void testStemmer() {
    testAnalyzer(new RussianAnalyzer());
  }

  // test PASSES
  public void testFixedRussianAnalyzer() {
    testAnalyzer(new RussianAnalyzer(getRussianCharSet()));
  }

  private void testAnalyzer(RussianAnalyzer analyzer) {
    try {
      TokenStream stream = analyzer.tokenStream(""text"", reader);
      assertEquals(""text"", stream.next().termText());
      assertNotNull(stream.next());
    } catch (IOException e) {
      fail(e.getMessage());
    }
  }

  private char[] getRussianCharSet() {
    int length = RussianCharsets.UnicodeRussian.length;
    final char[] russianChars = new char[length + 10];

    System
        .arraycopy(RussianCharsets.UnicodeRussian, 0, russianChars, 0, length);
    russianChars[length++] = '0';
    russianChars[length++] = '1';
    russianChars[length++] = '2';
    russianChars[length++] = '3';
    russianChars[length++] = '4';
    russianChars[length++] = '5';
    russianChars[length++] = '6';
    russianChars[length++] = '7';
    russianChars[length++] = '8';
    russianChars[length] = '9';
    return russianChars;
  }
}

{code} "
1,"Netscape proxy problem wtih POSTDescription:

When using httpClient to POST to a http url through a Netscape proxy server, 
the httpClient failed due to read error when reading status line.  The log seem 
to indicate that the proxy is talking HTTP/1.0 and does not expect the POST 
data to come.  I am using a modified version of the ClientApp from examples.  I 
will attach both the test program and log files.

Workaround:

If use PostMethod.setUseExpect (true), it will work.  But in many cases, it 
would be slower.

Related issues:

In doing the test, I also found out that the httpClient PostMehtod does not 
work when the request body is NOT set (not calling setRequestBody).  It also 
does not work with empty body (setRequestBody ("""")).  The attached 
clientApp.properties file has flags to test each case and I will attach the 
logs as well.  Excuse my ignorance, I do not know for sure what the HTTP spec. 
says about the body in the POST method.  But at least if the caller/app is 
wrong in not setting the body, some exception should be thrown.  It could also 
be my server's problem, please let me know if that is the case (I am using 
weblogic server 6.1)."
1,"Updates to multiple workspaces (e.g. in a transaction) locked in cluster journalRunning a transaction that updates multiple workspaces (e.g. a versioning operation) will be locked, as they all try to acquire a non-reentrant lock in the cluster's journal. Short-term fix is to make the lock re-entrant. In the long run, a transaction context sensitive lock may be more appropriate.

How to reproduce: enable clustering in the test environment and let the test o.a.j.core.XATest.testSetVersionLabel() run. This will result in a deadlock when committing the operation.

This was initially reported by Rafa Kwiecie as a problem when using springmodules and clustering but turned out to be general problem with transactions and clustering. Thanks for reporting it!"
1,"RangeQuery & RangeFilter used with collation seek to lowerTerm using compareTo()The constructor for RangeTermEnum initializes a TermEnum starting with lowerTermText, but when a collator is defined, all terms in the given field need to be checked, since collation can introduce non-Unicode orderings.  Instead, the RangeTermEnum constructor should test for a non-null collator, and if there is one, point the TermEnum at the first term in the given field.

LUCENE-1424 introduced this bug."
1,"spi2dav: avoid reusing the same document in repositoryserviceimpl... instead each call should create it's own document (credits due to jukka :)
that seems to avoid that odd npe in DomUtil."
1,TestPagedBytes failureant test -Dtestcase=TestPagedBytes -Dtestmethod=testDataInputOutput -Dtests.seed=268db1f3329b70d:3125365bc9c56c90:116e02aa4a70ec2f -Dtests.multiplier=5
1,NOT_ANALYZED fields can double-count offsetsIf the same field name has 2 NOT_ANALYZED field instances then the offsets are double-counted.
1,Unable to add/lock and unlock/remove Node with shared Session in 2 TransactionsIf you try to unlock and remove a node the NodeState can be run out of sync between the two operations.
1,"Instantiated/IndexWriter discrepanies * RAMDirectory seems to do a reset on tokenStreams the first time, this permits to initialise some objects before starting streaming, InstantiatedIndex does not.
 * I can Serialize a RAMDirectory but I cannot on a InstantiatedIndex because of : java.io.NotSerializableException: org.apache.lucene.index.TermVectorOffsetInfo

http://www.nabble.com/InstatiatedIndex-questions-to20576722.html

"
1,"NRT reader/writer over RAMDirectory memory leakwith NRT reader/writer, emptying an index using:
writer.deleteAll()
writer.commit()
doesn't release all allocated memory.

for example the following code will generate a memory leak:

/**
	 * Reveals a memory leak in NRT reader/writer<br>
	 * 
	 * The following main() does 10K cycles of:
	 * <ul>
	 * <li>Add 10K empty documents to index writer</li>
	 * <li>commit()</li>
	 * <li>open NRT reader over the writer, and immediately close it</li>
	 * <li>delete all documents from the writer</li>
	 * <li>commit changes to the writer</li>
	 * </ul>
	 * 
	 * Running with -Xmx256M results in an OOME after ~2600 cycles
	 */
	public static void main(String[] args) throws Exception {
		RAMDirectory d = new RAMDirectory();
		IndexWriter w = new IndexWriter(d, new IndexWriterConfig(Version.LUCENE_33, new KeywordAnalyzer()));
		Document doc = new Document();
		
		for(int i = 0; i < 10000; i++) {
			for(int j = 0; j < 10000; ++j) {
				w.addDocument(doc);
			}
			w.commit();
			IndexReader.open(w, true).close();

			w.deleteAll();
			w.commit();
		}
		
		w.close();
		d.close();
	}	"
1,"Version history recovery fails in case a version does not have a jcr:frozenNodeWith JCR-2551 in place, a version recovery mode has been introduced. Problem now is that in case a version is encountered that misses a mandatory jcr:frozenNode, an InternalError is thrown by o.a.j.c.version.InternalVersionHistoryImpl#createVersionInstance. Since o.a.j.c.RepositoryChecker#checkVersionHistory only catches Exception, it fails to catch it properly which leads to a complete repository shutdown.

Throwing for example a RuntimeException instead fixes the problem."
1,"Invalid journal records during XATransactionsDuring the prepare phase of a XATransaction, XAItemStateManager.prepare calls ShareItemStageManager.beginUpdate that, in case of a ClusterNode, calls ClusterNode.updatePrepared that does a ChangeLogRecord.write().

This last method is located in ClusterRecord and systematically write the begin and the end of the journal record.

As a consequence, useless corrupted records are written in the journal everytime a transaction ends without jackrabbit update! This causes polution of the journal, as other cluster nodes try to sync with the corrupted updates and fail doing so as ClusterRecordDeserializer can't deserialize it (the record identifier is empty).

ChangeLogRecord (and even other ClusterRecord implementations too) should only write if there's effective updates.

I propose the following solution:
*) add the following method in Changelog so clients can know if there's effective updates:
    public boolean hasUpdates() {
    	return !(addedStates.isEmpty() && modifiedStates.isEmpty() && deletedStates.isEmpty() && modifiedRefs.isEmpty());
    }

*) change ClusterRecord with:
    public final void write() throws JournalException {
    	
    	if (hasUpdates()) {
	        record.writeString(workspace);
	        doWrite();
	        record.writeChar(END_MARKER);
    	}
    }
    
    protected abstract boolean hasUpdates();

*) implement hasUpdates for every ClusterRecord implementation:
 ----> ChangeLogRecord:
    protected boolean hasUpdates() {
    	return changes.hasUpdates() || !events.isEmpty();
    }
 ----> LockRecord and NamespaceRecord:
    protected boolean hasUpdates() {
    	return true;
    }

 ----> NodeTypeRecord:
    protected boolean hasUpdates() {
    	return !collection.isEmpty();
    }

Best regards,

Stephane Landelle"
1,TermsFilter.getDocIdSet(context) NPE on missing fieldIf the context does not contain the field for a term when calling TermsFilter.getDocIdSet(AtomicReaderContext context) then a NullPointerException is thrown due to not checking for null Terms before getting iterator.
1,"Surround Query doesn't properly handle equals/hashcodeIn looking at using the surround queries with Solr, I am hitting issues caused by collisions due to equals/hashcode not being implemented on the anonymous inner classes that are created by things like DistanceQuery (branch 3.x, near line 76)"
1,"CorruptIndexException on indexing after a failure occurs after segments file creation but before any bytes are writtenFSDirectory.createOutput(..) uses a RandomAccessFile to do its work.  On my system the default FSDirectory.open(..) creates an NIOFSDirectory.  If createOutput is called on a segments_* file and a crash occurs between RandomAccessFile creation (file system shows a segments_* file exists but has zero bytes) but before any bytes are written to the file, subsequent IndexWriters cannot proceed.  The difficulty is that it does not know how to clear the empty segments_* file.  None of the file deletions will happen on such a segment file because the opening bytes cannot not be read to determine format and version.

An initial proposed patch file is attached below.

"
1,"error handling duplicate connection headersHttpMethodBase.shouldCloseConnection() does not correctly handle the case when
more than one connection header exists.  Reported by Ross Rankin."
1,"FieldBoostMapAttribute in contrib/qp is broken.While looking for more SuppressWarnings in lucene, i came across two of them in contrib/queryparser.

even worse, i found these revolved around using maps with CharSequence as key.

From the javadocs for CharSequence:

This interface does not refine the general contracts of the equals and hashCode methods. The result of comparing two objects that implement CharSequence is therefore, in general, undefined. Each object may be implemented by a different class, and there is no guarantee that each class will be capable of testing its instances for equality with those of the other. {color:red} It is therefore inappropriate to use arbitrary CharSequence instances as elements in a set or as keys in a map. {color}

"
1,"Jackrabbit thread contention issue due to fat lockHello,

We are running jackrabbit 1.4.5 using a persistent file data store within a weblogic container and encountering a variety of thread locking issues. To get around the problem, we are forced synchronize thread access to the JCR repository or reduce thread worker count to 1 which has a heavy performance impact on our application. I'm not exactly sure what the problem is and was wondering someone is looking into this issue and if there is a workaround/fix planned?

<Oct 30, 2008 10:45:32 AM PDT> <Error> <WebLogicServer> <BEA-000337> <[STUCK] ExecuteThread: '5' for queue: 'weblogic.kernel.Default (self-tuning)' has been busy for ""1,863"" seconds working on the request ""weblogic.work.j2ee.J2EEWorkManager$WorkWithListener@2117cc9"", which is more than the configured time (StuckThreadMaxTime) of ""1,800"" seconds. Stack trace:
Thread-94 ""[STUCK] ExecuteThread: '5' for queue: 'weblogic.kernel.Default (self-tuning)'"" <alive, in native, suspended, waiting, priority=1, DAEMON> {
    -- Waiting for notification on: EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock@152c384[fat lock]
    java.lang.Object.wait(Object.java:???)
    java.lang.Object.wait(Object.java:474)
    EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock.acquire(Unknown Source)
    org.apache.jackrabbit.core.journal.AbstractJournal.lockAndSync(AbstractJournal.java:235)
    org.apache.jackrabbit.core.journal.DefaultRecordProducer.append(DefaultRecordProducer.java:49)

}

>
<Oct 30, 2008 10:45:32 AM PDT> <Error> <WebLogicServer> <BEA-000337> <[STUCK] ExecuteThread: '2' for queue: 'weblogic.kernel.Default (self-tuning)' has been busy for ""1,916"" seconds working on the request ""weblogic.work.j2ee.J2EEWorkManager$WorkWithListener@227b6d4"", which is more than the configured time (StuckThreadMaxTime) of ""1,800"" seconds. Stack trace:
Thread-25 ""[STUCK] ExecuteThread: '2' for queue: 'weblogic.kernel.Default (self-tuning)'"" <alive, in native, suspended, waiting, priority=1, DAEMON> {
    -- Waiting for notification on: EDU.oswego.cs.dl.util.concurrent.LinkedNode@42d58e0[fat lock]
    java.lang.Object.wait(Object.java:???)
    java.lang.Object.wait(Object.java:474)
    EDU.oswego.cs.dl.util.concurrent.SynchronousChannel.put(Unknown Source)
    EDU.oswego.cs.dl.util.concurrent.PooledExecutor$WaitWhenBlocked.blockedAction(Unknown Source)
    EDU.oswego.cs.dl.util.concurrent.PooledExecutor.execute(Unknown Source)
...
    org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:334)
    org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:307)
    org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:317)
    org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1072)
    ^-- Holding lock: org.apache.jackrabbit.core.query.lucene.VolatileIndex@3eb0f41[thin lock]
    org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:895)
    org.apache.jackrabbit.jca.JCASessionHandle.save(JCASessionHandle.java:178)
    com.qpass.inventory.ingestion.IngestionServiceImpl$1.doInJCR(IngestionServiceImpl.java:124)
    com.qpass.inventory.model.JCRTemplate.execute(JCRTemplate.java:17)
    com.qpass.inventory.ingestion.IngestionServiceImpl.ingestProduct(IngestionServiceImpl.java:93)
    com.qpass.inventory.ingestion.bulk.AbstractBulkIngester.ingestProduct(AbstractBulkIngester.java:42)
    com.qpass.inventory.ingestion.bulk.ZipFileBulkIngester.processFile(ZipFileBulkIngester.java:35)
    com.qpass.inventory.ingestion.bulk.IngestionWorker.processFile(IngestionWorker.java:26)
    com.qpass.inventory.ingestion.bulk.IngestionWorker$1.run(IngestionWorker.java:64)
    org.springframework.scheduling.commonj.DelegatingWork.run(DelegatingWork.java:61)
    weblogic.work.j2ee.J2EEWorkManager$WorkWithListener.run(J2EEWorkManager.java:245)
    weblogic.work.ExecuteThread.execute(ExecuteThread.java:206)
    weblogic.work.ExecuteThread.run(ExecuteThread.java:173)
}

>
<Oct 30, 2008 10:45:32 AM PDT> <Error> <WebLogicServer> <BEA-000337> <[STUCK] ExecuteThread: '1' for queue: 'weblogic.kernel.Default (self-tuning)' has been busy for ""1,891"" seconds working on the request ""weblogic.work.j2ee.J2EEWorkManager$WorkWithListener@2117c83"", which is more than the configured time (StuckThreadMaxTime) of ""1,800"" seconds. Stack trace:
Thread-24 ""[STUCK] ExecuteThread: '1' for queue: 'weblogic.kernel.Default (self-tuning)'"" <alive, in native, suspended, waiting, priority=1, DAEMON> {
    -- Waiting for notification on: EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock@152c384[fat lock]
    java.lang.Object.wait(Object.java:???)
    java.lang.Object.wait(Object.java:474)
    EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock.acquire(Unknown Source)
    org.apache.jackrabbit.core.journal.AbstractJournal.lockAndSync(AbstractJournal.java:235)
    org.apache.jackrabbit.core.journal.DefaultRecordProducer.append(DefaultRecordProducer.java:49)
    org.apache.jackrabbit.core.cluster.ClusterNode$WorkspaceUpdateChannel.updateCreated(ClusterNode.java:556)
...


<Oct 30, 2008 11:21:30 AM PDT> <Warning> <netuix> <BEA-423420> <Redirect is executed in begin or refresh action. Redirect url is /console/console.portal?_nfpb=true&_pageLabel=HomePage1.>
<Oct 30, 2008 11:44:32 AM PDT> <Error> <WebLogicServer> <BEA-000337> <[STUCK] ExecuteThread: '4' for queue: 'weblogic.kernel.Default (self-tuning)' has been busy for ""1,803"" seconds working on the request ""Http Request: /inventory/rpc/searchService"", which is more than the configured time (StuckThreadMaxTime) of ""1,800"" seconds. Stack trace:
Thread-51 ""[STUCK] ExecuteThread: '4' for queue: 'weblogic.kernel.Default (self-tuning)'"" <alive, in native, suspended, waiting, priority=1, DAEMON> {
    -- Waiting for notification on: java.lang.Object@1569e04[fat lock]
    java.lang.Object.wait(Object.java:???)
    java.lang.Object.wait(Object.java:474)
    org.apache.jackrabbit.core.query.lucene.MultiIndex.getIndexReader(MultiIndex.java:694)
    org.apache.jackrabbit.core.query.lucene.SearchIndex.getIndexReader(SearchIndex.java:825)
    org.apache.jackrabbit.core.query.lucene.SearchIndex.executeQuery(SearchIndex.java:682)
    org.apache.jackrabbit.core.query.lucene.QueryResultImpl.executeQuery(QueryResultImpl.java:242)
    org.apache.jackrabbit.core.query.lucene.QueryResultImpl.getResults(QueryResultImpl.java:271)
    org.apache.jackrabbit.core.query.lucene.QueryResultImpl.<init>(QueryResultImpl.java:177)
    org.apache.jackrabbit.core.query.lucene.QueryImpl.execute(QueryImpl.java:105)
    org.apache.jackrabbit.core.query.QueryImpl.execute(QueryImpl.java:174)
    com.qpass.inventory.service.QueryProfiler.execute(QueryProfiler.java:20)
    com.qpass.inventory.service.SearchServiceImpl$1.doInJCR(SearchServiceImpl.java:59)
    com.qpass.inventory.model.JCRTemplate.execute(JCRTemplate.java:17)
    com.qpass.inventory.service.SearchServiceImpl.doSearch(SearchServiceImpl.java:54)
    com.qpass.inventory.ui.impl.SearchUIServiceImpl.search(SearchUIServiceImpl.java:48)
    sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:???)
    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:27)
    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    java.lang.reflect.Method.invoke(Method.java:570)
    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:309)
    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:182)
    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:148)
    org.acegisecurity.intercept.method.aopalliance.MethodSecurityInterceptor.invoke(MethodSecurityInterceptor.java:62)
    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:148)
    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:151)
    $Proxy74.search(Unknown Source)
    sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:???)
    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:27)
    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    java.lang.reflect.Method.invoke(Method.java:570)
    org.gwtwidgets.server.spring.GWTRPCServiceExporter.invokeMethodOnService(GWTRPCServiceExporter.java:157)
    org.gwtwidgets.server.spring.GWTRPCServiceExporter.processCall(GWTRPCServiceExporter.java:295)
    com.google.gwt.user.server.rpc.RemoteServiceServlet.doPost(RemoteServiceServlet.java:173)
    org.gwtwidgets.server.spring.GWTRPCServiceExporter.handleRequest(GWTRPCServiceExporter.java:361)
    com.qpass.base.ui.security.GWTServiceExporter.handleRequest(GWTServiceExporter.java:45)
    org.springframework.web.servlet.mvc.HttpRequestHandlerAdapter.handle(HttpRequestHandlerAdapter.java:49)
    org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:831)
    org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:781)
    org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:567)
    org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:511)
    javax.servlet.http.HttpServlet.service(HttpServlet.java:736)
    javax.servlet.http.HttpServlet.service(HttpServlet.java:851)
    weblogic.servlet.internal.StubSecurityHelper$ServletServiceAction.run(StubSecurityHelper.java:224)
    weblogic.servlet.internal.StubSecurityHelper.invokeServlet(StubSecurityHelper.java:108)
    weblogic.servlet.internal.ServletStubImpl.execute(ServletStubImpl.java:198)
    weblogic.servlet.internal.TailFilter.doFilter(TailFilter.java:26)
    weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:41)
    org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:259)
    org.acegisecurity.intercept.web.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:93)
    org.acegisecurity.intercept.web.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:71)
    org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:259)
    com.qpass.usersecurity.auth.UpdatePermissionsOnContextChangeFilter.doFilter(UpdatePermissionsOnContextChangeFilter.java:44)
    org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:259)
    org.acegisecurity.ui.AbstractProcessingFilter.doFilter(AbstractProcessingFilter.java:191)
    org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:259)
    org.acegisecurity.context.HttpSessionContextIntegrationFilter.doFilter(HttpSessionContextIntegrationFilter.java:195)
    org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:259)
    org.acegisecurity.util.FilterChainProxy.doFilter(FilterChainProxy.java:122)
    org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:236)
    org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:154)
    weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:41)
    com.qpass.base.applicationcontext.RequestContextFilter.doFilter(RequestContextFilter.java:103)
    org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:236)
    org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:154)
    weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:41)
    org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:90)
    org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:61)
    weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:41)
    weblogic.servlet.internal.RequestEventsFilter.doFilter(RequestEventsFilter.java:24)
    weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:41)
    weblogic.servlet.internal.WebAppServletContext$ServletInvocationAction.run(WebAppServletContext.java:3214)
    weblogic.security.acl.internal.AuthenticatedSubject.doAs(AuthenticatedSubject.java:308)
    weblogic.security.service.SecurityManager.runAs(SecurityManager.java:117)
    weblogic.servlet.internal.WebAppServletContext.securedExecute(WebAppServletContext.java:1946)
    weblogic.servlet.internal.WebAppServletContext.execute(WebAppServletContext.java:1868)
    weblogic.servlet.internal.ServletRequestImpl.run(ServletRequestImpl.java:1331)
    weblogic.work.ExecuteThread.execute(ExecuteThread.java:206)
    weblogic.work.ExecuteThread.run(ExecuteThread.java:173)
}




<Oct 2, 2008 2:09:36 PM PDT> <Error> <WebLogicServer> <BEA-000337> <[STUCK] ExecuteThread: '1' for queue: 'weblogic.kernel.Default (self-tuning)' has been busy for ""696"" seconds working on the request ""weblogic.work.j2ee.J2EEWorkManager$WorkWithListener@863e564"", which is more than the configured time (StuckThreadMaxTime) of ""600"" seconds. Stack trace:
Thread-21 ""[STUCK] ExecuteThread: '1' for queue: 'weblogic.kernel.Default (self-tuning)'"" <alive, in native, suspended, priority=1, DAEMON> {
    java.io.FileOutputStream.writeBytes(FileOutputStream.java:???)
    java.io.FileOutputStream.write(FileOutputStream.java:260)
    java.io.BufferedOutputStream.write(BufferedOutputStream.java:100)
    ^-- Holding lock: java.io.BufferedOutputStream@39d70a5[thin lock]
    org.apache.jackrabbit.core.persistence.util.FileSystemBLOBStore.put(FileSystemBLOBStore.java:88)
    org.apache.jackrabbit.core.persistence.bundle.util.BundleBinding.writeState(BundleBinding.java:561)
    org.apache.jackrabbit.core.persistence.bundle.util.BundleBinding.writeBundle(BundleBinding.java:245)
    org.apache.jackrabbit.core.persistence.bundle.Oracle9PersistenceManager.storeBundle(Oracle9PersistenceManager.java:114)
    ^-- Holding lock: org.apache.jackrabbit.core.persistence.bundle.Oracle9PersistenceManager@140f7b9[thin lock]
    org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.putBundle(AbstractBundlePersistenceManager.java:703)
    org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.store(AbstractBundlePersistenceManager.java:526)
    ^-- Holding lock: org.apache.jackrabbit.core.persistence.bundle.Oracle9PersistenceManager@140f7b9[thin lock]
    org.apache.jackrabbit.core.persistence.bundle.BundleDbPersistenceManager.store(BundleDbPersistenceManager.java:517)
    ^-- Holding lock: org.apache.jackrabbit.core.persistence.bundle.Oracle9PersistenceManager@140f7b9[thin lock]
    org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:699)
    org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:873)
    org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:334)
    org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:334)
    org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:307)
    org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:317)
    org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1072)
    ^-- Holding lock: org.apache.jackrabbit.core.XASessionImpl@1f2653b[thin lock]
    org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:895)
    org.apache.jackrabbit.jca.JCASessionHandle.save(JCASessionHandle.java:178)
    com.qpass.inventory.ingestion.IngestionServiceImpl$1.doInJCR(IngestionServiceImpl.java:140)
    com.qpass.inventory.model.JCRTemplate.execute(JCRTemplate.java:17)
    com.qpass.inventory.ingestion.IngestionServiceImpl.ingestProduct(IngestionServiceImpl.java:112)
    ^-- Holding lock: java.lang.Object@849ca9e[fat lock]
    com.qpass.inventory.ingestion.bulk.AbstractBulkIngester.ingestProduct(AbstractBulkIngester.java:42)
    com.qpass.inventory.ingestion.bulk.ZipFileBulkIngester.processFile(ZipFileBulkIngester.java:35)
    com.qpass.inventory.ingestion.bulk.IngestionWorker.processFile(IngestionWorker.java:26)
    com.qpass.inventory.ingestion.bulk.IngestionWorker$1.run(IngestionWorker.java:64)
    org.springframework.scheduling.commonj.DelegatingWork.run(DelegatingWork.java:61)
    weblogic.work.j2ee.J2EEWorkManager$WorkWithListener.run(J2EEWorkManager.java:245)
    weblogic.work.ExecuteThread.execute(ExecuteThread.java:206)
    weblogic.work.ExecuteThread.run(ExecuteThread.java:173)
"
1,"Simple Webdav: dir with same name as repository has incorrect behaviorUsing a default repository named ""default"" for example, creating a directory named ""default"" in that repository will have issues.  The directory will not behave correctly. 

Viewing the dir in Jetty http://localhost:8080/jackrabbit/repository/default/default/ will show the contents of the ""default"" directory.  However, clicking on any of the contents will go to an incorrect URL.  E.g. if a directory named ""test"" was created, then the URL for test will be ""http://localhost:8080/jackrabbit/repository/default/test/""  instead of ""http://localhost:8080/jackrabbit/repository/default/default/test/"".

Notice that there is only one ""default"" in the path provided by jackrabbit.  

This causes the contents of such directories to be inaccessible"
1,"DatabaseFileSystem: mysql.ddl works for mysql5 but not mysql 4.1.20Perhaps a new column ( primary key ) could get added to the table called uid, which is actually an md5checksum of FSENTRY_PATH and FSENTRY_NAME."
1,"SingleClientConnectionManager Needs to Recreate UniquePoolEntryDue to the change yesterday of adding some state into DefaultClientConnection (remembering when shutdown was called & aborting the next opening), SingeClientConnectionManager now breaks when subsequent requests are performed if the first one encountered an exception or was aborted.  

Attaching a patch with the fix + a testcase (that previously failed)."
1,"PerFieldCodecWrapper causes crashes if not all per field codes have been usedIf a PerFieldCodecWrapper is used an SegmentMerger tries to merge two segments where one segment only has a subset of the field PerFieldCodecWrapper defines SegmentMerger tries to open non-existing files since Codec#files(Directory, SegmentInfo, Set<String>) blindly copies the expected files into the given set. This also hits exceptions in CheckIndex and addIndexes(). 
The reason for this is that PerFieldCodecWrapper simply iterates over the codecs it knows and adds all files without checking if they are present in the given Directory. We need to have some mechnanism that check if the ""required"" files for a codec are present and only add the files to the set if that field is really there.

"
1,FastVectorHighlighter adds a multi value separator (space) to the end of the highlighted textThe FVH adds an additional ' ' (the multi value separator) to the end of the highlighted text.
1,"NodeState and NodeStateListener deadlock

Java stack information for the threads listed above:
===================================================
""jmssecondaryApplnJobExecutor-8"":
	at org.apache.jackrabbit.core.state.NodeState.getChildNodeEntry(NodeState.java:300)
	- waiting to lock <0x9e6c6d08> (a org.apache.jackrabbit.core.state.NodeState)
	at org.apache.jackrabbit.core.CachingHierarchyManager.nodeModified(CachingHierarchyManager.java:316)
	- locked <0xa09882a8> (a java.lang.Object)
	at org.apache.jackrabbit.core.CachingHierarchyManager.stateModified(CachingHierarchyManager.java:293)
	at org.apache.jackrabbit.core.state.StateChangeDispatcher.notifyStateModified(StateChangeDispatcher.java:111)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.stateModified(SessionItemStateManager.java:889)
	at org.apache.jackrabbit.core.state.StateChangeDispatcher.notifyStateModified(StateChangeDispatcher.java:111)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.stateModified(LocalItemStateManager.java:452)
	at org.apache.jackrabbit.core.state.XAItemStateManager.stateModified(XAItemStateManager.java:602)
	at org.apache.jackrabbit.core.state.StateChangeDispatcher.notifyStateModified(StateChangeDispatcher.java:111)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.stateModified(SharedItemStateManager.java:400)
	at org.apache.jackrabbit.core.state.ItemState.notifyStateUpdated(ItemState.java:244)
	at org.apache.jackrabbit.core.state.ChangeLog.persisted(ChangeLog.java:297)
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:749)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:1115)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:351)
	at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:354)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:326)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:325)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1111)
	- locked <0x9b1b0be0> (a org.apache.jackrabbit.core.XASessionImpl)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:915)
	at org.apache.jackrabbit.jca.JCASessionHandle.save(JCASessionHandle.java:180)
        ...
	at sun.reflect.GeneratedMethodAccessor1067.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:36)
	at sun.reflect.GeneratedMethodAccessor110.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:243)
	at javax.management.modelmbean.RequiredModelMBean.invokeMethod(RequiredModelMBean.java:1074)
	at javax.management.modelmbean.RequiredModelMBean.invoke(RequiredModelMBean.java:955)
	at org.springframework.jmx.export.SpringModelMBean.invoke(SpringModelMBean.java:88)
	at org.jboss.mx.server.RawDynamicInvoker.invoke(RawDynamicInvoker.java:164)
	at org.jboss.mx.modelmbean.RequiredModelMBeanInvoker.invoke(RequiredModelMBeanInvoker.java:127)
	at org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)
	at org.jboss.system.server.jmx.LazyMBeanServer.invoke(LazyMBeanServer.java:291)
	at javax.management.MBeanServerInvocationHandler.invoke(MBeanServerInvocationHandler.java:288)
	at $Proxy692.doDiscoveryNow(Unknown Source)
        ...
	at org.springframework.jms.listener.AbstractMessageListenerContainer.doInvokeListener(AbstractMessageListenerContainer.java:531)
	at org.springframework.jms.listener.AbstractMessageListenerContainer.invokeListener(AbstractMessageListenerContainer.java:466)
	at org.springframework.jms.listener.AbstractMessageListenerContainer.doExecuteListener(AbstractMessageListenerContainer.java:435)
	at org.springframework.jms.listener.AbstractPollingMessageListenerContainer.doReceiveAndExecute(AbstractPollingMessageListenerContainer.java:322)
	at org.springframework.jms.listener.AbstractPollingMessageListenerContainer.receiveAndExecute(AbstractPollingMessageListenerContainer.java:260)
	at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.invokeListener(DefaultMessageListenerContainer.java:944)
	at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.run(DefaultMessageListenerContainer.java:868)
	at java.lang.Thread.run(Thread.java:619)
""jmssecondaryApplnJobExecutor-7"":
	at org.apache.jackrabbit.core.CachingHierarchyManager.nodeAdded(CachingHierarchyManager.java:362)
	- waiting to lock <0xa09882a8> (a java.lang.Object)
	at org.apache.jackrabbit.core.state.StateChangeDispatcher.notifyNodeAdded(StateChangeDispatcher.java:159)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.nodeAdded(SessionItemStateManager.java:947)
	at org.apache.jackrabbit.core.state.NodeState.notifyNodeAdded(NodeState.java:882)
	at org.apache.jackrabbit.core.state.NodeState.addChildNodeEntry(NodeState.java:351)
	- locked <0x9e6c6d08> (a org.apache.jackrabbit.core.state.NodeState)
	at org.apache.jackrabbit.core.NodeImpl.createChildNode(NodeImpl.java:541)
	- locked <0xa00619a8> (a org.apache.jackrabbit.core.NodeImpl)
	at org.apache.jackrabbit.core.NodeImpl.internalAddChildNode(NodeImpl.java:802)
	at org.apache.jackrabbit.core.NodeImpl.internalAddNode(NodeImpl.java:735)
	at org.apache.jackrabbit.core.NodeImpl.addNodeWithUuid(NodeImpl.java:2200)
	- locked <0xa00619f8> (a org.apache.jackrabbit.core.NodeImpl)
	at org.apache.jackrabbit.core.NodeImpl.addNode(NodeImpl.java:2133)
	- locked <0xa00619f8> (a org.apache.jackrabbit.core.NodeImpl)
        ...
	at sun.reflect.GeneratedMethodAccessor1067.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:36)
	at sun.reflect.GeneratedMethodAccessor110.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:243)
	at javax.management.modelmbean.RequiredModelMBean.invokeMethod(RequiredModelMBean.java:1074)
	at javax.management.modelmbean.RequiredModelMBean.invoke(RequiredModelMBean.java:955)
	at org.springframework.jmx.export.SpringModelMBean.invoke(SpringModelMBean.java:88)
	at org.jboss.mx.server.RawDynamicInvoker.invoke(RawDynamicInvoker.java:164)
	at org.jboss.mx.modelmbean.RequiredModelMBeanInvoker.invoke(RequiredModelMBeanInvoker.java:127)
	at org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)
	at org.jboss.system.server.jmx.LazyMBeanServer.invoke(LazyMBeanServer.java:291)
	at javax.management.MBeanServerInvocationHandler.invoke(MBeanServerInvocationHandler.java:288)
	at $Proxy692.doDiscoveryNow(Unknown Source)
        ...
	at org.springframework.jms.listener.AbstractMessageListenerContainer.doInvokeListener(AbstractMessageListenerContainer.java:531)
	at org.springframework.jms.listener.AbstractMessageListenerContainer.invokeListener(AbstractMessageListenerContainer.java:466)
	at org.springframework.jms.listener.AbstractMessageListenerContainer.doExecuteListener(AbstractMessageListenerContainer.java:435)
	at org.springframework.jms.listener.AbstractPollingMessageListenerContainer.doReceiveAndExecute(AbstractPollingMessageListenerContainer.java:322)
	at org.springframework.jms.listener.AbstractPollingMessageListenerContainer.receiveAndExecute(AbstractPollingMessageListenerContainer.java:260)
	at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.invokeListener(DefaultMessageListenerContainer.java:944)
	at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.run(DefaultMessageListenerContainer.java:868)
	at java.lang.Thread.run(Thread.java:619)

Found 1 deadlock.

"
1,"Under Heavy load in a Cluster HTTP Threads Block and stall requestsUnder Heavy load created by mounting both nodes in the cluster in OSX Finder and then uploading large numebers of files to each node at the same time ( a few 1000), eventually one of the nodes stops responding and the Finder mount timesout and disconnects.

Once that happens that node becomes unusable.
More mount attempts will prompt for a password indicating HTTP is still running, but will timeout once the connection is authenticated.
Access by the Web Browser will prompt for a password, conenct and provide a once only listing of any collection in the workspace. If you try to refresh that collection, the HTTP request hangs forever."
1,"URI.getHost() generates IllegalArgumentExceptionHi guys,

I don't know if I'm doing something wrong or not but the following code:

   URI uri = new URI(""mailto:eay@cryptsoft.com"", true);
   System.out.println(uri.getHost());

generates the following exception:

java.lang.IllegalArgumentException: Component array of chars may not be null
	at org.apache.commons.httpclient.URI.decode(URI.java:1722)
	at org.apache.commons.httpclient.URI.getHost(URI.java:2780)

Could you help?

Also, I'm sorry I put the report under version ""3.0 Final"" but I couldn't find 
an entry for ""3.0-RC1"" (which I'm using at the moment).

Thanks a lot!

Bisser"
1,"potential infinite loop around InternalVersionImpl.getSuccessorsThere's an infinite loop waiting to happen when the underlying persisted version storage is defect:

{noformat}
at
org.apache.jackrabbit.core.version.InternalVersionImpl.getSuccessors(InternalVersionImpl.java:148)

at
org.apache.jackrabbit.core.version.InternalVersionHistoryImpl.init(InternalVersionHistoryImpl.java:165)

at
org.apache.jackrabbit.core.version.InternalVersionHistoryImpl.reload(InternalVersionHistoryImpl.java:180)

at
org.apache.jackrabbit.core.version.InternalVersionHistoryImpl.getVersion(InternalVersionHistoryImpl.java:299) 
{noformat}

(line numbers from 2.2)

What happens here is that when a version can not be instantiated, reload() is called, which in turn calls init(), which, as part of piece of code labeled ""fix legacy"" will call getSuccessors(), which in turn wants to instantiate versions.

"
1,"MultiTermsEnum over-shares between different Docs/AndPositionsEnumRobert found this in working on LUCENE-2352.

MultiTermsEnum incorrectly shared sub-enums on two different invocation of .docs/AndPositionsEnum."
1,"Lock.getNode() does not return lock holderspec: "" N.getLock().getNode() (where N is a locked node) ... If N is in the subtree of the lock holder, H, then this call will return H.""

now N is returned."
1,"MultiPhraseQuery sums its own idf instead of Similarity.MultiPhraseQuery is a generalized version of PhraseQuery, and computes IDF the same way by default (by summing across the terms).

The problem is it doesn't let the Similarity do this: PhraseQuery calls Similarity.idfExplain(Collection<Term> terms, IndexSearcher searcher),
but MultiPhraseQuery just sums itself, calling Similarity.idf(int, int) for each term.

"
1,"ConcurrentModificationException in IndexMergerThe IndexMerger.start() method can cause the following ConcurrentModificationException to be thrown since it doesn't protect against concurrent access to the busyMergers list. The workers started by the start() method will remove themselves from the busyMergers list, which makes it possible for a quick worker to concurrently modify the list before the start() method is finished iterating through it.

java.util.ConcurrentModificationException
	at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
	at java.util.AbstractList$Itr.next(AbstractList.java:343)
	at org.apache.jackrabbit.core.query.lucene.IndexMerger.start(IndexMerger.java:122)
	at org.apache.jackrabbit.core.query.lucene.MultiIndex.<init>(MultiIndex.java:325)
	at org.apache.jackrabbit.core.query.lucene.SearchIndex.doInit(SearchIndex.java:507)
	at org.apache.jackrabbit.core.query.AbstractQueryHandler.init(AbstractQueryHandler.java:78)
	at org.apache.jackrabbit.core.config.RepositoryConfigurationParser$1.getQueryHandler(RepositoryConfigurationParser.java:630)
	at org.apache.jackrabbit.core.config.WorkspaceConfig.getQueryHandler(WorkspaceConfig.java:215)
	at org.apache.jackrabbit.core.config.WorkspaceConfig.getQueryHandler(WorkspaceConfig.java:215)
	at org.apache.jackrabbit.core.SearchManager.<init>(SearchManager.java:173)
	at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.getSearchManager(RepositoryImpl.java:1868)
	at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.doPostInitialize(RepositoryImpl.java:2077)
	at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.initialize(RepositoryImpl.java:1996)
	at org.apache.jackrabbit.core.RepositoryImpl.initStartupWorkspaces(RepositoryImpl.java:535)
"
1,"null domains break Cookie.javathe domain is assumed to be non-null in a few places in Cookie.java, see matches
method, for example"
1,"PROPPATCH on collection gets 403 ForbiddenDefaultHandler.canImport(PropertyImportContext, boolean) prevents setting properties (PROPPATCH) on collections through WebDAV ... returns 403 Forbidden. It checks to see whether the contextItem is not a collection, or has a jcr:content node. This test fails for a collection and should probably allow collections or nodes that have a jcr:content subnode. Here is a patch for the change

Index: jackrabbit-jcr-server/src/main/java/org/apache/jackrabbit/server/io/DefaultHandler.java
===================================================================
--- jackrabbit-jcr-server/src/main/java/org/apache/jackrabbit/server/io/DefaultHandler.java	(revision 567695)
+++ jackrabbit-jcr-server/src/main/java/org/apache/jackrabbit/server/io/DefaultHandler.java	(working copy)
@@ -570,7 +570,7 @@
         }
         Item contextItem = context.getImportRoot();
         try {
-            return contextItem != null && contextItem.isNode() && (!isCollection || ((Node)contextItem).hasNode(JcrConstants.JCR_CONTENT));
+            return contextItem != null && contextItem.isNode() && (isCollection || ((Node)contextItem).hasNode(JcrConstants.JCR_CONTENT));
         } catch (RepositoryException e) {
             log.error(""Unexpected error: "" + e.getMessage());
             return false;
"
1,"JCR-RMI problem with large binary valuesAs reported on the mailing list, a JCR-RMI connection will hang when given a binary value that's larger than the default 64kB buffer size."
1,"FuzzyQuery produces a ""java.lang.NegativeArraySizeException"" in PriorityQueue.initialize if I use Integer.MAX_VALUE as BooleanQuery.MaxClauseCountPriorityQueue creates an ""java.lang.NegativeArraySizeException"" when initialized with Integer.MAX_VALUE, because Integer overflows. I think this could be a general problem with PriorityQueue. The Error occured when I set BooleanQuery.MaxClauseCount to Integer.MAX_VALUE and user a FuzzyQuery for searching."
1,"UUID check in BundleFsPersistenceManager.getListRecursive() leads to endless loopThe UUID comparison in getListRecursive() is wrong and leads to an endless loop when the test PersistenceManagerIteratorTest.getAllNodeIds() is run on a workspace using BundleFsPersistenceManager.

I'm not sure this always happens, but for sure in a workspace with no content (just root and jcr:system nodes).

There's also an problem with the test case. In batch mode the after NodeId is set to the last id returned by the previous get all nodes fetch. This means batch retrieval is never actually tested, because there is no NodeId after the last one."
1,"Query for all node fails after restartThe query handler initially indexes the node type definitions exposed at /jcr:system/jcr:nodetypes. However after a restart or a node type registration the UUIDs of those nodes will change because they consist of VirtualNodeStates. The index will still use the UUIDs of the initial indexing and will return a query result that refers to UUIDs that do not exist in the workspace anymore.

As an short term solution we should disable indexing of VirtualNodeStates.

Please note, this does not only apply to xpath queries but also sql queries of course."
1,"[jcr-rmi] workspace.copy doesn't workswapped parameters in org/apache/jackrabbit/rmi/client/ClientWorkspace/ClientWorkspace. 
"
1,"incorrect snippet returned with SpanScorerThis problem was reported by my customer. They are using Solr 1.3 and uni-gram, but it can be reproduced with Lucene 2.9 and WhitespaceAnalyzer.

{panel:title=Query}
(f1:""a b c d"" OR f2:""a b c d"") AND (f1:""b c g"" OR f2:""b c g"")
{panel}

The snippet we expected is:
{panel}
x y z <B>a</B> <B>b</B> <B>c</B> <B>d</B> e f g <B>b</B> <B>c</B> <B>g</B>
{panel}

but we got:
{panel}
x y z <B>a</B> b c <B>d</B> e f g <B>b</B> <B>c</B> <B>g</B>
{panel}

Program to reproduce the problem:
{code}
public class TestHighlighter {

  static final String CONTENT = ""x y z a b c d e f g b c g"";
  static final String PH1 = ""\""a b c d\"""";
  static final String PH2 = ""\""b c g\"""";
  static final String F1 = ""f1"";
  static final String F2 = ""f2"";
  static final String F1C = F1 + "":"";
  static final String F2C = F2 + "":"";
  static final String QUERY_STRING =
    ""("" + F1C + PH1 + "" OR "" + F2C + PH1 + "") AND (""
    + F1C + PH2 + "" OR "" + F2C + PH2 + "")"";
  static Analyzer analyzer = new WhitespaceAnalyzer();
  
  public static void main(String[] args) throws Exception {
    QueryParser qp = new QueryParser( F1, analyzer );
    Query query = qp.parse( QUERY_STRING );
    CachingTokenFilter stream = new CachingTokenFilter( analyzer.tokenStream( F1, new StringReader( CONTENT ) ) );
    Scorer scorer = new SpanScorer( query, F1, stream, false );
    Highlighter h = new Highlighter( scorer );
    System.out.println( ""query : "" + QUERY_STRING );
    System.out.println( h.getBestFragment( analyzer, F1,  CONTENT ) );
  }
}
{code}
"
1,"Merge error during add to index (IndexOutOfBoundsException)I've been batch-building indexes, and I've build a couple hundred indexes with 
a total of around 150 million records.  This only happened once, so it's 
probably impossible to reproduce, but anyway... I was building an index with 
around 9.6 million records, and towards the end I got this:

java.lang.IndexOutOfBoundsException: Index: 54, Size: 24
        at java.util.ArrayList.RangeCheck(ArrayList.java:547)
        at java.util.ArrayList.get(ArrayList.java:322)
        at org.apache.lucene.index.FieldInfos.fieldInfo(FieldInfos.java:155)
        at org.apache.lucene.index.FieldInfos.fieldName(FieldInfos.java:151)
        at org.apache.lucene.index.SegmentTermEnum.readTerm(SegmentTermEnum.java
:149)
        at org.apache.lucene.index.SegmentTermEnum.next
(SegmentTermEnum.java:115)
        at org.apache.lucene.index.SegmentMergeInfo.next
(SegmentMergeInfo.java:52)
        at org.apache.lucene.index.SegmentMerger.mergeTermInfos
(SegmentMerger.java:294)
        at org.apache.lucene.index.SegmentMerger.mergeTerms
(SegmentMerger.java:254)
        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:93)
        at org.apache.lucene.index.IndexWriter.mergeSegments
(IndexWriter.java:487)
        at org.apache.lucene.index.IndexWriter.maybeMergeSegments
(IndexWriter.java:458)
        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:310)
        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:294)"
1,"URLEncodedUtils fails to parse form-url-encoded entities that specify a charsetIf a form-url-encoded HTTP entity specifies a charset in its Content-Type header, then URLEncodedUtils.parse(HttpEntity) fails to parse it.

An entity with content type ""application/x-www-form-urlencoded; charset=UTF-8"" should be detected as form-url-encoded and parsed as such, honoring the specified character set. Currently the code requires an exact, case-insensitive match with ""application/x-www-form-urlencoded"" for an entity to be detected as form-url-encoded.

It appears that the author of URLEncodedUtils.parse(HttpEntity) tried to take character sets into account, but expected to find them in the Content-Encoding header instead of as a parameter in the Content-Length header. The HTTP 1.1 spec makes it clear that the Content-Encoding header is for specifying transformations like gzip compression or the identity transformation -- not for specifying the entity's character set.

Here are some helpful links.
http://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.4
http://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.5
http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.11

This is related to: https://issues.apache.org/jira/browse/HTTPCLIENT-884"
1,"Workspace.getImportHandler() doesn't handle namespace declarations in document view when they are reported as attributesXMIDocumentViewImportTest is copy of DocumentViewImportTest EXCEPT that createSimpleDocument is overridden.

New simple document is typical of XMI serializations from Eclipse Modeling Framework (EMF).

Four out of eight tests fail due to bad uri    Trace below:

javax.jcr.NamespaceException: www.apache.org/jackrabbit/test/namespaceImportTest7: is not a registered namespace uri.
	at org.apache.jackrabbit.core.NamespaceRegistryImpl.getPrefix(NamespaceRegistryImpl.java:378)
	at org.apache.jackrabbit.core.LocalNamespaceMappings.getPrefix(LocalNamespaceMappings.java:193)
	at org.apache.jackrabbit.core.SessionImpl.getNamespacePrefix(SessionImpl.java:1307)
	at org.apache.jackrabbit.test.api.XMIDocumentViewImportTest.checkImportSimpleXMLTree(XMIDocumentViewImportTest.java:176)
	at org.apache.jackrabbit.test.api.XMIDocumentViewImportTest.performTests(XMIDocumentViewImportTest.java:154)
	at org.apache.jackrabbit.test.api.XMIDocumentViewImportTest.doTestImportXML(XMIDocumentViewImportTest.java:119)
	at org.apache.jackrabbit.test.api.XMIDocumentViewImportTest.testWorkspaceImportXml(XMIDocumentViewImportTest.java:70)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at org.apache.jackrabbit.test.AbstractJCRTest.run(AbstractJCRTest.java:393)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:128)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)

"
1,"addIndexes(IndexReader) incorrectly applies existing deletesIf you perform these operations:
# deleteDocuments(Term) for all the new documents
# addIndexes(IndexReader)
# commit

Then addIndexes applies the previous deletes on the incoming indexes as well, which is incorrect. If you call addIndexes(Directory) instead, the deletes are applied beforehand, as they should. The solution, as Mike indicated here: http://osdir.com/ml/general/2011-03/msg20876.html is to add *flush(false,true)* to addIndexes(IndexReader).

I will create a patch with a matching unit test shortly."
1,"NodeType.canSetProperty() does not include type conversionfor example, NodeType.canSetProperty(String propertyName, Value value) must return true if the property requires a StringValue and value is a DateValue (but does not)"
1,In case of ConnectTimeoutException : HttpRequestRetryHandler is not used.
1,Disallow unregistering of node types still (possibly) in use
1,"Lock.obtain(timeout) behaves incorrectly for large timeoutsBecause timeout is a long, but internal values derived from timeout
are ints, its possible to overflow those internal values into negative
numbers and cause incorrect behavior.

Spinoff from this thread:

  http://www.gossamer-threads.com/lists/lucene/java-user/54376

"
1,"Parametrizing H1 and H2The DFR normalizations {{H1}} and {{H2}} are parameter-free. This is in line with the [original article|http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.101.742], but not with the [thesis|http://theses.gla.ac.uk/1570/], where H2 accepts a {{c}} parameter, nor with [information-based models|http://dl.acm.org/citation.cfm?id=1835490], where H1 also accepts a {{c}} parameter."
1,"webdav: nullpointer exception while getting the tikka detector seems to be introduced by https://issues.apache.org/jira/browse/JCR-2334

05.11.2009 14:28:27 *MARK * servletengine: Servlet threw exception: 
java.lang.NullPointerException
	at org.apache.jackrabbit.server.io.DefaultHandler.detect(DefaultHandler.java:668)
	at org.apache.jackrabbit.server.io.XmlHandler.canExport(XmlHandler.java:152)
	at org.apache.jackrabbit.server.io.DefaultHandler.canExport(DefaultHandler.java:557)
	at org.apache.jackrabbit.server.io.PropertyManagerImpl.exportProperties(PropertyManagerImpl.java:58)
	at org.apache.jackrabbit.webdav.simple.DavResourceImpl.initProperties(DavResourceImpl.java:320)
	at org.apache.jackrabbit.webdav.simple.DeltaVResourceImpl.initProperties(DeltaVResourceImpl.java:248)
	at org.apache.jackrabbit.webdav.simple.VersionControlledResourceImpl.initProperties(VersionControlledResourceImpl.java:320)
	at org.apache.jackrabbit.webdav.simple.DavResourceImpl.getProperties(DavResourceImpl.java:300)
	at org.apache.jackrabbit.webdav.MultiStatusResponse.<init>(MultiStatusResponse.java:181)
	at org.apache.jackrabbit.webdav.MultiStatus.addResourceProperties(MultiStatus.java:62)
	at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.doPropFind(AbstractWebdavServlet.java:447)
	at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.execute(AbstractWebdavServlet.java:235)
	at com.day.crx.j2ee.CRXDavServlet.service(CRXDavServlet.java:76)
	at com.day.crx.j2ee.ResourceServlet.service(ResourceServlet.java:97)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
	at com.day.j2ee.servletengine.ServletRuntimeEnvironment.service(ServletRuntimeEnvironment.java:228)
	at com.day.j2ee.servletengine.RequestDispatcherImpl.doFilter(RequestDispatcherImpl.java:315)
	at com.day.j2ee.servletengine.RequestDispatcherImpl.service(RequestDispatcherImpl.java:334)
	at com.day.j2ee.servletengine.RequestDispatcherImpl.service(RequestDispatcherImpl.java:378)
	at com.day.j2ee.servletengine.ServletHandlerImpl.execute(ServletHandlerImpl.java:313)
	at com.day.j2ee.servletengine.DefaultThreadPool$DequeueThread.run(DefaultThreadPool.java:134)
	at java.lang.Thread.run(Thread.java:613)"
1,"BasicCookieStore.getCookies() returns non-threadsafe collectionBasicCookieStore.getCookies() is a simple method.  It's synchronized, and it returns an unmodifiable wrapper around the underlying cookie list.  If the caller were to then iterate over it as another thread were to manipulate the cookie list via BasicCookieStore, this would create a thread un-safe situation because both threads aren't doing their reading/writing with the same lock (the reader doesn't even have a lock).

I suggest fixing this by using CopyOnWriteArrayList, or by making a defensive copy in getCookies()

This issue might apply to some of the other basic implementations of some of the interfaces but I haven't checked."
1,"Generated cluster node id should be persistedIf no cluster node id is specified in the configuration, a cluster node id is automatically generated. This id is never persisted, so after another startup a new, probably different cluster node id is used. Instead, an automatically generated cluster id should be persisted inside the repository home."
1,"Redirect 302 to the same URL causes max redirects exceptionI noticed that if the server returns a 302 without a URL in the link, the 
HttpClient follows the empty URL up to the maximum times (100 by default).  
Instead it should check and if the URL is an empty string it shouldn't try to 
follow the redirect.

12:18:17,430 [U:          ] [main                ] ERROR 
HttpMethodBase               - Narrowly avoided an infinite loop in execute
12:18:17,430 [U:          ] [main                ] DEBUG 
URLMonitor                   - Method.execute attempt 1 failed 
http://www.stagecoach.co.uk: 
org.apache.commons.httpclient.HttpRecoverableException: Maximum redirects (100) 
exceeded
12:18:17,430 [U:          ] [main                ] DEBUG 
URLMonitor                   - HttpRecoverableException 
(http://www.stagecoach.co.uk) : 
org.apache.commons.httpclient.HttpRecoverableException: Maximum redirects (100) 
exceeded
	at org.apache.commons.httpclient.HttpMethodBase.execute
(HttpMethodBase.java:1065)
	at com.verideon.veriguard.domain.URLMonitor.monitor(URLMonitor.java:189)
	at com.verideon.veriguard.domain.URLMonitor.monitor(URLMonitor.java:101)
	at com.verideon.veriguard.domain.TestURLMonitor.getPage
(TestURLMonitor.java:58)
	at com.verideon.veriguard.domain.TestURLMonitor.monitorURL
(TestURLMonitor.java:47)
	at com.verideon.veriguard.domain.TestURLMonitor.testMonitorURLStageCoach
(TestURLMonitor.java:138)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke
(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke
(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:324)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests
(RemoteTestRunner.java:392)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run
(RemoteTestRunner.java:276)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main
(RemoteTestRunner.java:167)

Result with telnet:

GET /
HTTP/1.1 302 Object moved
Server: Microsoft-IIS/5.0
Date: Tue, 01 Jul 2003 10:05:58 GMT
X-Powered-By: ASP.NET
Location: http://www.stagecoach.co.uk
Connection: Keep-Alive
Content-Length: 121
Content-Type: text/html
Set-Cookie: ASPSESSIONIDCQCSRAAB=IFJJLEADOPDDNNGHLPFBIIIE; path=/
Cache-control: private

<head><title>Object moved</title></head>
<body><h1>Object Moved</h1>This object may be found <a HREF="""">here</a>.</body>
Connection closed by foreign host."
1,"Virtual host setting does not apply when parsing and matching cookiesVirtual host setting does not apply when parsing and matching cookies.

Problem has been reported on the httpclient-dev list by Dan Levine"
1,"Rollback doesn't preserve integrity of original indexAfter several ""updateDocuments"" calls a rollback call does not return the index to the prior state.
This seems to occur if the number of updates exceeds the RAM buffer size i.e. when some flushing of updates occurs.

Test fails in Lucene 2.4, 2.9, 3.0.1 and 3.0.2

JUnit to follow.
"
1,"SO_TIMEOUT not set early enough for SOCKS proxies in PlainSocketFactoryI've created my own delegating SchemeSocketFactory implementation which supports setting SOCKS proxies on socket creation. In the connectSocket implementation, I previously just delegated to PlainSocketFactory.

The problem there was, that the SO_TIMEOUT was not set on the socket before the connection was established through the SOCKS proxy. This lead to a stop on the native read0 method because the socket is endlessly waiting for a read to occur from the proxy, so it can continue with the the connect to the actual socket destination through the proxy. I made sure I set the SO_TIMEOUT parameter in HttpParams, but it did not get honored by PlainSocketFactory.

To fix this and make HttpClient honor SO_TIMEOUT for SOCKS proxies, the following line has to be added:
  sock.setSoTimeout(HttpConnectionParams.getSoTimeout(params));
in PlainSocketFactory.connectSocket(...).

Heres the complete fixed method:

PlainSocketFactory:            
    public Socket connectSocket(
            final Socket socket,
            final InetSocketAddress remoteAddress,
            final InetSocketAddress localAddress,
            final HttpParams params) throws IOException, ConnectTimeoutException {
        if (remoteAddress == null) {
            throw new IllegalArgumentException(""Remote address may not be null"");
        }
        if (params == null) {
            throw new IllegalArgumentException(""HTTP parameters may not be null"");
        }
        Socket sock = socket;
        if (sock == null) {
            sock = createSocket();
        }
        if (localAddress != null) {
            sock.setReuseAddress(HttpConnectionParams.getSoReuseaddr(params));
            sock.bind(localAddress);
        }
        
        //FIX for SOCKS proxies which get stalled if they don't answer
        sock.setSoTimeout(HttpConnectionParams.getSoTimeout(params));
        
        int timeout = HttpConnectionParams.getConnectionTimeout(params);
        try {
            sock.connect(remoteAddress, timeout);
        } catch (SocketTimeoutException ex) {
            throw new ConnectTimeoutException(""Connect to "" + remoteAddress.getHostName() + ""/""
                    + remoteAddress.getAddress() + "" timed out"");
        }
        return sock;
    }

Currently I've implemented this in my delegating SchemeSocketFactory, because PlainSocketFactory misses this setting.

Dunno if there are other implementations of SocketFactory in HttpClient, which might need this fix. Anyway I hope this helps other people who get  headaches about halting threads because they use SOCKS proxies. :)"
1,"NPE in RepositoryServiceImpl.getPropertyInfo()under unknown conditions, i get a NPE in get property info, such as the 'getValue()' of the getstring dav property is null:

            } else if (props.contains(JCR_GET_STRING)) {
                // single valued non-binary property
                String str = props.get(JCR_GET_STRING).getValue().toString();
                QValue qValue = ValueFormat.getQValue(str, propertyType, getNamePathResolver(sessionInfo), getQValueFactory(sessionInfo));
                return new PropertyInfoImpl(propertyId, p, propertyType, qValue);
            } else {

the other properties in the propset are:
 - getstring: null
 - type: String
 - length: 0

the property in question is the last property of a node and it's an empty string. the error only occurs on certain usage patterns, but consistently. maybe depending on the fetch-depth or internal cache.

extending the check to:
            } else if (props.contains(JCR_GET_STRING) && props.get(JCR_GET_STRING).getValue() != null) {

solves the problem.
 
"
1,"Deleting binary property does not remove 'blob file' in filesystemwhen deleting a binary property or its containing node, the 'blob-file' sometime does not get removed.

the reason for this, is an open FileInputStream, that gets referenced in the property value."
1,Port fix for HTTPCLIENT-633 to 4.0The fix for MultiThreadedHttpConnectionManager from HTTPCLIENT-633 should be ported to ThreadSafeClientConnManager in 4.0.
1,"WebDAV LocatorFactoryImpl$Locator.getHref() constructs root resource URLs incorrectlycadaver was reporting an error when i tried to open / in my repository's default workspace at <https://localhost:8443/webdav/).

in tracking down the problem, i saw something strange - the multistatus response's href had an extra ""/"" tacked onto the end:

  <D:multistatus xmlns:D=""DAV:"">
    <D:response>
      <D:href>https://localhost:8443/webdav//</D:href>

WebdavServlet (rather, my subclass of it) is mapped as the default servlet of a webapp mounted at /webdav. i've configured the WebdavServlet with a resource path prefix of """" (incidentally, i'm not sure what that's meant to be used for - i see that when the value is not empty, it's appended to the response's href, but i don't know in what circumstance that would be useful).

when i requested a child node such as <https://localhost:8443/webdav/bcm>, the response's href was formed as expected:

  <D:multistatus xmlns:D=""DAV:"">
    <D:response>
      <D:href>https://localhost:8443/webdav/bcm/</D:href>

i found that LocatorFactoryImpl$Locator.getHref() was adding the extra ""/"" since the requested resource was a collection. i patched the method to not add the character when itemPath == ""/"", and cadaver stopped complaining. all is well.

i also patched WebdavServlet to default to an empty resource path prefix if one is not specified as a servlet init parameter.
"
1,"small float underflow detection bugUnderflow detection in small floats has a bug, and can incorrectly result in a byte value of 0 for a non-zero float."
1,"ResponseCachingPolicy uses integers for sizesResponseCachingPolicy currently uses integers for interpreting the size of Content-Length, as well internally.

This causes issues in attempting to use the module for caching entities that are over 2GB in size, the module does not fail gracefully, but throws a NumberFormatException

I have a patch that fixes this, by promoting the int -> long, which should allow for larger entities to be cached, it also updates the public facing API where possible, I don't think that the promotion should break compatibility massively

The changes can also be seen here:
https://github.com/GregBowyer/httpclient/commit/1197d3f94bd2eedcec32646cd6146748ca2e6fa1"
1,"NullPointerException from SegmentInfos.FindSegmentsFile.run() if FSDirectory.list() returns NULL Found this bug while running unit tests to verify an upgrade of our system from 1.4.3 to 2.1.0.  This bug did *not* occur during 1.4.3, it is new to 2.x (I'm pretty sure it's 2.1-only)

If the index directory gets deleted out from under Lucene after the FSDirectory has been created, then attempts to open an IndexWriter or IndexReader will result in an NPE.  Lucene should be throwing an IOException in this case.

Repro:
    1) Create an FSDirectory pointing somewhere in the filesystem (e.g. /foo/index/1)
    2) rm -rf the parent dir (rm -rf /foo/index)
    3) Try to open an IndexReader

Result: NullPointerException on line ""for(int i=0;i<files.length;i++) { "" -- 'files' is NULL.
 
Expect: IOException


....  

This is happening because of a missing NULL check in SegmentInfos$FindSegmentsFile.run():

        if (0 == method) {
          if (directory != null) {
            files = directory.list();
          } else {
            files = fileDirectory.list();
          }

          gen = getCurrentSegmentGeneration(files);

          if (gen == -1) {
            String s = """";
            for(int i=0;i<files.length;i++) { 
              s += "" "" + files[i];
            }
            throw new FileNotFoundException(""no segments* file found: files:"" + s);
          }
        }


The FSDirectory constructor will make sure the index dir exists, but if it is for some reason deleted out from underneath Lucene after the FSDirectory is instantiated, then java.io.File.list() will return NULL.  Probably better to fix FSDirectory.list() to just check for null and return a 0-length array:

(in org/apache/lucene/store/FSDirectory.java)
314c314,317
<         return directory.list(IndexFileNameFilter.getFilter());
---
>     String[] toRet = directory.list(IndexFileNameFilter.getFilter());
>     if (toRet == null)
>         return new String[]{};
>     return toRet;
"
1,"ItemState constructor throws IllegalArgumentExceptionWhen running ConcurrentReadWriteTest it may happen that a reading session gets an IllegalArgumentException:

Exception in thread ""Thread-7"" java.lang.IllegalArgumentException: illegal status: 0
	at org.apache.jackrabbit.core.state.ItemState.<init>(ItemState.java:138)
	at org.apache.jackrabbit.core.state.PropertyState.<init>(PropertyState.java:79)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.getPropertyState(LocalItemStateManager.java:121)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.getItemState(LocalItemStateManager.java:152)
	at org.apache.jackrabbit.core.state.XAItemStateManager.getItemState(XAItemStateManager.java:226)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:175)
	at org.apache.jackrabbit.core.ItemManager.createItemInstance(ItemManager.java:495)
	at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:326)
	at org.apache.jackrabbit.core.LazyItemIterator.prefetchNext(LazyItemIterator.java:90)
	at org.apache.jackrabbit.core.LazyItemIterator.<init>(LazyItemIterator.java:75)
	at org.apache.jackrabbit.core.ItemManager.getChildProperties(ItemManager.java:485)
	at org.apache.jackrabbit.core.NodeImpl.getProperties(NodeImpl.java:2481)
	at org.apache.jackrabbit.core.ConcurrentReadWriteTest$1$1.execute(ConcurrentReadWriteTest.java:61)
	at org.apache.jackrabbit.core.AbstractConcurrencyTest$Executor.run(AbstractConcurrencyTest.java:107)
	at java.lang.Thread.run(Thread.java:595)

Status 0 is STATUS_UNDEFINED. I think the following happens: when the reading session retrieves the ItemState from the SharedItemStateManager it is still valid but a short time later the writing session removes the item and changes the status to STATUS_UNDEFINED. Then the reading session tries to create an overlayed ItemState for the LocalItemStateManager using the changed status.

Adding the STATUS_UNDEFINED to the list of 'valid' status in the ItemState constructor seems to solve the issue, but I'm not sure if that's the right way to do it.

Opinions?"
1,"SQL2 query: QOMFormatter create incorrect NOT conditionsThen the following query is parsed:
SELECT test.* FROM test WHERE (NOT test.name = 'Hello') AND test.id = 3
then the SQL statement generated, it becomes:
SELECT test.* FROM test WHERE NOT test.name = 'Hello' AND test.id = 3
which is parsed differently and becomes:
SELECT test.* FROM test WHERE NOT (test.name = 'Hello' AND test.id = 3)"
1,"redefinition of xml-namespace mapping should not be allowedthe following throws an exception, but should work:

// remap xml namespace -> works
Session.setNamespacePrefix(""foobar"", ""http://www.w3.org/XML/1998/namespace"");

// revert mapping -> throws exception
Session.setNamespacePrefix(""xml"", ""http://www.w3.org/XML/1998/namespace"");"
1,"FileDataStore: garbage collection can delete files that are still neededIt looks like the FileDataStore garbage collection (both regular scan and persistence manager scan) can delete files that are still needed.

Currently it looks like the reason is the last access time resolution of the operating system. This is 2 seconds for FAT and Mac OS X, NTFS 100 ns, and 1 second for other file systems. That means file that are scanned at the very beginning are sometimes deleted, because they have a later last modified time then when the scan was started."
1,"Spell Checker suggestSimilar throws NPE when IndexReader is not null and field is nullThe SpellChecker.suggestSimilar(String word, int numSug, IndexReader ir,   String field, boolean morePopular) throws a NullPointerException when the IndexReader is not null, but the Field is.  The Javadocs say that it is fine to have the field be null, but doesn't comment on the fact that the IndexReader also needs to be null in that case.

"
1,"MultiThreadedHttpConnectionManager daemon Thread never GC'dOne of my colleagues was invoking HttpClient by way of a loop something like this:

for (int i = 0; i < 300; i++) {
    GetMethod method = new
GetMethod(""http://cvs.apache.org/viewcvs/jakarta-commons/httpclient/"");
    try {
        HttpClient httpClient = new HttpClient(new
MultiThreadedHttpConnectionManager());
        httpClient.executeMethod(method);
        byte[] bytes = method.getResponseBody();
    } finally {
        // always release the connection after we're done
        method.releaseConnection();
    }
}

He's in the process of revising his code so that he doesn't do this loop, which
other developers might point out as a non-optimal use, but along the way, he
discovered that the daemon thread that the MultiThreadedHttpConnectionManager
makes does not get garbage collected.  Of course, the connection manager itself
is also never gc'd.  While I think we can avoid this problem in our code, in the
more general case, clients may not actually be able to control the number of
MultiThreadedConnectionManagers they create, which could eventually cause
problems.  This makes me think the problem is deserving of a patch.

We found this problem with 2.0rc2, although presumably it also exists with the
CVS HEAD.

Patch to follow."
1,"shareable nodes: wrong path returned, causes remove() to delete wrong nodeIt seems that for shareable nodes it can happen that getPath() returns the wrong path (one of another node in the shared set):

/**
* Verify that shared nodes return correct paths.
*/
public void testPath() throws Exception {
   Node a1 = testRootNode.addNode(""a1"");
   Node a2 = a1.addNode(""a2"");
   Node b1 = a1.addNode(""b1"");
   b1.addMixin(""mix:shareable"");
   testRootNode.save();

   //now we have a shareable node N with path a1/b1

   Session session = testRootNode.getSession();
   Workspace workspace = session.getWorkspace();
   String path = a2.getPath() + ""/b2"";
   workspace.clone(workspace.getName(), b1.getPath(), path, false);

   //now we have another shareable node N' in the same shared set as N with path a1/a2/b2

   //using the path a1/a2/b2, we should get the node N' here
   Item item = session.getItem(path);
   String p = item.getPath();
   assertFalse(""unexpectedly got the path from another node from the same shared set "", p.equals(b1.getPath()));
} 

Note that when this happens, a subsequent remove() deletes the wrong node.

(Thanks Manfred for spotting this one)."
1,"Possible NPE in HttpHostHttpHost line 167 says:
        if (this.port != this.protocol.getDefaultPort()) {

However, a few lines above, protocol is checked for null.

Line 167 should probably read:

        if (this.protocol != null && this.port != this.protocol.getDefaultPort()) {
"
1,"Nodes having OPV=Ignore are removed on restoreJCR1.0 Specification mentions:

8.2.11.5 IGNORE
  Child Node
    On checkin of N, no state information about C will be stored in VN.
    On restore of VN, the child node C of the current N will remain and not be removed.
  Property
    On checkin of N, no state information about P will be stored in VN.
    On restore of VN, the property P of the current N will remain and not be removed.

but the current implementation removed the ignore child."
1,Lucene can incorrectly set the position of tokens that start a field with positonInc 0.More info in LUCENE-1465
1,"TestParallelTermEnum fails with Sep codecreproduceable in the 'preflexfixes' branch (since we test all codecs there) with: ant test-core -Dtestcase=TestParallelTermEnum -Dtests.codec=Sep

But I think there are probably more tests like this that have only been run with Standard and we might find more like this.
I don't think this should block LUCENE-2554.

{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestParallelTermEnum
    [junit] Testcase: test1(org.apache.lucene.index.TestParallelTermEnum):      Caused an ERROR
    [junit] read past EOF
    [junit] java.io.IOException: read past EOF
    [junit]     at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:154)
    [junit]     at org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:39)
    [junit]     at org.apache.lucene.store.DataInput.readVInt(DataInput.java:86)
    [junit]     at org.apache.lucene.index.codecs.sep.SingleIntIndexInput$Reader.next(SingleIntIndexInput.java:64)
    [junit]     at org.apache.lucene.index.codecs.sep.SepPostingsReaderImpl$SepDocsEnum.nextDoc(SepPostingsReaderImpl.java:316)
    [junit]     at org.apache.lucene.index.TestParallelTermEnum.test1(TestParallelTermEnum.java:188)
    [junit]     at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:316)
    [junit]
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.009 sec
    [junit]
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: random codec of testcase 'test1' was: Sep
    [junit] ------------- ---------------- ---------------
{noformat}
"
1,"session.setNamespacePrefix() creates ambiguous mappings1.) assume the following initial global mappings in the NamespaceRegistry 
(prefixes in lowercase, URIs in uppercase):

a  <-> A
b  <-> B
c  <-> C

2.) locally remap  the namespaces in a session using the following code:

            session.setNamespacePrefix(""x"", ""B"");
            session.setNamespacePrefix(""b"", ""C"");
            session.setNamespacePrefix(""c"", ""B"");

this results in the following session-local mappings:

a  <-> A
c  <-> B
b  <-> C

3.) now the following stmt:

            session.setNamespacePrefix(""b"", ""A"");

produces this ambiguous mapping:

b  <-> A
c  <?> B
c  <?> C

"
1,"Cannot clone BasicClientCookie2 without specified portsThe clone method returns a null pointer exception when called on a BasicClientCookie2 that does not use any ports properties.
In other words, it is impossible to clone a BasicClientCookie2 instance without ports specification.

In the clone() method, they are two main instructions :
 - calling clone() method on super
 - calling clone() method on the ports integer array (which is null)

It may be a good idea to check whether the array is null or not

"
1,"Saving concurrent sessions executing random operations causes a corrupt JCRRun the attached unit test. Several concurrent sessions add, move, and remove nodes. Then the index is removed and the repository is again started. The repository is in an inconsistent state and the index cannot be rebuild. Also a lot of exceptions occur. See (see Output before patch.txt). Note that the unit test also suffers from the deadlock of issue http://issues.apache.org/jira/browse/JCR-2525 about half the time."
1,"ConnPoolByRoute driving RouteSpecificPool to IllegalStateHi all,

I encountered an issue on ConnPoolByRoute / RouteSpecificPool on HTTPClient 4.0.1, akin to HTTPCLIENT-747 (it also leads to a java.lang.IllegalStateException: No entry created for this pool. HttpRoute[{}XXX] ), but it is not a concurrency issue (no race condition, just a logic error if I understood it correctly).

From my understanding, the error lies in ConnPoolByRoute#getEntryBlocking
Quoting from the code (line 309-314) :
RouteSpecificPool rospl = getRoutePool(route, true);
... 
} else if (hasCapacity && !freeConnections.isEmpty()) {

deleteLeastUsedEntry();
entry = createEntry(rospl, operator);

} else { ...

The short version of the issue is : under certain circumstances, #deleteLeastUsedEntry can remove rospl from the map of known RootSpecificPool. But as this code still holds on to the rospl instance, it will modify its state in a way the pool will never recover from later, not having any other way to access this instance when the connection gets released.

A Step by Step guide to what's going wrong.
0) You have to be in a condition that leads to the execution of said code extract (i.e. no free entry on the current route - but the route already is registered to the global pool -, current Route has capacity, max connections reached for the global pool, but there are free connections to destroy).
2) We arrive in deleteLeastUsedEntry(). We get the last entry from a queue. It can be that this entry is bound to the same (hashCode() wise) Route that the one we are getting a connection to (i.e. rospl instance held in the #getEntryBlocking context)
3) this entry can be the last of its pool, thus at this point, rospl.isUnused() == true
4) As a consequence, deleteEntry() will remove rospl from the routeToPool map
5) Back in the getEntryBlocking method, we do entry = createEntry(rospl, operator), which will do createdEntry() on the ""locally-scoped"" rospl instance that has just been removed from routeToPool 
6) When the connexion from this new entry is released at some point in the future, the rospl instance that got the createdEntry() does not exist anymore, and it is a new one that gets the freeEntry() call
7) App breaks : this newly created RouteSpecificPool throws IllegalStateException.

Step 0, though, is a rare condition that I only reached during stress tests, and on a SSL client-auth server. This is so because this is the only condition that I know of in HTTPClient, where there is a keep-alive connection in the RouteSpecificPool that can not be reused (when the State is set to the X500 principals of the client cert in the pool, but not in the request).

Possible fix (from what I understand) :
The rospl instance variable in the context of getEntryBlocking() should be protected against the consequences of #deleteLeastUsedEntry().
Not being confortable with all issues at hand, nor with the code base, the simplest thing I can think of would be to preemptively reset the rospl variable after deleteLeastUsedEntry(), thus writing the previous code extract as :

} else if (hasCapacity && !freeConnections.isEmpty()) {

deleteLeastUsedEntry();
// delete may have made deprecated the RouteSpecificPool instance
rospl = getRoutePool(route, true);
entry = createEntry(rospl, operator);

} else { ...


I have a test case that I will attach to this issue ASAP.
It is a simple example that triggers the above conditions with 3 HttpGet calls, in a serial fashion. As stated previsouly, these calls need nothing particular, except that one of these calls must go to a HTTPS server with client-side certificate authentication (I guess NTLM would be OK, anything that will place a non null state along with the route in BasicEntryPool).

I hope code is self-explainatory. I get 100% failure in my setup. Just configure your 2 URLS, configure classpath, set your keystores system properties, and launch.

Workaround :
Best workaround I found is : do not get to step 0.
The most robust way I found to do that (i.e. a way that does not involve things like setting max pool size to a gigantic number that can never be reached, ...) is to actively set the ClientContext.USER_TOKEN attribute in an exec context while submitting the request to the client.
Step 0 triggers when there is an idle connection that waits, and when this idle connection can not be reused, which can only happen if the request's ""USER_TOKEN"" does not match the BasicPoolEntry#getState(). As, in the SSL case, the state is the SSL Cert's X500PrincipalName, and I know it in advance, it's easy to set up front.

By the way, this taught me that I never could benefit from connection reuse strategies in this SSL case, as connections would always get into the pool with a USER_TOKEN that my requests never had. Don't know if it's mentionned somewhere in the documentation, but this is a noteworthy fact to me.

Please feel free to comment / correct any mistakes."
1,"document with no term vector fields after documents with term vector fields corrupts the indexIf a document with no term-vector-enabled fields is added after
document(s) that did have term vectors, as part of a single set of
buffered docs, then the term-vector documents file is corrupted
because we fail to write a ""0"" vInt.

Thanks to Grant for spotting this!

Spinoff from this thread:

    http://www.gossamer-threads.com/lists/lucene/java-dev/53306
"
1,"Deadlock between SingleClientConnManager.releaseConnection() and SingleClientConnManager.shutdown()It's possible to create a deadlock within SingleClientConnectionManager.

When JMeter interrupts a test, it calls HttpUriRequest.abort(), and as part of thread end processing it calls SingleClientConnManager.shutdown().

See deadlock details below.

I don't yet know why the shutdown is called before the abort finishes; that is probably a bug.

However, there may be a issue with the locking strategy within SCCM, hence this report.

""Thread-18"":
        at org.apache.http.impl.conn.SingleClientConnManager.releaseConnection(SingleClientConnManager.java:258)
        - waiting to lock <0x19e00118> (a org.apache.http.impl.conn.SingleClientConnManager)
        at org.apache.http.impl.conn.AbstractClientConnAdapter.abortConnection(AbstractClientConnAdapter.java:323)
        - locked <0x19e00148> (a org.apache.http.impl.conn.SingleClientConnManager$ConnAdapter)
        at org.apache.http.client.methods.HttpRequestBase.abort(HttpRequestBase.java:161)
        at org.apache.jmeter.protocol.http.sampler.HTTPHC4Impl.interrupt(HTTPHC4Impl.java:1090)
        at org.apache.jmeter.protocol.http.sampler.HTTPSamplerProxy.interrupt(HTTPSamplerProxy.java:77)
        at org.apache.jmeter.threads.JMeterThread.interrupt(JMeterThread.java:580)
        at org.apache.jmeter.engine.StandardJMeterEngine.tellThreadsToStop(StandardJMeterEngine.java:552)
        at org.apache.jmeter.engine.StandardJMeterEngine.access$2(StandardJMeterEngine.java:547)
        at org.apache.jmeter.engine.StandardJMeterEngine$StopTest.run(StandardJMeterEngine.java:284)
        at java.lang.Thread.run(Thread.java:662)
""Thread Group 1-1"":
        at org.apache.http.impl.conn.AbstractPooledConnAdapter.detach(AbstractPooledConnAdapter.java:106)
        - waiting to lock <0x19e00148> (a org.apache.http.impl.conn.SingleClientConnManager$ConnAdapter)
        at org.apache.http.impl.conn.SingleClientConnManager.shutdown(SingleClientConnManager.java:342)
        - locked <0x19e00118> (a org.apache.http.impl.conn.SingleClientConnManager)
        at org.apache.jmeter.protocol.http.sampler.HTTPHC4Impl.closeThreadLocalConnections(HTTPHC4Impl.java:1076)
        at org.apache.jmeter.protocol.http.sampler.HTTPHC4Impl.threadFinished(HTTPHC4Impl.java:1065)
        at org.apache.jmeter.protocol.http.sampler.HTTPSamplerProxy.threadFinished(HTTPSamplerProxy.java:71)
        at org.apache.jmeter.threads.JMeterThread$ThreadListenerTraverser.addNode(JMeterThread.java:553)
        at org.apache.jorphan.collections.HashTree.traverseInto(HashTree.java:986)
        at org.apache.jorphan.collections.HashTree.traverse(HashTree.java:969)
        at org.apache.jmeter.threads.JMeterThread.threadFinished(JMeterThread.java:528)
        at org.apache.jmeter.threads.JMeterThread.run(JMeterThread.java:308)
        at java.lang.Thread.run(Thread.java:662)
"
1,"o.a.j.core.integration.PrepareTestRepository fails on 2nd and every subsequent invocationconsole output: 

-------------------------------------------------------------------------------
Test set: org.apache.jackrabbit.core.integration.PrepareTestRepository
-------------------------------------------------------------------------------
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 3.428 sec <<< FAILURE!
testPrepareTestRepository(org.apache.jackrabbit.core.integration.PrepareTestRepository)  Time elapsed: 3.397 sec  <<< ERROR!
javax.jcr.RepositoryException: Invalid node type definition: {http://www.apache.org/jackrabbit/test}versionable already exists: {http://www.apache.org/jackrabbit/test}versionable already exists
	at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.registerNodeTypes(NodeTypeManagerImpl.java:308)
	at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.registerNodeTypes(NodeTypeManagerImpl.java:488)
	at org.apache.jackrabbit.core.integration.PrepareTestRepository.testPrepareTestRepository(PrepareTestRepository.java:49)
"
1,"NullPointerExc. when indexing empty field with term vectorsMark Harwood mentioned this on the user's list. Running the attached code 
you'll get this exception: 
 
Exception in thread ""main"" java.lang.NullPointerException 
	at 
org.apache.lucene.index.TermVectorsReader.clone(TermVectorsReader.java:303) 
	at 
org.apache.lucene.index.SegmentReader.getTermVectorsReader(SegmentReader.java:473) 
	at 
org.apache.lucene.index.SegmentReader.getTermFreqVectors(SegmentReader.java:507) 
	at 
org.apache.lucene.index.SegmentMerger.mergeVectors(SegmentMerger.java:204) 
	at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:94) 
	at 
org.apache.lucene.index.IndexWriter.mergeSegments(IndexWriter.java:618) 
	at 
org.apache.lucene.index.IndexWriter.flushRamSegments(IndexWriter.java:571) 
	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:339) 
	at TVBug.main(TVBug.java:16)"
1,"Don't leak deleted open file handles with pooled readersIf you have CFS enabled today, and pooling is enabled (either directly
or because you've pulled an NRT reader), IndexWriter will hold open
SegmentReaders against the non-CFS format of each merged segment.

So even if you close all NRT readers you've pulled from the writer,
you'll still see file handles open against files that have been
deleted.

This count will not grow unbounded, since it's limited by the number
of segments in the index, but it's still a serious problem since the
app had turned off CFS in the first place presumably to avoid risk of
too-many-open-files.  It's also bad because it ties up disk space
since these files would otherwise be deleted.
"
1,"IndexMerger throws null pointer exception without stacktraceI get the following errors in my log file randomly.  It seems to happen most often when creating the lucene indices, but has happened at other times as well:

[IndexMerger] ERROR - Error while merging indexes: java.lang.NullPointerException

The code at org.apache.jackrabbit.core.query.lucene.IndexMerger line 344 appears to be the point where the error is logged, but no other information is provided because the throwable isn't sent to the log (only the toString() version of the exception).  I haven't been able to tell if any indexes are corrupt when this happens.

I suggest that the logger be changed to determine where the null pointer is coming from first, then resolve the actual issue that is occurring.
"
1,"MoreLikeThis reuses a reader after it has already closed itMoreLikeThis has a fatal bug whereby it tries to reuse a reader for multiple fields:

{code}
    Map<String,Int> words = new HashMap<String,Int>();
    for (int i = 0; i < fieldNames.length; i++) {
        String fieldName = fieldNames[i];
        addTermFrequencies(r, words, fieldName);
    }
{code}

However, addTermFrequencies() is creating a TokenStream for this reader:

{code}
    TokenStream ts = analyzer.reusableTokenStream(fieldName, r);
    int tokenCount=0;
    // for every token
    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);
    ts.reset();
    while (ts.incrementToken()) {
        /* body omitted */
    }
    ts.end();
    ts.close();
{code}

When it closes this analyser, it closes the underlying reader.  Then the second time around the loop, you get:

{noformat}
Caused by: java.io.IOException: Stream closed
	at sun.nio.cs.StreamDecoder.ensureOpen(StreamDecoder.java:27)
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:128)
	at java.io.InputStreamReader.read(InputStreamReader.java:167)
	at com.acme.util.CompositeReader.read(CompositeReader.java:101)
	at org.apache.lucene.analysis.standard.StandardTokenizerImpl.zzRefill(StandardTokenizerImpl.java:803)
	at org.apache.lucene.analysis.standard.StandardTokenizerImpl.getNextToken(StandardTokenizerImpl.java:1010)
	at org.apache.lucene.analysis.standard.StandardTokenizer.incrementToken(StandardTokenizer.java:178)
	at org.apache.lucene.analysis.standard.StandardFilter.incrementTokenClassic(StandardFilter.java:61)
	at org.apache.lucene.analysis.standard.StandardFilter.incrementToken(StandardFilter.java:57)
	at com.acme.storage.index.analyser.NormaliseFilter.incrementToken(NormaliseFilter.java:51)
	at org.apache.lucene.analysis.LowerCaseFilter.incrementToken(LowerCaseFilter.java:60)
	at org.apache.lucene.search.similar.MoreLikeThis.addTermFrequencies(MoreLikeThis.java:931)
	at org.apache.lucene.search.similar.MoreLikeThis.retrieveTerms(MoreLikeThis.java:1003)
	at org.apache.lucene.search.similar.MoreLikeThis.retrieveInterestingTerms(MoreLikeThis.java:1036)
{noformat}

My first thought was that it seems like a ""ReaderFactory"" of sorts should be passed in so that a new Reader can be created for the second field (maybe the factory could be passed the field name, so that if someone wanted to pass a different reader to each, they could.)

Interestingly, the methods taking File and URL exhibit the same issue.  I'm not sure what to do about those (and we're not using them.)  The method taking File could open the file twice, but the method taking a URL probably shouldn't fetch the same URL twice.
"
1,"HttpClient does not properly handle 'application/x-www-form-urlencoded' encodingAs always I'd like to pass on my thanks, I'm finding HttpClient really useful.

The problem occurs because I use Struts map based ActionForm and these generate 
request parameters of the form:

<input type=""text"" name=""searchSelection(c)"">

When this is submitted using the PostMethod class the generateRequestBody() is 
called and in turn this calls the URI.encode() method with a BitSet of the 
acceptable characters. In this case the '(' and ')' characters are marked as 
acceptable.

The problem is that this does not work correctly when I submit it to my remote 
server. If however I issue the request directly (from a webpage rather than 
using HttpClient) it works and when I examine the request input stream I can see 
that the parameter has been re-written so that 'select(c)' is displayed as 
'select%28c%29'.

This may be my error because of encoding problems or the fact I am not setting 
the content type etc. correctly. Or it could be a bug. I'm afraid my HTTP 
knowledge is not good enough.

Chris Mein"
1,"TestDocValuesIndexing.testAddIndexes failures on docvalues branchdoc values branch r1124825, reproducible 
{code}
    [junit] Testsuite: org.apache.lucene.index.values.TestDocValuesIndexing
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.716 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestDocValuesIndexing -Dtestmethod=testAddIndexes -Dtests.seed=5939035003978436534:-6429764582682717131
    [junit] NOTE: test params are: codec=RandomCodecProvider: {id=MockRandom, BYTES_VAR_DEREF=MockRandom, INTS=Pulsing(freqCutoff=13)}, locale=da_DK, timezone=Asia/Macao
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestDocValuesIndexing]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=88582432,total=125632512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testAddIndexes(org.apache.lucene.index.values.TestDocValuesIndexing):     Caused an ERROR
    [junit] null
    [junit] java.nio.channels.ClosedChannelException
    [junit]     at sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:88)
    [junit]     at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:603)
    [junit]     at org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.readInternal(NIOFSDirectory.java:161)
    [junit]     at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:222)
    [junit]     at org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:39)
    [junit]     at org.apache.lucene.store.DataInput.readInt(DataInput.java:73)
    [junit]     at org.apache.lucene.store.BufferedIndexInput.readInt(BufferedIndexInput.java:162)
    [junit]     at org.apache.lucene.store.DataInput.readLong(DataInput.java:115)
    [junit]     at org.apache.lucene.store.BufferedIndexInput.readLong(BufferedIndexInput.java:175)
    [junit]     at org.apache.lucene.store.MockIndexInputWrapper.readLong(MockIndexInputWrapper.java:136)
    [junit]     at org.apache.lucene.index.values.PackedIntsImpl$IntsEnumImpl.<init>(PackedIntsImpl.java:263)
    [junit]     at org.apache.lucene.index.values.PackedIntsImpl$IntsEnumImpl.<init>(PackedIntsImpl.java:249)
    [junit]     at org.apache.lucene.index.values.PackedIntsImpl$IntsReader.getEnum(PackedIntsImpl.java:239)
    [junit]     at org.apache.lucene.index.values.DocValues.getEnum(DocValues.java:54)
    [junit]     at org.apache.lucene.index.values.TestDocValuesIndexing.getValuesEnum(TestDocValuesIndexing.java:484)
    [junit]     at org.apache.lucene.index.values.TestDocValuesIndexing.testAddIndexes(TestDocValuesIndexing.java:202)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1304)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1233)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.index.values.TestDocValuesIndexing FAILED
{code}

and

{code}

    [junit] Testsuite: org.apache.lucene.index.values.TestDocValuesIndexing
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.94 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestDocValuesIndexing -Dtestmethod=testAddIndexes -Dtests.seed=-3677966427932339626:-4746638811786223564
    [junit] NOTE: test params are: codec=RandomCodecProvider: {id=Standard, BYTES_FIXED_DEREF=MockSep, FLOAT_64=SimpleText}, locale=ca, timezone=Asia/Novosibirsk
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestDocValuesIndexing]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=88596152,total=125632512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testAddIndexes(org.apache.lucene.index.values.TestDocValuesIndexing):     Caused an ERROR
    [junit] Bad file descriptor
    [junit] java.io.IOException: Bad file descriptor
    [junit]     at java.io.RandomAccessFile.seek(Native Method)
    [junit]     at org.apache.lucene.store.SimpleFSDirectory$SimpleFSIndexInput.readInternal(SimpleFSDirectory.java:101)
    [junit]     at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:222)
    [junit]     at org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:39)
    [junit]     at org.apache.lucene.store.MockIndexInputWrapper.readByte(MockIndexInputWrapper.java:105)
    [junit]     at org.apache.lucene.index.values.Floats$FloatsReader.load(Floats.java:281)
    [junit]     at org.apache.lucene.index.values.SourceCache$DirectSourceCache.load(SourceCache.java:101)
    [junit]     at org.apache.lucene.index.values.DocValues.getSource(DocValues.java:101)
    [junit]     at org.apache.lucene.index.values.TestDocValuesIndexing.getSource(TestDocValuesIndexing.java:472)
    [junit]     at org.apache.lucene.index.values.TestDocValuesIndexing.getValuesEnum(TestDocValuesIndexing.java:482)
    [junit]     at org.apache.lucene.index.values.TestDocValuesIndexing.testAddIndexes(TestDocValuesIndexing.java:203)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1304)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1233)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.index.values.TestDocValuesIndexing FAILED
{code}"
1,"HTMLTextExtractor modifying UTF-8 encoded StringTrying to extract an HTML that is UTF-8 encoded is modifying the UTF-8 special char (like , , ,  etc).

This cause a wrong search, because lucene use this extractor to index content.

See attachments for an example of the problem."
1,"Crash when querying an index using multiple term positions.file: MultipleTermPositions.java, line: 201, function: skipTo(int).

This refers to the source that can currently be downloaded from the lucene site,
Lucene v. 1.4.3.

The function peek() returns null (because top() also retruned null). There is no
check for this, as far as I can understand. The function doc() is called on a
null-object, which results in a NullPointerException.

I switched the specified line to this one:

while(_termPositionsQueue.peek() != null && target >
_termPositionsQueue.peek().doc())

This got rid of the crash for me."
1,"JCR2SPI NodeEntryImpl throws NPE during reorderNodesTwo folder nodes are created below root. From the root node, the 2nd folder is ordered before the first node. The request is batched up correctly, but upon save, NodeEntryImpl throws a NullPointerException in the first line of the completeTransientChanges method, because revertInfo.oldParent is null.

Test code:

		final String FOLDER1 = ""folder1"", FOLDER2 = ""folder2"";
		
		// Create folder 1 on server in root
		Session serverSession = login(repository, creds);
		Node serverRootNode = serverSession.getRootNode();
		Node serverFolder1 = serverRootNode.addNode(FOLDER1, ""nt:folder"");
		
		// Create folder 2 on server in root
		Node serverFolder2 = serverRootNode.addNode(FOLDER2, ""nt:folder"");
		serverSession.save();
		
		// Validate order (TODO)
		
		// Perform reorder via client
		Session clientSession = login(clientRepository, creds);
		Node clientRootNode = clientSession.getRootNode();
		clientRootNode.orderBefore(FOLDER2, FOLDER1);
		clientSession.save(); <== Throws NPE

Call Stack:

    [junit] java.lang.NullPointerException
    [junit]     at org.apache.jackrabbit.jcr2spi.hierarchy.NodeEntryImpl.completeTransientChanges(NodeEntryImpl.java:1354)
    [junit]     at org.apache.jackrabbit.jcr2spi.hierarchy.NodeEntryImpl.access$1100(NodeEntryImpl.java:60)
    [junit]     at org.apache.jackrabbit.jcr2spi.hierarchy.NodeEntryImpl$RevertInfo.statusChanged(NodeEntryImpl.java:1465)
    [junit]     at org.apache.jackrabbit.jcr2spi.state.ItemState.setStatus(ItemState.java:257)
    [junit]     at org.apache.jackrabbit.jcr2spi.state.NodeState.adjustNodeState(NodeState.java:554)
    [junit]     at org.apache.jackrabbit.jcr2spi.state.NodeState.persisted(NodeState.java:276)
    [junit]     at org.apache.jackrabbit.jcr2spi.state.ChangeLog.persisted(ChangeLog.java:135)
    [junit]     at org.apache.jackrabbit.jcr2spi.WorkspaceManager.execute(WorkspaceManager.java:479)
    [junit]     at org.apache.jackrabbit.jcr2spi.state.SessionItemStateManager.save(SessionItemStateManager.java:149)
    [junit]     at org.apache.jackrabbit.jcr2spi.ItemImpl.save(ItemImpl.java:239)
    [junit]     at org.apache.jackrabbit.jcr2spi.SessionImpl.save(SessionImpl.java:317)
    [junit]     at TestWsNodeReorder.testReorderNodes(TestWsNodeReorder.java:72)


I'm using an SPI I implemented, in conjunction with the jcr2spi and spi2jcr bridges, coupled with a back-end Jackrabbit in-memory filesystem. So there's always the possibility that node or property SPI calls inject errors and cause this downstream problem."
1,"httpclient charset encooding loosing problemfile: org\apache\commons\httpclient\HttpConstants.java 


line: near 261


---------------------------------


public static String getContentString(final byte[] data, String charset) {


        return getContentString(data, 0, data.length);


    }


---------------------------------


must be


---


        return getContentString(data, 0, data.length, charset);


---"
1,"Changes of JCR-313 introduced db-transaction problemthe fix of JCR-313 changed the autocommit from 'true' to 'false', resulting the DatabaseFileSystems not to write back correctly anymore."
1,FineGrainedISMLocking problemsThe FineGrainedISMLocking strategy suffers from the same deadlock issue as was reported in JCR-2753 against DefaultISMLocking. Additionally the FineGrainedISMLocking class will also fail to function properly with XA transactions since it uses the current thread instead of the current transaction id to track re-entrancy.
1,"AnalyzingQueryParser can't work with leading wildcards.The getWildcardQuery mehtod in AnalyzingQueryParser.java need the following changes to accept leading wildcards:

	protected Query getWildcardQuery(String field, String termStr) throws ParseException
	{
		String useTermStr = termStr;
		String leadingWildcard = null;
		if (""*"".equals(field))
		{
			if (""*"".equals(useTermStr))
				return new MatchAllDocsQuery();
		}
		boolean hasLeadingWildcard = (useTermStr.startsWith(""*"") || useTermStr.startsWith(""?"")) ? true : false;

		if (!getAllowLeadingWildcard() && hasLeadingWildcard)
			throw new ParseException(""'*' or '?' not allowed as first character in WildcardQuery"");

		if (getLowercaseExpandedTerms())
		{
			useTermStr = useTermStr.toLowerCase();
		}

		if (hasLeadingWildcard)
		{
			leadingWildcard = useTermStr.substring(0, 1);
			useTermStr = useTermStr.substring(1);
		}

		List tlist = new ArrayList();
		List wlist = new ArrayList();
		/*
		 * somewhat a hack: find/store wildcard chars in order to put them back
		 * after analyzing
		 */
		boolean isWithinToken = (!useTermStr.startsWith(""?"") && !useTermStr.startsWith(""*""));
		isWithinToken = true;
		StringBuffer tmpBuffer = new StringBuffer();
		char[] chars = useTermStr.toCharArray();
		for (int i = 0; i < useTermStr.length(); i++)
		{
			if (chars[i] == '?' || chars[i] == '*')
			{
				if (isWithinToken)
				{
					tlist.add(tmpBuffer.toString());
					tmpBuffer.setLength(0);
				}
				isWithinToken = false;
			}
			else
			{
				if (!isWithinToken)
				{
					wlist.add(tmpBuffer.toString());
					tmpBuffer.setLength(0);
				}
				isWithinToken = true;
			}
			tmpBuffer.append(chars[i]);
		}
		if (isWithinToken)
		{
			tlist.add(tmpBuffer.toString());
		}
		else
		{
			wlist.add(tmpBuffer.toString());
		}

		// get Analyzer from superclass and tokenize the term
		TokenStream source = getAnalyzer().tokenStream(field, new StringReader(useTermStr));
		org.apache.lucene.analysis.Token t;

		int countTokens = 0;
		while (true)
		{
			try
			{
				t = source.next();
			}
			catch (IOException e)
			{
				t = null;
			}
			if (t == null)
			{
				break;
			}
			if (!"""".equals(t.termText()))
			{
				try
				{
					tlist.set(countTokens++, t.termText());
				}
				catch (IndexOutOfBoundsException ioobe)
				{
					countTokens = -1;
				}
			}
		}
		try
		{
			source.close();
		}
		catch (IOException e)
		{
			// ignore
		}

		if (countTokens != tlist.size())
		{
			/*
			 * this means that the analyzer used either added or consumed
			 * (common for a stemmer) tokens, and we can't build a WildcardQuery
			 */
			throw new ParseException(""Cannot build WildcardQuery with analyzer "" + getAnalyzer().getClass()
					+ "" - tokens added or lost"");
		}

		if (tlist.size() == 0)
		{
			return null;
		}
		else if (tlist.size() == 1)
		{
			if (wlist.size() == 1)
			{
				/*
				 * if wlist contains one wildcard, it must be at the end,
				 * because: 1) wildcards at 1st position of a term by
				 * QueryParser where truncated 2) if wildcard was *not* in end,
				 * there would be *two* or more tokens
				 */
				StringBuffer sb = new StringBuffer();
				if (hasLeadingWildcard)
				{
					// adding leadingWildcard
					sb.append(leadingWildcard);
				}
				sb.append((String) tlist.get(0));
				sb.append(wlist.get(0).toString());
				return super.getWildcardQuery(field, sb.toString());
			}
			else if (wlist.size() == 0 && hasLeadingWildcard)
			{
				/*
				 * if wlist contains no wildcard, it must be at 1st position
				 */
				StringBuffer sb = new StringBuffer();
				if (hasLeadingWildcard)
				{
					// adding leadingWildcard
					sb.append(leadingWildcard);
				}
				sb.append((String) tlist.get(0));
				sb.append(wlist.get(0).toString());
				return super.getWildcardQuery(field, sb.toString());
			}
			else
			{
				/*
				 * we should never get here! if so, this method was called with
				 * a termStr containing no wildcard ...
				 */
				throw new IllegalArgumentException(""getWildcardQuery called without wildcard"");
			}
		}
		else
		{
			/*
			 * the term was tokenized, let's rebuild to one token with wildcards
			 * put back in postion
			 */
			StringBuffer sb = new StringBuffer();
			if (hasLeadingWildcard)
			{
				// adding leadingWildcard
				sb.append(leadingWildcard);
			}
			for (int i = 0; i < tlist.size(); i++)
			{
				sb.append((String) tlist.get(i));
				if (wlist != null && wlist.size() > i)
				{
					sb.append((String) wlist.get(i));
				}
			}
			return super.getWildcardQuery(field, sb.toString());
		}
	}
"
1,"[PATCH] When locks are disabled, IndexWriter.close() throws NullPointerExceptionIf locks are disabled (via setting the System property 'disableLuceneLocks' to
true), IndexWriter throws a NullPointerException on closing. The reason is that
the attempt to call writeLock.release() fails because writeLock is null.
To correct this, just check for this case before releasing. A (trivial) patch is
attached."
1,"In XA transaction session.addLockToken() does not have effectFollowing sequence does not work as expected:
1. first tx (and first session)
  create node
  make it lockable
2. second tx (and second session)
  lock this node and save lock token
3. third tx (and third session)
  add saved lock token to session
  modify this locked node -> fails as if lock token was not added to session3

The same sequence works as expected without transactions.
I had to separate transactions 1 and 2 because JCR-1633 prevents node from being locked in same tx in which it was created."
1,"CacheEntryUpdater does not properly update cache entry resourceCacheEntryUpdater#updateCacheEntry() copies the old cache entry's resource, though I believe it should only do so if the response is a 304.  Otherwise it should take the response from the server to update the entry.  This method gets called when validating a cache entry and the server returns a 200 or 304."
1,"StandardTokenizer loses Korean charactersWhile using StandardAnalyzer, exp. StandardTokenizer with Korean text stream, StandardTokenizer ignores the Korean characters. This is because the definition of CJK token in StandardTokenizer.jj JavaCC file doesn't have enough range covering Korean syllables described in Unicode character map.
This patch adds one line of 0xAC00~0xD7AF, the Korean syllables range to the StandardTokenizer.jj code."
1,"Oracle JNDI DataSource supportWhen org.apache.jackrabbit.core.persistence.bundle.util.ConnectionFactory tries to get a connection from a JNDI Datasource without login and pasword, if no user/password are specified, they re retrieved as empty strings, not null, so it tries to do a ds.getConnection(user, password), which fails. Please complete the test line 66 as :
if ((user == null || user.length() > 0) && (password == null || password.length() > 0)) {

Sincerely,

Stphane Landelle"
1,"[PATCH] MultiSearcher problems with Similarity.docFreq()When MultiSearcher invokes its subsearchers, it is the subsearchers' docFreq()
that is accessed by Similarity.docFreq().  This causes idf's to be computed
local to each index rather than globally, which causes ranking across multiple
indices to not be equivalent to ranking across the entire global collection.

The attached files (if I can figure out how to attach them) provide a potential
partial solution for this.  They properly fix a simple test case, RankingTest,
that was provided by Daniel Naber.

The changes are:
  1.  Searcher:  Add topmostSearcher() field with getter and setter to record
the outermost Searcher.  Default to this.
  2.  MultiSearcher:  Pass down the topmostSearcher when creating the subsearchers.
  3.  IndexSearcher:  Call Query.weight() everywhere with the topmostSearcher
instead of this.
  4.  Query:  Provide a default implementation of Query.combine() so that
MultiSearcher works with all queries.

Problems or possible problems I see:
  1.  This does not address the same issue with RemoteSearchable. 
RemoteSearchable is not a Searcher, nor can it be due to lack of multiple
inheritance in Java, but Query.weight() requires a Searcher.  Perhaps
Query.weight() should be changed to take a Searchable, but this requires
changing many places and I suspect would break apps.
  2.  There may be other places that topmostSearcher should be used instead of this.
  3.  The default implementation for Query.combine() is a guess on my part - it
works for TermQuery.  It's fragile in that the default implementation will hide
bugs caused by queries that inadvertently omit a more precise Query.combine()
method.
  4.  The prior comment on Query.combine() indicates that whoever wrote it was
fully aware of this problem and so probably had another usage in mind, so the
whole issue may just be Daniel's usage in the test case.  It's not apparent to
me, so I probably don't understand something."
1,"ParallelMultiSearcher should shut down thread pool on closeParallelMultiSearcher does not shut down its internal thread pool on close. As a result, programs that create multiple instances of this class over their lifetime end up ""leaking"" threads."
1,"IndexReader.getCurrentVersion() and isCurrent should use commit lock.There is a race condition if one machine is checking the current version of an index while another wants to update the segments file in IndexWriter.close().

java.io.IOException: Cannot delete segments
	at org.apache.lucene.store.FSDirectory.renameFile(FSDirectory.java:213)
	at org.apache.lucene.index.SegmentInfos.write(SegmentInfos.java:90)
	at org.apache.lucene.index.IndexWriter$3.doBody(IndexWriter.java:503)
	at org.apache.lucene.store.Lock$With.run(Lock.java:109)
	at org.apache.lucene.index.IndexWriter.mergeSegments(IndexWriter.java:501)
	at org.apache.lucene.index.IndexWriter.flushRamSegments(IndexWriter.java:440)
	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:242)

On the windows platform reading the contents of a file disallows deleting the file.

I use Lucene to maintain an index of +-700.000 documents, one server adds documents, while other servers handle the searches.
The search servers poll the index version regularly to check if they have to reopen their IndexSearcher.
Once in a while (about once every two days on average), IndexWriter.close() fails because it cannot delete the previous segments file, even though it hold the commit lock.
The reason is probably that search servers are reading the segments file to check the version without using the commit lock.
"
1,"Multiple namespace definitions in CND prevent definition of node type without child nodesThe BNF in http://jackrabbit.apache.org/api-1/org/apache/jackrabbit/core/nodetype/compact/CompactNodeTypeDefReader.html
defines:

[...]
cnd ::= {ns_mapping | node_type_def}
[...]

so multiple namespace definitions should not affect the node type definitions.

However, the following CND definition will fail:

<namespace= 'http://www.mynamespace.co.uk/namespace'>
<nt = 'http://www.jcp.org/jcr/nt/1.0'>
[namespace:document] > nt:file
   - namespace:name (string) mandatory

<namespace= 'http://www.mynamespace.co.uk/namespace'>
<nt = 'http://www.jcp.org/jcr/nt/1.0'>
[namespace:document2] > nt:file
   - namespace:name (string) mandatory


Remove the second set of namespace definitions, and all's well:

<namespace= 'http://www.mynamespace.co.uk/namespace'>
<nt = 'http://www.jcp.org/jcr/nt/1.0'>
[namespace:document] > nt:file
   - namespace:name (string) mandatory

[namespace:document2] > nt:file
   - namespace:name (string) mandatory"
1,"don't call SegmentInfo.sizeInBytes for the merging segmentsSelckin has been running Lucene's tests on the RT branch, and hit this:
{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestIndexWriter
    [junit] Testcase: testDeleteAllSlowly(org.apache.lucene.index.TestIndexWriter):	FAILED
    [junit] Some threads threw uncaught exceptions!
    [junit] junit.framework.AssertionFailedError: Some threads threw uncaught exceptions!
    [junit] 	at org.apache.lucene.util.LuceneTestCase.tearDown(LuceneTestCase.java:535)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1246)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1175)
    [junit] 
    [junit] 
    [junit] Tests run: 67, Failures: 1, Errors: 0, Time elapsed: 38.357 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testDeleteAllSlowly -Dtests.seed=-4291771462012978364:4550117847390778918
    [junit] The following exceptions were thrown by threads:
    [junit] *** Thread: Lucene Merge Thread #1 ***
    [junit] org.apache.lucene.index.MergePolicy$MergeException: java.io.FileNotFoundException: _4_1.del
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:507)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:472)
    [junit] Caused by: java.io.FileNotFoundException: _4_1.del
    [junit] 	at org.apache.lucene.store.FSDirectory.fileLength(FSDirectory.java:290)
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.fileLength(MockDirectoryWrapper.java:549)
    [junit] 	at org.apache.lucene.index.SegmentInfo.sizeInBytes(SegmentInfo.java:287)
    [junit] 	at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3280)
    [junit] 	at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:2956)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:379)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:447)
    [junit] NOTE: test params are: codec=RandomCodecProvider: {=SimpleText, f6=Pulsing(freqCutoff=15), f7=MockFixedIntBlock(blockSize=1606), f8=SimpleText, f9=MockSep, f1=MockVariableIntBlock(baseBlockSize=99), f0=MockFixedIntBlock(blockSize=1606), f3=Pulsing(freqCutoff=15), f2=MockSep, f5=SimpleText, f4=Standard, f=MockFixedIntBlock(blockSize=1606), c=MockSep, termVector=MockRandom, d9=MockFixedIntBlock(blockSize=1606), d8=Pulsing(freqCutoff=15), d5=SimpleText, d4=Standard, d7=MockRandom, d6=MockVariableIntBlock(baseBlockSize=99), d25=MockRandom, d0=MockRandom, c29=MockFixedIntBlock(blockSize=1606), d24=MockVariableIntBlock(baseBlockSize=99), d1=Standard, c28=Standard, d23=SimpleText, d2=MockFixedIntBlock(blockSize=1606), c27=MockRandom, d22=Standard, d3=MockVariableIntBlock(baseBlockSize=99), d21=Pulsing(freqCutoff=15), d20=MockSep, c22=MockFixedIntBlock(blockSize=1606), c21=Pulsing(freqCutoff=15), c20=MockRandom, d29=MockFixedIntBlock(blockSize=1606), c26=Standard, d28=Pulsing(freqCutoff=15), c25=MockRandom, d27=MockRandom, c24=MockSep, d26=MockVariableIntBlock(baseBlockSize=99), c23=SimpleText, e9=MockRandom, e8=MockSep, e7=SimpleText, e6=MockFixedIntBlock(blockSize=1606), e5=Pulsing(freqCutoff=15), c17=MockFixedIntBlock(blockSize=1606), e3=Standard, d12=MockVariableIntBlock(baseBlockSize=99), c16=Pulsing(freqCutoff=15), e4=SimpleText, d11=MockFixedIntBlock(blockSize=1606), c19=MockSep, e1=MockSep, d14=Pulsing(freqCutoff=15), c18=SimpleText, e2=Pulsing(freqCutoff=15), d13=MockSep, e0=MockVariableIntBlock(baseBlockSize=99), d10=Standard, d19=MockVariableIntBlock(baseBlockSize=99), c11=SimpleText, c10=Standard, d16=Pulsing(freqCutoff=15), c13=MockRandom, c12=MockVariableIntBlock(baseBlockSize=99), d15=MockSep, d18=SimpleText, c15=MockFixedIntBlock(blockSize=1606), d17=Standard, c14=Pulsing(freqCutoff=15), b3=MockSep, b2=SimpleText, b5=Standard, b4=MockRandom, b7=MockVariableIntBlock(baseBlockSize=99), b6=MockFixedIntBlock(blockSize=1606), d50=MockFixedIntBlock(blockSize=1606), b9=Pulsing(freqCutoff=15), b8=MockSep, d43=MockSep, d42=SimpleText, d41=MockFixedIntBlock(blockSize=1606), d40=Pulsing(freqCutoff=15), d47=MockVariableIntBlock(baseBlockSize=99), d46=MockFixedIntBlock(blockSize=1606), b0=MockVariableIntBlock(baseBlockSize=99), d45=Standard, b1=MockRandom, d44=MockRandom, d49=MockVariableIntBlock(baseBlockSize=99), d48=MockFixedIntBlock(blockSize=1606), c6=Pulsing(freqCutoff=15), c5=MockSep, c4=MockVariableIntBlock(baseBlockSize=99), c3=MockFixedIntBlock(blockSize=1606), c9=MockVariableIntBlock(baseBlockSize=99), c8=SimpleText, c7=Standard, d30=SimpleText, d32=MockRandom, d31=MockVariableIntBlock(baseBlockSize=99), c1=SimpleText, d34=MockFixedIntBlock(blockSize=1606), c2=MockSep, d33=Pulsing(freqCutoff=15), d36=MockSep, c0=MockFixedIntBlock(blockSize=1606), d35=SimpleText, d38=MockSep, d37=SimpleText, d39=MockRandom, e92=MockFixedIntBlock(blockSize=1606), e93=MockVariableIntBlock(baseBlockSize=99), e90=MockRandom, e91=Standard, e89=MockVariableIntBlock(baseBlockSize=99), e88=SimpleText, e87=Standard, e86=Pulsing(freqCutoff=15), e85=MockSep, e84=MockVariableIntBlock(baseBlockSize=99), e83=MockFixedIntBlock(blockSize=1606), e80=MockFixedIntBlock(blockSize=1606), e81=SimpleText, e82=MockSep, e77=MockVariableIntBlock(baseBlockSize=99), e76=MockFixedIntBlock(blockSize=1606), e79=Pulsing(freqCutoff=15), e78=MockSep, e73=MockSep, e72=SimpleText, e75=Standard, e74=MockRandom, binary=MockFixedIntBlock(blockSize=1606), f98=Pulsing(freqCutoff=15), f97=MockSep, f99=Standard, f94=Standard, f93=MockRandom, f96=MockVariableIntBlock(baseBlockSize=99), f95=MockFixedIntBlock(blockSize=1606), e95=SimpleText, e94=Standard, e97=MockRandom, e96=MockVariableIntBlock(baseBlockSize=99), e99=MockFixedIntBlock(blockSize=1606), e98=Pulsing(freqCutoff=15), id=MockFixedIntBlock(blockSize=1606), f34=MockSep, f33=SimpleText, f32=MockFixedIntBlock(blockSize=1606), f31=Pulsing(freqCutoff=15), f30=MockRandom, f39=MockFixedIntBlock(blockSize=1606), f38=Standard, f37=MockRandom, f36=MockSep, f35=SimpleText, f43=Standard, f42=MockRandom, f45=MockVariableIntBlock(baseBlockSize=99), f44=MockFixedIntBlock(blockSize=1606), f41=MockSep, f40=SimpleText, f47=MockVariableIntBlock(baseBlockSize=99), f46=MockFixedIntBlock(blockSize=1606), f49=Pulsing(freqCutoff=15), f48=MockSep, content=Pulsing(freqCutoff=15), e19=Standard, e18=MockRandom, e17=MockSep, f12=Pulsing(freqCutoff=15), e16=SimpleText, f11=MockSep, f10=MockVariableIntBlock(baseBlockSize=99), e15=MockFixedIntBlock(blockSize=1606), e14=Pulsing(freqCutoff=15), f16=SimpleText, e13=MockFixedIntBlock(blockSize=1606), f15=Standard, e12=Pulsing(freqCutoff=15), e11=MockRandom, f14=Pulsing(freqCutoff=15), e10=MockVariableIntBlock(baseBlockSize=99), f13=MockSep, f19=Pulsing(freqCutoff=15), f18=MockRandom, f17=MockVariableIntBlock(baseBlockSize=99), e29=MockSep, e26=Standard, f21=SimpleText, e25=MockRandom, f20=Standard, e28=MockVariableIntBlock(baseBlockSize=99), f23=MockRandom, e27=MockFixedIntBlock(blockSize=1606), f22=MockVariableIntBlock(baseBlockSize=99), f25=MockRandom, e22=MockSep, f24=MockVariableIntBlock(baseBlockSize=99), e21=SimpleText, f27=MockFixedIntBlock(blockSize=1606), e24=Standard, f26=Pulsing(freqCutoff=15), e23=MockRandom, f29=MockSep, f28=SimpleText, e20=MockFixedIntBlock(blockSize=1606), field=Pulsing(freqCutoff=15), string=MockVariableIntBlock(baseBlockSize=99), e30=Pulsing(freqCutoff=15), e31=MockFixedIntBlock(blockSize=1606), a98=MockFixedIntBlock(blockSize=1606), e34=MockRandom, a99=MockVariableIntBlock(baseBlockSize=99), e35=Standard, f79=Pulsing(freqCutoff=15), e32=SimpleText, e33=MockSep, b97=Pulsing(freqCutoff=15), f77=Pulsing(freqCutoff=15), e38=MockFixedIntBlock(blockSize=1606), b98=MockFixedIntBlock(blockSize=1606), f78=MockFixedIntBlock(blockSize=1606), e39=MockVariableIntBlock(baseBlockSize=99), b99=SimpleText, f75=MockVariableIntBlock(baseBlockSize=99), e36=MockRandom, f76=MockRandom, e37=Standard, f73=Standard, f74=SimpleText, f71=MockSep, f72=Pulsing(freqCutoff=15), f81=Pulsing(freqCutoff=15), f80=MockSep, e40=MockSep, e41=MockRandom, e42=Standard, e43=MockFixedIntBlock(blockSize=1606), e44=MockVariableIntBlock(baseBlockSize=99), e45=MockSep, e46=Pulsing(freqCutoff=15), f86=SimpleText, e47=MockSep, f87=MockSep, e48=Pulsing(freqCutoff=15), f88=MockRandom, e49=Standard, f89=Standard, f82=MockVariableIntBlock(baseBlockSize=99), f83=MockRandom, f84=Pulsing(freqCutoff=15), f85=MockFixedIntBlock(blockSize=1606), f90=SimpleText, f92=MockRandom, f91=MockVariableIntBlock(baseBlockSize=99), str=MockFixedIntBlock(blockSize=1606), a76=MockVariableIntBlock(baseBlockSize=99), e56=MockVariableIntBlock(baseBlockSize=99), f59=MockSep, a77=MockRandom, e57=MockRandom, a78=Pulsing(freqCutoff=15), e54=Standard, f57=MockFixedIntBlock(blockSize=1606), a79=MockFixedIntBlock(blockSize=1606), e55=SimpleText, f58=MockVariableIntBlock(baseBlockSize=99), e52=MockSep, e53=Pulsing(freqCutoff=15), e50=MockFixedIntBlock(blockSize=1606), e51=MockVariableIntBlock(baseBlockSize=99), f51=SimpleText, f52=MockSep, f50=MockFixedIntBlock(blockSize=1606), f55=MockFixedIntBlock(blockSize=1606), f56=MockVariableIntBlock(baseBlockSize=99), f53=MockRandom, e58=MockVariableIntBlock(baseBlockSize=99), f54=Standard, e59=MockRandom, a80=MockVariableIntBlock(baseBlockSize=99), e60=MockVariableIntBlock(baseBlockSize=99), a82=Pulsing(freqCutoff=15), a81=MockSep, a84=SimpleText, a83=Standard, a86=MockRandom, a85=MockVariableIntBlock(baseBlockSize=99), a89=MockRandom, f68=Standard, e65=Pulsing(freqCutoff=15), f69=SimpleText, e66=MockFixedIntBlock(blockSize=1606), a87=SimpleText, e67=SimpleText, a88=MockSep, e68=MockSep, e61=Standard, e62=SimpleText, e63=MockVariableIntBlock(baseBlockSize=99), e64=MockRandom, f60=MockRandom, f61=Standard, f62=MockFixedIntBlock(blockSize=1606), f63=MockVariableIntBlock(baseBlockSize=99), e69=SimpleText, f64=MockSep, f65=Pulsing(freqCutoff=15), f66=Standard, f67=SimpleText, f70=Standard, a93=MockRandom, a92=MockVariableIntBlock(baseBlockSize=99), a91=SimpleText, e71=SimpleText, a90=Standard, e70=Standard, a97=MockSep, a96=SimpleText, a95=MockFixedIntBlock(blockSize=1606), a94=Pulsing(freqCutoff=15), c58=MockRandom, a63=Pulsing(freqCutoff=15), a64=MockFixedIntBlock(blockSize=1606), c59=Standard, c56=SimpleText, d59=MockVariableIntBlock(baseBlockSize=99), a61=MockVariableIntBlock(baseBlockSize=99), c57=MockSep, a62=MockRandom, c54=Pulsing(freqCutoff=15), c55=MockFixedIntBlock(blockSize=1606), a60=SimpleText, c52=MockVariableIntBlock(baseBlockSize=99), c53=MockRandom, d53=MockSep, d54=Pulsing(freqCutoff=15), d51=MockFixedIntBlock(blockSize=1606), d52=MockVariableIntBlock(baseBlockSize=99), d57=MockVariableIntBlock(baseBlockSize=99), b62=MockSep, d58=MockRandom, b63=Pulsing(freqCutoff=15), d55=Standard, b60=MockFixedIntBlock(blockSize=1606), d56=SimpleText, b61=MockVariableIntBlock(baseBlockSize=99), b56=SimpleText, b55=Standard, b54=Pulsing(freqCutoff=15), b53=MockSep, d61=MockVariableIntBlock(baseBlockSize=99), b59=Pulsing(freqCutoff=15), d60=MockFixedIntBlock(blockSize=1606), b58=MockRandom, b57=MockVariableIntBlock(baseBlockSize=99), c62=MockRandom, c61=MockVariableIntBlock(baseBlockSize=99), a59=Standard, c60=SimpleText, a58=MockRandom, a57=MockSep, a56=SimpleText, a55=MockFixedIntBlock(blockSize=1606), a54=Pulsing(freqCutoff=15), a72=SimpleText, c67=MockFixedIntBlock(blockSize=1606), a73=MockSep, c68=MockVariableIntBlock(baseBlockSize=99), a74=MockRandom, c69=MockSep, a75=Standard, c63=SimpleText, c64=MockSep, a70=Pulsing(freqCutoff=15), c65=MockRandom, a71=MockFixedIntBlock(blockSize=1606), c66=Standard, d62=Standard, d63=SimpleText, d64=MockVariableIntBlock(baseBlockSize=99), b70=Pulsing(freqCutoff=15), d65=MockRandom, b71=Standard, d66=Pulsing(freqCutoff=15), b72=SimpleText, d67=MockFixedIntBlock(blockSize=1606), b73=MockVariableIntBlock(baseBlockSize=99), d68=SimpleText, b74=MockRandom, d69=MockSep, b65=MockRandom, b64=MockVariableIntBlock(baseBlockSize=99), b67=MockFixedIntBlock(blockSize=1606), b66=Pulsing(freqCutoff=15), d70=Pulsing(freqCutoff=15), b69=MockSep, b68=SimpleText, d72=SimpleText, d71=Standard, c71=MockFixedIntBlock(blockSize=1606), c70=Pulsing(freqCutoff=15), a69=MockSep, c73=MockSep, c72=SimpleText, a66=Standard, a65=MockRandom, a68=MockVariableIntBlock(baseBlockSize=99), a67=MockFixedIntBlock(blockSize=1606), c32=MockFixedIntBlock(blockSize=1606), c33=MockVariableIntBlock(baseBlockSize=99), c30=MockRandom, c31=Standard, c36=Standard, a41=MockFixedIntBlock(blockSize=1606), c37=SimpleText, a42=MockVariableIntBlock(baseBlockSize=99), a0=MockSep, c34=MockSep, c35=Pulsing(freqCutoff=15), a40=Standard, b84=SimpleText, d79=MockFixedIntBlock(blockSize=1606), b85=MockSep, b82=Pulsing(freqCutoff=15), d77=MockRandom, c38=Standard, b83=MockFixedIntBlock(blockSize=1606), d78=Standard, c39=SimpleText, b80=MockVariableIntBlock(baseBlockSize=99), d75=SimpleText, b81=MockRandom, d76=MockSep, d73=Pulsing(freqCutoff=15), d74=MockFixedIntBlock(blockSize=1606), d83=MockFixedIntBlock(blockSize=1606), a9=MockVariableIntBlock(baseBlockSize=99), d82=Pulsing(freqCutoff=15), d81=MockRandom, d80=MockVariableIntBlock(baseBlockSize=99), b79=MockFixedIntBlock(blockSize=1606), b78=Standard, b77=MockRandom, b76=MockSep, b75=SimpleText, a1=MockFixedIntBlock(blockSize=1606), a35=Pulsing(freqCutoff=15), a2=MockVariableIntBlock(baseBlockSize=99), a34=MockSep, a3=MockSep, a33=MockVariableIntBlock(baseBlockSize=99), a4=Pulsing(freqCutoff=15), a32=MockFixedIntBlock(blockSize=1606), a5=Standard, a39=MockRandom, c40=Standard, a6=SimpleText, a38=MockVariableIntBlock(baseBlockSize=99), a7=MockVariableIntBlock(baseBlockSize=99), a37=SimpleText, a8=MockRandom, a36=Standard, c41=MockSep, c42=Pulsing(freqCutoff=15), c43=Standard, c44=SimpleText, c45=MockVariableIntBlock(baseBlockSize=99), a50=MockSep, c46=MockRandom, a51=Pulsing(freqCutoff=15), c47=Pulsing(freqCutoff=15), a52=Standard, c48=MockFixedIntBlock(blockSize=1606), a53=SimpleText, b93=MockRandom, d88=MockSep, c49=Pulsing(freqCutoff=15), b94=Standard, d89=Pulsing(freqCutoff=15), b95=MockFixedIntBlock(blockSize=1606), b96=MockVariableIntBlock(baseBlockSize=99), d84=MockRandom, b90=MockFixedIntBlock(blockSize=1606), d85=Standard, b91=SimpleText, d86=MockFixedIntBlock(blockSize=1606), b92=MockSep, d87=MockVariableIntBlock(baseBlockSize=99), d92=MockSep, d91=SimpleText, d94=Standard, d93=MockRandom, b87=MockVariableIntBlock(baseBlockSize=99), b86=MockFixedIntBlock(blockSize=1606), d90=MockFixedIntBlock(blockSize=1606), b89=Pulsing(freqCutoff=15), b88=MockSep, a44=SimpleText, a43=Standard, a46=MockRandom, a45=MockVariableIntBlock(baseBlockSize=99), a48=MockFixedIntBlock(blockSize=1606), a47=Pulsing(freqCutoff=15), c51=Pulsing(freqCutoff=15), a49=SimpleText, c50=MockSep, d98=MockVariableIntBlock(baseBlockSize=99), d97=MockFixedIntBlock(blockSize=1606), d96=Standard, d95=MockRandom, d99=MockSep, a20=MockSep, c99=MockRandom, c98=MockVariableIntBlock(baseBlockSize=99), c97=SimpleText, c96=Standard, b19=MockRandom, a16=MockSep, a17=Pulsing(freqCutoff=15), b17=SimpleText, a14=MockFixedIntBlock(blockSize=1606), b18=MockSep, a15=MockVariableIntBlock(baseBlockSize=99), a12=MockRandom, a13=Standard, a10=SimpleText, a11=MockSep, b11=MockVariableIntBlock(baseBlockSize=99), b12=MockRandom, b10=SimpleText, b15=SimpleText, b16=MockSep, a18=MockSep, b13=Pulsing(freqCutoff=15), a19=Pulsing(freqCutoff=15), b14=MockFixedIntBlock(blockSize=1606), b30=MockFixedIntBlock(blockSize=1606), a31=MockVariableIntBlock(baseBlockSize=99), a30=MockFixedIntBlock(blockSize=1606), b28=MockFixedIntBlock(blockSize=1606), a25=Standard, b29=MockVariableIntBlock(baseBlockSize=99), a26=SimpleText, a27=MockVariableIntBlock(baseBlockSize=99), a28=MockRandom, a21=MockFixedIntBlock(blockSize=1606), a22=MockVariableIntBlock(baseBlockSize=99), a23=MockSep, a24=Pulsing(freqCutoff=15), b20=Pulsing(freqCutoff=15), b21=MockFixedIntBlock(blockSize=1606), b22=SimpleText, b23=MockSep, a29=MockVariableIntBlock(baseBlockSize=99), b24=MockRandom, b25=Standard, b26=MockFixedIntBlock(blockSize=1606), b27=MockVariableIntBlock(baseBlockSize=99), b41=Standard, b40=MockRandom, c77=Standard, c76=MockRandom, c75=MockSep, c74=SimpleText, c79=MockVariableIntBlock(baseBlockSize=99), c78=MockFixedIntBlock(blockSize=1606), c80=MockRandom, c83=SimpleText, c84=MockSep, c81=Pulsing(freqCutoff=15), b39=Standard, c82=MockFixedIntBlock(blockSize=1606), b37=Standard, b38=SimpleText, b35=MockSep, b36=Pulsing(freqCutoff=15), b33=MockFixedIntBlock(blockSize=1606), b34=MockVariableIntBlock(baseBlockSize=99), b31=MockRandom, b32=Standard, str2=MockFixedIntBlock(blockSize=1606), b50=MockVariableIntBlock(baseBlockSize=99), b52=Pulsing(freqCutoff=15), str3=SimpleText, b51=MockSep, c86=MockVariableIntBlock(baseBlockSize=99), tvtest=MockSep, c85=MockFixedIntBlock(blockSize=1606), c88=Pulsing(freqCutoff=15), c87=MockSep, c89=Standard, c90=SimpleText, c91=MockSep, c92=MockRandom, c93=Standard, c94=MockFixedIntBlock(blockSize=1606), c95=MockVariableIntBlock(baseBlockSize=99), content1=Pulsing(freqCutoff=15), b46=MockVariableIntBlock(baseBlockSize=99), b47=MockRandom, content3=MockVariableIntBlock(baseBlockSize=99), b48=Pulsing(freqCutoff=15), content4=MockFixedIntBlock(blockSize=1606), b49=MockFixedIntBlock(blockSize=1606), content5=Standard, b42=MockSep, b43=Pulsing(freqCutoff=15), b44=Standard, b45=SimpleText}, locale=tr, timezone=MET
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestMergeSchedulerExternal, TestCharTokenizers, TestCodecs, TestFieldInfos, TestFlushByRamOrCountsPolicy, TestIndexReaderReopen, TestIndexWriter]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=69508608,total=127336448
{noformat}

Simon dug and it looks like this is a trunk issue, caused by LUCENE-1076 (only committed to trunk so far)."
1,"MatchAllDocsQuery.toString(String field) does not honor the javadoc contractShould be 

public String toString(String field){
  return ""*:*"";
}

QueryParser needs to be able to parse the String form of this query."
1,"ManagedConnection#cleanup doesn't refresh the sessionthe ManagedConnection is not cleaned up correctly. I think that the underlying jcr Session should be refreshed by calling
Session#refresh(false) at JCAManagedConnection#cleanup. In the current implementation a new Session see the changes stored in the transient level of a closed session"
1,"OpenBitSet#hashCode() may return false for identical sets.OpenBitSet uses an internal buffer of long variables to store set bits and an additional 'wlen' index that points 
to the highest used component inside {@link #bits} buffer.

Unlike in JDK, the wlen field is not continuously maintained (on clearing bits, for example). This leads to a situation when wlen may point
far beyond the last set bit. 

The hashCode implementation iterates over all long components of the bits buffer, rotating the hash even for empty components. This is against the contract of hashCode-equals. The following test case illustrates this:

{code}
// initialize two bitsets with different capacity (bits length).
BitSet bs1 = new BitSet(200);
BitSet bs2 = new BitSet(64);
// set the same bit.
bs1.set(3);
bs2.set(3);
        
// equals returns true (passes).
assertEquals(bs1, bs2);
// hashCode returns false (against contract).
assertEquals(bs1.hashCode(), bs2.hashCode());
{code}

Fix and test case attached."
1,"InitiatedIndex: CCE on casting NumericField to FieldAn unchecked cast to List<Field> throws a ClassCastException when applied to, for example, a NumericField.
Appearently, this has been fixed trunk, but for a 2.9.1 release, this could be helpful.
The patch can be applied against the 2.9.0 tag."
1,"Wire produces invalid log skipping zero bytes in certain casesWireLogInputStream class line 82 check if the byte returned is not -1 meaning end of stream. But the condition is wrong in case if this byte is 0, it should look like

if (l != -1) {
//...
}
"
1,"Query with document order fails when result set size > caching hierarchy manager sizeWhen a query returns a lot of nodes in the query result and document order is enabled (which is the default) then the query will fail with error messages in the log:

*ERROR* [main] DocOrderNodeIteratorImpl: Internal error: unable to determine document order of nodes: (DocOrderNodeIteratorImpl.java, line 241)
*ERROR* [main] DocOrderNodeIteratorImpl:    Node1: /stuff/node[2]/node[13]/node9 (DocOrderNodeIteratorImpl.java, line 242)
*ERROR* [main] DocOrderNodeIteratorImpl:    Node2: /stuff/node[2]/node[13]/node5 (DocOrderNodeIteratorImpl.java, line 243)

The critical size seems to be equivalent to the cache size of the caching hierarchy manager. Attached are two test cases. The first one simply creates test nodes and the second one executes a query for those nodes. Using the cache size of 10'000 in the CachingHierarchyManager#DEFAULT_UPPER_LIMIT everything works fine, but when this value is set to 1000 (you need to re-compile the class CachingHierarchyManager) the test fails with the mentioned errors."
1,"add/remove dispatchers from DelegatingObservationDispatcher is not synchronizedthe 'dispatchers' hashset in DelegatingObservationDispatcher is not synchronized and can lead to errors, when a workspace goes offline or is creating during event dispatching."
1,SQL2 ISDESCENDANTNODE BooleanQuery#TooManyClauses returnsThe initial fix is not generic enough. It still fails after adding twice the max clauses count.
1,"Field names can be wrong for stored fields / term vectors after mergingThe good news is this bug only exists in trunk... the bad news is it's
been here for some time (created by accident in LUCENE-2881).  But the
good news is it should strike fairly rarely.

SegmentMerger sometimes incorrectly thinks it can bulk-copy TVs/stored
fields when it cannot (because field numbers don't map to the same
names across segments).

I think it happens only with addIndexes, or indexes that have
pre-trunk segments, and then SM falsely thinks it can bulk-merge only
when the last field number has the same field name across segments.
"
1,"HttpClient should always override the host of HostConfiguration if an absolute request URI is givenThis bug most likely occurs on all patforms and OS's, but I have only tested it
on WinXP.

The HttpClient.executeMethod(HostConfiguration,HttpMethod,HttpState) will
receive and throw an IllegalArgumentException stating that ""host parameter is
null"" when a  HostConfiguration object is passed in that ONLY has a proxy set
(via HostConfiguration.setProxy(String, int)). Details to reproduce follow--the
bug can be easily reproduced by using the Apache Axis 1.2 CommonsHTTPSender
class (with JVM system props http.proxyHost, http.proxyPort set):

There is a bug in the Apache Commons HTTP Client 3.0rc2 that does not set the
hostname property
in the <code>HostConfiguration</code> object if the following two steps
are performed:<br>
1. You call
<code>HttpClient.executeMethod(HostConfiguration,HttpMethod,HttpState)</code>
with a <code>HostConfiguration</code> object and an <code>HttpMethod</code> object
(created using the HttpMethod(String uri) constructor).This method 
is called in this exact way in the Apache Axis 1.2 client
(CommonsHTTPSender.java lines 132 and 186).<br>
2. That <code>HostConfiguration</code> object only has a proxy set (using
setProxy(String, int)). This method 
is called in this exact way in the Apache Axis 1.2 client
(CommonsHTTPSender.java line 389).<br>

Apache Axis 1.2rc3 CommonsHTTPSender.java did not expose this bug in Commons
HTTP Client 3.0rc2 because
it set the <code>HostConfiguration</code> in a different manner, as follows:<br>
1. Call <code>HttpClient.setHostConfiguration(HostConfiguration)</code> first.
Again,
The <code>HostConfiguration</code> object must only have a proxy set and no host
name.<br>
2. Then call <code>HttpClient.executeMethod(HttpMethod)</code>.<br>

Using the above steps (as in Axis 1.2rc3 CommonsHTTPSender.java, invoke()
method), line 379 in HttpClient.java evaluates to true
because the argument <code>hostConfiguration</code> is null (see line 324 in
HttpClient.java) and the local 
variable <code>defaultHostConfiguration</code> ==
<code>HttpClient.setHostConfiguration(HostConfiguration)</code>
which was set in item #1 above. The hostname then gets set in the
<code>HostConfiguration</code>
object in line 384 of HttpClient.java."
1,"Kerberos cross-realm support is brokenThis issue is basically based on the same facts as this issue https://issues.sonatype.org/browse/AHC-71?focusedCommentId=129559#action_129559 
Since the Kerberos code looks the same, I assume that AHC used your code. The same patch can be applied to fix [this http://hc.apache.org/httpcomponents-client-ga/httpclient/xref/org/apache/http/impl/auth/NegotiateScheme.html#200] defective code."
1,"""Socket Closed"" IOException thrown by HttpConnectionHttpClient.java was modified in version 2.0 Final in method executeMethod().
The call to connection.setSoTimeout() used to be in RC3 after the call to
connection.isOpen(), but in the final version the call happens before the call 
to isOpen(). The result of the change is that the setSoTimeout() call could 
throw IOException because of closed socket.

I would fix the problem by adding to HttpConnection.setSoTimeout() (and to 
other similar methods in HttpConnection) an explicit check (a call to isOpen
()) whether the socket is closed as the existence of socket object does not 
guarantee it. I.e the following code:

    public void setSoTimeout(int timeout)
        throws SocketException, IllegalStateException {
        LOG.debug(""HttpConnection.setSoTimeout("" + timeout + "")"");
        soTimeout = timeout;
        if (socket != null) {
            socket.setSoTimeout(timeout);
        }
    }

would be changed to

    public void setSoTimeout(int timeout)
        throws SocketException, IllegalStateException {
        LOG.debug(""HttpConnection.setSoTimeout("" + timeout + "")"");
        soTimeout = timeout;
        if (isOpen()) {
            socket.setSoTimeout(timeout);
        }
    }"
1,"NPE in NearSpansUnordered.isPayloadAvailable() Using RC1 of lucene 2.4 resulted in null pointer exception with some constructed SpanNearQueries

Implementation of isPayloadAvailable() (results in exception)
{code}
 public boolean isPayloadAvailable() {
   SpansCell pointer = min();
   do {
     if(pointer.isPayloadAvailable()) {
       return true;
     }
     pointer = pointer.next;
   } while(pointer.next != null);

   return false;
  }
{code}

""Fixed"" isPayloadAvailable()
{code}
 public boolean isPayloadAvailable() {
   SpansCell pointer = min();
   while (pointer != null) {
     if(pointer.isPayloadAvailable()) {
       return true;
     }
     pointer = pointer.next;
   }

   return false;
  }
{code}

Exception produced:
{code}
  [junit] java.lang.NullPointerException
    [junit]     at org.apache.lucene.search.spans.NearSpansUnordered$SpansCell.access$300(NearSpansUnordered.java:65)
    [junit]     at org.apache.lucene.search.spans.NearSpansUnordered.isPayloadAvailable(NearSpansUnordered.java:235)
    [junit]     at org.apache.lucene.search.spans.NearSpansOrdered.shrinkToAfterShortestMatch(NearSpansOrdered.java:246)
    [junit]     at org.apache.lucene.search.spans.NearSpansOrdered.advanceAfterOrdered(NearSpansOrdered.java:154)
    [junit]     at org.apache.lucene.search.spans.NearSpansOrdered.next(NearSpansOrdered.java:122)
    [junit]     at org.apache.lucene.search.spans.SpanScorer.next(SpanScorer.java:54)
    [junit]     at org.apache.lucene.search.Scorer.score(Scorer.java:57)
    [junit]     at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:137)
    [junit]     at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:113)
    [junit]     at org.apache.lucene.search.Hits.getMoreDocs(Hits.java:113)
    [junit]     at org.apache.lucene.search.Hits.<init>(Hits.java:80)
    [junit]     at org.apache.lucene.search.Searcher.search(Searcher.java:50)
    [junit]     at org.apache.lucene.search.Searcher.search(Searcher.java:40)
    [junit]     at com.attivio.lucene.SpanQueryTest.search(SpanQueryTest.java:79)
    [junit]     at com.attivio.lucene.SpanQueryTest.assertHitCount(SpanQueryTest.java:75)
    [junit]     at com.attivio.lucene.SpanQueryTest.test(SpanQueryTest.java:67)
{code}

will attach unit test that causes exception (and passes with updated isPayloadAvailable())
"
1,"QOM to SQL2 doesn't cast numeric literalsSQL2 statements generated by the QueryObjectModel.getStatement() don't contain CAST(... AS ...) for numeric literals of types DECIMAL, DOUBLE, and LONG. The type information is lost, which can result in incorrect query results (depending on the query engine) if the generated SQL2 statement is executed."
1,PdfTextExtractor does not close temp file in case of an errorIf PDF parsing fails in PDFParser.parse() a temp file is not closed and results in an open file handle.
1,ChainedFilter does not work well in the event of filters in ANDNOTFirst ANDNOT operation takes place against a completely false bitset and will always return zero results.  
1,"Missing 'node removed' event when removing a versionWhen a version is removed only one 'node removed' event is triggered for the version node. Even though the frozen node under that version also gets removed there is no event for the frozen node.

See failing test cases:
org.apache.jackrabbit.core.observation.VersionEventsTest#testRemoveVersion()
org.apache.jackrabbit.core.observation.VersionEventsTest#testXARemoveVersion()
"
1,"MergeThread throws unchecked exceptions from toString()This causes nearly all thread-dumping routines to fail and in the effect obscure the original problem. I think this
should return a string (always), possibly indicating the underlying writer has been closed or something."
1,"[PATCH] NullPointerException when using nested SpanOrQuery in SpanNotQueryOverview description: 
I'm using the span query classes in Lucene to generate higher scores for 
search results where the search terms are closer together. In certain 
situations I want to exclude terms from the span. When I attempt to exclude 
more than one term I get an error. 
 
The example query I'm using is:  
 
'brighton AND tourism' -pier -contents 
 
I construct the query objects and the toString() version is: 
 
spanNot(spanNear([contents:brighton contents:tourism], 10, false), 
spanOr([contents:pier, contents:road])) 
  
 
Steps to reproduce: 
1. Construct a SpanNearQuery (must have at least one term, but at least two 
makes more sense) 
2. Construct a SpanOrQuery containing two or more terms 
3. Construct a SpanNotQuery to include the first query object and exclude the 
second (SpanOrQuery) 
4. Execute the search 
 
 
Actual Results: 
A null pointer exception is thrown while generating the scores within the 
search. 
 
Stack trace:  
java.lang.NullPointerException   
        at   
org.apache.lucene.search.spans.SpanOrQuery$1.doc(SpanOrQuery.java:174)   
        at   
org.apache.lucene.search.spans.SpanNotQuery$1.next(SpanNotQuery.java:75)   
        at org.apache.lucene.search.spans.SpanScorer.next(SpanScorer.java:50)   
        at org.apache.lucene.search.Scorer.score(Scorer.java:37)   
        at   
org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:92)   
        at org.apache.lucene.search.Hits.getMoreDocs(Hits.java:64)   
        at org.apache.lucene.search.Hits.<init>(Hits.java:43)   
        at org.apache.lucene.search.Searcher.search(Searcher.java:33)   
        at org.apache.lucene.search.Searcher.search(Searcher.java:27)   
        at   
com.runtimecollective.search.LuceneSearch.search(LuceneSearch.java:362)   
 
 
Expected Resuts: 
It executes the search and results where the first search terms (near query) 
are close together but without the second terms (or query) appearing."
1,"isCurrent() and getVersion() on an NRT reader are brokenRight now isCurrent() will always return true for an NRT reader and getVersion() will always return the version of the last commit.  This is because the NRT reader holds the live segmentInfos.

I think isCurrent() should return ""false"" when any further changes have occurred with the writer, else true.   This is actually fairly easy to determine, since the writer tracks how many docs & deletions are buffered in RAM and these counters only increase with each change.

getVersion should return the version as of when the reader was created."
1,"JCR-Server: Workspace.restore not mapped correctly(issue reported by David Kennedy)

Workspace.restore(Version[], boolean) won't work, since versions are not retrieved correctly. The version history that can be access from the request  resource, cannot be used to retrieve the versions needed for a workspace.restore call.

possible short term fix:
From the version-hrefs present in the request body of the UPDATE request, version resources must be built and
the corresponding version item retrieved.

alternative:
find a proper mapping for Workspace.restore(Version[], boolean). having UPDATE on a resource representing a javax.jcr.Node being mapped to a workspace.restore is odd.

"
1,"BoostingTermQuery.explain() bugsThere are a couple of minor bugs in BoostingTermQuery.explain().

1. The computation of average payload score produces NaN if no payloads were found. It should probably be:
float avgPayloadScore = super.score() * (payloadsSeen > 0 ? (payloadScore / payloadsSeen) : 1);

2. If the average payload score is zero, the value of the explanation is 0:
result.setValue(nonPayloadExpl.getValue() * avgPayloadScore);
If the query is part of a BooleanClause, this results in:
""no match on required clause...""
""failure to meet condition(s) of required/prohibited clause(s)""

The average payload score can be zero if the field boost = 0.

I've attached a patch to 'TestBoostingTermQuery.java', however, the test 'testNoPayload' fails in 'SpanScorer.score()' because the doc = -1. It looks like 'setFreqCurrentDoc() should have been called before 'score()'. Maybe someone more knowledgable of spans could investigate this.
"
1,"OCM:The UUID of the collection elements changes on update.On ocm.update transaction, the  Current implementation of DefaultCollectionConverterImpl recreates the colleciton-element nodes if there is no id field specificaiton.  This is completely valid for majority of the cases.  But I came across a case where the colleciton element has a uuid field.  In this case also what is happening with the current implementation is that it drops all the elements from the old collection-elements and recreates the new ones.  The major flip side is that now I am left with brand new UUIDs.  I think we should address the uniqueness characteristics specified through UUID also while mapping colleciton elements.

I have a patch and a TestCase to verify the same.  I have implemented it only for the digester.  If people feel the approach is right I will work out an annotation based testcase as well.  I do not think it is going to fail even with annotations.
"
1,"failing Node.lock() might leave inconsistent transient stateWhen I try to node.lock(true, false) a node and the lock fails due to lak of user privilegies, the lock stay in the user transient session. If a perform a node.refresh(false) the node still is locked in the transient session."
1,"Default proxy set at the client level has no effectDefault proxy set at the client level has no effect, as client parameters are not correctly propagated to the HttpRoutePlanner"
1,"Spellchecker's dictionary iterator misbehavesIn LuceneDictionary, the LuceneIterator.hasNext() method has two issues that makes it misbehave:

1) If hasNext is called more than once, items are skipped
2) Much more seriously, when comparing fieldnames it is done with != rather than .equals() with the potential result that nothing is indexed
"
1,"XPath relative path support missing for ""is null"" and ""is not null""I believe the change for issue JCR-247 is incomplete, for instance

  //*[@x]

and

  //*[foo/@x]

are parsed into the same query tree."
1,"ChunkedInputStream broken (2 bugs + fixes, 1 suggestion)Bug 1.

In the     

read(byte[] b, int off, int len)  method

of ChunkedInputStream, the number of bytes to read from the underlying 
InputStream is calculated wrongly. In the code this is done by

len = Math.min(len, chunkSize);

This could (and will) cause the server (also Apache) to indeed serve that 
number of bytes (let's say chunkSize), but it may be that we already had a 
number of bytes on the first read. The result is that the input is now NOT 
positioned on the end of a chunk and the rest of the reader fails because it 
cannot find CRLF or a valid chunksize.

Proposed fix (works, tested)
len = Math.min(len, chunkSize-pos);

Bug 2.

In the calculation of the chunkSize (method getChunkSizeFromInputStream) the 
conversion to int is done by calling             

result = Integer.parseInt(dataString, 16);

This is not robust and causes the occasional crash. The fix is simple and in 
fact implements what is done when the chunkSize is commented (see lines in code 
above)

result = Integer.parseInt(dataString.trim(), 16);

Tested and works.



Suggestion:
Same routine, input state machine. Perhaps just being pedantic..change while 
loop to:

        while (state != 2) {
            int b = in.read();
            if (b == -1) throw new IOException(""chunked stream ended 
unexpectedly"");
            switch (state) {
                case 0:
                    if (b == '\r')
                      state = 1;
                    else
                      baos.write(b);
                    break;
                case 1:
                    if (b == '\n')
                      state = 2;
                    else{
                     // this was not CRLF, so now write '\r' + this char
                      baos.write('\r');
                      baos.write(b);
                      state = 0;
                    }
                    break;
                default: throw new RuntimeException(""assertion failed"");
            }
        }.

In the     

read(byte[] b, int off, int len)  method

of ChunkedInputStream, the number of bytes to read from the underlying 
InputStream is calculated wrongly. In the code this is done by

len = Math.min(len, chunkSize);

This could (and will) cause the server (also Apache) to indeed serve that 
number of bytes (let's say chunkSize), but it may be that we already had a 
number of bytes on the first read. The result is that the input is now NOT 
positioned on the end of a chunk and the rest of the reader fails because it 
cannot find CRLF or a valid chunksize.

Proposed fix (works, tested)
len = Math.min(len, chunkSize-pos);

Bug 1.

In the     

read(byte[] b, int off, int len)  method

of ChunkedInputStream, the number of bytes to read from the underlying 
InputStream is calculated wrongly. In the code this is done by

len = Math.min(len, chunkSize);

This could (and will) cause the server (also Apache) to indeed serve that 
number of bytes (let's say chunkSize), but it may be that we already had a 
number of bytes on the first read. The result is that the input is now NOT 
positioned on the end of a chunk and the rest of the reader fails because it 
cannot find CRLF or a valid chunksize.

Proposed fix (works, tested)
len = Math.min(len, chunkSize-pos);

Bug 2.

In the calculation of the chunkSize (method getChunkSizeFromInputStream) the 
conversion to int is done by calling             

result = Integer.parseInt(dataString, 16);

This is not robust and causes the occasional crash. The fix is simple and in 
fact implements what is done when the chunkSize is commented (see lines in code 
above)

result = Integer.parseInt(dataString.trim(), 16);

Tested and works.



Suggestion:
Same routine, input state machine. Perhaps just being pedantic..change while 
loop to:

        while (state != 2) {
            int b = in.read();
            if (b == -1) throw new IOException(""chunked stream ended 
unexpectedly"");
            switch (state) {
                case 0:
                    if (b == '\r')
                      state = 1;
                    else
                      baos.write(b);
                    break;
                case 1:
                    if (b == '\n')
                      state = 2;
                    else{
                     // this was not CRLF, so now write '\r' + this char
                      baos.write('\r');
                      baos.write(b);
                      state = 0;
                    }
                    break;
                default: throw new RuntimeException(""assertion failed"");
            }
        }.

In the calculation of the chunkSize (method getChunkSizeFromInputStream) the 
conversion to int is done by calling             

result = Integer.parseInt(dataString, 16);

This is not robust and causes the occasional crash. The fix is simple and in 
fact implements what is done when the chunkSize is commented (see lines in code 
above)

result = Integer.parseInt(dataString.trim(), 16);

Tested and works.



Suggestion:
Same routine, input state machine. Perhaps just being pedantic..change while 
loop to:

        while (state != 2) {
            int b = in.read();
            if (b == -1) throw new IOException(""chunked stream ended 
unexpectedly"");
            switch (state) {
                case 0:
                    if (b == '\r')
                      state = 1;
                    else
                      baos.write(b);
                    break;
                case 1:
                    if (b == '\n')
                      state = 2;
                    else{
                     // this was not CRLF, so now write '\r' + this char
                      baos.write('\r');
                      baos.write(b);
                      state = 0;
                    }
                    break;
                default: throw new RuntimeException(""assertion failed"");
            }
        }"
1,"Checking of stale connections is brokenHttpConnections that went stale (dropped by server) throw SocketExceptions
instead of silently re-opening themselves, as has been the case with earlier
versions of HttpClient.

I think the problem for this can be found in HttpConnection:

  public boolean closeIfStale() throws IOException {
    if (used && isOpen && isStale()) {
      LOG.debug(""Connection is stale, closing..."");
      close();
      return true;
    }
    return false;
  }

staleness is only checked if used = true, but there is no code in HttpConnection
that sets the used flag. In other words: used is always false and isStale() is
never called."
1,"HttpClient 4.1 ignores request retry handler and stops retrying when a read timeout is followed by a connection refusalI encountered an issue while writing unit tests for the RestBackup(tm) API Client Library, https://github.com/mleonhard/restbackup-java .  HttpClient 4.1 is failing to retry when it encounters a read timeout followed by a connection refusal.  This problem occurs on Windows but not on Linux.  Below is a short program that reproduces the problem.  It performs the expected 5 request attempts on Linux but only 2 on Windows.

My Windows environment is a laptop with Windows 7 Ultimate 64-bit and Oracle Java SE Development Kit Update 21 32-bit.  My Linux environment is Amazon EC2 with Ubuntu 10.04 LTS 32-bit and Oracle Java SE Development Kit Update 21 32-bit.

This is my first bug report to an Apache project.  I'd like to add that I'm a big fan of the Commons libraries and Http Components.

Sincerely,
-Michael

=== RetryBug.java ===

import java.io.IOException;
import java.net.ServerSocket;
import java.util.logging.Logger;

import org.apache.http.client.HttpRequestRetryHandler;
import org.apache.http.client.methods.HttpGet;
import org.apache.http.impl.client.DefaultHttpClient;
import org.apache.http.params.CoreConnectionPNames;
import org.apache.http.protocol.HttpContext;

public class RetryBug {
    private static final Logger _log = Logger.getLogger(RetryBug.class.getName());

    public static void main(String[] args) throws IOException {
        ServerSocket serverSocket = new ServerSocket(0, 1);
        DefaultHttpClient httpClient = new DefaultHttpClient();
        HttpRequestRetryHandler retryHandler = new HttpRequestRetryHandler() {
                public boolean retryRequest(IOException e, int count, HttpContext context) {
                    _log.info(""count="" + count + "" "" + e.toString());
                    return count < 5;
                }
            };
        httpClient.setHttpRequestRetryHandler(retryHandler);
        httpClient.getParams().setIntParameter(CoreConnectionPNames.SO_TIMEOUT, 100);
        try {
            String url = ""http://127.0.0.1:"" + serverSocket.getLocalPort() + ""/"";
            httpClient.execute(new HttpGet(url));
        } finally {
            serverSocket.close();
        }
    }
}


=== Windows 7 ===

C:\RetryBug>md5sum httpcomponents-client-4.1-bin.zip
008ad15560249bcde42cfe34fdb4e858 *httpcomponents-client-4.1-bin.zip

C:\RetryBug>""c:\Program Files (x86)\Java\jdk1.6.0_21\bin\java.exe"" -version
java version ""1.6.0_21""
Java(TM) SE Runtime Environment (build 1.6.0_21-b06)
Java HotSpot(TM) Client VM (build 17.0-b16, mixed mode)

C:\RetryBug>""c:\Program Files (x86)\Java\jdk1.6.0_21\bin\javac.exe"" -cp httpcomponents-client-4.1\lib\commons-codec-1.4.jar;httpcomponents-client-4.1\lib\commons-logging-1.1.1.jar;httpcomponents-client-4.1\lib\httpclient-4.1.jar;httpcomponents-client-4.1\lib\httpcore-4.1.jar RetryBug.java

C:\RetryBug>""c:\Program Files (x86)\Java\jdk1.6.0_21\bin\java.exe"" -cp httpcomponents-client-4.1\lib\commons-codec-1.4.jar;httpcomponents-client-4.1\lib\commons-logging-1.1.1.jar;httpcomponents-client-4.1\lib\httpclient-4.1.jar;httpcomponents-client-4.1\lib\httpcore-4.1.jar;. RetryBug
Mar 9, 2011 9:14:36 PM RetryBug$1 retryRequest
INFO: count=1 java.net.SocketTimeoutException: Read timed out
Mar 9, 2011 9:14:36 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: I/O exception (java.net.SocketTimeoutException) caught when processing request: Read timed out
Mar 9, 2011 9:14:36 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: Retrying request
Exception in thread ""main"" org.apache.http.conn.HttpHostConnectException: Connection to http://127.0.0.1:56361 refused
        at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:158)
        at org.apache.http.impl.conn.AbstractPoolEntry.open(AbstractPoolEntry.java:149)
        at org.apache.http.impl.conn.AbstractPooledConnAdapter.open(AbstractPooledConnAdapter.java:121)
        at org.apache.http.impl.client.DefaultRequestDirector.tryExecute(DefaultRequestDirector.java:650)
        at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:454)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:820)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:754)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:732)
        at RetryBug.main(RetryBug.java:27)
Caused by: java.net.ConnectException: Connection refused: connect
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333)
        at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195)
        at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
        at java.net.Socket.connect(Socket.java:529)
        at org.apache.http.conn.scheme.PlainSocketFactory.connectSocket(PlainSocketFactory.java:120)
        at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:148)
        ... 8 more

C:\RetryBug>


=== Ubuntu 10 ===

$ md5sum httpcomponents-client-4.1-bin.tar.gz
f043c1cc016cb3b720be9fb020bfa755  httpcomponents-client-4.1-bin.tar.gz
$ ~/jdk1.6.0_21/bin/java -version
java version ""1.6.0_21""
Java(TM) SE Runtime Environment (build 1.6.0_21-b06)
Java HotSpot(TM) Client VM (build 17.0-b16, mixed mode, sharing)
$ ~/jdk1.6.0_21/bin/javac -cp httpcomponents-client-4.1/lib/httpclient-cache-4.1.jar:httpcomponents-client-4.1/lib/commons-logging-1.1.1.jar:httpcomponents-client-4.1/lib/httpcore-4.1.jar:httpcomponents-client-4.1/lib/httpclient-4.1.jar:httpcomponents-client-4.1/lib/httpmime-4.1.jar:httpcomponents-client-4.1/lib/commons-codec-1.4.jar RetryBug.java
$ ~/jdk1.6.0_21/bin/java -cp httpcomponents-client-4.1/lib/httpclient-cache-4.1.jar:httpcomponents-client-4.1/lib/commons-logging-1.1.1.jar:httpcomponents-client-4.1/lib/httpcore-4.1.jar:httpcomponents-client-4.1/lib/httpclient-4.1.jar:httpcomponents-client-4.1/lib/httpmime-4.1.jar:httpcomponents-client-4.1/lib/commons-codec-1.4.jar:. RetryBug
Mar 9, 2011 1:09:42 PM RetryBug$1 retryRequest
INFO: count=1 java.net.SocketTimeoutException: Read timed out
Mar 9, 2011 1:09:42 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: I/O exception (java.net.SocketTimeoutException) caught when processing request: Read timed out
Mar 9, 2011 1:09:42 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: Retrying request
Mar 9, 2011 1:09:42 PM RetryBug$1 retryRequest
INFO: count=2 java.net.SocketTimeoutException: Read timed out
Mar 9, 2011 1:09:42 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: I/O exception (java.net.SocketTimeoutException) caught when processing request: Read timed out
Mar 9, 2011 1:09:42 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: Retrying request
Mar 9, 2011 1:09:42 PM RetryBug$1 retryRequest
INFO: count=3 java.net.SocketTimeoutException: Read timed out
Mar 9, 2011 1:09:42 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: I/O exception (java.net.SocketTimeoutException) caught when processing request: Read timed out
Mar 9, 2011 1:09:42 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: Retrying request
Mar 9, 2011 1:09:42 PM RetryBug$1 retryRequest
INFO: count=4 java.net.SocketTimeoutException: Read timed out
Mar 9, 2011 1:09:42 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: I/O exception (java.net.SocketTimeoutException) caught when processing request: Read timed out
Mar 9, 2011 1:09:42 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: Retrying request
Mar 9, 2011 1:09:51 PM RetryBug$1 retryRequest
INFO: count=5 java.net.SocketTimeoutException: Read timed out
Exception in thread ""main"" java.net.SocketTimeoutException: Read timed out
        at java.net.SocketInputStream.socketRead0(Native Method)
        at java.net.SocketInputStream.read(SocketInputStream.java:129)
        at org.apache.http.impl.io.AbstractSessionInputBuffer.fillBuffer(AbstractSessionInputBuffer.java:149)
        at org.apache.http.impl.io.SocketInputBuffer.fillBuffer(SocketInputBuffer.java:110)
        at org.apache.http.impl.io.AbstractSessionInputBuffer.readLine(AbstractSessionInputBuffer.java:260)
        at org.apache.http.impl.conn.DefaultResponseParser.parseHead(DefaultResponseParser.java:98)
        at org.apache.http.impl.io.AbstractMessageParser.parse(AbstractMessageParser.java:252)
        at org.apache.http.impl.AbstractHttpClientConnection.receiveResponseHeader(AbstractHttpClientConnection.java:281)
        at org.apache.http.impl.conn.DefaultClientConnection.receiveResponseHeader(DefaultClientConnection.java:247)
        at org.apache.http.impl.conn.AbstractClientConnAdapter.receiveResponseHeader(AbstractClientConnAdapter.java:219)
        at org.apache.http.protocol.HttpRequestExecutor.doReceiveResponse(HttpRequestExecutor.java:298)
        at org.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:125)
        at org.apache.http.impl.client.DefaultRequestDirector.tryExecute(DefaultRequestDirector.java:622)
        at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:454)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:820)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:754)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:732)
        at RetryBug.main(RetryBug.java:27)
$"
1,WorkspaceInfo.dispose() does not deregister from obs dispatcherJCR-305 introduces an automatic disposing of idle workspaces. this can lead to memory leaks because the observation factory is not deregistered from the delegating one.
1,"JCR2SPI NamespaceRegistryImpl.unregisterNamespace passes prefix to storage when uri is expectedWhen trying to unregister a namespace through SPI, Jackrabbit throws a NamespaceException : <prefix>: is not a registered namespace uri.

javax.jcr.NamespaceRegistry.unregisterNamespace(String prefix) expects the namespace prefix. Though, org.apache.jackrabbit.jcr2spi.NamespaceRegistryImpl.unregisterNamespace(String prefix) calls directly org.apache.jackrabbit.jcr2spi.NamespaceStorage.unregisterNamespace(String uri), which expects the namespace uri.

The namespace registry should first retrieve the uri for the provided prefix."
1,"NPE when copying nodes with Workspace.copy()I get a NullpointerException when using Workspace.copy():

java.lang.NullPointerException
at org.apache.jackrabbit.core.BatchedItemOperations.copyNodeState(BatchedItemOperations.java:1834)
at org.apache.jackrabbit.core.BatchedItemOperations.copyNodeState(BatchedItemOperations.java:1806)
at org.apache.jackrabbit.core.BatchedItemOperations.copyNodeState(BatchedItemOperations.java:1806)
at org.apache.jackrabbit.core.BatchedItemOperations.copy(BatchedItemOperations.java:423)
at org.apache.jackrabbit.core.WorkspaceImpl.internalCopy(WorkspaceImpl.java:444)
at org.apache.jackrabbit.core.WorkspaceImpl.copy(WorkspaceImpl.java:666)
at xxx.MyClass.myMeth(MyClass.java)

It seems that it happens not all the time. The error occurs since we use Jackrabbit 1.6.0. We do not get the error with previous versions. It seems that we only get the error when trying to copy nodes that were created with Jackrabbit 1.4 and copied with Jackrabbit 1.6."
1,"JCR-2523 break the transaction handling in container managed environmentduring the cleanup (returning to the pool) of an jca managed connection,  an new internal session is created in the object JCAManagedConnection in the method cleanup, this is supposed to fix JCR-2523, The sideeffect is, that the XA-Resource (variable-xaResource) in JCAManagedConnection is not anymore the same XASessionImpl Object like the session Object. Subsequent calls on this connection, lead that the internal session variable is not anymore informed about the current transaction context. (XAItemStateManager, variables tx and txLog are null), because only the xaResource is informed about the new transaction context. Result is that the complete transaction handling does not work anymore.
I attached a sample project which shows this behaviour.
"
1,"Missing synchronization in InternalVersionHistoryImplThe InternalVersionHistoryImpl objects can be accessed (and modified) concurrently by multiple sessions, which can in some rare cases result in corruption in the internal cache map data structures. Access to these cache maps should be properly synchronized."
1,"Don't commit an empty segments_N when IW is opened with create=trueIf IW is opened with create=true, it forcefully commits an empty
segments_N.  But really it should not: if autoCommit is false, nothing
should be committed until commit or close is explicitly called.

Spinoff from http://www.nabble.com/no-segments*-file-found:-files:-Error-on-opening-index-td23219520.html
"
1,"FSDirectory.openFile(String) causes ClassCastExceptionWhen you call FSDirectory.openFile(String) you get a ClassCastException since FSIndexInput is not an org.apache.lucene.store.InputStream

The workaround is to reimplement using openInput(String). I personally don't need this to be fixed but wanted to document it here in case anyone else runs into this for any reason.

The reason I'm calling this is that I have a requirement on my project to create read only indexes and name the index segments consistently from one build to the next. So, after creating and optimizing the index, I rename the files and rewrite the segments file. It would be nice if I had an API that would allow me to say ""I only want one segment and I want its name to be 'foo'"". For instance IndexWriter.optimize(String segmentName)"
1,"IndexReader.clone can leave files openI hit this in working on LUCENE-1516.

When not using compound file format, if you clone an IndexReader, then close the original, then close the clone, the stored fields files (_X.fdt, _X.fdx) remain incorrectly open.

I have a test showing it; fix is trivial.  Will post patch & commit shortly."
1,"trunk TestRollingUpdates.testRollingUpdates seed failuretrunk r1152892
reproducable: always

{code}
junit-sequential:
    [junit] Testsuite: org.apache.lucene.index.TestRollingUpdates
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 1.168 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestRollingUpdates -Dtestmethod=testRollingUpdates -Dtests.seed=-5322802004404580273:-4001225075726350391
    [junit] WARNING: test method: 'testRollingUpdates' left thread running: merge thread: _c(4.0):cv3/2 _h(4.0):cv3 into _k
    [junit] RESOURCE LEAK: test method: 'testRollingUpdates' left 1 thread(s) running
    [junit] NOTE: test params are: codec=RandomCodecProvider: {docid=Standard, body=SimpleText, title=MockSep, titleTokenized=Pulsing(freqCutoff=20), date=MockFixedIntBlock(blockSize=1474)}, locale=lv_LV, timezone=Pacific/Fiji
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestRollingUpdates]
    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_26 (64-bit)/cpus=8,threads=1,free=128782656,total=158400512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testRollingUpdates(org.apache.lucene.index.TestRollingUpdates):   FAILED
    [junit] expected:<20> but was:<21>
    [junit] junit.framework.AssertionFailedError: expected:<20> but was:<21>
    [junit]     at org.apache.lucene.index.TestRollingUpdates.testRollingUpdates(TestRollingUpdates.java:76)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1522)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1427)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.index.TestRollingUpdates FAILED

{code}"
1,"Overwriting a reference property with different type corrupts rep- create node n1
- create node n2
- n2.setProperty(""prop"", n1)
- save()
- n2.setProperty(""prop"", ""hello, world."")
- save()
- n1.remove()
- save() --> exception

see also ReferencesTest case

btw: removing the property or overwriting with a different reference works."
1,"After RepositoryImpl instance has been created and shut down, some classes cannot be unloadedI've built a simple web-application, which contains one servlet loaded at start-up. In its init() method an instance of RepositoryImpl() is created, in its destroy() method this instance is stopped (using shutdown()).
From the servlet code, only classes in jackrabbit-core, JCR API and Servlet API are referenced.
jackrabbit-core version is 1.4.5, and jackrabbit-jcr-commons version is 1.4.2. Other jackrabbit libs are all of 1.4 version.

Even if servlet's doGet() method never gets called, when the web-application is redeployed, all its classes still hang in memory, which produces a memory leak.

init() method is 

    public void init() throws ServletException {
        super.init();
        try {
            RepositoryConfig repoConfig = RepositoryConfig.create(getClass().getResourceAsStream(""repository.xml""), ""."");
            repo = RepositoryImpl.create(repoConfig);
        } catch (Exception e) {
            throw new ServletException(e);
        }
    }

while destroy() method is

    public void destroy() {
        repo.shutdown();
        super.destroy();
    }

Even when I applied patches from JCR-1636 and added TransientFileFactory.shutdown() call to destroy() method, nothing has changed.
Tested this in Jetty 6.1.9 and Tomcat 6.0.14."
1,"Missing support for lock timeout and ownerHint in jcr-servertrying to set the lock timeout when creating a lock seems not to work over the davex transport. the timeout is always 2147483.

this was my test code:

import javax.jcr.*;
import javax.jcr.lock.*;

import org.apache.jackrabbit.jcr2spi.RepositoryImpl;
import org.apache.jackrabbit.jcr2spi.config.RepositoryConfig;


String url = ""http://localhost:8080/server/"";
String workspace = ""tests"";

RepositoryConfig config = new RepositoryConfigImplTest(repoUrl);
Repository repo = RepositoryImpl.create(config);

Credentials sc = new SimpleCredentials(""admin"",""admin"".toCharArray());
Session s = repo.login(sc,workspace);

Node t;
if (s.getRootNode().hasNode(""test"")) {
    t = s.getRootNode().getNode(""test"");
} else {
    t = s.getRootNode().addNode(""test"", ""nt:unstructured"");
}
t.addMixin(""mix:lockable"");
s.save();
LockManager m = s.getWorkspace().getLockManager();
Lock l = m.lock(t.getPath(), false, true, 10, ""me"");
System.out.println(l.getSecondsRemaining());

and the output is 2147483


the relevant communication fragment is below, i attach the full trace in case i miss something.

LOCK /server/tests/jcr%3aroot/test HTTP/1.1
Timeout: Second-10
Depth: 0
Link: <urn:uuid0c740bb9-042a-4ef2-b019-1a6c52784c29>; rel=""http://www.day.com/jcr/webdav/1.0/session-id""
Authorization: Basic YWRtaW46YWRtaW4=
User-Agent: Jakarta Commons-HttpClient/3.0
Host: localhost:8080
Content-Length: 254
Content-Type: text/xml; charset=UTF-8

<?xml version=""1.0"" encoding=""UTF-8"" standalone=""no""?><D:lockinfo xmlns:D=""DAV:""><D:lockscope><dcr:exclusive-session-scoped xmlns:dcr=""http://www.day.com/jcr/webdav/1.0""/></D:lockscope><D:locktype><D:write/></D:locktype><D:owner>me</D:owner></D:lockinfo>

HTTP/1.1 200 OK
Content-Type: text/xml; charset=utf-8
Content-Length: 450
Lock-Token: <aa724c28-3c24-41e8-a3b4-9fc129adf732>
Server: Jetty(6.1.x)

<?xml version=""1.0"" encoding=""UTF-8"" standalone=""no""?><D:prop xmlns:D=""DAV:""><D:lockdiscovery><D:activelock><D:lockscope><dcr:exclusive-session-scoped xmlns:dcr=""http://www.day.com/jcr/webdav/1.0""/></D:lockscope><D:locktype><D:write/></D:locktype><D:depth>0</D:depth><D:timeout>Second-2147483</D:timeout><D:owner>admin</D:owner><D:locktoken><D:href>aa724c28-3c24-41e8-a3b4-9fc129adf732</D:href></D:locktoken></D:activelock></D:lockdiscovery></D:prop>



by the way: if i do not explicitly logout before the program exits, the lock is also not released even though it is session based. should the session not trigger a logout on destruction?"
1,"ConcurrentScheduleManager.addMyself() has wrong intedThis method has the wrong index for the 'size' variable, I think it should b allInstances.size.

{code:java}
private void addMyself() {
    synchronized(allInstances) {
      final int size=0;
      int upto = 0;
      for(int i=0;i<size;i++) {
        final ConcurrentMergeScheduler other = (ConcurrentMergeScheduler) allInstances.get(i);
        if (!(other.closed && 0 == other.mergeThreadCount()))
          // Keep this one for now: it still has threads or
          // may spawn new threads
          allInstances.set(upto++, other);
      }
      allInstances.subList(upto, allInstances.size()).clear();
      allInstances.add(this);
    }
  }
{code}"
1,[PATCH] DbDataStore: Make sure streams are closedStream isn't closed on end of use. this patch fixes it.
1,"FrenchAnalyzer's tokenStream method does not honour the contract of AnalyzerIn {{Analyzer}} :
{code}
/** Creates a TokenStream which tokenizes all the text in the provided
    Reader.  Default implementation forwards to tokenStream(Reader) for 
    compatibility with older version.  Override to allow Analyzer to choose 
    strategy based on document and/or field.  Must be able to handle null
    field name for backward compatibility. */
  public abstract TokenStream tokenStream(String fieldName, Reader reader);
{code}


and in {{FrenchAnalyzer}}

{code}
public final TokenStream tokenStream(String fieldName, Reader reader) {

    if (fieldName == null) throw new IllegalArgumentException(""fieldName must not be null"");
    if (reader == null) throw new IllegalArgumentException(""reader must not be null"");
{code}"
1,"Bugs in org.apache.lucene.index.TermVectorsReader.clone()A couple of things:

- The implementation can return null which is not allowed.  It should throw a CloneNotSupportedException if that's the case.

- Part of the code reads:

    TermVectorsReader clone = null;
    try {
      clone = (TermVectorsReader) super.clone();
    } catch (CloneNotSupportedException e) {}

    clone.tvx = (IndexInput) tvx.clone();

If a CloneNotSupportedException is caught then ""clone"" will be null and the assignment to clone.tvx will fail with a null pointer exception."
1,"Deadlock caused by versioning operations within transactionDeadlock occurs, while running a very simple test, which is just trying
to checkout/checkin node within transaction concurrently from 2 threads.

Find enclosed thread dump, log and simple Java program.
I'm using UserTransaction implementation from jackrabbit test suite.

Regards
Przemo Pakulski
www.cognifide.com


Full thread dump Java HotSpot(TM) Client VM (1.4.2_08-b03 mixed mode):

""Thread-5"" prio=5 tid=0x03054c48 nid=0x180c in Object.wait() [355f000..355fd8c]
       at java.lang.Object.wait(Native Method)
       - waiting on <0x1148ef20> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock)
       at java.lang.Object.wait(Object.java:429)
       at EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock.acquire(Unknown Source)
       - locked <0x1148ef20> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock)
       at org.apache.jackrabbit.core.state.SharedItemStateManager.acquireWriteLock(SharedItemStateManager.java:1137)
       at org.apache.jackrabbit.core.state.SharedItemStateManager.access$200(SharedItemStateManager.java:110)
       at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:456)
       at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:651)
       at org.apache.jackrabbit.core.state.XAItemStateManager.prepare(XAItemStateManager.java:150)
       at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:128)
       - locked <0x11565ac8> (a org.apache.jackrabbit.core.TransactionContext)
       at org.apache.jackrabbit.core.XASessionImpl.prepare(XASessionImpl.java:300)
       at com.oyster.mom.contentserver.jcr.transaction.JackrabbitUserTransaction.commit(JackrabbitUserTransaction.java:102)
       at com.oyster.mom.contentserver.jcr.transaction.JrTestDeadlock.run(JrTestDeadlock.java:97)

""Thread-4"" prio=5 tid=0x0303b348 nid=0x9d0 in Object.wait() [351f000..351fd8c]
       at java.lang.Object.wait(Native Method)
       - waiting on <0x1148ef20> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock)
       at java.lang.Object.wait(Object.java:429)
       at EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock.acquire(Unknown Source)
       - locked <0x1148ef20> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock)
       at org.apache.jackrabbit.core.state.SharedItemStateManager.acquireWriteLock(SharedItemStateManager.java:1137)
       at org.apache.jackrabbit.core.state.SharedItemStateManager.access$200(SharedItemStateManager.java:110)
       at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:456)
       at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:651)
       at org.apache.jackrabbit.core.state.XAItemStateManager.prepare(XAItemStateManager.java:150)
       at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:128)
       - locked <0x1156f558> (a org.apache.jackrabbit.core.TransactionContext)
       at org.apache.jackrabbit.core.XASessionImpl.prepare(XASessionImpl.java:300)
       at com.oyster.mom.contentserver.jcr.transaction.JackrabbitUserTransaction.commit(JackrabbitUserTransaction.java:102)
       at com.oyster.mom.contentserver.jcr.transaction.JrTestDeadlock.run(JrTestDeadlock.java:97)

""IndexMerger"" daemon prio=5 tid=0x030388b8 nid=0x1858 in Object.wait() [34df000..34dfd8c]
       at java.lang.Object.wait(Native Method)
       - waiting on <0x114fd280> (a org.apache.commons.collections.buffer.BlockingBuffer)
       at java.lang.Object.wait(Object.java:429)
       at org.apache.commons.collections.buffer.BlockingBuffer.remove(BlockingBuffer.java:107)
       - locked <0x114fd280> (a org.apache.commons.collections.buffer.BlockingBuffer)
       at org.apache.jackrabbit.core.query.lucene.IndexMerger.run(IndexMerger.java:235)

""Thread-2"" daemon prio=5 tid=0x0303a230 nid=0xe4c in Object.wait() [349f000..349fd8c]
       at java.lang.Object.wait(Native Method)
       - waiting on <0x114fd2e0> (a java.util.TaskQueue)
       at java.util.TimerThread.mainLoop(Timer.java:429)
       - locked <0x114fd2e0> (a java.util.TaskQueue)
       at java.util.TimerThread.run(Timer.java:382)

""Thread-1"" daemon prio=5 tid=0x0301b7a0 nid=0x1a00 in Object.wait() [345f000..345fd8c]
       at java.lang.Object.wait(Native Method)
       - waiting on <0x114f9058> (a java.util.TaskQueue)
       at java.lang.Object.wait(Object.java:429)
       at java.util.TimerThread.mainLoop(Timer.java:403)
       - locked <0x114f9058> (a java.util.TaskQueue)
       at java.util.TimerThread.run(Timer.java:382)

""ObservationManager"" daemon prio=5 tid=0x02ef6c50 nid=0x10d8 in Object.wait() [341f000..341fd8c]
       at java.lang.Object.wait(Native Method)
       - waiting on <0x114f38e0> (a org.apache.commons.collections.buffer.BlockingBuffer)
       at java.lang.Object.wait(Object.java:429)
       at org.apache.commons.collections.buffer.BlockingBuffer.remove(BlockingBuffer.java:107)
       - locked <0x114f38e0> (a org.apache.commons.collections.buffer.BlockingBuffer)
       at org.apache.jackrabbit.core.observation.ObservationManagerFactory.run(ObservationManagerFactory.java:155)
       at java.lang.Thread.run(Thread.java:534)

""Signal Dispatcher"" daemon prio=10 tid=0x00a05590 nid=0x1914 waiting on condition [0..0]

""Finalizer"" daemon prio=9 tid=0x00a027f8 nid=0x17a4 in Object.wait() [2c9f000..2c9fd8c]
       at java.lang.Object.wait(Native Method)
       - waiting on <0x113db118> (a java.lang.ref.ReferenceQueue$Lock)
       at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:111)
       - locked <0x113db118> (a java.lang.ref.ReferenceQueue$Lock)
       at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:127)
       at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:159)

""Reference Handler"" daemon prio=10 tid=0x00a01478 nid=0x16d4 in Object.wait() [2c5f000..2c5fd8c]
       at java.lang.Object.wait(Native Method)
       - waiting on <0x113db180> (a java.lang.ref.Reference$Lock)
       at java.lang.Object.wait(Object.java:429)
       at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:115)
       - locked <0x113db180> (a java.lang.ref.Reference$Lock)

""main"" prio=5 tid=0x0003e6f0 nid=0x1470 in Object.wait() [7f000..7fc38]
       at java.lang.Object.wait(Native Method)
       - waiting on <0x11524f10> (a com.oyster.mom.contentserver.jcr.transaction.JrTestDeadlock)
       at java.lang.Thread.join(Thread.java:1001)
       - locked <0x11524f10> (a com.oyster.mom.contentserver.jcr.transaction.JrTestDeadlock)
       at java.lang.Thread.join(Thread.java:1054)
       at com.oyster.mom.contentserver.jcr.transaction.JrTestDeadlock.main(JrTestDeadlock.java:33)

""VM Thread"" prio=5 tid=0x00a42730 nid=0x17d0 runnable

""VM Periodic Task Thread"" prio=10 tid=0x00a45540 nid=0x1928 waiting on condition
""Suspend Checker Thread"" prio=10 tid=0x00a04af8 nid=0x17ac runnable

import javax.jcr.Node;
import javax.jcr.RepositoryException;
import javax.jcr.Session;
import javax.jcr.SimpleCredentials;

import org.apache.jackrabbit.core.RepositoryImpl;
import org.apache.jackrabbit.core.config.RepositoryConfig;

public class JrTestDeadlock extends Thread {

   private static final org.apache.commons.logging.Log log = org.apache.commons.logging.LogFactory.getLog(JrTestDeadlock.class);

   public static String REPOSITORY_HOME = ""d:/repo/jackrabbit/"";

   public static String REPOSITORY_CONFIG = REPOSITORY_HOME + ""repository.xml"";

   public static void main(String[] args) throws Exception {

       JrTestDeadlock test = new JrTestDeadlock(-1);
       test.startup();

       JrTestDeadlock tests[] = new JrTestDeadlock[2];

       for (int i = 0; i < tests.length; i++) {
           JrTestDeadlock x = new JrTestDeadlock(i);
           x.start();
           tests[i] = x;
       }

       for (int i = 0; i < tests.length; i++) {
           tests[i].join();
       }

       test.shutdown();
   }

   private static RepositoryImpl repository;

   private int id;

   public JrTestDeadlock(int i) {
       this.id = i;
   }

   public void startup() throws Exception {
       System.setProperty(""java.security.auth.login.config"", ""c:/jaas.config"");

       RepositoryConfig config = RepositoryConfig.create(REPOSITORY_CONFIG, REPOSITORY_HOME);
       repository = RepositoryImpl.create(config);

       Session session = repository.login(new SimpleCredentials(""admin"", ""admin"".toCharArray()));
       Node rootNode = session.getRootNode();
       if (!rootNode.hasNode(""folder"")) {
           Node folder = rootNode.addNode(""folder"");
           folder.addMixin(""mix:versionable"");
           folder.addMixin(""mix:lockable"");
           rootNode.save();
       }
       session.logout();
   }

   public void shutdown() throws RepositoryException {
       repository.shutdown();
   }

   public Node getFolder(Session session) throws RepositoryException {
       return session.getRootNode().getNode(""folder"");
   }

   public void run() {
       try {
           Session session = repository.login(new SimpleCredentials(""admin"", ""admin"".toCharArray()));
           for (int i = 0; i < 100; i++) {
               log.error(""START id:"" + id + "", i="" + i);

               boolean success = false;

               JackrabbitUserTransaction ut = new JackrabbitUserTransaction(session);
               try {
                   ut.begin();

                   Node folder = getFolder(session);
                   folder.checkout();
                   folder.checkin();

                   success = true;
                   log.info(""SUCCESS id:"" + id + "", i="" + i);
               }
               catch (Exception e) {
                   log.warn(""FAIL:"" + id + "", i="" + i, e);
               }
               finally {
                   try {
                       if (success) {
                           ut.commit();
                       }
                       else {
                           ut.rollback();
                       }
                   }
                   catch (Exception e) {
                       log.fatal(e);
                   }
               }
           }
           session.logout();
       }
       catch (RepositoryException e) {
           e.printStackTrace();
       }
   }
}


13:46 ERROR JrTestDeadlock.run(JrTestDeadlock.java:76) - START id:0, i=0
13:46 ERROR JrTestDeadlock.run(JrTestDeadlock.java:76) - START id:1, i=0
13:46 INFO  JrTestDeadlock.run(JrTestDeadlock.java:89) - SUCCESS id:0, i=0
13:46 INFO  JrTestDeadlock.run(JrTestDeadlock.java:89) - SUCCESS id:1, i=0
13:46 ERROR org.apache.jackrabbit.core.state.XAItemStateManager.prepare(XAItemStateManager.java:156) - org.apache.jackrabbit.core.state.StaleItemStateException: 233e656f-79f8-414d-9e37-3fce865b492d/{http://www.jcp.org/jcr/1.0}isCheckedOut has been modified externally
13:46 FATAL JrTestDeadlock.run(JrTestDeadlock.java:104) - javax.transaction.RollbackException: Transaction rolled back: XA_ERR=104
13:46 ERROR JrTestDeadlock.run(JrTestDeadlock.java:76) - START id:1, i=1
13:46 WARN  JrTestDeadlock.run(JrTestDeadlock.java:92) - FAIL:1, i=1
ax.jcr.InvalidItemStateException: f83a830b-abbf-4ab2-8625-b9e2c4802316: the item does not exist anymore
    at org.apache.jackrabbit.core.version.XAVersion.sanityCheck(XAVersion.java:81)
    at org.apache.jackrabbit.core.version.XAVersion.getInternalVersion(XAVersion.java:70)
    at org.apache.jackrabbit.core.version.AbstractVersion.getUUID(AbstractVersion.java:107)
    at org.apache.jackrabbit.core.NodeImpl.checkout(NodeImpl.java:2759)
    at JrTestDeadlock.run(JrTestDeadlock.java:85)
13:46 ERROR JrTestDeadlock.run(JrTestDeadlock.java:76) - START id:1, i=2
13:46 INFO  JrTestDeadlock.run(JrTestDeadlock.java:89) - SUCCESS id:1, i=2
13:51 WARN  org.apache.jackrabbit.core.TransactionContext.run(TransactionContext.java:239) - Transaction rolled back because timeout expired.

"
1,"URIResolverImpl: use of bitwise instead of logical AND operatorURIResolverImpl, line 111: 

                if (path != null & cache.containsItemId(uuidId)) {
"
1,"Merging implemented by codecs must catch aborted mergesThis is a regression (we lost functionality on landing flex).

When you close IW with ""false"" (meaning abort all running merges), IW asks the merge threads to abort.  The threads are supposed to periodically check if they are aborted and throw an exception if so.

But on the cutover to flex, where the codec can override how merging is done (but a default impl is in the base enum classes), we lost this."
1,"Deadlock in DBCP when accessing nodeI found a deadlock situation using JR 2.2.10, the problem is with DBCP 1.2.2 and is fixed in DBCP 1.3, JR trunk also uses DBCP 1.2.2 and should also be updated

The ticket in dbcp is #DBCP-270, related tickets are #DBCP-65 #DBCP-281 #DBCP-271

Stack trace of where my call is stalled:
{code}
main@1, prio=5, in group 'main', status: 'MONITOR'
	 blocks Timer-1@2545
	 waiting for Timer-1@2545 to release lock on {1}
	  at org.apache.commons.pool.impl.GenericObjectPool.addObjectToPool(GenericObjectPool.java:1137)
	  at org.apache.commons.pool.impl.GenericObjectPool.returnObject(GenericObjectPool.java:1076)
	  at org.apache.commons.dbcp.PoolableConnection.close(PoolableConnection.java:87)
	  at org.apache.commons.dbcp.PoolingDataSource$PoolGuardConnectionWrapper.close(PoolingDataSource.java:181)
	  at org.apache.jackrabbit.core.util.db.DbUtility.close(DbUtility.java:75)
	  at org.apache.jackrabbit.core.util.db.ResultSetWrapper.invoke(ResultSetWrapper.java:63)
	  at $Proxy12.close(Unknown Source:-1)
	  at org.apache.jackrabbit.core.persistence.pool.BundleDbPersistenceManager.loadBundle(BundleDbPersistenceManager.java:1042)
	  at org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.getBundle(AbstractBundlePersistenceManager.java:669)
	  at org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.load(AbstractBundlePersistenceManager.java:415)
	  at org.apache.jackrabbit.core.state.SharedItemStateManager.loadItemState(SharedItemStateManager.java:1830)
	  at org.apache.jackrabbit.core.state.SharedItemStateManager.getNonVirtualItemState(SharedItemStateManager.java:1750)
	  at org.apache.jackrabbit.core.state.SharedItemStateManager.getItemState(SharedItemStateManager.java:265)
	  at org.apache.jackrabbit.core.state.LocalItemStateManager.getNodeState(LocalItemStateManager.java:109)
	  at org.apache.jackrabbit.core.state.LocalItemStateManager.getItemState(LocalItemStateManager.java:174)
	  at org.apache.jackrabbit.core.state.XAItemStateManager.getItemState(XAItemStateManager.java:260)
	  at org.apache.jackrabbit.core.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:161)
	  at org.apache.jackrabbit.core.ItemManager.getItemData(ItemManager.java:382)
	  at org.apache.jackrabbit.core.ItemManager.getNode(ItemManager.java:669)
	  at org.apache.jackrabbit.core.ItemManager.getNode(ItemManager.java:647)
	  at org.apache.jackrabbit.core.LazyItemIterator.prefetchNext(LazyItemIterator.java:120)
	  at org.apache.jackrabbit.core.LazyItemIterator.next(LazyItemIterator.java:257)
	  at info.magnolia.jcr.iterator.DelegatingNodeIterator.next(DelegatingNodeIterator.java:79)
{code}

This is the offending thread:
{code}
Timer-1@2545 daemon, prio=5, in group 'main', status: 'MONITOR'
	 blocks main@1
	 waiting for main@1 to release lock on {1}
	  at org.apache.commons.dbcp.AbandonedTrace.addTrace(AbandonedTrace.java:176)
	  at org.apache.commons.dbcp.AbandonedTrace.init(AbandonedTrace.java:92)
	  at org.apache.commons.dbcp.AbandonedTrace.<init>(AbandonedTrace.java:82)
	  at org.apache.commons.dbcp.DelegatingStatement.<init>(DelegatingStatement.java:61)
	  at org.apache.commons.dbcp.DelegatingConnection.createStatement(DelegatingConnection.java:224)
	  at org.apache.commons.dbcp.PoolableConnectionFactory.validateConnection(PoolableConnectionFactory.java:331)
	  at org.apache.commons.dbcp.PoolableConnectionFactory.validateObject(PoolableConnectionFactory.java:312)
	  at org.apache.commons.pool.impl.GenericObjectPool.evict(GenericObjectPool.java:1217)
	  at org.apache.commons.pool.impl.GenericObjectPool$Evictor.run(GenericObjectPool.java:1341)
	  at java.util.TimerThread.mainLoop(Timer.java:512)
	  at java.util.TimerThread.run(Timer.java:462)
{code}"
1,"Cookie.parse exception when parsing expiry date in single quotesA Netscape-Enterprise/3.6 SP3 server sends a cookie where the parameter expires='Thu, 05-Dec-
2002 12:07:45 GMT'. 
Cookie.parse throws an exception because none of the four built-in formats 
matches - I have tested that the parse code works OK if the single quotes are omitted from the value 
being parsed.

Resolution: If the value of the 'expires' parameter starts and ends with a 
single quote then strip the first and last character before parsing."
1,"getAllLinearVersions does not return the base versionIt appears that for a given linear version history, getAllLinearVersions returns less versions than getAllVersions -- the root version seems to be missing."
1,"WorkspaceImpl.dispose() might cause ClassNotFoundExceptionWenn using Jackrabbit in an environment, where ClassLoaders may get inactivated in the sense, the loading new classes is not possible anymore, shutting down the repository may result in a ClassNotFoundException during WorkspaceImpl.dispose().

Reason for this is, that in the dispose() method, the ObservationManager is asked for all registered event listeners for them to be removed from the ObservationManager one-by-one. Asking for the listeners results in a new EventListenerIteratorImpl object being created.

If now, this class has never been used during the live time of the repository, this would cause a ClassNotFoundException because the class loader is not laoding classes anymore in the specific environment.

The specific environment is Eclipse, where one plugin is managing different Repository instances provided by separate plugins. When now the Jackrabbit provider plugin has already been stopped while the managing plugin tries to shutdown the Jackrabbit repository, the EventListenerIteratorImpl class cannot be loaded anymore and disposing the WorkspaceImpl in a controlled way fails.

I suggest adding an ObservationManagerImpl.dispose() method, which is called by the WorkspaceImpl like :
    WorkspaceImpl.dispose() {
       if (obsMgr != null) {
         obsMgr.dispose();
         obsMgr = null;
        }
    }

As a side effect of not calling getObservationManager[Impl]() the observation manager would also not be created if not existing yet.

As a side effect to having the dispose method is, that the ObservationManagerImpl class could also do other cleanup work in addition to clearing the listener lists."
1,"Cluster revision entries should be retrieved in orderThe selectRevisionStmtSQL (DatabaseJournal#buildSqlStatements) returns a result set which may not be ordered by REVISION_ID. This has the effect that cluster instances that want to synchronize to the latest revision do not update their local revision appropriately since they assume that the revision result set is ordered (see code in AbstractJournal#doSync). This might cause a lot of unnecessary CPU cycles on these machines with degraded performance as a result. Furthermore, it causes functional issues as well as events may be fired multiple times and in the wrong order."
1,"Request/Response race condition when doing multiple requests on the same connection.If one tries to do multiple request over the same socket connection a race 
condition occurs in the input/output streams.
eg. 
-- Some request -->
<- HTTP/1.1 200 OK
<- Some: Headers
<- 
<- The body.

-- Next request -->
<- HTTP/1.1 200 OK
<- More: Headers
<- 
<- Some data.

If the second request is sent, but the second response isn't yet received 
before the client starts to try to read it, it'll get 
a ""org.apache.commons.httpclient.HttpRecoverableException: Error in parsing the 
status  line from the response: unable to find line starting with ""HTTP/"""" 
exception (it will think ""The body."" is part of the second response).

The following code will reproduce the problem:

import java.io.*;
import java.net.*;
import java.util.*;
import org.apache.commons.httpclient.*;
import org.apache.commons.httpclient.methods.*;

public class HttpClientRaceBug {
    public static void main(String[] args) {
        try {
            SimpleHttpServer.listen(8987);
            HttpClient client = new HttpClient();
            client.startSession(""localhost"", 8987);
            client.getState().setCredentials(""Test Realm"",  
                new UsernamePasswordCredentials(""foo"", ""bar""));
            
            for (int i = 0; i < 100; i++) {
                GetMethod meth = new GetMethod();
                client.executeMethod(meth);
            }
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
    
    private static final class SimpleHttpServer implements Runnable {
        private Socket socket;
        public SimpleHttpServer(Socket socket) {
            this.socket = socket;
        }
        public static void listen(final int port) {
            Thread server = new Thread() {
                public void run() {
                    try {
                        ServerSocket ss = new ServerSocket(port);
                        while (true) {
                            new Thread(new 
                                SimpleHttpServer(ss.accept())).start();
                        }
                    } catch (Exception e) {
                        e.printStackTrace();
                    }
                }
            };
            
            server.setDaemon(true);
            server.start();
        }
        public void run() {
            try {
                BufferedReader in = new BufferedReader(new 
                    InputStreamReader(this.socket.getInputStream()));
                
                int len = 0;
                boolean auth = false;
                String line;
                while ((line = in.readLine()) != null) {
                    System.out.println(""> "" + line);
                    
                    if (line.trim().equals("""")) {
                        in.read(new char[len]);
                        doOutput(auth);
                        auth = false;
                        len = 0;
                        
                    } else if (line.indexOf(':') > -1) {
                        StringTokenizer tok = new StringTokenizer(line, "":"");
                        String key = tok.nextToken().toLowerCase();
                        if (key.equals(""content-length"")) {
                            len = Integer.parseInt(tok.nextToken().trim());
                        } else if (key.equals(""authorization"")) {
                            auth = true;
                        }
                    }
                }
            } catch (Exception e) {}
        }
        private static int count = 0;
        public void doOutput(boolean authorized) throws IOException {
            Writer out = new OutputStreamWriter(this.socket.getOutputStream());
            count++;
            
            String id = (count < 100) ? 
                ((count < 10) ? ""00"" + count : ""0"" + count) : """" + count;
            if (authorized) {
                write(out, ""HTTP/1.1 200 OK\r\n"");
            } else {
                write(out, ""HTTP/1.1 401 Unauthorized\r\n"");
            }
            write(out, ""WWW-Authenticate: Basic realm=\""Test Realm\""\r\n"");
            write(out, ""Response-Id: "" + id + ""\r\n"");
            write(out, ""Content-Type: text/html; charset=iso-8859-1\r\n"");
            write(out, ""Content-Length: 17\r\n\r\n"");
            write(out, ""My Response ("" + id + "")"");
            out.close();
        }
        private void write(Writer out, String text) throws IOException {
            System.out.print(""< "" + text);
            out.write(text);
        }
    }
}"
1,"DataStore.close() is never calledI've searched through the jackrabbit-core code and never found a call to DataStore.close(), although the method exists on the DataStore interface"
1,"If you hit the ""max term prefix"" warning when indexing, it never goes awaySilly bug.

If IW's infoStream is on, we warn whenever we hit a ridiculously long term (> 16 KB in length).  The problem is, we never reset this warning, so, once one doc contains such a massive term, we then keep warning over and over about that same term for future docs."
1,JCR2SPI: VersionManagerImpl.getVersionableNodeEntry uses toString() rather than getString() to obtain property valueVersionManagerImpl.getVersionableNodeEntry uses toString() rather than getString() to obtain property value.
1,"TestNRTManager hangdidn't check 3.x yet, just encountered this one running the tests"
1,"TestAddIndexes reproducible test failure on turnktrunk: r1133385

{code}
    [junit] Testsuite: org.apache.lucene.index.TestAddIndexes
    [junit] Tests run: 2843, Failures: 1, Errors: 0, Time elapsed: 137.121 sec
    [junit]
    [junit] ------------- Standard Output ---------------
    [junit] java.io.FileNotFoundException: _cy.fdx
    [junit]     at org.apache.lucene.store.RAMDirectory.fileLength(RAMDirectory.java:121)
    [junit]     at org.apache.lucene.store.MockDirectoryWrapper.fileLength(MockDirectoryWrapper.java:606)
    [junit]     at org.apache.lucene.index.SegmentInfo.sizeInBytes(SegmentInfo.java:294)
    [junit]     at org.apache.lucene.index.TieredMergePolicy.size(TieredMergePolicy.java:633)
    [junit]     at org.apache.lucene.index.TieredMergePolicy.useCompoundFile(TieredMergePolicy.java:611)
    [junit]     at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:2459)
    [junit]     at org.apache.lucene.index.TestAddIndexes$CommitAndAddIndexes3.doBody(TestAddIndexes.java:847)
    [junit]     at org.apache.lucene.index.TestAddIndexes$RunAddIndexesThreads$1.run(TestAddIndexes.java:675)
    [junit] java.io.FileNotFoundException: _cx.fdx
    [junit]     at org.apache.lucene.store.RAMDirectory.fileLength(RAMDirectory.java:121)
    [junit]     at org.apache.lucene.store.MockDirectoryWrapper.fileLength(MockDirectoryWrapper.java:606)
    [junit]     at org.apache.lucene.index.SegmentInfo.sizeInBytes(SegmentInfo.java:294)
    [junit]     at org.apache.lucene.index.TieredMergePolicy.size(TieredMergePolicy.java:633)
    [junit]     at org.apache.lucene.index.TieredMergePolicy.useCompoundFile(TieredMergePolicy.java:611)
    [junit]     at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:2459)
    [junit]     at org.apache.lucene.index.TestAddIndexes$CommitAndAddIndexes3.doBody(TestAddIndexes.java:847)
    [junit]     at org.apache.lucene.index.TestAddIndexes$RunAddIndexesThreads$1.run(TestAddIndexes.java:675)
    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestAddIndexes -Dtestmethod=testAddIndexesWithRollback -Dtests.seed=9026722750295014952:2645762923088581043 -Dtests.multiplier=3
    [junit] NOTE: test params are: codec=RandomCodecProvider: {id=SimpleText, content=SimpleText, d=MockRandom, c=SimpleText}, locale=fr, timezone=Africa/Kigali
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestAddIndexes]
    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=68050392,total=446234624
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testAddIndexesWithRollback(org.apache.lucene.index.TestAddIndexes):       FAILED
    [junit]
    [junit] junit.framework.AssertionFailedError:
    [junit]     at org.apache.lucene.index.TestAddIndexes.testAddIndexesWithRollback(TestAddIndexes.java:932)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1362)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1280)
    [junit]
    [junit]
    [junit] Test org.apache.lucene.index.TestAddIndexes FAILED
{code}


Fails randomly in my while(1) test run, and Fails after a few min of running: 
{code}
ant test -Dtestcase=TestAddIndexes -Dtests.seed=9026722750295014952:2645762923088581043 -Dtests.multiplier=3 -Dtests.iter=200 -Dtests.iter.min=1
{code}"
1,"ArrayStoreException while reregistering existing node types
class: NodeTypeManagerImpl
method: public NodeType[] registerNodeTypes(InputStream in, String contentType, boolean reregisterExisting)

                ...
                return (NodeType[]) nodeTypes.toArray(new NodeTypeDef[nodeTypes.size()]);
                ...

=> should be (I suppose !)

                return (NodeType[]) nodeTypes.toArray(new NodeType[nodeTypes.size()]);
"
1,"There are a few binary search implmentations in lucene that suffer from a now well known overflow bughttp://googleresearch.blogspot.com/2006/06/extra-extra-read-all-about-it-nearly.html

The places I see it are:

MultiSearcher.subSearcher(int)
TermInfosReader.getIndexOffset(Term)
MultiSegmentReader.readerIndex(int, int[], int)
MergeDocIDRemapper.remap(int)

I havn't taken much time to consider how likely any of these are to overflow. The values being averaged would have to be very large. That would rule out possible problems for at least a couple of these, but how about something like the MergeDocIDRemapper? Is there a document number that could be reached that has a chance of triggering this bug? If not we can close this and have a record of looking into it."
1,"corrupted paths after moving nodeswe just found a bug which corrupts the results of Node.getPath() - it seems to be related to older Jackrabbit bugs (e.g. JCR-768) but still happens in jackrabbit 1.3 and jackrabbit-1.4-SNAPSHOT

Basically we have a node with 3 subnodes (a, b, c), we move all of them to index 1 - this works fine, unless we call getPath() of the third Node before moving it.

The expected paths after moving would be:
a: /pages[37]/page/element[3]
b: /pages[37]/page/element[2]
c: /pages[37]/page/element

But we get these paths:

a: /pages[37]/page/element[3]
b: /pages[37]/page/element
c: /pages[37]/page/element"
1,"Deadlocks in ConcurrentVersioningWithTransactionsTestPatch follows for a ConcurrentVersioningWithTransactionsTest, based on the existing ConcurrentVersioningTest but using transactions around the versioning operations.

On my macbook, running the test with CONCURRENCY = 100 and NUM_OPERATIONS = 100 causes a deadlock after a few seconds, thread dumps follow.

Note that I had to ignore StaleItemStateException (which is probably justified, due to not locking stuff IIUC) to let the threads run long enough to show the problem.

Running the test a few times showed the same locking pattern several times: some threads are locked at line 87 (session.save(), no transaction) while others are at line 93 (transaction.commit()), in testConcurrentCheckinInTransaction():

    80    public void testConcurrentCheckinInTransaction() throws RepositoryException {
    81      runTask(new Task() {
    82        public void execute(Session session, Node test) throws RepositoryException {
    83          int i = 0;
    84          try {
    85            Node n = test.addNode(""test"");
    86            n.addMixin(mixVersionable);
    87            session.save();
    88            for (i = 0; i < NUM_OPERATIONS / CONCURRENCY; i++) {
    89              final UserTransaction utx = new UserTransactionImpl(test.getSession());
    90              utx.begin();
    91              n.checkout();
    92              n.checkin();
    93              utx.commit();
    94            }
    95            n.checkout();
    96          } catch (Exception e) {
    97            final String threadName = Thread.currentThread().getName();
    98            final Throwable deepCause = getLevel2Cause(e);
    99            if(deepCause!=null && deepCause instanceof StaleItemStateException) {
   100              // ignore 
   101            } else {
   102              throw new RepositoryException(threadName + "", i="" + i + "":"" + e.getClass().getName(), e);
   103            }
   104          }
   105        }
   106      }, CONCURRENCY);
   107    }"
1,"SpanOrQuery skipTo() doesn't always move forwardsIn SpanOrQuery the skipTo() method is improperly implemented if the target doc is less than or equal to the current doc, since skipTo() may not be called for any of the clauses' spans:

    public boolean skipTo(int target) throws IOException {
          if (queue == null) {
            return initSpanQueue(target);
          }

          while (queue.size() != 0 && top().doc() < target) {
            if (top().skipTo(target)) {
              queue.adjustTop();
            } else {
              queue.pop();
            }
          }
          
        	return queue.size() != 0;
        }

This violates the correct behavior (as described in the Spans interface documentation), that skipTo() should always move forwards, in other words the correct implementation would be:

    public boolean skipTo(int target) throws IOException {
          if (queue == null) {
            return initSpanQueue(target);
          }

          boolean skipCalled = false;
          while (queue.size() != 0 && top().doc() < target) {
            if (top().skipTo(target)) {
              queue.adjustTop();
            } else {
              queue.pop();
            }
            skipCalled = true;
          }
          
          if (skipCalled) {
        	return queue.size() != 0;
          }
          return next();
        }"
1,"TestIndexWriterOnDiskFull.testAddIndexOnDiskFull fails with java.lang.IllegalStateException: CFS has pending open files {noformat}
 Testsuite: org.apache.lucene.index.TestIndexWriterOnDiskFull
    [junit] Testcase: testAddIndexOnDiskFull(org.apache.lucene.index.TestIndexWriterOnDiskFull):	Caused an ERROR
    [junit] CFS has pending open files
    [junit] java.lang.IllegalStateException: CFS has pending open files
    [junit] 	at org.apache.lucene.store.CompoundFileWriter.close(CompoundFileWriter.java:162)
    [junit] 	at org.apache.lucene.store.CompoundFileDirectory.close(CompoundFileDirectory.java:206)
    [junit] 	at org.apache.lucene.index.IndexWriter.createCompoundFile(IndexWriter.java:4099)
    [junit] 	at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3661)
    [junit] 	at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3260)
    [junit] 	at org.apache.lucene.index.SerialMergeScheduler.merge(SerialMergeScheduler.java:37)
    [junit] 	at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1902)
    [junit] 	at org.apache.lucene.index.IndexWriter.forceMerge(IndexWriter.java:1716)
    [junit] 	at org.apache.lucene.index.IndexWriter.forceMerge(IndexWriter.java:1670)
    [junit] 	at org.apache.lucene.index.TestIndexWriterOnDiskFull.testAddIndexOnDiskFull(TestIndexWriterOnDiskFull.java:304)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$3$1.evaluate(LuceneTestCase.java:529)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:165)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)
    [junit] 
    [junit] 
    [junit] Tests run: 4, Failures: 0, Errors: 1, Time elapsed: 31.96 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterOnDiskFull -Dtestmethod=testAddIndexOnDiskFull -Dtests.seed=-7dd066d256827211:127c018cbf5b0975:20481cd18a7d8b6e -Dtests.multiplier=3 -Dtests.nightly=true -Dargs=""-Dfile.encoding=ISO8859-1""
    [junit] NOTE: test params are: codec=SimpleText, sim=RandomSimilarityProvider(queryNorm=true,coord=false): {field=DFR GB1, id=DFR I(F)L1, content=IB SPL-D3(800.0), f=DFR G2}, locale=de_AT, timezone=America/Cambridge_Bay
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestAssertions, TestSearchForDuplicates, TestMockAnalyzer, TestDocValues, TestPerFieldPostingsFormat, TestDocument, TestAddIndexes, TestConcurrentMergeScheduler, TestCrashCausesCorruptIndex, TestDocCount, TestDocumentsWriterDeleteQueue, TestFieldInfos, TestFilterIndexReader, TestFlex, TestIndexInput, TestIndexWriter, TestIndexWriterMergePolicy, TestIndexWriterMerging, TestIndexWriterNRTIsCurrent, TestIndexWriterOnDiskFull]
    [junit] NOTE: FreeBSD 8.2-RELEASE amd64/Sun Microsystems Inc. 1.6.0 (64-bit)/cpus=16,threads=1,free=39156976,total=180748288
{noformat}"
1,"Security issue - DigestScheme uses constant nonce count valueThe nonce count value in DigestScheme is static (set to 00000001) and never changes.  (also seen as comment in said file).

This means that it fails against servers that correctly detect man-in-the-middle or replay attacks, leading to additional 401 requests (every second time), or such servers must be configured to turn such checks off (which is either poor security or poor for performance).

I suggest that at minimum, this count is incremented for every call to DigestScheme#createDigest.  It should also be an instance variable instead of a static, as it really relates to the challenge (assuming cases where instances are cached for reuse).  AtomicInteger is a good choice for implementing this counter.

See RFC 2617 chapters 3.2.2 and 3.2.3"
1,"ISOLatin1AccentFilter discards position increments of filtered termsNot sure if this is a bug, but looks like one to me..."
1,"KeywordTokenizer does not set start/end offset of the Token it producesI think just adding these two lines in the next(Token) method is the right fix:

           reusableToken.setStartOffset(0);
           reusableToken.setEndOffset(upto);

I don't think this is a back compat issue because the start/end offset are now meaningless since they will inherit whatever the reusable token had previously been used for."
1,"JCR-SQL2 : no count when WHERE clause is providedwhenever you provide a where-clause to a sql2 select, jcr/jackrabbit does not provide the hit count.

E.g.:
   select * from [nt:unstructured]
   order by [jcr:score]
returns the hit count (query.execute().getRows().getSize()), 
whereas
  select * from [nt:unstructured]
  where entity = ""customer""
  order by [jcr:score]
doesn't.
"
1,"spi2dav: Accessing moved referenceble nodes results in PathNotFoundExceptionthe following code fragment causes a PathNotFoundException on an existing path
and there seems to be no way to recover the session from this incorrect state:

	// assuming an existing nt:file node at path /apps/foo/bar.txt
	Node n1 = session.getNode(""/apps/foo/bar.txt"");
	Node n2 = n1.getNode(""jcr:content"");
	n2.setProperty(""jcr:data"", new java.io.ByteArrayInputStream(((String)(""blahblah"")).getBytes()));
	n2.save();
	Workspace ws0 = session.getWorkspace();
	ws0.move(""/apps/foo"", ""/apps/foo1"");
	Node n3 = session.getNode(""/apps/foo1/bar.txt"");
	Node n4 = n3.getNode(""jcr:content"");
	n4.refresh(false);
	Node n5 = n3.getNode(""jcr:content"");     // => PathNotFoundException

Please note that the preceeding Node.refresh() call seems to cause the inconsistency.
the problem doesn't occur when omitting this call."
1,"XPath QueryFormat may produce malformed XPath statementWhen the query tree contains select properties *and* an order by clause, then the XPath QueryFormat will produce a malformed XPath statement.

E.g.:

//element(*, foo)/(@a|@b) order by @bar

round trips to:

//element(*, foo) order by @bar/(@a|@b)

"
1,"Setting Query.setOffset() passed the results total returns negative getSize() instead of zero1. Have a query that returns 3 results
2. Now set Query.setOffset(10) (passed the total of 3)
3. Row/NodeIterator.getSize() returns -7 (incorrect)

Expected: getSize() should return 0"
1,"Deprecated Serializer does not properly delegate method calls.The deprecated org.apache.jackrabbit.core.state.util.Serializer class does not actually forward method calls to its replacement. Instead it calls itself repeatedly, leading to infinite recursion. The attached test demonstrates this and yields the following trace:

<<
java.lang.StackOverflowError
	at org.apache.jackrabbit.core.state.util.Serializer.serialize(Serializer.java:39)
>>"
1,"The ""jackrabbit-pool-"" thread prevents the process from stoppingIf the repository is not closed, and a session is still logged in, then the process doesn't terminate because of a non-daemon thread named ""jackrabbit-pool-<n>"". Test case:

public class TestThreadPreventsExit {
    public static void main(String... a) throws Exception {
        new TransientRepository().login(
                new SimpleCredentials("""", new char[0]));
    }
}

This program doesn't stop.

The non-daemon thread was introduces as part of https://issues.apache.org/jira/browse/JCR-2465

The fix is to use a daemon thread."
1,"IllegalStateException: Authentication state already initializedHi,

I am running HttpClient 3.0 RC2 in my application and a user send me a logfile
telling ""IllegalStateException: Authentication state already initialized"". 

He wanted to access a site on SUN.com and is behind a proxy. The site seems to
redirect to a different domain.

I have attached a Debug+Trace HttpClient log.

Ben"
1,"The getOutputStream of the MemoryFileSystem class can replace a folder with a newly created fileIt seems that if the filePath parameter passed to the getOutputStream method of the MemoryFileSystem class points to an  existing folder and not to a file - the folder will be replaced with a newly created file.
The function should probably check whether the passed path points to a file and throw an exception if it points to a folder."
1,"PropertyValue constraint fails with implicit selectorName using JCR-SQL2Compiling a JCR-SQL2 query involving a PropertyValue constraint using a qualified property name fails if selectorName is not explicitly defined.

The following query works:

SELECT * FROM [my:thing] AS thing WHERE thing.[my:property] = 'abc'

the following doesn't:

SELECT * FROM [my:thing] AS thing WHERE [my:property] = 'abc'

(the ""AS thing"" is unecessary here, I can leave it out with the same result).

The second query results in an:
javax.jcr.query.InvalidQueryException: Query:
SELECT * FROM [my:thing] AS thing WHERE [(*)my:property] = 'abc';
expected: NOT, (

The spec final draft however states:

PropertyValue ::= [selectorName'.'] propertyName
   /* If only one selector exists in this query,
      explicit specification of the selectorName is
      optional */"
1,"NullPointerException when using HttpHead and Request/Response interceptorsWhen you try to execute a HttpHead object instead of a HttpGet object while using the add request/response interceptors, you get a nullpointerexception.

I can replicate the exception when using the ClientGZipContentCompression example that can be found at the HttpClient examples. But instead of using the HttpGet object I execute a HttpHead object. When I comment the interceptor parts out, I don't get the exception. 

This is the error stack trace I get when executing the code in netbeans:

Exception in thread ""main"" java.lang.NullPointerException
	at testhttphead.ClientGZipContentCompression$2.process(ClientGZipContentCompression.java:74)
	at org.apache.http.protocol.ImmutableHttpProcessor.process(ImmutableHttpProcessor.java:116)
	at org.apache.http.protocol.HttpRequestExecutor.postProcess(HttpRequestExecutor.java:342)
	at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:472)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:820)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:754)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:732)
	at testhttphead.ClientGZipContentCompression.main(ClientGZipContentCompression.java:92)
Java Result: 1

Here is the code that gives me the error:

package testhttphead;

import java.io.IOException;
import java.io.InputStream;
import java.util.zip.GZIPInputStream;
import org.apache.http.*;
import org.apache.http.client.methods.HttpHead;
import org.apache.http.entity.HttpEntityWrapper;
import org.apache.http.impl.client.DefaultHttpClient;
import org.apache.http.protocol.HttpContext;
import org.apache.http.util.EntityUtils;

/**
 * Demonstration of the use of protocol interceptors to transparently modify
 * properties of HTTP messages sent / received by the HTTP client.
 * <p/>
 * In this particular case HTTP client is made capable of transparent content
 * GZIP compression by adding two protocol interceptors: a request interceptor
 * that adds 'Accept-Encoding: gzip' header to all outgoing requests and a
 * response interceptor that automatically expands compressed response entities
 * by wrapping them with a uncompressing decorator class. The use of protocol
 * interceptors makes content compression completely transparent to the consumer
 * of the {@link org.apache.http.client.HttpClient HttpClient} interface.
 */
public class ClientGZipContentCompression {

    public final static void main(String[] args) throws Exception {
        DefaultHttpClient httpclient = new DefaultHttpClient();

        try {
            httpclient.addRequestInterceptor(new HttpRequestInterceptor() {

                public void process(
                        final HttpRequest request,
                        final HttpContext context) throws HttpException, IOException {
                    if (!request.containsHeader(""Accept-Encoding"")) {
                        request.addHeader(""Accept-Encoding"", ""gzip"");
                    }
                }
            });

            httpclient.addResponseInterceptor(new HttpResponseInterceptor() {

                public void process(
                        final HttpResponse response,
                        final HttpContext context) throws HttpException, IOException {
                    HttpEntity entity = response.getEntity();
                    Header ceheader = entity.getContentEncoding();
                    if (ceheader != null) {
                        HeaderElement[] codecs = ceheader.getElements();
                        for (int i = 0; i < codecs.length; i++) {
                            if (codecs[i].getName().equalsIgnoreCase(""gzip"")) {
                                response.setEntity(
                                        new GzipDecompressingEntity(response.getEntity()));
                                return;
                            }
                        }
                    }
                }
            });

            HttpHead httpHead = new HttpHead(""http://www.howest.be"");

            // Execute HTTP request
            System.out.println(""executing request "" + httpHead.getURI());
            HttpResponse response = httpclient.execute(httpHead);

            System.out.println(""----------------------------------------"");
            System.out.println(response.getStatusLine());
            System.out.println(response.getLastHeader(""Content-Encoding""));
            System.out.println(response.getLastHeader(""Content-Length""));
            System.out.println(""----------------------------------------"");

            HttpEntity entity = response.getEntity();

            if (entity != null) {
                String content = EntityUtils.toString(entity);
                System.out.println(content);
                System.out.println(""----------------------------------------"");
                System.out.println(""Uncompressed size: "" + content.length());
            }

        } finally {
            // When HttpClient instance is no longer needed,
            // shut down the connection manager to ensure
            // immediate deallocation of all system resources
            httpclient.getConnectionManager().shutdown();
        }
    }

    static class GzipDecompressingEntity extends HttpEntityWrapper {

        public GzipDecompressingEntity(final HttpEntity entity) {
            super(entity);
        }

        @Override
        public InputStream getContent()
                throws IOException, IllegalStateException {

            // the wrapped entity's getContent() decides about repeatability
            InputStream wrappedin = wrappedEntity.getContent();

            return new GZIPInputStream(wrappedin);
        }

        @Override
        public long getContentLength() {
            // length of ungzipped content is not known
            return -1;
        }
    }
}

With kind regards,

Peter"
1,Incorrect excerpt for index aggregatesIncorrect excerpts may be created when the relevant node has an index aggregate configured and the nodes have properties configured for the node scope index with some of them excluded for use in excerpts.
1,"Two or more writers over NFS can cause index corruptionWhen an index is used over NFS, and, more than one machine can be a
writer such that they swap roles quickly, it's possible for the index
to become corrupt if the NFS client directory cache is stale.

Not all NFS clients will show this.  Very recent versions of Linux's
NFS client do not seem to show the issue, yet, slightly older ones do,
and the latest Mac OS X one does as well.

I've been working with Patrick Kimber, who provided a standalone test
showing the problem (thank you Patrick!).  This came out of this
thread:

  http://www.gossamer-threads.com/lists/engine?do=post_view_flat;post=50680;page=1;sb=post_latest_reply;so=ASC;mh=25;list=lucene

Note that the first issue in that discussion has been resolved
(LUCENE-948).  This is a new issue.
"
1,"OutOfMemory problem: HandleMonitor does not release closed input streamsThe class o.a.j.core.fs.local.HandleMonitor does not release closed MonitoredInputStream. There is a close method, but it is never called. The input streams are kept in a hash set / map of HandleMonitor. Eventually, this leads to an OutOfMemory exception after opening / closing many files."
1,"Range queries fail on large repositoriesAs discussed on the user mailing list, queries on large repositories with date constraints like ""field > constant"" treat the constraint as always true, returning results that should not be returned."
1,"NPE in ObservationManagerImpl.getRegisteredEventListeners() during shutdown after broken startupSee JCR-2378. The variable ""dispatcher"" is passed as null in the constructor."
1,"Missing sync in IndexWriter.addIndexes(IndexReader[])The 3.x build just hit this:

{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestAddIndexes
    [junit] Testcase: testAddIndexesWithThreads(org.apache.lucene.index.TestAddIndexes):	FAILED
    [junit] expected:<3160> but was:<2701>
    [junit] junit.framework.AssertionFailedError: expected:<3160> but was:<2701>
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:779)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:745)
    [junit] 	at org.apache.lucene.index.TestAddIndexes.testAddIndexesWithThreads(TestAddIndexes.java:708)
    [junit] 
    [junit] 
    [junit] Tests run: 15, Failures: 1, Errors: 0, Time elapsed: 9.28 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] java.lang.AssertionError: RefCount is 0 pre-decrement for file ""_8a.tvf""
    [junit] 	at org.apache.lucene.index.IndexFileDeleter$RefCount.DecRef(IndexFileDeleter.java:608)
    [junit] 	at org.apache.lucene.index.IndexFileDeleter.decRef(IndexFileDeleter.java:505)
    [junit] 	at org.apache.lucene.index.IndexFileDeleter.decRef(IndexFileDeleter.java:496)
    [junit] 	at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:2972)
    [junit] 	at org.apache.lucene.index.TestAddIndexes$CommitAndAddIndexes.doBody(TestAddIndexes.java:681)
    [junit] 	at org.apache.lucene.index.TestAddIndexes$RunAddIndexesThreads$1.run(TestAddIndexes.java:624)
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestAddIndexes -Dtestmethod=testAddIndexesWithThreads -Dtests.seed=-6912763261803132408:-5575674032550262483 -Dtests.multiplier=3
    [junit] NOTE: test params are: locale=en_AU, timezone=America/Atka
{noformat}

It looks like it's caused by a long-standing missing sync (since at least 2.9.x).  I think likely we hit it just now thanks to adding random Thread.yield()'s in MockDirWrapper!"
1,"Hop 0 sample app doesn't exit because of on-daemon thread pool-1-thread-1When starting the sample app Hop 0 (or any other Hop sample app) if there is no ""repository"" directory, then the application doesn't exit because there is a non-daemon thread named ""pool-1-thread-1""."
1,FALSE predicate always returns trueorg.apache.jackrabbit.spi.commons.iterator.Predicates..FALSE always returns true instead of false.
1,"PulsingTermState.clone leaks memoryI looked at the heap dump from the OOME this morning (thank you Uwe
for turning this on!), and I think it's a real memory leak.

Well, not really a leak; rather, the cloned PulsingTermState, which we
cache in the terms dict cache, is hanging onto large byte[]
unnecessarily.
"
1,"Deadlock  on concurrent read & transactional write operationsIsuue has been introduced by resolving JCR-1755 (Transaction-safe versioning). This fixed changed sequence of commits, but at the same time order of acquiring locks has been disturbed.


"
1,"IOUtils - getCreated(...) - SimpleDateFormat is not threadsafeSimpleDateFormat is not threadsafe (http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4146524)
See exception in attachment.
IMHO it will be enough to synchronize 'format' method in HttpDateFormat class."
1,"SnowballFilter loses token position offsetSnowballFilter doesn't set the token position increment (and thus it defaults to 1).
This also affetcs SnowballAnalyzer since it uses SnowballFilter."
1,"Cookie.java hashCode method violates contractorg.apache.commons.httpclient.Cookie hashCode() does not meet object.hashCode
() contract.  Cookie.hashCode() returns different values even though data used 
in equals() comparison is the same.

Contract:**Whenever it is invoked on the same object more than once during an 
execution of a Java application, the hashCode method must consistently return 
the same integer, provided no information used in equals comparisons on the 
object is modified.**

Breaks use of cookie within collections such as when using contains().

Traced problem back to parent class NameValuePair.  Cookie.hashCode() calls 
NameValuePair.hashCode() which relies on name/value hashes.  Cookie does not 
rely on value to determine equality."
1,"Concurrent add/remove child node operations in a cluster may corrupt repository.Concurrent add/remove child node operations in a cluster may store an inconsistent list of child node entries, i.e. an entry in the list may appear that has no associated node. This eventually results in an ItemNotFoundException, the next time one of these bogus entries is accessed."
1,"CachingWrapperFilter throws NPE when Filter.getDocIdSet() returns nullFollowup for [http://www.lucidimagination.com/search/document/1014ea92f15677bd/filter_getdocidset_returning_null_and_what_this_means_for_cachingwrapperfilter]:

Daniel Noll is seeing an exception like this:

{noformat}
java.lang.NullPointerException
    at org.apache.lucene.search.CachingWrapperFilter.docIdSetToCache(CachingWrapperFilter.java:84)
    at org.apache.lucene.search.CachingWrapperFilter.getDocIdSet(CachingWrapperFilter.java:112)
    at com.nuix.storage.search.LazyConstantScoreQuery$LazyFilterWrapper.getDocIdSet(SourceFile:91)
    at org.apache.lucene.search.ConstantScoreQuery$ConstantScorer.<init>(ConstantScoreQuery.java:116)
    at org.apache.lucene.search.ConstantScoreQuery$ConstantWeight.scorer(ConstantScoreQuery.java:81)
    at org.apache.lucene.search.BooleanQuery$BooleanWeight.scorer(BooleanQuery.java:297)
    at org.apache.lucene.search.BooleanQuery$BooleanWeight.scorer(BooleanQuery.java:297)
    at org.apache.lucene.search.QueryWrapperFilter$2.iterator(QueryWrapperFilter.java:75)
{noformat}

The class of our own is just an intermediary which delays creating the Filter object...

{code}
@Override
public DocIdSet getDocIdSet(IndexReader reader) throws IOException {
            if (delegate == null) {
                delegate = factory.createFilter();
            }
            return delegate.getDocIdSet(reader);
}
{code}

Tracing through the code in CachingWrapperFilter, I can see that this NPE would occur if getDocIdSet() were to return null.

The Javadoc on Filter says that null will be returned if no documents will be accepted by the filter, but it doesn't seem that Lucene itself is handling null return values correctly, so which is correct?  The code or the Javadoc?  Supposing that null really is OK, does this cause any problems with how CachingWrapperFilter is implementing the caching?  I notice it's calling get() and then comparing against null so it wouldn't appear that it can distinguish ""the entry isn't in the cache"" from ""the entry is in the cache but it's null""."
1,"TestAddIndexes#testAddIndexesWithThreads fails on RealtimeSelckin reported two failures on LUCENE-3023 which I can unfortunately not reproduce at all. here are the traces

{noformat}
  [junit] Testsuite: org.apache.lucene.index.TestAddIndexes
    [junit] Testcase: testAddIndexesWithThreads(org.apache.lucene.index.TestAddIndexes):	FAILED
    [junit] expected:<3160> but was:<3060>
    [junit] junit.framework.AssertionFailedError: expected:<3160> but was:<3060>
    [junit] 	at org.apache.lucene.index.TestAddIndexes.testAddIndexesWithThreads(TestAddIndexes.java:783)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1226)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1154)
    [junit] 
    [junit] 
    [junit] Tests run: 18, Failures: 1, Errors: 0, Time elapsed: 14.272 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestAddIndexes -Dtestmethod=testAddIndexesWithThreads -Dtests.seed=6128854208955988865:2552774338676281184
    [junit] NOTE: test params are: codec=PreFlex, locale=no_NO_NY, timezone=America/Edmonton
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestToken, TestDateTools, Test2BTerms, TestAddIndexes]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=8,threads=1,free=84731792,total=258080768
    [junit] ------------- ---------------- ---------------
{noformat}
and 
{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestAddIndexes
    [junit] Testcase: testAddIndexesWithThreads(org.apache.lucene.index.TestAddIndexes):	FAILED
    [junit] expected:<3160> but was:<3060>
    [junit] junit.framework.AssertionFailedError: expected:<3160> but was:<3060>
    [junit] 	at org.apache.lucene.index.TestAddIndexes.testAddIndexesWithThreads(TestAddIndexes.java:783)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1226)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1154)
    [junit] 
    [junit] 
    [junit] Tests run: 18, Failures: 1, Errors: 0, Time elapsed: 14.841 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestAddIndexes -Dtestmethod=testAddIndexesWithThreads -Dtests.seed=4502815121171887759:-6764285049309266272
    [junit] NOTE: test params are: codec=PreFlex, locale=tr_TR, timezone=Mexico/BajaNorte
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestToken, TestDateTools, Test2BTerms, TestAddIndexes]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=8,threads=1,free=163663416,total=243335168
    [junit] ------------- ---------------- ---------------
{noformat}"
1,"[httpclient] Incorrect credentials loop infinitelyIf incorrect credentials are assigned to the request, HttpClient will loop 
forever.  It should only try once, and fail with an HttpException if a request 
with credentials set fails.

In org.apache.commons.httpclient.HttpMethodBase.execute(), a check is needed to 
track if credentials have been sent before."
1,"WeightedSpanTermExtractor incorrectly treats the same terms occurring in different query typesGiven a BooleanQuery with multiple clauses, if a term occurs both in a Span / Phrase query, and in a TermQuery, the results of term extraction are unpredictable and depend on the order of clauses. Concequently, the result of highlighting are incorrect.

Example text: t1 t2 t3 t4 t2
Example query: t2 t3 ""t1 t2""
Current highlighting: [t1 t2] [t3] t4 t2
Correct highlighting: [t1 t2] [t3] t4 [t2]

The problem comes from the fact that we keep a Map<termText, WeightedSpanTerm>, and if the same term occurs in a Phrase or Span query the resulting WeightedSpanTerm will have a positionSensitive=true, whereas terms added from TermQuery have positionSensitive=false. The end result for this particular term will depend on the order in which the clauses are processed.

My fix is to use a subclass of Map, which on put() always sets the result to the most lax setting, i.e. if we already have a term with positionSensitive=true, and we try to put() a term with positionSensitive=false, we set the result positionSensitive=false, as it will match both cases."
1,upgrade icu jar to 4.8.1.1 / remove lucenetestcase hackThis bug fix release fixes problems with icu under java7: http://bugs.icu-project.org/trac/ticket/8734
1,"When reopen returns a new IndexReader, both IndexReaders may now control the lifecycle of the underlying Directory which is managed by reference countingRough summary. Basically, FSDirectory tracks references to FSDirectory and when IndexReader.reopen shares a Directory with a created IndexReader and closeDirectory is true, FSDirectory's ref management will see two decrements for one increment. You can end up getting an AlreadyClosed exception on the Directory when the IndexReader is open.

I have a test I'll put up. A solution seems fairly straightforward (at least in what needs to be accomplished)."
1,"Wrong implementation of DocIdSetIterator.advance Implementations of {{DocIdSetIterator}} behave differently when advanced is called. Taking the following test for {{OpenBitSet}}, {{DocIdBitSet}} and {{SortedVIntList}} only {{SortedVIntList}} passes the test:
{code:title=org.apache.lucene.search.TestDocIdSet.java|borderStyle=solid}
...
	public void testAdvanceWithOpenBitSet() throws IOException {
		DocIdSet idSet = new OpenBitSet( new long[] { 1121 }, 1 );  // bits 0, 5, 6, 10
		assertAdvance( idSet );
	}

	public void testAdvanceDocIdBitSet() throws IOException {
		BitSet bitSet = new BitSet();
		bitSet.set( 0 );
		bitSet.set( 5 );
		bitSet.set( 6 );
		bitSet.set( 10 );
		DocIdSet idSet = new DocIdBitSet(bitSet);
		assertAdvance( idSet );
	}

	public void testAdvanceWithSortedVIntList() throws IOException {
		DocIdSet idSet = new SortedVIntList( 0, 5, 6, 10 );
		assertAdvance( idSet );
	}	

	private void assertAdvance(DocIdSet idSet) throws IOException {
		DocIdSetIterator iter = idSet.iterator();
		int docId = iter.nextDoc();
		assertEquals( ""First doc id should be 0"", 0, docId );

		docId = iter.nextDoc();
		assertEquals( ""Second doc id should be 5"", 5, docId );

		docId = iter.advance( 5 );
		assertEquals( ""Advancing iterator should return the next doc id"", 6, docId );
	}
{code}

The javadoc for {{advance}} says:
{quote}
Advances to the first *beyond* the current whose document number is greater than or equal to _target_.
{quote}
This seems to indicate that {{SortedVIntList}} behaves correctly, whereas the other two don't. 
Just looking at the {{DocIdBitSet}} implementation advance is implemented as:
{code}
bitSet.nextSetBit(target);
{code}
where the docs of {{nextSetBit}} say:
{quote}
Returns the index of the first bit that is set to true that occurs *on or after* the specified starting index
{quote}
"
1,toString() causes StackOverflowErrorfurther regressions of JCR-2763...
1,"Indexing configuration not refreshed after node type registrationThe indexing configuration has internal caches that speed up node type matches. Those caches are not updated on new node type registration and newly registered node types are not properly resolved when index-rules are checked.

See also test case in attached patch."
1,"Invalid behavior of StandardTokenizerImplThe following code prints the output of StandardAnalyzer:

        Analyzer analyzer = new StandardAnalyzer();
        TokenStream ts = analyzer.tokenStream(""content"", new StringReader(""<some text>""));
        Token t;
        while ((t = ts.next()) != null) {
            System.out.println(t);
        }

If you pass ""www.abc.com"", the output is (www.abc.com,0,11,type=<HOST>) (which is correct in my opinion).
However, if you pass ""www.abc.com."" (notice the extra '.' at the end), the output is (wwwabccom,0,12,type=<ACRONYM>).

I think the behavior in the second case is incorrect for several reasons:
1. It recognizes the string incorrectly (no argue on that).
2. It kind of prevents you from putting URLs at the end of a sentence, which is perfectly legal.
3. An ACRONYM, at least to the best of my understanding, is of the form A.B.C. and not ABC.DEF.

I looked at StandardTokenizerImpl.jflex and I think the problem comes from this definition:
// acronyms: U.S.A., I.B.M., etc.
// use a post-filter to remove dots
ACRONYM    =  {ALPHA} ""."" ({ALPHA} ""."")+

Notice how the comment relates to acronym as U.S.A., I.B.M. and not something else. I changed the definition to
ACRONYM    =  {LETTER} ""."" ({LETTER} ""."")+
and it solved the problem.

This was also reported here:
http://www.nabble.com/Inconsistent-StandardTokenizer-behaviour-tf596059.html#a1593383
http://www.nabble.com/Standard-Analyzer---Host-and-Acronym-tf3620533.html#a10109926
"
1,"Static variables need to be final (or access should be synchronised):Static variables need to be final (or access should be synchronised):

Index: module-client/src/main/java/org/apache/http/conn/params/HttpConnectionManagerParams.java
===================================================================
--- module-client/src/main/java/org/apache/http/conn/params/HttpConnectionManagerParams.java	(revision 652021)
+++ module-client/src/main/java/org/apache/http/conn/params/HttpConnectionManagerParams.java	(working copy)
@@ -53,7 +53,7 @@
     public static final int DEFAULT_MAX_TOTAL_CONNECTIONS = 20;
 
     /** The default maximum number of connections allowed per host */
-    private static ConnPerRoute DEFAULT_CONN_PER_ROUTE = new ConnPerRoute() {
+    private static final ConnPerRoute DEFAULT_CONN_PER_ROUTE = new ConnPerRoute() {
         
         public int getMaxForRoute(HttpRoute route) {
             return ConnPerRouteBean.DEFAULT_MAX_CONNECTIONS_PER_ROUTE;
"
1,"IndexReader overwrites future commits when you open it on a past commitHit this on trying to build up a test index for perf testing...

IndexReader (and Writer) accept an IndexCommit on open.

This is quite powerful, because, if you use a deletion policy that keeps multiple commits around, you can open a not-current commit, make some changes, write a new commit, all without altering the ""future"" commits.

I use this to first build up a big wikipedia index, including one commit w/ multiple segments, then another commit after optimize(), and then I open an writable IR to perform deletions off of both those commits.  This gives me a single test index that has all four combinations (single vs multi segment; deletions vs no deletions).

But IndexReader has a bug whereby it overwrites the segments_N file.  (IndexWriter works correctly)."
1,Background text extraction not possible when supportHighlighting is set trueThere is an IndexingQueue that holds nodes that are indexed with a background thread when text extraction takes more time than a configurable limit. When supportHighlighting is set to true the IndexingQueue is never used because the text extract is immediately requested in NodeIndexer. Instead the text extract should be retrieved only when the node is added to the index. 
1,"Typo in the DeltaVConstants class in constant XML_CHECKOUT_CHECKIN valueJust spotted a typo in the http://svn.apache.org/repos/asf/jackrabbit/trunk/jackrabbit-webdav/src/main/java/org/apache/jackrabbit/webdav/version/DeltaVConstants.java
(same is in released 1.4 version)

There's line
    public static final String XML_CHECKOUT_CHECKIN = ""checkin-checkout"";
Probably should be
    public static final String XML_CHECKOUT_CHECKIN = ""checkout-checkin"";
"
1,"Repository lock file is not removed on shutdownThe repository lock file is not removed when Jackrabbit runs on a windows platform:

*ERROR* [Thread-4] RepositoryImpl: Unable to release repository lock (RepositoryImpl.java, line 283)

I assume this problem does not occur on unix based platforms, because they allow to delete a file while another process still uses it."
1,"Duplicate key in DatabasePersistenceManagerHi,

I ran into the exception pasted below. We had 2 threads that both were saving. Maybe it is a race condition?  

Regards,

Martijn Hendriks
<GX> creative online development B.V.
 
t: 024 - 3888 261
f: 024 - 3888 621
e: martijnh@gx.nl
 
Wijchenseweg 111
6538 SW Nijmegen
http://www.gx.nl/ 


Jan 26, 2007 2:23:36 PM org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager store
SEVERE: failed to write property state: e3847bad-f1ee-4adb-a109-e134900935b7/{http://gx.nl}edit_language
ERROR 23505: The statement was aborted because it would have caused a duplicate key value in a unique or primary key constraint or unique in dex identified by 'DEFAULT_PROP_IDX' defined on 'DEFAULT_PROP'.
        at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
        at org.apache.derby.impl.sql.execute.IndexChanger.insertAndCheckDups(Unknown Source)
        at org.apache.derby.impl.sql.execute.IndexChanger.doInsert(Unknown Source)
        at org.apache.derby.impl.sql.execute.IndexChanger.insert(Unknown Source)
        at org.apache.derby.impl.sql.execute.IndexSetChanger.insert(Unknown Source)
        at org.apache.derby.impl.sql.execute.RowChangerImpl.insertRow(Unknown Source)
        at org.apache.derby.impl.sql.execute.InsertResultSet.normalInsertCore(Unknown Source)
        at org.apache.derby.impl.sql.execute.InsertResultSet.open(Unknown Source)
        at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
        at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
        at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeStatement(Unknown Source)
        at org.apache.derby.impl.jdbc.EmbedPreparedStatement.execute(Unknown Source)
        at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.executeStmt(DatabasePersistenceManager.java:835)
        at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.store(DatabasePersistenceManager.java:466)
        at org.apache.jackrabbit.core.persistence.AbstractPersistenceManager.store(AbstractPersistenceManager.java:75)
        at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.store(DatabasePersistenceManager.java:274)
        at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:675)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:808)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:326)
        at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:313)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:302)
        at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:295)
        at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1210)
"
1,"Unable to create repository using jackrabbit-webapp because a directory called ""jackrabbit"" already existsI mount the jackrabbit-webapp.war in a Jetty installation
* at startup i have the following exception:
ERROR RepositoryStartupServlet: Either create thejackrabbit/bootstrap.properties file or
ERROR RepositoryStartupServlet: use the '/config/index.jsp' for easy configuration.
ERROR RepositoryStartupServlet: RepositoryStartupServlet initializing failed: javax.servlet.ServletException: Repository startup configuration is not valid.
* then when i access http://localhost:8080/ i am forwarded to the page:
 http://localhost:8080/bootstrap/missing.jsp
* creating the repository by clicking on ""Create Content Repository"" button fails complaining that the jackrabbit directory already exists

Indeed, i find a jackrabbit directory in my JETTY_HOME (from where is started Jetty).

A workaround is to delete this ""jackrabbit"" directory and then i can create the repository by clicking on the previous button and therefore access the newly created repository."
1,"[PATCH] Error in GermanStemmer.java,v 1.11GermanStemmer.java,v 1.11 in  lucene-1.4-final
 at the end of a word is not replaced by ss"
1,"SpellChecker file descriptor leak - no way to close the IndexSearcher used by SpellChecker internallyI can't find any way to close the IndexSearcher (and IndexReader) that
is being used by SpellChecker internally.

I've worked around this issue by keeping a single SpellChecker open
for each index, but I'd really like to be able to close it and
reopen it on demand without leaking file descriptors.

Could we add a close() method to SpellChecker that will close the
IndexSearcher and null the reference to it? And perhaps add some code
that reopens the searcher if the reference to it is null? Or would
that break thread safety of SpellChecker?

The attached patch adds a close method but leaves it to the user to
call setSpellIndex to reopen the searcher if desired."
1,"Intermittent thread safety issue with EnwikiDocMakerIntermittent thread safety issue with EnwikiDocMaker

When I run the conf/wikipediaOneRound.alg, sometimes it gets started
OK, other times (about 1/3rd the time) I see this:

     Exception in thread ""Thread-0"" java.lang.RuntimeException: java.io.IOException: Bad file descriptor
     	at org.apache.lucene.benchmark.byTask.feeds.EnwikiDocMaker$Parser.run(EnwikiDocMaker.java:76)
     	at java.lang.Thread.run(Thread.java:595)
     Caused by: java.io.IOException: Bad file descriptor
     	at java.io.FileInputStream.readBytes(Native Method)
     	at java.io.FileInputStream.read(FileInputStream.java:194)
     	at org.apache.xerces.impl.XMLEntityManager$RewindableInputStream.read(Unknown Source)
     	at org.apache.xerces.impl.io.UTF8Reader.read(Unknown Source)
     	at org.apache.xerces.impl.XMLEntityScanner.load(Unknown Source)
     	at org.apache.xerces.impl.XMLEntityScanner.scanQName(Unknown Source)
     	at org.apache.xerces.impl.XMLNSDocumentScannerImpl.scanStartElement(Unknown Source)
     	at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl$FragmentContentDispatcher.dispatch(Unknown Source)
     	at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanDocument(Unknown Source)
     	at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
     	at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
     	at org.apache.xerces.parsers.XMLParser.parse(Unknown Source)
     	at org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source)
     	at org.apache.lucene.benchmark.byTask.feeds.EnwikiDocMaker$Parser.run(EnwikiDocMaker.java:60)
     	... 1 more

The problem is that the thread that pulls the XML docs is started as
soon as EnwikiDocMaker class is instantiated.  When it's started, it
uses the fileIS (FileInputStream) to feed the XML Parser.  But,
openFile is actually called twice on starting the alg, if you use any
task deriving from ResetInputsTask, which closes the original fileIS
that the XML parser may be using.

I changed the thread to instead start on-demand the first time next()
is called.  I also removed a redundant resetInputs() call (which was
opening the file more frequently than needed).  Finally, I added logic
in the thread to detect that the input stream was closed (because
LineDocMaker.resetInputs() was called, eg, if we are not running the
doc maker to exhaustion).

"
1,"CheckIndex incorrectly sees deletes as index corruptionThere is a silly bug in CheckIndex whereby any segment with deletes is
considered corrupt.

Thanks to Bogdan Ghidireac for reporting this."
1,"TestFSTs.testRandomWords throws AIOBE when ""verbose""=trueSeems like invalid utf-8 sometimes gets passed to Bytesref.utf8ToString() in the verbose ""println""s."
1,ArrayIndexOutOfBoundsException during indexinghttp://search.lucidimagination.com/search/document/f29fc52348ab9b63/arrayindexoutofboundsexception_during_indexing
1,"Persistence data of versioning not cleaned up correctlywhen deleting a version or removing its label, the respective persistence data is not always properly removed."
1,"ClassCastException when updating properties using WebDAVWhen issuing PROPPATCH commands, a ClassCastException is raised.

e.g. 

PROPPATCH /jackrabbit-webapp-1.4/repository/default/test/test_file_v.txt HTTP/1.1
Host: localhost:9000
Connection: TE
TE: trailers, deflate, gzip, compress
User-Agent: UCI DAV Explorer/0.91 RPT-HTTPClient/0.3-3E
Translate: f
Authorization: Basic Y3Jvc3NqYTp0ZXN0
Accept-Encoding: deflate, gzip, x-gzip, compress, x-compress
Content-type: text/xml
Content-length: 170

<A:propertyupdate xmlns:A=""DAV:"">
<A:set>
<A:prop>
<A:auto-version>checkout-checkin</A:auto-version>
</A:prop>
</A:set>
</A:propertyupdate>


results in



24.01.2008 15:38:34 *ERROR* [Webdav]: Servlet.service() for servlet Webdav threw
 exception (StandardWrapperValve.java, line 257)
java.lang.ClassCastException: org.apache.jackrabbit.webdav.property.DefaultDavPr
operty
        at org.apache.jackrabbit.webdav.simple.DavResourceImpl.alterProperties(D
avResourceImpl.java:456)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.doPropPatch
(AbstractWebdavServlet.java:457)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.execute(Abs
tractWebdavServlet.java:234)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.service(Abs
tractWebdavServlet.java:192)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:803)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(Appl
icationFilterChain.java:269)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationF
ilterChain.java:188)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperV
alve.java:210)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextV
alve.java:174)
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.j
ava:127)
        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.j
ava:117)
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineVal
ve.java:108)
        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.jav
a:151)
        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java
:870)
        at org.apache.coyote.http11.Http11BaseProtocol$Http11ConnectionHandler.p
rocessConnection(Http11BaseProtocol.java:665)
        at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(PoolTcpEndpo
int.java:528)
        at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt(LeaderFol
lowerWorkerThread.java:81)
        at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadP
ool.java:685)
        at java.lang.Thread.run(Thread.java:595)
24.01.2008 15:38:34 *ERROR* [Webdav]: Servlet.service() for servlet Webdav threw
 exception (SLF4JLocationAwareLog.java, line 174)
java.lang.ClassCastException: org.apache.jackrabbit.webdav.property.DefaultDavPr
operty
        at org.apache.jackrabbit.webdav.simple.DavResourceImpl.alterProperties(D
avResourceImpl.java:456)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.doPropPatch
(AbstractWebdavServlet.java:457)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.execute(Abs
tractWebdavServlet.java:234)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.service(Abs
tractWebdavServlet.java:192)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:803)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(Appl
icationFilterChain.java:269)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationF
ilterChain.java:188)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperV
alve.java:210)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextV
alve.java:174)
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.j
ava:127)
        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.j
ava:117)
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineVal
ve.java:108)
        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.jav
a:151)
        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java
:870)
        at org.apache.coyote.http11.Http11BaseProtocol$Http11ConnectionHandler.p
rocessConnection(Http11BaseProtocol.java:665)
        at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(PoolTcpEndpo
int.java:528)
        at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt(LeaderFol
lowerWorkerThread.java:81)
        at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadP
ool.java:685)
        at java.lang.Thread.run(Thread.java:595)
24.01.2008 15:53:54 *ERROR* [Webdav]: Servlet.service() for servlet Webdav threw
 exception (StandardWrapperValve.java, line 257)
java.lang.ClassCastException: org.apache.jackrabbit.webdav.property.DefaultDavPr
operty
        at org.apache.jackrabbit.webdav.simple.DavResourceImpl.alterProperties(D
avResourceImpl.java:456)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.doPropPatch
(AbstractWebdavServlet.java:457)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.execute(Abs
tractWebdavServlet.java:234)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.service(Abs
tractWebdavServlet.java:192)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:803)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(Appl
icationFilterChain.java:269)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationF
ilterChain.java:188)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperV
alve.java:210)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextV
alve.java:174)
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.j
ava:127)
        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.j
ava:117)
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineVal
ve.java:108)
        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.jav
a:151)
        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java
:870)
        at org.apache.coyote.http11.Http11BaseProtocol$Http11ConnectionHandler.p
rocessConnection(Http11BaseProtocol.java:665)
        at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(PoolTcpEndpo
int.java:528)
        at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt(LeaderFol
lowerWorkerThread.java:81)
        at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadP
ool.java:685)
        at java.lang.Thread.run(Thread.java:595)
24.01.2008 15:53:54 *ERROR* [Webdav]: Servlet.service() for servlet Webdav threw
 exception (SLF4JLocationAwareLog.java, line 174)
java.lang.ClassCastException: org.apache.jackrabbit.webdav.property.DefaultDavPr
operty
        at org.apache.jackrabbit.webdav.simple.DavResourceImpl.alterProperties(D
avResourceImpl.java:456)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.doPropPatch
(AbstractWebdavServlet.java:457)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.execute(Abs
tractWebdavServlet.java:234)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.service(Abs
tractWebdavServlet.java:192)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:803)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(Appl
icationFilterChain.java:269)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationF
ilterChain.java:188)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperV
alve.java:210)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextV
alve.java:174)
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.j
ava:127)
        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.j
ava:117)
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineVal
ve.java:108)
        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.jav
a:151)
        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java
:870)
        at org.apache.coyote.http11.Http11BaseProtocol$Http11ConnectionHandler.p
rocessConnection(Http11BaseProtocol.java:665)
        at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(PoolTcpEndpo
int.java:528)
        at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt(LeaderFol
lowerWorkerThread.java:81)
        at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadP
ool.java:685)
        at java.lang.Thread.run(Thread.java:595) "
1,webapp doesn't compile (use of enum keyword)AbstractConfig.java and JNDIConfig.java have local variables named 'enum' that aren't allowed when using JDK5 or later compilers.
1,"jcr2spi: Unprocessed ItemInfos call to RepositoryService#getItemInfosstefan reported the following problem:

- batchread config reads with depths infinity
- invalidate tree by calling Node.refresh(false)
- force loading of the tree (e.g. Node.getPath())

afterwards, there may still be invalidated item states indicating that not all ItemInfos were processed.
consequently, there are additional calls to getItemInfos that should have been covered by the loading of the tree.
the problem occuring is not related to limitation of the item-cache size.

problem analysis:

there is a bug in WorkspaceItemStateFactory#createItemStates.
there is a wrapper built around the ItemInfo-Iterator but later on the ItemInfo-Iterator is used instead of the wrapper, which pre-fetches items from the underlying iterator and process them upon hasNext()/next()."
1,"Creating and saving a mix:versionable node creates two VersionHistory nodesSteps:
   - Create a new mix:versionable node
      [ This creates a new VersionHistory node below jcr:persistentVersionStore
        and sets the new node's versionHistory property to the UUID of this
        VersionHistory node. ]
   - Save the session (or alternatively save the parent of the new node)
      [ This creates a new VersionHistory node below jcr:persistentVersionStore
        and sets the node's versionHistory property to the UUID of this
        VersionHistory node. ]

As you can see, you end up with two VersionHistory nodes for the same node, of which the first VersionHistory node is never used again, because the second VersionHistory node is used from now on."
1,"CJKTokenizer convert   HALFWIDTH_AND_FULLWIDTH_FORMS wrongCJKTokenizer have these lines..
                if (ub == Character.UnicodeBlock.HALFWIDTH_AND_FULLWIDTH_FORMS) {
                    /** convert  HALFWIDTH_AND_FULLWIDTH_FORMS to BASIC_LATIN */
                    int i = (int) c;
                    i = i - 65248;
                    c = (char) i;
                }

This is wrong. Some character in the block (e.g. U+ff68) have no BASIC_LATIN counterparts.
Only 65281-65374 can be converted this way.

The fix is

             if (ub == Character.UnicodeBlock.HALFWIDTH_AND_FULLWIDTH_FORMS && i <= 65474 && i> 65281) {
                    /** convert  HALFWIDTH_AND_FULLWIDTH_FORMS to BASIC_LATIN */
                    int i = (int) c;
                    i = i - 65248;
                    c = (char) i;
                }"
1,"UserManager: concurrent user creation using same intermediate path failsconcurrently creating users using same intermediate path fails with ""node ... has been modified externally"".

the problem is the intermediate path. if it doesn't exist multiple threads try to create it concurrently: 

o.a.jackrabbit.core.security.user.UserManagerImpl, line 1310ff:


            String[] segmts = defaultPath.split(""/"");
            NodeImpl folder = (NodeImpl) session.getRootNode();
            String authRoot = (isGroup) ? groupsPath : usersPath;

            for (String segment : segmts) {
                if (segment.length() < 1) {
                    continue;
                }
                if (folder.hasNode(segment)) {
                    folder = (NodeImpl) folder.getNode(segment);
                    if (Text.isDescendantOrEqual(authRoot, folder.getPath()) &&
                            !folder.isNodeType(NT_REP_AUTHORIZABLE_FOLDER)) {
                        throw new ConstraintViolationException(""Invalid intermediate path. Must be of type rep:AuthorizableFolder."");
                    }
                } else {
                    Node parent = folder;
                    folder = addNode(folder, session.getQName(segment), NT_REP_AUTHORIZABLE_FOLDER);
                }
            }

the attached test case illustrates this issue/"
1,"Workspace.clone throws ItemNotFoundException on a referenceable node with childrenAn ItemNotFoundException is thrown when a referenceable node with children is cloned, this happens after the first time the node is cloned.
            
Example:

            Node root = session.getRootNode();   
            Node parent = root.addNode(""parent"");
            parent.addMixin(""mix:referenceable"");
            session.save();
            
// clone parent
            WS2.clone(""default"", ""/parent"", ""/parent"", true);
            
            Node child = parent.addNode(""child"");
// add child
            child.addMixin(""mix:referenceable"");
            session.save();

// clone parent with child            
            WS2.clone(""default"", ""/parent"", ""/parent"", true); 

// clone parent again,   ItemNotFoundException - from now on can't clone parent node.
            WS2.clone(""default"", ""/parent"", ""/parent"", true);


Stacktrace:
javax.jcr.ItemNotFoundException: failed to build path of 229083e5-5f24-4102-b007-785f43be983a: cafebabe-cafe-babe-cafe-babecafebabe has no child entry for 229083e5-5f24-4102-b007-785f43be983a
	at org.apache.jackrabbit.core.HierarchyManagerImpl.buildPath(HierarchyManagerImpl.java:308)
	at org.apache.jackrabbit.core.CachingHierarchyManager.buildPath(CachingHierarchyManager.java:159)
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getPath(HierarchyManagerImpl.java:357)
	at org.apache.jackrabbit.core.CachingHierarchyManager.getPath(CachingHierarchyManager.java:221)
	at org.apache.jackrabbit.core.BatchedItemOperations.checkRemoveNode(BatchedItemOperations.java:700)
	at org.apache.jackrabbit.core.BatchedItemOperations.recursiveRemoveNodeState(BatchedItemOperations.java:1514)
	at org.apache.jackrabbit.core.BatchedItemOperations.removeNodeState(BatchedItemOperations.java:1216)
	at org.apache.jackrabbit.core.BatchedItemOperations.copyNodeState(BatchedItemOperations.java:1642)
	at org.apache.jackrabbit.core.BatchedItemOperations.copy(BatchedItemOperations.java:311)
	at org.apache.jackrabbit.core.WorkspaceImpl.internalCopy(WorkspaceImpl.java:294)
	at org.apache.jackrabbit.core.WorkspaceImpl.clone(WorkspaceImpl.java:401)
	at test.CloneTest.main(CloneTest.java:64)

            "
1,"Select * does not return declared properties of node type in FROM clauseThe query only returns the default columns: jcr:primaryType, jcr:score and jcr:path"
1,"NPE in OpenOfficeTextExtractorI try to load some Open Office Writer document (see attachment) and receive such exception. 

2008-06-10 17:19:59 <WARN > [btpool0-1] CompositeTextExtractor: Failed to extract text content(92)
java.lang.NullPointerException
    at org.apache.jackrabbit.extractor.OpenOfficeTextExtractor.extractText(OpenOfficeTextExtractor.java:7
8)
    at org.apache.jackrabbit.extractor.CompositeTextExtractor.extractText(CompositeTextExtractor.java:90)
    at org.apache.jackrabbit.core.query.lucene.JackrabbitTextExtractor.extractText(JackrabbitTextExtracto
r.java:195)
    at org.apache.jackrabbit.core.query.lucene.NodeIndexer.addBinaryValue(NodeIndexer.java:393)
    at org.apache.jackrabbit.core.query.lucene.NodeIndexer.addValue(NodeIndexer.java:282)
    at org.apache.jackrabbit.core.query.lucene.NodeIndexer.createDoc(NodeIndexer.java:221)
    at org.apache.jackrabbit.core.query.lucene.SearchIndex.createDocument(SearchIndex.java:892)
    at org.apache.jackrabbit.core.query.lucene.SearchIndex$2.next(SearchIndex.java:543)
    at org.apache.jackrabbit.core.query.lucene.MultiIndex.update(MultiIndex.java:428)
    at org.apache.jackrabbit.core.query.lucene.SearchIndex.updateNodes(SearchIndex.java:527)
    at org.apache.jackrabbit.core.SearchManager.onEvent(SearchManager.java:504)
    at org.apache.jackrabbit.core.observation.EventConsumer.consumeEvents(EventConsumer.java:231)
    at org.apache.jackrabbit.core.observation.ObservationDispatcher.dispatchEvents(ObservationDispatcher.
java:201)
    at org.apache.jackrabbit.core.observation.EventStateCollection.dispatch(EventStateCollection.java:425
)
    at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:737
)
    at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:873)
    at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:334)
    at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:337)
    at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:310)
    at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:317)
    at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1247)
    at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:897)
    at org.apache.jackrabbit.jca.JCASessionHandle.save(JCASessionHandle.java:178)"
1,CheckIndex overstates how many fields have norms enabledIt simply tells you how many unique fields there are... it should instead only say how many have norms.
1,CustomScoreQuery calls weight() where it should call createWeight()Thanks to Uwe for helping me track down this bug after I pulled my hair out for hours on LUCENE-3174.
1,"BitVector.isSparse is sometimes wrongIn working on LUCENE-3246, I found a few problems with
BitVector.isSparse:

  * Its math can overflow int, such that if there are enough deleted
    docs and maxDoc() is largish, isSparse may incorrectly return true

  * It over-estimates the size of the sparse file, since when
    estimating number of bytes for the vInt dgaps it uses bits.length
    instead of bits.length divided by number of set bits (ie, the
    ""average"" gap between set bits)

This is relatively harmless (just affects performance / size of .del
file on disk, not correctness).
"
1,"GData-server storage fix activation depthFixed nullpointer exception while rendering feeds with big amount of extensions. DB4O context.

"
1,"Analysis back compat breakOld and new style token streams don't mix well.
"
1,"3.x indexes have the wrong normType set in fieldinfos3.x codec claims the single byte norms are BYTES_VAR_STRAIGHT in FieldInfos,
but the norms implementation itself then has the type as FIXED_INTS_8."
1,"SessionItemStateManager.getIdOfRootTransientNodeState() may cause NPEregression of JCR-2425

in certain scenarios, calling SessionItemStateManager.getIdOfRootTransientNodeState() may cause a NPE.

Test case: 

        Repository repository = new TransientRepository(); 
        Session session = repository.login( 
                new SimpleCredentials(""admin"", ""admin"".toCharArray())); 
        Session session2 = repository.login( 
                new SimpleCredentials(""admin"", ""admin"".toCharArray())); 

        try { 
            while (session.getRootNode().hasNode(""test"")) { 
                session.getRootNode().getNode(""test"").remove(); 
            } 
            Node test = session.getRootNode().addNode(""test""); 
            session.save(); 
            Node x = test.addNode(""x""); 
            session.save(); 

            Node x2 = session2.getRootNode().getNode(""test"").getNode(""x""); 
            x2.remove(); 
            x.addNode(""b""); 
            session2.save(); 
            session.save(); // throws NPE 
        } finally { 
            session.logout(); 
            session2.logout(); 
        }"
1,"fix some more locale problems in lucene/solrset ANT_ARGS=""-Dargs=-Duser.language=tr -Duser.country=TR""
ant clean test

We should make sure this works across all of lucene/solr"
1,"JCR2SPI: several broken equals() comparisonsDetected by FindBugs:

H C EC: Call to equals() comparing unrelated class and interface in org.apache.jackrabbit.jcr2spi.nodetype.NodeTypeManagerImpl.nodeTypeReRegistered(QName)	xythos-jcr/src/main/java/org/apache/jackrabbit/jcr2spi/nodetype	NodeTypeManagerImpl.java	line 218	1190978573312	1664752
H C EC: Call to equals() comparing unrelated class and interface in org.apache.jackrabbit.jcr2spi.nodetype.NodeTypeManagerImpl.nodeTypeReRegistered(QName)	xythos-jcr/src/main/java/org/apache/jackrabbit/jcr2spi/nodetype	NodeTypeManagerImpl.java	line 227	1190978573312	1664753
H C EC: Call to equals() comparing unrelated class and interface in org.apache.jackrabbit.jcr2spi.nodetype.NodeTypeManagerImpl.nodeTypeUnregistered(QName)	xythos-jcr/src/main/java/org/apache/jackrabbit/jcr2spi/nodetype	NodeTypeManagerImpl.java	line 255	1190978573312	1664754
H C EC: Call to equals() comparing unrelated class and interface in org.apache.jackrabbit.jcr2spi.nodetype.NodeTypeManagerImpl.nodeTypeUnregistered(QName)	xythos-jcr/src/main/java/org/apache/jackrabbit/jcr2spi/nodetype	NodeTypeManagerImpl.java	line 264	1190978573312	1664755
H C EC: org.apache.jackrabbit.jcr2spi.WorkspaceManager.canAccess(String) uses equals to compare an array and nonarray	
"
1,"TrecContentSource should use a fixed encoding, rather than system dependentTrecContentSource opens InputStreamReader w/o a fixed encoding. On Windows, this means CP1252 (at least on my machine) which is ok. However, when I opened it on a Linux machine w/ a default of UTF-8, it failed to read the files. The patch changes it to use ISO-8859-1, which seems to be the right one (and http://mg4j.dsi.unimi.it/man/manual/ch01s04.html mentions this encoding in its example of a script which reads the data).

Patch to follow shortly."
1,"Versioned node importXML failsWhen importing system-view XML previously exported for a repository, any nodes with a version history cannot be reimported. This appears to be due to the version manager attempting to create a new version history for the node, which fails due to a previous history existing for the same UUID. The behavior occurs with ImportUUIDBehavior.IMPORT_UUID_COLLISION_REPLACE_EXISTING and  ImportUUIDBehavior.IMPORT_UUID_COLLISION_REMOVE_EXISTING, with the following stack trace:

javax.jcr.version.VersionException: History already exists for node a892651d-1688-46cd-bb12-14f2f0b3d886
	at org.apache.jackrabbit.core.version.VersionManagerImpl.createVersionHistory(VersionManagerImpl.java:194)
	at org.apache.jackrabbit.core.ItemImpl.initVersionHistories(ItemImpl.java:900)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1313)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:766)

I am using the 1.0-dev version, revision 209290 obtained on 05 Jul, 2005 at 9:18:02 EST. Attached please find my repository configuration and the test code. Thanks!


"
1,"EasyX509TrustManager no longer checks cert expiryEasyX509TrustManager was made even ""easier"" by the last commit:  a socket will
now be created when talking to a server with an expired certificate.

2 commits ago it looked like this (notice ""return false"" on line 107):

102             try {
103                 certificate.checkValidity();
104             }
105             catch (CertificateException e) {
106                 LOG.error(e.toString());
107                 return false;
108             }


Now it looks like this:

102             try {
103                 certificate.checkValidity();
104             }
105             catch (CertificateException e) {
106                 LOG.error(e.toString());
107             }


I'm proposing we just do:

102             certificate.checkValidity();

Now that we're using Java 1.4 in the contrib code, we'll just let the
CertificateException fly up the stack."
1,custom sort broken if IS uses executorservice
1,"DEFAULT_HEADERS not added to subsequent requestsDEFAULT_HEADERS are added to the original request only, not to subsequent requests for redirects or authentication."
1,"Search with Filter does not work!See attached JUnitTest, self-explanatory


"
1,"Cluster sync not always done when calling session.refresh(..)Session.refresh(..) is supposed to synchronize cluster changes, but this doesn't always happen, specially if the syncDelay is low. The reason is a wrong assumption in ClusterNode.sync: The code there to avoid duplicate sync calls doesn't always work as expected. The following algorithm is used:

        int count = syncCount;
        syncLock.acquire();
        if (count == syncCount) {
            journalSync();
            syncCount++;
        }
        syncLock.release();

The problem is that the background thread might be at the line ""syncCount++"" when Session.refresh(..) is called, so that the main thread believes journalSync was already called and thus doesn't call it."
1,"Can not instantiate lucene Analyzer in SearchIndexIn the Lucene 3, the there is no default constructor anymore in Analyzer classes


11:46:45.946 [main] WARN  o.a.j.core.query.lucene.SearchIndex - Invalid Analyzer class: org.apache.lucene.analysis.standard.StandardAnalyzer
java.lang.InstantiationException: org.apache.lucene.analysis.standard.StandardAnalyzer
        at java.lang.Class.newInstance0(Class.java:340) ~[na:1.6.0_26]
        at java.lang.Class.newInstance(Class.java:308) ~[na:1.6.0_26]
        at org.apache.jackrabbit.core.query.lucene.SearchIndex.setAnalyzer(SearchIndex.java:1892) ~[jackrabbit-core-2.4.0.jar:2.4.0]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.6.0_26]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) ~[na:1.6.0_26]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) ~[na:1.6.0_26]
        at java.lang.reflect.Method.invoke(Method.java:597) ~[na:1.6.0_26]
        at org.apache.jackrabbit.core.config.BeanConfig.setProperty(BeanConfig.java:255) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.config.BeanConfig.newInstance(BeanConfig.java:203) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.config.RepositoryConfigurationParser$1.getQueryHandler(RepositoryConfigurationParser.java:652) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.config.WorkspaceConfig.getQueryHandler(WorkspaceConfig.java:251) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.SearchManager.<init>(SearchManager.java:171) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.getSearchManager(RepositoryImpl.java:1855) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.doPostInitialize(RepositoryImpl.java:2092) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.initialize(RepositoryImpl.java:1997) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.RepositoryImpl.initStartupWorkspaces(RepositoryImpl.java:510) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:318) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:582) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.jndi.BindableRepository.createRepository(BindableRepository.java:141) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.jndi.BindableRepository.init(BindableRepository.java:117) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.jndi.BindableRepository.<init>(BindableRepository.java:106) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.jndi.BindableRepositoryFactory.getObjectInstance(BindableRepositoryFactory.java:52) [jackrabbit-core-2.4.0.jar:2.4.0]
"
1,"Token of  """" returns in CJKTokenizer + new TestCJKTokenizerThe """" string returns as Token in the boundary of two byte character and one byte character. 

There is no problem in CJKAnalyzer. 
When CJKTokenizer is used with the unit, it becomes a problem. (Use it with 
Solr etc.)"
1,"always apply position increment gap between valuesI'm doing some fancy stuff with span queries that is very sensitive to term positions.  I discovered that the position increment gap on indexing is only applied between values when there are existing terms indexed for the document.  I suspect this logic wasn't deliberate, it's just how its always been for no particular reason.  I think it should always apply the gap between fields.  Reference DocInverterPerField.java line 82:

if (fieldState.length > 0)
          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);

This is checking fieldState.length.  I think the condition should simply be:  if (i > 0).
I don't think this change will affect anyone at all but it will certainly help me.  Presently, I can either change this line in Lucene, or I can put in a hack so that the first value for the document is some dummy value which is wasteful."
1,"Possible concurrency bug with Workspace.copy() Hi,

Enclosed below is a test case that can be used to reproduce a
concurrency bug. This test case uses two con-current threads to
execute Workspace.copy() to copy a node to same destination. The
parent node has set its allowSameNameSiblings to false. According to
the javadoc of Workspace.copy(String srcAbsPath, String destAbsPath) :
""This method copies the node at srcAbsPath to the new location at
destAbsPath. If successful, the change is persisted immediately, there
is no need to call save."".  ""An ItemExistException is thrown if a
property already exists at destAbsPath or a node already exist there,
and same name siblings are not allowed. ""

However in reality this is not the case.  The test case can end up
with two child nodes with same names. Please note, not every run can
reproduce the problem, but generally I can get the problem within 3 to
10 iterations. I also got an InvalidItemStateException once (only
once).  Can someone kindly help to confirm if this is a bug in
Jackrabbit or maybe I am using JackRabbit in a wrong way? The test
case has been tested on Jackrabbit 1.6 branch
(http://svn.apache.org/repos/asf/jackrabbit/tags/1.6.0), Windows
Vista, JDK 1.5.0_14.

The test case is also attached for your convenience.

Thanks,
Jervis Liu

package org.apache.jackrabbit.core;

import org.apache.jackrabbit.test.AbstractJCRTest;
import javax.jcr.ItemExistsException;
import javax.jcr.Node;
import javax.jcr.Session;
import javax.jcr.Value;
import javax.jcr.NodeIterator;
import java.util.Random;
import java.util.ArrayList;
import java.util.Iterator;
import javax.jcr.nodetype.NodeType;

import org.apache.jackrabbit.test.NotExecutableException;
import javax.jcr.RepositoryException;
import javax.jcr.nodetype.NodeTypeManager;


public class ConcurrentCopyTest extends AbstractJCRTest {

    private static final int NUM_ITERATIONS = 40;
    private static final int NUM_SESSIONS = 2;

    String sourcePath;
    String destPath;

    public void testConcurrentCopy() throws Exception {
        for (int n = 0; n < NUM_ITERATIONS; n++) {
            System.out.println(""---Iteration---- "" + n);

            // clean up testRoot first
            if (testRootNode.hasNode(""ConcurrentCopyTestNode"")) {
                Node testNode = testRootNode.getNode(""ConcurrentCopyTestNode"");
                testNode.remove();
                testRootNode.save();
                System.out.println(""---old node removed---"");
            }

            // create a parent node where allowSameNameSiblings is set to false
            Node snsfNode = testRootNode.addNode(""ConcurrentCopyTestNode"",
                    ""nt:folder"");
            testRootNode.save();
            sourcePath = snsfNode.getPath();
            destPath = sourcePath + ""/"" + ""CopiedFromConcurrentCopyTestNode"";
            System.out.println(""---sourcePath-----------------"" + sourcePath);
            System.out.println(""---destPath-----------------"" + destPath);

            // firstly we verify it works with single thread.
            Session rootSession = helper.getSuperuserSession();
            rootSession.getWorkspace().copy(sourcePath, destPath + ""test"");

            // copy again to same destPath, expect an ItemExistsException
            try {
                rootSession.getWorkspace().copy(sourcePath, destPath + ""test"");
                fail(""Node exists below '"" + destPath + ""'. Test should fail."");
            } catch (ItemExistsException e) {
            }

            Thread[] threads = new Thread[NUM_SESSIONS];
            for (int i = 0; i < threads.length; i++) {
                // create new session
                Session session = helper.getSuperuserSession();
                TestSession ts = new TestSession(""s"" + i, session);
                Thread t = new Thread(ts);
                t.setName((NUM_ITERATIONS - n) + ""-s"" + i);
                t.start();
                log.println(""Thread#"" + i + "" started"");
                threads[i] = t;
                // Thread.yield();
                // Thread.sleep(100);
            }
            for (int i = 0; i < threads.length; i++) {
                threads[i].join();
            }

            NodeIterator results = testRootNode.getNode(
                    ""ConcurrentCopyTestNode"").getNodes(
                    ""CopiedFromConcurrentCopyTestNode"");
            while (results.hasNext()) {
                Node node = results.nextNode();
                System.out.println(""--result node- "" + node.getName());
            }

            assertEquals(1, results.getSize());
        }
    }

    // --------------------------------------------------------< inner classes >
    class TestSession implements Runnable {

        Session session;
        String identity;
        Random r;

        TestSession(String identity, Session s) {
            session = s;
            this.identity = identity;
            r = new Random();
        }

        private void randomSleep() {
            long l = r.nextInt(90) + 20;
            try {
                Thread.sleep(l);
            } catch (InterruptedException ie) {
            }
        }

        public void run() {

            log.println(""started."");
            String state = """";
            try {
                this.session.getWorkspace().copy(sourcePath, destPath);
                session.save();
                Node newNode =
testRootNode.getNode(""ConcurrentCopyTestNode/CopiedFromConcurrentCopyTestNode"");
                System.out.println(""--Added node- "" + newNode.getName());

                session.save();
                randomSleep();
            } catch (Exception e) {
                log.println(""Exception while "" + state + "": "" + e.getMessage());
                e.printStackTrace();
            } finally {
                session.logout();
            }

            log.println(""ended."");
        }
    }

}

"
1,"automaton termsenum bug when running with multithreaded searchThis one popped in hudson (with a test that runs the same query against fieldcache, and with a filter rewrite, and compares results)

However, its actually worse and unrelated to the fieldcache: you can set both to filter rewrite and it will still fail.
"
1,"Node merge method doesnt seems to recurse thru childs of the right source nodeI checked the NodeImpl.merge(...)

it seems the way it process the childs nodes is wrong
as it calls the merge on the childs of the src node that come from the source workspace.
plus in the case srcNode is null it would end on a NullPointerException as

it does  NodeIterator ni = srcNode.getNodes(); in the second statment of the if condition
"
1,"exception during writeRequest leaves the connection un-releasedThe execute method has the following (simplified) flow:
1) get connection
2) write request
3) read result
4) release connection.
The release in step 4 happens when the input is completely read, which works fine.
If an exception occurs between steps 1 and 2, the connection is also released
properly.
However, if an exception occurs during step 2, the connection is never released
back and the connection manager eventually runs out of connections.

The easiest way to test this is to make a simple subclass of PostMethod that
overrides the writeRequest method:

public class TestConnectionReleaseMethod extends PostMethod
{
    protected void writeRequest(HttpState state, HttpConnection conn) throws
IOException, HttpException
    {
         throw new IOException(""for testing"");
    }
}"
1,"Host configuration properties not updated when the method is redirectedthe above uri:

http://www.adobe.com/cgi-bin/redirect?http://lists.w3.org/Archives/Public/www-xsl-fo

generates two 302 responses:

from the original to http://lists.w3.org/Archives/Public/www-xsl-fo
and from that to http://lists.w3.org/Archives/Public/www-xsl-fo/

the client accepts and follows these redirects (a trace of the process shows it's working well) but when 
you ask the getmethod what uri we ended up at using the getURI() method it returns the bastardised 
result:

http://www.adobe.com/Archives/Public/www-xsl-fo/

instead of the correct 

http://lists.w3.org/Archives/Public/www-xsl-fo/

that the client has actually downloaded.

using cvsup'd copy showing version string "" Jakarta Commons-HttpClient/2.1m1"""
1,"DefaultHttpRequestRetryHandler must not retry non-idempotent http methods (violates RFC 2616)In DefaultHttpRequestRetryHandler, in case of NoHttpResponseException, the request is retried, without taking into account whether the http method is idempotent or not. This violates RFC 2616 section 8.1.4 which states :
{quote}
This means that clients, servers, and proxies MUST be able to recover
   from asynchronous close events. Client software SHOULD reopen the
   transport connection and retransmit the aborted sequence of requests
   without user interaction so long as the request sequence is
   idempotent (see section 9.1.2). Non-idempotent methods or sequences
   MUST NOT be automatically retried, although user agents MAY offer a
   human operator the choice of retrying the request(s).
{quote}

The fix is simple : at line 94, just remove the {{if (exception instanceof NoHttpResponseException) }} block. This way the idempotency of the method will be taken into account a bit further in the same method."
1,"Custom similarity is ignored when using MultiSearcherSymptoms:
I am using Searcher.setSimilarity() to provide a custom similarity that turns off tf() factor. However, somewhere along the way the custom similarity is ignored and the DefaultSimilarity is used. I am using MultiSearcher and BooleanQuery.

Problem analysis:
The problem seems to be in MultiSearcher.createWeight(Query) method. It creates an instance of CachedDfSource but does not set the similarity. As the result CachedDfSource provides DefaultSimilarity to queries that use it.

Potential solution:
Adding the following line:
    cacheSim.setSimilarity(getSimilarity());
after creating an instance of CacheDfSource (line 312) seems to fix the problem. However, I don't understand enough of the inner workings of this class to be absolutely sure that this is the right thing to do.

"
1,"SegmentReader.getFieldNames ignores FieldOption.DOC_VALUESwe use this getFieldNames api in segmentmerger if we merge something that isn't a SegmentReader (e.g. FilterIndexReader)

it looks to me that if you use a FilterIndexReader, call addIndexes(Reader...) the docvalues will be simply dropped.

I dont think its enough to just note that the field has docvalues either right? We need to also set the type 
correctly in the merged field infos? This would imply that instead of FieldOption.DOCVALUES, we need to have a 
FieldOption for each ValueType so that we correctly update the type.

But looking at FI.update/setDocValues, it doesn't look like we 'type-promote' here anyway?
"
1,"KeywordMarkerFilter resets keyword attribute state to false for tokens not in protwords.txtKeywordMarkerFilter sets true or false for the KeywordAttribute on all tokens. This erases previous state established further up the filter chain, for example in the case where a custom filter wants to prevent a token from being stemmed. 

If a token is already marked as a keyword (KeywordAttribute.isKeyword() == true), perhaps the KeywordMarkerFilterFactory should not re-set the state to false."
1,"BundleDbPersistenceManager does not work with MySQLIt seems that the bundle persistence manager base does not work with MySQL. A SQLException is thrown on the line ""con.commit();"" in BundleDbPersistenceManager.checkSchema() because autoCommit is set to true in the init method. For some reason, this is ignored by the Oracle and MSSQL drivers. Anyway, commenting out the line fixes the issue, I think."
1,"search vs explain - score discrepanciesI'm on a mission to demonstrate (and then hopefully fix) any inconsistencies between the score you get for a doc when executing a search, and the score you get when asking for an explanation of the query for that doc."
1,"BLOB Store: only open a stream when really necessaryCurrently, PropertyImpl.getValue() opens a FileInputStream if the BLOBStore is used.
If the application doesn't use the value, this stream is never closed. 

See also JCR-2067 (FileDataStore)"
1,Basic Authentification fails with non-ASCII username/password charactershttp://marc.theaimsgroup.com/?t=106866959500001&r=1&w=2
1,"MOVE method returns error 412 Precondition FailedHi, I was trying MacOS X 10.5 Finder's WebDAV client to do testing on Jackrabbit 1.4 which is hosted on Tomcat 5.5.25 on a Windows XP SP2 computer on a LAN. I encounter an error while doing remote editing, I was able to open the text document, but the problem is I couldn't save it.

I tried to find some log on Tomcat but sadly Jackrabbit didn't produces any log files regarding of my problem. So I used Ethereal 0.99.0 to check the packets from the Windows XP computer. The below trace is a summary from the exported text file of the packet analyzer where the problem occur:-

line 11818:-
No.     Time        Source                Destination           Protocol Info
4352 27.629257   10.60.1.90            10.60.1.187           HTTP     MOVE /jackrabbit-webapp-1.4/repository/default/.TemporaryItems/folders.501/TemporaryItems/(A%20Document%20Being%20Saved%20By%20TextEdit)/Copy%20of%20Request_for_GAMS_User_Account.rtf HTTP/1.1

Frame 4352 (592 bytes on wire, 592 bytes captured)
Ethernet II, Src: AppleCom_72:c3:5e (00:0d:93:72:c3:5e), Dst: 00:19:d1:a0:34:f7 (00:19:d1:a0:34:f7)
Internet Protocol, Src: 10.60.1.90 (10.60.1.90), Dst: 10.60.1.187 (10.60.1.187)
Transmission Control Protocol, Src Port: 64970 (64970), Dst Port: 8080 (8080), Seq: 69060, Ack: 90475, Len: 526
    Source port: 64970 (64970)
    Destination port: 8080 (8080)
    Sequence number: 69060    (relative sequence number)
    Next sequence number: 69586    (relative sequence number)
    Acknowledgement number: 90475    (relative ack number)
    Header length: 32 bytes
    Flags: 0x0018 (PSH, ACK)
    Window size: 524280 (scaled)
    Checksum: 0xd4f9 [correct]
    Options: (12 bytes)
Hypertext Transfer Protocol
    MOVE /jackrabbit-webapp-1.4/repository/default/.TemporaryItems/folders.501/TemporaryItems/(A%20Document%20Being%20Saved%20By%20TextEdit)/Copy%20of%20Request_for_GAMS_User_Account.rtf HTTP/1.1\r\n
        Request Method: MOVE
        Request URI: /jackrabbit-webapp-1.4/repository/default/.TemporaryItems/folders.501/TemporaryItems/(A%20Document%20Being%20Saved%20By%20TextEdit)/Copy%20of%20Request_for_GAMS_User_Account.rtf
        Request Version: HTTP/1.1
    User-Agent: WebDAVFS/1.5 (01508000) Darwin/9.1.0 (Power Macintosh)\r\n
    Accept: */*\r\n
    Destination: http://10.60.1.187:8080/jackrabbit-webapp-1.4/repository/default/au/gov/arc/www/rtf/Copy%20of%20Request_for_GAMS_User_Account.rtf\r\n
    Authorization: Basic YWRtaW46YWRtaW4=\r\n
        Credentials: admin:admin
    Content-Length: 0\r\n
    Connection: keep-alive\r\n
    Host: 10.60.1.187:8080\r\n
    \r\n

line 11850 -
No.     Time        Source                Destination           Protocol Info
4353 27.630345   10.60.1.187           10.60.1.90            HTTP     HTTP/1.1 412 Precondition Failed (text/html)

Frame 4353 (1191 bytes on wire, 1191 bytes captured)
Ethernet II, Src: 00:19:d1:a0:34:f7 (00:19:d1:a0:34:f7), Dst: AppleCom_72:c3:5e (00:0d:93:72:c3:5e)
Internet Protocol, Src: 10.60.1.187 (10.60.1.187), Dst: 10.60.1.90 (10.60.1.90)
Transmission Control Protocol, Src Port: 8080 (8080), Dst Port: 64970 (64970), Seq: 90475, Ack: 69586, Len: 1125
    Source port: 8080 (8080)
    Destination port: 64970 (64970)
    Sequence number: 90475    (relative sequence number)
    Next sequence number: 91600    (relative sequence number)
    Acknowledgement number: 69586    (relative ack number)
    Header length: 32 bytes
    Flags: 0x0018 (PSH, ACK)
    Window size: 65535
    Checksum: 0x1c18 [incorrect, should be 0xa2f0]
    Options: (12 bytes)
Hypertext Transfer Protocol
    HTTP/1.1 412 Precondition Failed\r\n
        Request Version: HTTP/1.1
        Response Code: 412
    Server: Apache-Coyote/1.1\r\n
    Content-Type: text/html;charset=utf-8\r\n
    Content-Length: 965\r\n
    Date: Fri, 29 Feb 2008 02:31:01 GMT\r\n
    \r\n
Line-based text data: text/html
    <html><head><title>Apache Tomcat/5.5.25 - Error report</title><style><!--H1 {font-family:Tahoma,Arial,sans-serif;color:white;background-color:#525D76;font-size:22px;} H2 {font-family:Tahoma,Arial,sans-serif;color:white;background-color:#52
"
1,"Large file download over webdav causes exceptionDownloading a large file (>2GB) from webdav causes an exception.

(Note: uploading the file works ok, when jackrabbit is configured to use the filesystem DataStore.)

When trying to retrieve the file with e.g. ""wget"", we get the following error:

Gozer:Desktop greg$ wget --http-user=xxx --http-passwd=xxx http://localhost:8080/jackrabbit/repository/workbench/pkgs/demo/zip/zips/largetest-1.zip
--08:59:50--  http://localhost:8080/jackrabbit/repository/workbench/pkgs/demo/zip/zips/largetest-1.zip
           => `largetest-1.zip'
Resolving localhost... done.
Connecting to localhost[127.0.0.1]:8080... connected.
HTTP request sent, awaiting response... 500 For input string: ""3156213760""
09:04:53 ERROR 500: For input string: ""3156213760"".

In the server log we see this:

06.03.2009 08:59:50 *INFO * RepositoryImpl: SecurityManager = class org.apache.jackrabbit.core.security.simple.SimpleSecurityManager (RepositoryImpl.java, line 432)
2009-03-06 09:04:53.822::WARN:  /jackrabbit/repository/workbench/pkgs/demo/zip/zips/largetest-1.zip
java.lang.NumberFormatException: For input string: ""3156213760""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)
	at java.lang.Integer.parseInt(Integer.java:459)
	at java.lang.Integer.parseInt(Integer.java:497)
	at org.apache.jackrabbit.webdav.io.OutputContextImpl.setContentLength(OutputContextImpl.java:60)
	at org.apache.jackrabbit.server.io.ExportContextImpl.informCompleted(ExportContextImpl.java:192)
	at org.apache.jackrabbit.server.io.IOManagerImpl.exportContent(IOManagerImpl.java:157)
	at org.apache.jackrabbit.webdav.simple.DavResourceImpl.spool(DavResourceImpl.java:332)
	at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.spoolResource(AbstractWebdavServlet.java:422)
	at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.doGet(AbstractWebdavServlet.java:388)
	at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.execute(AbstractWebdavServlet.java:229)
	at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.service(AbstractWebdavServlet.java:196)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:502)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:363)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:417)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
	at org.mortbay.jetty.handler.HandlerCollection.handle(HandlerCollection.java:114)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:324)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:534)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:864)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:533)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:207)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:403)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:409)
	at org.mortbay.thread.BoundedThreadPool$PoolThread.run(BoundedThreadPool.java:451)


The problem seems to lie in OutputContextImpl.java it makes the mistake of potentially trying to parse a Long as an Integer, here: http://svn.apache.org/repos/asf/jackrabbit/trunk/jackrabbit-webdav/src/main/java/org/apache/jackrabbit/webdav/io/OutputContextImpl.java

in the method setContentLength(long contentLength):

public void setContentLength(long contentLength) {
       int length = Integer.parseInt(contentLength + """");
       if (length >= 0) {
           response.setContentLength(length);
       }
   }

I'm not sure, but a fix might be like this:

public void setContentLength(long contentLength) {
       if(contentLength <= Integer.MAX_VALUE && contentLength >= 0) {
           response.setContentLength((int) contentLength);
       }else if (contentLength >  Integer.MAX_VALUE) {
            response.addHeader(""Content-Length"", Long.toString(contentLength));
       }
   }

This would at least set the Content-Length header, and in some preliminary tests does seem to allow downloading the files."
1,"Error logged when repository is shut downThis only happens with the bundle DerbyPersistenceManager.

In DerbyPersistenceManager.close() the embedded derby database is shut down and then super.close() is called. There the ConnectionRecoveryManager is closed, which tries to operate on a connection to the already shut down derby database. The log contains entries like:

25.03.2008 13:49:29 *ERROR* [Thread-5] ConnectionRecoveryManager: failed to close connection, reason: No current connection., state/code: 08003/40000 (ConnectionRecoveryManager.java, line 453)"
1,"Problems mapping custom collectionsWhen using a custom list that extends from java.util.AbstractList, ManageableCollectionUtil.getManageableCollection raises a JcrMappingException because it does not consider the custom list to be a java.util.List. This is because it uses ""if (object.getClass().equals(List.class))"" instead of ""if (object instanceof List)"". The same thing will probably happen when using a custom Collection, a custom ArrayList, etc. This is the stack trace:

org.apache.jackrabbit.ocm.exception.JcrMappingException: Unsupported collection 
type : *********** (MyCustomList class) 
        at org.apache.jackrabbit.ocm.manager.collectionconverter.ManageableColle 
ctionUtil.getManageableCollection(ManageableCollectionUtil.java:153) 
        at org.apache.jackrabbit.ocm.manager.objectconverter.impl.ObjectConverte 
rImpl.insertCollectionFields(ObjectConverterImpl.java:780) 
        at org.apache.jackrabbit.ocm.manager.objectconverter.impl.ObjectConverte 
rImpl.insert(ObjectConverterImpl.java:221) 
        at org.apache.jackrabbit.ocm.manager.objectconverter.impl.ObjectConverte 
rImpl.insert(ObjectConverterImpl.java:146) 
        at org.apache.jackrabbit.ocm.manager.impl.ObjectContentManagerImpl.inser 
t(ObjectContentManagerImpl.java:407) 

I have come up to this bug using a MyCustomList<MyClass>, with MyCustomList extending java.util.AbstractList<MyClass>."
1,"SharedFieldCache can cause a memory leakThe SharedFieldCache has some problems with the way it builds the cache:
 - as key is has the IndexReader
 - as value it has a inner cache (another map) that has as a key a static inner class called 'Key'.

This 'Key' holds a reference to the comparator used for in the queries ran.
Assuming this comparator is of any type that extends from AbstractFieldComparator (I think all of the custom JR comparators), then it keeps a reference to all the InderReader instances in order to be able to load the values as Comparable(s).

So the circle is complete and the SharedFieldCache entries never get GC'ed.

One option would have been to implement a 'purge' method on the cache, similar to the lucene mechanism, and when an InderReader gets closed is could call 'purge'. But that is both ugly AND is doesn't seem to work that well :)

A more radical option is to remove the cache completely. Each instance of SimpleFieldComparator (the only client of this cache) already builds an array of the available values, so the cache would only help other instances of the same type. We'll not analyze this further.

The proposed solution (patch will follow shortly) is to remove the Comparator reference from the Key class. 
It looks like it has no real purpose there, just to impact the 'equals' of the key, which makes no sense in the first place as the lucene query does not use the Comparator info at all.
If anything, using the same field and 2 different Comparators we'll get 2 different cache entries based on the same values from the lucene index.

Feedback is appreciated!








"
1,"QueryObjectModel does not generate the corresponding SQL2 Query when dealing with spaces in the pathThis is the original issue:
----------
I tried to get the childnodes of a node names ""/a b"" using the following code
  QueryManager queryManager=session.getWorkspace().getQueryManager();
  QueryObjectModelFactory qomf=queryManager.getQOMFactory();
  Source source1=qomf.selector(NodeType.NT_BASE, ""selector_0"");
  Column[] columns = new Column[]{qomf.column(""selector_0"", null, null)};
  Constraint constraint2 = qomf.childNode(""selector_0"", ""/a b"");
  QueryObjectModel qom = qomf.createQuery(source1, constraint2 , null, columns);

This is not giving any result when the session is acquired through webdav. But when connected using JNDI it is giving the child nodes. 

The sql statement getting created is 
SELECT selector_0.* FROM [nt:base] AS selector_0 WHERE ISCHILDNODE(selector_0, 
[/a b]).

When using webdav If i give this SQL2 query directly along with quotes around 
the path i.e. ['/a b'] then it is working as expected.
----------

this doesn't have anything to do with webdav. the problem is the QueryObjectModel generates an SQL2 query that is not 100% equivalent, it fails to escape paths that have spaces in them.
this way, in the case of davex remoting, the jr client will use the statement generated instead, which is not escaped, and will fail to return the expected nodes. 

This can be seen easily if we do a System.out.println(qom.getStatement())


"
1,"Filters need hashCode() and equals()Filters need to implement hashCode() and equals(), esp since certain query types can contain a filter (FilteredQuery, ConstantScoreQuery)"
1,"JCR2SPI: NPE when parentId returned by NodeInfo.getParentId does not show up in parent's child node listIn this custom SPI implementation, version history nodes appear as children of jcr:versionStorage, but jcr:versionStorage does not return them as children (which would be impractical for performance reasons - I expect similar approaches used by others...).

getParentId of a NodeInfo of a VersionHistory return the NodeId for jcr:versionStorage. In this case, I get the NPE below:

java.lang.NullPointerException
	at org.apache.jackrabbit.jcr2spi.state.WorkspaceItemStateFactory.createNodeState(WorkspaceItemStateFactory.java:99)
	at org.apache.jackrabbit.jcr2spi.state.CachingItemStateManager.resolve(CachingItemStateManager.java:168)
	at org.apache.jackrabbit.jcr2spi.state.CachingItemStateManager.getItemState(CachingItemStateManager.java:94)
	at org.apache.jackrabbit.jcr2spi.WorkspaceManager.getItemState(WorkspaceManager.java:328)
	at org.apache.jackrabbit.jcr2spi.state.TransientISFactory.createNodeState(TransientISFactory.java:120)
	at org.apache.jackrabbit.jcr2spi.state.CachingItemStateManager.resolve(CachingItemStateManager.java:168)
	at org.apache.jackrabbit.jcr2spi.state.CachingItemStateManager.getItemState(CachingItemStateManager.java:94)
	at org.apache.jackrabbit.jcr2spi.state.TransientItemStateManager.getItemState(TransientItemStateManager.java:209)
	at org.apache.jackrabbit.jcr2spi.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:155)
	at org.apache.jackrabbit.jcr2spi.SessionImpl.getNodeById(SessionImpl.java:271)
	at org.apache.jackrabbit.jcr2spi.SessionImpl.getNodeByUUID(SessionImpl.java:239)

Returning null in this special case fixes the problem over here, but seems to create new problems elsewhere.

Need to clarify the SPI itself, and potentially fix JCR2CPI.
"
1,"IndexOutOfBoundsException from FieldsReader after problem reading the indexThere is a situation where there is an IOException reading from Hits, and then the next time you get a NullPointerException instead of an IOException.

Example stack traces:

java.io.IOException: The specified network name is no longer available
	at java.io.RandomAccessFile.readBytes(Native Method)
	at java.io.RandomAccessFile.read(RandomAccessFile.java:322)
	at org.apache.lucene.store.FSIndexInput.readInternal(FSDirectory.java:536)
	at org.apache.lucene.store.BufferedIndexInput.readBytes(BufferedIndexInput.java:74)
	at org.apache.lucene.index.CompoundFileReader$CSIndexInput.readInternal(CompoundFileReader.java:220)
	at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:93)
	at org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:34)
	at org.apache.lucene.store.IndexInput.readVInt(IndexInput.java:57)
	at org.apache.lucene.index.FieldsReader.doc(FieldsReader.java:88)
	at org.apache.lucene.index.SegmentReader.document(SegmentReader.java:344)
	at org.apache.lucene.index.IndexReader.document(IndexReader.java:368)
	at org.apache.lucene.search.IndexSearcher.doc(IndexSearcher.java:84)
	at org.apache.lucene.search.Hits.doc(Hits.java:104)

That error is fine.  The problem is the next call to doc generates:

java.lang.NullPointerException
	at org.apache.lucene.index.FieldsReader.getIndexType(FieldsReader.java:280)
	at org.apache.lucene.index.FieldsReader.addField(FieldsReader.java:216)
	at org.apache.lucene.index.FieldsReader.doc(FieldsReader.java:101)
	at org.apache.lucene.index.SegmentReader.document(SegmentReader.java:344)
	at org.apache.lucene.index.IndexReader.document(IndexReader.java:368)
	at org.apache.lucene.search.IndexSearcher.doc(IndexSearcher.java:84)
	at org.apache.lucene.search.Hits.doc(Hits.java:104)

Presumably FieldsReader is caching partially-initialised data somewhere.  I would normally expect the exact same IOException to be thrown for subsequent calls to the method.
"
1,"TestSort testParallelMultiSort reproducible seed failuretrunk r1202157
{code}
    [junit] Testsuite: org.apache.lucene.search.TestSort
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.978 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestSort -Dtestmethod=testParallelMultiSort -Dtests.seed=-2996f3e0f5d118c2:32c8e62dd9611f63:7a90f44586ae8263 -Dargs=""-Dfile.encoding=UTF-8""
    [junit] WARNING: test method: 'testParallelMultiSort' left thread running: Thread[pool-1-thread-1,5,main]
    [junit] WARNING: test method: 'testParallelMultiSort' left thread running: Thread[pool-1-thread-2,5,main]
    [junit] WARNING: test method: 'testParallelMultiSort' left thread running: Thread[pool-1-thread-3,5,main]
    [junit] NOTE: test params are: codec=Lucene40: {short=Lucene40(minBlockSize=98 maxBlockSize=214), contents=PostingsFormat(name=MockSep), byte=PostingsFormat(name=SimpleText), int=Pulsing40(freqCutoff=4 minBlockSize=58 maxBlockSize=186), string=PostingsFormat(name=NestedPulsing), i18n=Lucene40(minBlockSize=98 maxBlockSize=214), long=PostingsFormat(name=Memory), double=Pulsing40(freqCutoff=4 minBlockSize=58 maxBlockSize=186), parser=MockVariableIntBlock(baseBlockSize=88), float=Lucene40(minBlockSize=98 maxBlockSize=214), custom=PostingsFormat(name=MockRandom)}, sim=RandomSimilarityProvider(queryNorm=false,coord=false): {short=BM25(k1=1.2,b=0.75), tracer=DFR I(ne)B2, byte=DFR I(ne)B3(800.0), contents=IB LL-LZ(0.3), int=DFR I(n)BZ(0.3), string=IB LL-D3(800.0), i18n=DFR GB2, double=DFR I(ne)B2, long=DFR GB1, parser=DFR GL2, float=BM25(k1=1.2,b=0.75), custom=DFR I(ne)Z(0.3)}, locale=ga_IE, timezone=America/Louisville
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestSort]
    [junit] NOTE: Linux 3.0.6-gentoo amd64/Sun Microsystems Inc. 1.6.0_29 (64-bit)/cpus=8,threads=4,free=78022136,total=125632512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testParallelMultiSort(org.apache.lucene.search.TestSort): FAILED
    [junit] expected:<[ZJ]I> but was:<[JZ]I>
    [junit] junit.framework.AssertionFailedError: expected:<[ZJ]I> but was:<[JZ]I>
    [junit]     at org.apache.lucene.search.TestSort.assertMatches(TestSort.java:1245)
    [junit]     at org.apache.lucene.search.TestSort.assertMatches(TestSort.java:1216)
    [junit]     at org.apache.lucene.search.TestSort.runMultiSorts(TestSort.java:1202)
    [junit]     at org.apache.lucene.search.TestSort.testParallelMultiSort(TestSort.java:855)
    [junit]     at org.apache.lucene.util.LuceneTestCase$3$1.evaluate(LuceneTestCase.java:523)
    [junit]     at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:149)
    [junit]     at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:51)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.search.TestSort FAILED
{code}"
1,LuceneQueryFactory should call QueryHits.close() after running a queryLuceneQueryFactory which is responsible for the JCR_SQL2 implementation does not close QueryHits after running a query.
1,"Locking bugIn org.apache.lucene.store.Lock, line 57 (lucene_1_4_final branch):

if (++sleepCount == maxSleepCount)

is incorrect, the sleepCount is incremented before the compare causing it
throwing the exception with out waiting for at least 1 interation.

Should be changed instead to:
if (sleepCount++ == maxSleepCount)

As this is a self-contained simple fix, I am not submitting a patch.

Thanks

-John"
1,Search results not orderedThe query statements in search.jsp do not have an order by.
1,"Prefix fulltext queries with Japanese or Chinese characters fail to matchPrefix fulltext queries with Japanese or Chinese characters do not match because the prefix part is not tokenized. This means, when the prefix length is >1 the sequence of characters is taken as one term to do the index lookup. This will not match anything because on indexing time such characters are always broken into individual tokens."
1,"DefaultRequestDirector converts redirects of PUT/POST to GET for status codes 301, 302, 307The DefaultRequestDirector treats redirect requests created by all redirect status codes (HttpStatus.SC_MOVED_TEMPORARILY: , HttpStatus.SC_MOVED_PERMANENTLY, HttpStatus.SC_SEE_OTHER, HttpStatus.SC_TEMPORARY_REDIRECT) the same, converting PUT/POST methods to GET.  The HttpClient Tutorial even documents this as being in accordance with the specification, but I don't believe that's true.

Per the RFC (http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html), conversion of PUT/POST to GET is appropriate only for 303 (See Other).  The others do not suggest this behavior.  In fact, the following notes attached to them call it out as incorrect.

301 (Moved Permanently) has this note:

      Note: When automatically redirecting a POST request after
      receiving a 301 status code, some existing HTTP/1.0 user agents
      will erroneously change it into a GET request.

And 302 (Found) say this:

      Note: RFC 1945 and RFC 2068 specify that the client is not allowed
      to change the method on the redirected request.  However, most
      existing user agent implementations treat 302 as if it were a 303
      response, performing a GET on the Location field-value regardless
      of the original request method. The status codes 303 and 307 have
      been added for servers that wish to make unambiguously clear which
      kind of reaction is expected of the client.

The currently implemented behavior is causing problems with interacting with Central Authentication Service protected resources, among other things."
1,"Preemptive auth flags disregarded during ssl tunnel creationUsing a Squid2.4 proxy, the connection is dropped when trying to connect to a 
ssl site. In order for the connection to remain open, preemptive authorization 
is needed for the proxy. The preemptive authorization flags are not propagated 
down to where the ssl tunnel is created in HttpMethodDirectors executeConnect 
method. A new ConnectMethod object is created for the tunnel but the preemptive 
flags set as parameters are not being set on the new ConnectMethod object.

Here is the code that would replicate the problem using a Squid(2.4) proxy :

HttpClient client = new HttpClient();
client.getHostConfiguration().setProxyHost(new ProxyHost(""someproxy"", 3128));
client.getParams().setAuthenticationPreemptive(true);
client.getState().setProxyCredentials(AuthScope.ANY, new 
UsernamePasswordCredentials(""user"", ""password""));
GetMethod httpget = new GetMethod(""https://www.verisign.com/"");
httpget.getProxyAuthState().setPreemptive();
client.executeMethod(httpget);
httpget.releaseConnection();"
1,"TestParser.testSpanTermXML fails with some simshere is why this test sometimes fails (my explanation in the test i wrote):

{noformat}
  /** make sure all sims work with spanOR(termX, termY) where termY does not exist */
  public void testCrazySpans() throws Exception {
    // The problem: ""normal"" lucene queries create scorers, returning null if terms dont exist
    // This means they never score a term that does not exist.
    // however with spans, there is only one scorer for the whole hierarchy:
    // inner queries are not real queries, their boosts are ignored, etc.
{noformat}

Basically, SpanQueries aren't really queries, you just get one scorer. it calls extractTerms on the whole hierarchy and computes weights (e.g. IDF) on
the whole bag of terms, even if they don't exist.

This is fine, we already have tests that sim's won't bug-out in computeStats() here: however they don't expect to actually score documents based on
these terms that don't exist... however this is exactly what happens in Spans because it doesn't use sub-scorers.

Lucene's sim avoids this with the (docFreq + 1)
"
1,"benchmark cannot parse highlight-vs-vector-highlight.alg, but only on 3.x?!A new test (TestPerfTasksParse.testParseExamples) was added in LUCENE-3768 that 
guarantees all .alg files in the conf/ directory can actually be parsed...

But highlight-vs-vector-highlight.alg cannot be parsed on 3.x (NumberFormatException), 
however it works fine on trunk... and the .alg is exactly the same in both cases.

{noformat}
    [junit] ------------- Standard Error -----------------
    [junit] java.lang.NumberFormatException: For input string: ""maxFrags[3.0],fields[body]""
    [junit] 	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1222)
    [junit] 	at java.lang.Float.parseFloat(Float.java:422)
    [junit] 	at org.apache.lucene.benchmark.byTask.tasks.SearchTravTask.setParams(SearchTravTask.java:76)
    [junit] 	at org.apache.lucene.benchmark.byTask.tasks.SearchTravRetVectorHighlightTask.setParams(SearchTravRetVectorHighlightTask.java:124)
    [junit] 	at org.apache.lucene.benchmark.byTask.utils.Algorithm.<init>(Algorithm.java:112)
    [junit] 	at org.apache.lucene.benchmark.byTask.TestPerfTasksParse.testParseExamples(TestPerfTasksParse.java:132)
{noformat}
"
1,"trectopicsreader doesn't properly read descriptions or narrativesTrecTopicsReader does not read these fields correctly, as demonstrated by the test case.
"
1,"Rare thread hazard in IndexWriter.commit()The nightly build 2 nights ago hit this:

{code}
 NOTE: random seed of testcase 'testAtomicUpdates' was: -5065675995121791051
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testAtomicUpdates(org.apache.lucene.index.TestAtomicUpdate):	FAILED
    [junit] expected:<100> but was:<91>
    [junit] junit.framework.AssertionFailedError: expected:<100> but was:<91>
    [junit] 	at org.apache.lucene.index.TestAtomicUpdate.runTest(TestAtomicUpdate.java:142)
    [junit] 	at org.apache.lucene.index.TestAtomicUpdate.testAtomicUpdates(TestAtomicUpdate.java:194)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:88)
{code}

It's an intermittant failure that only happens when multiple threads
are calling commit() at once.  With autoComit=true and
ConcurrentMergeScheduler, this can happen more often because each
merge thread calls commit after it's done.

The problem happens when one thread has already begun the commit
process, but another two or more threads then come along wanting to
also commit after further changes have happened.  Those two or more
threads would wait until the currently committing thread finished, and
then they'd wake up and do their commit.  The problem was, after
waking up they would fail to check whether they had been superseded,
ie whether another thread had already committed more up-to-date
changes.

The fix is simple -- after waking up, check again if your commit has
been superseded, and skip your commit if so.
"
1,"IndexWriter.optimize(boolean doWait) ignores doWait parameter{{IndexWriter.optimize(boolean doWait)}} ignores the doWait parameter and always calls {{optimize(1, true)}}.

That does not seem to be the intended behavior, based on the doc comment."
1,"import must not ignore xml prefixed attributesXML import currently ignores attributes that are in the xml namespace.
e.g., DocViewImportHandler's startElement():

                if (atts.getQName(i).startsWith(""xml:"")) {
                    // skipping xml:space, xml:lang, etc.
                    log.debug(""skipping reserved/system attribute "" + atts.getQName(i));
                    continue;
                }

That is a significant loss of information, since xml:base, xml:lang, and xml:id attributes are critical to the content.  We should register the xml prefix as a reserved namespace (not needing an xmlns declaration) and then treat it like any other attribute.

Here are some useful XML examples:

http://xformsinstitute.com/essentials/browse/ch03s02.php
http://www.zvon.org/HowTo/Output/
http://www.w3.org/Math/testsuite/testsuite/TortureTests/Complexity/complex1.xml
http://intertwingly.net/wiki/pie/EchoExample
http://support.sas.com/onlinedoc/913/getDoc/en/engxml.hlp/a002973381.htm

"
1,Session#move doesn't trigger rebuild of parent node aggregationThe summary says it all.
1,"XercesImpl is missing in WebDav contrib project$ /usr/local/maven/bin/maven
 __  __
|  \/  |__ _Apache__ ___
| |\/| / _` \ V / -_) ' \  ~ intelligent projects ~
|_|  |_\__,_|\_/\___|_||_|  v. 1.0.2

build:start:

multiproject:install:
multiproject:projects-init:
    [echo] Gathering project list
Starting the reactor...
Our processing order:
JCRWebdavServer Webdav Library
JCRWebdavServer Server Library
JCRWebdavServer Client Library
JCRWebdavServer WebApplication
+----------------------------------------
| Gathering project list JCRWebdavServer Webdav Library
| Memory: 3M/4M
+----------------------------------------
+----------------------------------------
| Gathering project list JCRWebdavServer Server Library
| Memory: 3M/4M
+----------------------------------------
+----------------------------------------
| Gathering project list JCRWebdavServer Client Library
| Memory: 3M/4M
+----------------------------------------
+----------------------------------------
| Gathering project list JCRWebdavServer WebApplication
| Memory: 3M/4M
+----------------------------------------
Starting the reactor...
Our processing order:
JCRWebdavServer Webdav Library
JCRWebdavServer Server Library
JCRWebdavServer Client Library
JCRWebdavServer WebApplication
+----------------------------------------
| Executing multiproject:install-callback JCRWebdavServer Webdav Library
| Memory: 3M/4M
+----------------------------------------
Attempting to download jackrabbit-commons-1.0-SNAPSHOT.jar.
Response content length is not known
Artifact /org.apache.jackrabbit/jars/jackrabbit-commons-1.0-SNAPSHOT.jar doesn't exists in remote repository, but it exists locally

multiproject:goal:
build:start:

multiproject:install-callback:
    [echo] Running jar:install for JCRWebdavServer Webdav Library
java:prepare-filesystem:

java:compile:
    [echo] Compiling to /home/jeremi/src/jackrabbit/trunk/contrib/jcr-server/webdav/target/classes
    [javac] Compiling 109 source files to /home/jeremi/src/jackrabbit/trunk/contrib/jcr-server/webdav/target/classes
/home/jeremi/src/jackrabbit/trunk/contrib/jcr-server/webdav/src/java/org/apache/jackrabbit/webdav/WebdavResponseImpl.java:26: package org.apache.xml.serialize does not exist
import org.apache.xml.serialize.OutputFormat;
                                ^
/home/jeremi/src/jackrabbit/trunk/contrib/jcr-server/webdav/src/java/org/apache/jackrabbit/webdav/WebdavResponseImpl.java:27: package org.apache.xml.serialize does not exist
import org.apache.xml.serialize.XMLSerializer;
                                ^
/home/jeremi/src/jackrabbit/trunk/contrib/jcr-server/webdav/src/java/org/apache/jackrabbit/webdav/WebdavResponseImpl.java:149: cannot resolve symbol
symbol  : class OutputFormat 
location: class org.apache.jackrabbit.webdav.WebdavResponseImpl
                OutputFormat format = new OutputFormat(""xml"", ""UTF-8"", true);
                ^
/home/jeremi/src/jackrabbit/trunk/contrib/jcr-server/webdav/src/java/org/apache/jackrabbit/webdav/WebdavResponseImpl.java:149: cannot resolve symbol
symbol  : class OutputFormat 
location: class org.apache.jackrabbit.webdav.WebdavResponseImpl
                OutputFormat format = new OutputFormat(""xml"", ""UTF-8"", true);
                                          ^
/home/jeremi/src/jackrabbit/trunk/contrib/jcr-server/webdav/src/java/org/apache/jackrabbit/webdav/WebdavResponseImpl.java:150: cannot resolve symbol
symbol  : class XMLSerializer 
location: class org.apache.jackrabbit.webdav.WebdavResponseImpl
                XMLSerializer serializer = new XMLSerializer(out, format);
                ^
/home/jeremi/src/jackrabbit/trunk/contrib/jcr-server/webdav/src/java/org/apache/jackrabbit/webdav/WebdavResponseImpl.java:150: cannot resolve symbol
symbol  : class XMLSerializer 
location: class org.apache.jackrabbit.webdav.WebdavResponseImpl
                XMLSerializer serializer = new XMLSerializer(out, format);
                                               ^
Note: Some input files use or override a deprecated API.
Note: Recompile with -deprecation for details.
6 errors

BUILD FAILED
File...... /home/jeremi/.maven/cache/maven-multiproject-plugin-1.3.1/plugin.jelly
Element... maven:reactor
Line...... 217
Column.... -1
Unable to obtain goal [multiproject:install-callback] -- /home/jeremi/.maven/cache/maven-java-plugin-1.5/plugin.jelly:63:-1: <ant:javac> Compile failed; see the compiler error output for details.
Total time: 8 seconds
Finished at: Fri Jan 27 07:14:37 CET 2006

$"
1,"suggest.fst.Sort.BufferSize should not automatically fail just because of freeMemory()Follow up op dev thread: [FSTCompletionTest failure ""At least 0.5MB RAM buffer is needed"" | http://markmail.org/message/d7ugfo5xof4h5jeh]"
1,"spi2davex: session-scoped lock tokens not included in if-headerdetected while running API lock tests.
org.apache.jackrabbit.test.api.lock.DeepLockTest#testParentChildDeepLock failed though it used to work with spi2dav.

fix is simple: SessionInfoImpl.getAllLockTokens must be used to populate the if-header as it is done in spi2dav."
1,"jcr:frozenUuid does not contain jcr:contentWhen I store versionable files, I get problems retrieving the jcr:data from a custom node type.

I am storing a node type:

xrc:learningContent
        pd: xrc:Keywords
        pd: xrc:MimeType
        pd: jcr:mixinTypes
        pd: xrc:Description
        pd: xrc:Language
        pd: xrc:Creator
        pd: jcr:created
        pd: xrc:Title
        pd: jcr:primaryType
Extends: nt:resource
        pd: jcr:uuid
        pd: jcr:mixinTypes
        pd: jcr:data
        pd: jcr:encoding
        pd: jcr:mimeType
        pd: jcr:lastModified
        pd: jcr:primaryType

So I commit the changes, then later pull up the version and get it's frozenNode.

Node frozenNode = v.getNode(JcrConstants.JCR_FROZENNODE);

And then I return all of the properties contained within:

PropertyIterator pi = frozenNode.getProperties();
                while (pi.hasNext()) {
                    System.out.println(pi.nextProperty().getName());
}


All that are returned are:

jcr:frozenUuid
jcr:uuid
jcr:frozenPrimaryType
jcr:frozenMixinTypes
jcr:primaryType

Here is the frozen node type:

nt:frozenNode
        pd: *
        pd: *
        pd: jcr:frozenUuid
        pd: jcr:uuid
        pd: jcr:mixinTypes
        pd: jcr:frozenPrimaryType
        pd: jcr:frozenMixinTypes
        pd: jcr:primaryType



So basically it would seem that the recursive copy inside the InternalFrozenNodeImpl is not working. But it seems that is not the case from the code trace I did. Add this to line 368 of InternalFrozenNodeImpl.java

System.out.println(""New node created. Props: "");
        try {
            PropertyState [] ps = node.getProperties();
            for (PropertyState p : ps) {
                System.out.println(p.getName());
                System.out.println(p.toString());
            }
            NodeStateEx [] ns = node.getChildNodes();
            for (NodeStateEx n : ns) {
                System.out.println(n.getName());
                System.out.println(n.toString());
            }
        } catch (ItemStateException e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        }


And you will get the result:


New node created. Props:
{http://www.jcp.org/jcr/1.0}uuid
org.apache.jackrabbit.core.state.PropertyState@10dd791
{http://www.jcp.org/jcr/1.0}frozenPrimaryType
org.apache.jackrabbit.core.state.PropertyState@1c38291
{http://www.jcp.org/jcr/1.0}frozenMixinTypes
org.apache.jackrabbit.core.state.PropertyState@b12fbb
{http://www.jcp.org/jcr/1.0}baseVersion
org.apache.jackrabbit.core.state.PropertyState@b4d4b6
{http://www.jcp.org/jcr/1.0}primaryType
org.apache.jackrabbit.core.state.PropertyState@1f9045f
{http://www.jcp.org/jcr/1.0}isCheckedOut
org.apache.jackrabbit.core.state.PropertyState@18e16b5
{http://www.jcp.org/jcr/1.0}frozenUuid
org.apache.jackrabbit.core.state.PropertyState@174cb00
{http://www.jcp.org/jcr/1.0}predecessors
org.apache.jackrabbit.core.state.PropertyState@bb7c1b
{http://www.jcp.org/jcr/1.0}data
org.apache.jackrabbit.core.state.PropertyState@d10133
{http://www.jcp.org/jcr/1.0}versionHistory
org.apache.jackrabbit.core.state.PropertyState@1a5f001
{http://www.jcp.org/jcr/1.0}encoding
org.apache.jackrabbit.core.state.PropertyState@12fe3ef
{http://www.jcp.org/jcr/1.0}mimeType
org.apache.jackrabbit.core.state.PropertyState@11d92c8
{http://www.jcp.org/jcr/1.0}lastModified
org.apache.jackrabbit.core.state.PropertyState@8fb83a
New node created. Props:
{http://www.xerceo.com/learn/jcr-1.0}Keywords
org.apache.jackrabbit.core.state.PropertyState@18808f3
{http://www.jcp.org/jcr/1.0}uuid
org.apache.jackrabbit.core.state.PropertyState@397a4
{http://www.jcp.org/jcr/1.0}frozenPrimaryType
org.apache.jackrabbit.core.state.PropertyState@1d88ffd
{http://www.xerceo.com/learn/jcr-1.0}Creator
org.apache.jackrabbit.core.state.PropertyState@d5625b
{http://www.xerceo.com/learn/jcr-1.0}Language
org.apache.jackrabbit.core.state.PropertyState@12c70e6
{http://www.xerceo.com/learn/jcr-1.0}Title
org.apache.jackrabbit.core.state.PropertyState@a836b3
{http://www.jcp.org/jcr/1.0}frozenMixinTypes
org.apache.jackrabbit.core.state.PropertyState@19f273c
{http://www.jcp.org/jcr/1.0}primaryType
org.apache.jackrabbit.core.state.PropertyState@1c8e97d
{http://www.jcp.org/jcr/1.0}frozenUuid
org.apache.jackrabbit.core.state.PropertyState@15915a3
{http://www.jcp.org/jcr/1.0}predecessors
org.apache.jackrabbit.core.state.PropertyState@19ba907
{http://www.xerceo.com/learn/jcr-1.0}MimeType
org.apache.jackrabbit.core.state.PropertyState@763ca1
{http://www.xerceo.com/learn/jcr-1.0}Description
org.apache.jackrabbit.core.state.PropertyState@8687e8
{http://www.jcp.org/jcr/1.0}versionHistory
org.apache.jackrabbit.core.state.PropertyState@44ca0f
{http://www.jcp.org/jcr/1.0}content
org.apache.jackrabbit.core.version.NodeStateEx@2da721

So the new Node definately has these new properties.

Do I have to somehow extend my frozenNode to work with this? Can anyone help me?"
1,"ConcurrentModificationException in QueryStatImplRunning with qurystats enabled the Query#execute can throw ConcurrentModificationException

caused by the iterator which backing collection is changed from another thread

see logQuery method
        Iterator<QueryStatDtoImpl> iterator = popularQueries.iterator();
        while (iterator.hasNext()) {
-->            QueryStatDtoImpl qsdi = iterator.next();
            if (qsdi.equals(qs)) {
                qs.setOccurrenceCount(qsdi.getOccurrenceCount() + 1);
                iterator.remove();
                break;
            }
        }
        popularQueries.offer(qs);
"
1,"NullPointerException in NegotiateScheme- server is configured to allow client to authenticate with kerberos with principal foobar
- client, using httpclient with a registered authscheme SPNEGO set as a NegotiateSchemeFactory

- when the client authenticate with the (correct) principal foobar, it works !
- when the client authenticate with the (wrong) principal fooba, it fails with a NPE below.


Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.commons.codec.binary.Base64.encodeBase64(Base64.java:233)
	at org.apache.commons.codec.binary.Base64.encode(Base64.java:521)
	at org.apache.http.impl.auth.NegotiateScheme.authenticate(NegotiateScheme.java:240)
	at org.apache.http.client.protocol.RequestTargetAuthentication.process(RequestTargetAuthentication.java:99)
	at org.apache.http.protocol.ImmutableHttpProcessor.process(ImmutableHttpProcessor.java:108)
	at org.apache.http.protocol.HttpRequestExecutor.preProcess(HttpRequestExecutor.java:167)
	at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:460)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:689)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:624)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:602)
"
1,"Database Data Store: close result setsThe database data store doesn't close one result sets. This is not a problem for most databases, but anyway should be fixed."
1,"Data Store: UTFDataFormatException when using large minRecordLengthIf using a value larger than 33000 for minRecordLength, and then trying to store a value with 33000 bytes, the following exception is thrown: UTFDataFormatException. The reason is that values are serialized using DataOutputStream.writeUTF. There is size limitation of 65 K when using this method. Small entries are hex encoded, and there is a prefix, so the limitation for minRecordLength should be 32000.

This is a problem for both FileDataStore and DbDataStore.
"
1,"Core: WEAKREFERENCE properties object have type REFERENCE when being read from the persistent layerit seems to me that WEAKREFERENCE properties are properly created and stored as such but are read as REFERENCE 
properties when built again from the persistent layer.

how to reproduce:

- create a new WEAKREFERENCE property and save the changes
- force reading from the persistent layer  (in my case I used Day's CRX and restartet the server)
- the former WEAKREFERENCE will now be displayed as REFERENCE.

"
1,"Jcr2Spi: ExportSysViewTest#testExportSysView_handler_session_saveBinary_* occasionally failingfrom time to time i saw ExportSysViewTest#testExportSysView_handler_session_saveBinary_* test failing. this doesn't occur consistently and i never managed to reproduce it when running the tests in the idea.
"
1,"Workspace operations (copy/clone) do not handle references correctlyREFERENCE properties created through Workspace.copy() or Workspace.clone() are not reflected by 
Node.getReferences() and are as a consequence not enforced."
1,"NPE if you open IW with CREATE on an index with no segments fileI have a simple test case that hits this NPE:

{noformat}
    [junit] java.lang.NullPointerException
    [junit] 	at java.io.File.<init>(File.java:305)
    [junit] 	at org.apache.lucene.store.NIOFSDirectory.openInput(NIOFSDirectory.java:67)
    [junit] 	at org.apache.lucene.store.FSDirectory.openInput(FSDirectory.java:333)
    [junit] 	at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:213)
    [junit] 	at org.apache.lucene.index.IndexFileDeleter.<init>(IndexFileDeleter.java:218)
    [junit] 	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:1113)
    [junit] 	at org.apache.lucene.index.TestIndexWriter.testNoSegmentFile(TestIndexWriter.java:4975)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:277)
{noformat}

It happens if you have an aborted index, ie, there are segment files in there (*.frq, *.tis, etc.) but no segments_N file, and then you try to open an IW with CREATE on that index."
1,"LuceneTaxonomyReader .decRef() may close the inner IR, renderring the LTR in a limbo.TaxonomyReader which supports ref-counting, has a decRef() method which delegates to an inner IndexReader and calls its .decRef(). The latter may close the reader (if the ref is zeroes) but the taxonomy would remain 'open' which will fail many of its method calls.

Also, the LTR's .close() method does not work in the same manner as IndexReader's - which calls decRef(), and leaves the real closing logic to the decRef(). I believe this should be the right approach for the fix."
1,TestNRTManager test failurereproduces for me
1,"Garbage collection deletes temporary files in FileDataStoreIn FileDataStore.addRecord(InputStream), a temporary file is created. The data is written to the file and then it is moved to its final location (based on the contents hash).

If the garbage collector runs whilst this temp file is present, it deletes it (on Solaris 10 at least), and the addRecord fails at the attempt to rename the now non-existent temp file.

I am attaching a minimal patch that prevents these temp files being deleted by deleteOlderRecursive(..), regardless of their lastModified() value.

I have made this a Minor priority, since there is the obvious workaround of disabling the GC.
"
1,"NPE in EventStateCollectionWhen removing a Version with a versionlabel and restoring an other Version from the same containing history within 1 transaction, a NPE occured. When debugging I noticed the method createEventStates was entered with an UUID from a versionLabel. The ChangeLog.get(id) returned null.

Caused by: java.lang.NullPointerException
	at org.apache.jackrabbit.core.observation.EventStateCollection.getNodeType(EventStateCollection.java:614)
	at org.apache.jackrabbit.core.observation.EventStateCollection.createEventStates(EventStateCollection.java:381)
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:697)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:1085)
	at org.apache.jackrabbit.core.state.XAItemStateManager.prepare(XAItemStateManager.java:163)
	at org.apache.jackrabbit.core.version.XAVersionManager.prepare(XAVersionManager.java:509)
	at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:154)
	at org.apache.jackrabbit.core.XASessionImpl.prepare(XASessionImpl.java:331)
	at org.springmodules.jcr.jackrabbit.support.JackRabbitUserTransaction.commit(JackRabbitUserTransaction.java:100)
	at org.springmodules.jcr.jackrabbit.LocalTransactionManager.doCommit(LocalTransactionManager.java:192)"
1,"Text.unescape() should should preserve 'unicode' charactersWhen an input to Text.unescape() contains characters > \u00ff, the most significant byte is lost resulting in garbled output. The unescape() function should preserve such characters in order to be useful to decode Internationalized Resource Identifiers (RFC 3987). "
1,"Node.checkin() throws ArrayIndexOutOfBoundsExceptionI get an ArrayIndexOutOfBoundsException for index 0 when checking-in a node. After drilling into the code I found, that during checkin, the jcr:uuid property (defined as OPV INITIALIZE) is not copied from the node to the frozen node.

After checkin though the implementation tries to access the string value of the jcr:uuid property, which is not existing, hence the internal property implementation throws the exception when accessing the first element in the empty value array.

As a workaround I currently the set jcr:uuid property to OPV=COPY in the mixin:referenceable node type. But I could imagine, that this might be incorrect according to the spec, yet it works in my use case."
1,jackrabbit-jcr-client tests fail (and are disabled in pom)I suggest to enable the tests and fix the issues causing them to fail. 
1,"Bundle cache is not rolled back when the storage of a ChangeLog failsThe bundle cache in the bundle persistence managers is not restored to its old state when the AbstractBundlePersistenceManager.store(ChangeLog changeLog) method throws an exception. If, for instance, the storage of references fails then the AbstractBundlePersistenceManager.putBundle(NodePropBundle bundle) method has already been called for all modified bundles. Because of the connection rollback, the bundle cache will be out-of-sync with the persistent state. As a result, the SharedItemStateManager will have an incorrect view of the persistent state.
Furthermore, if the blockOnConnectionLoss property is set to true, then the BundleDbPersistenceManager can be caught in an infinite loop because of invalid SQL inserts because of an incorrect bundle cache; see attached stacktrace."
1,"Large distances in Spatial go beyond Prime MEridianhttp://amidev.kaango.com/solr/core0/select?fl=*&json.nl=map&wt=json&radius=5000&rows=20&lat=39.5500507&q=honda&qt=geo&long=-105.7820674

Get an error when using Solr when distance is calculated for the boundary box past 90 degrees.


Aug 4, 2009 1:54:00 PM org.apache.solr.common.SolrException log
SEVERE: java.lang.IllegalArgumentException: Illegal lattitude value 93.1558669413734
        at org.apache.lucene.spatial.geometry.FloatLatLng.<init>(FloatLatLng.java:26)
        at org.apache.lucene.spatial.geometry.shape.LLRect.createBox(LLRect.java:93)
        at org.apache.lucene.spatial.tier.DistanceUtils.getBoundary(DistanceUtils.java:50)
        at org.apache.lucene.spatial.tier.CartesianPolyFilterBuilder.getBoxShape(CartesianPolyFilterBuilder.java:47)
        at org.apache.lucene.spatial.tier.CartesianPolyFilterBuilder.getBoundingArea(CartesianPolyFilterBuilder.java:109)
        at org.apache.lucene.spatial.tier.DistanceQueryBuilder.<init>(DistanceQueryBuilder.java:61)
        at com.pjaol.search.solr.component.LocalSolrQueryComponent.prepare(LocalSolrQueryComponent.java:151)
        at org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:174)
        at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:131)
        at org.apache.solr.core.SolrCore.execute(SolrCore.java:1328)
        at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:341)
        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:244)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:128)
        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102)
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)
        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:286)
        at org.apache.coyote.http11.Http11AprProcessor.process(Http11AprProcessor.java:857)
        at org.apache.coyote.http11.Http11AprProtocol$Http11ConnectionHandler.process(Http11AprProtocol.java:565)
        at org.apache.tomcat.util.net.AprEndpoint$Worker.run(AprEndpoint.java:1509)
        at java.lang.Thread.run(Thread.java:619)


"
1,"Uncaught AbstractMethodError exception in in DomUtil.createFactory()DomUtil.createFactory() throws an uncaught AbstractMethodError exception when xerces is on the classpath and the jackrabbit webdav module is used. 

This can render the class unusable when used in conjunction with the xerces library. 
"
1,"NullPointerException in BooleanFilter BooleanFilter getDISI() method used with QueryWrapperFilter occur NullPointerException,
if any QueryWrapperFilter not match terms in IndexReader.

---------------------------------------------------
java.lang.NullPointerException
	at org.apache.lucene.util.OpenBitSetDISI.inPlaceAnd(OpenBitSetDISI.java:66)
	at org.apache.lucene.search.BooleanFilter.getDocIdSet(BooleanFilter.java:102)
	at org.apache.lucene.search.IndexSearcher.searchWithFilter(IndexSearcher.java:551)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:532)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:463)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:433)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:356)
	at test.BooleanFilterTest.main(BooleanFilterTest.java:50)
---------------------------------------------------

null-check below lines.
---------------------------------------------------
res = new OpenBitSetDISI(getDISI(shouldFilters, i, reader), reader.maxDoc());
res.inPlaceOr(getDISI(shouldFilters, i, reader));
res = new OpenBitSetDISI(getDISI(notFilters, i, reader), reader.maxDoc());
res.inPlaceNot(getDISI(notFilters, i, reader));
res = new OpenBitSetDISI(getDISI(mustFilters, i, reader), reader.maxDoc());
res.inPlaceAnd(getDISI(mustFilters, i, reader));
---------------------------------------------------"
1,"Using WildcardQuery with MultiSearcher, and Boolean MUST_NOT clauseWe are searching across multiple indices using a MultiSearcher. There seems to be a problem when we use a WildcardQuery to exclude documents from the result set. I attach a set of unit tests illustrating the problem.

In these tests, we have two indices. Each index contains a set of documents with fields for 'title',  'section' and 'index'. The final aim is to do a keyword search, across both indices, on the title field and be able to exclude documents from certain sections (and their subsections) using a
WildcardQuery on the section field.
 
 e.g. return documents from both indices which have the string 'xyzpqr' in their title but which do not lie
 in the news section or its subsections (section = /news/*).
 
The first unit test (testExcludeSectionsWildCard) fails trying to do this.
 If we relax any of the constraints made above, tests pass:
 
* Don't use WildcardQuery, but pass in the news section and it's child section to exclude explicitly (testExcludeSectionsExplicit)</li>
* Exclude results from just one section, not it's children too i.e. don't use WildcardQuery(testExcludeSingleSection)</li>
* Do use WildcardQuery, and exclude a section and its children, but just use one index thereby using the simple
   IndexReader and IndexSearcher objects (testExcludeSectionsOneIndex).
* Try the boolean MUST clause rather than MUST_NOT using the WildcardQuery i.e. only include results from the /news/ section
   and its children."
1,"Extend mimetype list of text extractorsDo you think it would be possible to extend the mimetype list of the
MsPowerpoint and MsExcel textextractors with ""application/powerpoint"" and
""application/excel""? 

It just took me half an hour to figure out why my
documents didn't turn up in a jackrabbit fulltext-search and maybe other
users might run into the same problem...

I'm not sure if there is some kind of standard which lists the possible
default mimetypes but after a quick google search it seems to me that they
are not that uncommon.
"
1,"Fix and simplify CryptedSimpleCredentialsthe credentials retrieved from UserImpl and used to validate the simplecredentials passed to the repository login is overly complex
and buggy as it tries to match all kind credentials variants with and without hashed password.
in particular it contains the following problems:
- simplecredentials containing the hashed pw are considered valid
- passwords startign with {something} cause inconsistencies and may even prevent the user from login

it should be improved as follows:
- simplecredentials are always expected to contain the plain text password both for creation and
  comparison with the cryptedsimplecredentials.
- creating cryptedsimplecredentials from uid/pw however is left unchanged: the specified pw is
  hashed with the default algorithm if it turns out not to be in the hashed format.
- in addition the pw should also be hashed if it has the form {something}whatever but something
  is an invalid algorithm.
"
1,"KeywordTokenizer does not properly set the end offsetKeywordTokenizer sets the Token's term length attribute but appears to omit the end offset. The issue was discovered while using a highlighter with the KeywordAnalyzer. KeywordAnalyzer delegates to KeywordTokenizer propagating the bug. 

Below is a JUnit test (source is also attached) that exercises various analyzers via a Highlighter instance. Every analyzer but the KeywordAnazlyzer successfully wraps the text with the highlight tags, such as ""<b>thetext</b>"". When using KeywordAnalyzer the tags appear before the text, for example: ""<b></b>thetext"". 

Please note NewKeywordAnalyzer and NewKeywordTokenizer classes below. When using NewKeywordAnalyzer the tags are properly placed around the text. The NewKeywordTokenizer overrides the next method of the KeywordTokenizer setting the end offset for the returned Token. NewKeywordAnalyzer utilizes KeywordTokenizer to produce proper token.

Unless there is an objection I will gladly post a patch in the very near future . 

-----------------------------
package lucene;

import java.io.IOException;
import java.io.Reader;

import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.analysis.KeywordAnalyzer;
import org.apache.lucene.analysis.KeywordTokenizer;
import org.apache.lucene.analysis.SimpleAnalyzer;
import org.apache.lucene.analysis.StopAnalyzer;
import org.apache.lucene.analysis.Token;
import org.apache.lucene.analysis.TokenStream;
import org.apache.lucene.analysis.Tokenizer;
import org.apache.lucene.analysis.WhitespaceAnalyzer;
import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.search.highlight.Highlighter;
import org.apache.lucene.search.highlight.QueryScorer;
import org.apache.lucene.search.highlight.SimpleHTMLFormatter;
import org.apache.lucene.search.highlight.WeightedTerm;
import org.junit.Test;
import static org.junit.Assert.*;

public class AnalyzerBug {

	@Test
	public void testWithHighlighting() throws IOException {
		String text = ""thetext"";
		WeightedTerm[] terms = { new WeightedTerm(1.0f, text) };

		Highlighter highlighter = new Highlighter(new SimpleHTMLFormatter(
				""<b>"", ""</b>""), new QueryScorer(terms));

		Analyzer[] analazers = { new StandardAnalyzer(), new SimpleAnalyzer(),
				new StopAnalyzer(), new WhitespaceAnalyzer(),
				new NewKeywordAnalyzer(), new KeywordAnalyzer() };

		// Analyzers pass except KeywordAnalyzer
		for (Analyzer analazer : analazers) {
			String highighted = highlighter.getBestFragment(analazer,
					""CONTENT"", text);
			assertEquals(""Failed for "" + analazer.getClass().getName(), ""<b>""
					+ text + ""</b>"", highighted);
			System.out.println(analazer.getClass().getName()
					+ "" passed, value highlighted: "" + highighted);
		}
	}
}

class NewKeywordAnalyzer extends KeywordAnalyzer {

	@Override
	public TokenStream reusableTokenStream(String fieldName, Reader reader)
			throws IOException {
		Tokenizer tokenizer = (Tokenizer) getPreviousTokenStream();
		if (tokenizer == null) {
			tokenizer = new NewKeywordTokenizer(reader);
			setPreviousTokenStream(tokenizer);
		} else
			tokenizer.reset(reader);
		return tokenizer;
	}

	@Override
	public TokenStream tokenStream(String fieldName, Reader reader) {
		return new NewKeywordTokenizer(reader);
	}
}

class NewKeywordTokenizer extends KeywordTokenizer {
	public NewKeywordTokenizer(Reader input) {
		super(input);
	}

	@Override
	public Token next(Token t) throws IOException {
		Token result = super.next(t);
		if (result != null) {
			result.setEndOffset(result.termLength());
		}
		return result;
	}
}
"
1,"workspace.copy causes 2 nodes in the same workspace to have the same version historyworkspace.copy creates a copy of a versionable node with a new uuid which share the same version. ""In a given workspace, there is at most one versionable node per version history"" (4.11 spec)"
1,"need a test that uses termsenum.seekExact() (which returns true), then calls next()i tried to do some seekExact (where the result must exist) then next()ing in the faceting module,
and it seems like there could be a bug here.

I think we should add a test that mixes seekExact/seekCeil/next like this, to ensure that
if seekExact returns true, that the enum is properly positioned."
1,"Authentication fails with proxied SSL ConnectionsWhen connecting through a proxy, using SSL and authentication HttpClient winds 
up sending a GET request to the proxy after the initial auth required response, 
the proxy then obviously responds with a not implemented response since it 
can't handle a GET request to an SSL URL.  In essence the following is 
happening:

1. HttpClient sends Connect response.
2. Proxy responds 200 Connect OK
3. HttpClient uses SSL connection to send the request to the web server.
4. Web server responds with not authorized and closes the connection.
5. HttpClient opens a new connection to the proxy and issues a GET request for 
the SSL URL.
6. Proxy returns 501 not implemented.

I'll attach a full log to this bug.

This is likely to be hard to fix since the retry is performed in HttpMethodBase 
but the Connect method is executed by HttpClient so a fix for this may be best 
waiting for 2.1.  This looks very similar to HTTPCLIENT-195 except that that bug is 
marked as fixed and this one still doesn't work, this also applies to 
authentication schemes other than NTLM (testing NTLM and basic).

My best evaluation is that the web server returns Connection: close when it 
rejects the authorization attempt and then HttpMethodBase is incapable of 
creating a new SSL connection through the proxy.  The only thing I can think of 
that could be done prior to 2.1 to fix this is to send a Connection: keep-alive 
as well as the Proxy-Connection: Keep-Alive we're already sending with the 
original request."
1,"Removal of a node with shared subnodes failsA simple testcase:

Set up (first transaction):
Node a1 = testRootNode.addNode(""a1"");
Node a2 = testRootNode.addNode(""a2"");
a2.addMixin(""mix:shareable"");
session.save();
// now we have a shareable node N with path a2

Workspace workspace = session.getWorkspace();
String path = a1.getPath() + ""/b1"";
workspace.clone(workspace.getName(), a2.getPath(), path, false);
session.save();
// now we have another shareable node N' in the same shared set as N with path a1/b1

Test(second transaction):
testRootNode.remove(""a1"");
session.save();

At least in a transactional repository the node will not be removed, an error will be thrown instead."
1,"testIWondiskfull unreferenced files failureNOTE: reproduce with: ant test -Dtestcase=TestIndexWriterOnDiskFull -Dtestmethod=testAddDocumentOnDiskFull -Dtests.seed=aff9b14dd518cfb:4d2f112726e2947f:-2b03094a43a947ee -Dtests.multiplier=3 -Dargs=""-Dfile.encoding=ISO8859-1""

Reproduces some of the time..."
1,"save() might create new transient propertiesIt seems that when a new node is saved through the parent node, new properties might get created, which are not saved. To persist those properties the new node must be saved again.

Example:

(Consider a mixin type ""extVer"" extending the standard type mix:versionable.)

      Node node = parent.addNode(""newNode"", ""nt:base"");
      node.addMixin(""extVer"");
      // ""mix:versionable"" properties do not exist here
      
      // save the new node
      parent.save();

      // now ""mix:versionable"" properties like ""jcr:isCheckedOut""
      // exist in the ""node"" but:
      //    node.getProperty(""jcr:isCheckedOut"").isNew() == true
      // fix:
      node.save();

If the last node.save() opertation would not be done, a RepositoryException would result if a node.checkIn() would be done immediately after parent.save().

This seems counterintuitive and seems like an error. I wonder whether the properties should not be added upon ""node.addMixin"" ? At least ""parent.save()"" should (or might I say must ?) not only add the properties but also save them."
1,"contrib/benchmark assumes Locale.US for parsing dates in Reuters collectionSimpleDateFormat used for parsing dates in Reuters documents is instantiated without specifying a locale. So it is using the default locale. If that happens to be US, it will work. But for another locale a parse exception is likely.

Affects both StandardBenchmarker and ReutersDocMaker.

Fix is trivial - specify Locale.US for SimpleDateFormat's constructor.
"
1,"Changes from Session.move() to a top-level node aren't seen in a second sessionI'll attach a test case, but basically...

* Create two sessions
* Create a top-level node in the first session and save it.
* Move the top-level node using the first session
* In the second session, try itemExists() for the path of the node. It returns true when it should be false."
1,"Intermitted failure on DocValues branchI lately ran into two random failures on the CSF branch that seem not to be related to docValues but I can't reproduce them neither on docvalues branch nor on trunk.

{code}
jError Message

IndexFileDeleter doesn't know about file _1e.tvx
Stacktrace

junit.framework.AssertionFailedError: IndexFileDeleter doesn't know about file _1e.tvx
	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:979)
	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:917)
	at org.apache.lucene.index.IndexWriter.filesExist(IndexWriter.java:3633)
	at org.apache.lucene.index.IndexWriter.startCommit(IndexWriter.java:3699)
	at org.apache.lucene.index.IndexWriter.prepareCommit(IndexWriter.java:2407)
	at org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:2478)
	at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2460)
	at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2444)
	at org.apache.lucene.index.TestIndexWriterExceptions.testRandomExceptionsThreads(TestIndexWriterExceptions.java:213)
Standard Output

NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterExceptions -Dtestmethod=testRandomExceptionsThreads -Dtests.seed=-6528669668419768890:4860241142852689334 -Dtests.codec=randomPerField -Dtests.multiplier=3
NOTE: test params are: codec=PreFlex, locale=sv, timezone=Atlantic/South_Georgia
Standard Error

NOTE: all tests run in this JVM:
[TestDemo, TestToken, TestBinaryDocument, TestCodecs, TestDirectoryReader, TestIndexInput, TestIndexWriterExceptions]
{code}

and

{code}

[junit] Testsuite: org.apache.lucene.index.TestIndexReaderReopen
    [junit] Testcase: testThreadSafety(org.apache.lucene.index.TestIndexReaderReopen):	Caused an ERROR
    [junit] MockDirectoryWrapper: cannot close: there are still open files: {_4_3.frq=1, _4_3.pos=1, _4_0.frq=1, _4_0.prx=1, _4.pst=1, _4_3.pyl=1, _4_3.skp=1, _4_0.tis=1, _4_3.doc=1, _4_3.tis=1}
    [junit] java.lang.RuntimeException: MockDirectoryWrapper: cannot close: there are still open files: {_4_3.frq=1, _4_3.pos=1, _4_0.frq=1, _4_0.prx=1, _4.pst=1, _4_3.pyl=1, _4_3.skp=1, _4_0.tis=1, _4_3.doc=1, _4_3.tis=1}
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:387)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen.testThreadSafety(TestIndexReaderReopen.java:859)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:979)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:917)
    [junit] Caused by: java.lang.RuntimeException: unclosed IndexInput
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.openInput(MockDirectoryWrapper.java:342)
    [junit] 	at org.apache.lucene.store.Directory.openInput(Directory.java:122)
    [junit] 	at org.apache.lucene.index.codecs.standard.StandardPostingsReader.<init>(StandardPostingsReader.java:49)
    [junit] 	at org.apache.lucene.index.codecs.standard.StandardCodec.fieldsProducer(StandardCodec.java:87)
    [junit] 	at org.apache.lucene.index.PerFieldCodecWrapper$FieldsReader.<init>(PerFieldCodecWrapper.java:119)
    [junit] 	at org.apache.lucene.index.PerFieldCodecWrapper.fieldsProducer(PerFieldCodecWrapper.java:211)
    [junit] 	at org.apache.lucene.index.SegmentReader$CoreReaders.<init>(SegmentReader.java:137)
    [junit] 	at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:532)
    [junit] 	at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:509)
    [junit] 	at org.apache.lucene.index.DirectoryReader.<init>(DirectoryReader.java:238)
    [junit] 	at org.apache.lucene.index.DirectoryReader.doReopen(DirectoryReader.java:500)
    [junit] 	at org.apache.lucene.index.DirectoryReader.access$000(DirectoryReader.java:48)
    [junit] 	at org.apache.lucene.index.DirectoryReader$2.doBody(DirectoryReader.java:493)
    [junit] 	at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:623)
    [junit] 	at org.apache.lucene.index.DirectoryReader.doReopenNoWriter(DirectoryReader.java:488)
    [junit] 	at org.apache.lucene.index.DirectoryReader.doReopen(DirectoryReader.java:446)
    [junit] 	at org.apache.lucene.index.DirectoryReader.reopen(DirectoryReader.java:406)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen$9.run(TestIndexReaderReopen.java:770)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen$ReaderThread.run(TestIndexReaderReopen.java:897)
    [junit] 
    [junit] 
    [junit] Tests run: 17, Failures: 0, Errors: 1, Time elapsed: 13.766 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexReaderReopen -Dtestmethod=testThreadSafety -Dtests.seed=-5455993123574190959:-1935535300313439968 -Dtests.codec=randomPerField -Dtests.multiplier=3
    [junit] NOTE: test params are: codec=RandomCodecProvider: {field5=MockVariableIntBlock(baseBlockSize=29), id=Standard, fielda=Standard, field4=MockFixedIntBlock(blockSize=924), field3=Standard, field2=SimpleText, id2=Standard, field6=MockSep, field1=Pulsing(freqCutoff=8)}, locale=zh_CN, timezone=Asia/Hovd
{code}

I haven't seen those before - let me know if you have!"
1,"Be consistent about negative vInt/vLongToday, write/readVInt ""allows"" a negative int, in that it will encode and decode correctly, just horribly inefficiently (5 bytes).

However, read/writeVLong fails (trips an assert).

I'd prefer that both vInt/vLong trip an assert if you ever try to write a negative number... it's badly trappy today.  But, unfortunately, we sometimes rely on this... had we had this assert in 'since the beginning' we could have avoided that.

So, if we can't add that assert in today, I think we should at least fix readVLong to handle negative longs... but then you quietly spend 9 bytes (even more trappy!)."
1,"smartcn analyzer throw NullPointer exception when the length of analysed text over 32767That's all because of org.apache.lucene.analysis.cn.smart.hhmm.SegGraph's makeIndex() method:
  public List<SegToken> makeIndex() {
    List<SegToken> result = new ArrayList<SegToken>();
    int s = -1, count = 0, size = tokenListTable.size();
    List<SegToken> tokenList;
    short index = 0;
    while (count < size) {
      if (isStartExist(s)) {
        tokenList = tokenListTable.get(s);
        for (SegToken st : tokenList) {
          st.index = index;
          result.add(st);
          index++;
        }
        count++;
      }
      s++;
    }
    return result;
  }

here 'short index = 0;' should be 'int index = 0;'. And that's reported here http://code.google.com/p/imdict-chinese-analyzer/issues/detail?id=2 and http://code.google.com/p/imdict-chinese-analyzer/issues/detail?id=11, the author XiaoPingGao have already fixed this bug:http://code.google.com/p/imdict-chinese-analyzer/source/browse/trunk/src/org/apache/lucene/analysis/cn/smart/hhmm/SegGraph.java"
1,"Chunked Stream Encoding Problems Fails to throw ExceptionsUsing the HttpClient 2.0.1 with Sun's JDK 1.4.1_01 and connecting to a site 
that appareantly has problems generating proper chunked output causes the http 
client to catch and log an exception then return null data. Ideally the http 
client should throw the IOException to the calling class so that it can be 
handled by the programmer. It's not a problem that an exception is being 
generated it is a bug that the exception is being trapped in the somewhere in 
the httpclient code.

2004-08-27 21:19:01,013 main HttpMethodBase [ERROR]: I/O failure reading 
response body
java.io.IOException: chunked stream ended unexpectedly
        at 
org.apache.commons.httpclient.ChunkedInputStream.getChunkSizeFromInputStream
(ChunkedInputStream.java:234)
        at org.apache.commons.httpclient.ChunkedInputStream.nextChunk
(ChunkedInputStream.java:205)
        at org.apache.commons.httpclient.ChunkedInputStream.read
(ChunkedInputStream.java:160)
        at java.io.FilterInputStream.read(FilterInputStream.java:111)
        at org.apache.commons.httpclient.AutoCloseInputStream.read
(AutoCloseInputStream.java:110)
        at java.io.FilterInputStream.read(FilterInputStream.java:90)
        at org.apache.commons.httpclient.AutoCloseInputStream.read
(AutoCloseInputStream.java:129)
        at org.apache.commons.httpclient.HttpMethodBase.getResponseBody
(HttpMethodBase.java:685)
        at com.algorim.ei.cets.EmailPreProcessor.processMessage
(EmailPreProcessor.java:565)
        at com.algorim.ei.cets.EmailUpdate.run(EmailUpdate.java:332)
        at com.algorim.ei.cets.EmailUpdate.main(EmailUpdate.java:89)

Request and response that are causing the error:
GET /aeq.aspx?k=32226&k=sb1313@xcorp5.com HTTP/1.1
User-Agent: Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.0; .NET CLR 
1.1.4322)
Host: 38.117.227.56

HTTP/1.1 200 OK
Date: Fri, 27 Aug 2004 20:55:27 GMT
Server: Microsoft-IIS/6.0
X-Powered-By: ASP.NET
X-AspNet-Version: 1.1.4322
Transfer-Encoding: chunked
Cache-Control: private
Content-Type: text/html; charset=utf-8

179
<html><head><META HTTP-EQUIV=Refresh CONTENT=""1; 
URL=http://www.datingresults.com/default.asp?
p=7090&PRM=38664""</head><body><script>win2=win
dow.open('http://m.qmct.com/images/d.html?
a=1', 'newwin','toolbar=0,width=730,height=500');if (win2 != null) win2.blur
();window.focus();wind
ow.location = 'http://www.datingresults.com/default.asp?
p=7090&PRM=38664';</script></body></html>"
1,"IndexWriter can flush too early when flushing by RAM usageThere is a silly bug in how DocumentsWriter tracks its RAM usage:
whenever term vectors are enabled, it incorrectly counts the space
used by term vectors towards flushing, when in fact this space is
recycled per document.

This is not a functionality bug.  All it causes is flushes to happen
too frequently, and, IndexWriter will use less RAM than you asked it
to.  To work around it you can simply give it a bigger RAM buffer.

I will commit a fix shortly."
1,DWPT doesn't see changes to DW#infoStreamDW does not push infostream changes to DWPT since DWPT#infoStream is final and initialized on DWPTPool initialization (at least for initial DWPT) we should push changes to infostream to DWPT too
1,"NPE doing local sensitive sorting when sort field is missingIf you do a local sensitive sort against a field that is missing from some documents in the index an NPE will get thrown.

Attached is a patch which resolved the issue and updates the sort test case to give coverage to this issue."
1,DWFlushControl does not take active DWPT out of the loop on fullFlushWe have seen several OOM on TestNRTThreads and all of them are caused by DWFlushControl missing DWPT that are set as flushPending but can't full due to a full flush going on. Yet that means that those DWPT are filling up in the background while they should actually be checked out and blocked until the full flush finishes. Even further we currently stall on the maxNumThreadStates while we should stall on the num of active thread states. I will attach a patch tomorrow.
1,"relative URIs with internal double-slashes ('//') misparsedURI.parseUriReference()'s heuristic for interpreting URI parts is thrown off by relative URIs which include an internal '//'. As a result, portions of the supplied relative URI (path) can be lost. 

For example:

URI rel = new URI(""foo//bar//baz"");
rel.toString();
(java.lang.String) //bar//baz

The culprit seems to be line 1961 of URI improperly concluding that two slashes later than the beginning of 'tmp' are still indicative the URI is a 'net_path'. 

A possible quick fix might be to add a '!isStartedFromPath &&' to the beginning of the line 1961 test, making the line:

            if (!isStartedFromPath && at + 2 < length && tmp.charAt(at + 1) == '/') {

... and thus preventing the misguided authority-parsing from happening when earlier analysis already identified the current string as a strictly path-oriented URI.

(It also appears the setting of the is_net_path boolean at the end of this if's block may be wrong; this code is run for hier_path URIs that are not net_paths in the 2396 syntax. For example:

URI uri = new URI(""http://www.example.com/some/page"");
uri.isNetPath();
 (boolean) true 

)"
1,"import of multivalue properties with single value results in incorrect property creationWhen importing a file exported with system view, a value of a multivalued property is stored as a singlevalue property. The bug seems to be that for some reason, even if PropDef.isMultiple() is true for a given property, no ValueFormatException is thrown when setting the property as single value.

Workaround:

It works if I change PropInfo.apply() line 136 to 

if (va.length == 1 && !def.isMultiple()) {
...

"
1,"XML import always throws ItemExistsException when trying to overwrite existing nodesAccording to the JCR-API, it should be possible to govern the import of XML serialized referenceable nodes in case of UUID collision. Unfortunately, the UUID conflict is handled too late during import, an ItemExistsException is always thrown beforehand due to not allowed same-name-siblings.

Simply try to import a previously exported referenceable node twice, providing either

- ImportUUIDBehavior.IMPORT_UUID_COLLISION_REMOVE_EXISTING or
- ImportUUIDBehavior.IMPORT_UUID_COLLISION_REPLACE_EXISTING.

This will fail and result in an ItemExistsException."
1,"CJKTokenizer generates tokens with incorrect offsetsIf I index a Japanese *multi-valued* document with CJKTokenizer and highlight a term with FastVectorHighlighter, the output snippets have incorrect highlighted string. I'll attach a program that reproduces the problem soon."
1,"StandardCodec sometimes supplies skip pointers past EOFPretty sure this is 4.0-only:
I added an assertion, the test to reproduce is:

ant test-core -Dtestcase=TestPayloadNearQuery -Dtestmethod=testMinFunction -Dtests.seed=4841190615781133892:3888521539169738727 -Dtests.multiplier=3

{noformat}
    [junit] Testcase: testMinFunction(org.apache.lucene.search.payloads.TestPayloadNearQuery):  FAILED
    [junit] invalid skip pointer: 404, length=337
    [junit] junit.framework.AssertionFailedError: invalid skip pointer: 404, length=337
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1127)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1059)
    [junit]     at org.apache.lucene.index.codecs.MultiLevelSkipListReader.init(MultiLevelSkipListReader.java:176)
    [junit]     at org.apache.lucene.index.codecs.standard.DefaultSkipListReader.init(DefaultSkipListReader.java:50)
    [junit]     at org.apache.lucene.index.codecs.standard.StandardPostingsReader$SegmentDocsAndPositionsAndPayloadsEnum.advance(StandardPostingsReader.java:742)
    [junit]     at org.apache.lucene.search.spans.TermSpans.skipTo(TermSpans.java:72)
{noformat}"
1,"HttpConnectionParams.setConnectionTimeout(int) has no effect if host unreachableI have just modified MultiThreadedExample.java by adding
httpClient.getHttpConnectionManager().getParams().setConnectionTimeout(5000); in
order to set a connection timeout on the client side. Then I have added a LAN
url to urisToGet array. The ip of this url (""http://192.168.254.1/"") is not
assigned to any computer.

After running the client, I get the expected message ( error:
org.apache.commons.httpclient.ConnectTimeoutException: The host did not accept
the connection within timeout of 5000 ms) but only after 20 seconds.

I use java version ""1.5.0_04"". This is not a JVM bug since normal connection
procedure times out after 5 seconds as expected:
        SocketAddress addr = new InetSocketAddress(""192.168.254.1"", 80);
        try {
            
            SocketChannel channel = SocketChannel.open();
            channel.socket().connect(addr, 5000);            
            System.out.println(""connected"");
            
        } catch (Exception e) {
            e.printStackTrace();
        }"
1,"QueryParser.getFieldQuery(String,String) doesn't set default slop on MultiPhraseQuerythere seems to have been an oversight in calling mph.setSlop(phraseSlop) in QueryParser.getFieldQuery(String,String).  The result being that in some cases, the ""default slop"" value doesnt' get set right (sometimes, ... see below).

when i tried amending TestMultiAnalyzer to demonstrate the problem, I discovered that the grammer aparently always calls getFieldQuery(String,String,int) -- even if no ""~slop"" was specified in the text being parsed, in which case it passes the default as if it were specified.
(just to clarify: i haven't comfirmed this from a detailed reading of the grammer/code, it's just what i've deduced based on observation of the test)

The problem isn't entirely obvious unless you have a subclasses of QueryParser and try to call getFieldQuery(String,String) directly.   

In my case, I had overridden getFieldQuery(String,String) to call super.getFieldQuery(String,String) and wrap the result in a DisjunctionMaxQuery ... I don't care about supporting the ~slop syntax, but i do care about the default slop and i wasn't getting lucky the way QueryParser does, because getFieldQuery(String,String,int) wasn't getting back something it could call setSlop() with the (default) value it got from the javacc generated code.

My description may not make much sense, but hopefull the test patch i'm about to attach will.  The fix is also in the patch, and is fairly trivial.

(disclaimer: i don't have javacc installed, so I tested this patch by manually making the change to both QueryParser.java ... it should only be commited by someone with javacc who can regen the java file and confirm that my jj change doesn't have some weird bug in it)



"
1,"Redirection of a POST methodI execute a PostMethod to an URL which redirects me to a HTML page. If I set 
follow redirects to true the HttpClient wants to execute once more a POST. Of 
course a POST is not allowed to HTML pages. I think the HttpClient should 
exectue a GET method instead. That's also what is in the RFC2616:

10.3 Redirection 3xx

   This class of status code indicates that further action needs to be
   taken by the user agent in order to fulfill the request.  The action
   required MAY be carried out by the user agent without interaction
   with the user if and only if the method used in the second request is
   GET or HEAD. A client SHOULD detect infinite redirection loops, since
   such loops generate network traffic for each redirection.

      Note: previous versions of this specification recommended a
      maximum of five redirections. Content developers should be aware
      that there might be clients that implement such a fixed
      limitation."
1,"literal plus (+) character in path components of HttpURL is not preserved.When a literal plus character is included in the path component of an URL, it is
not encoded, but get decoded during getPath() to a space.

Reproducible with the following:

HttpURL httpURL = new HttpURL(""http://localhost/test+test"");
System.out.println(httpURL.getPath());

Output:
""test test""

The following path fixes the issue (This patch does not appear to break anything
 else):

Patch against SVN Repo:
URL: http://svn.apache.org/repos/asf/jakarta/commons/proper/httpclient/trunk
Repository UUID: 13f79535-47bb-0310-9956-ffa450edef68
Revision: 405803

Index: src/java/org/apache/commons/httpclient/URI.java
===================================================================
--- src/java/org/apache/commons/httpclient/URI.java (revision 405803)
+++ src/java/org/apache/commons/httpclient/URI.java (working copy)
@@ -1552,6 +1552,7 @@
         allowed_abs_path.or(abs_path);
         // allowed_abs_path.set('/');  // aleady included
         allowed_abs_path.andNot(percent);
+        allowed_abs_path.clear('+');
     }


@@ -1563,6 +1564,7 @@
     static {
         allowed_rel_path.or(rel_path);
         allowed_rel_path.clear('%');
+        allowed_rel_path.clear('+');
     }"
1,"TestSimpleExplanations failure{noformat}
ant test -Dtestcase=TestSimpleExplanations -Dtestmethod=testDMQ8 -Dtests.seed=-7e984babece66153:3e3298ae627b33a9:3093059db62bcc71
{noformat}

fails w/ this on current trunk... looks like silly floating point precision issue:

{noformat}

    [junit] Testsuite: org.apache.lucene.search.TestSimpleExplanations
    [junit]   1.4508595 = (MATCH) sum of:
    [junit]     1.4508595 = (MATCH) weight(field:yy in 2) [DefaultSimilarity], result of:
    [junit]       1.4508595 = score(doc=2,freq=1.0 = termFreq=1
    [junit] ), product of:
    [junit]         1.287682 = queryWeight, product of:
    [junit]           1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]           1.0 = queryNorm
    [junit]         1.1267219 = fieldWeight in 2, product of:
    [junit]           1.0 = tf(freq=1.0), with freq of:
    [junit]             1.0 = termFreq=1
    [junit]           1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]           0.875 = fieldNorm(doc=2)
    [junit]   145085.95 = (MATCH) weight(field:xx^100000.0 in 2) [DefaultSimilarity], result of:
    [junit]     145085.95 = score(doc=2,freq=1.0 = termFreq=1
    [junit] ), product of:
    [junit]       128768.2 = queryWeight, product of:
    [junit]         100000.0 = boost
    [junit]         1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]         1.0 = queryNorm
    [junit]       1.1267219 = fieldWeight in 2, product of:
    [junit]         1.0 = tf(freq=1.0), with freq of:
    [junit]           1.0 = termFreq=1
    [junit]         1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]         0.875 = fieldNorm(doc=2)
    [junit]  expected:<145086.66> but was:<145086.69>)
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.544 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestSimpleExplanations -Dtestmethod=testDMQ8 -Dtests.seed=144152895b276837:eb7ba4953db943f:33373b79a971db02
    [junit] NOTE: test params are: codec=PreFlex, sim=RandomSimilarityProvider(queryNorm=false,coord=false): {field=DefaultSimilarity, alt=DFR I(ne)LZ(0.3), KEY=IB LL-D2}, locale=en_IN, timezone=Pacific/Samoa
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestSimpleExplanations]
    [junit] NOTE: Linux 2.6.33.6-147.fc13.x86_64 amd64/Sun Microsystems Inc. 1.6.0_21 (64-bit)/cpus=24,threads=1,free=130426744,total=189988864
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testDMQ8(org.apache.lucene.search.TestSimpleExplanations):	FAILED
    [junit] ((field:yy field:w5^100.0) | field:xx^100000.0)~0.5: score(doc=2)=145086.66 != explanationScore=145086.69 Explanation: 145086.69 = (MATCH) max plus 0.5 times others of:
    [junit]   1.4508595 = (MATCH) sum of:
    [junit]     1.4508595 = (MATCH) weight(field:yy in 2) [DefaultSimilarity], result of:
    [junit]       1.4508595 = score(doc=2,freq=1.0 = termFreq=1
    [junit] ), product of:
    [junit]         1.287682 = queryWeight, product of:
    [junit]           1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]           1.0 = queryNorm
    [junit]         1.1267219 = fieldWeight in 2, product of:
    [junit]           1.0 = tf(freq=1.0), with freq of:
    [junit]             1.0 = termFreq=1
    [junit]           1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]           0.875 = fieldNorm(doc=2)
    [junit]   145085.95 = (MATCH) weight(field:xx^100000.0 in 2) [DefaultSimilarity], result of:
    [junit]     145085.95 = score(doc=2,freq=1.0 = termFreq=1
    [junit] ), product of:
    [junit]       128768.2 = queryWeight, product of:
    [junit]         100000.0 = boost
    [junit]         1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]         1.0 = queryNorm
    [junit]       1.1267219 = fieldWeight in 2, product of:
    [junit]         1.0 = tf(freq=1.0), with freq of:
    [junit]           1.0 = termFreq=1
    [junit]         1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]         0.875 = fieldNorm(doc=2)
    [junit]  expected:<145086.66> but was:<145086.69>
    [junit] junit.framework.AssertionFailedError: ((field:yy field:w5^100.0) | field:xx^100000.0)~0.5: score(doc=2)=145086.66 != explanationScore=145086.69 Explanation: 145086.69 = (MATCH) max plus 0.5 times others of:
    [junit]   1.4508595 = (MATCH) sum of:
    [junit]     1.4508595 = (MATCH) weight(field:yy in 2) [DefaultSimilarity], result of:
    [junit]       1.4508595 = score(doc=2,freq=1.0 = termFreq=1
    [junit] ), product of:
    [junit]         1.287682 = queryWeight, product of:
    [junit]           1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]           1.0 = queryNorm
    [junit]         1.1267219 = fieldWeight in 2, product of:
    [junit]           1.0 = tf(freq=1.0), with freq of:
    [junit]             1.0 = termFreq=1
    [junit]           1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]           0.875 = fieldNorm(doc=2)
    [junit]   145085.95 = (MATCH) weight(field:xx^100000.0 in 2) [DefaultSimilarity], result of:
    [junit]     145085.95 = score(doc=2,freq=1.0 = termFreq=1
    [junit] ), product of:
    [junit]       128768.2 = queryWeight, product of:
    [junit]         100000.0 = boost
    [junit]         1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]         1.0 = queryNorm
    [junit]       1.1267219 = fieldWeight in 2, product of:
    [junit]         1.0 = tf(freq=1.0), with freq of:
    [junit]           1.0 = termFreq=1
    [junit]         1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]         0.875 = fieldNorm(doc=2)
    [junit]  expected:<145086.66> but was:<145086.69>
    [junit] 	at org.apache.lucene.search.CheckHits.verifyExplanation(CheckHits.java:324)
    [junit] 	at org.apache.lucene.search.CheckHits$ExplanationAsserter.collect(CheckHits.java:494)
    [junit] 	at org.apache.lucene.search.Scorer.score(Scorer.java:60)
    [junit] 	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:580)
    [junit] 	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:363)
    [junit] 	at org.apache.lucene.search.CheckHits.checkExplanations(CheckHits.java:302)
    [junit] 	at org.apache.lucene.search.QueryUtils.checkExplanations(QueryUtils.java:92)
    [junit] 	at org.apache.lucene.search.QueryUtils.check(QueryUtils.java:126)
    [junit] 	at org.apache.lucene.search.QueryUtils.check(QueryUtils.java:122)
    [junit] 	at org.apache.lucene.search.QueryUtils.check(QueryUtils.java:106)
    [junit] 	at org.apache.lucene.search.CheckHits.checkHitCollector(CheckHits.java:89)
    [junit] 	at org.apache.lucene.search.TestExplanations.qtest(TestExplanations.java:99)
    [junit] 	at org.apache.lucene.search.TestSimpleExplanations.testDMQ8(TestSimpleExplanations.java:224)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$2$1.evaluate(LuceneTestCase.java:611)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:148)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:50)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.search.TestSimpleExplanations FAILED
{noformat}"
1,"XMLTextFilter does not extract text elementsXMLTextFilter only returns the text from attributes, not the content of text elements,"
1,"Jenkins builds hang quite often in TestIndexWriterWithThreads.testCloseWithThreadsLast hung test run: [https://builds.apache.org/job/Lucene-Solr-tests-only-trunk/10638/console]

{noformat}
[junit] ""main"" prio=5 tid=0x0000000801ef3800 nid=0x1965c waiting on condition [0x00007fffffbfd000]
[junit]    java.lang.Thread.State: WAITING (parking)
[junit] 	at sun.misc.Unsafe.park(Native Method)
[junit] 	- parking to wait for  <0x0000000825d853a8> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
[junit] 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
[junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:838)
[junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:871)
[junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1201)
[junit] 	at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)
[junit] 	at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)
[junit] 	at org.apache.lucene.index.DocumentsWriterFlushControl.markForFullFlush(DocumentsWriterFlushControl.java:403)
[junit] 	at org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:557)
[junit] 	- locked <0x0000000825d81998> (a org.apache.lucene.index.DocumentsWriter)
[junit] 	at org.apache.lucene.index.IndexWriter.prepareCommit(IndexWriter.java:2776)
[junit] 	- locked <0x0000000825d7d840> (a java.lang.Object)
[junit] 	at org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:2904)
[junit] 	- locked <0x0000000825d7d830> (a java.lang.Object)
[junit] 	at org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:1156)
[junit] 	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1099)
[junit] 	at org.apache.lucene.index.TestIndexWriterWithThreads.testCloseWithThreads(TestIndexWriterWithThreads.java:200)
[junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
[junit] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[junit] 	at java.lang.reflect.Method.invoke(Method.java:616)
[junit] 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
[junit] 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
[junit] 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
[junit] 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
[junit] 	at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
[junit] 	at org.apache.lucene.util.LuceneTestCase$2$1.evaluate(LuceneTestCase.java:611)
[junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
[junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
[junit] 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
[junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:148)
[junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:50)
[junit] 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
[junit] 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
[junit] 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
[junit] 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
[junit] 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
[junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
[junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
[junit] 	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
[junit] 	at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
[junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
[junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
[junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:743)
{noformat}"
1,"Cluster Node ID should be trimmedIf the cluster node ID is not configured in repository.xml, it is read from the file cluster_node.id instead. In case this file is edited by hand, some editors (e.g. vi) insert a trailing newline character (""\n""). This leads to the cluster node ID to contain a blank character. While I don't expect this to cause any issues, it is inconvenient for debugging and also introduces line-breaks in log files. I suggest to trim the cluster node ID, so only non-blank characters are used."
1,"JCA will not compile with J2EE1.3 classesIn JCAManagedConnectionFactory, the constructor invoked to throw ResourceException does not exist under J2EE1.3 / JCA1.1 classes.
 throw new ResourceException(e)  -  line 136 and line 277.

Instead the code needs to do something like:

            ResourceException exception = new ResourceException(""Failed to create session"");
            exception.setLinkedException(e);
            throw exception;

This will allow it to compile/run under J2EE1.3
"
1,"session.move() throws ItemExistsException despite same name siblingscode to reproduce:

            Session session = r.login(new SimpleCredentials(""johndoe"", """".toCharArray()), wspName);
            Node root = session.getRootNode();

            // setup test case
            if (!root.hasNode(""foo"")) {
                root.addNode(""foo"");
                root.save();
            }
            if (!root.hasNode(""bar"")) {
                root.addNode(""bar"");
                root.save();
            }

            session.move(""/foo"", ""/bar"");    // ==> ItemExistsException
"
1,"NullPointerException when accessing the about.jsp page because of missing /META-INF/NOTICE.TXTAccessing http://localhost:8080/about.jsp produces:

java.lang.NullPointerException
	at org.apache.jsp.about_jsp.output(org.apache.jsp.about_jsp:39)
	at org.apache.jsp.about_jsp._jspService(org.apache.jsp.about_jsp:103)
	at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:109)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:389)
	at org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:486)
	at org.apache.jasper.servlet.JspServlet.service(JspServlet.java:380)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:502)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:363)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:417)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.handler.RequestLogHandler.handle(RequestLogHandler.java:49)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:324)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:534)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:864)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:533)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:207)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:403)
	at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:522)

This is because the jar misses the following file:
/META-INF/NOTICE.TXT
but there is /NOTICE.TXT and /META-INF/NOTICE

This problem is not reproducible with jackrabbit-webapp-2.0-beta1.war.
"
1,"DocumentsWriter blocks flushes when applyDeletes takes forever - memory not releasedIn DocumentsWriter we have a safety check that applies all deletes if the deletes consume too much RAM to prevent too-frequent flushing of a long tail of tiny segments. If we enter applyAllDeletes we essentially lock on IW -> BufferedDeletes which is fine since this usually doesn't take long and doesn't keep DWPTs from indexing. Yet, if that takes long and at the same time a semgent is flushed and subsequently published to the IW we take the lock on the ticket queue and the IW. Now this prevents all other threads to append to the ticketQueue which is done BEFORE we actually flush the segment concurrently and free up the RAM.

Essentially its ok to block on the IW lock but we should not keep concurrent flushed from execution just because we apply deletes. The threads will block once they try to execute maybeMerge after the segment is flushed so we don't pile up subsequent memory but we should actually allow the DWPT to be flushed since we actually try to get rid of memory.

I ran into this by accident due to a coding bug using delete queries instead of terms for each document. This thread dump show the problem:

{noformat}
""Application Worker Thread"" prio=10 tid=0x00007fdda0238000 nid=0x3256 waiting for
monitor entry [0x00007fddad3c2000]
  java.lang.Thread.State: BLOCKED (on object monitor)
       at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:424)
       - waiting to lock <0x00007fddb74ff990> (a
org.apache.lucene.index.DocumentsWriter$TicketQueue)
       at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:320)
       at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:393)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1484)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)

""Application Worker Thread"" prio=10 tid=0x00007fdda0236000 nid=0x3255 waiting for
monitor entry [0x00007fddad4c3000]
  java.lang.Thread.State: BLOCKED (on object monitor)
       at org.apache.lucene.index.IndexWriter.updatePendingMerges(IndexWriter.java:1854)
       - waiting to lock <0x00007fddb74fe350> (a
org.apache.solr.update.SolrIndexWriter)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1848)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1843)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1493)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)

""Application Worker Thread"" prio=10 tid=0x00007fdda0234000 nid=0x3254 waiting for
monitor entry [0x00007fddad5c4000]
  java.lang.Thread.State: BLOCKED (on object monitor)
       at org.apache.lucene.index.IndexWriter.updatePendingMerges(IndexWriter.java:1854)
       - waiting to lock <0x00007fddb74fe350> (a
org.apache.solr.update.SolrIndexWriter)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1848)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1843)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1493)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)

""Application Worker Thread"" prio=10 tid=0x00007fdda0232000 nid=0x3253 waiting for
monitor entry [0x00007fddad6c5000]
  java.lang.Thread.State: BLOCKED (on object monitor)
       at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:424)
       - waiting to lock <0x00007fddb74ff990> (a
org.apache.lucene.index.DocumentsWriter$TicketQueue)
       at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:320)
       at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:393)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1484)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)

""Application Worker Thread"" prio=10 tid=0x00007fdda0230800 nid=0x3252 waiting for
monitor entry [0x00007fddad7c6000]
  java.lang.Thread.State: BLOCKED (on object monitor)
       at org.apache.lucene.index.IndexWriter.updatePendingMerges(IndexWriter.java:1854)
       - waiting to lock <0x00007fddb74fe350> (a
org.apache.solr.update.SolrIndexWriter)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1848)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1843)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1493)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)

""Application Worker Thread"" prio=10 tid=0x00007fdda022e800 nid=0x3251 runnable
[0x00007fddad8c6000]
  java.lang.Thread.State: RUNNABLE
       at java.nio.Bits.copyToArray(Bits.java:715)
       at java.nio.DirectByteBuffer.get(DirectByteBuffer.java:233)
       at org.apache.lucene.store.MMapDirectory$MMapIndexInput.readBytes(MMapDirectory.java:319)
       at org.apache.lucene.index.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum$Frame.loadBlock(BlockTreeTermsReader.java:2283)
       at org.apache.lucene.index.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum.seekExact(BlockTreeTermsReader.java:1600)
       at org.apache.lucene.util.TermContext.build(TermContext.java:97)
       at org.apache.lucene.search.TermQuery.createWeight(TermQuery.java:180)
       at org.apache.lucene.search.BooleanQuery$BooleanWeight.<init>(BooleanQuery.java:186)
       at org.apache.lucene.search.BooleanQuery.createWeight(BooleanQuery.java:423)
       at org.apache.lucene.search.IndexSearcher.createNormalizedWeight(IndexSearcher.java:583)
       at org.apache.lucene.search.QueryWrapperFilter.getDocIdSet(QueryWrapperFilter.java:55)
       at org.apache.lucene.index.BufferedDeletesStream.applyQueryDeletes(BufferedDeletesStream.java:431)
       at org.apache.lucene.index.BufferedDeletesStream.applyDeletes(BufferedDeletesStream.java:268)
       - locked <0x00007fddb751e1e8> (a
org.apache.lucene.index.BufferedDeletesStream)
       at org.apache.lucene.index.IndexWriter.applyAllDeletes(IndexWriter.java:2852)
       - locked <0x00007fddb74fe350> (a org.apache.solr.update.SolrIndexWriter)
       at org.apache.lucene.index.DocumentsWriter.applyAllDeletes(DocumentsWriter.java:188)
       at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:470)
       at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:320)
       at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:393)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1484)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)
       

""Application Worker Thread"" prio=10 tid=0x00007fdda022d800 nid=0x3250 waiting for
monitor entry [0x00007fddad9c8000]
  java.lang.Thread.State: BLOCKED (on object monitor)
       at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:424)
       - waiting to lock <0x00007fddb74ff990> (a
org.apache.lucene.index.DocumentsWriter$TicketQueue)
       at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:320)
       at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:393)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1484)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)
   
""Application Worker Thread"" prio=10 tid=0x00007fdda022d000 nid=0x324f waiting for
monitor entry [0x00007fddadac9000]
  java.lang.Thread.State: BLOCKED (on object monitor)
       at org.apache.lucene.index.IndexWriter.useCompoundFile(IndexWriter.java:2274)
       - waiting to lock <0x00007fddb74fe350> (a
org.apache.solr.update.SolrIndexWriter)
       at org.apache.lucene.index.IndexWriter.prepareFlushedSegment(IndexWriter.java:2156)
       at org.apache.lucene.index.DocumentsWriter.publishFlushedSegment(DocumentsWriter.java:526)
       at org.apache.lucene.index.DocumentsWriter.finishFlush(DocumentsWriter.java:506)
       at org.apache.lucene.index.DocumentsWriter.applyFlushTickets(DocumentsWriter.java:483)
       - locked <0x00007fddb74ff990> (a
org.apache.lucene.index.DocumentsWriter$TicketQueue)
       at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:449)
       at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:320)
       at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:393)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1484)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)

{noformat}"
1,"Lower-Case Search-Function works with Upper-Case Searchstringif you perform a query like this
testroot/*[jcr:like(fn:lower-case(@prop1), 'FO%')]
you get valid results even though the value in the property has the ""foo"" value
The search works with lower and upper-case search strings."
1,"TestFSTs.testRandomWords failureWas running some while(1) tests on the docvalues branch (r1103705) and the following test failed:

{code}
    [junit] Testsuite: org.apache.lucene.util.automaton.fst.TestFSTs
    [junit] Testcase: testRandomWords(org.apache.lucene.util.automaton.fst.TestFSTs):	FAILED
    [junit] expected:<771> but was:<TwoLongs:771,771>
    [junit] junit.framework.AssertionFailedError: expected:<771> but was:<TwoLongs:771,771>
    [junit] 	at org.apache.lucene.util.automaton.fst.TestFSTs$FSTTester.verifyUnPruned(TestFSTs.java:540)
    [junit] 	at org.apache.lucene.util.automaton.fst.TestFSTs$FSTTester.doTest(TestFSTs.java:496)
    [junit] 	at org.apache.lucene.util.automaton.fst.TestFSTs$FSTTester.doTest(TestFSTs.java:359)
    [junit] 	at org.apache.lucene.util.automaton.fst.TestFSTs.doTest(TestFSTs.java:319)
    [junit] 	at org.apache.lucene.util.automaton.fst.TestFSTs.testRandomWords(TestFSTs.java:940)
    [junit] 	at org.apache.lucene.util.automaton.fst.TestFSTs.testRandomWords(TestFSTs.java:915)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1282)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1211)
    [junit] 
    [junit] 
    [junit] Tests run: 7, Failures: 1, Errors: 0, Time elapsed: 7.628 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: Ignoring nightly-only test method 'testBigSet'
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestFSTs -Dtestmethod=testRandomWords -Dtests.seed=-269475578956012681:0
    [junit] NOTE: test params are: codec=PreFlex, locale=ar, timezone=America/Blanc-Sablon
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestToken, TestCodecs, TestIndexReaderReopen, TestIndexWriterMerging, TestNoDeletionPolicy, TestParallelReaderEmptyIndex, TestParallelTermEnum, TestPerSegmentDeletes, TestSegmentReader, TestSegmentTermDocs, TestStressAdvance, TestTermVectorsReader, TestSurrogates, TestMultiFieldQueryParser, TestAutomatonQuery, TestBooleanScorer, TestFuzzyQuery, TestMultiTermConstantScore, TestNumericRangeQuery64, TestPositiveScoresOnlyCollector, TestPrefixFilter, TestQueryTermVector, TestScorerPerf, TestSloppyPhraseQuery, TestSpansAdvanced, TestWindowsMMap, TestRamUsageEstimator, TestSmallFloat, TestUnicodeUtil, TestFSTs]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=137329960,total=208207872
    [junit] ------------- ---------------- ---------------
    [junit] TEST org.apache.lucene.util.automaton.fst.TestFSTs FAILED
{code}

I am not able to reproduce"
1,"CachingHierarchyManager synchronization problemThe method CachingHierarchyManager.resolveNodePath(..) does not synchronize on the cacheMonitor object. This can result in an endless loop in cache(), in NullPointerException or in other unexpected behavior in CachingHierarchyManager."
1,RepositoryConfig created by Jcr2spiRepositoryFactory should always return same RepositoryService instanceThe Jcr2spiRepositoryFactory uses a default implementation of RepositoryConfig if none is passed to it by the user. Currently this default implementation returns a new RepositoryService instance on each call to getRepositoryService(). This is not correct since the consumer of the RepositoryConfig instance expects the same RepositoryService instance on every call. 
1,"TokenStream.next(Token) reuse 'policy': calling Token.clear() should be responsibility of producer.Tokenizers which implement the reuse form of the next method:
    next(Token result) 
should reset the postionIncrement of the returned token to 1."
1,"SQL2 Left Outer JoinCreate this nodes.
def n1 = root.addNode(""node1"", ""sling:SamplePage"");
n1.setProperty(""n1prop1"", ""page1"");
def n2 = n1.addNode(""node2"", ""sling:SampleContent"");
n2.setProperty(""n2prop1"", ""content1"");

Execute this Query:
Select * from [sling:SamplePage] as page left outer join [sling:SampleContent] as content on ISDESCENDANTNODE(content,page) where page.n1prop1 = 'page1' and content.n2prop1 = 'content1';
The resultset have 1 row with 2 Nodes. This OK.

Then execute this:
Select * from [sling:SamplePage] as page left outer join [sling:SampleContent] as content on ISDESCENDANTNODE(content,page) where page.n1prop1 = 'page1' and content.n2prop1 = 'XXXXX';

The resultset has 1 row with 1 node.
This wrong. The result should be 0 rows.

Old Versions, prior 2.2.2 have also 0 rows as result.

Also, if nodes ""n2"" not exists, jackrabbit reports 1 row as result.

"
1,"SpellChecker not working because of stale IndexSearcherThe SpellChecker unit test did not work, because of a stale IndexReader and IndexSearcher instance after calling indexDictionary(Dictionary)."
1,"SessionImpl.createSession uses same Subject/LoginContextSessionImpl.createSession(String) uses the same loginctx/subject to create a new session.
this will cause problems if Session.logout() is called on the original instance.

i suggest to fix that by creating a new subject for the new session instance."
1,"XATest error: commit from different thread but same XID must not blockI'm seeing the following test error quite often in the CI server at work:

testDistributedThreadAccess(org.apache.jackrabbit.core.XATest)  Time elapsed: 0.213 sec  <<< ERROR!
javax.transaction.SystemException: commit from different thread but same XID must not block
	at org.apache.jackrabbit.core.UserTransactionImpl.commit(UserTransactionImpl.java:147)
	at org.apache.jackrabbit.core.XATest.testDistributedThreadAccess(XATest.java:1637)

It seems to be a system-specific issue, as I've never seen the same error locally or on Hudson."
1,jcr:encoding not respected in NodeIndexerThe value of the jcr:encoding property is not passed to the TextFilter instances.
1,"TestDocValuesIndexing reproducible  test failuredocvalues branch: r1131275

{code}
    [junit] Testsuite: org.apache.lucene.index.values.TestDocValuesIndexing
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.81 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestDocValuesIndexing -Dtestmethod=testAddIndexes -Dtests.seed=-3253978684351194958:-8331223747763543724
    [junit] NOTE: test params are: codec=RandomCodecProvider: {id=Standard, BYTES_VAR_STRAIGHT=Pulsing(freqCutoff=12), BYTES_FIXED_SORTED=MockRandom}, locale=es_MX, timezone=Pacific/Chatham
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestDocValuesIndexing]
    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=89168480,total=125632512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testAddIndexes(org.apache.lucene.index.values.TestDocValuesIndexing):     FAILED
    [junit] [first=BYTES_FIXED_SORTED, second=BYTES_VAR_STRAIGHT] expected:<9> but was:<10>
    [junit] junit.framework.AssertionFailedError: [first=BYTES_FIXED_SORTED, second=BYTES_VAR_STRAIGHT] expected:<9> but was:<10>
    [junit]     at org.apache.lucene.index.values.TestDocValuesIndexing.testAddIndexes(TestDocValuesIndexing.java:208)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1348)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1266)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.index.values.TestDocValuesIndexing FAILED
{code}"
1,"TermVectors corruption case when autoCommit=falseI took Yonik's awesome test case (TestStressIndexing2) and extended it to also compare term vectors, and, it's failing.

I still need to track down why, but it seems likely a separate issue."
1,"index corruption autoCommit=falseIn both Lucene 2.3 and trunk, the index becomes corrupted when autoCommit=false"
1,"CloseableThreadLocal should allow null ObjectsCloseableThreadLocal does not allow null Objects in its get() method, but does nothing to prevent them in set(Object). The comment in get() before assert v != null is irrelevant - the application might have passed null.

Null is an important value for Analyzers. Since tokenStreams (a ThreadLocal private member in Analyzer) is not accessible by extending classes, the only way for an Analyzer to reset the tokenStreams is by calling setPreviousTokenStream(null).

I will post a patch w/ a test"
1,"Passing a null fieldname to MemoryFields#terms in MemoryIndex throws a NPEI found this when querying a MemoryIndex using a RegexpQuery wrapped by a SpanMultiTermQueryWrapper.  If the regexp doesn't match anything in the index, it gets rewritten to an empty SpanOrQuery with a null field value, which then triggers the NPE."
1,"Thread safety issue can cause index corruption when autoCommit=true and multiple threads are committingThis is only present in 2.9 trunk, but has been there since
LUCENE-1516 was committed I believe.

It's rare to hit: it only happens if multiple calls to commit() are in
flight (from different threads) and where at least one of those calls
is due to a merge calling commit (because autoCommit is true).

When it strikes, it leaves the index corrupt because it incorrectly
removes an active segment.  It causes exceptions like this:
{code}
java.io.FileNotFoundException: _1e.fnm
	at org.apache.lucene.store.MockRAMDirectory.openInput(MockRAMDirectory.java:246)
	at org.apache.lucene.index.FieldInfos.<init>(FieldInfos.java:67)
	at org.apache.lucene.index.SegmentReader.initialize(SegmentReader.java:536)
	at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:468)
	at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:414)
	at org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:641)
	at org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:627)
	at org.apache.lucene.index.DocumentsWriter.applyDeletes(DocumentsWriter.java:923)
	at org.apache.lucene.index.IndexWriter.applyDeletes(IndexWriter.java:4987)
	at org.apache.lucene.index.IndexWriter.doFlushInternal(IndexWriter.java:4165)
	at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:4025)
	at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:4016)
	at org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:2077)
	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:2040)
	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:2004)
	at org.apache.lucene.index.TestStressIndexing2.indexRandom(TestStressIndexing2.java:210)
	at org.apache.lucene.index.TestStressIndexing2.testMultiConfig(TestStressIndexing2.java:104)
{code}

It's caused by failing to increment changeCount inside the same
synchronized block where segmentInfos was changed, in commitMerge.
The fix is simple -- I plan to commit shortly.
"
1,"Issue while loading list of classes at that path itself.Hi,

I cannot retrieve list of objects that are directly under the path that they were saved in. I did not know where to simulate this issue and hence I have used DigesterSimpleQueryTest. I have attached the path for the newly added test case testObjectListRetrievalAtBasePath. In case the patch is not up to the mark I have attached the modified file too.

Instead of creating Page in /test if I create it in /sample/test and search in /sample/test it returns nothing but if I search in /sample it would return the object.

Another important point here is that it is causing issues while retrieving Page class, the other test cases that are retrieving Paragraph class (embedded inside Page class) are still working fine!

Regards,

Kaizer"
1,"FilteredDocIdSet does not handle a case where the inner set iterator is nullDocIdSet#iterator is allowed to return null, when used in FilteredDocIdSet, if null is returned from the inner set, the FilteredDocIdSetIterator fails since it does not allow for nulls to be passed to it.

The fix is simple, return null in FilteredDocIdSet in the iterator method is the iterator is null."
1,"IndexOutOfBoundsException at ShingleMatrixFilter's Iterator#hasNext methodI tried to use the ShingleMatrixFilter within Solr. To test the functionality etc., I first used the built-in field analysis view.The filter was configured to be used only at query time analysis with ""_"" as spacer character and a min. and max. shingle size of 2. The generation of the shingles for query strings with this filter seems to work at this view, but by turn on the highlighting of indexed terms that will match the query terms, the exception was thrown. Also, each time I tried to query the index the exception was immediately thrown.

Stacktrace:
{code}
java.lang.IndexOutOfBoundsException: Index: 1, Size: 1
	at java.util.ArrayList.RangeCheck(Unknown Source)
	at java.util.ArrayList.get(Unknown Source)
	at org.apache.lucene.analysis.shingle.ShingleMatrixFilter$Matrix$1.hasNext(ShingleMatrixFilter.java:729)
	at org.apache.lucene.analysis.shingle.ShingleMatrixFilter.next(ShingleMatrixFilter.java:380)
	at org.apache.lucene.analysis.StopFilter.next(StopFilter.java:120)
	at org.apache.lucene.analysis.TokenStream.next(TokenStream.java:47)
	...
{code}

Within the hasNext method, there is the {{s-1}}-th Column from the ArrayList {{columns}} requested, but there isn't this entry within columns.

I created a patch that checks, if {{columns}} contains enough entries."
1,"MatchAllDocsQuery doesn't honor boost or queryNormMatchAllDocsQuery doesn't pay attention to either it's own boost, or lucene's query normalization factor."
1,"removing source parent node after session move throws on savethe following code fragment illustrates the problem:

        /**
         * create the following node tree:
         *     
         *       + A
         *         + B
         *            + C
         *         + D
         */
        Node A;
        if (root.hasNode(""A"")) {
            A = root.getNode(""A"");
        } else {
            A = root.addNode(""A"");
        }
        Node B = A.addNode(""B"");
        Node C = B.addNode(""C"");
        Node D = A.addNode(""D"");
        root.save();

        // move C under D
        session.move(""/A/B/C"", ""/A/D/C"");
        // remove B
        A.getNode(""B"").remove();
        /**
         * the expected resulting node tree:
         *     
         *       + A
         *         + D
         *            + C
         */
        A.save();


==> the last save() will throw 
javax.jcr.RepositoryException: inconsistency: failed to retrieve transient state for ...
 "
1,"303 Redirects are not handled properlyWhen the server spits back a 303 (See Other), the redirect is not handled. 
Looking at the code, I saw that the processRedirectResponse method in
HttpMethodBase does not check for SC_SEE_OTHER in the case statement. 
SC_SEE_OTHER is a redirect and should be handled appropriately.

Here is a trace from the output of the client and server.

GET http://172.30.229.75/CGI/Screenshot HTTP/1.1 
Authorization: Basic c3VwZXJ1c2VyOnJvb3Q= 
Host: 172.30.229.75 
User-Agent: Jakarta Commons-HttpClient/2.0M1 

HTTP/1.1 303 See Other 
Location: http://172.30.229.75/FS/CIP_0_5842
Content-Length: 0 
Server: *snip*"
1,"MSSqlFileSystem - JNDI & several configuration issuesthere are several configuration issues using the org.apache.jackrabbit.core.fs.db.MSSqlFileSystem
my (working) configuration (repository.xml) looks like:

<FileSystem class=""org.apache.jackrabbit.core.fs.db.MSSqlFileSystem"">
 <param name=""driver"" value=""javax.naming.InitialContext""/>
 <param name=""url"" value=""java:MYDatasource""/>
 <param name=""schema"" value=""mssql""/>
 <param name=""schemaObjectPrefix"" value=""MYPREFIX_""/>
 <param name=""user"" value=""MYUSERNAME""/> 
 <param name=""password"" value=""MYPASSWORD""/>
 <param name=""tableSpace"" value=""""/>
</FileSystem>

i have to unnecessarily specify username & password, because the MSSqlFileSystem presets them to an empty string instead of null. funnily enough  the tableSpace is preset to null, which leads to a nullpointer in createSchemaSql



"
1,"Test failures in jcr-rmi and jcr2davIntegration testing currently fails for jcr-rmi:
  testCloneNodesAccessDenied(org.apache.jackrabbit.test.api.WorkspaceCloneTest)
  testCopyNodesAccessDenied(org.apache.jackrabbit.test.api.WorkspaceCopyBetweenWorkspacesTest)
  testCopyNodesAccessDenied(org.apache.jackrabbit.test.api.WorkspaceCopyTest)
  testMoveNodesAccessDenied(org.apache.jackrabbit.test.api.WorkspaceMoveTest)
  testImpersonate(org.apache.jackrabbit.test.api.ImpersonateTest)
  testCheckPermission(org.apache.jackrabbit.test.api.CheckPermissionTest)
  testRemoveItem4(org.apache.jackrabbit.test.api.SessionRemoveItemTest)
  testReadOnlyPermission(org.apache.jackrabbit.test.api.HasPermissionTest)
  testGetPrivileges(org.apache.jackrabbit.test.api.security.RSessionAccessControlDiscoveryTest)
  testNotHasPrivileges(org.apache.jackrabbit.test.api.security.RSessionAccessControlDiscoveryTest)
  testGetApplicablePolicies(org.apache.jackrabbit.test.api.security.RSessionAccessControlPolicyTest)
  testGetPolicy(org.apache.jackrabbit.test.api.security.RSessionAccessControlPolicyTest)
  testGetEffectivePolicy(org.apache.jackrabbit.test.api.security.RSessionAccessControlPolicyTest)
  testMoveNode(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testSetProperty(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testSetValue(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testDeleteNode(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testDeleteProperty(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testWorkspaceMoveNode(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testCopyNode(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)

and jcr2dav:
  testCloneNodesAccessDenied(org.apache.jackrabbit.test.api.WorkspaceCloneTest)
  testCopyNodesAccessDenied(org.apache.jackrabbit.test.api.WorkspaceCopyBetweenWorkspacesTest)
  testCopyNodesAccessDenied(org.apache.jackrabbit.test.api.WorkspaceCopyTest)
  testMoveNodesAccessDenied(org.apache.jackrabbit.test.api.WorkspaceMoveTest)
  testCheckPermission(org.apache.jackrabbit.test.api.CheckPermissionTest)
  testRemoveItem4(org.apache.jackrabbit.test.api.SessionRemoveItemTest)
  testReadOnlyPermission(org.apache.jackrabbit.test.api.HasPermissionTest)
  testMoveNode(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testSetProperty(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testSetValue(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testDeleteNode(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testDeleteProperty(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testWorkspaceMoveNode(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testCopyNode(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
"
1,"DbInputStream does not support mark()/reset() when exhausted.The DbDataStore implementation uses a DbInputStream to read binary properties from the database. When a new binary property is created, Jackrabbit attempts to index it. Tika's CharsetDetector is used in the process, which marks the input stream, reads the first 8000 bytes and then resets the stream.

This results in the stacktrace shown at the end of the issue, if the following two conditions hold true:
* the property is larger than the minRecordLength configuration of the Datastore and
* the property is smaller than 8000 bytes

The DbInputStream needs to have the following properties:
1. lazy instantiation of the underlying stream
2. auto-close underlying stream when EOF is reached
3. fully support mark()/reset() even if  the underlying stream is auto-closed due to 2.


12.03.2010 15:53:28 *WARN * LazyTextExtractorField: Failed to extract text from a binary property (LazyTextExtractorField.java, line 165)
java.io.EOFException
        at org.apache.jackrabbit.core.data.db.DbInputStream.reset(DbInputStream.java:180)
        at org.apache.tika.io.ProxyInputStream.reset(ProxyInputStream.java:156)
        at org.apache.tika.io.ProxyInputStream.reset(ProxyInputStream.java:156)
        at org.apache.tika.parser.txt.CharsetDetector.setText(CharsetDetector.java:131)
        at org.apache.tika.parser.txt.TXTParser.parse(TXTParser.java:77)
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:120)
        at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:101)
        at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:114)
        at org.apache.jackrabbit.core.query.lucene.LazyTextExtractorField$ParsingTask.run(LazyTextExtractorField.java:160)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:207)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
"
1,"Do not launch new merges if IndexWriter has hit OOMEif IndexWriter has hit OOME, it defends itself by refusing to commit changes to the index, including merges.  But this can lead to infinite merge attempts because we fail to prevent starting a merge.

Spinoff from http://www.nabble.com/semi-infinite-loop-during-merging-td23036156.html."
1,"Node.orderBefore and JackrabbitNode.rename should check for ability to modify children-collection on parent nodecurrently the implementation of Node.orderBefore and JackrabbitNode.rename perform the same validation that is executed
for a move operation which includes removal of the original node. however, the methods mentioned above only include
a manipulation on the child-node-collection of the parent (subset of the current check). therefore the permission check should be 
adjusted accordingly."
1,"NullPointerException in IndexModifier.close()We upgraded from Lucene 2.0.0. to 2.3.1 hoping this would resolve this issue.

http://jira.codehaus.org/browse/MRM-715

Trace is as below for Lucene 2.3.1:
java.lang.NullPointerException
at org.apache.lucene.index.IndexModifier.close(IndexModifier.java:576)
at org.apache.maven.archiva.indexer.lucene.LuceneRepositoryContentIndex.closeQuietly(LuceneRepositoryContentIndex.java:416)
at org.apache.maven.archiva.indexer.lucene.LuceneRepositoryContentIndex.modifyRecord(LuceneRepositoryContentIndex.java:152)
at org.apache.maven.archiva.consumers.lucene.IndexContentConsumer.processFile(IndexContentConsumer.java:169)
at org.apache.maven.archiva.repository.scanner.functors.ConsumerProcessFileClosure.execute(ConsumerProcessFileClosure.java:51)
at org.apache.commons.collections.functors.IfClosure.execute(IfClosure.java:117)
at org.apache.commons.collections.CollectionUtils.forAllDo(CollectionUtils.java:388)
at org.apache.maven.archiva.repository.scanner.RepositoryContentConsumers.executeConsumers(RepositoryContentConsumers.java:283)
at org.apache.maven.archiva.proxy.DefaultRepositoryProxyConnectors.transferFile(DefaultRepositoryProxyConnectors.java:597)
at org.apache.maven.archiva.proxy.DefaultRepositoryProxyConnectors.fetchFromProxies(DefaultRepositoryProxyConnectors.java:157)
at org.apache.maven.archiva.web.repository.ProxiedDavServer.applyServerSideRelocation(ProxiedDavServer.java:447)
at org.apache.maven.archiva.web.repository.ProxiedDavServer.fetchContentFromProxies(ProxiedDavServer.java:354)
at org.apache.maven.archiva.web.repository.ProxiedDavServer.process(ProxiedDavServer.java:189)
at org.codehaus.plexus.webdav.servlet.multiplexed.MultiplexedWebDavServlet.service(MultiplexedWebDavServlet.java:119)
at org.apache.maven.archiva.web.repository.RepositoryServlet.service(RepositoryServlet.java:155)
at javax.servlet.http.HttpServlet.service(HttpServlet.java:803)"
1,"WorkspaceAccessManager defined with SecurityManager that keeps users per workspace must test if user existsthe WorkspaceAccessManager defined with the security manager keeping users per workspace currently returns true upon calls to grant(Set, String) if
a workspace with the given name exists.

while this is fine for the initial check upon session creation, it obviously isn't for all method calls that test for accessible workspace names, such as
Workspace#getAccessibleWorkspaceName, Workspace#clone and copy across workspaces.

instead it should test if any of the specified principals corresponds to a valid user within the workspace identified by the given workspaceName."
1,"GzipDecompressingEntity (and therefore ContentEncodingHttpClient) not consistent with EntityUtils.consumeEntityInvoking EntityUtils.consume( entity ) after a previous call to entity.getContent (and subsequent processing of the content) throws a java.io.EOFException when gzip decompression support is enabled via ContentEncodingHttpClient or some similar mechanism.  I invoke EntityUtils.consume in a 'finally' block - maybe I'm not using the API correctly ... ?  

java.io.EOFException
	at java.util.zip.GZIPInputStream.readUByte(GZIPInputStream.java:207)
	at java.util.zip.GZIPInputStream.readUShort(GZIPInputStream.java:197)
	at java.util.zip.GZIPInputStream.readHeader(GZIPInputStream.java:136)
	at java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:58)
	at java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:68)
	at org.apache.http.client.entity.GzipDecompressingEntity.getContent(GzipDecompressingEntity.java:63)
	at org.apache.http.conn.BasicManagedEntity.getContent(BasicManagedEntity.java:88)
	at org.apache.http.util.EntityUtils.consume(EntityUtils.java:65)

I believe the problem is that the underlying DecompressingEntity allocates a new GzipInputStream for each call to getContent, rather than caching the stream created by the first getContent call.  
       http://svn.apache.org/repos/asf/httpcomponents/httpclient/trunk/httpclient/src/main/java/org/apache/http/client/entity/DecompressingEntity.java
The ""CustomProtocolInterceptors"" example has the same bug:  http://hc.apache.org/httpcomponents-client-ga/examples.html

I worked around the problem implementing the example with my own GzipDecompressingEntity (scala code - lazy value not evaluated till accessed):

  class GzipDecompressingEntity( entity:http.HttpEntity) extends http.entity.HttpEntityWrapper(entity) {
    private lazy val gzipStream = new GZIPInputStream( entity.getContent() )
    
    /** 
     * Wrap entity stream in GZIPInputStream
     */
    override def getContent():java.io.InputStream = gzipStream

    /**
     * Return -1 - don't know unzipped content size
     */
    override def getContentLength():Long = -1L
  }

"
1,"Occasional NullPointerException in ItemManagerFrom time to time I see a NullPointerException in ItemManager when running ConcurrentReadWriteTest. The exception is probably caused by another session that removes the property, which has the effect that the ItemState in ItemData is set to null.

Exception in thread ""Thread-11"" java.lang.NullPointerException
	at org.apache.jackrabbit.core.ItemManager.canRead(ItemManager.java:313)
	at org.apache.jackrabbit.core.ItemManager.getItemData(ItemManager.java:293)
	at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:226)
	at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:486)
	at org.apache.jackrabbit.core.LazyItemIterator.prefetchNext(LazyItemIterator.java:111)
	at org.apache.jackrabbit.core.LazyItemIterator.<init>(LazyItemIterator.java:93)
	at org.apache.jackrabbit.core.LazyItemIterator.<init>(LazyItemIterator.java:75)
	at org.apache.jackrabbit.core.ItemManager.getChildProperties(ItemManager.java:658)
	at org.apache.jackrabbit.core.NodeImpl.getProperties(NodeImpl.java:2663)
	at org.apache.jackrabbit.core.ConcurrentReadWriteTest$1$1.execute(ConcurrentReadWriteTest.java:65)
	at org.apache.jackrabbit.core.AbstractConcurrencyTest$Executor.run(AbstractConcurrencyTest.java:206)
	at java.lang.Thread.run(Thread.java:595)

This issue does not occur in a release but only in trunk."
1,"URI.normalize() errorcode:

----------------------------
import org.apache.commons.httpclient.URI;

class Main {
  publi static void main(String[] args) throws Exception {
    URI uri = new URI(""http"", null, ""host"", -1, ""/tmp/../yo"", null, null);
    uri.normalize();
    System.out.println(uri);
  }
} /// end of Main
----------------------------

prints:

http://host/tmp/../yo

instead of

http://host/yo"
1,RMI reference not automatically bound by the standalone serverThe RMI servlet in the 1.5.0 standalone server is only initialized (and the remote reference bound to the RMI registry) when the http://.../rmi URL is first accessed. The RMI binding should be made as soon as the standalone server starts.
1,"IndexWriter.updateDocument is no longer atomicSpinoff from LUCENE-847.

Ning caught that as of LUCENE-843, we lost the atomicity of the delete
+ add in IndexWriter.updateDocument.

Ning suggested a simple fix: move the buffered deletes into
DocumentsWriter and let it do the delete + add atomically.  This has a
nice side effect of also consolidating the ""time to flush"" logic in
DocumentsWriter.

"
1,"Concurrent Session.move() operations failurePerforming concurrent move operations may cause failures similar to the following:

javax.jcr.RepositoryException: Unable to update item: node /
       at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1147)
       at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:920)
       at ConcurrentMoveTest$1.execute(ConcurrentMoveTest.java:30)
       at org.apache.jackrabbit.core.AbstractConcurrencyTest$Executor.run(AbstractConcurrencyTest.java:209)
       at java.lang.Thread.run(Thread.java:637)
Caused by: org.apache.jackrabbit.core.state.ItemStateException: Unable
to resolve path for item: 79a0fbdb-49fd-4830-a842-5ab11842cd17
       at org.apache.jackrabbit.core.observation.EventStateCollection.getPath(EventStateCollection.java:683)
       at org.apache.jackrabbit.core.observation.EventStateCollection.createEventStates(EventStateCollection.java:268)
       at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:702)
       at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:1110)
       at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:1140)
       at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:351)
       at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:354)
       at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:326)
       at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:328)
       at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1141)
       ... 4 more
Caused by: javax.jcr.ItemNotFoundException: failed to build path of
79a0fbdb-49fd-4830-a842-5ab11842cd17:
826f0c19-9956-402a-9c0d-93089eedcc1c has no child entry for
79a0fbdb-49fd-4830-a842-5ab11842cd17
       at org.apache.jackrabbit.core.HierarchyManagerImpl.buildPath(HierarchyManagerImpl.java:291)
       at org.apache.jackrabbit.core.CachingHierarchyManager.buildPath(CachingHierarchyManager.java:198)
       at org.apache.jackrabbit.core.HierarchyManagerImpl.getPath(HierarchyManagerImpl.java:395)
       at org.apache.jackrabbit.core.CachingHierarchyManager.getPath(CachingHierarchyManager.java:232)
       at org.apache.jackrabbit.core.observation.EventStateCollection.getPath(EventStateCollection.java:678)
       ... 13 more
"
1,"AccessControlManager#setPolicy may fail for new applicable policy despite jcr:modifyAccessControl privilege being grantedthe sequence AccessControlManager.getApplicablePolicies -> modify -> AccessControlManager#setPolicy fails
due to bug in AC evaluation if target is an AC-item but not yet existing. in this case the
wrong parent node is used for AC-evaluation."
1,"Unclosed files when aggregated property states are indexedThis is a regression caused by JCR-1990.

The lucene document for the node that contains the aggregated property may contain an extractor job that has an open file handle to the jcr:data binary property. The document must be disposed after the properties are transferred."
1,"JCR2SPI: lockmgr isn't aware about external unlock (CacheBehavior.OBSERVATION)issue occurring with CacheBehavior.OBSERVATION only:

the lock manager expects the jcr:lockIsDeep property to be created upon successful lock.
this however isn't the case since the time, we changed the Operation.persisted method to invalidate the affected states. consequently the mgr never started to listen on changes made to the jcr:lockIsDeep property and consequently wasn't aware of an external removal.

suggested fix:
force re-loading of the lock holding node."
1,"org.apache.commons.httpclient.HeaderElement fail to parse cookie headerif Set-Cookie header has value such ""expires=Mon, .."".
org.apache.commons.httpclient.HeaderElement will fail to parse this header.

Cause:
In the source cord:
-------------
            try {
                /*
                 * Following to RFC 2109 and 2965, in order not to conflict
                 * with the next header element, make it sure to parse tokens.
                 * the expires date format is ""Wdy, DD-Mon-YY HH:MM:SS GMT"".
                 * Notice that there is always comma(',') sign.
                 * For the general cases, rfc1123-date, rfc850-date.
                 */
                if (tokenizer.hasMoreTokens()) {
                    String s = nextToken.toLowerCase();
                    if (nextToken.endsWith(""mon"") 
                        || s.endsWith(""tue"")
                        || s.endsWith(""wed"") 
                        || s.endsWith(""thu"")
                        || s.endsWith(""fri"")
                        || s.endsWith(""sat"")
                        || s.endsWith(""sun"")
                        || s.endsWith(""monday"") 
                        || s.endsWith(""tuesday"") 
---- snip ---
 
""if (nextToken.endsWith(""mon"") "" is wrong.
this must be ""if (s.endsWith(""mon"") "".

Source cord version:
 * $Header:
/home/cvspublic/jakarta-commons/httpclient/src/java/org/apache/commons/httpclient/HeaderElement.java,v
1.17 2003/03/08 21:30:02 olegk Exp $
 * $Revision: 1.17 $
 * $Date: 2003/03/08 21:30:02 $"
1,"replace invalid U+FFFF character during indexingIf the invalid U+FFFF character is embedded in a token, it actually causes indexing to silently corrupt the index by writing duplicate terms into the terms dict.  CheckIndex will catch the error, and merging will hit exceptions (I think).

We already replace invalid surrogate pairs with the replacement character U+FFFD, so I'll just do the same with U+FFFF."
1,"DataStore: garbage collection fails if a workspace is not initializedThe test case GCEventListenerTest fails with the following exception:

testEventListener(org.apache.jackrabbit.core.data.GCEventListenerTest)  Time elapsed: 10.235 sec  <<< ERROR!
java.lang.IllegalStateException: workspace 'test' not initialized
	at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.getPersistenceManager(RepositoryImpl.java:1703)
	at org.apache.jackrabbit.core.SessionImpl.createDataStoreGarbageCollector(SessionImpl.java:694)
	at org.apache.jackrabbit.core.data.GCEventListenerTest.doTestEventListener(GCEventListenerTest.java:75)
	at org.apache.jackrabbit.core.data.GCEventListenerTest.testEventListener(GCEventListenerTest.java:49)

"
1,"QueryManager.createQuery() exception handlingQuery q = this.superuser.getWorkspace().getQueryManager()
                .createQuery(""SELECT * FROM nt:base"", Query.XPATH);

produces:
org.apache.jackrabbit.core.query.xpath.TokenMgrError: Lexical error at line 1, column 28.  Encountered: ""b"" (98), after : "":""
	at org.apache.jackrabbit.core.query.xpath.XPathTokenManager.getNextToken(XPathTokenManager.java:14546)
	at org.apache.jackrabbit.core.query.xpath.XPath.jj_ntk(XPath.java:9187)
	at org.apache.jackrabbit.core.query.xpath.XPath.PredicateList(XPath.java:5195)
	at org.apache.jackrabbit.core.query.xpath.XPath.AxisStep(XPath.java:4707)
	at org.apache.jackrabbit.core.query.xpath.XPath.StepExpr(XPath.java:4597)
	at org.apache.jackrabbit.core.query.xpath.XPath.RelativePathExpr(XPath.java:4511)
	at org.apache.jackrabbit.core.query.xpath.XPath.PathExpr(XPath.java:4482)
	at org.apache.jackrabbit.core.query.xpath.XPath.ValueExpr(XPath.java:4125)
	at org.apache.jackrabbit.core.query.xpath.XPath.UnaryExpr(XPath.java:4032)
	at org.apache.jackrabbit.core.query.xpath.XPath.CastExpr(XPath.java:3935)
	at org.apache.jackrabbit.core.query.xpath.XPath.CastableExpr(XPath.java:3898)
	at org.apache.jackrabbit.core.query.xpath.XPath.TreatExpr(XPath.java:3861)
	at org.apache.jackrabbit.core.query.xpath.XPath.InstanceofExpr(XPath.java:3824)
	at org.apache.jackrabbit.core.query.xpath.XPath.IntersectExceptExpr(XPath.java:3748)
	at org.apache.jackrabbit.core.query.xpath.XPath.UnionExpr(XPath.java:3672)
	at org.apache.jackrabbit.core.query.xpath.XPath.MultiplicativeExpr(XPath.java:3622)
	at org.apache.jackrabbit.core.query.xpath.XPath.AdditiveExpr(XPath.java:3510)
	at org.apache.jackrabbit.core.query.xpath.XPath.RangeExpr(XPath.java:3451)
	at org.apache.jackrabbit.core.query.xpath.XPath.ComparisonExpr(XPath.java:3353)
	at org.apache.jackrabbit.core.query.xpath.XPath.AndExpr(XPath.java:3290)
	at org.apache.jackrabbit.core.query.xpath.XPath.OrExpr(XPath.java:3227)
	at org.apache.jackrabbit.core.query.xpath.XPath.ExprSingle(XPath.java:2214)
	at org.apache.jackrabbit.core.query.xpath.XPath.ForClause(XPath.java:2337)
	at org.apache.jackrabbit.core.query.xpath.XPath.FLWORExpr(XPath.java:2233)
	at org.apache.jackrabbit.core.query.xpath.XPath.ExprSingle(XPath.java:2133)
	at org.apache.jackrabbit.core.query.xpath.XPath.Expr(XPath.java:2094)
	at org.apache.jackrabbit.core.query.xpath.XPath.QueryBody(XPath.java:2066)
	at org.apache.jackrabbit.core.query.xpath.XPath.MainModule(XPath.java:512)
	at org.apache.jackrabbit.core.query.xpath.XPath.Module(XPath.java:387)
	at org.apache.jackrabbit.core.query.xpath.XPath.QueryList(XPath.java:151)
	at org.apache.jackrabbit.core.query.xpath.XPath.XPath2(XPath.java:118)
	at org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder.<init>(XPathQueryBuilder.java:224)
	at org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder.createQuery(XPathQueryBuilder.java:255)
	at org.apache.jackrabbit.core.query.QueryParser.parse(QueryParser.java:57)
	at org.apache.jackrabbit.core.query.lucene.QueryImpl.<init>(QueryImpl.java:119)
	at org.apache.jackrabbit.core.query.lucene.SearchIndex.createExecutableQuery(SearchIndex.java:158)
	at org.apache.jackrabbit.core.query.QueryImpl.<init>(QueryImpl.java:90)
	at org.apache.jackrabbit.core.SearchManager.createQuery(SearchManager.java:192)
	at org.apache.jackrabbit.core.query.QueryManagerImpl.createQuery(QueryManagerImpl.java:87)
	at org.apache.jackrabbit.test.api.query.IllegalXPathTest.testIllegalStatement(IllegalXPathTest.java:45)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:324)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at org.apache.jackrabbit.test.AbstractJCRTest.run(AbstractJCRTest.java:401)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:474)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:342)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:194)
"
1,"rep:similar in xpath does not workThis query //*[rep:similar(., '/content/en')] produces an exception:

24.09.2009 16:56:48.156 *ERROR* [0:0:0:0:0:0:0:1%0 [1253804208093] GET /libs/cq/search/content/querydebug.html HTTP/1.1] org.apache.sling.engine.impl.SlingMainServlet service: Uncaught SlingException java.lang.ArrayIndexOutOfBoundsException: -1
	at org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder.visit(LuceneQueryBuilder.java:612)
	at org.apache.jackrabbit.spi.commons.query.RelationQueryNode.accept(RelationQueryNode.java:115)
	at org.apache.jackrabbit.spi.commons.query.NAryQueryNode.acceptOperands(NAryQueryNode.java:143)
	at org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder.visit(LuceneQueryBuilder.java:489)
	at org.apache.jackrabbit.spi.commons.query.LocationStepQueryNode.accept(LocationStepQueryNode.java:166)
	at org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder.visit(LuceneQueryBuilder.java:468)
	at org.apache.jackrabbit.spi.commons.query.PathQueryNode.accept(PathQueryNode.java:74)
	at org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder.visit(LuceneQueryBuilder.java:257)
	at org.apache.jackrabbit.spi.commons.query.QueryRootNode.accept(QueryRootNode.java:115)
	at org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder.createLuceneQuery(LuceneQueryBuilder.java:247)
	at org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder.createQuery(LuceneQueryBuilder.java:227)
	at org.apache.jackrabbit.core.query.lucene.QueryImpl.execute(QueryImpl.java:111)
	at org.apache.jackrabbit.core.query.QueryImpl.execute(QueryImpl.java:127)
"
1,"BooleanQuery explain with boost==0BooleanWeight.explain() uses the returned score of subweights to determine if a clause matched.
If any required clause has boost==0, the returned score will be zero and the explain for the entire BooleanWeight will be simply  Explanation(0.0f, ""match required"").

I'm not sure what the correct fix is here.  I don't think it can be done based on score alone, since that isn't how scorers work.   Perhaps we need a new method ""boolean Explain.matched()"" that returns true on a match, regardless of what the score may be? 

Related to the problem above, even if no boosts are zero, it it sometimes nice to know *why* a particular query failed to match.  It would mean a longer explanation, but maybe we should include non matching explains too?"
1,"Brazilian Analyzer doesn't remove stopwords when uppercase is givenThe order of filters matter here, just need to apply lowercase token filter before removing stopwords

	result = new StopFilter( result, stoptable );
		result = new BrazilianStemFilter( result, excltable );
		// Convert to lowercase after stemming!
		result = new LowerCaseFilter( result );

Lowercase must come before BrazilianStemFilter

At the end of day I'll attach a patch, it's straightforward"
1,Benchmark deletes.alg failsBenchmark deletes.alg fails because the index reader defaults to open readonly.  
1,"Lucene fails to close file handles under certain situationsAs a followon to LUCENE-820, I've added a further check in
MockRAMDirectory to assert that there are no open files when the
directory is closed.

That check caused a few unit tests to fail, and in digging into the
reason I uncovered these cases where Lucene fails to close file
handles:

  * TermInfosReader.close() was setting its ThreadLocal enumerators to
    null without first closing the SegmentTermEnum in there.  It looks
    like this was part of the fix for LUCENE-436.  I just added the
    call to close.

    This is somewhat severe since we could leak many file handles for
    use cases that burn through threads and/or indexes.  Though,
    FSIndexInput does have a finalize() to close itself.

  * Flushing of deletes in IndexWriter opens SegmentReader to do the
    flushing, and it correctly calls close() to close the reader.  But
    if an exception is hit during commit and before actually closing,
    it will leave open those handles.  I fixed this first calling
    doCommit() and then doClose() in a finally.  The ""disk full"" tests
    we now have were hitting this.

  * IndexWriter's addIndexes(IndexReader[]) method was opening a
    reader but not closing it with a try/finally.  I just put a
    try/finally in.

I've also changed some unit tests to use MockRAMDirectory instead of
RAMDirectory to increase testing coverage of ""leaking open file
handles"".
"
1,"Unreferenced sessions should get garbage collectedIf an application opens many sessions and doesn't close them, they are never garbage collected. After some time, the virtual machine will run out of memory. This code will run out of memory after a few thousand logins:

Repository rep = new TransientRepository();
for (int i = 0; ; i++) {
  rep.login(new SimpleCredentials("""", new char[0]));
}

Using a finalizer to close SessionImpl doesn't work, because it seems there are references from the (hard referenced part of the cache) to the SessionImpl objects. Maybe it is possible to remove those references, or change them to weak references.
"
1,"IndexReader.termDocs() retrieves no documentsTermDocs object returned by indexReader.termDocs() retrieves no documents, howerver, the documents are retrieved correctly when using indexReader.termDocs(Term), indexReader.termDocs(null) and indexSearcher.search(Query)."
1,CompactNodeTypeDefWriter does not escaped names properlyCompactNodeTypeDefWriter does not escaped names properly. If the name includes a '-' or a '+' the names must be surrounded by single quotes.
1,"CartesianPolyFilterBuilder doesn't handle edge case around the 180 meridianTest case:  
Points all around the globe, plus two points at 0, 179.9 and 0,-179.9 (on each side of the meridian).  Then, do a Cartesian Tier filter on a point right near those two.  It will return all the points when it should just return those two.

The flawed logic is in the else clause below:
{code}
if (longX2 != 0.0) {
		//We are around the prime meridian
		if (longX == 0.0) {
			longX = longX2;
			longY = 0.0;
        	shape = getShapeLoop(shape,ctp,latX,longX,latY,longY);
		} else {//we are around the 180th longitude
			longX = longX2;
			longY = -180.0;
			shape = getShapeLoop(shape,ctp,latY,longY,latX,longX);
	}
{code}

Basically, the Y and X values are transposed.  This currently says go from longY (-180) all the way around  to longX which is the lower left longitude of the box formed.  Instead, it should go from the lower left long to -180."
1,"Escaped wildcard character in wildcard term not handled correctlyIf an escaped wildcard character is specified in a wildcard query, it is treated as a wildcard instead of a literal.
e.g., t\??t is converted by the QueryParser to t??t - the escape character is discarded."
1,Index recovery may fail when redo log contains nodes that are part of an index aggregateSearchIndex.mergeAggregatedNodeIndexes() will throw a NullPointerException because index is not yet set. The call is made from the recovery code that is triggered in the MultiIndex constructor.
1,"LockTest.testLogout fails to refresh session before checking lock from other sessionLockTest.testLogout() fails to refresh the session before checking the lock state of a node that was locked by another session.

Proposal:

Insert 

  n1.refresh(false);

before 

  assertTrue(""node must be locked"", n1.isLocked());

"
1,"JCAResourceAdapter must implement SerializableWe are running Weblogic 10.0 servers in cluster environment.   When deploying the rar, we always get this warning from weblogic stdout.log: 

<Jan 15, 2009 2:42:10 AM PST> <Warning> <Connector> <BEA-190155> <Compliance checking/validation of the resource adapter /home/user/jackrabbit_rar/jackrabbit-jca-1.5.0.rar resulted in the following warnings:  The ra.xml <resourceadapter-class> class 'org.apache.jackrabbit.jca.JCAResourceAdapter' should implement java.io.Serializable but does not.> 

When trying to do the JNDI lookup the repository, we got the error ""No Object found: jackrabbit|null"".   The jackrabbit entry in the jndi tree is visible only as a javax.naming.reference and not as the JCARepositoryHandle due to the above warning.  Due to that, we can't deploy jackrabbit-jca in Test/Production environment.  

I'm no expert in JCA, but feel it is fairly easy to implement Serializable for  JCAResourceAdapter.  Please help us out.
"
1,"Inflater.end() method not always called in FieldsReader
We've just found an insidious memory leak in our own application as we did not always call Deflater.end() and Inflater.end(). As documented here;

http://bugs.sun.com/view_bug.do?bug_id=4797189

The non-heap memory that the native zlib code uses is not freed in a timely manner.

FieldsWriter appears safe as no exception can be thrown between the Deflater's creation and end() as it uses a ByteArrayOutputStream

FieldsReader, however, is not safe. In the event of a DataFormatException the call to end() will not occur."
1,"Clustering: re-registration of nodetypes is not  synchronizedThe re-registration of nodetypes is not yet synchronized between clusternodes, although re-registration is already (partially) implemented in the NodeTypeRegistry."
1,"MS Excel Mime Type missing in MsExcelTextExtractor The MsExcelTextExtractor listens to mime type ""application/vnd.ms-excel"", but storing excels will result in mime type ""application/msexcel"", too. Such tagged files will not be indexed by the MsExcelTextExtractor. The class should register itself to both mime types like the MsWordTextExtractor does. "
1,"bogus positions create a corrumpt indexIts pretty common that positionIncrement can overflow, this happens really easily 
if people write analyzers that don't clearAttributes().

It used to be the case that if this happened (and perhaps still is in 3.x, i didnt check),
that IW would throw an exception.

But i couldnt find the code checking this, I wrote a test and it makes a corrumpt index..."
1,"RFC4918IfHeaderTest.testPutIfLockToken could fail with 412 Precondition FailedIn org.apache.jackrabbit.webdav.server.RFC4918IfHeaderTest:110 (webdav-test), 
the lock request is initialized with a timeout of 1800 milliseconds, which is rounded as Timeout: Second-1 (at org.apache.jackrabbit.webdav.header.TimeoutHeader:46).

The assertion in the finally block MUST fail (412, Precondition Failed) if the lock has expired (cf. RFC 4918, Section 10.4.10).

The lock request should be initialized with a higher timeout, at least several seconds."
1,"More Locale problems in LuceneThis is a followup to LUCENE-1836: I found some more Locale problems in Lucene with Date Formats. Even for simple date formats only consisting of numbers (like ISO dates), you should always give the US locale. Because the dates in DateTools should sort according to String.compare(), it is important, that the decimal digits are western ones. In some strange locales, this may be different. Whenever you want to format dates for internal formats you exspect to behave somehow, you should at least set the locale to US, which uses ASCII. Dates entered by users and displayed to users, should be formatted according to the default or a custom specified locale.
I also looked for DecimalFormat (especially used for padding numbers), but found no problems."
1,BoostingNearQuery doesn't have hashCode/equals
1,"preflex codec returns wrong terms if you use an empty field namespinoff from LUCENE-3473.

I have a standalone test for this... the termsenum is returning a bogus extra empty-term (I assume it has no postings, i didnt try).

This causes the checkindex test in LUCENE-3473 to fail, because there are 4 terms instead of 3. 

"
1,"Oracle JDBC Class Cast ExceptionWhen utilizing the OraclePersistenceManager (package org.apache.jackrabbit.core.persistence.db) (I realize this is marked as deprecated) we noticed during our migration from Jackrabbit 1.6.1 to 2.2.10/11 that when starting the application server an error message is displayed to us that indicates that the Connection object passed to the createTemporaryBlob method of the BLOB class can't be cast to oracle.jdbc.OracleConnection

Here the interesting lines from our log:
2012-03-15 17:15:47,926 ERROR [org.apache.jackrabbit.core.persistence.db.OraclePersistenceManager] failed to write node state: cafebabe-cafe-babe-cafe-babecafebabe
java.lang.ClassCastException: org.apache.commons.dbcp.PoolingDataSource$PoolGuardConnectionWrapper cannot be cast to oracle.jdbc.OracleConnection
	at oracle.sql.BLOB.createTemporary(BLOB.java:708)
	at org.apache.jackrabbit.core.persistence.db.OraclePersistenceManager.createTemporaryBlob(OraclePersistenceManager.java:375)

I want to highlight at this point that the do not see the issue when using the Oracle Bundled persistence manager, however due to the fact that we haven't used the bundled version in the past we have a lot of customers with repo layouts that can not be used by the bundled persistence manager - we ran some tests and noticed that the consistency check fails.
-> At the moment there is no good upgrade path to move a repo to the bundled structure, the paths provided thus far are shaky at best.

I did find a solution to the problem that has shown no issues thus far and wanted to share this with you:

It is a one line change that can be made before the wrapped connection is passed to the Oracle driver:
org.apache.jackrabbit.core.util.db.ConnectionFactory.unwrap(con);

This then solves the problem, I also wanted to share that we are using an XA datasource."
1,"Paths not correct after reordering childrenReordered, unsaved children of a node do not have the correct path. In the test case attached, the following operation is attempted with three SNS children named b[1], b[2], b[3]: the last element is ordered before the first three times, which should result in the initial children order.
"
1,"The repeats mechanism in SloppyPhraseScorer is broken when doc has tokens at same positionIn LUCENE-736 we made fixes to SloppyPhraseScorer, because it was
matching docs that it shouldn't; but I think those changes caused it
to fail to match docs that it should, specifically when the doc itself
has tokens at the same position.
"
1,"in trunk if you switch up omitNorms while indexing, you get a corrumpt norms filedocument 1 has 
  body: norms=true
  title: norms=true
document 2 has 
  body: norms=false
  title: norms=true

when seeing 'body' for the first time, normswriterperfield gets 'initial fieldinfo' and 
saves it away, which says norms=true

however, at flush time we dont check, so we write the norms happily anyway.
then SegmentReader reads the norms later: it skips ""body"" since it omits norms
and if you ask for the norms of 'title' it instead returns the bogus ""body"" norms.

asserting that SegmentReader ""plans to"" read the whole .nrm file exposes the bug."
1,"DefaultMethodRetryHandler bugDefaultMethodRetryHandler does not seem to test correctly for the number of
attempts to retry a given method. It seems to bail out one attempt too early:

if (executionCount >= this.retryCount) {
  // Do not retry if over max retry count
  return false;
}

For example, if I set the retryCount to 1, HttpClient does not retry the method
at all. At least that's what I'm seeing when I step through it with a debugger."
1,"ManageableCollectionUtil doesn't support MapsManageableCollectionUtil has two getManageableCollection methods, which do not currently return a ManageableCollection which wraps Maps. 

ManagedHashMap already exists in the codebase which I assume was created for this purpose, so both getManageableCollection methods could be modified so that they do something like:

            if (object instanceof Map){
                return new ManagedHashMap((Map)object);
            }


An alternative solution might be to modify the JCR mapping to support explicitly defining the 'ManagedXXX' class."
1,"LuceneDictionary skips first word in enumerationThe current code for LuceneDictionary will always skip the first word of the TermEnum. The reason is that it doesn't initially retrieve TermEnum.term - its first call is to TermEnum.next, which moves it past the first term (line 76).
To see this problem cause a failure, add this test to TestSpellChecker:
similar = spellChecker.suggestSimilar(""eihgt"",2);
      assertEquals(1, similar.length);
      assertEquals(similar[0], ""eight"");

Because ""eight"" is the first word in the index, it will fail.
"
1,NotQuery does not implement extractTerms()If the not() function is used in query together with the rep:excerpt() function an UnsupportedOperationException is thrown.
1,"jcr-server: DefaultItemCollection#unlock does not call DavSession#removeReferenceDefaultItemCollection#unlock does not remove the token-reference from the DavSession that has been added
before upon creating the lock. This causes pending lock-references thus the cache entries in JCRWebdavServer
will not be cleared filling up the cache although the locks have been properly released. 
"
1,"spi2davex: Batch fails to create/modify properties with non-ascii characters namesthe spi2davex batch implementation fails upon creation/modification of all property types that have their value sent as
separate stringpart or binarypart AND contain non-ascii characters in their property name.

from what i've seen this is due to a limitation in HttpClient 3.x Part#sendDispositionHeader that always writes the part name
as ascii-bytes. in a related discussion [1] specification compliance and usability were addressed.

looking at the server-side part revealed that org.apache.commons.fileupload.FileUploadBase#FileItemIteratorImpl
is prepared to receive non-ascii characters in a header value.
a simple test also showed that curl is perfectly able to send utf-8 part names.

based on this information and given the fact that spi2dav and the server-sided part are intended to communicate
with one other rather than with any kind of custom clients, i suggest to add a simple fix by patching the parts used
within spi2davex.

btw: in HttpClient 4.x there seems to be a workaround for this problem [2]

[1] http://www.mail-archive.com/httpclient-dev@jakarta.apache.org/msg04637.html
[2] https://issues.apache.org/jira/browse/HTTPCLIENT-293"
1,"FileInputStream never closed in HTMLParserHTMLParser.java contains this code: 
 
  public HTMLParser(File file) throws FileNotFoundException { 
    this(new FileInputStream(file)); 
  } 
 
This FileInputStream should be closed with the close() method, as there's no 
guarantee that the garbage collection will run and do this for you. I don't 
know how to fix this without changing the API to take a FileInputStream 
instead of a File, as the call to this() must be the first thing in the 
constructor, i.e. you cannot create the stream, call this(...), and then close 
the stream."
1,"WorkspaceManager.dispose() should wait until change feed thread is stoppedThe WorkspaceManager currently only interrupts the change feed thread, but does not wait until it stops."
1,"SSL does not seem to work at allWhenever I try to request content via https I get this exception:


Exception in thread ""main"" javax.net.ssl.SSLException: hostname in certificate didn't match: <140.211.11.131> != <*.apache.org>
	at org.apache.http.conn.ssl.AbstractVerifier.verify(AbstractVerifier.java:220)
	at org.apache.http.conn.ssl.BrowserCompatHostnameVerifier.verify(BrowserCompatHostnameVerifier.java:54)
	at org.apache.http.conn.ssl.AbstractVerifier.verify(AbstractVerifier.java:149)
	at org.apache.http.conn.ssl.AbstractVerifier.verify(AbstractVerifier.java:130)
	at org.apache.http.conn.ssl.SSLSocketFactory.createSocket(SSLSocketFactory.java:399)
	at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:143)
	at org.apache.http.impl.conn.AbstractPoolEntry.open(AbstractPoolEntry.java:149)
	at org.apache.http.impl.conn.AbstractPooledConnAdapter.open(AbstractPooledConnAdapter.java:108)
	at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:415)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:641)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:576)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:554)
	at HttpsTest.fails(HttpsTest.java:25)
	at HttpsTest.main(HttpsTest.java:12)


I can reproduce this whith the following code:


import org.apache.http.client.HttpClient;
import org.apache.http.client.methods.HttpGet;
import org.apache.http.impl.client.DefaultHttpClient;

public class HttpsTest {

    public static void main(final String[] args) throws Exception {
        final HttpClient client = new DefaultHttpClient();
        final HttpGet req = new HttpGet(""https://www.apache.org"");
        client.execute(req);
    }

}
"
1,"TestFSTs.testRealTerms produces a corrupt indexseems to be prox/skip related: the test passes, but the checkindex upon closing fails.

ant test-core -Dtestcase=TestFSTs -Dtests.seed=-4012305283315171209:0 -Dtests.multiplier=3 -Dtests.nightly=true -Dtests.linedocsfile=c:/data/enwiki.random.lines.txt.gz

Note: to get the enwiki.random.lines.txt.gz you have to fetch it from hudson (warning 1 gigabyte file).
you also have to run the test a few times to trigger it.

ill upload the index this thing makes to this issue.
"
1,"Highlighter doesn't support NumericRangeQuery or deprecated RangeQuerySucks. Will throw a NullPointer exception. 

Only NumericRangeQuery will throw the exception.
RangeQuery just won't highlight."
1,"SampleComparable doesn't work well in contrib/remote testsAs discovered in LUCENE-1749, when using identical instances of a SortComparator you get multiple entries in the FieldCache.

demonstrating this bug currently requires the patches in LUCENE-1749.

See markmiller's comment here...
https://issues.apache.org/jira/browse/LUCENE-1749?focusedCommentId=12735190#action_12735190"
1,"contrib-spatial java.lang.UnsupportedOperationException on QueryWrapperFilter.getDocIdSetWe use in our Project (which is in the devel phase) the latest Snapshot release of lucene. After i updated to the latest Snapshot a few days ago one of our JUnit tests fails and throws the following error:

java.lang.UnsupportedOperationException
	at org.apache.lucene.search.Query.createWeight(Query.java:91)
	at org.apache.lucene.search.QueryWrapperFilter.getDocIdSet(QueryWrapperFilter.java:72)
	at org.apache.lucene.misc.ChainedFilter.getDISI(ChainedFilter.java:150)
	at org.apache.lucene.misc.ChainedFilter.initialResult(ChainedFilter.java:173)
	at org.apache.lucene.misc.ChainedFilter.getDocIdSet(ChainedFilter.java:211)
	at org.apache.lucene.misc.ChainedFilter.getDocIdSet(ChainedFilter.java:141)
	at org.apache.lucene.search.ConstantScoreQuery$ConstantScorer.<init>(ConstantScoreQuery.java:116)
	at org.apache.lucene.search.ConstantScoreQuery$ConstantWeight.scorer(ConstantScoreQuery.java:81)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:244)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:172)
	at org.apache.lucene.search.Searcher.search(Searcher.java:183)
	at org.hibernate.search.query.QueryHits.updateTopDocs(QueryHits.java:100)
	at org.hibernate.search.query.QueryHits.<init>(QueryHits.java:61)
	at org.hibernate.search.query.QueryHits.<init>(QueryHits.java:51)
	at org.hibernate.search.query.FullTextQueryImpl.getQueryHits(FullTextQueryImpl.java:373)
	at org.hibernate.search.query.FullTextQueryImpl.list(FullTextQueryImpl.java:293)
	...

I think it appeared after the Hudson build 917... and the following commit of the Query.java http://hudson.zones.apache.org/hudson/job/Lucene-trunk/917/changes#detail4 and is in connection with this JIRA issue: LUCENE-1771
I hope i'm at the right place and that you can fix it. Thanks!"
1,"CloseableThreadLocal does not work well with Tomcat thread poolingWe tracked down a large memory leak (effectively a leak anyway) caused
by how Analyzer users CloseableThreadLocal.
CloseableThreadLocal.hardRefs holds references to Thread objects as
keys.  The problem is that it only frees these references in the set()
method, and SnowballAnalyzer will only call set() when it is used by a
NEW thread.

The problem scenario is as follows:

The server experiences a spike in usage (say by robots or whatever)
and many threads are created and referenced by
CloseableThreadLocal.hardRefs.  The server quiesces and lets many of
these threads expire normally.  Now we have a smaller, but adequate
thread pool.  So CloseableThreadLocal.set() may not be called by
SnowBallAnalyzer (via Analyzer) for a _long_ time.  The purge code is
never called, and these threads along with their thread local storage
(lucene related or not) is never cleaned up.

I think calling the purge code in both get() and set() would have
avoided this problem, but is potentially expensive.  Perhaps using 
WeakHashMap instead of HashMap may also have helped.  WeakHashMap 
purges on get() and set().  So this might be an efficient way to
clean up threads in get(), while set() might do the more expensive
Map.keySet() iteration.

Our current work around is to not share SnowBallAnalyzer instances
among HTTP searcher threads.  We open and close one on every request.

Thanks,
Matt"
1,"JCR-SQL2 query with multiple columns in result only returns last column when using Row.getValues()When running a query like below on an in-process repository (via TransientRepository) or via RMI access, a call to Row.getValues() only returns the last column selected:

       SELECT property1, property2 FROM [nodetype]

QueryResult.getColumnNames() returns the right set of columns.

Stepping through the code shows that org.apache.jackrabbit.core.query.lucene.join.AbstractRow has the implementation of getValues() - this creates a new Values array, then overwrites it multiple times in a for loop that iterates once per column. That doesn't sound like the desired behaviour.

Getting values via individual calls to Row.getValue(""property1"") gives the correct results.

"
1,"document view: importXML() fails on protected property jcr:primaryTypewhen trying to import an xml document where elements contain the attribute jcr:primaryType the import fails with:

javax.jcr.nodetype.ConstraintViolationException: cannot set the value of a protected property /testroot/docviewimport/doc/jcr:primaryType
	at org.apache.jackrabbit.core.PropertyImpl.setValue(PropertyImpl.java:907)
	at org.apache.jackrabbit.core.NodeImpl.setProperty(NodeImpl.java:1044)
	at org.apache.jackrabbit.core.xml.DocViewImportHandler.startElement(DocViewImportHandler.java:124)
	at org.apache.jackrabbit.core.xml.ImportHandler.startElement(ImportHandler.java:164)
	at org.apache.xerces.parsers.AbstractSAXParser.startElement(Unknown Source)
	at org.apache.xerces.impl.XMLNSDocumentScannerImpl.scanStartElement(Unknown Source)
	at org.apache.xerces.impl.XMLNSDocumentScannerImpl$NSContentDispatcher.scanRootElementHook(Unknown Source)
	at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl$FragmentContentDispatcher.dispatch(Unknown Source)
	at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanDocument(Unknown Source)
	at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
	at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
	at org.apache.xerces.parsers.XMLParser.parse(Unknown Source)
	at org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source)
	at org.apache.jackrabbit.core.SessionImpl.importXML(SessionImpl.java:836)
	at org.apache.jackrabbit.test.api.DocViewImportTest.setUp(DocViewImportTest.java:92)
	at com.intellij.rt.execution.junit2.JUnitStarter.main(JUnitStarter.java:31)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:78)

if i understand the spec correctly, the import process should take care of this attribute and determine the node type of the new nodes based on it."
1,"DefaultLoginModule/SimpleLoginModule don't support custom PrincipalProviderWhen configuring a custom PrincipalProvider for the SimpleLoginModule or DefaultLoginModule, inside of a repository.xml file with configuration such as the following:

    <Security appName=""Jackrabbit"">
        <AccessManager
            class=""org.apache.jackrabbit.core.security.DefaultAccessManager"">
        </AccessManager>
        <LoginModule
            class=""org.apache.jackrabbit.core.security.authentication.DefaultLoginModule"">
            <param name=""principalprovider"" value=""com.foo.jcr.BasicPrincipalProvider""/>
        </LoginModule>
      <SecurityManager class=""org.apache.jackrabbit.core.DefaultSecurityManager"">         
      </SecurityManager>    
    </Security>

And that yields the following stacktrace:

javax.jcr.LoginException: org.apache.jackrabbit.core.security.authentication.DefaultLoginModule does not support 'principalprovider: org.apache.jackrabbit.core.security.authentication.DefaultLoginModule does not support 'principalprovider: org.apache.jackrabbit.core.security.authentication.DefaultLoginModule does not support 'principalprovider
	at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1353)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:53)
	at com.foo.jcr.PrincipalProviderTest.testPrincipalProvider(PrincipalProviderTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at org.junit.internal.runners.TestMethod.invoke(TestMethod.java:59)
	at org.junit.internal.runners.MethodRoadie.runTestMethod(MethodRoadie.java:98)
	at org.junit.internal.runners.MethodRoadie$2.run(MethodRoadie.java:79)
	at org.junit.internal.runners.MethodRoadie.runBeforesThenTestThenAfters(MethodRoadie.java:87)
	at org.junit.internal.runners.MethodRoadie.runTest(MethodRoadie.java:77)
	at org.junit.internal.runners.MethodRoadie.run(MethodRoadie.java:42)
	at org.junit.internal.runners.JUnit4ClassRunner.invokeTestMethod(JUnit4ClassRunner.java:88)
	at org.junit.internal.runners.JUnit4ClassRunner.runMethods(JUnit4ClassRunner.java:51)
	at org.junit.internal.runners.JUnit4ClassRunner$1.run(JUnit4ClassRunner.java:44)
	at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:27)
	at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:37)
	at org.junit.internal.runners.JUnit4ClassRunner.run(JUnit4ClassRunner.java:42)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:45)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)
Caused by: javax.security.auth.login.LoginException: org.apache.jackrabbit.core.security.authentication.DefaultLoginModule does not support 'principalprovider
	at org.apache.jackrabbit.core.security.authentication.LocalAuthContext.login(LocalAuthContext.java:68)
	at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1346)
	... 24 more
javax.security.auth.login.LoginException: org.apache.jackrabbit.core.security.authentication.DefaultLoginModule does not support 'principalprovider
	at org.apache.jackrabbit.core.security.authentication.LocalAuthContext.login(LocalAuthContext.java:68)
	at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1346)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:53)
	at com.foo.jcr.PrincipalProviderTest.testPrincipalProvider(PrincipalProviderTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at org.junit.internal.runners.TestMethod.invoke(TestMethod.java:59)
	at org.junit.internal.runners.MethodRoadie.runTestMethod(MethodRoadie.java:98)
	at org.junit.internal.runners.MethodRoadie$2.run(MethodRoadie.java:79)
	at org.junit.internal.runners.MethodRoadie.runBeforesThenTestThenAfters(MethodRoadie.java:87)
	at org.junit.internal.runners.MethodRoadie.runTest(MethodRoadie.java:77)
	at org.junit.internal.runners.MethodRoadie.run(MethodRoadie.java:42)
	at org.junit.internal.runners.JUnit4ClassRunner.invokeTestMethod(JUnit4ClassRunner.java:88)
	at org.junit.internal.runners.JUnit4ClassRunner.runMethods(JUnit4ClassRunner.java:51)
	at org.junit.internal.runners.JUnit4ClassRunner$1.run(JUnit4ClassRunner.java:44)
	at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:27)
	at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:37)
	at org.junit.internal.runners.JUnit4ClassRunner.run(JUnit4ClassRunner.java:42)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:45)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)"
1,"DBDataStore doesn't support concurrent readsMy understanding is that setting parameter copyWhenReading to true should allow concurrent reads by spooling binary property to temporary file and free database resources (connection) immediately to make it available for other threads.

After applying patch for JCR-1388, DBDataStore doesn't support concurrent reads anymore, resultSet is kept open and db connection is blocked until the stream is read and closed. When copyWhenReading is set to true db connection should be released immediately, this is the reason i guess why temporary file is used."
1,"The CredentialsWrapper should use a empty String as userId if custom Credentials are usedIf custom Credentials are used we get a IllegalArgumentException from the AbstractQValueFactory while executing SessionItemStateManager.computeSystemGeneratedPropertyValues().
The 2 Properties jcr:createdBy and jcr:lastModified could not be created."
1,ItemManager registers itself as listener too earlyThis is similar to JCR-2168 but for ItemManager and SessionItemStateManager.
1,"Path returned by FileSystemBLOBStore.createId() is not absoluteHi,

I have developed my own FileSystem in which I call FileSystemPathUtil.checkFormat(path) for every operation on the file system.
When the file system is called to store a BLOB value, the path I get is always relative, resulting in a ""not an absolute path"" FileSystemException.

The problem has been traced back to org.apache.jackrabbit.core.state.util.FileSystemBLOBStore.creatId().
I think there should be a:
   sb.append(FileSystem.SEPARATOR_CHAR);
before the for loop.

Thanks."
1,"cache module produces improperly formatted Warning header when revalidation failsThe warning header currently attached to a stale response by the caching module when validation with the origin server fails is not a properly-formatted Warning header.

http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.46"
1,"TestGrouping failure{noformat}
ant test -Dtestcase=TestGrouping -Dtestmethod=testRandom -Dtests.seed=295cdb78b4a442d4:-4c5d64ef4d698c27:-425d4c1eb87211ba
{noformat}

fails with this on current trunk:

{noformat}

    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestGrouping -Dtestmethod=testRandom -Dtests.seed=295cdb78b4a442d4:-4c5d64ef4d698c27:-425d4c1eb87211ba
    [junit] NOTE: test params are: codec=RandomCodecProvider: {id=MockRandom, content=MockSep, sort2=SimpleText, groupend=Pulsing(freqCutoff=3 minBlockSize=65 maxBlockSize=132), sort1=Memory, group=Memory}, sim=RandomSimilarityProvider(queryNorm=true,coord=false): {id=DFR I(F)L2, content=DFR BeL3(800.0), sort2=DFR GL3(800.0), groupend=DFR G2, sort1=DFR GB3(800.0), group=LM Jelinek-Mercer(0.700000)}, locale=zh_TW, timezone=America/Indiana/Indianapolis
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestGrouping]
    [junit] NOTE: Linux 2.6.33.6-147.fc13.x86_64 amd64/Sun Microsystems Inc. 1.6.0_21 (64-bit)/cpus=24,threads=1,free=143246344,total=281804800
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testRandom(org.apache.lucene.search.grouping.TestGrouping):	FAILED
    [junit] expected:<11> but was:<7>
    [junit] junit.framework.AssertionFailedError: expected:<11> but was:<7>
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:148)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:50)
    [junit] 	at org.apache.lucene.search.grouping.TestGrouping.assertEquals(TestGrouping.java:980)
    [junit] 	at org.apache.lucene.search.grouping.TestGrouping.testRandom(TestGrouping.java:865)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$2$1.evaluate(LuceneTestCase.java:611)
    [junit] 
    [junit] 
{noformat}

I dug for a while... the test is a bit sneaky because it compares sorted docs (by score) across 2 indexes.  Index #1 has no deletions; Index #2 has same docs, but organized into doc blocks by group, and has some deletions.  In theory (I think) even though the deletions will cause scores to differ across the two indices, it should not alter the sort order of the docs.  Here is the explain output of the docs that sorted differently:

{noformat}
#1: top hit in the ""has deletes doc-block"" index (id=239):

explain: 2.394486 = (MATCH) weight(content:real1 in 292)
[DFRSimilarity], result of:
 2.394486 = score(DFRSimilarity, doc=292, freq=1.0), computed from:
   1.0 = termFreq=1
   41.944084 = NormalizationH3, computed from:
     1.0 = tf
     5.3102274 = avgFieldLength
     2.56 = len
   102.829 = BasicModelBE, computed from:
     41.944084 = tfn
     880.0 = numberOfDocuments
     239.0 = totalTermFreq
   0.023286095 = AfterEffectL, computed from:
     41.944084 = tfn


#2: hit in the ""no deletes normal index"" (id=229)

ID=229 explain=2.382285 = (MATCH) weight(content:real1 in 225)
[DFRSimilarity], result of:
 2.382285 = score(DFRSimilarity, doc=225, freq=1.0), computed from:
   1.0 = termFreq=1
   41.765594 = NormalizationH3, computed from:
     1.0 = tf
     5.3218827 = avgFieldLength
     10.24 = len
   101.879845 = BasicModelBE, computed from:
     41.765594 = tfn
     786.0 = numberOfDocuments
     215.0 = totalTermFreq
   0.023383282 = AfterEffectL, computed from:
     41.765594 = tfn

Then I went and called explain on the ""no deletes normal index"" for
the top doc (id=239):

explain: 2.3822558 = (MATCH) weight(content:real1 in 17)
[DFRSimilarity], result of:
 2.3822558 = score(DFRSimilarity, doc=17, freq=1.0), computed from:
   1.0 = termFreq=1
   42.165264 = NormalizationH3, computed from:
     1.0 = tf
     5.3218827 = avgFieldLength
     2.56 = len
   102.8307 = BasicModelBE, computed from:
     42.165264 = tfn
     786.0 = numberOfDocuments
     215.0 = totalTermFreq
   0.023166776 = AfterEffectL, computed from:
     42.165264 = tfn
{noformat}"
1,"EventFilter misses Events for same NodetypeIf an ObservationListener registers with a NodeType-filter, 
it only gets informed about events on Sub-NodeTypes of the ones specified in the filter but not on the NodeType itself.

Example:
========
ObservationManager om = wsp.getObservationManager();
om.addEventListener(listener, Event.PROPERTY_ADDED, ""/"", true, null, new String[]{""nt:unstructured""}, true);

would receive notifications on nodes of type ""rep:root"", which is based on ""nt:unstructured"" but not of ""nt:unstructured""


"
1,"Deadlock inside XASession on WeblogicIn one of our client deployments on WebLogic 9.2 we observed JackRabbit sessions going stale in a load test. This was observed against release 1.6.1 (to which we migrated due to concurrency related issues JCR-2081 and JCR-2237). Same effect with 2.0.0.
 
I could finally reproduce this issue locally. And it seems to boil down to WLS invoking the sequence of <prepare> ... <release> ... <commit> on one XA session from multiple threads, as it seems breaking assumptions of the thread-bound java.util.concurrent-RWLock based DefaultISMLocking class.
Effectively the setActiveXid(..) method on DefaultISMLocking$RWLock fails as the old active XID was not yet cleared. With the result of more and more sessions deadlocking in below's invocation stack.

{code}
""[ACTIVE] ExecuteThread: '27' for queue: 'weblogic.kernel.Default (self-tuning)'"" daemon prio=1 tid=0x33fc3ec0 nid=0x2324 in Object.wait() [0x2156a000..0x2156beb0] at java.lang.Object.wait(Native Method) - waiting on <0x68a54698> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock) at java.lang.Object.wait(Object.java:474) at EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock.acquire(Unknown Source) - locked <0x68a54698> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock) at org.apache.jackrabbit.core.state.DefaultISMLocking$1.<init>(DefaultISMLocking.java:64) at org.apache.jackrabbit.core.state.DefaultISMLocking.acquireWriteLock(DefaultISMLocking.java:61) at org.apache.jackrabbit.core.version.AbstractVersionManager.acquireWriteLock(AbstractVersionManager.java:146) at org.apache.jackrabbit.core.version.XAVersionManager$1.prepare(XAVersionManager.java:562) at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:154) - locked <0x6dc2ad88> (a org.apache.jackrabbit.core.TransactionContext) at org.apache.jackrabbit.core.XASessionImpl.prepare(XASessionImpl.java:331) at org.apache.jackrabbit.jca.TransactionBoundXAResource.prepare(TransactionBoundXAResource.java:68) at weblogic.connector.security.layer.AdapterLayer.prepare(AdapterLayer.java:397) at weblogic.connector.transaction.outbound.XAWrapper.prepare(XAWrapper.java:297) at weblogic.transaction.internal.XAServerResourceInfo.prepare(XAServerResourceInfo.java:1276) at weblogic.transaction.internal.XAServerResourceInfo.prepare(XAServerResourceInfo.java:499) at weblogic.transaction.internal.ServerSCInfo$1.execute(ServerSCInfo.java:335) at weblogic.kernel.Kernel.executeIfIdle(Kernel.java:243) at weblogic.transaction.internal.ServerSCInfo.startPrepare(ServerSCInfo.java:326) at weblogic.transaction.internal.ServerTransactionImpl.localPrepare(ServerTransactionImpl.java:2516) at weblogic.transaction.internal.ServerTransactionImpl.globalPrepare(ServerTransactionImpl.java:2211) at weblogic.transaction.internal.ServerTransactionImpl.internalCommit(ServerTransactionImpl.java:266) at weblogic.transaction.internal.ServerTransactionImpl.commit(ServerTransactionImpl.java:227) at weblogic.transaction.internal.TransactionManagerImpl.commit(TransactionManagerImpl.java:283) at org.springframework.transaction.jta.JtaTransactionManager.doCommit(JtaTransactionManager.java:1028) at org.springframework.transaction.support.AbstractPlatformTransactionManager.processCommit(AbstractPlatformTransactionManager.java:709) at org.springframework.transaction.support.AbstractPlatformTransactionManager.commit(AbstractPlatformTransactionManager.java:678)
{code}"
1,"Workspace is shut down while creating initial indexThis only happens when a maxIdleTime is configured for the workspaces in the repository.xml and the workspace to index is not the default workspace.

The idle check considers a workspace as idle when there only a system session is open and the configured idle time elapsed. This is also the case when the workspace is initializing.

The repository should either check if a workspace is still initializing or we need to move the search manager initialization into the WorkspaceInfo.doInitialize() method.

"
1,"ClassCastException bei unregisterNodeTypeI have a NodeType with various childnodes which I want to unregister. If I call:

    
      NodeTypeManager ndmg = session.getWorkspace().getNodeTypeManager();
      NodeTypeRegistry ntReg = ((NodeTypeManagerImpl) ndmg).getNodeTypeRegistry();
      ntReg.unregisterNodeType(new QName(""testURI"",""Page""));


I get a 

java.lang.ClassCastException
 at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.getDependentNodeTypes(NodeTypeRegistry.java:1242)
 at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.unregisterNodeType(NodeTypeRegistry.java:1120)
 at de.freaquac.test.JCRTest.main(JCRTest.java:80)

It looks to me like there are QNames in the Iterator but NodeTyeDefs are expected.
"
1,"Removed version is not invalidatedwhen a version is removed, it's internal represenation is not evicted from the cache. this can leed to unexpected behaviours. XATest.removeVersion() tests this. this also happens in a non-transactional environment."
1,"Cookies with ',' in the value string is not parsed correctly in some casesThis version extracts the ""Set-Cookie"" statementes of the following
HTTP response headers incorrectly.

The HTTP response is sent when executing GET method on --->
""http://my.taishinbank.com.tw/netbank/nbslogin.asp?
subFunID=https://my.taishinbank.com.tw/netbank/AccountQuery/QAccbyID.asp""

After the HttpClient extracts Set-Cookie from the response, it generates a wrong
cookie statement---->

  [INFO] wire - ->> ""Cookie: $Version=0; _mysite=520163500; 1027657033=null; 
   1027787539=null; 0=null; $Path=/; cata=11; $Path=/;   
   ASPSESSIONIDGGGQQXEU=ADLCDAGAJLKEBJEKBOMMAMOB; 
   $Path=/""

, where it shall 
be ""_mysite=520163500,1027657033,1027787539,1027787539,0;"" ,but 
not ""_mysite=520163500; 1027657033=null; 1027787539=null; 0=null;""
 

Thank you"
1,"TestStressIndexing has intermittent failuresSee http://www.gossamer-threads.com/lists/lucene/java-dev/55092 copied below:

 OK, I have seen this twice in the last two days:
Testsuite: org.apache.lucene.index.TestStressIndexing
[junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 18.58
sec
[junit]
[junit] ------------- Standard Output ---------------
[junit] java.lang.NullPointerException
[junit] at
org.apache.lucene.store.RAMInputStream.readByte(RAMInputStream.java:67)
[junit] at
org.apache.lucene.store.IndexInput.readInt(IndexInput.java:66)
[junit] at org.apache.lucene.index.SegmentInfos
$FindSegmentsFile.run(SegmentInfos.java:544)
[junit] at
org
.apache
.lucene.index.DirectoryIndexReader.open(DirectoryIndexReader.java:63)
[junit] at
org.apache.lucene.index.IndexReader.open(IndexReader.java:209)
[junit] at
org.apache.lucene.index.IndexReader.open(IndexReader.java:192)
[junit] at
org.apache.lucene.search.IndexSearcher.<init>(IndexSearcher.java:56)
[junit] at org.apache.lucene.index.TestStressIndexing
$SearcherThread.doWork(TestStressIndexing.java:111)
[junit] at org.apache.lucene.index.TestStressIndexing
$TimedThread.run(TestStressIndexing.java:55)
[junit] ------------- ---------------- ---------------
[junit] Testcase:
testStressIndexAndSearching
(org.apache.lucene.index.TestStressIndexing): FAILED
[junit] hit unexpected exception in search1
[junit] junit.framework.AssertionFailedError: hit unexpected
exception in search1
[junit] at
org
.apache
.lucene.index.TestStressIndexing.runStressTest(TestStressIndexing.java:
159)
[junit] at
org
.apache
.lucene
.index
.TestStressIndexing
.testStressIndexAndSearching(TestStressIndexing.java:187)
[junit]
[junit]
[junit] Test org.apache.lucene.index.TestStressIndexing FAILED

Subsequent runs have, however passed. Has anyone else hit this on
trunk?

I am running using ""ant clean test""

I'm on a Mac Pro 4 core, 4GB machine, if that helps at all. Not sure
how to reproduce at this point, but strikes me as a threading issue.
Oh joy!

I'll try to investigate more tomorrow to see if I can dream up a test
case.

-Grant 

"
1,"MMapDirectory chunking is buggyMMapDirectory uses chunking with MultiMMapIndexInput.
 
Because Java's ByteBuffer uses an int to address the
values, it's necessary to access a file >
Integer.MAX_VALUE in size using multiple byte buffers.

But i noticed from the clover report the entire MultiMMapIndexInput class is completely untested: no surprise since all tests make tiny indexes.
"
1,"Lock expires almost immediatelyWhen a timeoutHint other than Long.MAX_VALUE is given to the javax.jcr.lock.LockManager API:

   lock(String absPath, boolean isDeep, boolean isSessionScoped, long timeoutHint, String ownerInfo)

a timeoutTime in seconds will be computed as follows (o.a.j.core.lock.LockInfo#updateTimeoutTime):

   long now = (System.currentTimeMillis() + 999) / 1000; // round up
   this.timeoutTime = now + timeoutHint;

the TimeoutHandler in o.a.j.core.lock.LockManagerImpl running every second will then check whether the timeout has expired (o.a.j.core.lock.LockInfo#isExpired):

    public boolean isExpired() {
        return timeoutTime != Long.MAX_VALUE
            && timeoutTime * 1000 > System.currentTimeMillis();
    }

Obviously, the latter condition is true from the very beginning. Replacing '>' with '<' or '<=' should do the trick."
1,I/O exceptions can cause loss of buffered deletesSome I/O exceptions that result in segmentInfos rollback operations can cause buffered deletes that existed before the rollback creation point to be incorrectly lost when the IOException triggers a rollback.
1,"Deadlock case in IndexWriter on exception just before flushIf a document hits a non-aborting exception, eg something goes wrong
in tokenStream.next(), and, that document had triggered a flush
(due to RAM or doc count) then DocumentsWriter will deadlock because
that thread marks the flush as pending but fails to clear it on
exception.

I have a simple test case showing this, and a fix fixing it."
1,"ZombieHierarchyManager can return wrong child node entries for replaced nodesThe ZombieHierarchyManager currently implements the two getChildNodeEntry methods like this:

1) look up child node in old, overlayed state, which might contain removed child nodes
2) if not found, ask the super implementation (ie. get the child node from the up-to-date list)

The purpose of the ZombieHM is to be able to return removed item ids from the attic. However, the behavior above is IMO wrong, as it should first find an existing child node with the given name (or id):

1) look up child node in super implementation (ie. get the child node from the up-to-date list)
2) if not found, look in the old, overlayed state if it might have been removed

I was able to reproduce this issue when replacing a node (but note the custom access manager in 1.4.x used as explained below): create /replaced/subnode structure, save the session, remove the replaced node and add /replaced and then /replaced/subnode again:

        Node rootNode = session.getRootNode();
        
        // 1. create structure /replaced/subnode
        Node test = rootNode.addNode(""replaced"", NT);
        test.addNode(""subnode"", NT);
        // 2. persist changes
        session.save();

        // 3. remove node and recreate it
        test.remove();
        test = rootNode.addNode(""replaced"", NT);
        
        // 4. create previous child with same name
        test.addNode(""subnode"", NT);
        
        // 5. => gives exception
        test.getNode(""subnode"").getNodes();

To complicate things further, this was only triggered by a custom access manager, and all based upon Jackrabbit 1.4.x. Back then (pre-1.5 and new security stuff era), the access manager would get a ZombieHM as its hierarchy manager. If its implementation called resolvePath() on the HM for checking read-access in the final getNodes() call, where the tree will be traversed using the getChildNdeEntry(NodeState, Name, int) method, it would get the old node id and hence fail if it would try to retrieve it from the real item state manager.

Thus with a Jackrabbit >= 1.5 and 2.0 the above code will work fine, because the ZombieHM is not used.

However, we might want to fix it for 1.4.x and also check the other uses of the ZombieHM in the current trunk, which I couldn't test. These are (explicit and implicit): ChangeLogBasedHierarchyMgr, SessionItemStateManager.getDescendantTransientItemStates(NodeId), ItemImpl.validateTransientItems(Iterable<ItemState>, Iterable<ItemState>) and SessionItemStateManager.getDescendantTransientItemStatesInAttic(NodeId).
"
1,"Wildcard query with no wildcard characters in the term throws StringIndexOutOfBounds exception
Query q1 = new WildcardQuery(new Term(""Text"", ""a""));
Hits hits = searcher.search(q1);


Caught Exception
java.lang.StringIndexOutOfBoundsException : String index out of range: -1
    at java.lang.String.substring(Unknown Source)
    at org.apache.lucene.search.WildcardTermEnum.<init>(WildcardTermEnum.java:65)
    at org.apache.lucene.search.WildcardQuery.getEnum (WildcardQuery.java:38)
    at org.apache.lucene.search.MultiTermQuery.rewrite(MultiTermQuery.java:54)
    at org.apache.lucene.search.IndexSearcher.rewrite(IndexSearcher.java:137)
    at org.apache.lucene.search.Query.weight (Query.java:92)
    at org.apache.lucene.search.Hits.<init>(Hits.java:41)
    at org.apache.lucene.search.Searcher.search(Searcher.java:44)
    at org.apache.lucene.search.Searcher.search(Searcher.java:36)
    at QuickTest.main(QuickTest.java:45)


From Erik Hatcher

Feel free to log this as a bug report in our JIRA issue tracker.  It
seems like a reasonable change to make, such that a WildcardQuery
without a wildcard character would behave like TermQuery."
1,"Http Authentication with invalid credentials causes infinite loopAt HttpMethodBase(460), a break statement is executed only if
log.isInfoEnabled(). The break statement needs to be moved outside of the if
statement so that it breaks if realms already contains foo. Patch submitted on
mailing list as per Apache site guidelines."
1,"OverlappingFileLockException with JRE 1.6Per email discussion:
On Mon, 2007-02-26 at 10:26 +0100, Marcel Reutegger wrote:
> > just my 2c, I didn't really investigated this issue in more detail...
> >
> > according to the javadoc of FileChannel.tryLock() the
> > OverlappingFileLockException is thrown if the JVM already holds a lock on the
> > channel.
> >
> > in contrast, the current check in the repository startup method primarily
> > focuses on the situation where *two* JVMs start a repository on the same home
> > directory.
> >
> > I'd say the OverlappingFileLockException is thrown because two repository
> > instances are startup within the *same* JVM using the same repository home
> > directory.
> >
> > I suggest we add a catch clause, which also covers OverlappingFileLockException
> > in addition to IOException.
> >
> > regards
> >   marcel
> >
> > Stefan Guggisberg wrote:
> > > btw, afaik OverlappingFileLockException is only thrown on linux,
> > > FileChannel#getLock on windows e.g. returns null in the same situation.
> > >
> > > you might want to test on a different platform to further isolate the
> > > issue.
> > > you could also place a breakpoint at the top of the
> > > RepositoryImpl#acquireRepositoryLock
> > > method, step through the code, verify the contents of your fs etc.
> >
>


=== Original email

On 2/19/07, Patrick Haggood <codezilla@> wrote:
I'm using Linux, Sun Java 6 and Jackrabbit 1.3 with Derby persistance.
I have a putNode(object) function that's giving the above error in unit
tests.  It always fails after the second update, even when I swap tests
(i.e. save user doc then save user).  Prior to each test, I delete the
repository directory.

Do I need to set explicit locks before/after each session.save()?

*********** Unit Test
DBConn dbc;

    public SessionUtilTest(String testName) {
        super(testName);
        dbc = new DBConn();
    }

// Note - putUser and putDocument both use putNode after determining
which rootnode will be used

   /**
     * Test of putUnityUser method, of class unityjsr170.jr.SessionUtil.
     */
    public void testPutUnityUser() {
        System.out.println(""putUnityUser"");
        UnityUser usr = usr1;
        SessionUtil instance = dbc.getSutil();
        String result = instance.putUnityUser(usr1);
        assertNotNull(result);
        usr = (UnityUser) instance.getUnityUserByID(result);
        assertEquals(usr1.getName(),usr.getName());
    }
       
    /**
     * Test of putUnityDocument method, of class
unityjsr170.jr.SessionUtil.
     */
    public void testPutUnityDocument() {
        System.out.println(""putUnityDocument"");
        UnityDocument udoc = adr1;
        SessionUtil instance = dbc.getSutil();
        String result = instance.putUnityDocument(udoc);   <---- File
Lock Error
        assertNotNull(result);
        udoc = (UnityDocument) instance.getUnityDocumentByID(result);
        assertEquals(adr1.getName(),udoc.getName());
    }


********* Here's where I setup my repository connection

    public DBConn(){
        sutil = null;
        try {
            rp = new TransientRepository();
            sutil= new SessionUtil(rp);
        } catch (IOException ex) {
            ex.printStackTrace();
        }
    }
    
    public void shutdown(){
        sutil.closeAll();
    }
    
    public SessionUtil getSutil(){
        return sutil;
    }

****************  SessionUtil

    public SessionUtil(Repository rp){
        try {
            session = rp.login(new
SimpleCredentials(""username"",""password"".toCharArray()));
            
        } catch (LoginException ex) {
            ex.printStackTrace();
        } catch (RepositoryException ex) {
            ex.printStackTrace();
        } 
        
    }
    
    public void closeAll(){
        try {
            session.logout();
        } catch (Exception ex) {
            ex.printStackTrace();
            System.out.println(""Error closing repository"");
        }
    }
    
 // Put a node on the tree under the root node, return the uuid of the
new or updated node
    private String putNode(String nodetype, UnityBaseObject ubo){
        String resultuuid =null;
        String uname = ubo.getName();
        String utype = ubo.getType();
        String objectuid = ubo.getId();
        Node pnode; //  node to add or update
        Session ses = null;
        try {
            ses = getSession();
            // Does updateable node already have node Id?
            if (objectuid==null) {
                Node rn = ses.getRootNode();
                pnode = rn.addNode(utype);
                pnode.addMixin(""mix:referenceable"");
            } else{
                // grab existing node by uuid
                pnode = ses.getNodeByUUID(objectuid);
            }
            // Did we get an updateable node?
            if (pnode!=null){
                ubo.setId(pnode.getUUID());
                String unityXML =
utrans.getXMLStringFromUnityBaseObject(ubo);
                // update all the properties
                pnode.setProperty(""name"",ubo.getName());
                pnode.setProperty(""type"",ubo.getType());
                pnode.setProperty(""xmldata"",unityXML);
                ses.save();
                resultuuid = ubo.getId();
            }
        } catch(Exception e) {
            e.printStackTrace();
        } 
        return resultuuid;
    }

    private Session getSession(){
        return session;
    }
    

************  repository.xml

 <Workspace name=""${wsp.name}"">
        <FileSystem
class=""org.apache.jackrabbit.core.fs.local.LocalFileSystem"">
            <param name=""path"" value=""${wsp.home}""/>
        </FileSystem>
        <PersistenceManager
class=""org.apache.jackrabbit.core.state.db.DerbyPersistenceManager"">
            <param name=""url"" value=""jdbc:derby:
${wsp.home}/db;create=true""/>
            <param name=""schemaObjectPrefix"" value=""${wsp.name}_""/>
        </PersistenceManager>
        <SearchIndex
class=""org.apache.jackrabbit.core.query.lucene.SearchIndex"">
            <param name=""path"" value=""${wsp.home}/index""/>
            <param name=""useCompoundFile"" value=""true""/>
            <param name=""minMergeDocs"" value=""100""/>
            <param name=""volatileIdleTime"" value=""3""/>
            <param name=""maxMergeDocs"" value=""100000""/>
            <param name=""mergeFactor"" value=""10""/>
            <param name=""bufferSize"" value=""10""/>
            <param name=""cacheSize"" value=""1000""/>
            <param name=""forceConsistencyCheck"" value=""false""/>
            <param name=""autoRepair"" value=""true""/>
            <param name=""analyzer""
value=""org.apache.lucene.analysis.standard.StandardAnalyzer""/>
        </SearchIndex>
    </Workspace>

"
1,"Intermittent failure in TestIndexWriter.testCommitThreadSafetyMark's while(1) hudson box found this failure (and I can repro it too):

{noformat}
Error Message

MockRAMDirectory: cannot close: there are still open files: {_1m.cfs=1,
_1k.cfs=1, _14.cfs=1, _1g.cfs=1, _1h.cfs=1, _1f.cfs=1, _1n.cfs=1,
_1i.cfs=1, _1j.cfs=1, _1l.cfs=1}

Stacktrace

java.lang.RuntimeException: MockRAMDirectory: cannot close: there are
still open files: {_1m.cfs=1, _1k.cfs=1, _14.cfs=1, _1g.cfs=1,
_1h.cfs=1, _1f.cfs=1, _1n.cfs=1, _1i.cfs=1, _1j.cfs=1, _1l.cfs=1}
       at
org.apache.lucene.store.MockRAMDirectory.close(MockRAMDirectory.java:282)
       at
org.apache.lucene.index.TestIndexWriter.testCommitThreadSafety(TestIndexWriter.java:4616)
       at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:328)

Standard Output

NOTE: random codec of testcase 'testCommitThreadSafety' was: Sep

Standard Error

The following exceptions were thrown by threads:
*** Thread: Thread-1784 ***
java.lang.RuntimeException: junit.framework.AssertionFailedError: null
       at org.apache.lucene.index.TestIndexWriter$9.run(TestIndexWriter.java:4606)
Caused by: junit.framework.AssertionFailedError: null
       at junit.framework.Assert.fail(Assert.java:47)
       at junit.framework.Assert.assertTrue(Assert.java:20)
       at junit.framework.Assert.assertTrue(Assert.java:27)
       at org.apache.lucene.index.TestIndexWriter$9.run(TestIndexWriter.java:4597)
{noformat}"
1,"Unmatched right parentheses truncates queryThe query processor truncates a query when right parentheses are unmatched.
E.g.:

 secret AND illegal) AND access:confidential

will not result in a ParseException instead will run as:

 secret AND illegal"
1,"new QueryParser over-increment position for MultiPhraseQueryIf the new QP is parsing a phrase, and when the analyzer runs on the text within the phrase it produces some tokens with posIncr=0, a MultiPhraseQuery is produced.  But, the positions of the added terms are over-incremented, and don't match what the current QueryParser does."
1,"CheckIndex should allow term position = -1
Spinoff from this discussion:

    http://mail-archives.apache.org/mod_mbox/lucene-java-user/200803.mbox/%3CPine.LNX.4.62.0803292323350.16762@radix.cryptio.net%3E

Right now CheckIndex claims the index is corrupt if you index a Token with -1 position, which happens if your first token has positionIncrementGap set to 0.

But, as far as I can tell, Lucene doesn't ""mind"" when this happens.

So I plan to fix CheckIndex to allow this case.  I'll backport to 2.3.2 as well.

LUCENE-1253 is one example where Lucene's core analyzers could do this."
1,queries with zero boosts don't workQueries consisting of only zero boosts result in incorrect results.
1,"JCR Server has concurrency issues on JcrWebdavServer.SessionCache internal HashMap cachesAfter doing the davex remoting performance work outlined in JCR-3026, the increased concurrency on my jcr server exposed a lot of errors related to getting and putting from the JcrWebdavServer.SessionCache's internal HashMap's.  This problem with HashMap's is a well known concurrency error and was easily fixed by upgrading these maps to ConcurrentHashMaps.  Performance seems dramatically better.  

The fix includes exposure of a tuning parameter that allows the user to set the expected concurrency level.  This is the number of concurrent requests you expect the server to be handling.  In the typical davex remoting scenario, this means you should tune this server side value to match the total max connections of all clients pointed at the server.  See JCR-3026. 

USAGE:  Set the 'concurrency-level' init param for the JcrRemotingServlet, via the web.xml of the jackabbit-webapp component.  Default value is 50.  Or you can intervene in a lower level api if appropriate."
1," inconsistent session and persistent state after ReferentialIntegrityExceptionWhen a ReferentialIntegrityException occurs in a session it seems that subsequent actions on that session may result in a inconsistent session state AND even inconsistent persistent state. The latter will even make jackrabbit fail to bootstrap an index from that persistent state.

Typical rootcause:

Caused by: org.apache.jackrabbit.core.state.NoSuchItemStateException: ddb9d3ea-59c1-4eb4-a83e-332f646d4f40
        at org.apache.jackrabbit.core.state.SharedItemStateManager.getItemState(SharedItemStateManager.java:270)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.createIndex(MultiIndex.java:1082)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.createIndex(MultiIndex.java:1088)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.createInitialIndex(MultiIndex.java:395)

Bootstrap failure:

java.io.IOException: Error indexing workspace
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.createInitialIndex(MultiIndex.java:402)
        at org.apache.jackrabbit.core.query.lucene.SearchIndex.doInit(SearchIndex.java:465)
        at org.apache.jackrabbit.core.query.AbstractQueryHandler.init(AbstractQueryHandler.java:59)
        at org.apache.jackrabbit.core.SearchManager.initializeQueryHandler(SearchManager.java:553)

"
1,"Highligter fails to include non-token at end of string to be highlightedThe following code extract show the problem


		TermQuery query= new TermQuery( new Term( ""data"", ""help"" )); 
		Highlighter hg = new Highlighter(new SimpleHTMLFormatter(), new QueryScorer( query ));
		hg.setTextFragmenter( new NullFragmenter() );
		
		String match = null;
		try {
			match = hg.getBestFragment( new StandardAnalyzer(), ""data"", ""help me [54-65]"" );
		} catch (IOException e) {
			e.printStackTrace();
		}
		System.out.println( match );


The sytsem outputs 

<B>help</B> me [54-65


would expect 

<B>help</B> me [54-65]



"
1,"System search manager uses a SessionItemStateManagerAs noted in JCR-2000, the system search manager (responsible for indexing the /jcr:system subtree) uses the SessionItemStateManager instance of the system session instead of the SharedItemStateManager of the underlying default workspace.

This can cause a deadlock (see the thread dumps in JCR-2000) when one thread is accessing the LockManager (that also uses the system session) while another thread is persisting versioning changes.

See the search-on-sism.patch attachment in JCR-2000 for a fix to this issue."
1,"FilterIndexReader doesn't work correctly with post-flex SegmentMergerIndexWriter.addIndexes(IndexReader...) internally uses SegmentMerger to add data from input index readers. However, SegmentMerger uses the new post-flex API to do this, which bypasses the pre-flex TermEnum/TermPositions API that FilterIndexReader implements. As a result, filtering is not applied."
1,"QNodeTypeDefinitionImpl.getSerializablePropertyDefs() might return non serializable property definitions QNodeTypeDefinitionImpl.getSerializablePropertyDefs() returns a set-version of the passed in parameter, irrespective of whether the property defs are serializable or not.

See http://markmail.org/thread/65ngqvyxnu4nn3su"
1,"QueryUtils should check that equals properly handles nullIts part of the equals contract, but many classes currently violate"
1,"jcr2spi: versionmanager#checkout(NodeState) should not forward to checkout(NodeState, NodeId)VersionManager#checkout(NodeState nodeState) is called if activity is not supported and thus should call the
corresponding SPI method instead of checkout(NodeState, NodeId activityId)"
1,"Writers blocked forever when waiting on update operations  Thread 1 calls Session.save() and has a write lock.

Thread 2 is in XA prepare() and is waiting on thread 1 in FineGrainedISMLocking.acquireWriteLock().

Thread 1's save calls SharedItemStateManager.Update#end() and performs a write-lock downgrade to a read-lock, then (at the end of Update#end()) it calls readLock.release(). FineGrainedISMLocking.ReadLockImpl#release thinks activeWriterId is of the current transation and does not notify any writers (activeWriterId is not being reset on downgrade in what seems to be a related to JCR-2753).
Thread 1 waits forever."
1,"ItemStates in the ChangeLog can not be retrieved in the sequence they were created/modified/deletedThe itemstates are ordered by the hash code.
It's an issue with PersistenceManagers that check referencial integrity (e.g. rdbms)."
1,"ConnectionRecoveryManager is created twice in DBDataStore init methodIt seems that after introducing pool, old initizialization of ConnectionRecoveryManager has not been removed.

Index: DbDataStore.java
===================================================================
--- DbDataStore.java	(revision 605626)
+++ DbDataStore.java	(working copy)
@@ -479,8 +479,6 @@
             initDatabaseType();
             connectionPool = new Pool(this, maxConnections);
             ConnectionRecoveryManager conn = getConnection();
-            conn = new ConnectionRecoveryManager(false, driver, url, user, password);
-            conn.setAutoReconnect(true);
             DatabaseMetaData meta = conn.getConnection().getMetaData();
             log.info(""Using JDBC driver "" + meta.getDriverName() + "" "" + meta.getDriverVersion());
             meta.getDriverVersion();

Duplicated initialization should be removed , but i've never run this code yet."
1,"WebDav MKCOL on a directory that already exists generates a IllegalStateExceptionwhen performing a MKCOL on a resource that already exists, following is thrown.

31.03.2010 16:14:10.760 *ERROR* [127.0.0.1 [1270012450463] MKCOL /org.apache.sling.launchpad.testing-6-SNAPSHOT/apps HTTP/1.1] org.apache.sling.engine.impl.SlingMainServlet service: Uncaught Throwable java.lang.IllegalStateException: Response has already been committed
       at org.apache.sling.engine.impl.SlingHttpServletResponseImpl.checkCommitted(SlingHttpServletResponseImpl.java:398)
       at org.apache.sling.engine.impl.SlingHttpServletResponseImpl.setStatus(SlingHttpServletResponseImpl.java:265)
       at org.apache.jackrabbit.webdav.WebdavResponseImpl.setStatus(WebdavResponseImpl.java:276)
       at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.doMkCol(AbstractWebdavServlet.java:548)
       at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.execute(AbstractWebdavServlet.java:256)
       at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.service(AbstractWebdavServlet.java:196)
       at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
       at org.apache.sling.engine.impl.request.RequestData.service(RequestData.java:523)
....


I think a return after the sendError is required ?
in AbstractWebdavServlet.doMkCol(...) 


   protected void doMkCol(WebdavRequest request, WebdavResponse response,
                          DavResource resource) throws IOException, DavException {

       DavResource parentResource = resource.getCollection();
       if (parentResource == null || !parentResource.exists() || !parentResource.isCollection()) {
           // parent does not exist or is not a collection
           response.sendError(DavServletResponse.SC_CONFLICT);
           return;
       }
       // shortcut: mkcol is only allowed on deleted/non-existing resources
       if (resource.exists()) {
           response.sendError(DavServletResponse.SC_METHOD_NOT_ALLOWED);
+          return;
       }

       if (request.getContentLength() > 0 || request.getHeader(""Transfer-Encoding"") != null) {
           parentResource.addMember(resource, getInputContext(request, request.getInputStream()));
       } else {
           parentResource.addMember(resource, getInputContext(request, null));
       }
       response.setStatus(DavServletResponse.SC_CREATED);
   }



"
1,"nt:versionedChild problemProblem occurs when both parent and child beans are versionable.  Jackrabbit creates an nt:versionedChild node that is referenced by the parent node, referencing the childs versionedHistory node of the child.  The current OCM code does not handle this correctly and produces an error:  ""Node type  'nt:versionedChild' does not match descriptor node type 'nt:unstructured'""

Below is a example code of the problem and a patch that appears to correctly resolve the problem.


 Within ObjectConverterImpl created the below method.

        public Node getActualNode(Session session,Node node) throws
 RepositoryException
        {
                NodeType type = node.getPrimaryNodeType();
                if (type.getName().equals(""nt:versionedChild""))
                {

                        String uuid =
 node.getProperty(""jcr:childVersionHistory"").getValue().getString();
                        Node actualNode = session.getNodeByUUID(uuid);
                        String name = actualNode.getName();
                        actualNode = session.getNodeByUUID(name);

                        return actualNode;
                }

                return node;
        }

 AND modified the following to call the above method


        public Object getObject(Session session, Class clazz, String path)
        {
                try {
                        if (!session.itemExists(path)) {
                                return null;
                        }

                        if (requestObjectCache.isCached(path))
                    {
                        return requestObjectCache.getObject(path);
                    }

                        ClassDescriptor classDescriptor =
 getClassDescriptor(clazz);

                        checkNodeType(session, classDescriptor);

                        Node node = (Node) session.getItem(path);
                        if (!classDescriptor.isInterface()) {
                                {
                                node = getActualNode(session,node);
                                checkCompatiblePrimaryNodeTypes(session,
 node, classDescriptor, true);
                                }
                        }

                        ClassDescriptor alternativeDescriptor = null;
                        if
 (classDescriptor.usesNodeTypePerHierarchyStrategy())
 {
                                if
 (node.hasProperty(ManagerConstant.DISCRIMINATOR_PROPERTY_NAME))
 {
                        String className =
 node.getProperty(ManagerConstant.DISCRIMINATOR_PROPERTY_NAME
 ).getValue().getString();
                        alternativeDescriptor =
 getClassDescriptor(ReflectionUtils.forName(className));
                                }
                        } else {
                                if
 (classDescriptor.usesNodeTypePerConcreteClassStrategy())
 {
                                        String nodeType =
 node.getPrimaryNodeType().getName();
                                        if
 (!nodeType.equals(classDescriptor.getJcrType()))
 {
                                            alternativeDescriptor =
 classDescriptor.getDescendantClassDescriptor(nodeType);

                                            // in case we an alternative
 could not be found by walking
                                            // the class descriptor
 hierarchy, check whether we
 would
                                            // have a descriptor for the
 node type directly (which
                                            // may the case if the class
 descriptor hierarchy is
                                            // incomplete due to missing
 configuration. See JCR-1145
                                            // for details.
                                            if (alternativeDescriptor ==
 null) {
                                                alternativeDescriptor =
 mapper.getClassDescriptorByNodeType(nodeType);
                                            }
                                        }
                                }
                        }

                        // if we have an alternative class descriptor,
 check whether its
                        // extends (or is the same) as the requested class.
                        if (alternativeDescriptor != null) {
                            Class alternativeClazz =
 ReflectionUtils.forName(alternativeDescriptor.getClassName());
                            if (clazz.isAssignableFrom(alternativeClazz)) {
                                clazz = alternativeClazz;
                                classDescriptor = alternativeDescriptor;
                            }
                        }

                        // ensure class is concrete (neither interface nor
 abstract)
                        if (clazz.isInterface() ||
 Modifier.isAbstract(clazz.getModifiers())) {
                            throw new JcrMappingException( ""Cannot
 instantiate non-concrete
 class "" + clazz.getName()
                        + "" for node "" + path + "" of type "" +
 node.getPrimaryNodeType().getName());
                        }

            Object object =
 ReflectionUtils.newInstance(classDescriptor.getClassName());

            if (! requestObjectCache.isCached(path))
            {
                          requestObjectCache.cache(path, object);
            }

            simpleFieldsHelp.retrieveSimpleFields(session,
 classDescriptor, node, object);
                        retrieveBeanFields(session, classDescriptor, node,
 path, object, false);
                        retrieveCollectionFields(session, classDescriptor,
 node, object, false);

                        return object;
                } catch (PathNotFoundException pnfe) {
                        // HINT should never get here
                        throw new
 ObjectContentManagerException(""Impossible to get
 the object
 at "" + path, pnfe);
                } catch (RepositoryException re) {
                        throw new
 org.apache.jackrabbit.ocm.exception.RepositoryException(""Impossible to
 get the object at "" + path, re);
                }
        }




>
>
>
> > I am building a test application against OCM.  I have the following
> > classes that are annotated for OCM.  The problem is that when I update
> and
> > version the root object PressRelease the Bean Author is versioned to
> > nt:versionedChild.  While the OCM is checking for node type
> compatibility
> > it is throwing the following exception.  It looks like the
> versionedChild
> > is not handled correctly.  Any suggestions?
> >
> > I also attempted to retrieve the version based on the version name for
> the
> > rootVersion but also trapped. From a Version object how should I access
> > each of the versioned entries?
> >
> > Thanks
> > Wes
> >
> > @Node (jcrMixinTypes=""mix:versionable"")
> > public class PressRelease
> > {
> >       @Field(path=true) String path;
> >       @Field String title;
> >       @Field Date pubDate;
> >       @Field String content;
> >       @Bean Author author;
> >       @Collection (elementClassName=Comment.class) List<Comment>
> comments = new
> > ArrayList<Comment>();
> >
> >       public String getPath() {
> >               return path;
> >       }
> >       public void setPath(String path) {
> >               this.path = path;
> >       }
> >       public String getContent() {
> >               return content;
> >       }
> >       public void setContent(String content) {
> >               this.content = content;
> >       }
> >       public Date getPubDate() {
> >               return pubDate;
> >       }
> >       public void setPubDate(Date pubDate) {
> >               this.pubDate = pubDate;
> >       }
> >       public String getTitle() {
> >               return title;
> >       }
> >       public void setTitle(String title) {
> >               this.title = title;
> >       }
> >       public Author getAuthor() {
> >               return author;
> >       }
> >       public void setAuthor(Author author) {
> >               this.author = author;
> >       }
> >       public List<Comment> getComments() {
> >               return comments;
> >       }
> >       public void setComments(List<Comment> comments) {
> >               this.comments = comments;
> >       }
> >
> >
> > }
> >
> > @Node (jcrMixinTypes=""mix:versionable"")
> > public class Author {
> >
> >       @Field(path=true) String path;
> >       @Field String name;
> >
> >
> >       public String getName() {
> >               return name;
> >       }
> >       public void setName(String name) {
> >               this.name = name;
> >       }
> >       public String getPath() {
> >               return path;
> >       }
> >       public void setPath(String path) {
> >               this.path = path;
> >       }
> >
> > }
> >
> > MAIN
> >
> >       while (versionIterator.hasNext())
> >       {
> >           Version version = (Version) versionIterator.next();
> >           System.out.println(""version found : ""+ version.getName() + "" -
> "" +
> >                                 version.getPath() + "" - "" +
> > version.getCreated().getTime());
> >
> >
> >           if (!version.getName().equals(""jcr:rootVersion""))
> >           {
> >
> > //      Get the object matching to the first version
> >           pressRelease = (PressRelease)
> > ocm.getObject(""/newtutorial"",version.getName());
> >
> >
> >               System.out.println(""PressRelease title : "" +
> pressRelease.getTitle());
> >               System.out.println(""             author: "" +
> > pressRelease.getAuthor().getName());
> >               System.out.println(""            content: "" +
> pressRelease.getContent());
> >               List comments = pressRelease.getComments();
> >               Iterator iterator = comments.iterator();
> >               while (iterator.hasNext())
> >               {
> >                       comment = (Comment) iterator.next();
> >                       System.out.println(""Comment : <"" + comment.getData()
> + "">"" +
> > comment.getText());
> >               }
> >           }
> >       }
> >
> >
> > CONSOLE
> > version found : jcr:rootVersion -
> >
> /jcr:system/jcr:versionStorage/fc/0b/fd/fc0bfd89-c487-4fbe-930f-d837e5dfed79/jcr:rootVersion
> > - Thu Feb 28 15:54:42 EST 2008
> > version found : 1.0 -
> >
> /jcr:system/jcr:versionStorage/fc/0b/fd/fc0bfd89-c487-4fbe-930f-d837e5dfed79/1.0
> > - Thu Feb 28 15:54:59 EST 2008
> > Exception in thread ""main""
> > org.apache.jackrabbit.ocm.exception.ObjectContentManagerException:
> Cannot
> > map object of type 'com..pc.repository.Author'. Node type
> > 'nt:versionedChild' does not match descriptor node type
> 'nt:unstructured'
> >       at
> >
> org.apache.jackrabbit.ocm.manager.objectconverter.impl.ObjectConverterImpl.checkCompatiblePrimaryNodeTypes
> (ObjectConverterImpl.java:552)
> >       at
> >
> org.apache.jackrabbit.ocm.manager.objectconverter.impl.ObjectConverterImpl.getObject
> (ObjectConverterImpl.java:361)
> >       at
> >
> org.apache.jackrabbit.ocm.manager.beanconverter.impl.DefaultBeanConverterImpl.getObject
> (DefaultBeanConverterImpl.java:80)
> >       at
> >
> org.apache.jackrabbit.ocm.manager.objectconverter.impl.ObjectConverterImpl.retrieveBeanField
> (ObjectConverterImpl.java:666)
> >       at
> >
> org.apache.jackrabbit.ocm.manager.objectconverter.impl.ObjectConverterImpl.retrieveBeanFields
> (ObjectConverterImpl.java:621)
> >       at
> >
> org.apache.jackrabbit.ocm.manager.objectconverter.impl.ObjectConverterImpl.getObject
> (ObjectConverterImpl.java:309)
> >       at
> >
> org.apache.jackrabbit.ocm.manager.impl.ObjectContentManagerImpl.getObject(
> ObjectContentManagerImpl.java:313)
> >       at com.pc.repository.Main.main(Main.java:345)






"
1,"ChangeLog serialization causes cache inconsistenciesThe ordering of actions is taken into account when a ChangeLog is built through session manipulations (see, for instance,  ChangeLog.deleted(ItemState state)). When it is serialized in ClusterNode.write(Record record, ChangeLog changeLog, EventStateCollection esc), however, this implicit ordering might be changed. As a consequence,  the deserialization in ClusterNode.consume(Record record) might produce a different ChangeLog with the effect that the local caches get out-of-sync with the persistent state of the repository.

The issue should be reproducable as follows:
- Setup a clustered environment with two Jackrabbit instances, say A and B.
- On instance A add a property ""P"" with value ""x"" to some node and save the session.
- On instance B read property ""P"" -> it will have value ""x"".
- On instance A delete property P and then add it again with value ""y"" and save the session.
- On instance B read property ""P"" -> it will still have value ""x"" after the cluster sync..."
1,"incorrect definition of built-in node type nt:hierarchyNodethe property jcr:created of nt:hierarchyNode should be non-mandatory according to the specification (jcr 1.0 and jcr 1.0.1).
both the definition of the built-in node type and the related test case should be fixed accordingly."
1,"JCA Concurrent Modification Exception when JCAManagedConnection.cleanup() calledThe JCAManagedConnection.closeHandles() method causes a ConcurrentModificationException if the handles list is not empty.
This is caused by modification of the handles list by removeHandle(), while closeHandles() is iterating over the list.

Under SunOne AppServer 7 this can be caused simply by not closing the Session handle before the transaction commits.

It is probably not even necessary to send connectionClosed events during cleanup().  According to the API for connectionClosed, the event indicates that an application component has closed  the connection handle.  cleanup() is a container initiated action, and so the connectionClosed event is not applicable.


java.util.ConcurrentModificationException
    at java.util.LinkedList$ListItr.checkForComodification(LinkedList.java:552)
    at java.util.LinkedList$ListItr.next(LinkedList.java:488)
    at org.apache.jackrabbit.jca.JCAManagedConnection.closeHandles(JCAManagedConnection.java:382)
    at org.apache.jackrabbit.jca.JCAManagedConnection.cleanup(JCAManagedConnection.java:145)
    at com.sun.enterprise.resource.IASPoolObjectImp.cleanup(IASPoolObjectImp.java:243)
    at com.sun.enterprise.resource.IASGenericPoolObjects.transactionCompleted(IASGenericPoolObjects.java:794)
    at com.sun.enterprise.resource.ResourcePoolManagerImpl.transactionCompleted(ResourcePoolManagerImpl.java:347)
    at com.sun.enterprise.resource.ResourcePoolManagerImpl$SynchronizationListener.afterCompletion(ResourcePoolManagerImpl.java:644)
    at com.sun.jts.jta.SynchronizationImpl.after_completion(SynchronizationImpl.java:70)

"
1,"If there is more than 15 seconds between HttpClient.execute() calls using a MultipartEntity, a ProtocolException is thrown complaining about the Content-Length header already being present.I am not sure if this time-related behaviour is intentional or not (I have only been using this library for a few weeks) , but even if a timeout is to be expected, the exception thrown ought to indicate that there is a time component involved. ""org.apache.http.ProtocolException: Content-Length header already present"" is incredibly misleading. 

A simple-ish compileable program to reproduce the bug is as follows:

import java.nio.charset.Charset;
import org.apache.http.HttpResponse;
import org.apache.http.client.methods.HttpPost;
import org.apache.http.client.params.ClientPNames;
import org.apache.http.client.params.CookiePolicy;
import org.apache.http.entity.mime.MultipartEntity;
import org.apache.http.entity.mime.content.StringBody;
import org.apache.http.impl.client.DefaultHttpClient;
public class Simple {
    static public void main(String [] args)
    {
        try
        {
            DefaultHttpClient client = new DefaultHttpClient();
            client.getParams().setParameter(
                        ClientPNames.COOKIE_POLICY, CookiePolicy.BROWSER_COMPATIBILITY);
            MultipartEntity entity;
            StringBody stringBody;
            HttpPost post;
            HttpResponse response;
            entity = new MultipartEntity();
            stringBody = new StringBody(""field contents"",Charset.forName(""ISO-8859-1""));
            entity.addPart(""field"", stringBody);  
            post = new HttpPost(""http://localhost/simple.php"");
            post.setEntity(entity); 
            response = client.execute(post);
            
            //The exception does not occur if the content is not consumed
            response.getEntity().consumeContent();
            System.out.println(""First post done"");
            
            //The exception does not occur if the time interval between the requests is too short
            Thread.sleep(15000);
            
            //The exception naturally doesn't occur if a new HttpClient is created
            //client = new DefaultHttpClient();

            entity = new MultipartEntity();
            stringBody = new StringBody(""field contents"",Charset.forName(""ISO-8859-1""));
            entity.addPart(""field"", stringBody);  

            post = new HttpPost(""http://localhost/simple.php"");
            post.setEntity(entity); 
            response = client.execute(post); //Will throw the following:
            /*
                org.apache.http.ProtocolException: Content-Length header already present
                at org.apache.http.protocol.RequestContent.process(RequestContent.java:70)
                at org.apache.http.protocol.BasicHttpProcessor.process(BasicHttpProcessor.java:290)
                at org.apache.http.protocol.HttpRequestExecutor.preProcess(HttpRequestExecutor.java:160)
                at org.apache.http.impl.client.DefaultClientRequestDirector.execute(DefaultClientRequestDirector.java:356)
                at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:501)
                at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:456)
                at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:422)
                at test.Simple.main(Simple.java:57)
             */ 
            System.out.println(""Second post done"");
        }
        catch(Exception e)
        {
            System.out.println(e);
            e.printStackTrace();
        }
    }    
}"
1,"Inconsistent scoring with SpanTermQuery in BooleanQueryWhen a SpanTermQuery is added to a BooleanQuery, incorrect results are 
returned.

I am running Lucene 1.9 RC1 on Windows XP.  I have a test case which has 
several tests.  It has an index with 4 identical documents in it.

When two TermQuerys are used in a BooleanQuery, the score looks like this:
  4 hits for search: two term queries
    ID:1 (score:0.54932046)
    ID:2 (score:0.54932046)
    ID:3 (score:0.54932046)
    ID:4 (score:0.54932046)

Notice how it is correctly setting the score to be the same for each document.

When two SpanQuerys are used in a BooleanQuery, the score looks like this:
  2 hits for search: two span queries
    ID:1 (score:0.3884282)
    ID:4 (score:0.1942141)

Notice how it only returned two documents instead of four.  And the two it did 
return have differing scores.

I believe that there is an error in the scoring algorithm that is making the 
other two documents not show up."
1,"Several Codecs use the same files - PerFieldCodecWrapper can not hold two codec using the same filesCurrently we have a rather simple file naming scheme which prevents us from using more than one codec in a segment that relies on the same file.  For instance pulsing and standard codec can not be used together since they both need the .frq .tii .tis etc. To make this work we either need to write distinct per codec files or set a per field / codec file ID. While the first solution seems to be quiet verbose the second one seems to be more flexible too.

One possibility to do that would be to assign a unique id to each SegmentsWriteState when opening the FieldsConsumer and write the IDs into the segments file to eventually load it once the segment is opened. Otherwise our PerFieldCodec feature will not be really flexible nor useful though.  "
1,"Constants.LUCENE_MAIN_VERSION is inlined in code compiled against Lucene JAR, so version detection is incorrectWhen you compile your own code against the Lucene 2.9 version of the JARs and use the LUCENE_MAIN_VERSION constant and then run the code against the 3.0 JAR, the constant still contains 2.9, because javac inlines primitives and Strings into the class files if they are public static final and are generated by a constant (not method).

The attached fix will fix this by using a ident(String) functions that return the String itsself to prevent this inlining.

Will apply to 2.9, trunk and 2.9 BW branch. No I can also reenable one test I removed because of this."
1,"Deadlock for some Query objects in the equals method (f.ex. PhraseQuery) in a concurrent environmentSome Query objects in lucene 2.3.2 (and previous versions) have internal variables using Vector.   These variables are used during the call to the equals method.   In a concurrent environment a deadlock might occur.    The attached code example shows this happening in lucene 2.3.2, but the patch in LUCENE-1346 fixes this issue (though that doesn't seem to be the intention of that patch according to the description :-)"
1,"User-Agent string violates RFCOur User-Agent says ""Jakarta Commons-HttpClient/3.1-rc1"". But space is a reserved character to separate individual *products* and comments according to RFC 2616, section 14.43. Jakarta is not a product. At the same time we may want to drop the Jakarta name altogether.

We should change this to something more standard like: 

""Apache-HttpClient/3.1-rc1 (""+ System.getProperty(""os.name"") +"";""+ System.getProperty(""os.arch"") +"") ""+
""Java/""+ System.getProperty(""java.vm.version"") +"" (""+ System.getProperty(""java.vm.vendor"") +"")""

which renders:

""Apache-HttpClient/3.1-rc1 (Windows XP 5.1;x86) Java/1.5.0_08 (Sun Microsystems Inc.)""

Sun's internal Http client uses something like ""Java/1.5.0_08"".

I am completely ignoring the fact that real-world user agents use almost arbitrary strings.
Some fine examples of misbehaviour from my private logs:

""Jakmpqes dihurxf wfyiupsc"" -- apparently somebody has to hide something...
""Missigua Locator 1.9""
""Poodle predictor 1.0""
""shelob v1.0""
""ISC Systems iRc Search 2.1""
""ping.blogug.ch aggregator 1.0""
""http://www.uni-koblenz.de/~flocke/robot-info.txt""  -- ...sigh

I am very tempted to write a User-Agent string validator that prevents misuse of this field in HttpClient."
1,"initVersions crashes with NPEAfter delete some old versions. I get serious problems accessing the version history.
This is the stacktrace:
java.lang.NullPointerException
	at org.apache.jackrabbit.core.version.VersionIteratorImpl.initVersions(VersionIteratorImpl.java:169)
	at org.apache.jackrabbit.core.version.VersionIteratorImpl.<init>(VersionIteratorImpl.java:87)
	at org.apache.jackrabbit.core.version.VersionIteratorImpl.<init>(VersionIteratorImpl.java:72)
	at org.apache.jackrabbit.core.version.VersionHistoryImpl.getAllVersions(VersionHistoryImpl.java:92)

I stepped threw the code and see that the Method 
    currentVersion.getSuccessors() 
returns an empty Array.

After all the VersionHistory seems to be corrupt!!"
1,"Highlight fragment does not extend to maxDocCharsToAnalyzeThe current highlighter code checks whether the total length of the text to highlight is strictly smaller than maxDocCharsToAnalyze before adding any text remaining after the last token to the fragment. This means that if maxDocCharsToAnalyse is set to exactly the length of the text and the last token of the text is the term to highlight and is followed by non-token text, this non-token text will not be highlighted.

For example, consider the phrase ""this is a text with searchterm in it"". ""In"" and ""it"" are not tokenized because they're stopwords. Setting maxDocCharsToAnalyze to 36 (the length of the sentence) and searching for ""searchterm"" gives a fragment ending in ""searchterm"". The expected behaviour is to have ""in it"" at the end of the fragment, since maxDocCharsToAnalyse explicitely states that the whole phrase should be considered."
1,"DisjunctionMaxScorer.skipTo has bug that keeps it from skippingas reported on the mailing list, DisjunctionMaxScorer.skipTo is broken if called before next in some situations...

http://www.nabble.com/Potential-issue-with-DisjunctionMaxScorer-tf3846366.html#a10894987"
1,"MsExcelTextFilter throws Exception. Repository is not startableIf i try to add a Excel File (see attachment) i get this Exception

Caused by: java.lang.NumberFormatException: You cannot get a string value from a numeric cell
	at org.apache.poi.hssf.usermodel.HSSFCell.getStringCellValue(HSSFCell.java:800)
	at org.apache.jackrabbit.core.query.MsExcelTextFilter$1.initializeReader(MsExcelTextFilter.java:97)
	at org.apache.jackrabbit.core.query.LazyReader.read(LazyReader.java:79)

The bad news is that if you add this file the repository is not startabel anymore because the file is in the redo.log and you
get a blocker !

The stack from the restart after NumberFormatException

19.09.2006 08:47:23 *ERROR* RepositoryImpl: Unable to start repository, forcing shutdown... 
19.09.2006 08:47:23 *INFO * RepositoryImpl: Shutting down repository... 
19.09.2006 08:47:23 *INFO * RepositoryImpl: shutting down workspace 'default'... 
19.09.2006 08:47:23 *INFO * ObservationManagerFactory: Notification of EventListeners stopped. 
19.09.2006 08:47:23 *INFO * RepositoryImpl: workspace 'default' has been shutdown 

I think its very important that you can not block a whole repository if the indexer throws a exception.
thanks
claus"
1,"HostConfiguration socketFactory is ignoredHostConfiguration doesn't use its host.protocol to execute an HttpMethod with an absolute URL.  It should, if the Protocol's scheme is the same as the method's URL scheme.

This bug makes it difficult to integrate a specialized SSL connection algorithm (in a SecureProtocolSocketFactory) with a module implemented on top of HttpClient.  The latter module must not execute methods with absolute URLs.  Of course, this is difficult when one doesn't control that module.  For example, I recently tried to integrate SSL certificate-based client authentication with XFire.  XFire provides a reasonable API for replacing its HttpClient, but one must hack its source code to prevent it from executing methods with absolute URLs.

Protocol.registerProtocol is a possible answer, but it can't support two or more SSL connection algorithms for one HTTPS host and port."
1,NodeTypeDefinitionFactory does not set PropertyDefinition#isQueryOrderable
1,"Incorrect check for replace when importing item with colliding idWhen fixing JCR-1128 bug was introduced due to incorrect check for UUID behavior. Current code is:
201 : 	 if (!(existing.getId().equals(id)
202 : 	&& (uuidBehavior == ImportUUIDBehavior.IMPORT_UUID_COLLISION_REMOVE_EXISTING
203 :	|| uuidBehavior == ImportUUIDBehavior.IMPORT_UUID_COLLISION_REMOVE_EXISTING))) {
204 :	throw new ItemExistsException(existing.safeGetJCRPath());
205 :	}

While it should check for ImportUUIDBehavior.IMPORT_UUID_COLLISION_REPLACE_EXISTING in one of the cases (line 202 or 203).
Also it is possible that id of imported item is not known and therefore value of ""id"" variable is null and check will always fail. Would be nice if this case can be handled as well.
"
1,"ERROR 40XD0: Container has been closed exception with Derby DBThis seems very similar to JCR-1039, only I am getting it on 1.4 using the regular DatabasePersistenceManager.
Was the fix for JCR-1039 in 1.3.3 merged to 1.4.x?

Here is the relevant part of the exception:

INFO | jvm 1 | 2008/04/10 14:00:37 | Caused by: javax.jcr.RepositoryException: failed to retrieve item state of item fb648866-a236-42aa-8039-df68f26dd2ad/{http://www.jcp.org/jcr/1.0}data: failed to read property state: fb648866-a236-42aa-8039-df68f26dd2ad/{http://www.jcp.org/jcr/1.0}data: failed to read property state: fb648866-a236-42aa-8039-df68f26dd2ad/{http://www.jcp.org/jcr/1.0}data
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.ItemManager.createItemInstance(ItemManager.java:570)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:395)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.NodeImpl.getProperty(NodeImpl.java:2553)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.artifactory.jcr.JcrFile.getStream(JcrFile.java:133)
INFO | jvm 1 | 2008/04/10 14:00:37 | ... 55 more
INFO | jvm 1 | 2008/04/10 14:00:37 | Caused by: org.apache.jackrabbit.core.state.ItemStateException: failed to read property state: fb648866-a236-42aa-8039-df68f26dd2ad/{http://www.jcp.org/jcr/1.0}data
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.load(DatabasePersistenceManager.java:406)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.state.SharedItemStateManager.loadItemState(SharedItemStateManager.java:1161)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.state.SharedItemStateManager.getNonVirtualItemState(SharedItemStateManager.java:1086)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.state.SharedItemStateManager.getItemState(SharedItemStateManager.java:248)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.state.LocalItemStateManager.getPropertyState(LocalItemStateManager.java:118)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.state.LocalItemStateManager.getItemState(LocalItemStateManager.java:150)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.state.XAItemStateManager.getItemState(XAItemStateManager.java:226)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:175)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.ItemManager.createItemInstance(ItemManager.java:564)
INFO | jvm 1 | 2008/04/10 14:00:37 | ... 58 more
INFO | jvm 1 | 2008/04/10 14:00:37 | Caused by: javax.jcr.RepositoryException: Error creating temporary file: ERROR 40XD0: Container has been closed.: ERROR 40XD0: Container has been closed.
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.value.BLOBInTempFile.<init>(BLOBInTempFile.java:69)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.value.BLOBInTempFile.getInstance(BLOBInTempFile.java:103)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.value.InternalValue.getBLOBFileValue(InternalValue.java:630)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.value.InternalValue.create(InternalValue.java:265)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.persistence.util.Serializer.deserialize(Serializer.java:296)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.load(DatabasePersistenceManager.java:397)
INFO | jvm 1 | 2008/04/10 14:00:37 | ... 66 more
INFO | jvm 1 | 2008/04/10 14:00:37 | Caused by: java.io.IOException: ERROR 40XD0: Container has been closed.
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.derby.impl.store.raw.data.OverflowInputStream.fillByteHolder(Unknown Source)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.derby.impl.store.raw.data.BufferedByteHolderInputStream.read(Unknown Source)
INFO | jvm 1 | 2008/04/10 14:00:37 | at java.io.DataInputStream.read(DataInputStream.java:132)
INFO | jvm 1 | 2008/04/10 14:00:37 | at java.io.FilterInputStream.read(FilterInputStream.java:116)
INFO | jvm 1 | 2008/04/10 14:00:37 | at java.io.FilterInputStream.read(FilterInputStream.java:116)
INFO | jvm 1 | 2008/04/10 14:00:37 | at java.io.SequenceInputStream.read(SequenceInputStream.java:191)
INFO | jvm 1 | 2008/04/10 14:00:37 | at java.io.InputStream.read(InputStream.java:85)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.value.BLOBInTempFile.<init>(BLOBInTempFile.java:61)
INFO | jvm 1 | 2008/04/10 14:00:37 | ... 71 more"
1,"RepositoryException when using BindVariables in JCR-SQL2 CONTAINSWhen using a BindVariable in a JCR-SQL2 CONTAINS constraint, the query fails with a RepositoryException.

For example:

String sql = ""SELECT * FROM [nt:unstructured] WHERE ISCHILDNODE([/testroot]) AND CONTAINS(mytext, $searchExpression)"";
Query q = superuser.getWorkspace().getQueryManager().createQuery(sql, Query.JCR_SQL2);
q.bindValue(""searchExpression"", superuser.getValueFactory().createValue(""fox""));
q.execute();

Results in:

javax.jcr.RepositoryException: Unknown static operand type: org.apache.jackrabbit.spi.commons.query.qom.BindVariableValueImpl@591a4d
        at org.apache.jackrabbit.core.query.lucene.LuceneQueryFactoryImpl.create(LuceneQueryFactoryImpl.java:215)
        at org.apache.jackrabbit.core.query.lucene.constraint.FullTextConstraint.<init>(FullTextConstraint.java:42)
        at org.apache.jackrabbit.core.query.lucene.constraint.ConstraintBuilder$Visitor.visit(ConstraintBuilder.java:175)
        at org.apache.jackrabbit.spi.commons.query.qom.FullTextSearchImpl.accept(FullTextSearchImpl.java:117)
        at org.apache.jackrabbit.core.query.lucene.constraint.ConstraintBuilder$Visitor.visit(ConstraintBuilder.java:137)
        at org.apache.jackrabbit.spi.commons.query.qom.AndImpl.accept(AndImpl.java:72)
        at org.apache.jackrabbit.core.query.lucene.constraint.ConstraintBuilder.create(ConstraintBuilder.java:82)
        at org.apache.jackrabbit.core.query.lucene.QueryObjectModelImpl.execute(QueryObjectModelImpl.java:109)
        at org.apache.jackrabbit.core.query.QueryImpl.execute(QueryImpl.java:127)

I tried to fix this issue but there is no way to access the BindVariables from the ConstraintBuilder from the LuceneQueryFactoryImpl and the ConstraintBuilder just passes the FullTextSearchImpl QOM subtree to the factory (via FullTextConstraint constructor) without any further visiting. If the signature would be ""LuceneQueryFactoryImpl#create(FullTextSearchImpl fts, Value searchExpression)"" we could visit the StaticOperand in the ConstraintBuilder and then modify the FullTextSearchImpl constructor accordingly, but this would imply that LuceneQueryFactory interface would need to be change accordingly and I don't know what that would mean."
1,"Moving a node while index is merged leads to inconsistent indexThe IndexMerger keeps track of nodes that are deleted from the index and applies that change also to the merged index, but if the same node is added again to the index during the merge process the index becomes inconsistent."
1,"XPath query with child axis predicatesExecuting a query using a long child path in a child axis predicate (like //*[a/b/c/d/e/@prop='something']) may return too many or not enough nodes.

I'll attach a zip file containing 2 tests cases showing this issue (I apologize but the test data are in French).
"
1,"DefaultHttpParamsFactory violates applet sandboxThe DefaultHttpParamsFactory in nightly build 20031009 makes two calls to 
System.getProperties().  This is by default verboten in an applet.  I have 
patched the source to catch the security exceptions and set the properties to a 
default value.  My modified code block follows:

        // TODO: To be removed. Provided for backward compatibility
        try {
          String agent = System.getProperties().getProperty
(""httpclient.useragent"");
          if (agent != null) {
            params.setParameter(HttpMethodParams.USER_AGENT, agent);
          }
        }
        catch (SecurityException dontCare) { }

        // TODO: To be removed. Provided for backward compatibility
        try {
          String preemptiveDefault = System.getProperties()
              .getProperty(""httpclient.authentication.preemptive"");
          if (preemptiveDefault != null) {
            preemptiveDefault = preemptiveDefault.trim().toLowerCase();
            if (preemptiveDefault.equals(""true"")) {
              params.setParameter
(HttpClientParams.PREEMPTIVE_AUTHENTICATION, ""on"");
            }
            else if (preemptiveDefault.equals(""false"")) {
              params.setParameter
(HttpClientParams.PREEMPTIVE_AUTHENTICATION, ""off"");
            }
          }
        }
        catch(SecurityException dontCare) { }"
1,JCARepositoryManager does not close InputStreamJCR-3129 opened a already closed issue [JCR-1667]
1,"checkIfNodeLocked()  in jcr mapping layer  does not behave properly when open scoped locks are usedI am planning to use open-scoped lock.  For which , I need to persist the locktoken along with the node  so that it can be used by another session for unlocking.Tested with Jackrabbit RMI client and it works fine.

But I am using jcr-mapping layer to achieve the above in my project.Here I want that  as soon as a node is checked out, it gets locked by the session and the lock is stored in ""lockToken"" property of node ""Document"". For that I need to update the Document node after locking .

*public void checkout(String path)throws CMSException {
       pm = getPersistenceManager();
        try{
           pm.checkout(path);*
*            String lockToken = pm.lock(path,true,false);   **        
    Document doc = this.getDocument(path);**           
  doc.setLockToken(lockToken);     //for persisting lockToken
           doc.update();
        }catch(LockedException le){
          System.out.println(le.getLockedNodePath() + ""is locked by"" + le.getLockOwner());         }catch(Exception e){
            throw new CMSException(e.getMessage(),e.getCause());
        }
   }*


Here doc.update() fails with Locked Exception.  The problem here is PersistenceManagerImpl has a method checkIfNodeLocked(path)  which returns LockException if node is locked. This method is checked before every update/insert. So, I am not able to update a locked node. I need to persist the locktoken in the node . What is the reason of checking  for a lock before saving ? Ideally , it should throw error only if node is locked and session does not hold the lockToken .

If the session who has locked the node tries to save the node without unlocking, it should be allowed .

p.s.
I am able to achieve the above by simple Jackrabbit RMI client.
<code>
               ClientRepositoryFactory factory = new ClientRepositoryFactory();
               Repository repository = factory.getRepository(""rmi://localhost:1101/jackrabbit"");
               Session session = repository.login(new SimpleCredentials(""superuser"", ""superuser"".toCharArray()),""Portal"");                        String user = session.getUserID();
               String name = repository.getDescriptor(Repository.REP_NAME_DESC);
               System.out.println(
                       ""Logged in as "" + user + "" to a "" + name + "" repository."");

               /* Testing the locks functionality */
               Node n = session.getRootNode().getNode(""cms/childfolder1/check.txt"");

              * Lock lck = n.lock(true, false); // deeplock,open-scoped
               n.setProperty(""ps:locktoken"",lck.getLockToken());
               n.setProperty(""ps:language"", ""sanskrit"");
*
               System.out.println(""Lock#isLive="" + lck.isLive());
               System.out.println(""Node#isLocked="" +  session.getRootNode().getNode(""cms/childfolder1/check.txt"").isLocked());
               session.save();
               session.logout();
     <code> "
1,"LLRect.createBox returned box does not contains all points in (center,distance) discLLRect,createBox computation of a bouding box for a disc given center and distance doest not contains all the point in the distance.

Example : the point north by distance doest not have Lat inferior of Lat of the UpperRight corner of the returned box"
1,"Impossible to import a string containing _x0020_  with Session.importXmlThe importXml uses the ValueHelper.serialize methods. The option ""decodeBlanks"" does a simple string replace which replaces _x0020_ in spaces (line 695 and 793). This option is always set to true unless the imported data is binary. See: BufferedStringValue and StringValue getValue methods.

The result is that it is now impossible to import a string with _x0020_ in it, because it gets translated in a space. The simple solution would be to just turn off the declodeBlanks option, but I'm not sure why it was added in the first place. Another option would be to use real encoding instead of a replace like the o.a.j.util.ISO9075.

"
1,"Item.isNew() does not work correctly within a transactionjavadoc on Item.isNew() states:

     * Returns <code>true</code> if this is a new item, meaning that it exists only in transient
     * storage on the <code>Session</code> and has not yet been saved. Within a transaction,
     * <code>isNew</code> on an <code>Item</code> may return <code>false</code> (because the item
     * has been saved) even if that <code>Item</code> is not in persistent storage (because the
     * transaction has not yet been committed).

but currently, Item.isNew() returns ""true"" after beeing saved in a transaction."
1,"ItemStateException on concurrently committing transactions of versioning operationssee tests in JCR-335


org.apache.jackrabbit.core.state.ItemStateException: Unable to resolve path for item: 69d80165-7ef5-4b6b-8aa9-be9c9be1f994
	at org.apache.jackrabbit.core.observation.EventStateCollection.getPath(EventStateCollection.java:525)
	at org.apache.jackrabbit.core.observation.EventStateCollection.createEventStates(EventStateCollection.java:377)
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:547)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:668)
	at org.apache.jackrabbit.core.state.XAItemStateManager.prepare(XAItemStateManager.java:151)
	at org.apache.jackrabbit.core.version.XAVersionManager.prepare(XAVersionManager.java:431)
	at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:129)
	at org.apache.jackrabbit.core.XASessionImpl.prepare(XASessionImpl.java:309)
	at test.JCRUserTransaction.commit(JCRUserTransaction.java:74)
	at org.apache.jackrabbit.JRTestDeadlock.run(JRTestDeadlock.java:110)
Caused by: javax.jcr.ItemNotFoundException: failed to build path of 69d80165-7ef5-4b6b-8aa9-be9c9be1f994: a0ecd4b0-a442-4b1e-a2f6-51441f40d452 has no child entry for 69d80165-7ef5-4b6b-8aa9-be9c9be1f994
	at org.apache.jackrabbit.core.HierarchyManagerImpl.buildPath(HierarchyManagerImpl.java:308)
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getPath(HierarchyManagerImpl.java:357)
	at org.apache.jackrabbit.core.observation.EventStateCollection.getPath(EventStateCollection.java:520)
	... 9 more"
1,"trunk:  TestDocumentsWriterDeleteQueue.testStressDeleteQueue seed failurefails 100% of the time for me, trunk r1152089

{code}
    [junit] Testsuite: org.apache.lucene.index.TestDocumentsWriterDeleteQueue
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.585 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestDocumentsWriterDeleteQueue -Dtestmethod=testStressDeleteQueue -Dtests.seed=724635056932528964:-56
53725200660632980
    [junit] NOTE: test params are: codec=RandomCodecProvider: {}, locale=en_US, timezone=Pacific/Port_Moresby
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestDocumentsWriterDeleteQueue]
    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_26 (64-bit)/cpus=8,threads=1,free=86067624,total=125632512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testStressDeleteQueue(org.apache.lucene.index.TestDocumentsWriterDeleteQueue):    FAILED
{code}"
1,"MaxCount not working correctly in user/group query when restricting to group membersFor user/group queries having a scope *and* a limit clause maxCount does not work correctly.

    builder.setScope(""contributors"", false);
    builder.setLimit(0, 50);

In the above case, the result might contain to few results. 

This is related to JCR-2829"
1,"If index has more than Integer.MAX_VALUE terms, seeking can it AIOOBE due to long/int overflowTom hit a new long/int overflow case: http://markmail.org/thread/toyl2ujcl4suqvf3

This is a regression, in 3.1, introduced with LUCENE-2075.

Worse, our Test2BTerms failed to catch this, so I've fixed that test to show the failure."
1,"Combination of BooleanQuery and PhrasePrefixQuery can provoke UnsupportedOperationExceptionA BooleanQuery including a PhrasePrefixQuery can cause an exception to be thrown
from BooleanScorer#skipTo when the search is executed:  

java.lang.UnsupportedOperationException
	at org.apache.lucene.search.BooleanScorer.skipTo(BooleanScorer.java:189)
	at org.apache.lucene.search.ConjunctionScorer.doNext(ConjunctionScorer.java:53)
	at org.apache.lucene.search.ConjunctionScorer.next(ConjunctionScorer.java:48)
	at org.apache.lucene.search.Scorer.score(Scorer.java:37)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:92)
	at org.apache.lucene.search.Hits.getMoreDocs(Hits.java:64)
	at org.apache.lucene.search.Hits.<init>(Hits.java:43)
	at org.apache.lucene.search.Searcher.search(Searcher.java:33)
	at org.apache.lucene.search.Searcher.search(Searcher.java:27)
        ... (non-lucene code)

The problem appears to be that PhrasePrefixQuery optimizes itself into a
BooleanQuery when it contains only one term.  However, it does this in the
createWeight() method of its scorer instead of in the rewrite method of the
query itself.  Thus it bypasses the boolean typecheck when BooleanQuery is
deciding whether to use ConjunctionScorer or BooleanScorer, eventually resulting
in the UOE."
1,"RowIterator view of result for query '//*' only returns jcr:path columnThe RowIterator view of a query result for '//*' only returns the jcr:path column. The spec states that this query is equivalent to:
select * from nt:base. Furthermore a query that selects * properties must return all non-residual properties that are declared for this node type and are not multi-valued. The pseudo properties jcr:path and jcr:score must always be available.

For nt:base this is:
- jcr:primaryType
- jcr:path
- jcr:score"
1,"[PATCH] Problem with Sort logic on tokenized fieldsWhen you set s SortField to a Text field which gets tokenized
FieldCacheImpl uses the term to do the sort, but then sorting is off 
especially with more then one word in the field. I think it is much 
more logical to sort by field's string value if the sort field is Tokenized and
stored. This way you'll get the CORRECT sort order"
1,"MatchAllDocsQueryNode toString() creates invalid XML-TagMatchAllDocsQueryNode.toString() returns ""<matchAllDocs field='*' term='*'>"", which is inavlid XML should read ""<matchAllDocs field='*' term='*' />.
"
1,"RegexCapabilities is not SerializableThe class RegexQuery is marked Serializable by its super class, but it contains a RegexCapabilities which is not Serializable. Thus attempting to serialize the query results in an exception. 

Making RegexCapabilities serializable should be no problem since its subclasses contain only serializable classes (java.util.regex.Pattern and org.apache.regexp.RE)."
1,"GetMethod.java checks the ""used"" flag which cannot be set at this timeGetMethod.getResponseBodyAsStream calls HttpMethodBase.checkUsed, which asserts
the flag ""used"" is ""true"". But at this time, ""used"" cannot be true, as ""used"" is
set to true in HttpMethodBase.processRequest, two lines after readResponse is
called (which in turn calls readResponseBody / readResponseBodyAsStream).

Maybe ""requestSent"" is the flag which should be checked instead of ""used""?

My stack trace: (fragment)

java.lang.IllegalStateException: Not Used.
        at
org.apache.commons.httpclient.HttpMethodBase.checkUsed(HttpMethodBase.java:1642)
        at
org.apache.commons.httpclient.methods.GetMethod.getResponseBodyAsStream(GetMethod.java:309)
        at
org.apache.commons.httpclient.methods.GetMethod.readResponseBody(GetMethod.java:428)
        at
org.apache.commons.httpclient.HttpMethodBase.readResponse(HttpMethodBase.java:1893)
        at
org.apache.commons.httpclient.HttpMethodBase.processRequest(HttpMethodBase.java:2496)
        at
org.apache.commons.httpclient.HttpMethodBase.execute(HttpMethodBase.java:1062)
        at
org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:599)
        at
org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:497)
        at
org.apache.webdav.lib.WebdavResource.getMethodData(WebdavResource.java:2227)
        at
org.apache.webdav.lib.WebdavResource.getMethodData(WebdavResource.java:2206)
[...]"
1,"ObjectContentManagerImpl.getObject(Query) throws NoSuchElementException when query does not match an objectWhen a query returns no objects, ObjectContentManagerImpl.getObject(Query) throws the following exception:

java.util.NoSuchElementException
        at java.util.AbstractList$Itr.next(AbstractList.java:427)
        at org.apache.jackrabbit.ocm.manager.impl.ObjectContentManagerImpl.getObject(ObjectContentManagerImpl.java:538)

Javadocs for ObjectContentManager interface suggest that a ObjectContentManagerException should be thrown in this case."
1,"reopen on NRT reader should share readers w/ unchanged segmentsA repoen on an NRT reader doesn't seem to share readers for those segments that are unchanged.
http://search.lucidimagination.com/search/document/9f0335d480d2e637/nrt_and_caching_based_on_indexreader"
1,"Incorrect CND for mix:etagJackrabbit currently defined mix:etag as follows:

[mix:etag]
  mixin
  // currently has a default value because auto-creation not handled see JCR-2116
  - jcr:etag (STRING) = '' protected autocreated

I think this violates the spec, which says:

[mix:etag] mixin
  - jcr:etag (STRING) protected autocreated

This also affects the predefined node type test in jackrabbit-jcr-tests where mix-etag.txt is:

NodeTypeName
  mix:etag
Supertypes
  []
IsMixin
  true
HasOrderableChildNodes
  false
PrimaryItemName
  null
PropertyDefinition
  Name jcr:etag
  RequiredType STRING
  DefaultValues []
  AutoCreated true
  Mandatory false
  OnParentVersion COPY
  Protected true
  Multiple false

but should rather be:

NodeTypeName
  mix:etag
Supertypes
  []
IsMixin
  true
HasOrderableChildNodes
  false
PrimaryItemName
  null
PropertyDefinition
  Name jcr:etag
  RequiredType STRING
  DefaultValues null               <===
  AutoCreated true
  Mandatory false
  OnParentVersion COPY
  Protected true
  Multiple false
"
1,"jcr2spi: NPE with SessionImporter#checkIncludesMixReferenceable if NodeInfo doesn't contain mixin namesissue reported by tobi:

java.lang.NullPointerException
	at java.util.Arrays$ArrayList.<init>(Arrays.java:2355)
	at java.util.Arrays.asList(Arrays.java:2341)
	at org.apache.jackrabbit.jcr2spi.xml.SessionImporter.checkIncludesMixReferenceable(SessionImporter.java:637)
	at org.apache.jackrabbit.jcr2spi.xml.SessionImporter.startNode(SessionImporter.java:209)

including test case:

    public void testEmptyMixins() throws Exception {
        String xml = ""<?xml version=\""1.0\"" encoding=\""UTF-8\""?>\n"" +
                ""<sv:node xmlns:nt=\""http://www.jcp.org/jcr/nt/1.0\""\n"" +
                ""         xmlns:sv=\""http://www.jcp.org/jcr/sv/1.0\""\n"" +
                ""         xmlns:mix=\""http://www.jcp.org/jcr/mix/1.0\""\n"" +
                ""         xmlns:jcr=\""http://www.jcp.org/jcr/1.0\""\n"" +
                ""         sv:name=\""testnode1\"">\n"" +
                ""    <sv:property sv:name=\""jcr:primaryType\""
sv:type=\""Name\"">\n"" +
                ""        <sv:value>nt:unstructured</sv:value>\n"" +
                ""    </sv:property>\n"" +
                ""    <sv:property sv:name=\""jcr:title\"" sv:type=\""String\"">\n"" +
                ""        <sv:value>Test Node</sv:value>\n"" +
                ""    </sv:property>\n"" +
                ""    <sv:property sv:name=\""jcr:uuid\"" sv:type=\""String\"">\n"" +
                ""        <sv:value>1234</sv:value>\n"" +
                ""    </sv:property>\n"" +
                ""</sv:node>"";

        InputStream in = new ByteArrayInputStream(xml.getBytes());
        session.importXML(""/"", in,
ImportUUIDBehavior.IMPORT_UUID_COLLISION_THROW);
        session.save();
    }"
1,"Catch SocketTimeoutException not InterruptedIOExceptionThere are a couple of places where you're catching InterruptedIOException 
that should catch SocketTimeoutException instead.  For example, from 
HttpConnection:

    protected boolean isStale() {
        boolean isStale = true;
        if (isOpen) {
            // the connection is open, but now we have to see if we can 
read it
            // assume the connection is not stale.
            isStale = false;
            try {
                if (inputStream.available() == 0) {
                    try {
                        socket.setSoTimeout(1);
                        inputStream.mark(1);
                        int byteRead = inputStream.read();
                        if (byteRead == -1) {
                            // again - if the socket is reporting all data 
read,
                            // probably stale
                            isStale = true;
                        } else {
                            inputStream.reset();
                        }
                    } finally {
                        socket.setSoTimeout(this.params.getSoTimeout());
                    }
                }
            } catch (InterruptedIOException e) {
                // aha - the connection is NOT stale - continue on!

Here the catch of InterruptedIOException is intended to happen when 
inputStream.read() terminates due to the socket.setSoTimeout() time being 
reached.  However, it could also occur because Thread.interrupt() has been 
called, in which case ""continue on"" is not what should happen, instead, the 
request should terminate.

There are legitimate reasons why someone might want to interrupt the 
httpclient code, for example, httpclient does not provide a hard timeout on 
the total length of time a request may take, including connecting, sending 
the request, and receiving the complete response, so to enforce a hard 
timeout it is necessary to run the request in a worker thread and interrupt 
it if it hasn't completed before the timeout expires (the technique used in 
your TimeoutController class).

Note that SocketTimeoutException was added in 1.4.  For compatibility with 
older jdk versions, the code can catch InterruptedIOException and use 
getClass() to see whether it is a SocketTimeoutException.

There are probably other places in the code where InterruptedIOException is 
caught and interpreted as a socket timeout, and where Thread.interrupt() 
will not have the proper effect of causing the request to terminate ASAP, 
but I'm not familiar enough with the code to find them all."
1,"Adding empty ParallelReader indexes to an IndexWriter may cause ArrayIndexOutOfBoundsException or NoSuchElementExceptionHi,
I recently stumbled upon this:

It is possible (and perfectly legal) to add empty indexes (IndexReaders) to an IndexWriter. However, when using ParallelReaders in this context, in two situations RuntimeExceptions may occur for no good reason.

Condition 1:
The indexes within the ParallelReader are just empty.

When adding them to the IndexWriter, we get a java.util.NoSuchElementException triggered by ParallelTermEnum's constructor. The reason for that is the TreeMap#firstKey() method which was assumed to return null if there is no entry (which is not true, apparently -- it only returns null if the first key in the Map is null).


Condition 2 (Assuming the aforementioned bug is fixed):
The indexes within the ParallelReader originally contained one or more fields with TermVectors, but all documents have been marked as deleted.

When adding the indexes to the IndexWriter, we get a java.lang.ArrayIndexOutOfBoundsException triggered by TermVectorsWriter#addAllDocVectors. The reason here is that TermVectorsWriter assumes that if the index is marked to have TermVectors, at least one field actually exists for that. This unfortunately is not true, either.

Patches and a testcase demonstrating the two bugs are provided.

Cheers,
Christian"
1,"inconsistent session state after Item/Session.save() throwing ReferentialIntegrityExceptionissue reported by Tomasz.Dabrowski@cognifide.com on jackrabbit dev list.

code fragment to reproduce issue:

// setup test case

Node parent = root.addNode(""a"", ""nt:unstructured"");
Node child1 = parent.addNode(""b"", ""nt:unstructured"");
child1.addMixin(""mix:referenceable"");
Node child2 = parent.addNode(""c"", ""nt:unstructured"");
child2.setProperty(""ref"", child1);
root.save();

// perform test

try {
    child1.remove();
    parent.save();
} catch (ReferentialIntegrityException rie) {
    // expected since child1 is still being referenced by property ""ref"" of child2
}

parent.remove();     // ==> should succeed but throws ItemNotFoundException 

"
1,"Unable to delete a non session-scoped locked node in XA EnvironmentYou must first add a valid lockToken to the Session and then try to remove this node in a XA Environment.
This will resulting in a NoSuchItemStateException: State has been marked destroyed.
The  problem is that the unlock Operation will be done after that the node has been marked for destroyed.
"
1,"minimizeHopcroft OOMEs on smallish (2096 states, finite) automatonNot sure what's up w/ this... if you check out the blocktree branch (LUCENE-3030) and comment out the @Ignore in TestTermsEnum2.testFiniteVersusInfinite then this should hit OOME: {[ant test-core -Dtestcase=TestTermsEnum2 -Dtestmethod=testFiniteVersusInfinite -Dtests.seed=-2577608857970454726:-2463580050179334504}}"
1,"offsets issues with multiword synonymsas reported on the list, there are some strange offsets with FSTSynonyms, in the case of multiword synonyms.

as a workaround it was suggested to use the older synonym impl, but it has bugs too (just in a different way).
"
1,"DefaultPrincipalProvider#collectGroupMembership puts wrong principal instance into the cacheDefaultPrincipalProvider#collectGroupMembership adds the passed principal instance to the cache. This may cause
inconsistencies as the cache should only contain principals obtained from by the provider."
1,"Upgrade to commons-compress 1.2Commons Compress bug COMPRESS-127 was fixed in 1.2, so the workaround in benchmark's StreamUtils is no longer required. Compress is also used in solr. Replace with new jar in both benchmark and solr and get rid of that workaround."
1,"Issues with compiled permissions of ACL provider- should not use search for infrastructure checks
- event listener never discarded."
1,"XMLPersistenceManager trims string property valuesThe XMLPersistenceManager trims the text of property values read in, so what's returned doesn't match the value set if it included whitespace at the start or end."
1,"Ordering of methods in PostMethod changes behaviourI have just spent the best part of two days trying to work out why
a servlet running in Tomcat was not getting UTF-8 when I had set my
client to send UTF-8. It turns out that if I set my PostMethod request
header after setting the request body the content does not get sent as
UTF-8.

The following gets sent as UTF-8:

      PostMethod post = new PostMethod(destinationUrl.toString());
      post.setStrictMode(false);
      post.setRequestHeader(""Content-Type"",""text/xml; charset=UTF-8"");
      post.setRequestHeader(""user-agent"", ""myAgent"");
      post.setRequestBody(content);
      post.setFollowRedirects(true);

the following doesn't:

      PostMethod post = new PostMethod(destinationUrl.toString());
      post.setStrictMode(false);
      post.setRequestBody(content);
      post.setRequestHeader(""Content-Type"",""text/xml; charset=UTF-8"");
      post.setRequestHeader(""user-agent"", ""myAgent"");
      post.setFollowRedirects(true);

In a live execution I would understand that order makes a big difference, but
when you fill out an object that feels like defining the values of a Java Bean
this likely to be less obvious."
1,"NRTCachingDir has invalid asserts (if same file name is written twice)Normally Lucene is write-once (except for segments.gen file, which NRTCachingDir never caches), but in some tests (TestDoc, TestCrash) we can write the same file more than once.

I don't think NRTCachingDir should have these asserts, and I think on createOutput it should remove any old file if present.

I also found & fixed a possible concurrency issue (if more than one thread syncs at the same time; IndexWriter doesn't ever do this today but it has in the past)."
1,"When HttpClient-Cache cannot open cache file, should act like missSet up HttpClient-Cache like this:
final String cacheDir = ""cachedir"";
HttpClient cachingHttpClient;
final CacheConfig cacheConfig = new CacheConfig();
cacheConfig.setSharedCache(false);
cacheConfig.setMaxObjectSizeBytes(262144); //256kb

if(! new File(cacheDir, ""httpclient-cache"").exists()){
	if(!new File(cacheDir, ""httpclient-cache"").mkdir()){
		throw new RuntimeException(""failed to create httpclient cache directory: "" + new File(cacheDir, ""httpclient-cache"").getAbsolutePath());
	}
}
final ResourceFactory resourceFactory = new FileResourceFactory(new File(cacheDir, ""httpclient-cache""));

final HttpCacheStorage httpCacheStorage = new ManagedHttpCacheStorage(cacheConfig);

cachingHttpClient = new CachingHttpClient(client, resourceFactory, httpCacheStorage, cacheConfig);

Then make a request:
final HttpGet get = new HttpGet(url);
final HttpResponse response = cachingHttpClient.execute(get);
final StatusLine statusLine = response.getStatusLine();
if (statusLine.getStatusCode() >= 300) {
	if(statusLine.getStatusCode() == 404)
		throw new NoResultException();
    throw new HttpResponseException(statusLine.getStatusCode(),
            statusLine.getReasonPhrase());
}
response.getEntity().getContent();

Everything worked as expected.

Now delete the cache directory (""cachedir/httpclient-cache"" in this example).

And make the same request again.

Actual:
 Caused by: java.lang.IllegalStateException: Content has been consumed
	at org.apache.http.entity.BasicHttpEntity.getContent(BasicHttpEntity.java:84)
	at org.apache.http.conn.BasicManagedEntity.getContent(BasicManagedEntity.java:100)

Expected:
HttpClient shouldn't throw an exception - it should just perform the request again acting like a cache miss."
1,"FieldCache should not pay attention to deleted docs when creating entriesThe FieldCache uses a key that ignores deleted docs, so it's actually a bug to use deleted docs when creating an entry.  It can lead to incorrect values when the same entry is used with a different reader."
1,"SO_TIMEOUT parameter on the method level has no effectThis bug has been reported on the HttpClient user list by Ilya Kharmatsky <ilyak
-at- mainsoft.com>"
1,"StackOverflowError in HttpConnectionWhen the HttpConnection#WrappedOutputStream.flush () encounters IOException 
druign write, it is calling HttpConnection.close which calls 
HttpConnection.closeSocketAndStreams and which eventually calls 
HttpConnection#WrappedOutputStream.flush again.  The circular calls will cause 
StackOverflowError.

I run into this accidentally when I was trying to extend HttpConnection.  But 
looking through the code, I believe any IOException may cause the same 
problem.  The circular calls should be either removed or controlled.  Below is 
part of teh stack trace

java.lang.StackOverflowError
        at java.lang.Exception.<init>(Unknown Source)
        at java.io.IOException.<init>(Unknown Source)
        at java.net.SocketException.<init>(Unknown Source)
        at java.net.SocketOutputStream.socketWrite(Native Method)
        at java.net.SocketOutputStream.write(Unknown Source)
        at org.apache.commons.httpclient.HttpConnection$WrappedOutputStream.writ
e(HttpConnection.java:1273)
        at java.io.BufferedOutputStream.flushBuffer(Unknown Source)
        at java.io.BufferedOutputStream.flush(Unknown Source)
        at java.io.FilterOutputStream.close(Unknown Source)
        at org.apache.commons.httpclient.HttpConnection.closeSocketAndStreams(Ht
tpConnection.java:1083)
        at org.apache.commons.httpclient.HttpConnection.close(HttpConnection.jav
a:1024)
        at org.apache.commons.httpclient.HttpConnection$WrappedOutputStream.hand
leException(HttpConnection.java:1235)
        at org.apache.commons.httpclient.HttpConnection$WrappedOutputStream.writ
e(HttpConnection.java:1275)
        at java.io.BufferedOutputStream.flushBuffer(Unknown Source)
        at java.io.BufferedOutputStream.flush(Unknown Source)
        at java.io.FilterOutputStream.close(Unknown Source)
        at org.apache.commons.httpclient.HttpConnection.closeSocketAndStreams(Ht
tpConnection.java:1083)
        at org.apache.commons.httpclient.HttpConnection.close(HttpConnection.jav
a:1024)
        at org.apache.commons.httpclient.HttpConnection$WrappedOutputStream.hand
leException(HttpConnection.java:1235)
        at org.apache.commons.httpclient.HttpConnection$WrappedOutputStream.writ
e(HttpConnection.java:1275)
        at java.io.BufferedOutputStream.flushBuffer(Unknown Source)
        at java.io.BufferedOutputStream.flush(Unknown Source)
        at java.io.FilterOutputStream.close(Unknown Source)
        at org.apache.commons.httpclient.HttpConnection.closeSocketAndStreams(Ht
tpConnection.java:1083)
        at org.apache.commons.httpclient.HttpConnection.close(HttpConnection.jav
a:1024)
        at org.apache.commons.httpclient.HttpConnection$WrappedOutputStream.hand
leException(HttpConnection.java:1235)
        at org.apache.commons.httpclient.HttpConnection$WrappedOutputStream.writ
e(HttpConnection.java:1275)
        at java.io.BufferedOutputStream.flushBuffer(Unknown Source)
        at java.io.BufferedOutputStream.flush(Unknown Source)
        at java.io.FilterOutputStream.close(Unknown Source)
        at org.apache.commons.httpclient.HttpConnection.closeSocketAndStreams(Ht
tpConnection.java:1083)
        at org.apache.commons.httpclient.HttpConnection.close(HttpConnection.jav
a:1024)
        at org.apache.commons.httpclient.HttpConnection$WrappedOutputStream.hand
leException(HttpConnection.java:1235)
        at org.apache.commons.httpclient.HttpConnection$WrappedOutputStream.writ
e(HttpConnection.java:1275)"
1,"[PATCH] TermInfosReader, SegmentTermEnum Out Of Memory ExceptionWe've been experiencing terrible memory problems on our production search server, running lucene (1.4.3).

Our live app regularly opens new indexes and, in doing so, releases old IndexReaders for garbage collection.

But...there appears to be a memory leak in org.apache.lucene.index.TermInfosReader.java.
Under certain conditions (possibly related to JVM version, although I've personally observed it under both linux JVM 1.4.2_06, and 1.5.0_03, and SUNOS JVM 1.4.1) the ThreadLocal member variable, ""enumerators"" doesn't get garbage-collected when the TermInfosReader object is gc-ed.

Looking at the code in TermInfosReader.java, there's no reason why it _shouldn't_ be gc-ed, so I can only presume (and I've seen this suggested elsewhere) that there could be a bug in the garbage collector of some JVMs.

I've seen this problem briefly discussed; in particular at the following URL:
  http://java2.5341.com/msg/85821.html
The patch that Doug recommended, which is included in lucene-1.4.3 doesn't work in our particular circumstances. Doug's patch only clears the ThreadLocal variable for the thread running the finalizer (my knowledge of java breaks down here - I'm not sure which thread actually runs the finalizer). In our situation, the TermInfosReader is (potentially) used by more than one thread, meaning that Doug's patch _doesn't_ allow the affected JVMs to correctly collect garbage.

So...I've devised a simple patch which, from my observations on linux JVMs 1.4.2_06, and 1.5.0_03, fixes this problem.

Kieran
PS Thanks to daniel naber for pointing me to jira/lucene

@@ -19,6 +19,7 @@
 import java.io.IOException;

 import org.apache.lucene.store.Directory;
+import java.util.Hashtable;

 /** This stores a monotonically increasing set of <Term, TermInfo> pairs in a
  * Directory.  Pairs are accessed either by Term or by ordinal position the
@@ -29,7 +30,7 @@
   private String segment;
   private FieldInfos fieldInfos;

-  private ThreadLocal enumerators = new ThreadLocal();
+  private final Hashtable enumeratorsByThread = new Hashtable();
   private SegmentTermEnum origEnum;
   private long size;

@@ -60,10 +61,10 @@
   }

   private SegmentTermEnum getEnum() {
-    SegmentTermEnum termEnum = (SegmentTermEnum)enumerators.get();
+    SegmentTermEnum termEnum = (SegmentTermEnum)enumeratorsByThread.get(Thread.currentThread());
     if (termEnum == null) {
       termEnum = terms();
-      enumerators.set(termEnum);
+      enumeratorsByThread.put(Thread.currentThread(), termEnum);
     }
     return termEnum;
   }
@@ -195,5 +196,15 @@
   public SegmentTermEnum terms(Term term) throws IOException {
     get(term);
     return (SegmentTermEnum)getEnum().clone();
+  }
+
+  /* some jvms might have trouble gc-ing enumeratorsByThread */
+  protected void finalize() throws Throwable {
+    try {
+        // make sure gc can clear up.
+        enumeratorsByThread.clear();
+    } finally {
+        super.finalize();
+    }
   }
 }



TermInfosReader.java, full source:
======================================
package org.apache.lucene.index;

/**
 * Copyright 2004 The Apache Software Foundation
 *
 * Licensed under the Apache License, Version 2.0 (the ""License"");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.io.IOException;

import org.apache.lucene.store.Directory;
import java.util.Hashtable;

/** This stores a monotonically increasing set of <Term, TermInfo> pairs in a
 * Directory.  Pairs are accessed either by Term or by ordinal position the
 * set.  */

final class TermInfosReader {
  private Directory directory;
  private String segment;
  private FieldInfos fieldInfos;

  private final Hashtable enumeratorsByThread = new Hashtable();
  private SegmentTermEnum origEnum;
  private long size;

  TermInfosReader(Directory dir, String seg, FieldInfos fis)
       throws IOException {
    directory = dir;
    segment = seg;
    fieldInfos = fis;

    origEnum = new SegmentTermEnum(directory.openFile(segment + "".tis""),
                                   fieldInfos, false);
    size = origEnum.size;
    readIndex();
  }

  public int getSkipInterval() {
    return origEnum.skipInterval;
  }

  final void close() throws IOException {
    if (origEnum != null)
      origEnum.close();
  }

  /** Returns the number of term/value pairs in the set. */
  final long size() {
    return size;
  }

  private SegmentTermEnum getEnum() {
    SegmentTermEnum termEnum = (SegmentTermEnum)enumeratorsByThread.get(Thread.currentThread());
    if (termEnum == null) {
      termEnum = terms();
      enumeratorsByThread.put(Thread.currentThread(), termEnum);
    }
    return termEnum;
  }

  Term[] indexTerms = null;
  TermInfo[] indexInfos;
  long[] indexPointers;

  private final void readIndex() throws IOException {
    SegmentTermEnum indexEnum =
      new SegmentTermEnum(directory.openFile(segment + "".tii""),
			  fieldInfos, true);
    try {
      int indexSize = (int)indexEnum.size;

      indexTerms = new Term[indexSize];
      indexInfos = new TermInfo[indexSize];
      indexPointers = new long[indexSize];

      for (int i = 0; indexEnum.next(); i++) {
	indexTerms[i] = indexEnum.term();
	indexInfos[i] = indexEnum.termInfo();
	indexPointers[i] = indexEnum.indexPointer;
      }
    } finally {
      indexEnum.close();
    }
  }

  /** Returns the offset of the greatest index entry which is less than or equal to term.*/
  private final int getIndexOffset(Term term) throws IOException {
    int lo = 0;					  // binary search indexTerms[]
    int hi = indexTerms.length - 1;

    while (hi >= lo) {
      int mid = (lo + hi) >> 1;
      int delta = term.compareTo(indexTerms[mid]);
      if (delta < 0)
	hi = mid - 1;
      else if (delta > 0)
	lo = mid + 1;
      else
	return mid;
    }
    return hi;
  }

  private final void seekEnum(int indexOffset) throws IOException {
    getEnum().seek(indexPointers[indexOffset],
	      (indexOffset * getEnum().indexInterval) - 1,
	      indexTerms[indexOffset], indexInfos[indexOffset]);
  }

  /** Returns the TermInfo for a Term in the set, or null. */
  TermInfo get(Term term) throws IOException {
    if (size == 0) return null;

    // optimize sequential access: first try scanning cached enum w/o seeking
    SegmentTermEnum enumerator = getEnum();
    if (enumerator.term() != null                 // term is at or past current
	&& ((enumerator.prev != null && term.compareTo(enumerator.prev) > 0)
	    || term.compareTo(enumerator.term()) >= 0)) {
      int enumOffset = (int)(enumerator.position/enumerator.indexInterval)+1;
      if (indexTerms.length == enumOffset	  // but before end of block
	  || term.compareTo(indexTerms[enumOffset]) < 0)
	return scanEnum(term);			  // no need to seek
    }

    // random-access: must seek
    seekEnum(getIndexOffset(term));
    return scanEnum(term);
  }

  /** Scans within block for matching term. */
  private final TermInfo scanEnum(Term term) throws IOException {
    SegmentTermEnum enumerator = getEnum();
    while (term.compareTo(enumerator.term()) > 0 && enumerator.next()) {}
    if (enumerator.term() != null && term.compareTo(enumerator.term()) == 0)
      return enumerator.termInfo();
    else
      return null;
  }

  /** Returns the nth term in the set. */
  final Term get(int position) throws IOException {
    if (size == 0) return null;

    SegmentTermEnum enumerator = getEnum();
    if (enumerator != null && enumerator.term() != null &&
        position >= enumerator.position &&
	position < (enumerator.position + enumerator.indexInterval))
      return scanEnum(position);		  // can avoid seek

    seekEnum(position / enumerator.indexInterval); // must seek
    return scanEnum(position);
  }

  private final Term scanEnum(int position) throws IOException {
    SegmentTermEnum enumerator = getEnum();
    while(enumerator.position < position)
      if (!enumerator.next())
	return null;

    return enumerator.term();
  }

  /** Returns the position of a Term in the set or -1. */
  final long getPosition(Term term) throws IOException {
    if (size == 0) return -1;

    int indexOffset = getIndexOffset(term);
    seekEnum(indexOffset);

    SegmentTermEnum enumerator = getEnum();
    while(term.compareTo(enumerator.term()) > 0 && enumerator.next()) {}

    if (term.compareTo(enumerator.term()) == 0)
      return enumerator.position;
    else
      return -1;
  }

  /** Returns an enumeration of all the Terms and TermInfos in the set. */
  public SegmentTermEnum terms() {
    return (SegmentTermEnum)origEnum.clone();
  }

  /** Returns an enumeration of terms starting at or after the named term. */
  public SegmentTermEnum terms(Term term) throws IOException {
    get(term);
    return (SegmentTermEnum)getEnum().clone();
  }

  /* some jvms might have trouble gc-ing enumeratorsByThread */ 
  protected void finalize() throws Throwable {
    try {
        // make sure gc can clear up.
        enumeratorsByThread.clear();
    } finally {
        super.finalize();
    }
  }
}
"
1,"MultiFieldQueryParser ignores slop parameterMultiFieldQueryParser.getFieldQuery(String, String, int) calls super.getFieldQuery(String, String), thus obliterating any slop parameter present in the query.

It should probably be changed to call super.getFieldQuery(String, String, int), except doing only that will result in a recursive loop which is a side-effect of what may be a deeper problem in MultiFieldQueryParser -- getFieldQuery(String, String, int) is documented as delegating to getFieldQuery(String, String), yet what it actually does is the exact opposite.  This also causes problems for subclasses which need to override getFieldQuery(String, String) to provide different behaviour.
"
1,"Node.orderBefore does not check permissionsIt seems that Node.orderBefore(String, String) does not check if the editing session is allowed to modify the parent, neither immediately nor upon saving the transient changes.

This issue was found by Alexandre Capt. Thanks!"
1,"Event filtering by path not working as specifiedWhen filtering node events by path, the event filter doesn't compare using the ""associated parent path"", see JSR-170, 8.3.3:

""The set of events can be filtered by specifying restrictions based on characteristics of the associated parent node of the event. The associated parent node of an event is the parent node of the item at (or formerly at) the path returned by Event.getPath. The following restrictions are available:

 absPath, isDeep: Only events whose associated parent node is at absPath (or within its subtree, if isDeep is true) will be received. It is permissible to register a listener for a path where no node currently exists.""

(for property events, filtering is correct)

To fix this, the special handling of node events in EventFilter.blocks() simply needs to be removed.
"
1,"Flexible QueryParser fails with local different from en_USI get the following error during the mentioned testcases on my computer, if I use the Locale de_DE (windows 32):

{code}
    [junit] Testsuite: org.apache.lucene.queryParser.standard.TestQPHelper
    [junit] Tests run: 29, Failures: 1, Errors: 0, Time elapsed: 1,156 sec
    [junit]
    [junit] ------------- Standard Output ---------------
    [junit] Result: (fieldX:xxxxx fieldy:xxxxxxxx)^2.0
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testLocalDateFormat(org.apache.lucene.queryParser.standard.TestQPHelper): FAILED
    [junit] expected:<1> but was:<0>
    [junit] junit.framework.AssertionFailedError: expected:<1> but was:<0>
    [junit]     at org.apache.lucene.queryParser.standard.TestQPHelper.assertHits(TestQPHelper.java:1148)
    [junit]     at org.apache.lucene.queryParser.standard.TestQPHelper.testLocalDateFormat(TestQPHelper.java:1005)
    [junit]     at org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:201)
    [junit]
    [junit]
    [junit] Test org.apache.lucene.queryParser.standard.TestQPHelper FAILED
    [junit] Testsuite: org.apache.lucene.queryParser.standard.TestQueryParserWrapper
    [junit] Tests run: 27, Failures: 1, Errors: 0, Time elapsed: 1,219 sec
    [junit]
    [junit] ------------- Standard Output ---------------
    [junit] Result: (fieldX:xxxxx fieldy:xxxxxxxx)^2.0
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testLocalDateFormat(org.apache.lucene.queryParser.standard.TestQueryParserWrapper):       FAILED
    [junit] expected:<1> but was:<0>
    [junit] junit.framework.AssertionFailedError: expected:<1> but was:<0>
    [junit]     at org.apache.lucene.queryParser.standard.TestQueryParserWrapper.assertHits(TestQueryParserWrapper.java:1120)
    [junit]     at org.apache.lucene.queryParser.standard.TestQueryParserWrapper.testLocalDateFormat(TestQueryParserWrapper.java:985)
    [junit]     at org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:201)
    [junit]
    [junit]
    [junit] Test org.apache.lucene.queryParser.standard.TestQueryParserWrapper FAILED
{code}

With en_US as locale it works."
1,"LogSource.setLevel incorrectly uses entrySetWhen I call LogSource.setLevel, I get the following exception:

java.lang.ClassCastException: java.util.HashMap$Entry
	at org.apache.commons.httpclient.log.LogSource.setLevel
(LogSource.java:158)

The calling code is :

    LogSource.setLevel (Log.OFF);

The error (I believe) is that you should get the value set from the map, not 
the entry set (in LogSource):

    static public void setLevel(int level) {
        Iterator it = _logs.entrySet().iterator(); <-- should be _logs.values()
        while(it.hasNext()) {
            Log log = (Log)(it.next());
            log.setLevel(level);
        }
    }"
1,"add checks/asserts if you search across a closed readerif you try to search across a closed reader (and/or searcher too),
there are no checks, not even assertions statements.

this results in crazy scary stacktraces deep inside places like FSTs/various term dictionary implementations etc.

In some situations, depending on codec, you wont even get an error (i'm sure its fun when you try to retrieve the stored fields!)
"
1,Creating QValue from stream: stream not closedQValueFactoryImpl.create(InputStream) does not close the input stream as mandated by the contract. 
1,"In modules/analysys/icu, ant gennorm2 does not work
Command to run gennorm2 does not work at present.  Also, icupkg needs to be called to convert the binary file to big-endian.

I will attach a patch."
1,"302 response without location header throws exceptionHi, 

According to HTTP 1.1 Spec : http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html#sec10.3.3
""The temporary URI SHOULD be given by the Location field in the response. Unless the request method was HEAD, the entity of the response SHOULD contain a short hypertext note with a hyperlink to the new URI(s).""

Now, in ""DefaultRedirectStrategy.getLocationURI()"", there's a ProtocolException thrown if location header is null.  

if (locationHeader == null) {
    // got a redirect response, but no location header
    throw new ProtocolException(
        ""Received redirect response "" + response.getStatusLine()
       + "" but no location header"");
 }

The specs says ""SHOULD"" and not ""MUST"". ProtocolException ""signals that an HTTP protocol violation has occurred"", which is not exactly true."
1,"InternalVersionManagerBase; missing null check after getNode()There are at least two instances where we check for a node with hasNode(), and then call getNode() without checking for null."
1,"DavMethodBase#getResponseException fails if the body is not (valid) XMLI have a set up that uses the JCR Webdav Server from a custom remote client.
I've noticed one thing, anytime I request a node that doesn't exist the error that comes back from the server is as follows:

[Fatal Error] :1:941: The element type ""HR"" must be terminated by the matching end-tag ""</HR>"".
javax.jcr.RepositoryException: The element type ""HR"" must be terminated by the matching end-tag ""</HR>"".: The element type ""HR"" must be terminated by the matching end-tag ""</HR>"".

Doesn't really make sense, but that is OK, I can handle that.

My problem:
I have a partially populated repository that at the root has a few nodes like
/edu/....
/com/ibm/..

So, I want to create a few nodes of type nt:folder under

com/myCompany/folder1

I have no problem creating them, but since ""com"" already exists I end up with
com[2]/myCompany/folder1.

So, I went ahead and used the parentNode.hasNode(""folderName"") method.

This method returns true for the ""com"" portion, but when I test for the ""myCompany"" folder which should return false I get the error response shown above from the server.

The webdav request looks as follows:
PROPFIND /jackrabbit/server/default/jcr%3aroot/com/myCompany

The snippet of code looks as follows:
  private Node createFolders (Session session, Node parentNode, List <String> folders)
	  throws RepositoryException {
    Node folderNode = null;
    for (String folder : folders) {
	if (parentNode.hasNode(folder))
	    folderNode = parentNode.getNode(folder);
	else
	    folderNode = parentNode.addNode(folder, ""nt:folder"");
	parentNode = folderNode;
    }
    session.save();
    return (folderNode);
  }"
1,"Missing equals and hashcode preventing the re-use of SharedFieldSortComparatorAs briefly mentioned in the dev email list, improperly implemented (i.e., missing - using the default Object implementation) equals and hashcode in SearchIndex.java prevents the reuse of a SharedFieldSortComparator between different queries when nothing has changed in the repository.  In tests, this appears to have a fairly significant negative performance impact.

Please see the following for the correct code:

http://svn.apache.org/viewvc?view=rev&revision=506908
"
1,"TestIndexFileDeleter checkIndex failfound on 3.x

{noformat}
ant test-tag -Dtestcase=TestIndexFileDeleter -Dtestmethod=testDeleteLeftoverFiles -Dtests.seed=7631088157098800527:4270221915205524915
{noformat}"
1,"Missing XPath escape in query.jspAs reported by Canberk Bolat of ADEO Security in a private communication, there search.jsp script in jackrabbit-webapp is missing an escape when it injects the path of a ""related:"" query into the constructed XPath statement. Further analysis showed that this issue has no security implications, so we can treat this as a normal bug report.

search.jsp
...
String q = request.getParameter(""q"");
...
      if (q != null && q.length() > 0) {
           String stmt;
           if (q.startsWith(""related:"")) {
               String path = q.substring(""related:"".length());
               stmt = ""//element(*, nt:file)[rep:similar(jcr:content,
'"" + path + ""/jcr:content')]/rep:excerpt(.) order by @jcr:score
descending"";
               queryTerms = ""similar to <b>"" +
Text.encodeIllegalXMLCharacters(path) + ""</b>"";
           }
...

"
1,"Versioning fixup leaves persistence in a state where the node can't be made versionable againJackrabbit's version recovery mode (org.apache.jackrabbit.version.recovery system property) disconnects all version histories that expose problems that manifest in unexpected exceptions being thrown. ""disconnects"" means removing the properties defined for mix:versionable and removing the mixin type. The actual versioning related nodes remain in place.

The problem: when re-adding mix:versionable, ItemSaveOperation.initVersionHistories tries to create the new version history in the same location (the path being derived from the versionable node's identifier), and consequently fails because of the broken underlying storage.

(attaching a work-in-progress test case that illustrates the problem)"
1,moving locked node removes locked statewhen moving a locked node it looses it locked state.
1,"Provider org.apache.xalan.processor.TransformerFactoryImpl not found""maven jar"" fails with the following error message on a fresh Jackrabbit source tree:

BUILD FAILED
File...... /home/hukka/tmp/jackrabbit/maven.xml
Element... ant:xslt
Line...... 146
Column.... 25
Provider org.apache.xalan.processor.TransformerFactoryImpl not found
Total time: 4 seconds
Finished at: Sun Feb 13 10:09:03 EET 2005
"
1,"Sorting produces duplicatesIf you run the code below the exception will be thrown. I believe that it isn't 
correct behaviour (the duplicities, of course), index id of hits should be 
unique as it is without sort.

Lucene versions:
1.4-final
1.4.1
CVS 1.5-rc1-dev


import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.document.Document;
import org.apache.lucene.document.Field;
import org.apache.lucene.index.IndexReader;
import org.apache.lucene.index.IndexWriter;
import org.apache.lucene.queryParser.ParseException;
import org.apache.lucene.queryParser.QueryParser;
import org.apache.lucene.search.Hits;
import org.apache.lucene.search.IndexSearcher;
import org.apache.lucene.search.Query;
import org.apache.lucene.search.Searcher;
import org.apache.lucene.search.Sort;
import org.apache.lucene.search.SortField;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.RAMDirectory;

import java.io.IOException;
import java.util.HashSet;
import java.util.LinkedList;
import java.util.ListIterator;
import java.util.Set;

/**
 * Run this test with Lucene 1.4 final or 1.4.1
 */
public class DuplicityTest
{
    public static void main(String[] args) throws IOException, ParseException
    {
        Directory directory = create_index();

        search_index(directory);
    }

    private static void search_index(Directory directory) throws IOException, 
ParseException
    {
        IndexReader reader = IndexReader.open(directory);
        Searcher searcher = new IndexSearcher(reader);

        Sort sort = new Sort(new SortField(""co"", SortField.INT, false));

        Query q = QueryParser.parse(""sword"", ""text"", new StandardAnalyzer());

        find_duplicity(searcher.search(q), ""no sort"");

        find_duplicity(searcher.search(q, sort), ""using sort"");

        searcher.close();
        reader.close();
    }

    private static void find_duplicity(Hits hits, String message) throws 
IOException
    {
        System.out.println(message + "" hits size: "" + hits.length());

        Set set = new HashSet();
        for (int i = 0; i < hits.length(); i++) {
//            System.out.println(hits.id(i) + "": "" + hits.doc(i).toString());
            Integer id = new Integer(hits.id(i));
            if (!set.contains(id))
                set.add(id);
            else
                throw new RuntimeException(""duplicity found, index id: "" + id);
        }
        System.out.println(""no duplicity found"");
    }

    private static LinkedList words;

    static {
        words = new LinkedList();

        words.add(""word"");
        words.add(""sword"");
        words.add(""dwarf"");
        words.add(""whale"");
        words.add(""male"");
    }

    private static Directory create_index() throws IOException
    {
        Directory directory = new RAMDirectory();

        ListIterator e_words1 = words.listIterator();
        ListIterator e_words2 = words.listIterator(words.size());

        IndexWriter writer = new IndexWriter(directory, new StandardAnalyzer(), 
true);

        int co = 1;

        for (int i = 0; i < 300; i++) {

            if (!e_words1.hasNext()) {
                e_words1 = words.listIterator();
                e_words1.hasNext();
            }
            String word1 = (String)e_words1.next();
            if (!e_words2.hasPrevious()) {
                e_words2 = words.listIterator(words.size());
                e_words2.hasPrevious();
            }
            String word2 = (String)e_words2.previous();

            Document doc = new Document();

            doc.add(Field.Keyword(""co"", String.valueOf(co)));
            doc.add(Field.Text(""text"", word1 + "" "" + word2));
            writer.addDocument(doc);

            if (i % 20 == 0)
                co++;
        }
        writer.optimize();
        System.err.println(""index size: "" + writer.docCount());
        writer.close();

        return directory;
    }
}"
1,"MultiPhraseQuery throws AIOOBESee thread ""MultiPhraseQuery throws ArrayIndexOutOfBounds Exception"" on dev@ by Jayendra Patil."
1,"FieldInfo omitTerms bugAround line 95 you have:

    if (this.omitTf != omitTf) {
      this.omitTf = true;                // if one require omitTf at least once, it remains off for life
    }

Both references of the omitTf booleans in the if statement refer to the same field. I am guessing its meant to be other.omitTf like the norms code above it."
1,"When creating multiple repository instances pointing to the same home, opening a second session will remove the .lock fileThe following test case can be used to reproduce the bug:
Repository repo1 = new TransientRepository(repoConfig);
Session session1_1 = repo1.login(...);
Session session2_2 = repo1.login(...);
Repository repo2 = new TransientRepository(repoConfig); // Will not fail (expected)
Session session2_1 = repo2.login(...); // Will fail with javax.jcr.RepositoryException: The repository home /tmp/_repository appears to be already locked by the current process (expected)
Session session2_2 = repo2.login(...); // Will work!
Repository repo3 = new TransientRepository(repoConfig); // Will not fail either (expected)
Session session3_1 = repo3.login(...); // Will fail with javax.jcr.RepositoryException: The repository home /tmp/_repository appears to be already locked by the current process (expected)
Session session3_2 = repo3.login(...); // Will fail with javax.jcr.RepositoryException: Directory was previously created with a different LockFactory instance

Open the first session in repo2 will fails but will also remove the .lock file, thus the second
session will succeed and may corrupt the repository because there are multiple session
opened from multiple repository.
The same behaviour occurs for repo3, the .lock file is removed but it is a slightly different case
as a new exception will be thrown while creating the Lucene index.

This is a clearly a twisted case as repositories pointing to the same home must not be created
simultaneously but i think that it must be more robust to prevent data corruption.

I reproduce the bug on JR 1.4.7 and 1.5.3 but i think it affects at least all versions of JR < 1.5.3."
1,"Bad check for sv:name attribute presence in system view importsax content handler checks the wrong variable to see if it's null in SysViewImportHandler

name vs. svName

patch fixes this"
1,"Request with two forward slashes for path failsThe following code demonstrates the problem:
        DefaultHttpClient client = new DefaultHttpClient();
        client.execute(new HttpGet(""http://www.google.com//""));

When a request is made, the DefaultRequestDirector invokes rewriteRequestURI(). I don't fully understand why this method does what it does. For a non-proxied request, it attempts to render the URI to a relative URI. In doing so, it tries to create a relative URI whose content is ""//"". Per RFC 2396 section 5 (Relative URI References), a relative URI that begins with ""//"" is a network-path reference, and the ""//"" must be immediately followed by an authority. Therefore, while ""http://www.google.com//"" is a valid absolute URI, ""//"" is not a valid relative one. The resulting exception:

[...]
Caused by: org.apache.http.ProtocolException: Invalid URI: http://www.google.com//
	at org.apache.http.impl.client.DefaultRequestDirector.rewriteRequestURI(DefaultRequestDirector.java:339)
	at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:434)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:641)
	... 31 more
Caused by: java.net.URISyntaxException: Expected authority at index 2: //
	at java.net.URI$Parser.fail(URI.java:2809)
	at java.net.URI$Parser.failExpecting(URI.java:2815)
	at java.net.URI$Parser.parseHierarchical(URI.java:3063)
	at java.net.URI$Parser.parse(URI.java:3024)
	at java.net.URI.<init>(URI.java:578)
	at org.apache.http.client.utils.URIUtils.createURI(URIUtils.java:106)
	at org.apache.http.client.utils.URIUtils.rewriteURI(URIUtils.java:141)
	at org.apache.http.client.utils.URIUtils.rewriteURI(URIUtils.java:159)
	at org.apache.http.impl.client.DefaultRequestDirector.rewriteRequestURI(DefaultRequestDirector.java:333)
	... 33 more
"
1,"Sloppy Phrase Scorer matches the doc ""A B C D E"" for query = ""B C B""~2This is an extension of https://issues.apache.org/jira/browse/LUCENE-697

In addition to abnormalities Yonik pointed out in 697, there seem to be other issues with slopy phrase search and scoring.

1) A phrase with a repeated word would be detected in a document although it is not there.
I.e. document = A B D C E , query = ""B C B"" would not find this document (as expected), but query ""B C B""~2 would find it. 
I think that no matter how large the slop is, this document should not be a match.

2) A document containing both orders of a query, symmetrically, would score differently for the queru and for its reveresed form.
I.e. document = A B C B A would score differently for queries ""B C""~2 and ""C B""~2, although it is symmetric to both.

I will attach test cases that show both these problems and the one reported by Yonik in 697. "
1,"I/O exception in DocsWriter add or updateDocument may not delete unreferenced filesIf an I/O exception is thrown in DocumentsWriter#addDocument or #updateDocument, the stored fields files may not be cleaned up."
1,"Explicit VirtualHosts Can Cause Issues On RedirectsIf you set an explicit virtual host then a getmethod may get stuck in a redirect
loop (up to maxRedirects).

e.g. execute a get on www.google.com (with a www.google.com virtualhost).  That
redirects to www.google.co.nz (at least if you come from an NZ IP).  The current
httpclient behavior is to then connect to www.google.co.nz but pass through,
with the request, ""Host: www.google.com"".  Google will then reply with another
www.google.co.nz redirect and the loop continues.

There are probably a few ways to work around this.  It seems reasonable to drop
an explicity set virtual host in the event a redirect redirects to a different
uri authority.  The following patch works for me:


diff -Naur
../../t2/commons-httpclient/src/java/org/apache/commons/httpclient/HttpMethodDirector.java
src/java/org/apache/commons/httpclient/HttpMethodDirector.java
---
../../t2/commons-httpclient/src/java/org/apache/commons/httpclient/HttpMethodDirector.java
2005-12-22 01:06:55.000000000 +1300
+++ src/java/org/apache/commons/httpclient/HttpMethodDirector.java	2005-12-22
19:09:51.000000000 +1300
@@ -605,6 +605,27 @@
 					redirectUri = new URI(currentUri, redirectUri);
 				}
 			}
+            do {
+                // scenario we're trying to avoid:
+                // virtual host is set (e.g. google.com); a request to that
server responds
+                // with a redirect to google.co.nz; we issue a request to
google.co.nz with 
+                // a virtual host request of google.com
+                // 
+                // This code will remove any set virtual host if the redirect
is to a different 
+                // domain
+
+                if(redirectUri.isRelativeURI()) {
+                    break;
+                }
+
+                String vhost = hostConfiguration.getParams().getVirtualHost();
+                if(vhost==null)
+                    break;
+                if(redirectUri.getAuthority()!=currentUri.getAuthority()) {
+                    hostConfiguration.getParams().setVirtualHost(null);
+                }
+            } while(false);
+"
1,"OCM: translate-project goal not foundThe jackrabbit-ocm POM doesn't specify the required version of the Retrotranslator plugin it uses. In some cases this causes the build to use an older version of the plugin that doesn't come with the translate-plugin goal.

The goal is included in the latest version (1.0-alpha-4) of the plugin."
1,"on disk full during close, FSIndexOutput fails to close descriptorThe close method just does this:

{code}
      if (isOpen) {
        super.close();
        file.close();
        isOpen = false;
      }
{code}

But super.close = BufferedIndexOutput.close, which tries to flush its buffer.  If disk is full (or something else is wrong) then we hit an exception and don't actually close the descriptor.

I will put a try/finally in so we always close, taking care to preserve the original exception. I'll commit shortly & backport to 2.3.2"
1,"HttpMultipart doesn't generate Content-Type part header in mode BROWSER_COMPATIBLEBrowsers (tested with Firefox 3.6 and IE6) send a Content-Type header for file parts, what org.apache.http.entity.mime.HttpMultipart doesn't do in BROWSER_COMPATIBLE mode.


Example:

-----------------------------142889018617181602061216500409

Content-Disposition: form-data; name=""myFileFieldName2""; filename=""webtest.png""

Content-Type: image/png


In HtmlUnit we wil subclass HttpEntity and MultipartEntity to fix this problem."
1,"Highlighter has problems when you use StandardAnalyzer with LUCENE_29 or simplier StopFilter with stopWordsPosIncr mode switched onThis is a followup on LUCENE-1987:

If you set in HighligterTest the constant static final Version TEST_VERSION = Version.LUCENE_24 to LUCENE_29 or LUCENE_CURRENT, the test testSimpleQueryScorerPhraseHighlighting fails. Please note, that currently (before LUCENE-2002 is fixed), you must also set the QueryParser to respect posIncr."
1,"SegmentInfo.sizeInBytes ignore includeDocStore when cachingI noticed that SegmentInfo's sizeInBytes cache is potentially buggy -- it doesn't take into account 'includeDocStores'. I.e., if you call it once w/ 'false' (sizeInBytes won't include the store files) and then with 'true' (or vice versa), you won't get the right sizeInBytes (it won't re-compute, with the store files).

I'll fix and add a test case demonstrating the bug."
1,"Term vectors missing after addIndexes + optimizeI encountered a problem with addIndexes where term vectors disappeared following optimize(). I wrote a simple test case which demonstrates the problem. The bug appears with both addIndexes() versions, but does not appear if addDocument is called twice, committing changes in between.

I think I tracked the problem down to IndexWriter.mergeMiddle() -- it sets term vectors before merger.merge() was called. In the addDocs case, merger.fieldInfos is already populated, while in the addIndexes case it is empty, hence fieldInfos.hasVectors returns false.

will post a patch shortly."
1,"Mixin removal exceptionWhen trying to remove a mixin from a non nt:unstructured node (in my case nt:resource), you get the following exception:

Unable to alter mixin types: javax.jcr.nodetype.ConstraintViolationException: no matching property definition found for {http://www.day.com/jcr/cq/1.0}lastRolledout

lastRolledout property is defined by the mixin cq:LiveRelationship that I am trying to remove."
1,"PROPFIND response to a request for a property that does not exist reports an empty DAV:prop element  A PROPFIND response to a request for a property that does not exist reports an empty DAV:prop element:

Request:

<propfind xmlns=""DAV:""><prop><doesnotexist/></prop></propfind>

Response:

<D:multistatus xmlns:xml='http://www.w3.org/XML/1998/namespace' xmlns:D='DAV:'>
  <D:response>
    <D:href>...</D:href>
    <D:propstat>
      <D:prop/>
      <D:status>HTTP/1.1 200 OK</D:status>
    </D:propstat>
    <D:propstat>
      <D:prop><D:doesnotexist/></D:prop>
      <D:status>HTTP/1.1 404 Not Found</D:status>
    </D:propstat>
  </D:response>
</D:multistatus>

  "
1,"FilterImpl.getStringValue() does not use custom converter class specified in @Field annotationI have a POJO with the following field:

    @Field(converter = LocaleConverter.class)
    private Locale                locale;

When I attempt to query for objects based on this field, I get a NullPointerException:

java.lang.NullPointerException
        at org.apache.jackrabbit.ocm.query.impl.FilterImpl.getStringValue(FilterImpl.java:281)
        at org.apache.jackrabbit.ocm.query.impl.FilterImpl.addEqualTo(FilterImpl.java:129)

FilterImpl should preferentially use the atomic type converter defined in the @Field annotation to convert the value, then fallback to the global converters.  Converting the Locale to a string for use in the query is a workaround, but the logic for string conversion should only reside in my LocaleConverter class.
"
1,"BundleDBPersistenceManager does not free blobStore resourcesWhen removing binary property from node or removing node containing binary property, resources occupied by binary property are not freed (orphaned records remains in associated ${schemaObjectPrefix}BINVAL table)."
1,"WorkspaceConfig.init() throws NullPointerException if Search configuration is missingWhen the search configuration is missing from the repository.xml configuration, the WorkspaceConfig.sc field is null and consequently the WorkspaceConfig.init() method throws a NullPointerException.

This is by itself a bug, especially since missing search configuration is perfectly ok resulting in Jackrabbit not building the search index (which is - believe it or - what really want).

On that matter, since the configuration file structure seems to be implied by the configuration framework but the DTD is inlined into the configuration file, the configuration framework should act very gracefully to missing or wrong or unexpected configuration elements. Thus, for example, if the file system configuration (WorkspaceConfig.fsc) would be missing, the WorkspaceConfig should probably hint at this point and not throw a NullPointerException without further explanations (of course throwing anything at all is still better than going wild)."
1,"IllegalNameException when importing document view with two mixinsAs reported by Manuel Simoni on the dev mailing list:

----

I have nodes with two mixin types, s1NT:comment and s1NT:authored.

I am exporting the document view:

session.exportDocumentView(session.getRootNode().getPath(),
outputStream, false, false);

When I try to import the document again:

session.getWorkspace().importXML(session.getRootNode().getPath(),
inputStream, ImportUUIDBehavior.IMPORT_UUID_COLLISION_THROW);

...I get this exception:

java.lang.Exception: javax.jcr.InvalidSerializedDataException: failed to
parse XML stream: illegal jcr:mixinTypes value: s1NT:comment
s1NT:authored: illegal jcr:mixinTypes value: s1NT:comment s1NT:authored
       at
at.systemone.wiki.webservice.ImportControllerImpl.importDocumentView(ImportControllerImpl.java:30)
       at
at.systemone.wiki.webservice.ImportControllerTest.testImport(ImportControllerTest.java:12)
       at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
       at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
       at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
       at java.lang.reflect.Method.invoke(Method.java:585)
       at junit.framework.TestCase.runTest(TestCase.java:154)
       at junit.framework.TestCase.runBare(TestCase.java:127)
       at junit.framework.TestResult$1.protect(TestResult.java:106)
       at junit.framework.TestResult.runProtected(TestResult.java:124)
       at junit.framework.TestResult.run(TestResult.java:109)
       at junit.framework.TestCase.run(TestCase.java:118)
       at junit.framework.TestSuite.runTest(TestSuite.java:208)
       at junit.framework.TestSuite.run(TestSuite.java:203)
       at
org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:478)
       at
org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:344)
       at
org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)
Caused by: javax.jcr.InvalidSerializedDataException: failed to parse XML
stream: illegal jcr:mixinTypes value: s1NT:comment s1NT:authored:
illegal jcr:mixinTypes value: s1NT:comment s1NT:authored
       at
org.apache.jackrabbit.core.WorkspaceImpl.importXML(WorkspaceImpl.java:718)
       at
at.systemone.wiki.webservice.ImportControllerImpl.importDocumentView(ImportControllerImpl.java:27)
       ... 16 more
Caused by: org.apache.jackrabbit.name.IllegalNameException:
's1NT:comment s1NT:authored' is not a valid name
       at
org.apache.jackrabbit.core.xml.DocViewImportHandler.startElement(DocViewImportHandler.java:217)
       at
org.apache.jackrabbit.core.xml.ImportHandler.startElement(ImportHandler.java:234)
       at org.apache.xerces.parsers.AbstractSAXParser.startElement(Unknown Source)
       at
org.apache.xerces.impl.XMLNSDocumentScannerImpl.scanStartElement(Unknown
Source)
       at
org.apache.xerces.impl.XMLDocumentFragmentScannerImpl$FragmentContentDispatcher.dispatch(Unknown
Source)
       at
org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanDocument(Unknown
Source)
       at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
       at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
       at org.apache.xerces.parsers.XMLParser.parse(Unknown Source)
       at org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source)
       at
org.apache.jackrabbit.core.WorkspaceImpl.importXML(WorkspaceImpl.java:709)
       ... 17 more

I think the importer chokes on this node (it is the first node with
these two mixin types in the XML file):

<s1:comment jcr:primaryType=""s1NT:page"" jcr:mixinTypes=""s1NT:comment
s1NT:authored"" jcr:uuid=""3ff1022b-3e21-4e44-9a2e-ae3b67e833e5""
jcr:isCheckedOut=""true""
jcr:versionHistory=""17186f20-dab2-42d8-8f66-0895472debea""
jcr:frozenMixinTypes=""s1NT:comment s1NT:authored""
jcr:frozenUuid=""3ff1022b-3e21-4e44-9a2e-ae3b67e833e5""
s1:author=""8db75ec7-eee8-44d8-aeb2-fbd116ea7e01"" s1:title=""""
jcr:predecessors=""fb9eefb2-e2f8-414c-be94-185111a89be9""
s1:creationDate=""2005-09-07T17:59:12.589Z""
s1:editor=""7b778c51-d8d8-474b-a621-f5759fc24cd0"" s1:orphanedPage=""false""
s1:modificationDate=""2006-03-18T17:55:49.838Z"" s1:lowercaseTitle=""""
jcr:baseVersion=""fb9eefb2-e2f8-414c-be94-185111a89be9""
s1:currentEditor=""8db75ec7-eee8-44d8-aeb2-fbd116ea7e01""
jcr:frozenPrimaryType=""s1NT:page"">

----

This issue is related to JCR-325, but there should be a lot easier fix for this special case."
1,"Fix unexpected behavior of Text.getName()Text.getName() and variants does return an empty string, if the given path is already a name. eg:

Text.getName(""foo"") returns """" and not ""foo"" as one would expect for relative paths.
suggest to change this."
1,"cache module generates exceptions for non-compliant responses without consuming response bodiesIn the ResponseProtocolCompliance class, the caching module checks the incoming origin response to attempt to make it compliant with RFC2616. However, if there are instances where this is not possible, it currently throws an exception without consuming the origin response body; this causes a connection leak if the general try..catch..finally pattern documented on the HttpClient interface Javadoc is followed.
"
1,"[PATCH] Wirelog correctionsThis patch 

* fixes the problem reported by Geir H. Pettersen <geir at cellus.no>. See
http://marc.theaimsgroup.com/?t=108072355300004&r=1&w=2 for details

* increases the priority of HTTP request/status line & HTTP headers output from
DEBUG to INFO. Quite often request/response content generate excessive amount of
output in the wirelog and produce no valuable debug information of what so ever.
By setting wirelog verbosity to INFO one can turn off the logging of 
request/response content.

I believe the patch should be applied to both CVS HEAD and 2.0 branch. Please
let me know if you agree

Oleg"
1,"""Index already present"" exception when opening a restored repositoryI have created a new repository, added one node, then copied all files while Jackrabbit is running.
Then closed the repository, restored the backup, and tried to open the repository.
Unfortunately, this resulted in the following exception:

javax.jcr.RepositoryException: Index already present: Index already present: Index already present
	at org.apache.jackrabbit.core.SearchManager.initializeQueryHandler(SearchManager.java:575)
	at org.apache.jackrabbit.core.SearchManager.<init>(SearchManager.java:255)
	at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.getSearchManager(RepositoryImpl.java:1613)
	at org.apache.jackrabbit.core.RepositoryImpl.initWorkspace(RepositoryImpl.java:606)

The backup contains the following index file:
workspaces/default/index/redo.log
There are no other files or directories or files in that directory (also no _n directories). The content of redo.log is:

-1 STR
-1 ADD cafebabe-cafe-babe-cafe-babecafebabe
-1 COM
0 STR
0 DEL cafebabe-cafe-babe-cafe-babecafebabe
0 ADD fa87759b-f9fe-4ba8-986c-d1914ffce3de
0 ADD eda04b36-9c21-4712-bedf-206c36f0e3d2
0 ADD ae917cca-a0bb-4ac0-a16d-805aac6c7b10
0 ADD cafebabe-cafe-babe-cafe-babecafebabe
0 COM
1 STR
1 ADD 0121f271-bbe7-4f71-a793-5f4380f3c487
1 COM
"
1,"empty path not handled correctlyWhen requesting for a URL which has an empty path, e.g. http://abcnews.go.com ,
the code sends the following line:

GET  HTTP/1.1

which should be

GET / HTTP/1.1

instead"
1,"Range Query works only with lower case termsI am performing a range query that returns results if the terms are lower 
case, but does not return result when the terms are mixed case.

In my collection, I have terms alpha, beta, delta, gamma.  I am using the 
StandardAnalyzer for both indexing and searching.

The query [alpha TO gamma] returns all four terms.  When I perform the query 
[Alpha TO Gamma], no results are returned.

It appears the lowerCaseFilter(), which is a part of the StandardAnalyzer, 
does not work properly on the search terms.  I've used Luke to peek at my 
collection, and the terms are all lower case in the collection.

I'm fairly new to Lucene, so I hope I'm not making a ""common mistake""."
1,"System view XML uses hardcoded sv: prefixJackrabbit enforces that the xml docment that imported into repository through the use of  ContentHandler have attributte ""name"" with specific prefix (""sv""), instead of specific namespace (""com.cisco.topos.jcr.sv"").

Example of wrong behavior:
Calling
marshaller.marshal(entry, session.getImportContentHandler(session.getNodeByUUID(channelId).getPath(), ImportUUIDBehavior.IMPORT_UUID_CREATE_NEW));

where entry is object that represent xml structure with namespace ""com.cisco.topos.jcr.sv"" assigned to prefix other then ""sv"" or as default namespace  will cause  exception
java.lang.RuntimeException: javax.xml.bind.MarshalException

 javax.jcr.InvalidSerializedDataException: missing mandatory sv:name attribute of element sv:node
	at org.apache.jackrabbit.core.xml.SysViewImportHandler.startElement(SysViewImportHandler.java:122)
	at org.apache.jackrabbit.core.xml.ImportHandler.startElement(ImportHandler.java:192)
	at com.sun.xml.bind.v2.runtime.output.SAXOutput.endStartTag(SAXOutput.java:80)
	at com.sun.xml.bind.v2.runtime.XMLSerializer.endAttributes(XMLSerializer.java:273)
	at com.sun.xml.bind.v2.runtime.XMLSerializer.childAsSoleContent(XMLSerializer.java:531)
	at com.sun.xml.bind.v2.runtime.ClassBeanInfoImpl.serializeRoot(ClassBeanInfoImpl.java:283)
	at com.sun.xml.bind.v2.runtime.XMLSerializer.childAsRoot(XMLSerializer.java:461)
	at com.sun.xml.bind.v2.runtime.MarshallerImpl.write(MarshallerImpl.java:292)
	... 24 more


"
1,"workspace.copy does not copy binary properties properlyWorkspace copy works fine for everything else but if you copy a hierarchy which contains binary properties
and remove the source after copying it removes the binary from ""copied"" newly created hierarchy as well.

How to reproduce:

1. create hierarchy with any type of nodes  - /site / en / image [Type Binary]
2. create another hierarchy at different level - /site2
3. copy /site/en under /site2
-- till now everything is fine, its a proper copy and if you export xml out of these 2 hierarchies you will see that ""image"" is actually copied
4. now delete /site/en
5. you will see all other properties as copied before /site2/en... except binary.

are binary types are always referenced? even if you copy via workspace copy
"
1,"typo in the mimetypes.properties fileThe Powerpoint mime-type (ppt) is wrong in mimetypes.properties file. It was written with a equals(=) caracter in place of a dot (.).
"
1,"thread starving in MultiThreadedHttpConnectionManagerHi folks,

I might have found a bug in MTHCM. It has to do with removing HostConnectionPool instances that have no more connections in them. That was a fix for a memory leak we previously had. There are two cases where the pools get deleted. One is in handleLostConnection: (excerpt)
  ...
  if (hostPool.numConnections == 0) mapHosts.remove(config);
  notifyWaitingThread(config);
  ...

Could this delete a pool in which there is still a thread waiting to get a connection? If so, the thread would remain in the global pool. But even if it is interrupted there, it would still use the old HostConnectionPool in which no connection will ever become available again.

I suggest to change the removal check in both cases to:
  if ((hostPool.numConnections < 1) && hostPool.waitingThreads.isEmpty)

What do you think?"
1,deadlock on concurrent commit/lockingthere can happen an iterlock between the dispatching part of the shared item state manager and the lockmanager.
1,"problem with isIPv4address() for relative uri'sthe following block of code:

try {
	URI uri = new URI(""http://10.0.1.10:8830"");
	System.out.println(""is IP=""+uri. isIPv4address());
	uri = new URI(uri, ""/04-1.html"");
	System.out.println(""is IP=""+uri. isIPv4address());
} catch (URIException e) { ; }

returns the output:

is IP=true
is IP=false

so by being created from a relative uri URI objects don't have the right setting of  isIPv4address()."
1,"Node.restore() fails for existing non-versioned OPV=Version child nodesI have a node whose definition has properties and child nodes.  The
definitions of the nodetypes for the node and the child include
mix:versionable.  The properties definitions have onParentVersion=COPY and
the child nodes have onParentVersion=VERSION.  When I create a node with
child nodes and checkin and then restore the node, I get a
""....VersionException: Restore of root node not allowed""  This is
occurring on the restore of the child node.

According to the spec:

Child Node
On checkin of N, the node VN will get a subnode of type nt:versionedChild
with the same name as C. The single property of this node,
jcr:childVersionHistory is a REFERENCE to the version history of C (not to
C or any actual version of C). This also requires that C itself be
versionable (otherwise it would not have a version history).
.
.
.
On restore of VN, if the workspace currently has an already existing node
corresponding to C?s version history and the removeExisting flag of the
restore is set to true, then that instance of C becomes the child of the
restored N. If the workspace currently has an already existing node
corresponding to C?s version history and the removeExisting flag of the
restore is set to false then an ItemExistsException is thrown.


I'm restoring the node using

   node.restore(version, true);

Is this expected behavior?"
1,"DateValue.getDate not a copyI noticed that getDate() in org.apache.jackrabbit.value.DateValue is returned 
by reference. According to the specification it should be a copy. (see.  JSR 170 section 6.2.7)

 
 private Calendar date;
 
 public Calendar getDate()
             throws ValueFormatException, IllegalStateException,
             RepositoryException {
         setValueConsumed();
 
         if (date != null) {
             return date; // <-- HERE
         } else {
             throw new ValueFormatException(""empty value"");
         }
     }

short test:

ValueFactory factory = session.getValueFactory();
 Value v = factory.createValue(GregorianCalendar.getInstance());
 Calendar c0 = v.getDate();   
 Calendar c1 = v.getDate();
               
 if(c0 == c1){
                   out.println(""error - references are equal"");
                    out.println(c0);
 }"
1,"Deadlock when concurrently committing and reading versioning statesthere is a rear occation when one thread commits a transaction and another thread reads versioing related information, so that a deadlock can occurr. 

example:

Thread1:
                        ut.begin();
                        session.getWorkspace().clone(""default"", ""/content"", ""/content"", true);
                        ut.commit();

Thread2:
                        VersionHistory vh = folder.getVersionHistory();
                        VersionIterator iter = vh.getAllVersions();
                        while (iter.hasNext()) {
                            Version v = iter.nextVersion();
                        }


to fix this issue we must ensure, that methods below the shareditemstatemgr do not call higher instances (like itemmgr) again."
1,"Index segments are only committed on closeThere is a check in AbstractIndex.commit(), which prevents that deleted documents are committed to the index. Up to lucene version 2.0 the index was locked when there were pending changes. Beginning with lucene 2.1 this is not true anymore. See LUCENE-701.

This is a regression of JCR-788, hence it does not occur in a release but only in trunk."
1,"processing a synonym in a token stream will remove the following token from the streamIf you do a phrase search on a field derived from a fieldtype with the synonym filter which includes a synonym, the term following the synonym vanishes after synonym expansion.

e.g. http://host:port/solr/corename/select/?q=desc:%22xyzzy%20%20bbb%20pot%20of%20gold%22&version=2.2&start=0&rows=10&indent=on&debugQuery=true   (bbb is in the default synonyms file, desc is a ""text"" fieldtype)

outputs
....
<str name=""rawquerystring"">desc:""xyzzy  bbb pot of gold""</str>
<str name=""querystring"">desc:""xyzzy  bbb pot of gold""</str>
<str name=""parsedquery"">PhraseQuery(desc:""xyzzy bbbb 1 bbbb 2 of gold"")</str>
<str name=""parsedquery_toString"">desc:""xyzzy bbbb 1 bbbb 2 of gold""</str>
....

You can also see this behavior using the admin console analysis.jsp

Solr 3.3 behaves properly.
"
1,Some code still compares string equality instead using equalsI found a couple of places where we still use string == otherstring which don't look correct. I will attache a patch soon.
1,"SegmentInfo should explicitly track whether that segment wrote term vectorsToday SegmentInfo doesn't know if it has vectors, which means its files() method must check if the files exist.

This leads to subtle bugs, because Si.files() caches the files but then we fail to invalidate that later when the term vectors files are created.

It also leads to sloppy code, eg TermVectorsReader ""gracefully"" handles being opened when the files do not exist.  I don't like that; it should only be opened if they exist.

This also fixes these intermittent failures we've been seeing:

{noformat}
junit.framework.AssertionFailedError: IndexFileDeleter doesn't know about file _1e.tvx
       at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:979)
       at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:917)
       at org.apache.lucene.index.IndexWriter.filesExist(IndexWriter.java:3633)
       at org.apache.lucene.index.IndexWriter.startCommit(IndexWriter.java:3699)
       at org.apache.lucene.index.IndexWriter.prepareCommit(IndexWriter.java:2407)
       at org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:2478)
       at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2460)
       at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2444)
       at org.apache.lucene.index.TestIndexWriterExceptions.testRandomExceptionsThreads(TestIndexWriterExceptions.java:213)
{noformat}"
1,"SharedItemStateManager not properly synchronizedSome time ago we removed synchronized modifiers from the methods store() hasItemState() and getItemState(). While some care has been taken to ensure the cache integrity, I think the contract for the SharedItemStateManager (SISM) is now broken. The JavaDoc does not clearly document this, but I think all relevant methods of the SISM working on ItemStates should be atomic.

E.g. a call to hasItemState() should not return true for an ItemState that another thread is currently adding in store(). Similarly a getItemState() should not return an ItemState that is currenly added or modified in a store() operation.

Currently I see two options:
- Change the methods to synchronized again. This will actually serialize all calls to the SISM.
- Implement a more sophisticated synchronisation. E.g. multiple store operations can still be allowed, as long as their ChangeLogs do not intersect. Retrieving ItemStates might still be allowed while a ChangeLog is stored, as long as the ItemState to retrieve is not part of the ChangeLog.

Comments and suggestions are very welcome."
1,"Inconsistency when version with a label is removedWhile executing random operations on the version storage I came across a situation where a version is removed that has a version label.

The current behaviour in jackrabbit is IMO not correct because the version node gets removed, but the version label is still present in the version history. This means there is a referenceable property that points to nowhere.

So I guess either:

- the version label property must be removed as well when the version is removed
or
- the remove version operation should fail because the version is still referenced

I wasn't able to find a relevant section in the spec, though I must admit that I don't know the versioning section that well."
1,"AbstractClientConnAdapter doesn't ensure that only one of ConnectionReleaseTrigger.abortConnection, .releaseConnection has effectIf HttpUriRequest.abort() is called at about the same time that the request completes, it's possible for an aborted connection to be returned to the pool.  The next time the connection is used, HttpClient.execute fails without retrying, throwing this exception:

java.io.IOException: Connection already shutdown
	at org.apache.http.impl.conn.DefaultClientConnection.opening(DefaultClientConnection.java:112)
	at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:120)
	at org.apache.http.impl.conn.AbstractPoolEntry.open(AbstractPoolEntry.java:147)
	at org.apache.http.impl.conn.AbstractPooledConnAdapter.open(AbstractPooledConnAdapter.java:101)
	at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:381)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:641)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:576)

Steps to reproduce:
1) Set a breakpoint in ThreadSafeClientConnManager.releaseConnection just after ""reusable"" is set (and found to be true).
2) Run to the breakpoint in releaseConnection.
3) Call HttpUriRequest.abort.
4) Let releaseConnection complete.

When the connection is next used, the exception will be thrown.

Snippet from ThreadSafeClientConnManager:
    public void releaseConnection(ManagedClientConnection conn, long validDuration, TimeUnit timeUnit) {
		...
            boolean reusable = hca.isMarkedReusable();
            if (log.isDebugEnabled()) {                             // breakpoint here
                if (reusable) {
                    log.debug(""Released connection is reusable."");
                } else {
                    log.debug(""Released connection is not reusable."");
                }
            }
            hca.detach();
            if (entry != null) {
                connectionPool.freeEntry(entry, reusable, validDuration, timeUnit);
            }
        }
    }


I think that AbstractClientConnAdapter should be modified as follows:

1) Add ""released"" flag:

    /** True if the connection has been released. */
    private boolean released;

2) Modify abortConnection:

    public void abortConnection() {
        synchronized(this) {
            if (aborted || released) {
                return;
            }
            aborted = true;
        }
        unmarkReusable(); // this line and all that follow unchanged

3) Modify releaseConnection:

    public void releaseConnection() {
        synchronized(this) {
            if (aborted || released) {
                return;
            }
            released = true;
        }
        if (connManager != null) {
            connManager.releaseConnection(this, duration, TimeUnit.MILLISECONDS);
        }
    }

"
1,"BytesRef copy short missed the length settingwhen storing a short type integer to BytesRef, BytesRef missed the length setting. then it will cause the storage size is ZERO if no continuous options on this BytesRef"
1,"HttpUrl does not accept unescaped passwords- Taken from an email from Gustav Munkby posted to the HttpClient dev mailing list -

If I do:

HTTPUrl url = new HTTPUrl(""kurt"", ""nicepass#"", hostname, 80, path);

throws a URIException with message ""port number invalid"".

First of all the message is wrong...

Next attempt was to urlencode the password, which resulted in the above line working, but the 
password was sent url-encoded to the destination, which can hardly be the desired behaviour?"
1,Stackoverflow when calling deprecated CharArraySet.copy()Calling CharArraySet#copy(set) without the version argument (deprecated) with an instance of CharArraySet results in a stack overflow as this method checks if the given set is a CharArraySet and then calls itself again. This was accidentially introduced due to an overloaded alternative method during LUCENE-2169 which was not used in the final patch.
1,"return value of PostMethod#removeParameter<HttpClient2.0-rc1>

About 
public boolean removeParameter(String paramName)
                 throws IllegalArgumentException
method.

-------------------------------------------------
PostMethod method = new PostMethod(uri);
method.addParameter(""name"", ""Matsui Hideki"");
boolean b;
b = method.removeParameter(""name""); // returns ""true"".
b = method.removeParameter(""XXXX""); // returns ""true"". why??? 
---------------------------------------------------

sorry for my poor english."
1,"Dirty Internal State on Transaction-Rollback during Global Transaction (container managed transaction)Running the following code inside an Global Transaction (JTA, container managed transaction) causes problems.

Session session = getRepsoitorySession(); 
      Node rootNode = session.getRootNode(); 

      Node test = rootNode.addNode(""test""); 
      test.addMixin(CTVRepositoryKonstanten.NODE_MIX_TYP_VERSION); 
      session.save(); 
      throw new RuntimeException(""testException"");

Everythink is fine, but if we execute it a second time we get an org.apache.jackrabbit.core.state.NoSuchItemStateException

org.apache.jackrabbit.core.state.NoSuchItemStateException: b36d91bc-8687-428c-a767-2e087b13191a 
at org.apache.jackrabbit.core.state.SharedItemStateManager.getItemState(SharedItemStateManager.java:270) 
at org.apache.jackrabbit.core.state.LocalItemStateManager.getNodeState(LocalItemStateManager.java:107) 
at org.apache.jackrabbit.core.state.LocalItemStateManager.getItemState(LocalItemStateManager.java:172) 
at org.apache.jackrabbit.core.state.XAItemStateManager.getItemState(XAItemStateManager.java:260) 
at org.apache.jackrabbit.core.version.NodeStateEx.store(NodeStateEx.java:519) 
at org.apache.jackrabbit.core.version.NodeStateEx.store(NodeStateEx.java:489) 
at org.apache.jackrabbit.core.version.AbstractVersionManager.getParentNode(AbstractVersionManager.java:414) 
at org.apache.jackrabbit.core.version.AbstractVersionManager.createVersionHistory(AbstractVersionManager.java:357) 
at org.apache.jackrabbit.core.version.XAVersionManager.createVersionHistory(XAVersionManager.java:148) 
at org.apache.jackrabbit.core.version.AbstractVersionManager.getVersionHistory(AbstractVersionManager.java:273) 
at org.apache.jackrabbit.core.ItemImpl.initVersionHistories(ItemImpl.java:738) 
at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1097) 
at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:915) 
at org.apache.jackrabbit.jca.JCASessionHandle.save(JCASessionHandle.java:180) 
at de.continentale.repo.CTVRepository.erstelleDokument(CTVRepository.java:2267)

We think that there is some internal state that is not cleaned up on rollback.
Restarting the runtime (Application Server) ""solved"" this.

May be there are some same causes like in: JCR-2503, JCR-2613

"
1,"UserAccessControlProvider handles users who dont have Jackrabbit managed Principals or User node inconsistently.JR core 2.0.0
In UserAccessControlProvider.compilePermissions(...), if no principal relating to a user node can be found, then a set or read only compiled permissions is provided. That set gives the session read only access to the entire security workspace regardless of path.

If the user node is found, then an instance of UserAccessControlProvider.CompilePermissions is used and in UserAccessControlProvider.CompilePermissions.buildResult(...) there is a check for no user node. If there is no user node, all permissions are denied regardless of path.

Although the first case will never happen for an installation of Jackrabbit where there are no custom PrincipalManagers, I suspect, based on the impl of UserAccessControlProvider.CompilePermissions.buildResult(...) was to deny all access to the security workspace where there was no corresponding user node in a set of principals.

Since this does not effect JR unless there is an external Principal Manager its a bit hard to produce a compact unit test, the issue was found by looking at the code."
1,"ISO8601 uses default DecimalFormat constructor using locale specific digitsISO8601.java uses the default DecimalFormat constructor which uses locale specific DecimalFormatSymbols. Runnning Jackrabbit in an Indian locale the format() produces a date using DEVANAGARI numeric digits. The saved version (UTF-8) encoded is much longer than usual and is not transportable. On parsing, DecimalFormat works, but TimeZone.getTimeZone(""GMT+09:30"") (with Indian numeric digits) fails and null is returned from ISO8601. Later this traceback occurs.

2010-02-22 15:14:04,059[http-0.0.0.0-8080-16] ERROR org.apache.jackrabbit.core.persistence.bundle.BundleFsPersistenceManager - failed to write bundle: ff629488-ebb9-4300-a63b-341553cc1140
java.lang.IllegalArgumentException: argument can not be null
	at org.apache.jackrabbit.util.ISO8601.format(ISO8601.java:217)
	at org.apache.jackrabbit.core.value.InternalValue.toString(InternalValue.java:531)
	at org.apache.jackrabbit.core.persistence.bundle.util.BundleBinding.writeState(BundleBinding.java:689)
	at org.apache.jackrabbit.core.persistence.bundle.util.BundleBinding.writeBundle(BundleBinding.java:273)
	at org.apache.jackrabbit.core.persistence.bundle.BundleFsPersistenceManager.storeBundle(BundleFsPersistenceManager.java:664)
	at org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.putBundle(AbstractBundlePersistenceManager.java:703)
	at org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.store(AbstractBundlePersistenceManager.java:643)


ISO8601 probably meant the chars to be ASCII, and so the constructor with a fixed locale is more appropriate (and this doesn't encounter the TimeZone issue either).

    private static final DecimalFormat XX_FORMAT = new DecimalFormat(""00"", new DecimalFormatSymbols(Locale.US));
    private static final DecimalFormat XXX_FORMAT = new DecimalFormat(""000"", new DecimalFormatSymbols(Locale.US));
    private static final DecimalFormat XXXX_FORMAT = new DecimalFormat(""0000"", new DecimalFormatSymbols(Locale.US));
 "
1,"Save fails after setting a binary property twiceSetting a binary property twice discards the blob value of the first property state but does not remove the change from the changelog, resulting in an error on save:

javax.jcr.RepositoryException: this BLOBFileValue has been disposed
	at org.apache.jackrabbit.core.value.RefCountingBLOBFileValue.copy(RefCountingBLOBFileValue.java:105)

will attach patch that adds the respective test to the jcr2spi tests."
1,"Restore of empty multivalue property always changes property type to StringWhen you do a restore of  empty multivalue property (OPV=COPY), restored property always has the String type (no matter of property type in frozen state). The solution is to set the property type from frozen state instead of retriving it from 'first' value. If mulitvalue does not have any values the type is set to UNDEFINED and finally changed to STRING in restore method.

Attached patch with test case."
1,"Some equals methods do not check for null argumentThe equals methods in the following classes do not check for a null argument and thus would incorrectly fail with a null pointer exception if passed null:

- org.apache.lucene.index.SegmentInfo
- org.apache.lucene.search.function.CustomScoreQuery
- org.apache.lucene.search.function.OrdFieldSource
- org.apache.lucene.search.function.ReverseOrdFieldSource
- org.apache.lucene.search.function.ValueSourceQuery

If a null parameter is passed to equals() then false should be returned."
1,"In DatabasePersistenceManager.store(), if the exception is null or its cause is not an SQLException, then the PM keeps looping foreverIn the line
if (ise != null && ise.getCause() instanceof SQLException && --trials > 0) {
if one of the first two checks fails, the shortcircuit doesn't decrement trials."
1,"Memory leak in MultiThreadedHttpClient caused by bad .equals()Note: I have '2.0 release candidate 1'; I'm not sure which version this
translates into.  The bug is definitely present in the current source.

MultiThreadedHttpClient uses the following code:

// Look for a list of connections for the given config
HostConnectionPool listConnections = (HostConnectionPool) 
    mapHosts.get(hostConfiguration);
if (listConnections == null) { 
    // First time for this config
    listConnections = new HostConnectionPool();
    listConnections.hostConfiguration = hostConfiguration;
    mapHosts.put(hostConfiguration, listConnections);
}


The hash map relys on HostConfiguration's .equals() to resolve equality &
determine if there is a mapping for the configuration.

HostConfiguration has the following in it's .equals() method:

if (!protocol.equals(config.getProtocol())) {
    return false;
}

. . . and Protocol has:

if (obj instanceof Protocol) {
            
    Protocol p = (Protocol) obj;
            
    return (
        defaultPort == p.getDefaultPort()
        && scheme.equalsIgnoreCase(p.getScheme())
        && secure == p.isSecure()
        && socketFactory.equals(p.getSocketFactory()));

}

However, there is no .equals() method in any of the ProtocolSocketFactory
objects, and there isn't any note in the interface about the necessity of the
.equals() method."
1,"IndexWriter retains references to Readers used in Fields (memory leak)As described in [1] IndexWriter retains references to Reader used in Fields and that can lead to big memory leaks when using tika's ParsingReaders (as those can take 1MB per ParsingReader). 

[2] shows a screenshot of the reference chain to the Reader from the IndexWriter taken with Eclipse MAT (Memory Analysis Tool) . The chain is the following:

IndexWriter -> DocumentsWriter -> DocumentsWriterThreadState -> DocFieldProcessorPerThread  -> DocFieldProcessorPerField -> Fieldable -> Field (fieldsData) 


-------------
[1] http://markmail.org/thread/ndmcgffg2mnwjo47
[2] http://skitch.com/ecerulm/n7643/eclipse-memory-analyzer

"
1,"cache module should populate Via header to capture upstream and downstream protocolsBecause the cache module is currently implemented as a decorator that behaves like a transparent caching proxy, we need it to correctly populate the Via header so that we can preserve the record of which protocol versions were used upstream and downstream from the caching module.

This is a MUST per the RFC:
http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.45"
1,"Unreleased 2.3 version of IndexWriter.optimize()  consistly throws java.lang.IllegalArgumentException out-of-the-boxSince the upcoming 2.3 version of Lucene has support for the setRAMBufferSizeMB() method in Index Writer,  I thought I would test its performance.   So, using my application that was built upon (and worked with) Lucene 2.2,  I downloaded the nightly build 2007-10-26_03-16-46 and rebuilt my application with new code setting setRAMBufferSizeMB() from a properties file.   My test data resides in a database table of 30 columns holding 1.25 million records.   The good news is that performance is superior to Lucene 2.2.  The indexing completes in roughly 1/3 the time.   The bad news is the Index Writer.optimize() step now throws an java.lang.IllegalArgumentException.
I also run tests against various other tables.  Indexing smaller amounts of data did not throw the exception.  Indexing largers amounts of data did throw the exception.  Note, I also tested nightly builds dating back to 2007-10-05.

...
INFO:  SEIndexThread.commitCheck...
INFO:    ----Commit point reached:  1200000
INFO:  SEIndexThread.commitCheck...
INFO:    ----Commit point reached:  1225000
INFO:  SEIndexThread.commitCheck...
INFO:    ----Commit point reached:  1250000
INFO:  SEIndexThread.closeIndex()...
INFO:    ----commit point reached:  1250659
INFO:    ----optimize index
INFO: SEIndexThread():  java.lang.IllegalArgumentException

java.lang.IllegalArgumentException
        at java.lang.Thread.setPriority(Thread.java(Compiled Code))
        at org.apache.lucene.index.ConcurrentMergeScheduler.merge(ConcurrentMerg
eScheduler.java(Compiled Code))
        at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1750)
        at org.apache.lucene.index.IndexWriter.optimize(IndexWriter.java:1686)
        at org.apache.lucene.index.IndexWriter.optimize(IndexWriter.java:1652)
        at LuceneSearchEngine.optimizeIndex(LuceneSearchEngine.java:643)
        at LuceneSearchEngine.optimizeIndex(LuceneSearchEngine.java:636)
        at SEIndexThread.closeIndex(SEIndexThread.java:674)
        at SEIndexThread.processSearchObject(SEIndexThread.java:487)
        at SEIndexThread.prepareIndex(SEIndexThread.java:391)
        at SEIndexThread.run(SEIndexThread.java:41)

"
1,"Move operation may turn AC caches stalethe EntryCollector instance associated with a given workspace is listening to any modifications made to the access control content
(add, modification and removal of access control lists). however, due to the structure of the ac content (the ac node being attached to the affected nodes) 
caches may become stale if a node is moved that contains ac information somewhere in the subtree.

in order to circumvent that problem the EntryCollector should in addition collect any kind of move operations
and make sure that the caches are updated accordingly."
1,"NPE when versioning operations are concurrentInternalVersionManagerBase.getParentNode occasionally throws an NPE:

    protected static NodeStateEx getParentNode(NodeStateEx parent, String uuid, Name interNT)
            throws RepositoryException {
        NodeStateEx n = parent;
        for (int i = 0; i < 3; i++) {
            Name name = getName(uuid.substring(i * 2, i * 2 + 2));
            if (n.hasNode(name)) {
                n = n.getNode(name, 1);
                assert n != null;
            } else if (interNT != null) {
                n.addNode(name, interNT, null, false);
                n.store();
                n = n.getNode(name, 1);
                assert n != null;
            } else {
                return null;
            }
        }
        return n;
    }

Apparently getNode occasionally returns null due to race conditions.

Changing the code to what's below appears to fix it:



    protected static NodeStateEx getParentNode(NodeStateEx parent, String uuid, Name interNT)
            throws RepositoryException {
        NodeStateEx n = parent;
        for (int i = 0; i < 3; i++) {
            Name name = getName(uuid.substring(i * 2, i * 2 + 2));
            NodeStateEx n2 = n.getNode(name, 1);
            if (n2 != null) {
                n = n2;
            } else if (interNT != null) {
                n2 = n.addNode(name, interNT, null, false);
                n.store();
                n = n2;
            } else {
                return null;
            }
        }
        return n;
    }

(but likely moves the race condition somewhere else)"
1,"Protocol interceptors not called when executing CONNECT methodsWhen the DefaultRequestDirector tries to establish a route via a proxy to a https target, registered protocol interceptors aren't being called in the createTunnelToTarget method. "
1,"The PostMethod did not bring back response headers from proxy serversDescription:

When doing tunnelling through proxy servers, in case of 407 response, the 
wrapper class ConnectMethod failed to pass the response header back to the 
wrapped method (PostMethod in our case).  As result, the response headers are 
not passed back to the application.

Proposed Fix:
Change the ConnectMethod to use the wrapped method instance to get response 
headers.  It will be reinitialized again if ""CONNECT"" is successful.

Also have to modify the addProxyAuthorizationRequestHeader code to use the 
wrapped method for the authenticator to work."
1,"URI Absolutization does not follow browser behaviorThis was encountered using Heritrix to crawl a prominent website.

The URI resulting from the HttpClient URI constructor (base, relative) does not follow browser behavior:
URI newUrl = new URI(new URI(""http://www.theirwebsite.com/browse/results?type=browse&att=1""), ""?sort=0&offset=11&pageSize=10"")

Results in newUrl:
http://www.theirwebsite.com/browse/?sort=0&offset=11&pageSize=10

The desired behavior based on Firefox and IE should be:
http://www.theirwebsite.com/browse/results?sort=0&offset=11&pageSize=10

These browsers treat the question mark similar to a directory separator and do not require a file to be specified before the query.

HttpClient's current behavior does not correspond to current browser behavior and leads to an inability to crawl certain websites if HttpClient's URI class is used.

"
1,Auth state is not correctly maintained if a successful NTLM authentication results in a redirectHttpClient fails to update the auth state correctly if a successful NTLM authentication results in a redirect response. Reported by Valentin Popov <valentin.po at gmail.com>
1,"Version.merge() corrupts repositoryVersion.merge() corrupts repository. somehow the 'protected' flags of the nt:version nodetype is not properly checked.
this happens due to a wrong test case that calls merge on a Version instead of a normal Node.
"
1,"StringRequestEntity.getContentLength wrong for multibyte charsWhen setting up a PostMethod containing a StringRequestEntity with umlauts and
charset UTF-8 the content-length header is wrong. It should be the number
of bytes, but is the number of chars by now.

(e.g.
Content-Type: text/xml; charset=UTF-8
body='')

Bug-location: org.apache.commons.httpclient.methods.StringRequestEntity"
1,"XML serialization in JDK 1.4 broken (mostly for WebDAV)WebDAV uses XmlRequestEntity for serializing XML, which in turn uses org.apache.jackrabbit.commons.xml.SerializingContentHandler to work around the JDK 1.4 problem (serializing in absence of explicit namespace declarations).

The following test fails under JDK 1.4, but passed with newer JDKs:

    public void testXmlSerialization() throws ParserConfigurationException, IOException, SAXException {
        
        DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();
        dbf.setNamespaceAware(true);
        DocumentBuilder db = dbf.newDocumentBuilder();
        
        Document doc = db.newDocument();
        doc.appendChild(doc.createElementNS(""DAV:"", ""propfind""));
        
        XmlRequestEntity xmlent = new XmlRequestEntity(doc);
        ByteArrayOutputStream bos = new ByteArrayOutputStream();
        xmlent.writeRequest(bos);
        
        Document doc2 = db.parse(new ByteArrayInputStream(bos.toByteArray()));
        Element docelem = doc2.getDocumentElement();
        assertEquals(""DAV:"", docelem.getNamespaceURI());
    }"
1,"TransientRepository does not shutdown if first login failsThe TransientRepository.login() method initializes the underlying repository when it is first called (initially or after the repository has previously been shut down) bug doesn't shut down the initialized repository if the login fails. If the application then decides to exit or otherwise not start another session, then the repository remains in an initialized state with no active sessions.

This issue should be fixed by properly handling login failures in the TransientRepository.login() method."
1,bad route computed for redirected requestsBasicRouteDirector appears to miscalculate complex routes. Example to follow. 
1,"IndexCommit.getFileNames() should not return dupsIf the index was created with autoCommit false, and more than 1
segment was flushed during the IndexWriter session, then the shared
doc-store files are incorrectly duplicated in
IndexCommit.getFileNames().  This is because that method is walking
through each SegmentInfo, appending its files to a list.  Since
multiple SegmentInfo's may share the doc store files, this causes dups.

To fix this, I've added a SegmentInfos.files(...) method, and
refactored all places that were computing their files one SegmentInfo
at a time to use this new method instead.
"
1,"Cluster revision file not closed on repository shutdown.After having shut down a repository that has configured clustering, the cluster revision file is still open."
1,"TestIndexWriterDelete fails randomly10 out of 9 runs with that see fail on my trunk:

ant test-core -Dtestcase=TestIndexWriterDelete -Dtestmethod=testErrorAfterApplyDeletes -Dtests.seed=4269712067829708991:1888184886355172227 -Dtests.codec=randomPerField


with this result:

{code}

junit-sequential:
    [junit] Testsuite: org.apache.lucene.index.TestIndexWriterDelete
    [junit] Tests run: 1, Failures: 2, Errors: 0, Time elapsed: 1.725 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterDelete -Dtestmethod=testErrorAfterApplyDeletes -Dtests.seed=4269712067829708991:1888184886355172227 -Dtests.codec=randomPerField
    [junit] NOTE: test params are: codec=RandomCodecProvider: {id=Standard, contents=SimpleText, city=MockSep}, locale=ar_QA, timezone=VST
    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestIndexWriterDelete]
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testErrorAfterApplyDeletes(org.apache.lucene.index.TestIndexWriterDelete):	FAILED
    [junit] 
    [junit] junit.framework.AssertionFailedError: 
    [junit] 	at org.apache.lucene.index.TestIndexWriterDelete.testErrorAfterApplyDeletes(TestIndexWriterDelete.java:736)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1043)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:981)
    [junit] 
    [junit] 
    [junit] Testcase: testErrorAfterApplyDeletes(org.apache.lucene.index.TestIndexWriterDelete):	FAILED
    [junit] ConcurrentMergeScheduler hit unhandled exceptions
    [junit] junit.framework.AssertionFailedError: ConcurrentMergeScheduler hit unhandled exceptions
    [junit] 	at org.apache.lucene.util.LuceneTestCase.tearDown(LuceneTestCase.java:503)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1043)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:981)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.index.TestIndexWriterDelete FAILED
{code}"
1,"Inconsistent behaviour sorting against field with no related documentsIn StringSortedHitQueue - generateSortIndex seems to mistake 
the TermEnum having values as indicating that the sort field 
has entries in the index.

In the case where the search has matching results an ArrayIndexOutOfBounds
exception is thrown in sortValue (line 177 StringSortedHitQueue)
as generateSortIndex creates a terms array of zero length and fieldOrder
contains 0 for all documents.

It would seem more helpful if:
a) generateSortIndex catches the lack of any documents with the sort field.

or

b) reserve terms[0] as a special value for documents that do not have
matching sort field values. ie Change the current implementation to add 1
to the index and change terms[0] to ensure it sorts ""untagged"" documents to
first or last.

For my application Id much prefer solution (b) as it allows much smaller 
indexes and make searching using sort values less brittle.

Thats the best my communication skills can muster just now. Could change
current code to something like:

private final int[] generateSortIndex()
throws IOException {

	final int[] retArray = new int[reader.maxDoc()];
	final String[] mterms = new String[reader.maxDoc() + 1];  // guess length
	if (retArray.length > 0) {
		TermDocs termDocs = reader.termDocs();
		// change this value to control if documents without sort field come first or last
		mterms[0] = """";  // XXXXXXXXX change
		int t = 1;  // current term number  XXXXXXXXXXXXX change
		try {
	

			do {
				Term term = enumerator.term();
				if (term.field() != field) break;

				// store term text
				// we expect that there is at most one term per document
				if (t >= mterms.length) throw new RuntimeException (""there are more terms
than documents in field \""""+field+""\"""");
				mterms[t] = term.text();

				// store which documents use this term
				termDocs.seek (enumerator);
				while (termDocs.next()) {
					retArray[termDocs.doc()] = t;
				}

				t++;
			} while (enumerator.next());
		} finally {
			termDocs.close();
		}

		// if there are less terms than documents,
		// trim off the dead array space
		if (t < mterms.length) {
			terms = new String[t];
			System.arraycopy (mterms, 0, terms, 0, t);
		} else {
			terms = mterms;
		}
	}
	return retArray;
}

Having very quick look at IntegerSortedHitQueue would seem possible
to do same thing. Maybe creating Integer wrapper objects once.

Hope that made some sort of sense. Im not very familiar with the code
or Lucene terminology.
If the above seems like a useful approach Id be glad to generate patches
for a cleaned up version.

Thanks

Sam"
1,"MultiReader should make a private copy of the subReaders arraySpinoff from here:

    http://mail-archives.apache.org/mod_mbox/lucene-java-user/200806.mbox/%3C88F3F6A4-FBFB-43DF-890D-DB5F0D9A2461@gmail.com%3E

Because MultiReader just holds a reference to the array that was passed in, it's possible to hit scary exceptions (that look like index corruption) if that array is later altered eg by reopening some of the readers.

The fix is trivial: just make a private copy."
1,"Query does not work after logging into workspace with no indexesWhen I login to workspace that does not have indexes, they are created but my queries do not return results unless I relog. Output from running attached code sample is:

Node [node1240842434531] created in workspace [test1240842434312]
Property [name] set to [someValueOfMyProperty]
Asking query: select * from nt:unstructured where nt:name like 'someValueOfMyProperty'
Found: 1 nodes before deleting index.
Asking query: select * from nt:unstructured where nt:name like 'someValueOfMyProperty'
Found: 0 nodes after deleting index.
done"
1,"Node.restore() throws java.lang.ClassCastExceptionI'm trying to upgrade to 1.5 using existing 1.3.x repository. Restore of versionable node throws ClassCastException.

Caused by: java.lang.ClassCastException: org.apache.jackrabbit.uuid.UUID
	at org.apache.jackrabbit.core.value.InternalValue.getString(InternalValue.java:436)
	at org.apache.jackrabbit.core.version.InternalFrozenNodeImpl.<init>(InternalFrozenNodeImpl.java:113)
	at org.apache.jackrabbit.core.version.AbstractVersionManager.createInternalVersionItem(AbstractVersionManager.java:576)
	at org.apache.jackrabbit.core.version.VersionManagerImpl.getItem(VersionManagerImpl.java:258)
	at org.apache.jackrabbit.core.version.InternalVersionImpl.getFrozenNode(InternalVersionImpl.java:111)
	at org.apache.jackrabbit.core.version.VersionImpl.getFrozenNode(VersionImpl.java:120)
	at org.apache.jackrabbit.core.NodeImpl.internalRestore(NodeImpl.java:4180)
	at org.apache.jackrabbit.core.NodeImpl.internalRestore(NodeImpl.java:4141)
	at org.apache.jackrabbit.core.NodeImpl.restore(NodeImpl.java:3429)

It seems that bug has been introduced already in 1.4 as part of JCR-926 (InternalValue cleanup).

Index: C:/data/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/version/InternalFrozenNodeImpl.java
===================================================================
--- C:/data/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/version/InternalFrozenNodeImpl.java	(revision 549117)
+++ C:/data/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/version/InternalFrozenNodeImpl.java	(working copy)
@@ -109,10 +109,10 @@
             PropertyState prop = props[i];
             if (prop.getName().equals(QName.JCR_FROZENUUID)) {
                 // special property
-                frozenUUID = UUID.fromString(node.getPropertyValue(QName.JCR_FROZENUUID).internalValue().toString());
+                frozenUUID = UUID.fromString(node.getPropertyValue(QName.JCR_FROZENUUID).getString());

Probably one of the assumptions made was wrong :
- The type of QName.JCR_FROZENUUID is STRING (Object.toString() was used before)."
1,"Session.checkPermission(""/"", ""add_node"") throws PathNotFoundException instead of AccessControlExceptionWhen invoking Session.checkPermission(""/"", ""add_node""), a PathNotFoundException is thrown:

Exception in thread ""main"" javax.jcr.PathNotFoundException: no such ancestor path of degree 1
	at org.apache.jackrabbit.spi.commons.name.PathFactoryImpl$PathImpl.getAncestor(PathFactoryImpl.java:443)
	at org.apache.jackrabbit.core.SessionImpl.checkPermission(SessionImpl.java:710)
	at Test.main(Test.java:35)

i assume that getAncestor(1) used to return null in an earlier version.



"
1,"MultipartEntity incorrectly computes unknown lengthIf any Part of a MultipartEntity reports an unknown length (-1), MultipartEntity
reports an erroneous length value. It should report an unknown length (-1) if
any of the parts is of unknown length, that would cause the POST to be chunked.

See
http://mail-archives.apache.org/mod_mbox/jakarta-httpclient-user/200510.mbox/ajax/%3ceb3d689c0510250851t2eb78462tbf701135bbf718c9@mail.gmail.com%3e"
1,"BasicClientCookie.toString() contains 'name' instead of 'value' when writing out cookie value{noformat}
buffer.append(""[name: "");
buffer.append(this.value);
{noformat}

should be

{noformat}
buffer.append(""[value: "");
buffer.append(this.value);
{noformat}

Will provide a patch soon."
1,cookies > 20 years invalidated 
1,"Lock tokens reains in session after unlockI do the followin steps:

* node.lock()
* session.getLockTokens() -> This show the lock token generated by the
previous lock.
* node.unlock()
* session.getLockTokens() -> Still show me the generated token ??

If I unlock a node, the token should'n be delete from the session?"
1,"Can not set the ""Proxy-connection"" headerWhen using a proxy the HttpClient refuses to set the ""Proxy-connection"" header
to the value ""close"". The value will be converted to ""keep-alive"" when the final
request is sent to network.

The following code snippet can be used to replicate the defect. Method is GET:
...
method.removeRequestHeader(""Proxy-Connection"");
logger.debug(""Proxy-Connection header removed."");
method.addRequestHeader(""Proxy-Connection"", ""close"");
logger.debug(""Proxy-Connection header set to: "" +
method.getRequestHeader(""Proxy-Connection"") );
try {
  	int statusCode = httpclient.executeMethod( method );
...

Now if you look at the wire log, you will notice that the actual value will be
""keep-alive""."
1,"Concurrent Repository.login() throws IllegalStateExceptionSee test case: org.apache.jackrabbit.test.core.ConcurrentLoginTest

java.lang.IllegalStateException: workspace 'default' not initialized
	at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.getItemStateProvider(RepositoryImpl.java:1448)
	at org.apache.jackrabbit.core.RepositoryImpl.getWorkspaceStateManager(RepositoryImpl.java:712)
	at org.apache.jackrabbit.core.SessionImpl.<init>(SessionImpl.java:247)
	at org.apache.jackrabbit.core.SessionImpl.<init>(SessionImpl.java:214)
	at org.apache.jackrabbit.core.XASessionImpl.<init>(XASessionImpl.java:98)
	at org.apache.jackrabbit.core.RepositoryImpl.createSessionInstance(RepositoryImpl.java:1233)
	at org.apache.jackrabbit.core.RepositoryImpl.createSession(RepositoryImpl.java:800)
	at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1119)
	at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1146)
	at org.apache.jackrabbit.core.jndi.BindableRepository.login(BindableRepository.java:181)
	at org.apache.jackrabbit.core.ConcurrentLoginTest$1.run(ConcurrentLoginTest.java:56)
	at java.lang.Thread.run(Thread.java:534)"
1,"ContentLengthInputStream does not implement available() properlyContentLengthInputStream should either extend FilterInputStream or should delegate available() to wrappedStream.

Otherwise, available() on the response stream (an instance of AutoCloseInputStream, which is properly extending FilterInputStream, and, therefore, delegating to the ContentLengthInputStream) always returns 0.

This issue is important for the clients that try to improve performance by processing all data that can be read in a non-blocking way before blocking on the network."
1,repository-2.0.dtd missingWe introduced new configuration elements that need to be reflected in a new DTD version.
1,"Setting WebDAV property without value causes NPE in DAVResourceImplA WebDAV PROPPATCH of a property without a value <prf:SomeProperty/> causes a NPE in DAVResourceImpl when the value is retrieved and the toString() method called on it. Here is a patch that works around the problem.

Index: C:/jprojects/eclipse/jackrabbit/jcr-server/server/src/java/org/apache/jackrabbit/webdav/simple/DavResourceImpl.java
===================================================================
--- C:/jprojects/eclipse/jackrabbit/jcr-server/server/src/java/org/apache/jackrabbit/webdav/simple/DavResourceImpl.java	(revision 388517)
+++ C:/jprojects/eclipse/jackrabbit/jcr-server/server/src/java/org/apache/jackrabbit/webdav/simple/DavResourceImpl.java	(working copy)
@@ -930,7 +930,10 @@
      */
     private void setJcrProperty(DavProperty property) throws RepositoryException {
         // retrieve value
-        String value = property.getValue().toString();
+        String value = """";
+        if (property.getValue() != null) {
+            value = property.getValue().toString();
+        }
         // set value; since multivalued-properties are not listed in the set
         // of available properties, this extra validation-check is omitted.
         node.setProperty(getJcrName(property.getName()), value);
"
1,"javax.jcr.NamespaceException: : is not a registered namespace uriUsing the first hops with both versions 1.2.3 and 1.3, the repository is created successfully the first time it is run.  Subsequent attempts to login result in a javax.jcr.NamespaceException.


DEBUG - Initializing transient repository
INFO - Starting repository...
INFO - LocalFileSystem initialized at path repository\repository
Exception in thread ""main"" javax.jcr.NamespaceException: : is not a registered namespace uri.
	at org.apache.jackrabbit.core.NamespaceRegistryImpl.getPrefix(NamespaceRegistryImpl.java:538)
	at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.checkNamespace(NodeTypeRegistry.java:1292)
	at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.validateNodeTypeDef(NodeTypeRegistry.java:1415)
	at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.internalRegister(NodeTypeRegistry.java:1221)
	at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.<init>(NodeTypeRegistry.java:671)
	at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.create(NodeTypeRegistry.java:118)
	at org.apache.jackrabbit.core.RepositoryImpl.createNodeTypeRegistry(RepositoryImpl.java:571)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:262)
	at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:584)
	at org.apache.jackrabbit.core.TransientRepository$2.getRepository(TransientRepository.java:245)
	at org.apache.jackrabbit.core.TransientRepository.startRepository(TransientRepository.java:265)
	at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:333)
	at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:388)
	at testing.FirstHops.main(FirstHops.java:24)"
1,"Cloning a tree containing shareable nodes into another workspace throws ItemExistsExceptionThere's a problem when trying to clone a tree in another workspace, when this tree contains shareable nodes.

Let ws1 be one workspace, which contains one node A. This node has two sub-nodes B and C. B and C share a shareable sub-node D :

A 
|   \
B  C
|    |
D  D

Let ws2 be a second workspace. Then calling ws2.clone(""ws1"" , ""/A"" , ""/A"" , false) throws an ItemExistsException ( copyNodeState line 1628 ) . This is done when the copyNodeState is checking if the nodeId is already present in the workspace - which is the case when copying the second instance of the shareable node. I can't find in the specification something about this case - but it would be logical to add a share to the node when coming across this situation - at least in the CLONE ( and probable COPY too ) cases. I don't know what would be expected in the CLONE_REMOVE_EXISTING case - we might not want to remove the node if it's shareable, and also add a share here.

I fixed the issue by handling the case the node is shareable in the COPY and CLONE cases of copyNodeState - you'll find attached the corresponding patch. Do you think this solution is ok ?"
1,CLONE -Aggregate include ignored if no primaryType setIf the include element of an aggregate definition does not have a primaryType attribute then the include is never matched.
1,"IndexReader.lastModified - throws NPEIndexReader.lastModified(String dir) or its variants always return NPE on 2.3, perhaps something to do with SegmentInfo."
1,webapp welcome page shows incorrect port when port is the default portSee summary.
1,"RangeQuery - add equals and hashCode methodsI'm attaching a patch with an equals() and hashCode() implementation for 
RangeQuery.java, and a new unit test for TestRangeQuery.java, as per a recent 
message on lucene-user mailing list (subject ""RangeQuery doesn't override 
equals() or hashCode() - intentional?"")

patches to follow"
1,CompactNodeTypeDefWriter uses bad format for residual properties CompactNodeTypeDefWriter writes {}* instead of * for residual properties. 
1,"Session returned does not offers transaction supportThe javax.jcr.Session instance returned by the repository is an implementation of org.apache.jackrabbit.jca.JCASessionHandle which doesn't implement the interface org.apache.jackrabbit.api.XASession.
"
1,"MultiSearcher.rewrite() incorrectly rewrites queriesThis was reported on the userlist, in the context of range queries.

Its also easy to make our existing tests fail with my patch on LUCENE-2751:
{noformat}
ant test-core -Dtestcase=TestBoolean2 -Dtestmethod=testRandomQueries -Dtests.seed=7679849347282878725:-903778383189134045
{noformat}

The fundamental problem is that MultiSearcher first rewrites against individual subs, 
then uses Query.combine() which simply OR's these sub-clauses.

This is incorrect for expanded MUST_NOT queries (e.g. from wildcard), as it violates demorgan's law.
"
1,"Problems with jackrabbit-standaloneI'm having problems with the jackrabbit-standalone component not starting up properly because of two issues:

* The bundle packaging doesn't include the WEB-INF/config.xml file. I assume this is because of the more recent bundle plugin version treating whitespace differently in the inlining settings.

* The RMI binding fails if a local RMI registry is already running with another repository reference. I'm thinking of simply disabling the RMI registry bindings and using the http://localhost:8080/rmi address as the recommended RMI endpoint."
1,"Removing a mixin that adds a same-name-sibling child node throws an ItemNotFoundExceptionApologies in advance if this has previously been noted in JIRA or on the lists but I couldn't find anything. When removing a mixin that defines a same-name-sibling child an ItemNotFoundException is thrown due to child node indicies not being maintained. A similar method of NodeImpl (onRemove) recongnized this and amended to remove from the tail but the removeMixin also has this issue.

Simple test reproduction:

Types:

[mix:foo] mixin
	+ bar (nt:bar) multiple
	
[nt:bar] > nt:unstructured, mix:referenceable

Code:

public class RemoveMixinTest extends AbstractServerTest {
    private static final String MIXIN = ""mix:foo"";
    private static final String CHILD = ""bar"";
    private static final String PTYPE = ""nt:bar"";
    
    public void testRemoveMixin() throws RepositoryException {
        Session session = getSession();
        Node root = session.getRootNode().addNode(""root"");
        root.addMixin(MIXIN);
        
        root.addNode(CHILD, PTYPE);
        root.addNode(CHILD, PTYPE);
        root.addNode(CHILD, PTYPE);
        
        session.save();
        
        for (NodeIterator it = root.getNodes(); it.hasNext(); ) {
            Node node = it.nextNode();
            System.out.println(node.getPath() + "" : "" + node.getUUID());
        }
        
        try {
            root.removeMixin(MIXIN);
            root.save();
        } catch (RepositoryException ex) {
            ex.printStackTrace();
        }
    }
}

Output:

/root/bar : 0b09e0b4-0727-4194-978a-4eadfbf93fa8
/root/bar[2] : 84d5e556-6f12-43fb-98e3-614bcf1f7bb7
/root/bar[3] : 8db95029-df3b-4e26-affb-438de0206cf5

javax.jcr.ItemNotFoundException: 8db95029-df3b-4e26-affb-438de0206cf5
at org.apache.jackrabbit.core.ItemManager.createItemInstance(ItemManager.java:463)
	at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:319)
	at org.apache.jackrabbit.core.NodeImpl.removeMixin(NodeImpl.java:1212)
	at org.apache.jackrabbit.core.NodeImpl.removeMixin(NodeImpl.java:2624)
	at com.ms.appmw.rcf.server.RemoveMixinTest.testRemoveMixin(RemoveMixinTest.java:48)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at msjava.base.testutils.junit3.MSTestCase.runTest(MSTestCase.java:203)
	at msjava.base.testutils.junit3.TestCaseTearDownEvenIfSetUpFails.runBare(TestCaseTearDownEvenIfSetUpFails.java:92)
	at msjava.base.testutils.junit3.MSTestCase.runBare(MSTestCase.java:170)
	at junit.framework.TestResult$1.protect(TestResult.java:106)

The missing UUID for the last node is the one not found because upon removal of the second the index [2] is still valid. Thanks!"
1,"When fetching node ids in checks for the checker all queries should use the same orderingThe ""bundleSelectAllIdsSQL"" query and the ""bundleSelectAllIdsFromSQL"" should use the same ordering."
1,"NodeTypeRegistry.reregister unregisters dependent typesNodeTypeRegistry.reregister allows modifying a registered node type if the difference to the currently registered node type with the same name is TRIVIAL according to NodeTypeDefDiff.

Before registering the new node type definition the old node type is unregistered. The side effect of that first step is that also all NodeTypes, which depend (extend ?) the node type to be re-registered, are removed from the registry.

After the modified node type is then registered, the previously registered dependent node types will not be registered anymore and will not be known any more.

While it makes sense to me, to temporarily unregister dependent node types, those must be registered again after the re-registered node type has been registered. Otherwise the system may become pretty useless."
1,"BasicClientConnectionManager.releaseConnection accesses poolEntry using non-standard lockAccording to the annotation, poolEntry is @GuardedBy(""this"").

However, in at least one place, it is accessed without holding a lock on this: 

BasicClientConnectionManager.releaseConnection synchronizes on managedConn, and then accesses poolEntry without synchronising on this.

[Synch. only works if all parties use the same lock.]"
1,"HttpClient does not retry authentication when multiple challenges are present if the primary one failsI'm trying to request a page from IIS (6 and 7.5).  If the IIS is configured with providers for ""negotiate"" and ""ntlm"" then the Negotiate authentication is tried and fails, but it does not then try to use the NTLM authentication which is what I require.  If I removed ""negotiate"" as a provider from IIS and just use NTLM then all works well - but this is not a solution as I don't have control of the web servers. 

Output below...


[DEBUG] SingleClientConnManager - Get connection for route HttpRoute[{}->http://WIN-HNB91NNAB2G]
[DEBUG] DefaultClientConnectionOperator - Connecting to WIN-HNB91NNAB2G/147.183.80.134:80
[DEBUG] RequestAddCookies - CookieSpec selected: best-match
[DEBUG] DefaultHttpClient - Attempt 1 to execute request
[DEBUG] DefaultClientConnection - Sending request: GET / HTTP/1.1
[DEBUG] wire - >> ""GET / HTTP/1.1[\r][\n]""
[DEBUG] wire - >> ""Host: WIN-HNB91NNAB2G[\r][\n]""
[DEBUG] wire - >> ""Connection: Keep-Alive[\r][\n]""
[DEBUG] wire - >> ""User-Agent: Apache-HttpClient/4.1 (java 1.5)[\r][\n]""
[DEBUG] wire - >> ""[\r][\n]""
[DEBUG] headers - >> GET / HTTP/1.1
[DEBUG] headers - >> Host: WIN-HNB91NNAB2G
[DEBUG] headers - >> Connection: Keep-Alive
[DEBUG] headers - >> User-Agent: Apache-HttpClient/4.1 (java 1.5)
[DEBUG] wire - << ""HTTP/1.1 401 Unauthorized[\r][\n]""
[DEBUG] wire - << ""Content-Type: text/html[\r][\n]""
[DEBUG] wire - << ""Server: Microsoft-IIS/7.5[\r][\n]""
[DEBUG] wire - << ""WWW-Authenticate: Negotiate[\r][\n]""
[DEBUG] wire - << ""WWW-Authenticate: NTLM[\r][\n]""
[DEBUG] wire - << ""Date: Fri, 15 Jul 2011 12:15:11 GMT[\r][\n]""
[DEBUG] wire - << ""Content-Length: 58[\r][\n]""
[DEBUG] wire - << ""[\r][\n]""
[DEBUG] DefaultClientConnection - Receiving response: HTTP/1.1 401 Unauthorized
[DEBUG] headers - << HTTP/1.1 401 Unauthorized
[DEBUG] headers - << Content-Type: text/html
[DEBUG] headers - << Server: Microsoft-IIS/7.5
[DEBUG] headers - << WWW-Authenticate: Negotiate
[DEBUG] headers - << WWW-Authenticate: NTLM
[DEBUG] headers - << Date: Fri, 15 Jul 2011 12:15:11 GMT
[DEBUG] headers - << Content-Length: 58
[DEBUG] DefaultHttpClient - Connection can be kept alive indefinitely
[DEBUG] DefaultHttpClient - Target requested authentication
[DEBUG] DefaultTargetAuthenticationHandler - Authentication schemes in the order of preference: [negotiate, NTLM, Digest, Basic]
[DEBUG] DefaultTargetAuthenticationHandler - negotiate authentication scheme selected
[DEBUG] NegotiateScheme - Received challenge '' from the auth server
[DEBUG] DefaultHttpClient - Authorization challenge processed
[DEBUG] DefaultHttpClient - Authentication scope: NEGOTIATE <any realm>@win-hnb91nnab2g:80
[DEBUG] DefaultHttpClient - Found credentials
[DEBUG] wire - << ""You do not have permission to view this directory or page.""
[DEBUG] RequestAddCookies - CookieSpec selected: best-match
[DEBUG] NegotiateScheme - init WIN-HNB91NNAB2G
[ERROR] RequestTargetAuthentication - Authentication error: Invalid name provided (Mechanism level: Could not load configuration file C:\WINDOWS\krb5.ini (The system cannot find the file specified))
[DEBUG] DefaultHttpClient - Attempt 2 to execute request
[DEBUG] DefaultClientConnection - Sending request: GET / HTTP/1.1
[DEBUG] wire - >> ""GET / HTTP/1.1[\r][\n]""
[DEBUG] wire - >> ""Host: WIN-HNB91NNAB2G[\r][\n]""
[DEBUG] wire - >> ""Connection: Keep-Alive[\r][\n]""
[DEBUG] wire - >> ""User-Agent: Apache-HttpClient/4.1 (java 1.5)[\r][\n]""
[DEBUG] wire - >> ""[\r][\n]""
[DEBUG] headers - >> GET / HTTP/1.1
[DEBUG] headers - >> Host: WIN-HNB91NNAB2G
[DEBUG] headers - >> Connection: Keep-Alive
[DEBUG] headers - >> User-Agent: Apache-HttpClient/4.1 (java 1.5)
[DEBUG] wire - << ""HTTP/1.1 401 Unauthorized[\r][\n]""
[DEBUG] wire - << ""Content-Type: text/html[\r][\n]""
[DEBUG] wire - << ""Server: Microsoft-IIS/7.5[\r][\n]""
[DEBUG] wire - << ""WWW-Authenticate: Negotiate[\r][\n]""
[DEBUG] wire - << ""WWW-Authenticate: NTLM[\r][\n]""
[DEBUG] wire - << ""Date: Fri, 15 Jul 2011 12:15:11 GMT[\r][\n]""
[DEBUG] wire - << ""Content-Length: 58[\r][\n]""
[DEBUG] wire - << ""[\r][\n]""
[DEBUG] DefaultClientConnection - Receiving response: HTTP/1.1 401 Unauthorized
[DEBUG] headers - << HTTP/1.1 401 Unauthorized
[DEBUG] headers - << Content-Type: text/html
[DEBUG] headers - << Server: Microsoft-IIS/7.5
[DEBUG] headers - << WWW-Authenticate: Negotiate
[DEBUG] headers - << WWW-Authenticate: NTLM
[DEBUG] headers - << Date: Fri, 15 Jul 2011 12:15:11 GMT
[DEBUG] headers - << Content-Length: 58
[DEBUG] DefaultHttpClient - Connection can be kept alive indefinitely
[DEBUG] DefaultHttpClient - Target requested authentication
[DEBUG] NegotiateScheme - Received challenge '' from the auth server
[DEBUG] NegotiateScheme - Authentication already attempted
[DEBUG] DefaultHttpClient - Authorization challenge processed
[DEBUG] DefaultHttpClient - Authentication scope: NEGOTIATE <any realm>@win-hnb91nnab2g:80
[DEBUG] DefaultHttpClient - Authentication failed
[DEBUG] wire - << ""You do not have permission to view this directory or page.""
content:You do not have permission to view this directory or page.
[DEBUG] SingleClientConnManager - Releasing connection org.apache.http.impl.conn.SingleClientConnManager$ConnAdapter@17fa65e
"
1,"Restoring a deleted version does not work (throws Exception)java.lang.IllegalStateException: Not in edit mode
	at org.apache.jackrabbit.core.state.LocalItemStateManager.createNew(LocalItemStateManager.java:258)
	at org.apache.jackrabbit.core.version.NodeStateEx.createChildNode(NodeStateEx.java:561)
	at org.apache.jackrabbit.core.version.NodeStateEx.addNode(NodeStateEx.java:525)
	at org.apache.jackrabbit.core.version.NodeStateEx.addNode(NodeStateEx.java:505)
	at org.apache.jackrabbit.core.version.VersionManagerImplRestore.restore(VersionManagerImplRestore.java:201)
	at org.apache.jackrabbit.core.VersionManagerImpl.restore(VersionManagerImpl.java:240)
	at org.apache.jackrabbit.core.NodeImpl.restore(NodeImpl.java:3379)
	at org.apache.jackrabbit.test.api.version.RestoreTest.testRestoreRemoved(RestoreTest.java:812)"
1,"Cookies with names containing blanks or starting with $ should be rejected by RFC2109 spec onlyReported by John Patterson:

> The Cookie class does not like names with spaces in them.  It throws an
> IllegalArgumentException.  Unfortunately the server that my app interacts
> with uses a space in the cookie name.  Both IE and Mozilla don't mind."
1,"AbstractHttpClient.determineTarget(HttpUriRequest)Issue with 4.1 beta1 fails to parse the right host from the URL, eg. http://my.site.com/search/?for=http://other.site.com
This fails request for eg. REST that has a param value with ':' or '?' or '/'.

AbstractHttpClient.determineTarget(HttpUriRequest)
httpcomponents-client-4.0.3:
    private HttpHost determineTarget(HttpUriRequest request) {
        // A null target may be acceptable if there is a default target.
        // Otherwise, the null target is detected in the director.
        HttpHost target = null;

        URI requestURI = request.getURI();
        if (requestURI.isAbsolute()) {
            target = new HttpHost(
                    requestURI.getHost(),
                    requestURI.getPort(),
                    requestURI.getScheme());
        }
        return target;
    }


httpcomponents-client-4.1-beta1:
    private HttpHost determineTarget(HttpUriRequest request) throws ClientProtocolException {
        // A null target may be acceptable if there is a default target.
        // Otherwise, the null target is detected in the director.
        HttpHost target = null;

        URI requestURI = request.getURI();
        if (requestURI.isAbsolute()) {
            String ssp = requestURI.getSchemeSpecificPart();
            ssp = ssp.substring(2, ssp.length()); //remove ""//"" prefix
            int end = ssp.indexOf(':') > 0 ? ssp.indexOf(':') :
                    ssp.indexOf('/') > 0 ? ssp.indexOf('/') :
                    ssp.indexOf('?') > 0 ? ssp.indexOf('?') : ssp.length();
            String host = ssp.substring(0, end);

            int port = requestURI.getPort();
            String scheme = requestURI.getScheme();
            if (host == null || """".equals(host)) {
                throw new ClientProtocolException(
                        ""URI does not specify a valid host name: "" + requestURI);
            }
            target = new HttpHost(host, port, scheme);
        }
        return target;
    }
"
1,"NullPointerException when accessing the SimpleWebdavServlet at the prefix pathWhen accessing the SimpleWebdavServlet with the ""root"" path, that is the same path as set with the resource-path-prefix, a NullPointerException is thrown:

java.lang.NullPointerException
	at org.apache.jackrabbit.name.ParsingPathResolver.getQPath(ParsingPathResolver.java:91)
	at org.apache.jackrabbit.name.CachingPathResolver.getQPath(CachingPathResolver.java:74)
	at org.apache.jackrabbit.core.SessionImpl.getQPath(SessionImpl.java:601)
	at org.apache.jackrabbit.core.SessionImpl.getItem(SessionImpl.java:804)
	at org.apache.sling.jcr.api.internal.PooledSession.getItem(PooledSession.java:157)
	at org.apache.jackrabbit.webdav.simple.ResourceFactoryImpl.getNode(ResourceFactoryImpl.java:140)
	at org.apache.jackrabbit.webdav.simple.ResourceFactoryImpl.createResource(ResourceFactoryImpl.java:89)
	at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.service(AbstractWebdavServlet.java:187)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:487)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:362)
	at org.ops4j.pax.web.service.internal.HttpServiceServletHandler.handle(HttpServiceServletHandler.java:51)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:722)
	at org.ops4j.pax.web.service.internal.HttpServiceContext.handle(HttpServiceContext.java:87)
	at org.ops4j.pax.web.service.internal.JettyServerHandlerCollection.handle(JettyServerHandlerCollection.java:63)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:139)
	at org.mortbay.jetty.Server.handle(Server.java:324)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:505)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:828)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:514)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:211)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:380)
	at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)
	at org.mortbay.thread.BoundedThreadPool$PoolThread.run(BoundedThreadPool.java:450)


The problem seems to be that the ResourceFactoryImpl.createResource method (or rather the getNode method) is not prepared to a DavResourceLocator instance whose resourcePath is null.

I could imagine, that the ResourceFactoryImpl.getNode() method might want to return the root node in this case ?"
1,"java.lang.ArrayIndexOutOfBoundsException while importXML in Java 6Using:
- Jackrabbit 1.3
- Java:
  java version ""1.6.0_02""
  Java(TM) SE Runtime Environment (build 1.6.0_02-b05)
  Java HotSpot(TM) Client VM (build 1.6.0_02-b05, mixed mode, sharing)

When importing attached XML, I get an exception:
Caused by: java.lang.ArrayIndexOutOfBoundsException
        at java.lang.System.arraycopy(Native Method)
        at org.apache.jackrabbit.core.xml.BufferedStringValue.append(BufferedStringValue.java:201)
        at org.apache.jackrabbit.core.xml.SysViewImportHandler.characters(SysViewImportHandler.java:187)
        at org.apache.jackrabbit.core.xml.ImportHandler.characters(ImportHandler.java:200)
        at com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.characters(AbstractSAXParser.java:538)
        at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:461)
        at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:807)
        at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:737)
        at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:107)
        at com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.parse(AbstractSAXParser.java:1205)
        at com.sun.org.apache.xerces.internal.jaxp.SAXParserImpl$JAXPSAXParser.parse(SAXParserImpl.java:522)
        at javax.xml.parsers.SAXParser.parse(SAXParser.java:395)
        at org.apache.jackrabbit.core.SessionImpl.importXML(SessionImpl.java:1116)
...

If I use Java 1.5, then it works.

java version ""1.5.0_12""
Java(TM) 2 Runtime Environment, Standard Edition (build 1.5.0_12-b04)
Java HotSpot(TM) Client VM (build 1.5.0_12-b04, mixed mode, sharing)
"
1,"problem in jcr-server/project.properties dependencies?There seems to be a mismatch in the version numbers in the dependencies defined in jcr-server/project.properties. It refers to version 1.0, yet the other components are already at 1.1.

The attached patch solves the problem for me, but I don't claim to fully understand how this works...
"
1,"HttpURL creates wrong authority String when user info is changedWhen changing the user info on an existing HttpURL which has additional port
information, the new authority String contains a wrong hostname part: instead of
getting ""hostname:portnumber"" the string is ""hostnameportnumber"", i.e. the "":""
is missing.
Methods which needs to be changed are:

setRawPassword(...)
setRawUser(...)
setRawUserinfo(...)

(look for the line
String hostport = (_port == -1) ? hostname : hostname + _port;
)

Andreas Fnger
ESIGN Software GmbH"
1,"Wrong cookie matching port number reported when using a proxyFollowing the example given in https://issues.apache.org/jira/browse/HTTPCLIENT-852 and the route HttpRoute[{}->http://xyz.webfactional.com:7295->http://www.seoconsultants.com]:

one of the new cookies is reported to be added as:

[java] 2009/05/28 19:58:23:398 CEST [DEBUG] RequestAddCookies - Cookie [version: 0][name: ASPSESSIONIDCSARBQBA][value: MAMPAMKCBDJJFKNAAPKPMDAA][domain: www.seoconsultants.com][path: /][expiry: null] match [www.seoconsultants.com:7295/]

whereas it should be:

[java] 2009/05/28 19:57:46:667 CEST [DEBUG] RequestAddCookies - Cookie [version: 0][name: ASPSESSIONIDCSARBQBA][value: AAMPAMKCMBINHNEHPFEBFADA][domain: www.seoconsultants.com][path: /][expiry: null] match [www.seoconsultants.com:80/]

i.e. the same as without using a proxy. 7295 is the port number used to access the proxy. The target domain www.seoconsultants.com is accessed through the regular HTTP port number 80, thus the cookie matching should also refer to port 80 and not the proxy's port."
1,"SegmentReader.doCommit should be sync'd; norms methods need not be sync'dI fixed the failure in TestNRTThreads, but in the process tripped an assert because SegmentReader.doCommit isn't sync'd.

So I sync'd it, but I don't think the norms APIs need to be sync'd -- we populate norms up front and then never change them.  Un-sync'ing them is important so that in the NRT case calling IW.commit doesn't block searches trying to pull norms.

Also some small code refactoring."
1,"NullPointerException thrown by equals method in SpanOrQueryPart of our code utilizes the equals method in SpanOrQuery and, in certain cases (details to follow, if necessary), a NullPointerException gets thrown as a result of the String ""field"" being null.  After applying the following patch, the problem disappeared:

Index: src/java/org/apache/lucene/search/spans/SpanOrQuery.java
===================================================================
--- src/java/org/apache/lucene/search/spans/SpanOrQuery.java    (revision 465065)
+++ src/java/org/apache/lucene/search/spans/SpanOrQuery.java    (working copy)
@@ -121,7 +121,8 @@
     final SpanOrQuery that = (SpanOrQuery) o;

     if (!clauses.equals(that.clauses)) return false;
-    if (!field.equals(that.field)) return false;
+    if (field != null && !field.equals(that.field)) return false;
+    if (field == null && that.field != null) return false;

     return getBoost() == that.getBoost();
   }

"
1,"XA Transaction RecoveryIf i add a node to the repository i get a XAException because i run into a Timeout ... 
I see the Warn Message: Transaction rolled back because timeout expired.
The default Timeout is set to 5 sec and i dont know how to set it to a higher value
The Problem is if i restart my server websphere has a RecoveryManager and he try to recover this Transaction
and then i get a NullpointerException in JCAManagedConnectionFactory. createManagedConnection beacuse the given 
ConnectionRequestInfo is null.
So i dont know why the RecoveryManager tries to recover the Transaction ? The only solution for me is to delete the Tran-Log Files wich keep Websphere to recvoer
XA Trasnactions.
"
1,The move method doesn't remove the source nodeHere is a small unit test that demonstrate that the method move doesn't remove the source node. 
1,"TestIndexWriter.testThreadInterruptDeadlock failed (can't reproduce)trunk: r1134163 

ran it a few times with tests.iter=200 and couldn't reproduce, but i believe you like an issue anyway.

{code}
    [junit] Testsuite: org.apache.lucene.index.TestIndexWriter
    [junit] Testcase: testThreadInterruptDeadlock(org.apache.lucene.index.TestIndexWriter):     FAILED
    [junit]
    [junit] junit.framework.AssertionFailedError:
    [junit]     at org.apache.lucene.index.TestIndexWriter.testThreadInterruptDeadlock(TestIndexWriter.java:1203)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1403)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1321)
    [junit]
    [junit]
    [junit] Tests run: 40, Failures: 1, Errors: 0, Time elapsed: 23.79 sec
    [junit]
    [junit] ------------- Standard Output ---------------
    [junit] CheckIndex failed
    [junit] ERROR: could not read any segments file in directory
    [junit] java.io.FileNotFoundException: segments_2w
    [junit]     at org.apache.lucene.store.MockDirectoryWrapper.openInput(MockDirectoryWrapper.java:407)
    [junit]     at org.apache.lucene.index.codecs.DefaultSegmentInfosReader.openInput(DefaultSegmentInfosReader.java:112)
    [junit]     at org.apache.lucene.index.codecs.DefaultSegmentInfosReader.read(DefaultSegmentInfosReader.java:45)
    [junit]     at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:257)
    [junit]     at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:287)
    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:698)
    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:533)
    [junit]     at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:283)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:311)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:154)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:144)
    [junit]     at org.apache.lucene.index.TestIndexWriter$IndexerThreadInterrupt.run(TestIndexWriter.java:1154)
    [junit]
    [junit] CheckIndex FAILED: unexpected exception
    [junit] java.lang.RuntimeException: CheckIndex failed
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:158)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:144)
    [junit]     at org.apache.lucene.index.TestIndexWriter$IndexerThreadInterrupt.run(TestIndexWriter.java:1154)
    [junit] IndexReader.open FAILED: unexpected exception
    [junit] java.io.FileNotFoundException: segments_2w
    [junit]     at org.apache.lucene.store.MockDirectoryWrapper.openInput(MockDirectoryWrapper.java:407)
    [junit]     at org.apache.lucene.index.codecs.DefaultSegmentInfosReader.openInput(DefaultSegmentInfosReader.java:112)
    [junit]     at org.apache.lucene.index.codecs.DefaultSegmentInfosReader.read(DefaultSegmentInfosReader.java:45)
    [junit]     at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:257)
    [junit]     at org.apache.lucene.index.DirectoryReader$1.doBody(DirectoryReader.java:88)
    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:698)
    [junit]     at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:84)
    [junit]     at org.apache.lucene.index.IndexReader.open(IndexReader.java:500)
    [junit]     at org.apache.lucene.index.IndexReader.open(IndexReader.java:293)
    [junit]     at org.apache.lucene.index.TestIndexWriter$IndexerThreadInterrupt.run(TestIndexWriter.java:1161)

    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testThreadInterruptDeadlock -Dtests.seed=6733070832417768606:3130345095020099096
    [junit] NOTE: test params are: codec=RandomCodecProvider: {=MockRandom, f6=SimpleText, f7=MockRandom, f8=MockSep, f9=Standard, f1=SimpleText, f0=Standard, f3=Standard, f2=MockSep, f5=Pulsing(freqCutoff=12),
 f4=MockFixedIntBlock(blockSize=552), c=MockVariableIntBlock(baseBlockSize=43), d9=MockVariableIntBlock(baseBlockSize=43), d8=MockRandom, d5=MockSep, d4=Pulsing(freqCutoff=12), d7=MockFixedIntBlock(blockSize=55
2), d6=MockVariableIntBlock(baseBlockSize=43), d25=MockSep, d0=MockVariableIntBlock(baseBlockSize=43), c29=MockFixedIntBlock(blockSize=552), d24=Pulsing(freqCutoff=12), d1=MockFixedIntBlock(blockSize=552), c28=
Standard, d23=SimpleText, d2=SimpleText, c27=MockSep, d22=Standard, d3=MockRandom, d21=MockRandom, d20=SimpleText, c22=MockSep, c21=Pulsing(freqCutoff=12), c20=SimpleText, d29=SimpleText, c26=MockVariableIntBlo
ck(baseBlockSize=43), d28=Standard, c25=MockRandom, d27=MockVariableIntBlock(baseBlockSize=43), c24=Pulsing(freqCutoff=12), d26=MockRandom, c23=MockFixedIntBlock(blockSize=552), e9=Pulsing(freqCutoff=12), e8=St
andard, e7=MockSep, e6=MockRandom, e5=SimpleText, c17=MockFixedIntBlock(blockSize=552), e3=MockFixedIntBlock(blockSize=552), d12=Pulsing(freqCutoff=12), c16=MockVariableIntBlock(baseBlockSize=43), e4=Pulsing(fr
eqCutoff=12), d11=MockFixedIntBlock(blockSize=552), c19=MockRandom, e1=MockSep, d14=MockVariableIntBlock(baseBlockSize=43), c18=SimpleText, e2=Standard, d13=MockRandom, e0=SimpleText, d10=MockSep, d19=MockVaria
bleIntBlock(baseBlockSize=43), c11=MockVariableIntBlock(baseBlockSize=43), c10=MockRandom, d16=Standard, c13=MockRandom, c12=SimpleText, d15=MockSep, d18=Pulsing(freqCutoff=12), c15=Standard, d17=MockFixedIntBl
ock(blockSize=552), c14=MockSep, b3=Standard, b2=MockSep, b5=Pulsing(freqCutoff=12), b4=MockFixedIntBlock(blockSize=552), b7=MockFixedIntBlock(blockSize=552), b6=MockVariableIntBlock(baseBlockSize=43), d50=Mock
Random, b9=MockRandom, b8=SimpleText, d43=SimpleText, d42=Standard, d41=MockVariableIntBlock(baseBlockSize=43), d40=MockRandom, d47=Pulsing(freqCutoff=12), d46=MockFixedIntBlock(blockSize=552), b0=MockRandom, d
45=Standard, b1=MockVariableIntBlock(baseBlockSize=43), d44=MockSep, d49=MockRandom, d48=SimpleText, c6=SimpleText, c5=Standard, c4=MockVariableIntBlock(baseBlockSize=43), c3=MockRandom, c9=MockFixedIntBlock(bl
ockSize=552), c8=Standard, c7=MockSep, d30=Standard, d32=Pulsing(freqCutoff=12), d31=MockFixedIntBlock(blockSize=552), c1=Pulsing(freqCutoff=12), d34=MockFixedIntBlock(blockSize=552), c2=MockSep, d33=MockVariab
leIntBlock(baseBlockSize=43), d36=MockRandom, c0=SimpleText, d35=SimpleText, d38=MockSep, d37=Pulsing(freqCutoff=12), d39=MockVariableIntBlock(baseBlockSize=43), e92=MockFixedIntBlock(blockSize=552), e93=Pulsin
g(freqCutoff=12), e90=MockSep, e91=Standard, e89=Standard, e88=MockVariableIntBlock(baseBlockSize=43), e87=MockRandom, e86=MockFixedIntBlock(blockSize=552), e85=MockVariableIntBlock(baseBlockSize=43), e84=MockS
ep, e83=Pulsing(freqCutoff=12), e80=MockFixedIntBlock(blockSize=552), e81=SimpleText, e82=MockRandom, e77=Standard, e76=MockSep, e79=Pulsing(freqCutoff=12), e78=MockFixedIntBlock(blockSize=552), e73=MockVariabl
eIntBlock(baseBlockSize=43), e72=MockRandom, e75=SimpleText, e74=Standard, binary=MockSep, f98=MockRandom, f97=SimpleText, f99=MockSep, f94=Pulsing(freqCutoff=12), f93=MockFixedIntBlock(blockSize=552), f96=Mock
VariableIntBlock(baseBlockSize=43), f95=MockRandom, e95=MockRandom, e94=SimpleText, e97=Standard, e96=MockSep, e99=MockSep, e98=Pulsing(freqCutoff=12), id=Standard, f34=SimpleText, f33=Standard, f32=MockVariabl
eIntBlock(baseBlockSize=43), f31=MockRandom, f30=MockFixedIntBlock(blockSize=552), f39=SimpleText, f38=MockVariableIntBlock(baseBlockSize=43), f37=MockRandom, f36=Pulsing(freqCutoff=12), f35=MockFixedIntBlock(b
lockSize=552), f43=MockSep, f42=Pulsing(freqCutoff=12), f45=MockFixedIntBlock(blockSize=552), f44=MockVariableIntBlock(baseBlockSize=43), f41=Standard, f40=MockSep, f47=SimpleText, f46=Standard, f49=MockSep, f4
8=Pulsing(freqCutoff=12), content=Standard, e19=Standard, e18=MockSep, e17=SimpleText, f12=MockRandom, e16=Standard, f11=SimpleText, f10=MockFixedIntBlock(blockSize=552), e15=MockVariableIntBlock(baseBlockSize=
43), e14=MockRandom, f16=MockFixedIntBlock(blockSize=552), e13=MockSep, e12=Pulsing(freqCutoff=12), f15=MockVariableIntBlock(baseBlockSize=43), e11=SimpleText, f14=MockSep, e10=Standard, f13=Pulsing(freqCutoff=
12), f19=Standard, f18=MockVariableIntBlock(baseBlockSize=43), f17=MockRandom, e29=MockRandom, e26=MockSep, f21=Standard, e25=Pulsing(freqCutoff=12), f20=MockSep, e28=MockFixedIntBlock(blockSize=552), f23=Pulsi
ng(freqCutoff=12), e27=MockVariableIntBlock(baseBlockSize=43), f22=MockFixedIntBlock(blockSize=552), f25=MockRandom, e22=MockFixedIntBlock(blockSize=552), f24=SimpleText, e21=MockVariableIntBlock(baseBlockSize=
43), f27=Standard, e24=MockRandom, f26=MockSep, e23=SimpleText, f29=MockSep, f28=Pulsing(freqCutoff=12), e20=Pulsing(freqCutoff=12), field=MockSep, string=MockVariableIntBlock(baseBlockSize=43), e30=MockFixedIn
tBlock(blockSize=552), e31=Pulsing(freqCutoff=12), a98=MockSep, e34=SimpleText, a99=Standard, e35=MockRandom, f79=MockSep, e32=MockVariableIntBlock(baseBlockSize=43), e33=MockFixedIntBlock(blockSize=552), b97=M
ockRandom, f77=MockRandom, e38=MockVariableIntBlock(baseBlockSize=43), b98=MockVariableIntBlock(baseBlockSize=43), f78=MockVariableIntBlock(baseBlockSize=43), e39=MockFixedIntBlock(blockSize=552), b99=Standard,
 f75=MockFixedIntBlock(blockSize=552), e36=Pulsing(freqCutoff=12), f76=Pulsing(freqCutoff=12), e37=MockSep, f73=Pulsing(freqCutoff=12), f74=MockSep, f71=Standard, f72=SimpleText, f81=Standard, f80=MockSep, e40=
MockVariableIntBlock(baseBlockSize=43), e41=Standard, e42=SimpleText, e43=MockSep, e44=Standard, e45=MockFixedIntBlock(blockSize=552), e46=Pulsing(freqCutoff=12), f86=Standard, e47=SimpleText, f87=SimpleText, e
48=MockRandom, f88=Pulsing(freqCutoff=12), e49=MockSep, f89=MockSep, f82=MockVariableIntBlock(baseBlockSize=43), f83=MockFixedIntBlock(blockSize=552), f84=SimpleText, f85=MockRandom, f90=Pulsing(freqCutoff=12),
 f92=MockVariableIntBlock(baseBlockSize=43), f91=MockRandom, str=MockRandom, a76=Standard, e56=Standard, f59=Pulsing(freqCutoff=12), a77=SimpleText, e57=SimpleText, a78=Pulsing(freqCutoff=12), e54=MockRandom, f
57=Standard, a79=MockSep, e55=MockVariableIntBlock(baseBlockSize=43), f58=SimpleText, e52=MockVariableIntBlock(baseBlockSize=43), e53=MockFixedIntBlock(blockSize=552), e50=Pulsing(freqCutoff=12), e51=MockSep, f
51=MockSep, f52=Standard, f50=MockRandom, f55=MockVariableIntBlock(baseBlockSize=43), f56=MockFixedIntBlock(blockSize=552), f53=Pulsing(freqCutoff=12), e58=MockFixedIntBlock(blockSize=552), f54=MockSep, e59=Pul
sing(freqCutoff=12), a80=Pulsing(freqCutoff=12), e60=Pulsing(freqCutoff=12), a82=MockVariableIntBlock(baseBlockSize=43), a81=MockRandom, a84=MockRandom, a83=SimpleText, a86=Standard, a85=MockSep, a89=SimpleText
, f68=MockVariableIntBlock(baseBlockSize=43), e65=Pulsing(freqCutoff=12), f69=MockFixedIntBlock(blockSize=552), e66=MockSep, a87=MockVariableIntBlock(baseBlockSize=43), e67=MockVariableIntBlock(baseBlockSize=43
), a88=MockFixedIntBlock(blockSize=552), e68=MockFixedIntBlock(blockSize=552), e61=SimpleText, e62=MockRandom, e63=MockSep, e64=Standard, f60=MockFixedIntBlock(blockSize=552), f61=Pulsing(freq

Cutoff=12), f62=MockRandom, f63=MockVariableIntBlock(baseBlockSize=43), e69=Standard, f64=SimpleText, f65=MockRandom, f66=MockSep, f67=Standard, f70=MockFixedIntBlock(blockSize=552), a93=MockSep, a92=Pulsing(freqCutoff=12), a91=SimpleText, e71=SimpleText, a90=Standard, e70=Standard, a97=MockVariableIntBlock(baseBlockSize=43), a96=MockRandom, a95=Pulsing(freqCutoff=12), a94=MockFixedIntBlock(blockSize=552), c58=MockRandom, a63=MockFixedIntBlock(blockSize=552), a64=Pulsing(freqCutoff=12), c59=MockVariableIntBlock(baseBlockSize=43), c56=MockFixedIntBlock(blockSize=552), d59=MockRandom, a61=MockSep, c57=Pulsing(freqCutoff=12), a62=Standard, c54=Pulsing(freqCutoff=12), c55=MockSep, a60=SimpleText, c52=Standard, c53=SimpleText, d53=SimpleText, d54=MockRandom, d51=MockVariableIntBlock(baseBlockSize=43), d52=MockFixedIntBlock(blockSize=552), d57=Pulsing(freqCutoff=12), b62=Standard, d58=MockSep, b63=SimpleText, d55=Standard, b60=MockRandom, d56=SimpleText, b61=MockVariableIntBlock(baseBlockSize=43), b56=Standard, b55=MockSep, b54=MockRandom, b53=SimpleText, d61=MockVariableIntBlock(baseBlockSize=43), b59=MockVariableIntBlock(baseBlockSize=43), d60=MockRandom, b58=MockSep, b57=Pulsing(freqCutoff=12), c62=Standard, c61=MockSep, a59=MockVariableIntBlock(baseBlockSize=43), c60=MockRandom, a58=MockRandom, a57=MockFixedIntBlock(blockSize=552), a56=MockVariableIntBlock(baseBlockSize=43), a55=MockSep, a54=Pulsing(freqCutoff=12), a72=MockRandom, c67=Standard, a73=MockVariableIntBlock(baseBlockSize=43), c68=SimpleText, a74=Standard, c69=Pulsing(freqCutoff=12), a75=SimpleText, c63=MockVariableIntBlock(baseBlockSize=43), c64=MockFixedIntBlock(blockSize=552), a70=MockVariableIntBlock(baseBlockSize=43), c65=SimpleText, a71=MockFixedIntBlock(blockSize=552), c66=MockRandom, d62=MockSep, d63=Standard, d64=MockFixedIntBlock(blockSize=552), b70=Standard, d65=Pulsing(freqCutoff=12), b71=Pulsing(freqCutoff=12), d66=MockVariableIntBlock(baseBlockSize=43), b72=MockSep, d67=MockFixedIntBlock(blockSize=552), b73=MockVariableIntBlock(baseBlockSize=43), d68=SimpleText, b74=MockFixedIntBlock(blockSize=552), d69=MockRandom, b65=Pulsing(freqCutoff=12), b64=MockFixedIntBlock(blockSize=552), b67=MockVariableIntBlock(baseBlockSize=43), b66=MockRandom, d70=SimpleText, b69=MockRandom, b68=SimpleText, d72=MockSep, d71=Pulsing(freqCutoff=12), c71=Pulsing(freqCutoff=12), c70=MockFixedIntBlock(blockSize=552), a69=Pulsing(freqCutoff=12), c73=MockVariableIntBlock(baseBlockSize=43), c72=MockRandom, a66=MockRandom, a65=SimpleText, a68=Standard, a67=MockSep, c32=MockSep, c33=Standard, c30=SimpleText, c31=MockRandom, c36=MockVariableIntBlock(baseBlockSize=43), a41=Pulsing(freqCutoff=12), c37=MockFixedIntBlock(blockSize=552), a42=MockSep, a0=MockRandom, c34=Pulsing(freqCutoff=12), c35=MockSep, a40=SimpleText, b84=MockSep, d79=MockFixedIntBlock(blockSize=552), b85=Standard, b82=SimpleText, d77=MockSep, c38=Standard, b83=MockRandom, d78=Standard, c39=SimpleText, b80=MockRandom, d75=Standard, b81=MockVariableIntBlock(baseBlockSize=43), d76=SimpleText, d73=MockRandom, d74=MockVariableIntBlock(baseBlockSize=43), d83=MockRandom, a9=MockFixedIntBlock(blockSize=552), d82=SimpleText, d81=MockFixedIntBlock(blockSize=552), d80=MockVariableIntBlock(baseBlockSize=43), b79=MockFixedIntBlock(blockSize=552), b78=MockSep, b77=Pulsing(freqCutoff=12), b76=SimpleText, b75=Standard, a1=Pulsing(freqCutoff=12), a35=Pulsing(freqCutoff=12), a2=MockSep, a34=MockFixedIntBlock(blockSize=552), a3=MockVariableIntBlock(baseBlockSize=43), a33=Standard, a4=MockFixedIntBlock(blockSize=552), a32=MockSep, a5=MockRandom, a39=MockRandom, c40=SimpleText, a6=MockVariableIntBlock(baseBlockSize=43), a38=SimpleText, a7=Standard, a37=MockFixedIntBlock(blockSize=552), a8=SimpleText, a36=MockVariableIntBlock(baseBlockSize=43), c41=MockFixedIntBlock(blockSize=552), c42=Pulsing(freqCutoff=12), c43=MockRandom, c44=MockVariableIntBlock(baseBlockSize=43), c45=SimpleText, a50=MockVariableIntBlock(baseBlockSize=43), c46=MockRandom, a51=MockFixedIntBlock(blockSize=552), c47=MockSep, a52=SimpleText, c48=Standard, a53=MockRandom, b93=MockFixedIntBlock(blockSize=552), d88=MockRandom, c49=MockVariableIntBlock(baseBlockSize=43), b94=Pulsing(freqCutoff=12), d89=MockVariableIntBlock(baseBlockSize=43), b95=MockRandom, b96=MockVariableIntBlock(baseBlockSize=43), d84=Pulsing(freqCutoff=12), b90=SimpleText, d85=MockSep, b91=Pulsing(freqCutoff=12), d86=MockVariableIntBlock(baseBlockSize=43), b92=MockSep, d87=MockFixedIntBlock(blockSize=552), d92=Standard, d91=MockSep, d94=Pulsing(freqCutoff=12), d93=MockFixedIntBlock(blockSize=552), b87=MockFixedIntBlock(blockSize=552), b86=MockVariableIntBlock(baseBlockSize=43), d90=SimpleText, b89=MockRandom, b88=SimpleText, a44=MockVariableIntBlock(baseBlockSize=43), a43=MockRandom, a46=SimpleText, a45=Standard, a48=Standard, a47=MockSep, c51=MockFixedIntBlock(blockSize=552), a49=MockFixedIntBlock(blockSize=552), c50=MockVariableIntBlock(baseBlockSize=43), d98=MockFixedIntBlock(blockSize=552), d97=MockVariableIntBlock(baseBlockSize=43), d96=MockSep, d95=Pulsing(freqCutoff=12), d99=MockRandom, a20=MockVariableIntBlock(baseBlockSize=43), c99=SimpleText, c98=Standard, c97=MockVariableIntBlock(baseBlockSize=43), c96=MockRandom, b19=MockVariableIntBlock(baseBlockSize=43), a16=Pulsing(freqCutoff=12), a17=MockSep, b17=Pulsing(freqCutoff=12), a14=Standard, b18=MockSep, a15=SimpleText, a12=SimpleText, a13=MockRandom, a10=MockVariableIntBlock(baseBlockSize=43), a11=MockFixedIntBlock(blockSize=552), b11=MockFixedIntBlock(blockSize=552), b12=Pulsing(freqCutoff=12), b10=Standard, b15=SimpleText, b16=MockRandom, a18=MockRandom, b13=MockVariableIntBlock(baseBlockSize=43), a19=MockVariableIntBlock(baseBlockSize=43), b14=MockFixedIntBlock(blockSize=552), b30=MockRandom, a31=MockSep, a30=Pulsing(freqCutoff=12), b28=SimpleText, a25=MockVariableIntBlock(baseBlockSize=43), b29=MockRandom, a26=MockFixedIntBlock(blockSize=552), a27=SimpleText, a28=MockRandom, a21=MockSep, a22=Standard, a23=MockFixedIntBlock(blockSize=552), a24=Pulsing(freqCutoff=12), b20=MockRandom, b21=MockVariableIntBlock(baseBlockSize=43), b22=Standard, b23=SimpleText, a29=Pulsing(freqCutoff=12), b24=MockSep, b25=Standard, b26=MockFixedIntBlock(blockSize=552), b27=Pulsing(freqCutoff=12), b41=Pulsing(freqCutoff=12), b40=MockFixedIntBlock(blockSize=552), c77=MockRandom, c76=SimpleText, c75=MockFixedIntBlock(blockSize=552), c74=MockVariableIntBlock(baseBlockSize=43), c79=SimpleText, c78=Standard, c80=MockSep, c83=MockRandom, c84=MockVariableIntBlock(baseBlockSize=43), c81=MockFixedIntBlock(blockSize=552), b39=MockFixedIntBlock(blockSize=552), c82=Pulsing(freqCutoff=12), b37=Standard, b38=SimpleText, b35=MockRandom, b36=MockVariableIntBlock(baseBlockSize=43), b33=MockVariableIntBlock(baseBlockSize=43), b34=MockFixedIntBlock(blockSize=552), b31=Pulsing(freqCutoff=12), b32=MockSep, str2=MockSep, b50=MockVariableIntBlock(baseBlockSize=43), b52=SimpleText, str3=MockVariableIntBlock(baseBlockSize=43), b51=Standard, c86=Standard, tvtest=MockSep, c85=MockSep, c88=Pulsing(freqCutoff=12), c87=MockFixedIntBlock(blockSize=552), c89=MockVariableIntBlock(baseBlockSize=43), c90=SimpleText, c91=MockRandom, c92=Standard, c93=SimpleText, c94=Pulsing(freqCutoff=12), c95=MockSep, content1=MockRandom, b46=Pulsing(freqCutoff=12), b47=MockSep, content3=MockFixedIntBlock(blockSize=552), b48=MockVariableIntBlock(baseBlockSize=43), content4=MockVariableIntBlock(baseBlockSize=43), b49=MockFixedIntBlock(blockSize=552), content5=Pulsing(freqCutoff=12), b42=SimpleText, b43=MockRandom, b44=MockSep, b45=Standard}, locale=sk, timezone=America/Rainy_River
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestDateTools, TestDeletionPolicy, TestDocsAndPositions, TestFlex, TestIndexReaderCloneNorms, TestIndexWriter]
    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=86065896,total=127401984
    [junit] ------------- ---------------- ---------------
    [junit] TEST org.apache.lucene.index.TestIndexWriter FAILED
{code}"
1,"IndexReader.isCurrent incorrectly returns false after writer.prepareCommit has been calledSpinoff from thread ""2 phase commit with external data"" on java-user.

The IndexReader should not see the index as changed, after a prepareCommit has been called but before commit is called."
1,"Missing Content-Length header causes a SocketExceptionEssentially, we have an invalid HTTP server (Stellent CMS actually and we will file a bug with them), 
which is returning headers like:

HTTP/1.1 401 Unauthorized
WWW-Authenticate: Basic ""Secure Realm""
Connection: keep-alive

Which is clearly missing the Content-Length header.  Now, previously HttpClient handled this 
perfectly by reading until the end of the connection (ie: treating it like it was a Connection: close), 
however for some reason a socket exception is being thrown and the invalid connection is added 
back into the connection pool and then every connection to the server after that thows an 
exception.

See the thread ""SocketException with invalid server"" for the full discussion of the issue.

I'll attach a patch that fixes the problem.  The biggest thing to consider is the changes to the 
duplicate Connection header test cases which resolves around the question: if Connection: keep-
alive is present but no Content-Length is provided, should the connection be closed?  The patch 
requires the answer to be yes and I really can't see any other way to do it."
1,"ItemInfoCacheImpl.getNodeInfo() and .getPropertyInfo() might not clear all relevant entriesItemInfoCacheImpl.getNodeInfo() and .getPropertyInfo() remove the retrieved entry from the cache.

since entries might be cached by id AND path, entires identified by path are not removed from the cache if they're retrieved by id."
1,"JVM bug 4949631 causes BufferOverflowException in HttpMethodBase.getResponseBodyAsStringava.nio.BufferOverflowException
        at java.nio.charset.CoderResult.throwException(CoderResult.java:259)
        at java.lang.StringCoding$CharsetSD.decode(StringCoding.java:188)
        at java.lang.StringCoding.decode(StringCoding.java:224)
        at java.lang.String.<init>(String.java:320)
        at
org.apache.commons.httpclient.HttpConstants.getContentString(HttpConstants.java:199)
        at
org.apache.commons.httpclient.HttpConstants.getContentString(HttpConstants.java:233)
        at
org.apache.commons.httpclient.HttpMethodBase.getResponseBodyAsString(HttpMethodBase.java:735)


This seems to be caused by a known JVM bug:
http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4949631

Strings over 16Mb can cause the problem.   Some workarounds are listed, the
essence being to split the string and call getBytes on each piece and reassemble
with a ByteBuffer."
1,"norms reading fails with FileNotFound in exceptional caseIf we can't get to the bottom of this, we can always add the fileExists check back...

{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestIndexWriterExceptions
    [junit] Testcase: testRandomExceptionsThreads(org.apache.lucene.index.TestIndexWriterExceptions):	Caused an ERROR
    [junit] No sub-file with id _nrm.cfs found (fileName=_19_nrm.cfs files: [.fdt, .fnm, .per, .fdx])
    [junit] java.io.FileNotFoundException: No sub-file with id _nrm.cfs found (fileName=_19_nrm.cfs files: [.fdt, .fnm, .per, .fdx])
    [junit] 	at org.apache.lucene.store.CompoundFileDirectory.createSlicer(CompoundFileDirectory.java:313)
    [junit] 	at org.apache.lucene.store.CompoundFileDirectory.<init>(CompoundFileDirectory.java:65)
    [junit] 	at org.apache.lucene.codecs.lucene40.Lucene40DocValuesProducer.<init>(Lucene40DocValuesProducer.java:48)
    [junit] 	at org.apache.lucene.codecs.lucene40.Lucene40NormsFormat$Lucene40NormsDocValuesProducer.<init>(Lucene40NormsFormat.java:70)
    [junit] 	at org.apache.lucene.codecs.lucene40.Lucene40NormsFormat.docsProducer(Lucene40NormsFormat.java:49)
    [junit] 	at org.apache.lucene.codecs.lucene40.Lucene40NormsFormat.docsProducer(Lucene40NormsFormat.java:62)
    [junit] 	at org.apache.lucene.index.SegmentCoreReaders.<init>(SegmentCoreReaders.java:122)
    [junit] 	at org.apache.lucene.index.SegmentReader.<init>(SegmentReader.java:54)
    [junit] 	at org.apache.lucene.index.DirectoryReader$1.doBody(DirectoryReader.java:65)
    [junit] 	at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:660)
    [junit] 	at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:55)
    [junit] 	at org.apache.lucene.index.IndexReader.open(IndexReader.java:242)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions.testRandomExceptionsThreads(TestIndexWriterExceptions.java:304)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$3$1.evaluate(LuceneTestCase.java:530)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:165)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)
    [junit] 
    [junit] 
    [junit] Tests run: 22, Failures: 0, Errors: 1, Time elapsed: 3.439 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterExceptions -Dtestmethod=testRandomExceptionsThreads -Dtests.seed=-4ea45cb40d17460b:-459bfb455a2351b9:1abd8f0f3a0611b9 -Dargs=""-Dfile.encoding=UTF-8""
    [junit] NOTE: test params are: codec=Lucene40: {field=MockVariableIntBlock(baseBlockSize=31), id=PostingsFormat(name=NestedPulsing), content=Pulsing40(freqCutoff=2 minBlockSize=58 maxBlockSize=186), contents=MockVariableIntBlock(baseBlockSize=31), content1=MockVariableIntBlock(baseBlockSize=31), content2=PostingsFormat(name=MockSep), content4=Pulsing40(freqCutoff=2 minBlockSize=58 maxBlockSize=186), content5=MockFixedIntBlock(blockSize=964), content6=PostingsFormat(name=Memory), content7=PostingsFormat(name=MockRandom), crash=PostingsFormat(name=NestedPulsing), subid=PostingsFormat(name=NestedPulsing)}, sim=RandomSimilarityProvider(queryNorm=false,coord=true): {other=DFR GB3(800.0), contents=IB SPL-L3(800.0), content=DFR GL3(800.0), id=DFR I(F)L1, field=IB LL-DZ(0.3), content1=DFR I(ne)BZ(0.3), content2=DFR I(n)3(800.0), content3=DFR GZ(0.3), content4=DFR I(ne)B2, content5=IB LL-L3(800.0), content6=IB SPL-D2, crash=DFR I(F)3(800.0), content7=DFR I(F)B3(800.0), subid=IB LL-L1}, locale=de_CH, timezone=Canada/Saskatchewan
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestAssertions, TestNumericTokenStream, TestSimpleAttributeImpl, TestImpersonation, TestPulsingReuse, TestDocument, TestAddIndexes, TestAtomicUpdate, TestByteSlices, TestCheckIndex, TestConcurrentMergeScheduler, TestConsistentFieldNumbers, TestCrashCausesCorruptIndex, TestDocCount, TestDocumentWriter, TestFlex, TestForceMergeForever, TestIndexInput, TestIndexReader, TestIndexWriterConfig, TestIndexWriterExceptions]
    [junit] NOTE: Linux 3.0.0-14-generic amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=8,threads=1,free=186661872,total=245104640
{noformat}
"
1,"org.apache.lucene.search.BooleanQuery$TooManyClauses when using '>' operatorwhen using a query with a '>' operator, the query engine does not scale with number of matching properties and a org.apache.lucene.search.BooleanQuery$TooManyClauses exception is thrown"
1,SQL2 ISDESCENDANTNODE can throw BooleanQuery#TooManyClauses if there are too many matching child nodesRunning a query that has a ISDESCENDANTNODE clause can easily go over the max clause limit from lucene's BooleanQuery when there's a bigger hierarchy involved.
1,"Bundle Persistence Manager error - failing to read bundle the first timeCode:
NodeIterator entiter = null;
Node root = null, contNode = null, entsNode = null;

try
{
    root = session.getRootNode();
    contNode = root.getNode(""sr:cont"");
    entsNode = contNode.getNode(""sr:ents"");
    entiter = entsNode.getNodes();
}
catch (Exception e)
{
    logger.error(""Getting ents nodes"", e);
}

Output:
12359 [http-8080-Processor24] ERROR org.apache.jackrabbit.core.persistence.bundle.BundleDbPersistenceManager - failed to read bundle: c3a09c19-cc6b-45bd-a42e-c4c925b67d02: java.io.IOException: ERROR 40XD0: Container has been closed
12375 [http-8080-Processor24] ERROR com.taxila.editor.sm.RepoOperations - Getting ents nodes
javax.jcr.PathNotFoundException: sr:ents
    at org.apache.jackrabbit.core.NodeImpl.getNode(NodeImpl.java:2435)
    at com.taxila.editor.sm.RepoOperations.getEntityNodes (RepoOperations.java:4583)
    at com.taxila.editor.sm.RepoOperations.displayEntities(RepoOperations.java:4159)
    at com.taxila.editor.sm.RepoOperations.displayEntities(RepoOperations.java:4114)
    at com.taxila.editor.em.um.MainEntityForm.reset (MainEntityForm.java:215)
    at org.apache.struts.taglib.html.FormTag.doStartTag(FormTag.java:640)
    at org.apache.jsp.pages.jsp.entity.MainEntity_jsp._jspService(MainEntity_jsp.java:414)
    at org.apache.jasper.runtime.HttpJspBase.service (HttpJspBase.java:97)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
    at org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:332)
    at org.apache.jasper.servlet.JspServlet.serviceJspFile (JspServlet.java:314)
    at org.apache.jasper.servlet.JspServlet.service(JspServlet.java:264)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter (ApplicationFilterChain.java:252)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:173)
    at org.apache.catalina.core.ApplicationDispatcher.invoke(ApplicationDispatcher.java :672)
    at org.apache.catalina.core.ApplicationDispatcher.processRequest(ApplicationDispatcher.java:463)
    at org.apache.catalina.core.ApplicationDispatcher.doForward(ApplicationDispatcher.java:398)
    at org.apache.catalina.core.ApplicationDispatcher.forward (ApplicationDispatcher.java:301)
    at org.apache.struts.action.RequestProcessor.doForward(RequestProcessor.java:1014)
    at org.apache.struts.action.RequestProcessor.processForwardConfig(RequestProcessor.java:417)
    at org.apache.struts.action.RequestProcessor.processActionForward(RequestProcessor.java:390)
    at org.apache.struts.action.RequestProcessor.process(RequestProcessor.java:271)
    at org.apache.struts.action.ActionServlet.process (ActionServlet.java:1292)
    at org.apache.struts.action.ActionServlet.doPost(ActionServlet.java:510)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:709)
    at javax.servlet.http.HttpServlet.service (HttpServlet.java:802)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:252)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java :173)
    at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:213)
    at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:178)
    at org.apache.catalina.core.StandardHostValve.invoke (StandardHostValve.java:126)
    at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:105)
    at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:107)
    at org.apache.catalina.connector.CoyoteAdapter.service (CoyoteAdapter.java:148)
    at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:869)
    at org.apache.coyote.http11.Http11BaseProtocol$Http11ConnectionHandler.processConnection(Http11BaseProtocol.java :664)
    at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(PoolTcpEndpoint.java:527)
    at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt(LeaderFollowerWorkerThread.java:80)
    at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run (ThreadPool.java:684)
    at java.lang.Thread.run(Unknown Source)

On the other hand if I do this:
Code:
try
{
    root = session.getRootNode ();
    contNode = root.getNode(""sr:cont"");
    entsNode = contNode.getNode(""sr:ents"");
    entiter = entsNode.getNodes();
}
catch (Exception e)
{
    logger.error(""Getting ents nodes"", e);
    try
    {
        entsNode = contNode.getNode(""sr:ents"");
        entiter = entsNode.getNodes();
    }
    catch (Exception e1)
    {
        e1.printStackTrace();
    }
}

Output:
The first error as in the previous case comes, but the second execution of the entsNode = contNode.getNode(""sr:ents""); statement returns the right node, and hence the iterator."
1,"Not configuring the adminId, anonymousId, or defaultuserId causes login module to ignore credentialsUsing the DefaultLoginModule, DefaultAccessManager, and DefaultSecurityManager and calling Repository.login(Credentials) causes the following stack trace to be thrown.  

javax.jcr.LoginException: LoginModule ignored Credentials: LoginModule ignored Credentials: LoginModule ignored Credentials
	at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1353)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:53)
	at com.cerner.system.configuration.repository.jcr.JackrabbitTest.testLoginWithCredentials(JackrabbitTest.java:23)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at org.junit.internal.runners.TestMethod.invoke(TestMethod.java:59)
	at org.junit.internal.runners.MethodRoadie.runTestMethod(MethodRoadie.java:98)
	at org.junit.internal.runners.MethodRoadie$2.run(MethodRoadie.java:79)
	at org.junit.internal.runners.MethodRoadie.runBeforesThenTestThenAfters(MethodRoadie.java:87)
	at org.junit.internal.runners.MethodRoadie.runTest(MethodRoadie.java:77)
	at org.junit.internal.runners.MethodRoadie.run(MethodRoadie.java:42)
	at org.junit.internal.runners.JUnit4ClassRunner.invokeTestMethod(JUnit4ClassRunner.java:88)
	at org.junit.internal.runners.JUnit4ClassRunner.runMethods(JUnit4ClassRunner.java:51)
	at org.junit.internal.runners.JUnit4ClassRunner$1.run(JUnit4ClassRunner.java:44)
	at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:27)
	at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:37)
	at org.junit.internal.runners.JUnit4ClassRunner.run(JUnit4ClassRunner.java:42)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:45)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)
Caused by: javax.security.auth.login.FailedLoginException: LoginModule ignored Credentials
	at org.apache.jackrabbit.core.security.authentication.LocalAuthContext.login(LocalAuthContext.java:73)
	at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1346)
	... 24 more
javax.security.auth.login.FailedLoginException: LoginModule ignored Credentials
	at org.apache.jackrabbit.core.security.authentication.LocalAuthContext.login(LocalAuthContext.java:73)
	at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1346)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:53)
	at com.cerner.system.configuration.repository.jcr.JackrabbitTest.testLoginWithCredentials(JackrabbitTest.java:23)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at org.junit.internal.runners.TestMethod.invoke(TestMethod.java:59)
	at org.junit.internal.runners.MethodRoadie.runTestMethod(MethodRoadie.java:98)
	at org.junit.internal.runners.MethodRoadie$2.run(MethodRoadie.java:79)
	at org.junit.internal.runners.MethodRoadie.runBeforesThenTestThenAfters(MethodRoadie.java:87)
	at org.junit.internal.runners.MethodRoadie.runTest(MethodRoadie.java:77)
	at org.junit.internal.runners.MethodRoadie.run(MethodRoadie.java:42)
	at org.junit.internal.runners.JUnit4ClassRunner.invokeTestMethod(JUnit4ClassRunner.java:88)
	at org.junit.internal.runners.JUnit4ClassRunner.runMethods(JUnit4ClassRunner.java:51)
	at org.junit.internal.runners.JUnit4ClassRunner$1.run(JUnit4ClassRunner.java:44)
	at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:27)
	at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:37)
	at org.junit.internal.runners.JUnit4ClassRunner.run(JUnit4ClassRunner.java:42)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:45)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)

A testcase and repository.xml file will be attached shortly."
1,"errors in text filters can cause indexing to fail without warning the clienti've opened this issue to track the discussion at <http://thread.gmane.org/gmane.comp.apache.jackrabbit.devel/5086>. briefly, exceptions thrown by text filters are logged and swallowed by jackrabbit; there's no way for a text filter to signal to the jcr client that indexing failed.

some solutions have been proposed, including throwing an unchecked exception, which doesn't allow jackrabbit to maintain transactional integrity, and giving filters veto power over the observed repository operation. depending on the difficulty of the solution that is eventually determined to be correct, it may be sufficient for 1.0 to document the issue and perhaps improve the warning/error logging.
"
1,"Jcr-Server: BasicCredentialsProviderTest throws NPE if defaultAuthHeader init param misses the passwordissue reported by dominique jaeggi:

a missing-auth-header init param that has the form ""uid"" instead of ""uid:pw"" or ""uid:"" results in NPE upon SimpleCredentials creation.



"
1,"Query Builder and jcr:deref problem. Can't add predicate after jcr:derefCannot add a predicate (like [@property = 'value'] after a jcr:deref function.
The query builder throws an ""InvalidQueryException: Unsupported location for jcr:deref()"".

So for example, the query :

//element(*,nt:category)[@member]/jcr:deref(@member, '*')[@property='value'] 

is invalid and it should be valid.

"
1,"Connection with the proxy is not reopened if an proxy auth failure occurs while SSL tunnel is being establishedConnection with the proxy is not reopened if an proxy auth failure occurs while
SSL tunnel is being established.

This problem has been reported by on the httpclient-user by Gebhard Gaukler
<gebhard.gaukler at db.com>.

My bad.

Oleg"
1,NPE in event polling threadThis exception occurs when running the jcr2dav integration tests. This surfaces as a  side effect of JCR-3046. The root cause is refresh(Event) not guarding against null values returned from Event.getItemId().
1,"ConstantScoreRangeQuery - fixes ""too many clauses"" exceptionConstantScoreQuery wraps a filter (representing a set of documents) and returns
a constant score for each document in the set.

ConstantScoreRangeQuery implements a RangeQuery that works for any number of
terms in the range.  It rewrites to a ConstantScoreQuery that wraps a RangeFilter.

Still needed:
  - unit tests (these classes have been tested and work fine in-house, but the
current tests rely on too much application specific code)
  - code review of Weight() implementation (I'm unsure If I got all the score
normalization stuff right)
  - explain() implementation

NOTE: requires Java 1.4 for BitSet.nextSetBit()"
1,"DistanceFilter problem with deleted documentsI know this is the locallucene lib, but wanted to make sure we don't get this bug when it gets into lucene contrib.

I suspect that the issue is that deleted documents are trying to be evaluated by the filter.  I did some debugging and I confirmed that it is bombing on a document that is marked as deleted (using Luke).


Thanks!

Using the locallucene library 1.51, I get a NullPointerException at line 123 of DistanceFilter
The method is 	public BitSet bits(IndexReader reader) 
The line is double x = NumberUtils.SortableStr2double(sx);

The stack trace is:
java.lang.NullPointerException
	at org.apache.solr.util.NumberUtils.SortableStr2long(NumberUtils.java:149)
	at org.apache.solr.util.NumberUtils.SortableStr2double(NumberUtils.java:104)
	at com.pjaol.search.geo.utils.DistanceFilter.bits(DistanceFilter.java:123)
	at org.apache.lucene.search.Filter.getDocIdSet(Filter.java:49)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:140)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:112)
	at org.apache.lucene.search.Hits.getMoreDocs(Hits.java:113)
	at org.apache.lucene.search.Hits.<init>(Hits.java:90)
	at org.apache.lucene.search.Searcher.search(Searcher.java:72)"
1,"Directory#copy leaks file handlesDirectory#copy doesn't close the target directories output stream if sourceDir.openInput(srcFile) throws an Exception. Before LUCENE-3218 Directory#copy wasn't used extensively so this wasn't likely to happen during tests. Today we had a failure on the 3.x branch that is likely caused by this bug:

{noformat}
[junit] Testsuite: org.apache.lucene.index.TestAddIndexes
    [junit] Testcase: testAddIndexesWithRollback(org.apache.lucene.index.TestAddIndexes):	Caused an ERROR
    [junit] MockDirectoryWrapper: cannot close: there are still open files: {_co.cfs=1}
    [junit] java.lang.RuntimeException: MockDirectoryWrapper: cannot close: there are still open files: {_co.cfs=1}
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:483)
    [junit] 	at org.apache.lucene.index.TestAddIndexes$RunAddIndexesThreads.closeDir(TestAddIndexes.java:693)
    [junit] 	at org.apache.lucene.index.TestAddIndexes.testAddIndexesWithRollback(TestAddIndexes.java:924)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1277)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1195)
    [junit] Caused by: java.lang.RuntimeException: unclosed IndexOutput: _co.cfs
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.addFileHandle(MockDirectoryWrapper.java:410)
    [junit] 	at org.apache.lucene.store.MockCompoundFileDirectoryWrapper.<init>(MockCompoundFileDirectoryWrapper.java:39)
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.createCompoundOutput(MockDirectoryWrapper.java:439)
    [junit] 	at org.apache.lucene.index.SegmentMerger.createCompoundFile(SegmentMerger.java:128)
    [junit] 	at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:3101)
    [junit] 	at org.apache.lucene.index.TestAddIndexes$CommitAndAddIndexes3.doBody(TestAddIndexes.java:839)
    [junit] 	at org.apache.lucene.index.TestAddIndexes$RunAddIndexesThreads$1.run(TestAddIndexes.java:667)
    [junit] 
    [junit] 
    [junit] Tests run: 18, Failures: 0, Errors: 1, Time elapsed: 9.034 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] java.lang.IllegalStateException: CFS has pending open files
    [junit] 	at org.apache.lucene.store.CompoundFileWriter.close(CompoundFileWriter.java:143)
    [junit] 	at org.apache.lucene.store.CompoundFileDirectory.close(CompoundFileDirectory.java:181)
    [junit] 	at org.apache.lucene.store.DefaultCompoundFileDirectory.close(DefaultCompoundFileDirectory.java:58)
    [junit] 	at org.apache.lucene.store.MockCompoundFileDirectoryWrapper.close(MockCompoundFileDirectoryWrapper.java:55)
    [junit] 	at org.apache.lucene.index.SegmentMerger.createCompoundFile(SegmentMerger.java:139)
    [junit] 	at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:3101)
    [junit] 	at org.apache.lucene.index.TestAddIndexes$CommitAndAddIndexes3.doBody(TestAddIndexes.java:839)
    [junit] 	at org.apache.lucene.index.TestAddIndexes$RunAddIndexesThreads$1.run(TestAddIndexes.java:667)
{noformat}"
1,"While you could use a custom Sort Comparator source with remote searchable before, you can no longer do so with FieldComparatorSourceFieldComparatorSource is not serializable, but can live on a SortField"
1,"MultiReader.numDocs incorrect after undeleteAllCalling MultiReader.undeleteAll does not clear cached numDocs value. So the subsequent numDocs() call returns a wrong value if there were deleted documents in the index. Following patch fixes the bug and adds a test showing the issue.


Index: src/test/org/apache/lucene/index/TestMultiReader.java
===================================================================
--- src/test/org/apache/lucene/index/TestMultiReader.java       (revision 354923)
+++ src/test/org/apache/lucene/index/TestMultiReader.java       (working copy)
@@ -69,6 +69,18 @@
     assertTrue(vector != null);
     TestSegmentReader.checkNorms(reader);
   }
+
+  public void testUndeleteAll() throws IOException {
+    sis.read(dir);
+    MultiReader reader = new MultiReader(dir, sis, false, readers);
+    assertTrue(reader != null);
+    assertEquals( 2, reader.numDocs() );
+    reader.delete(0);
+    assertEquals( 1, reader.numDocs() );
+    reader.undeleteAll();
+    assertEquals( 2, reader.numDocs() );
+  }
+

   public void testTermVectors() {
     MultiReader reader = new MultiReader(dir, sis, false, readers);
Index: src/java/org/apache/lucene/index/MultiReader.java
===================================================================
--- src/java/org/apache/lucene/index/MultiReader.java   (revision 354923)
+++ src/java/org/apache/lucene/index/MultiReader.java   (working copy)
@@ -122,6 +122,7 @@
     for (int i = 0; i < subReaders.length; i++)
       subReaders[i].undeleteAll();
     hasDeletions = false;
+    numDocs = -1;      // invalidate cache
   }

   private int readerIndex(int n) {    // find reader for doc n:"
1,"PayloadTermQuery's explain is broken when span score is not includedWhen setting includeSpanScore to false with PayloadTermQuery, the explain is broken."
1,"recoverable exceptions when reading are not retriedIf a recoverable exception occurs after a request is written then the method is
not retried."
1,"Embedded Derby fails under JBoss because of JMX-related conflictsJBoss fails to start due to a bug in Derby-10.4.2.0. The dependency should be agains derby-10.4.2.1 which seems to has this bug fixed. More info at https://issues.apache.org/jira/browse/DERBY-3887

Please, include this fix in the upcoming 1.6.3"
1,"creating empty field + empty term leads to invalid indexSpinoff from LUCENE-3526.

* if you create new Field("""", """"), you get IllegalArgumentException from Field's ctor: ""name and value cannot both be empty""
* But there are tons of other ways to index an empty term for the empty field (for example initially make it ""garbage"" then .setValue(""""), or via tokenstream).
* If you do this, and you have assertions enabled, you will trip an assert (the assert is fixed in trunk, in LUCENE-3526)
* But If you don't have assertions enabled, you will create a corrupt index: test: terms, freq, prox...ERROR [term : docFreq=1 != num docs seen 0 + num docs deleted 0]
"
1,"wrong charset indication in HttpConstants.getContentString()Around line 236 in HttpConstants.getConstentString() the charset is wrongly indicated as 
""DEFAULT_CONTENT_CHARSET"" where it should have been indicated as ""charset"" like in the 
getContentBytes function.

            if (LOG.isWarnEnabled()) {
                LOG.warn(""Unsupported encoding: "" 
                    + DEFAULT_CONTENT_CHARSET // <== should be the variable ""charset"" here
                    + "". Default HTTP encoding used"");
            }

Wrong copy/paste I guess :-)

ZC."
1,"NTLM Proxy and basic host authorizationUsing a Microsoft proxy with NTLM validation enabled the authorization against a
remote host does not work. This, of course, assuming that the page is correctly
fetched (which currently is not), see the NTLM authentication bug number 24327"
1,"basetokenstreamtestcase should fail if tokenstream starts with posinc=0This is meaningless for a tokenstream to start with posinc=0,

Its also caused problems and hairiness in the indexer (LUCENE-1255, LUCENE-1542),
and it makes senseless tokenstreams. We should add a check and fix any that do this.

Furthermore the same bug can exist in removing-filters if they have enablePositionIncrements=false.
I think this option is useful: but it shouldnt mean 'allow broken tokenstream', it just means we
don't add gaps. 

If you remove tokens with enablePositionIncrements=false it should not cause the TS to start with
positionincrement=0, and it shouldnt 'restructure' the tokenstream (e.g. moving synonyms on top of a different word).
It should just not add any 'holes'.
"
1,"DocumentWriter closes TokenStreams too earlyThe DocumentWriter closes a TokenStream as soon as it has consumed its tokens. The javadoc of TokenStream.close() says that it releases resources associated with the stream. However, the DocumentWriter keeps references of the resources (i. e. payload byte arrays, term strings) until it writes the postings to the new segment, which means that DocumentWriter should call TokenStream.close() after it has written the postings.

This problem occurs in multithreaded applications where e. g. pooling is used for the resources. My patch adds a new test to TestPayloads which shows this problem. Multiple threads add documents with payloads to an index and use a pool of byte arrays for the payloads. TokenStream.close() puts the byte arrays back into the pool. The test fails with the old version but runs successfully with the patched version. 

All other units tests pass as well.
"
1,"Contrib query org.apache.lucene.search.BoostingQuery sets boost on constructor Query, not cloned copyBoostingQuery sets the boost value on the passed context Query

    public BoostingQuery(Query match, Query context, float boost) {
      this.match = match;
      this.context = (Query)context.clone();        // clone before boost
      this.boost = boost;

      context.setBoost(0.0f);                      // ignore context-only matches
    }

This should be 
      this.context.setBoost(0.0f);                      // ignore context-only matches

Also, boost value of 0.0 may have wrong effect - see discussion at

http://www.mail-archive.com/java-user@lucene.apache.org/msg12243.html 

"
1,NRTCachingDirectory.deleteFile always throws exceptionSilly bug.
1,"Binary field content lost during optimizeScenario:

* create an index with arbitrary content, and close it
* open IndexWriter again, and add a document with binary field (stored but not compressed)
* close IndexWriter _without_ optimizing, so that the new document is in a separate segment.
* open IndexReader. You can read the last document and its binary field just fine.
* open IndexWriter, optimize the index, close IndexWriter
* open IndexReader. Now the field is still present (not null) and is marked as binary, but the data is not there - Field.getBinaryLength() returns 0.
"
1,"fix reverseStringFilter for unicode 4.0ReverseStringFilter is not aware of supplementary characters: when it reverses it will create unpaired surrogates, which will be replaced by U+FFFD by the indexer (but not at query time).
The wrong words will conflate to each other, and the right words won't match, basically the whole thing falls apart.

This patch implements in-place reverse with the algorithm from apache harmony AbstractStringBuilder.reverse0()
"
1,"charset in Content-Type header shouldn't be in quotesThe charset value in the Content-Type header returned from IOUtil.buildContentType is enclosed in quotes. This value should be a token which does not include double quotes.

Index: C:/jprojects/eclipse/jackrabbit/jcr-server/server/src/java/org/apache/jackrabbit/server/io/IOUtil.java
===================================================================
--- C:/jprojects/eclipse/jackrabbit/jcr-server/server/src/java/org/apache/jackrabbit/server/io/IOUtil.java	(revision 397215)
+++ C:/jprojects/eclipse/jackrabbit/jcr-server/server/src/java/org/apache/jackrabbit/server/io/IOUtil.java	(working copy)
@@ -112,7 +112,7 @@
     public static String buildContentType(String mimeType, String encoding) {
         String contentType = mimeType;
         if (contentType != null && encoding != null) {
-            contentType += ""; charset=\"""" + encoding + ""\"""";
+            contentType += ""; charset="" + encoding;
         }
         return contentType;
     }
"
1,"HttpClient:- Connections not released when SSL Tunneling fails.Trying to use HTTPS, and SSL tunneling fails as expected because the host is not accepted by the squid proxy, so squid proxy return 403. 

The problem I am seeing is that, when ever this happens the connections are not released to the pool. I traced the code and it appears that in 
HttpMethidDirector.java:  executeWithRetry()
when executeConnect() return false and there is no retry, the connections are not released.

Is this expected? Or am I doing something wrong."
1,"Error downloading text file with gzip content encodingHello I am getting an exception when I try to download certain files.

I don't have control over the host server, only the client.  Here's my client code:

		HttpParams params = new BasicHttpParams();
		params.setParameter(CoreConnectionPNames.CONNECTION_TIMEOUT, 300000L);
		params.setParameter(ClientPNames.HANDLE_REDIRECTS, true);

		// This client indicates to servers that it will support 'gzip'
		// and 'deflate' compressed responses.
		ContentEncodingHttpClient.setDefaultHttpParams(params);
		ContentEncodingHttpClient client = new ContentEncodingHttpClient();

		if (user != null && password != null) {
			String hostname = url.getHost();
			HttpHost hostHttp = new HttpHost(hostname, 80, ""http"");
			HttpHost hostHttps = new HttpHost(hostname, 443, ""https"");
			client.getCredentialsProvider().setCredentials(
			        new AuthScope(hostname, 80), 
			        new UsernamePasswordCredentials(user, password));
	
			client.getCredentialsProvider().setCredentials(
			        new AuthScope(hostname, 443), 
			        new UsernamePasswordCredentials(user, password));
	
			// Create AuthCache instance
			AuthCache authCache = new BasicAuthCache();
			// Generate BASIC scheme object and add it to the local auth cache
			BasicScheme basicAuth = new BasicScheme();
			authCache.put(hostHttp, basicAuth);
			authCache.put(hostHttps, basicAuth);
	
			// Add AuthCache to the execution context
			BasicHttpContext localcontext = new BasicHttpContext();
			localcontext.setAttribute(ClientContext.AUTH_CACHE, authCache);
		}
		HttpGet httpget = new HttpGet(url.toString());
		httpget.setHeader(""If-Modified-Since"", lastModified);


		HttpResponse response = client.execute(httpget);
		responseCode = response.getStatusLine().getStatusCode();
		HttpEntity entity = response.getEntity();
		if (responseCode == HttpStatus.SC_NOT_MODIFIED) {
			
		} else if (responseCode == HttpStatus.SC_OK && entity != null) {
			outStream = new BufferedOutputStream(new FileOutputStream(outFilename));
			entity.writeTo(outStream);
		}

Here's the log output:

DEBUG [2011-08-02 01:23:01,031] [org.apache.http.impl.conn.SingleClientConnManager:212] Get connection for route HttpRoute[{}->http://<host>]
DEBUG [2011-08-02 01:23:01,036] [org.apache.http.impl.conn.DefaultClientConnectionOperator:145] Connecting to <host>/<IP>:80
DEBUG [2011-08-02 01:23:01,057] [org.apache.http.client.protocol.RequestAddCookies:132] CookieSpec selected: best-match
DEBUG [2011-08-02 01:23:01,057] [org.apache.http.client.protocol.RequestAuthCache:75]   Auth cache not set in the context
DEBUG [2011-08-02 01:23:01,058] [org.apache.http.impl.client.DefaultRequestDirector:631]        Attempt 1 to execute request
DEBUG [2011-08-02 01:23:01,058] [org.apache.http.impl.conn.DefaultClientConnection:264] Sending request: GET <file> HTTP/1.1
DEBUG [2011-08-02 01:23:01,058] [org.apache.http.impl.conn.Wire:63]     >> ""GET <file> HTTP/1.1[\r][\n]""
DEBUG [2011-08-02 01:23:01,058] [org.apache.http.impl.conn.Wire:63]     >> ""If-Modified-Since: Mon, 01 Aug 2011 18:26:09 CEST[\r][\n]""
DEBUG [2011-08-02 01:23:01,059] [org.apache.http.impl.conn.Wire:63]     >> ""Host: <host>[\r][\n]""
DEBUG [2011-08-02 01:23:01,059] [org.apache.http.impl.conn.Wire:63]     >> ""Connection: Keep-Alive[\r][\n]""
DEBUG [2011-08-02 01:23:01,059] [org.apache.http.impl.conn.Wire:63]     >> ""User-Agent: Apache-HttpClient/4.1.1 (java 1.5)[\r][\n]""
DEBUG [2011-08-02 01:23:01,059] [org.apache.http.impl.conn.Wire:63]     >> ""Accept-Encoding: gzip,deflate[\r][\n]""
DEBUG [2011-08-02 01:23:01,060] [org.apache.http.impl.conn.Wire:63]     >> ""[\r][\n]""
DEBUG [2011-08-02 01:23:01,060] [org.apache.http.impl.conn.DefaultClientConnection:268] >> GET <file> HTTP/1.1
DEBUG [2011-08-02 01:23:01,060] [org.apache.http.impl.conn.DefaultClientConnection:271] >> If-Modified-Since: Mon, 01 Aug 2011 18:26:09 CEST
DEBUG [2011-08-02 01:23:01,060] [org.apache.http.impl.conn.DefaultClientConnection:271] >> Host: <host>
DEBUG [2011-08-02 01:23:01,061] [org.apache.http.impl.conn.DefaultClientConnection:271] >> Connection: Keep-Alive
DEBUG [2011-08-02 01:23:01,061] [org.apache.http.impl.conn.DefaultClientConnection:271] >> User-Agent: Apache-HttpClient/4.1.1 (java 1.5)
DEBUG [2011-08-02 01:23:01,061] [org.apache.http.impl.conn.DefaultClientConnection:271] >> Accept-Encoding: gzip,deflate
DEBUG [2011-08-02 01:23:01,085] [org.apache.http.impl.conn.Wire:63]     << ""HTTP/1.1 200 OK[\r][\n]""
DEBUG [2011-08-02 01:23:01,086] [org.apache.http.impl.conn.Wire:63]     << ""Server: nginx/0.8.54[\r][\n]""
DEBUG [2011-08-02 01:23:01,086] [org.apache.http.impl.conn.Wire:63]     << ""Date: Mon, 01 Aug 2011 23:23:01 GMT[\r][\n]""
DEBUG [2011-08-02 01:23:01,086] [org.apache.http.impl.conn.Wire:63]     << ""Content-Type: text/plain[\r][\n]""
DEBUG [2011-08-02 01:23:01,086] [org.apache.http.impl.conn.Wire:63]     << ""Last-Modified: Wed, 20 Jul 2011 14:39:57 GMT[\r][\n]""
DEBUG [2011-08-02 01:23:01,087] [org.apache.http.impl.conn.Wire:63]     << ""Transfer-Encoding: chunked[\r][\n]""
DEBUG [2011-08-02 01:23:01,087] [org.apache.http.impl.conn.Wire:63]     << ""Connection: keep-alive[\r][\n]""
DEBUG [2011-08-02 01:23:01,087] [org.apache.http.impl.conn.Wire:63]     << ""Vary: Accept-Encoding[\r][\n]""
DEBUG [2011-08-02 01:23:01,087] [org.apache.http.impl.conn.Wire:63]     << ""Expires: Wed, 31 Aug 2011 23:23:01 GMT[\r][\n]""
DEBUG [2011-08-02 01:23:01,088] [org.apache.http.impl.conn.Wire:63]     << ""Cache-Control: max-age=2592000[\r][\n]""
DEBUG [2011-08-02 01:23:01,088] [org.apache.http.impl.conn.Wire:63]     << ""Content-Encoding: gzip[\r][\n]""
DEBUG [2011-08-02 01:23:01,088] [org.apache.http.impl.conn.Wire:63]     << ""[\r][\n]""
DEBUG [2011-08-02 01:23:01,088] [org.apache.http.impl.conn.DefaultClientConnection:249] Receiving response: HTTP/1.1 200 OK
DEBUG [2011-08-02 01:23:01,089] [org.apache.http.impl.conn.DefaultClientConnection:252] << HTTP/1.1 200 OK
DEBUG [2011-08-02 01:23:01,089] [org.apache.http.impl.conn.DefaultClientConnection:255] << Server: nginx/0.8.54
DEBUG [2011-08-02 01:23:01,089] [org.apache.http.impl.conn.DefaultClientConnection:255] << Date: Mon, 01 Aug 2011 23:23:01 GMT
DEBUG [2011-08-02 01:23:01,089] [org.apache.http.impl.conn.DefaultClientConnection:255] << Content-Type: text/plain
DEBUG [2011-08-02 01:23:01,089] [org.apache.http.impl.conn.DefaultClientConnection:255] << Last-Modified: Wed, 20 Jul 2011 14:39:57 GMT
DEBUG [2011-08-02 01:23:01,090] [org.apache.http.impl.conn.DefaultClientConnection:255] << Transfer-Encoding: chunked
DEBUG [2011-08-02 01:23:01,090] [org.apache.http.impl.conn.DefaultClientConnection:255] << Connection: keep-alive
DEBUG [2011-08-02 01:23:01,090] [org.apache.http.impl.conn.DefaultClientConnection:255] << Vary: Accept-Encoding
DEBUG [2011-08-02 01:23:01,090] [org.apache.http.impl.conn.DefaultClientConnection:255] << Expires: Wed, 31 Aug 2011 23:23:01 GMT
DEBUG [2011-08-02 01:23:01,090] [org.apache.http.impl.conn.DefaultClientConnection:255] << Cache-Control: max-age=2592000
DEBUG [2011-08-02 01:23:01,091] [org.apache.http.impl.conn.DefaultClientConnection:255] << Content-Encoding: gzip
DEBUG [2011-08-02 01:23:01,091] [org.apache.http.impl.client.DefaultRequestDirector:477]        Connection can be kept alive indefinitely
DEBUG [2011-08-02 01:23:01,131] [org.apache.http.impl.conn.Wire:63]     << ""600a[\r][\n]""
DEBUG [2011-08-02 01:23:01,132] [org.apache.http.impl.conn.Wire:77]     << ""[0x1f]""
DEBUG [2011-08-02 01:23:03,838] [org.apache.http.impl.conn.Wire:63]     << ""[\r][\n]""

.... (Content)

DEBUG [2011-08-02 01:23:03,839] [org.apache.http.impl.conn.SingleClientConnManager:267] Releasing connection org.apache.http.impl.conn.SingleClientConnManager$ConnAdapter@2aa3873
DEBUG [2011-08-02 01:23:03,839] [org.apache.http.impl.conn.SingleClientConnManager:285] Released connection open but not reusable.
DEBUG [2011-08-02 01:23:03,839] [org.apache.http.impl.conn.DefaultClientConnection:152] Connection shut down
ERROR [2011-08-02 01:23:03,840] [app]        Exception downloading file
java.io.EOFException
        at java.util.zip.GZIPInputStream.readUByte(GZIPInputStream.java:224)
        at java.util.zip.GZIPInputStream.readUShort(GZIPInputStream.java:214)
        at java.util.zip.GZIPInputStream.readHeader(GZIPInputStream.java:153)
        at java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:75)
        at java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:85)
        at org.apache.http.client.entity.GzipDecompressingEntity.getContent(GzipDecompressingEntity.java:63)
        at org.apache.http.util.EntityUtils.consume(EntityUtils.java:65)
        at org.apache.http.conn.BasicManagedEntity.ensureConsumed(BasicManagedEntity.java:98)
        at org.apache.http.conn.BasicManagedEntity.writeTo(BasicManagedEntity.java:115)
        at util.FileDownload.download(FileDownload.java:188) <--- my app

Does this happen because the server doesn't specify the content length?"
1,"IndexWriter.setMaxMergeDocs gives non-backwards-compatible exception ""out of the box""Yonik hit this (see details in LUCENE-994): because we have switched
to LogByteSizeMergePolicy by default in IndexWriter, which uses MB to
limit max size of merges (setMaxMergeMB), when an existing app calls
setMaxMergeDocs (or getMaxMergeDocs) it will hit an
IllegalArgumentException on dropping in the new JAR.

I think the simplest solution is to fix LogByteSizeMergePolicy to
allow setting of the max by either MB or by doc count, just like how
in LUCENE-1007 allows flushing by either MB or docCount or both."
1,"Base64 bug - last buffer not flushedI found an issue when using the org.apache.jackrabbit.util.Base64.encode(InputStream in, OutputStream out) method. It appears that the issue is that the last buffer is not flushed on the Writer that it creates before returning from the method. I was able to work around this issue by creating a Writer my own program, and call another encode method, and then call the flush() method before using the data. The source in trunk appears the same.
"
1,"if you use setNorm, lucene writes a headerless separate norms fileIn this case SR.reWrite just writes the bytes with no header...
we should write it always.

we can detect in these cases (segment written <= 3.1) with a 
sketchy length == maxDoc check.
"
1,"MS Proxy with NTLM authentication set up does not workWhen I try to go via a MS Proxy which is set up with NTLM authentication I
always get a ""407"" error, no matter which credentials used."
1,JCR2SPI: incomplete changelog when combining move with removal of new destination parent
1,"Bug in duplicate mapping checkThere is a bug in the MappingDescriptor for checking if a mapping for a node type is already available. The following patch solves this problem:

Index: /Users/cziegeler/Developer/workspaces/default/jackrabbit/jackrabbit-ocm/src/main/java/org/apache/jackrabbit/ocm/mapper/model/MappingDescriptor.java
===================================================================
--- /Users/cziegeler/Developer/workspaces/default/jackrabbit/jackrabbit-ocm/src/main/java/org/apache/jackrabbit/ocm/mapper/model/MappingDescriptor.java	(revision 614136)
+++ /Users/cziegeler/Developer/workspaces/default/jackrabbit/jackrabbit-ocm/src/main/java/org/apache/jackrabbit/ocm/mapper/model/MappingDescriptor.java	(working copy)
@@ -75,7 +75,7 @@
         if (null != classDescriptor.getJcrType() && !  """".equals(classDescriptor.getJcrType()) && 
         		 ! ManagerConstant.NT_UNSTRUCTURED.equals(classDescriptor.getJcrType()))
         {
-        	if ((classDescriptorsByNodeType.get(classDescriptor.getClassName()) != null) &&
+        	if ((classDescriptorsByNodeType.get(classDescriptor.getJcrType()) != null) &&
         		classDescriptor.usesNodeTypePerConcreteClassStrategy()	)
         	{
         	    log.warn(""Duplicate classdescriptor for node type : "" + classDescriptor.getJcrType());	
"
1,"[SPI] Node.setProperty with null value throws ItemNotFoundExceptionNode.setProperty with a null value should not throw a ItemNotFoundException in the case a property with the given name does not exist. Rather should it return a stale property which throws an InvalidItemStateException when its methods are accessed. 

This behavior is also consistent with jackrabbit-core.
"
1,"highlighting exact phrase with overlapping tokens fails.Fields with overlapping token are not highlighted in search results when searching exact phrases, when using TermVector.WITH_OFFSET.

The document builded in MemoryIndex for highlight does not preserve positions of tokens in this case. Overlapping tokens get ""flattened"" (position increment always set to 1), the spanquery used for searching relevant fragment will fail to identify the correct token sequence because the position shift.

I corrected this by adding a position increment calculation in sub class StoredTokenStream. I added junit test covering this case.

I used the eclipse codestyle from trunk, but style add quite a few format differences between repository and working copy files. I tried to reduce them, but some linewrapping rules still doesn't match.

Correction patch joined"
1,rep:excerpt() may return malformed XMLThe rep:excerpt() function does not encode the prefined XML entities but writes them as is into the excerpt XML. This may produce malformed XML.
1,"StringIndexOutOfBound exception in RFC2109 cookie validate when host name contains no domain information and is short in length than the cookie domain.If the target server is identified by hostname only (no domain) and the domain
of the cookie is greater in length than the target hostname, a
StringIndexOutOfBoundsException occurs.

Offending line(s) of code: 174-176 in o.a.c.h.cookie.RFC2109Spec.java"
1,"HttpState.clearCookies() should be synchronizedThe HttpState class has a clearCookies method that is not synchronized but
should be considering it modifies an ArrayList (which is unsynchronized). All
other methods which modify or read from the ArrayList are synchronized except
the clearCookies method. 

I stumbled upon this fact because a webapp I am working on that uses HttpClient
threw an IllegalArgumentException indicating that one of the cookies in the
array returned from HttpState.getCookies() was null, which shouldn't be
possible.  Upon further inspection and testing, the only possible option is that
the threadsafety hole left by the unsynchronized clearCookies method caused the
issue."
1,"ConnectionTimeoutException doesn't releaseConnection()When a ConnectionTimeoutException is thrown, HttpConnection doesn't seem to
release the connection. Instead, the connection is properly released if an
InterruptedIOException is thrown.

This is the pattern I use:

Try {
     method.execute(...);
     method.getResponseBodyAsString();
 } catch (ConnectionTimeoutException cte) {
     ...
 } catch (InterruptedIOException ioe) {
     ...
 } finally {
     method.releaseConnection();
     LOG.info(""RELEASED"");   
 }

The following log shows that no actual release is performed, while the message
""RELEASED"" is logged.

10544  DEBUG [MainCheck2] httpclient.HttpConnection - enter
HttpConnection.isResponseAvailable(int)
10930  WARN  [MainCheck1] httpclient.HttpConnection - The host
www.pccomputing.com:80 (or proxy null:-1) did not accept the connection within
timeout of 3000 milliseconds
10931  WARN  [MainCheck1] CheckPerformer - Connection Timeout occurred..
org.apache.commons.httpclient.HttpConnection$ConnectionTimeoutException
at org.apache.commons.httpclient.HttpConnection.open(HttpConnection.java:659) 
...
at PersistenceCheck$MainCheck.run(PersistenceCheck.java:306)
10932  INFO  [MainCheck1] CheckPerformer - RELEASED

->Here no call to HttpConnection.releaseConnection() is performed. 

Thanks"
1,"Registering NodeType with defaultvalues fails with IndexOutOfBoundsWhen trying to register more than one nodetpye with default values I get the following exception:

Caused by: java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.toNodeTypeDef(NodeTypeManagerImpl.java:790)
	at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.registerNodeTypes(NodeTypeManagerImpl.java:560)

I assume there is an index missmatch in the implementation

                Value[] values = pdefs[i].getDefaultValues();
                if (values != null) {
                    InternalValue[] qvalues = new InternalValue[values.length];
                    for (int j = 0; j < values.length; j++) {
                        try {
-->                            qvalues[j] = InternalValue.create(values[i], session);
                        } catch (ValueFormatException e) {
                            throw new InvalidNodeTypeDefinitionException(
                                    ""Invalid default value format"", e);
                        }
                    }
                    qpdef.setDefaultValues(qvalues);
                }
"
1,"wordnet parsing bugA user reported that wordnet parses the prolog file incorrectly.

Also need to check the wordnet parser in the memory contrib for this problem.

If this is a false alarm, i'm not worried, because the test will be the first unit test wordnet package ever had.

{noformat}
For example, looking up the synsets for the
word ""king"", we get:

java SynLookup wnindex king
baron
magnate
mogul
power
queen
rex
scrofula
struma
tycoon

Here, ""scrofula"" and ""struma"" are extraneous. This happens because, the line
parser code in Syns2Index.java interpretes the two consecutive single quotes
in entry s(114144247,3,'king''s evil',n,1,1) in  wn_s.pl file, as
termination
of the string and separates into ""king"". This entry concerns
synset of words ""scrofula"" and ""struma"", and thus they get inserted in the
synset of ""king"". *There 1382 such entries, in wn_s.pl* and more in other
WordNet
Prolog data-base files, where such use of two consecutive single quotes
appears.

We have resolved this by adding a statement in the line parsing portion of
Syns2Index.java, as follows:

           // parse line
           line = line.substring(2);
          * line = line.replaceAll(""\'\'"", ""`""); // added statement*
           int comma = line.indexOf(',');
           String num = line.substring(0, comma);  ... ... etc.
In short we replace ""''"" by ""`"" (a back-quote). Then on recreating the
index, we get:

java SynLookup zwnindex king
baron
magnate
mogul
power
queen
rex
tycoon
{noformat}"
1,"TestNRTThreads test failurehit a fail in TestNRTThreads running tests over and over:
"
1,"Spatial checks for a string in an int,double map{code}
  private Map<Integer,Double> distances;
{code}

{code}
    if (precise != null) {
      double xLat = getPrecision(lat, precise);
      double xLng = getPrecision(lng, precise);
      
      String k = new Double(xLat).toString() +"",""+ new Double(xLng).toString();
    
      Double d = (distances.get(k));
      if (d != null){
        return d.doubleValue();
      }
    }
{code}

Something is off here eh?"
1,"Cookie with domain .mydomain.com not sent to host mydomain.comA cookie with for example 
  .mydomain.com 
as domain property is not sent to the host
  mydomain.com
(without www. or anything else before ""mydomain.com"")

This concern all CookieSpec as the relevant code is located in CookieSpecBase:

    public boolean domainMatch(final String host, final String domain) {
        return host.endsWith(domain);
    }

It should be changed for instance to something like:

    public boolean domainMatch(final String host, final String domain) {
        // take care of host ""myDomain.com"" and domain "".myDomain.com""
        return host.endsWith(domain)
	|| _host.equals(_domain.substring(1));
    }"
1,"cache returns cached responses even if validators not consistent with all conditional headersThis is a MUST-level requirement in the RFC, where if both ETags and Last-Modified dates are used as validators in a conditional request, a cache cannot return a cached response unless it is consistent with all the conditional headers in the request. There is a unit test for this already, but it is incorrect (it uses 'If-Unmodified-Since' instead of 'If-Modified-Since' in the test case).

"
1,"An HTTP ""204 NO CONTENT"" response results in dropped connectionAfter receiving a ""204 NO CONTENT"" response, HttpClient always closes the 
connection.

This did not happen in earlier versions and appears to have been caused by a 
recent fix to bug# 34262."
1,"NPE in NearSpansUnordered from PayloadNearQueryThe following query causes a NPE in NearSpansUnordered, and is reproducible with the the attached unit test. The failure occurs on the last document scored.
"
1,"PropertyState binary type desirialsation only returns half of contentCreate a PropertyState for a binary Property (e.g jcr:data) set a value larger than the BLOBFileValues#MAX_BUFFER_SIZE  (e.g. 300Kbyte) serialse it.
On deserialisation the resulting PropertyState's InternalValue's size is only half as the origianl (e.g. 150Kbyte)

Most probably this is due to the States InputStream implementation marking bytes twice to be read.
Following fix solves the issue for call to #read(byte[], in, int),
but other Stream methods may fail as well.

Index: jackrabbit/java/org/apache/jackrabbit/core/state/PropertyState.java
===================================================================
--- jackrabbit/java/org/apache/jackrabbit/core/state/PropertyState.java (revision 399293)
+++ jackrabbit/java/org/apache/jackrabbit/core/state/PropertyState.java (working  copy)
@@ -305,7 +305,6 @@
                                 len = (int) (length - consumed);
                             }
                             int read = super.read(b, off, len);
-                            consumed += read;
                             return read;
                         }
"
1,"XMLPersistenceManager incorrectly handles propertiesJCR Property instances are written by the XMLPersistenceManager as java.util.Properties files and loaded through the loadPropertyState() method as Properties files. Unfortunately the reload() method tries to re-load the Property states from XML files, which is not possible."
1,"Importing strings with special characters failsBoth Session.importXML and Workspace.importXML don't work correctly in some cases.

Importing very large foreign language (for example, Chinese) text property values could result in incorrect values on some platforms. The reason is, BufferedStringValue (buffers very large string to a temporary file) uses the platform default encoding to read and write the text.

BufferedStringValue is relatively slow on some systems when importing large texts or binary data because of using FD().sync().

If an exported string value contains a carriage return (\r), this character was truncated on some platforms.

If an exported string value contains a characters with code below 32 excluding newline (\n) and tab (\t) - for example form feed (\f) - the imported string value was base64 encoded.
"
1,"XMLPersistanceManager doesn't preserve a property's 'multiValued' attributewhen a multi-valued property is persisted and later read using the XMLPersistenceManager the 'multiValued' attribute is lost, i.e. PropertyState.isMultiValued() returns always false."
1,"FieldsReader does not regard offset and position flagsWhen creating a Field the FieldsReader looks at the storeTermVector flag of the FieldInfo. If true Field.TermVector.YES is used as parameter. But it should be checked if storeOffsetWithTermVector and storePositionWithTermVector are set and Field.TermVector.WITH_OFFSETS, ...WITH_POSITIONS, or ...WITH_POSITIONS_OFFSETS should be used as appropriate."
1,"Writers on two machines over NFS can hit FNFE due to stale NFS client cachingIssue spawned from this thread:

  http://www.gossamer-threads.com/lists/lucene/java-user/50680

When IndexFileDeleter lists the directory, looking for segments_X
files to load, if it hits a FNFE on opening such a file it should
catch this and treat it as if the file does not exist.

On NFS (and possibly other file systems), a directory listing is not
guaranteed to be ""current""/coherent.  Specifically, if machine #1 has
just removed file ""segments_n"" and shortly thereafer machine #2 does a
dir listing, it's possible (likely?) that the dir listing will still
show that segments_n exists.

I think the fix is simple: catch the FNFE and just handle it as if the
segments_n does not in fact exist.

"
1,"entity returns the same stream for getContent()BasicHttpEntity and GzipDecompressingEntity will return the same stream
when getContent() is called multiple times. That is not allowed by the
HttpEntity interface. They should rather throw an IllegalStateException.

Some tests and EntityUtils rely on getContent to return the same stream
for multiple calls.

patch follows,
  Roland"
1,"Unusual Http status lineThe web server at http://alces.med.umn.edu/Candida.html returns the following
status line:

HTTP 200 Document follows

This page loads in the 3 browsers I tried (though Safari actually rendered the
headers).  The current version of HttpClient reads through the whole page
looking for a line that starts with HTTP/.  I don't know how big of a problem
this is, but it's a fairly easy fix.  Patch to follow."
1,"DefaultHttpMethodRetryHandler does not check whether the failed method has been abortedDefaultHttpMethodRetryHandler does not check whether the failed method has been
aborted."
1,"Buffered deletes under count RAMI found this while working on LUCENE-2548: when we freeze the deletes (create FrozenBufferedDeletes), when we set the bytesUsed we are failing to account for RAM required for the term bytes (and now term field)."
1,"Request is retried if preemptive authentication failsHello,

I'm using premptive authentification from an Axis client using BASIC Http
authentification. When the user isn't authenticated/authorized by server (in my
case, credentials are expired), httpclient runs a ""Chalenge"" that produces a
second request to server with same credentials.

when using preemptive mode, chalenge should be skipped if authentication scheme
hasn't changed !"
1,"DatabaseJournal assigns same revision id to different revisionsRunning a transaction that updates multiple workspaces (e.g. a versioning operation) will fail in DatabaseJournal, because every individual update will ultimately be assigned the same revision id. An indication of this failure when e.g. using Oracle as backend for journaling will look as follows::

java.sql.SQLException: ORA-00001: unique constraint (JOURNAL_IDX) violated
 at oracle.jdbc.dbaccess.DBError.throwSqlException(DBError.java:134)
 at oracle.jdbc.ttc7.TTIoer.processError(TTIoer.java:289)
 at oracle.jdbc.ttc7.Oall7.receive(Oall7.java:590)
 at oracle.jdbc.ttc7.TTC7Protocol.doOall7(TTC7Protocol.java:1973)
 at oracle.jdbc.ttc7.TTC7Protocol.executeFetch(TTC7Protocol.java:977)
 at oracle.jdbc.driver.OracleStatement.executeNonQuery(OracleStatement.java:2205)
 at oracle.jdbc.driver.OracleStatement.doExecuteOther(OracleStatement.java:2064)
 at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:2989)
 at oracle.jdbc.driver.OraclePreparedStatement.executeUpdate(OraclePreparedStatement.java:658)
 at oracle.jdbc.driver.OraclePreparedStatement.execute(OraclePreparedStatement.java:736)
 at org.apache.jackrabbit.core.journal.DatabaseJournal.append(DatabaseJournal.java:293)
 ... 24 more

This bug has been reported by Rafa Kwiecie."
1,"SegmentReader.hasSeparateNorms always returns falseThe loop in that method looks like this: 
 
for(int i = 0; i < 0; i++){ 
 
I guess ""i < 0"" should be replaced by ""i < result.length""?"
1,"Cluster information is not persisted to database when connected to case sensitive MS SQL Server 2005After a call to Session::save, we observed that cluster information was not written to the ${schemaObjectPrefix}JOURNAL and ${schemaObjectPrefix}GLOBAL_REVISION tables. We tested against Oracle 10 database servers and MS Sql Server 2005 servers. The problem was noticed only with MS Sql Server 2005. 

Initially, the problem was masked since the test was written as part of our unit test environment and the exceptions generated by JDBC were not showing up in the logs. A separate test with was carried out as shown by the code below

<pre>
import java.io.FileInputStream;

import javax.jcr.Node;
import javax.jcr.Repository;
import javax.jcr.Session;
import javax.jcr.SimpleCredentials;

import org.apache.jackrabbit.core.TransientRepository;
import org.apache.jackrabbit.core.config.RepositoryConfig;

public class Main
{
    public static void main(String[] args)
        throws Exception
    {
        System.setProperty(""org.apache.jackrabbit.core.cluster.node_id"", ""testid"");
        
        RepositoryConfig config = RepositoryConfig.create(new FileInputStream(""repository.xml""), ""repository"");
        
        Repository repository = new TransientRepository();
        
        Session session = repository.login(new SimpleCredentials(""username"", ""password"".toCharArray()));
        
        Node root = session.getRootNode();
        
        root.addNode(""node1"");
        root.addNode(""node2"");
        root.addNode(""node3"");
        
        session.save();
    }
}
</pre>

The configuration file used to configure the repository is attached.

After debugging this, we obtained the exceptions that were previously not visible. Note that, JackRabbit continues to run (is that because the cluster code is running in a separate thread?) even after this exception. The problem was that the 'revision_id' field did not exist. The mssql.ddl schema file sets up the table names in capitals. However, at least two of the SQL statements in DatabaseJournal use lower case table names. For example:-

<pre>
        updateGlobalStmt = con.prepareStatement(
                ""update "" + schemaObjectPrefix + ""global_revision "" +
                ""set revision_id = revision_id + 1"");
        selectGlobalStmt = con.prepareStatement(
                ""select revision_id "" +
                ""from "" + schemaObjectPrefix + ""global_revision"");
</pre>

An additional error is that the mssql.ddl file is missing the following:

<pre>
# Inserting the one and only revision counter record now helps avoiding race conditions
insert into ${schemaObjectPrefix}GLOBAL_REVISION VALUES(0)
</pre>

Fixing the above two issues, fixed the problem with MS SQL Server 2005."
1,"HttpClient enter 100% for endless timeI was working masively using HttpClient (I was testing it for usage within a 
server) and it got to 100% CPU for an endless time.

I was querying urls of the type 
http://search.barnesandnoble.com/booksearch/results.asp?WRD=<text>&sort=R&SAT=1

To reproduce it, run 100-200 urls with random words instead of <text> and 
you'll probably reproduce the problem."
1,"SegmentReader.setNorm can fail to remove separate norms file, on Windows
While working through LUCENE-710 I hit this bug: on Windows
only, when SegmentReader.setNorm is called, but separate norms
(_X_N.sY) had already been previously saved, then, on closing the
reader, we will write the next gen separate norm file correctly
(_X_N+1.sY) but fail to delete the current one.

It's quite minor because the next writer to touch the index will
remove the stale file.

This is because the Norm class still holds the IndexInput open when
the reader commits."
1,"HttpMethodBase#aborted variable mistakenly declared transient instead of volatileHttpMethodBase#aborted variable mistakenly declared transient instead of volatile. This is quite nasty. 

Do we want to cut an emergency release (3.0.2) because of that or can this wait until 3.1-beta1?

Fix attached.

Oleg"
1,"ShingleMatrixFilter eaily throws StackOverFlow as the complexity of a matrix growsShingleMatrixFilter#next makes a recursive function invocation when the current permutation iterator is exhausted or if the current state of the permutation iterator already has produced an identical shingle. In a not too complex matrix this will require a gigabyte sized stack per thread.

My solution is to avoid the recursive invocation by refactoring like this:

{code:java}
public Token next(final Token reusableToken) throws IOException {
    assert reusableToken != null;
    if (matrix == null) {
      matrix = new Matrix();
      // fill matrix with maximumShingleSize columns
      while (matrix.columns.size() < maximumShingleSize && readColumn()) {
        // this loop looks ugly
      }
    }

    // this loop exists in order to avoid recursive calls to the next method
    // as the complexity of a large matrix
    // then would require a multi gigabyte sized stack.
    Token token;
    do {
      token = produceNextToken(reusableToken);
    } while (token == request_next_token);
    return token;
  }

  
  private static final Token request_next_token = new Token();

  /**
   * This method exists in order to avoid reursive calls to the method
   * as the complexity of a fairlt small matrix then easily would require
   * a gigabyte sized stack per thread.
   *
   * @param reusableToken
   * @return null if exhausted, instance request_next_token if one more call is required for an answer, or instance parameter resuableToken.
   * @throws IOException
   */
  private Token produceNextToken(final Token reusableToken) throws IOException {

{code}

"
1,".toString on empty MultiPhraseQuery hits NPERoss Woolf hit this on java-user thread ""MultiPhraseQuery.toString() throws null pointer exception"".  It's still present on trunk..."
1,"Registering a Nodetype based on an existing NodeType failIf I create a new NodeTypeTemplate using the code show below,

           NodeTypeManagerImpl ntm = (NodeTypeManagerImpl) session.getWorkspace().getNodeTypeManager();
           NodeTypeDefinition nt = (NodeTypeDefinition) ntm.getNodeType(""wr:entity"");
           NodeTypeTemplate ntt = ntm.createNodeTypeTemplate(nt);

the list of declaredSuperType contains the same name of the original nodeType (repeted twice) and not the declaredSuperType of the original nodeType (in this example [nt:base, nt:file])

          ntt.getDeclaredSupertypeNames(); -> [wr:entity, wr:entity]"
1,"After IW.addIndexesNoOptimize, IW.close may hangSpinoff from here:

  http://mail-archives.apache.org/mod_mbox/lucene-java-user/200804.mbox/%3c43128.192.168.1.71.1208561409.webmail@192.168.1.71%3e

The addIndexesNoOptimize method first merges eligible segments
according to the MergePolicy, and then copies over one by one any
remaining ""external"" segments.

That copy can possibly (rather rarely) result in new merges becoming
eligible because its size can change if the index being added was
created with autoCommit=false.

However, we fail to then invoke the MergeScheduler to run these
merges.  As a result, in close, where we wait until all running and
pending merges complete, we will never return.

The fix is simple: invoke the merge scheduler inside
copyExternalSegments() if any segments were copied.  I also added
defensive invocation of the merge scheduler during close, just in case
other code paths could allow for a merge to be added to the pending
queue but not scheduled.

"
1,"Constructor of IndexWriter let's runtime exceptions pop up, while keeping the writeLock obtainedConstructor of IndexWriter let's runtime exceptions pop up, while keeping the writeLock obtained.

The init method in IndexWriter catches IOException only (I got NegativeArraySize by reading up a _corrupt_ index), and now, there is no way to recover, since the writeLock will be kept obtained. Moreover, I don't have IndexWriter instance either, to ""grab"" the lock somehow, since the init() method is called from IndexWriter constructor.

Either broaden the catch to all exceptions, or at least provide some circumvention to clear up. In my case, I'd like to ""fallback"", just delete the corrupted index from disk and recreate it, but it is impossible, since the LOCK_HELD NativeFSLockFactory's entry about obtained WriteLock is _never_ cleaned out and is no (at least apparent) way to clean it out forcibly. I can't create new IndexWriter, since it will always fail with LockObtainFailedException."
1,"URIUtils.extractHost(...) throws a NumberFormatException line 310Original Jboss-seam-wicket-booking application in Jboss-4.2.3.GA started, post a login request thanks httpclient, then NumberFormatException.



regarding this page :
http://hc.apache.org/httpcomponents-client-dev/httpclient/clover/org/apache/http/client/utils/URIUtils.html

305 	   	// Extract the port suffix, if present
306 	   	if (host != null) {
307 	  	   int colon = host.indexOf(':');
308 	   	   if (colon >= 0) {
309 	   	      if (colon+1 < host.length()) {
310 	   	          port = Integer.parseInt(host.substring(colon+1));
311 	   	      }
312 	  	   host = host.substring(0,colon);
313 	   	   }
314 	   	}

resolving the port throw a NumberFormatException

java.lang.NumberFormatException: For input string: ""8080;jsessionid=9E9EDA0B6E1CDD499A0A15C4A8F212D8""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)
	at java.lang.Integer.parseInt(Integer.java:458)
	at java.lang.Integer.parseInt(Integer.java:499)
	at org.apache.http.client.utils.URIUtils.extractHost(URIUtils.java:310)
	at org.apache.http.impl.client.AbstractHttpClient.determineTarget(AbstractHttpClient.java:764)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:754)
	at org.tagbrowser.api.TagBrowser.request(TagBrowser.java:109)


another case of this problem canbe found hier :
https://gitorious.org/yacy/rc1/commit/8b0920b0b5eb67ae17eec24c1bf3a059543cb6e8/diffs"
1,"FSDirectory.copy() impl is unsafeThere are a couple of issues with it:

# FileChannel.transferFrom documents that it may not copy the number of bytes requested, however we don't check the return value. So need to fix the code to read in a loop until all bytes were copied..
# When calling addIndexes() w/ very large segments (few hundred MBs in size), I ran into the following exception (Java 1.6 -- Java 1.5's exception was cryptic):
{code}
Exception in thread ""main"" java.io.IOException: Map failed
    at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:770)
    at sun.nio.ch.FileChannelImpl.transferToTrustedChannel(FileChannelImpl.java:450)
    at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:523)
    at org.apache.lucene.store.FSDirectory.copy(FSDirectory.java:450)
    at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:3019)
Caused by: java.lang.OutOfMemoryError: Map failed
    at sun.nio.ch.FileChannelImpl.map0(Native Method)
    at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:767)
    ... 7 more
{code}

I changed the impl to something like this:
{code}
long numWritten = 0;
long numToWrite = input.size();
long bufSize = 1 << 26;
while (numWritten < numToWrite) {
  numWritten += output.transferFrom(input, numWritten, bufSize);
}
{code}

And the code successfully adds the indexes. This code uses chunks of 64MB, however that might be too large for some applications, so we definitely need a smaller one. The question is how small so that performance won't be affected, and it'd be great if we can let it be configurable, however since that API is called by other API, such as addIndexes, not sure it's easily controllable.

Also, I read somewhere (can't remember now where) that on Linux the native impl is better and does copy in chunks. So perhaps we should make a Linux specific impl?"
1,"Problems with BundleDbPersistenceManager getAllNodeIdsWhen using MySQL:
The problem arises when the method parameter maxcount is less than the total amount of records in the bundle table.

First of all I found out that mysql orders the nodeid objects different than jackrabbit does. The following test describes this idea:

    public void testMySQLOrderByNodeId() throws Exception {
        NodeId nodeId1 = new NodeId(""7ff9e87c-f87f-4d35-9d61-2e298e56ac37"");
        NodeId nodeId2 = new NodeId(""9fd0d452-b5d0-426b-8a0f-bef830ba0495"");

        PreparedStatement stmt = connection.prepareStatement(""SELECT NODE_ID FROM DEFAULT_BUNDLE WHERE NODE_ID = ? OR NODE_ID = ? ORDER BY NODE_ID"");

        Object[] params = new Object[] { nodeId1.getRawBytes(), nodeId2.getRawBytes() };
        stmt.setObject(1, params[0]);
        stmt.setObject(2, params[1]);

        ArrayList<NodeId> nodeIds = new ArrayList<NodeId>();
        ResultSet resultSet = stmt.executeQuery();
        while(resultSet.next()) {
            NodeId nodeId = new NodeId(resultSet.getBytes(1));
            System.out.println(nodeId);
            nodeIds.add(nodeId);
        }
        Collections.sort(nodeIds);
        for (NodeId nodeId : nodeIds) {
            System.out.println(nodeId);
        }
    }

Which results in the following output:

7ff9e87c-f87f-4d35-9d61-2e298e56ac37
9fd0d452-b5d0-426b-8a0f-bef830ba0495
9fd0d452-b5d0-426b-8a0f-bef830ba0495
7ff9e87c-f87f-4d35-9d61-2e298e56ac37


Now the problem with the getAllNodeIds method is that it fetches an extra 10 records on top of maxcount (to avoid a problem where the first key is not the one you that is wanted). Afterwards it skips a number of records again, this time using nodeid.compareto. This compareto statement returns true unexpectedly for mysql because the code doesn't expect the mysql ordering.

I had the situation where I had about 17000 records in the bundle table but consecutively getting the ids a thousand records at a time returned only about 8000 records in all.
"
1,"SloppyPhraseScorer sometimes computes Infinite freqreported on user list:
http://www.lucidimagination.com/search/document/400cbc528ed63db9/score_of_infinity_on_dismax_query
"
1,"Problems with File Copy using WebDAVWhen i make a copy of files from one workspace to other (CTRL-C -> CTRL-V). The file isnt copied, but the original file is deleted."
1,"NodeTypeDefDiff compares to restrictiveThe NodeTypeDefDiff class is used to compare NodeTypeDef instances. Unfortunately this class reports two NodeTypeDef instances which are not equal but have no structural difference as having trivial changes. The correct result would be to have no modification at all.

I suggest to modify the NodeTypeDefDiff.init() method such, that the initial type is ""NONE"" instead of ""TRIVIAL"" and to first compare the ""hasOrderableChildNodes"" first and raise the level to ""TRIVIAL"" if not equal. Next the rest of the current comparisons would follow."
1,"Use of java.net.URI.resolve() is buggyThe use of java.net.URI.resolve() is buggy (see <http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4708535>). Affected class: org.apache.http.impl.client.DefaultRedirectHandler.

Proposed solution: Create a resolve(URI, String) method in o.a.h.client.utils.URLUtils."
1,"IndexReader.isCurrent() lies if documents were only removed by latest commitUsecase is as following:

1. Get indexReader via indexWriter.
2. Delete documents by Term via indexWriter. 
3. Commit indexWriter.
4. indexReader.isCurrent() returns true.

Usually there is a check if index reader is current. If not then it is reopened (re-obtained via writer or ect.). But this causes the problem when documents can still be found through the search after deletion.
Testcase is attached."
1,"DisjunctionSumScorer gives slightly (float iotas) different scores when you .nextDoc vs .advanceSpinoff from LUCENE-1536.

I dug into why we hit a score diff when using luceneutil to benchmark
the patch.

At first I thought it was BS1/BS2 difference, but because of a bug in
the patch it was still using BS2 (but should be BS1) -- Robert's last
patch fixes that.

But it's actually a diff in BS2 itself, whether you next or advance
through the docs.

It's because DisjunctionSumScorer, when summing the float scores for a
given doc that matches multiple sub-scorers, might sum in a different
order, when you had .nextDoc'd to that doc than when you had .advance'd
to it.

This in turn is because the PQ used by that scorer (ScorerDocQueue)
makes no effort to break ties.  So, when the top N scorers are on the
same doc, the PQ doesn't care what order they are in.

Fixing ScorerDocQueue to break ties will likely be a non-trivial perf
hit, though, so I'm not sure whether we should do anything here..."
1,"TermSpans skipTo() doesn't always move forwardsIn TermSpans (or the anonymous Spans class returned by SpansTermQuery, depending on the version), the skipTo() method is improperly implemented if the target doc is less than or equal to the current doc:

  public boolean skipTo(int target) throws IOException {
          // are we already at the correct position?
          if (doc >= target) {
            return true;
          }

          ...


This violates the correct behavior (as described in the Spans interface documentation), that skipTo() should always move forwards, in other words the correct implementation would be:

if (doc >= target) {
  return next();
}

This bug causes particular problems if one wants to use the payloads feature - this is because if one loads a payload, then performs a skipTo() to the same document, then tries to load the ""next"" payload, the spans hasn't changed position and it attempts to load the same payload again (which is an error)."
1,FastVectorHighlighter: highlighted term is out of alignment in multi-valued NOT_ANALYZED field
1,"Highlighting overlapping tokens outputs doubled wordsIf for the text ""the fox did not jump"" we generate following tokens :
(the, 0, 0-3),({fox},0,0-7),(fox,1,4-7),(did,2,8-11),(not,3,12,15),(jump,4,16,18)

If TermVector for field is stored WITH_OFFSETS and not WITH_POSITIONS_OFFSETS, highlighing would output
""the<em>the fox</em> did not jump""

I join a patch with 2 additive JUnit tests and a fix of TokenSources class where token ordering by offset did'nt manage well overlapping tokens.
"
1,"Data Store: DB2 fails to create the tableDB2 throws an exception(1) when creating the table. The correct SQL sentence to create it is:
createTable=CREATE TABLE ${tablePrefix}${table}(ID VARCHAR(255) PRIMARY KEY NOT NULL, LENGTH BIGINT, LAST_MODIFIED BIGINT, DATA BLOB(1000M))
(1): Sorry but I don't have the exception information since I made this change a few weeks ago."
1,"Locking two same-name siblings and unlocking first apparently unlocks second instead.Executing the following test that unlocks the first of two locked same-name siblings:

public void testLocking() throws RepositoryException {
       Session jcrSession = ((S1SessionImpl) session).getSession();
       Node rootNode = jcrSession.getRootNode();

       Node n1 = rootNode.addNode(""path"");
       n1.addMixin(""mix:lockable"");
       Node n2 = rootNode.addNode(""path"");
       n2.addMixin(""mix:lockable"");

       jcrSession.save();

       n1.lock(true, true);
       n2.lock(true, true);

       System.out.println(""n1.isLocked() = "" + n1.isLocked());
       System.out.println(""n2.isLocked() = "" + n2.isLocked());
       assertTrue(n1.isLocked());
       assertTrue(n2.isLocked());

       n1.save();
       n1.unlock();

       System.out.println(""n1.isLocked() = "" + n1.isLocked());
       System.out.println(""n2.isLocked() = "" + n2.isLocked());
       assertFalse(n1.isLocked());
       assertTrue(n2.isLocked());
   }

Results in:

n1.isLocked() = true
n2.isLocked() = true
n1.isLocked() = true
n2.isLocked() = false

which is wrong."
1,"MultiSearcher.explain returns incorrect score/explanation relating to docFreqCreating 2 different indexes, searching  each individually and print score details and compare to searching both indexes with MulitSearcher and printing score details.  
 
The ""docFreq"" value printed isn't correct - the values it prints are as if each index was searched individually.

Code is like:
{code}
MultiSearcher multi = new MultiSearcher(searchables);
Hits hits = multi.search(query);
for(int i=0; i<hits.length(); i++)
{
  Explanation expl = multi.explain(query, hits.id(i));
  System.out.println(expl.toString());
}
{code}

I raised this in the Lucene user mailing list and was advised to log a bug, email thread given below.

{noformat} 
-----Original Message-----
From: Chris Hostetter  
Sent: Friday, December 07, 2007 10:30 PM
To: java-user
Subject: Re: does the MultiSearcher class calculate IDF properly?


a quick glance at the code seems to indicate that MultiSearcher has code 
for calcuating the docFreq accross all of the Searchables when searching 
(or when the docFreq method is explicitly called) but that explain method 
just delegates to Searchable that the specific docid came from.

if you compare that Explanation score you got with the score returned by 
a HitCollector (or TopDocs) they probably won't match.

So i would say ""yes MultiSearcher calculates IDF properly, but 
MultiSeracher.explain is broken.  Please file a bug about this, i can't 
think of an easy way to fix it, but it certianly seems broken to me.


: Subject: does the MultiSearcher class calculate IDF properly?
: 
: I tried the following.  Creating 2 different indexes, search each
: individually and print score details and compare to searching both
: indexes with MulitSearcher and printing score details.  
: 
: The ""docFreq"" value printed don't seem right - is this just a problem
: with using Explain together with the MultiSearcher?
: 
: 
: Code is like:
: MultiSearcher multi = new MultiSearcher(searchables);
: Hits hits = multi.search(query);
: for(int i=0; i<hits.length(); i++)
: {
:   Explanation expl = multi.explain(query, hits.id(i));
:   System.out.println(expl.toString());
: }
: 
: 
: Output:
: id = 14 score = 0.071
: 0.07073946 = (MATCH) fieldWeight(contents:climate in 2), product of:
:   1.0 = tf(termFreq(contents:climate)=1)
:   1.8109303 = idf(docFreq=1)
:   0.0390625 = fieldNorm(field=contents, doc=2)
{noformat} "
1,"Garbage data when reading a compressed, text field, lazilylazy compressed text fields is a case that was neglected during lazy field implementation.  TestCase and patch provided.

"
1,"PrecedenceQueryParser misinterprets queries starting with NOT""NOT foo AND baz"" is parsed as ""-(+foo +baz)"" instead of ""-foo +bar"".

(I'm setting parser.setDefaultOperator(PrecedenceQueryParser.AND_OPERATOR) but the issue applies otherwise too.)
"
1,"While indexing Turkish web pages, ""Parse Aborted: Lexical error...."" occursWhen I try to index Turkish page if there is a Turkish specific character in the HTML specific tag HTML parser gives ""Parse Aborted: Lexical error.on ... line"" error.
For this case ""<IMG SRC=""../images/head.jpg"" WIDTH=570 HEIGHT=47 BORDER=0 ALT="""">"" exception address """" character (which has 351 ascii value) as an error. OR  character in title tag.
<a title=""()"">

Turkish character in the content do not create any problem."
1,"PathElement.equals doesn't handle INDEX_UNDEFINEDPathElement (and therefore Path) comparisons fail when INDEX_UNDEFINED is used (it's treated differently from INDEX_DEFAULT).
"
1,"Deadlock in acl.EntryCollector / ItemManagerHere's another three-way deadlock that we've encountered:

* Thread A holds a downgraded SISM write lock and is about to start delivering observation events to synchronous listeners
* Thread B wants to write something and blocks waiting for the SISM write lock (since A holds the lock)
* Thread C wants to read something and blocks waiting for the SISM read lock (since B waits for the lock)

Normally such a scenario is handled without any problems, but there's a problem if the session used by thread C has a synchronous observation listener that attempts to read something from the repository during event delivery. In this case the following can happen:

* Thread C holds the ItemManager synchronization lock higher up in the call chain
* Observation listener code called by thread A attempts to read something from the repository, and blocks trying to acquire the ItemManager synchronization lock (since C holds it)

In principle such a scenario should never happen as an observation listener (much less a synchronous one) should never try to use the session that might already be in use by another thread.

Unfortunately the EntryCollector class in o.a.j.core.security.authorization.acl does not follow this guideline, which leads to the deadlock as shown below:

Thread A:
""127.0.0.1 [1297191119365] POST /bin/wcmcommand HTTP/1.0"" nid=1179 state=BLOCKED
    - waiting on <0x11c329fd> (a org.apache.jackrabbit.core.ItemManager)
    - locked <0x11c329fd> (a org.apache.jackrabbit.core.ItemManager)
     owned by 127.0.0.1 [1297191138443] POST /bin/wcmcommand HTTP/1.0 id=67
    at org.apache.jackrabbit.core.ItemManager.getNode(ItemManager.java:653)
    at org.apache.jackrabbit.core.ItemManager.getNode(ItemManager.java:605)
    at org.apache.jackrabbit.core.SessionImpl.getNode(SessionImpl.java:1406)
    at org.apache.jackrabbit.core.security.authorization.acl.EntryCollector.onEvent(EntryCollector.java:253)
    at org.apache.jackrabbit.core.observation.EventConsumer.consumeEvents(EventConsumer.java:246)
    at org.apache.jackrabbit.core.observation.ObservationDispatcher.dispatchEvents(ObservationDispatcher.java:214)
    at org.apache.jackrabbit.core.observation.EventStateCollection.dispatch(EventStateCollection.java:475)
    at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:786)
    at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:1488)
    at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:349)
    at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:354)
    at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:324)
    at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:328)
    at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1141)
    at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:920)

Thread B:
""Thread-2438"" nid=2582 state=WAITING
    - waiting on <0x166e790e> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock)
    - locked <0x166e790e> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock)
    at java.lang.Object.wait(Native Method)
    at java.lang.Object.wait(Object.java:485)
    at EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock.acquire(Unknown Source)
    at org.apache.jackrabbit.core.state.DefaultISMLocking$WriteLockImpl.<init>(DefaultISMLocking.java:76)
    at org.apache.jackrabbit.core.state.DefaultISMLocking$WriteLockImpl.<init>(DefaultISMLocking.java:70)
    at org.apache.jackrabbit.core.state.DefaultISMLocking.acquireWriteLock(DefaultISMLocking.java:64)
    at org.apache.jackrabbit.core.state.SharedItemStateManager.acquireWriteLock(SharedItemStateManager.java:1808)
    at org.apache.jackrabbit.core.state.SharedItemStateManager.access$200(SharedItemStateManager.java:112)
    at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:565)
    at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:1458)
    at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:1488)
    at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:349)
    at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:354)
    at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:324)
    at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:328)
    at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1141)
    at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:920)

Thread C:
""127.0.0.1 [1297191138443] POST /bin/wcmcommand HTTP/1.0"" nid=67 state=WAITING
    - waiting on <0xf820edb> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$ReaderLock)
    - locked <0xf820edb> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$ReaderLock)
    at java.lang.Object.wait(Native Method)
    at java.lang.Object.wait(Object.java:485)
    at EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$ReaderLock.acquire(Unknown Source)
    at org.apache.jackrabbit.core.state.DefaultISMLocking$ReadLockImpl.<init>(DefaultISMLocking.java:102)
    at org.apache.jackrabbit.core.state.DefaultISMLocking$ReadLockImpl.<init>(DefaultISMLocking.java:96)
    at org.apache.jackrabbit.core.state.DefaultISMLocking.acquireReadLock(DefaultISMLocking.java:53)
    at org.apache.jackrabbit.core.state.SharedItemStateManager.acquireReadLock(SharedItemStateManager.java:1794)
    at org.apache.jackrabbit.core.state.SharedItemStateManager.getItemState(SharedItemStateManager.java:257)
    at org.apache.jackrabbit.core.state.LocalItemStateManager.getNodeState(LocalItemStateManager.java:107)
    at org.apache.jackrabbit.core.state.LocalItemStateManager.getItemState(LocalItemStateManager.java:171)
    at org.apache.jackrabbit.core.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:200)
    at org.apache.jackrabbit.core.ItemManager.getItemData(ItemManager.java:391)
    at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:337)
    at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:638)
    at org.apache.jackrabbit.core.security.authorization.acl.ACLProvider$AclPermissions.canRead(ACLProvider.java:507)
      - locked java.lang.Object@6ad9b475
    at org.apache.jackrabbit.core.security.DefaultAccessManager.canRead(DefaultAccessManager.java:251)
    at org.apache.jackrabbit.core.query.lucene.QueryResultImpl.isAccessGranted(QueryResultImpl.java:374)
    at org.apache.jackrabbit.core.query.lucene.QueryResultImpl.collectScoreNodes(QueryResultImpl.java:353)
    at org.apache.jackrabbit.core.query.lucene.QueryResultImpl.getResults(QueryResultImpl.java:310)
    at org.apache.jackrabbit.core.query.lucene.SingleColumnQueryResult.<init>(SingleColumnQueryResult.java:70)
    at org.apache.jackrabbit.core.query.lucene.QueryImpl.execute(QueryImpl.java:133)
    at org.apache.jackrabbit.core.query.QueryImpl.execute(QueryImpl.java:127)
"
1,"Error reading dataHi,

I have some problems with HttpClient HEAD. It works fine with a build of 
20020720 of HttpClient though.

It seems HttpClient is not reading correctly the returned HTTP response.

I'm attaching the logs.

Here is the output from Cactus build:



     [java]     [junit] Testcase: testLongProcess took 3.645 sec
     [java]     [junit]         Caused an ERROR
     [java]     [junit] Failed to get the test results. This is probably due 
to an error that happen
ed on the server side when trying to execute the tests. Here is what was 
returned by the server : [<
html><head><Long Process></head><body>Some data</body></html>
     [java]     [junit] ]
     [java]     [junit] org.apache.cactus.util.ChainedRuntimeException: Failed 
to get the test resul
ts. This is probably due to an error that happened on the server side when 
trying to execute the tes
ts. Here is what was returned by the server : [<html><head><Long 
Process></head><body>Some data</bod
y></html>
     [java]     [junit] ]
     [java]     [junit]         at 
org.apache.cactus.client.AbstractHttpClient.doTest(Unknown Source
)
     [java]     [junit]         at 
org.apache.cactus.AbstractWebTestCase.runWebTest(Unknown Source)
     [java]     [junit]         at 
org.apache.cactus.AbstractWebTestCase.runGenericTest(Unknown Sour
ce)
     [java]     [junit]         at org.apache.cactus.ServletTestCase.runTest
(Unknown Source)
     [java]     [junit]         at org.apache.cactus.AbstractTestCase.runBare
(Unknown Source)
     [java]     [junit] org.apache.cactus.client.ParsingException: Not a valid 
response. First 100 c
haracters of the reponse: [</webresult>HTTP/1.1 200 OK
     [java]     [junit] Server: Resin/2.1.2
     [java]     [junit] Content-Length: 23
     [java]     [junit] Date: Tue, 13 Aug 2002 08:45:2]
     [java]     [junit]         at 
org.apache.cactus.client.WebTestResultParser.readExceptionClassna
me(Unknown Source)
     [java]     [junit]         at 
org.apache.cactus.client.WebTestResultParser.parse(Unknown Source

Thanks
-Vincent"
1,"Observation tests failPath returned by Event.getPath() is wrong. It always returns the path to the parent node connected to the event. That is, if e.g. a node /foo/bar is created the path /foo is returned instead of /foo/bar.

This issue had been introduced with changed from api version 0.14 to 0.15."
1,"Bundle of events may be dropped due to NP.In [1], if the resolver fails to lookup a node entry, a NP is thrown. This exception will break the loop which forwards the events to the observer in [2].
This will result in an observer not receiving events that he should have.

[1] org.apache.jackrabbit.jcr2spi.hierarchy.HierarchyManagerImpl#lookup(ItemId workspaceItemId)
[2] org.apache.jackrabbit.jcr2spi.WorkspaceManager#onEventReceived(EventBundle[] eventBundles,InternalEventListener[] lstnrs)"
1,"An IOException or RuntimeException leaves the underlying socket in an undetermined stateIf an application level IOException or RuntimeException occurs, the underlying
socket will be in an undetermined state. In many cases, this will lead to zombie
connections in the pool that do not respond properly.

Simple example: uploading a file via POST. If we promise the server 1MB of data.
Shortly after starting the transfer an IOException occurs (e.g. the NFS server
the file was residing on stops responding). The connection is returned to the
pool (see HTTPCLIENT-302) but the the server is still expecting close to 1MB of data
on that socket. The next request on that socket (e.g. a GET) will send the HTTP
header but  the server thinks the header is part of the old stream and doesn't
respond."
1,"BlockJoinQuery advance fails on an assert in case of a single parent with child segmentThe BlockJoinQuery will fail on an assert when advance in called on a segment with a single parent with a child. The call to parentBits.prevSetBit(parentTarget - 1) will cause -1 to be returned, and the assert will fail, though its valid. Just removing the assert fixes the problem, since nextDoc will handle it properly.

Also, I don't understand the ""assert parentTarget != 0;"", with a comment of each parent must have one child. There isn't really a reason to add this constraint, as far as I can tell..., just call nextDoc in this case, no?"
1,"Benchmark package uses new TopFieldCollector but also still uses AUTO without resolving it - result is, our sort algorithms won't runAUTO does not work with TopFieldCollector. If you want to use AUTO with TopFieldCollector, we have a convienence method called detectType on SortField, but it is package protected and so cannot be used here as a stop gap or by users if they wanted to mix AUTO with TopFieldCollector. Lucene does still handle this for back compat internally. Solr got bit here when it was switched to use TopFieldCollector - no auto resolution was added (detectType help couldn't have been used due to visibility), and the result was that plugin code that used to be able to use AUTO would now blow up. You shouldn't use AUTO in Solr anyway though.

The Benchmark package got bit as well  when it moved to TopFieldCollector. Sort algorithms allowed auto if you specified it, or if you left off the type. Now our sort algs fail because they didn't specify a type.

I'll change to require the type to be specified to get the algs working again. I was thinking of just putting auto resolution in as a stop gap till 3.0 (when auto is removed), but since detectFieldType is package protected and I don't want to repeat it, disallowing auto seems the best way to go."
1,"Oracle bundle PM fails checking schema if 2 users use the same databaseWhen using the OracleBundlePersistenceManager there is an issue when two users use the same database for persistence. In  that case, the checkSchema() method of the BundleDbPersistenceManager  does not work like it should. More precisely, the call ""metaData.getTables(null,  null, tableName, null);"" will also includes table names of other  schemas/users. Effectively, only the first user of a database is able to create  the schema.

probably same issue as here: JCR-582"
1,"Benchmark alg line -  {[AddDoc(4000)]: 4} : * - causes an infinite loopBackground in http://www.mail-archive.com/java-dev@lucene.apache.org/msg10831.html 
The line  
   {[AddDoc(4000)]: 4} : * 
causes an infinite loop because the parallel sequence would mask the exhaustion from the outer sequential sequence.

To fix this the DocMaker exhaustion check should be modified to rely  on the doc maker instance only, and to be reset when the inputs are being reset. "
1,"spi2dav: ItemInfoCache causes failure of (Workspace)RestoreTest#testRestoreWithUUIDConflict and variantswhile running the API version tests i found the (Workspace)RestoreTest.testRestoreWithUUIDConfict and variants failing. to be precise the test passes but
transiently removing the versionableNode2 in the teardown fails upon removal of the jcr:uuid property of the moved childnode.

having a closer look at it revealed that the problem is caused in the WorkspaceItemStateFactory where the property entry is retrieved from the
cache and subsequently checking if the path really matches fails. for test purposes i prevented the usage of the cached entry by returning false in WorkspaceItemStateFactory.isUpToDate  => the tests passed. 

as far as i know the same tests pass with spi2jcr.
michael, could it be that this is caused by a flaw in the iteminfo-cache logic? or is there something specific that needs to be adjusted in spi2dav?"
1,"non-contiguous LogMergePolicy should be careful to not select merges already runningNow that LogMP can do non-contiguous merges, the fact that it disregards which segments are already being merged is more problematic since it could result in it returning conflicting merges and thus failing to run multiple merges concurrently.
"
1,"Parsing built-in CND and XML nodetypes does not result in equal nt-definitionsi created a test in order to make sure builtin-nodetypes.xml and builtin-nodetypes.cnd provide the same definitions (actually i only wanted to test my own changes).

it reveals that the existing built-in NodeTypeDefinitions are not equal due to the following reason:

- in the xml-format nt:base is always specified if no other super type extends from nt:base
- in the cnd notation the nt:base is omitted (see below for quote from appendix of jsr 283) even if other super type(s) are
  defined and none of them extends from nt:base.

this affects the following nodetypes (all extending from mix:referenceable only):

nt:versionHistory
nt:version
nt:frozenNode
nt:resource


quote from public-review of jsr 283:

""7.2.2.4 Supertypes [...]
After the node type name comes the optional list of supertypes. If this element is not present and the node type is not a mixin (see 7.2.2.5 Options), then a supertype of nt:base is assumed.""


I'm not totally sure, if according to the quote above the built-in cnd-definitions are valid at all. since it states, that the nt:base is assumed if no other super type is defined. In the case of the node types above, mix:referenceable is defined to be the only super type, which is not totally true... the non-mixin types are always sub types of nt:base.

In either case: From my understanding the node types resulting from parsing the xml and the cnd file should be equal.
If the definitions are valid, we may need to adjust the CompactNodeTypeDefReader.




"
1,"IW.getReader() returns inconsistent reader on RT BranchI extended the testcase TestRollingUpdates#testUpdateSameDoc to pull a NRT reader after each update and asserted that is always sees only one document. Yet, this fails with current branch since there is a problem in how we flush in the getReader() case. What happens here is that we flush all threads and then release the lock (letting other flushes which came in after we entered the flushAllThread context, continue) so that we could concurrently get a new segment that transports global deletes without the corresponding add. They sneak in while we continue to open the NRT reader which in turn sees inconsistent results.

I will upload a patch soon"
1,"incorrect jcr:uuid on frozen subnodeThe following program:

import javax.jcr.Repository;
import javax.jcr.Session;
import javax.jcr.SimpleCredentials;
import javax.jcr.Node;
import org.apache.jackrabbit.core.TransientRepository;

public class debug2 {
    public static void main(String[] args) throws Exception {
        Repository repository = new TransientRepository();
        Session session = repository.login(
                new SimpleCredentials(""username"", ""password"".toCharArray()));
        try {
            Node root = session.getRootNode();

            Node foo = root.addNode(""foo"");
            foo.addMixin(""mix:versionable"");

            Node bar = foo.addNode(""bar"");
            bar.addMixin(""mix:referenceable"");
            System.out.println(""bar:            "" + bar.getUUID());

            session.save();
            foo.checkin();

            Node frozenbar = foo.getBaseVersion().getNode(""jcr:frozenNode"").getNode(""bar"");
            System.out.println(""frozenbar UUID: "" + frozenbar.getUUID());
            System.out.println(""jcr:uuid:       "" + frozenbar.getProperty(""jcr:uuid"").getValue().getString());
            System.out.println(""jcr:frozenUuid: "" + frozenbar.getProperty(""jcr:frozenUuid"").getValue().getString());

        } finally {
            session.logout();
        }
    }
}

Gives as sample output:
bar:            fcf0affb-7476-4a64-a480-3039e8c53d53
frozenbar UUID: ed9fece9-9837-4ecc-9b7e-55bdfb8284e2
jcr:uuid:       fcf0affb-7476-4a64-a480-3039e8c53d53
jcr:frozenUuid: fcf0affb-7476-4a64-a480-3039e8c53d53

The jcr:uuid of the frozen bar is incorrect (althoug getUUID() returns the correct value).
"
1,"NPE Exception Thrown By FileJournal During Commit OperationThe following exception stack traces appearing repeatedly during a performance test of a JCR cluster at a customer site. 

ERROR - Unexpected error while preparing log entry.
java.lang.NullPointerException
	at org.apache.jackrabbit.core.cluster.FileRevision.unlock(FileRevision.java:117)
	at org.apache.jackrabbit.core.cluster.FileRevision.get(FileRevision.java:146)
	at org.apache.jackrabbit.core.cluster.FileJournal.sync(FileJournal.java:296)
	at org.apache.jackrabbit.core.cluster.FileJournal.begin(FileJournal.java:435)
	at org.apache.jackrabbit.core.cluster.ClusterNode.updatePrepared(ClusterNode.java:399)
	at org.apache.jackrabbit.core.cluster.ClusterNode.access$000(ClusterNode.java:40)
	at org.apache.jackrabbit.core.cluster.ClusterNode$WorkspaceUpdateChannel.updatePrepared(ClusterNode.java:559)
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:647)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:778)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:808)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:326)
	at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:313)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:302)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:295)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1204)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:821)
	
ERROR - Unexpected error while committing log entry.
java.lang.NullPointerException
	at org.apache.jackrabbit.core.cluster.FileJournal.commit(FileJournal.java:660)
	at org.apache.jackrabbit.core.cluster.ClusterNode.updateCommitted(ClusterNode.java:425)
	at org.apache.jackrabbit.core.cluster.ClusterNode$WorkspaceUpdateChannel.updateCommitted(ClusterNode.java:566)
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:712)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:808)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:326)
	at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:313)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:302)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:295)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1204)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:821)
	"
1,"Registering cyclic dependent nodetypes does not workwhen registering the followin 2 nodetypes:

[foo] 
+ mybar (bar)

[bar]
+ myfoo (foo)

NodeTypeRegistry.registerNodeTypes(Collection) throws:

 org.apache.jackrabbit.core.nodetype.InvalidNodeTypeDefException: the following node types could not be registered because of unresolvable dependencies: {}foo {}bar 
"
1,"missing sync in InternalVersionManagerImpl.externalUpdate can cause ConcurrentModificationExceptionIn

        for (Map.Entry<ItemId, InternalVersionItem> entry : versionItems.entrySet()) {
            if (changes.has(entry.getKey())) {
                items.add(entry.getValue());
            }
        }

we need to sync on versionItems, I believe."
1,"Create or Append mode determined before obtaining write lockIf an IndexWriter(""writer1"") is opened in CREATE_OR_APPEND mode, it determines whether to CREATE or APPEND before obtaining the write lock.  When another IndexWriter(""writer2"") is in the process of creating the index, this can result in writer1 entering create mode and then waiting to obtain the lock.  When writer2 commits and releases the lock, writer1 is already in create mode and overwrites the index created by write2.

This bug was probably effected by LUCENE-2386 as prior to that Lucene generated an empty commit when a new index was created.  I think the issue could still have occurred prior to that but the two IndexWriters would have needed to be opened nearly simultaneously and the first IndexWriter would need to release the lock before the second timed out."
1,"Version 1.3 reports IOException when re-creating an indexVersion: Lucene 1.3 final 
Error reported when I am (re-)doing an initialization on the index created 
previously:
java.io.IOException: couldn't delete _26a.f1

The problem disappearred after a re-start of the jvm, some files may be locked 
after the index writer action !
Problem does not appear in Version 1.2."
1,"Webdav: creating resource in case of RepositoryExceptionif accessing item fails for any other reason than PathNotFoundException, creating
the resource should rather fail (throwing 403).

(reported by brian)"
1,"TestTermsEnum.testIntersectRandom fail{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestTermsEnum
    [junit] Testcase: testIntersectRandom(org.apache.lucene.index.TestTermsEnum):	FAILED
    [junit] (null)
    [junit] junit.framework.AssertionFailedError
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1530)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1432)
    [junit] 	at org.apache.lucene.index.codecs.BlockTreeTermsReader$FieldReader$IntersectEnum.getState(BlockTreeTermsReader.java:894)
    [junit] 	at org.apache.lucene.index.codecs.BlockTreeTermsReader$FieldReader$IntersectEnum.seekToStartTerm(BlockTreeTermsReader.java:969)
    [junit] 	at org.apache.lucene.index.codecs.BlockTreeTermsReader$FieldReader$IntersectEnum.<init>(BlockTreeTermsReader.java:786)
    [junit] 	at org.apache.lucene.index.codecs.BlockTreeTermsReader$FieldReader.intersect(BlockTreeTermsReader.java:483)
    [junit] 	at org.apache.lucene.index.TestTermsEnum.testIntersectRandom(TestTermsEnum.java:293)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1530)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1432)
    [junit] 
    [junit] 
    [junit] Tests run: 6, Failures: 1, Errors: 0, Time elapsed: 14.762 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestTermsEnum -Dtestmethod=testIntersectRandom -Dtests.seed=320d0949741fc6b1:3cabdb9b04d0d243:-4b7c80572775ed92 -Dtests.multiplier=3
{noformat}"
1,"Removal of versions throws javax.jcr.ReferentialIntegrityExceptionFrom the following thread : http://www.mail-archive.com/jackrabbit-dev%40incubator.apache.org/msg03483.html

When trying to remove a version of a Node  the VersionHistory.removeVersion() method throws : ""javax.jcr.ReferentialIntegrityException: Unable to remove version. At least once referenced."".

Secton 8.2.2.10 (Removal of Versions) of the specification indicates that the version graph should be automatically repaired upon removal. Then, VersionHistory.removeVersion() should take care of references. (In fact, a user cannot alter the references (jcr:predecessors and jcr:successors), since they are protected properties.)

Here's the example (*updated) :

Node root1 = session.getRootNode() ;
Node test1 = root1.addNode(""test"") ;
test1.addMixin(""mix:versionable"");
test1.setProperty(""test"", ""1"");
session.save();
test1.checkin();

test1.checkout();
test1.setProperty(""test"", ""2"");
session.save();
test1.checkin();

test1.checkout();
test1.setProperty(""test"", ""3"");
session.save();
test1.checkin();

String baseVersion = test1.getBaseVersion().getName();
System.out.println(""Base version name: "" + baseVersion);

VersionHistory vh = test1.getVersionHistory();
for (VersionIterator vi = vh.getAllVersions(); vi.hasNext(); ) {
    Version currenVersion = vi.nextVersion();
    String versionName = currenVersion.getName();
    if (!versionName.equals(""jcr:rootVersion"") && !versionName.equals(baseVersion)) { 
        String propertyValue = currenVersion.getNode(""jcr:frozenNode"").getProperty(""test"").getString();
        System.out.println(""Removing version : "" + versionName + "" with value: "" + propertyValue);
        vh.removeVersion(versionName);
    }
}

Regards, 

Nicolas"
1,"Cookies with null path attribute are rejected in the compatibility modeWeblogic sends cookies with path empty, httpclient emits a warning
and doesn't send back the cookie to server.

Maybe httpclient works in the RFC's ways but this doesn't reproduce
common web browsers behaviours. Our application works well with IE,
Opera and Netscape, httpunit also sends back the cookie to the server.

When receving the response, httpclient emits the followin warning :

[WARN] HttpMethod - -Invalid cookie header: ""JTD=O%
2FdF13CDb1W7H2GNfUTS2YQ3Zt6bCW6ZKZRvVJ9FwaadQLxXVI7rgii%2FwbxeCsqym7dcWKDxSj%
2Bg1ubJRSVRhYGb7wRLjp5c0v2R3QrCIXVhMKDjuwuXDXnjbH3LHSWG7bfzJSmS7nXk9R%
2FqMIRHb5najLQkU7WkuPGgXUnUln%2BF51TajkVmXkrLMYN7MHDT48BEHvFQFNXBlmSRejWqrd%
2Fiiao0flObOrT3HcaWI09B1vekpAcPmgvMD2oZzXQWJwjDZIX6QoVVD6U8CXPSvVQjITyaxf6AqaS%
2BAFJgRsqbZBc0%2BV5G%2FnzE87ggOVIozfPFn99ny0kxiPGBEisJIy%3D%3D; Version=1; 
Path=; Max-Age=604800"". Missing value for path attribute

That's right, maybe the http header is not correct, but I think httpclient
should handle this case without error in order to have the same behaviour
as common browsers. We have no way to give a better value to this path."
1,HTML Text Extractor does not extract or index numericsNumerics such as addresses/dates/financial figures are not extracted or indexed by the current HTML Extractor.  These values are handled properly and searchable when done via the PlainTextExtractor
1,MoreLikeThis ignores custom similarityMoreLikeThis only allows the use of the DefaultSimilarity.  Patch shortly
1,"Wrong creation of AuthScope objectClass Name: org.apache.http.client.protocol.RequestAuthCache
Line #: 118-119

Issue: Want to create a new Object of AuthScope by passing host name, port and scheme name but due to incorrect constructor call, Getting a object with realm name as scheme name.
Current Code: Credentials creds = credsProvider.getCredentials(new AuthScope(host.getHostName(), host.getPort(), null, schemeName));"
1,"Exception in HttpConnection because of unchecked buffer sizeFrom the httpclient-dev mailing list:

Date: Tue, 8 Mar 2005 19:08:35 +0100
Subject: Error with multiple connections

Hello,

 

I am having some problems while trying multiple connections over a
HttpClient object with a MultiThreadedHttpConnectionManager. I am
launching 10 threads and each thread executes some GetMethods using this
HttpClient object.

 

Some times I got an error like this:

 

java.lang.IllegalArgumentException: Buffer size <= 0

      at java.io.BufferedInputStream.<init>(Unknown Source)

      at
org.apache.commons.httpclient.HttpConnection.open(HttpConnection.java:70
3)

      at
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpCon
nectionAdapter.open(MultiThreadedHttpConnectionManager.java:1170)

      at
org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:6
28)

      at
org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:4
97)

      at Main$Hilo.run(Main.java:58)

 

Does anybody have any idea? 

 

Thanks in advance,

Jorge"
1,"addIndexes unexpectedly closes indexIt seems that in 1.4rc2, a call to IndexWriter.addIndexes (IndexReader[]) will
close the provided IndexReader; in 1.3-final this does not happen.  So my code
which uses addIndexes to merge new information into an index and then calls
close() on the IndexReader now crashes with an ""already closed"" exception.  I
can attach test code which works in 1.3 but not in 1.4rc2 if that would be helpful.

If this is an intentional change in behavior, it needs to be documented.  Thanks!"
1,"DocValues infinite loop caused by - a call to getMinValue | getMaxValue | getAverageValueorg.apache.lucene.search.function.DocValues offers 3 public (optional) methods to access value statistics like min, max and average values of the internal values. A call to one of the methods will result in an infinite loop. The internal counter is not incremented. 
I added a testcase, javadoc and a slightly different implementation to it. I guess this is not breaking any back compat. as a call to those methodes would have caused an infinite loop anyway.
I changed the return value of all of those methods to Float.NaN if the DocValues implementation does not contain any values.

It might be considerable to fix this in 2.4.2 and 2.3.3

"
1,"Jcr-Server: ValuesProperty missing property type informationJCR specific dav-property ValuesProperty does not reveal the PropertyType of the value, which is therefore lost during (de)serialization. 

Solution: 
- Pass type of the JCR-value as attribute to the xml-element containing the value."
1,"when many query clases are specified in boolean or dismax query, highlighted terms are always ""yellow"" if multi-colored feature is usedThe problem is the following snippet:

{code}
protected String getPreTag( int num ){
  return preTags.length > num ? preTags[num] : preTags[0];
}
{code}

it should be:

{code}
protected String getPreTag( int num ){
  int n = num % preTags.length;
  return  preTags[n];
}
{code}
"
1,"TCK: OrderByMultiTypeTest doesn't respect nodetype configuration propertyOrderByMultiTypeTest creates test data by calling addNode(String).  This fails if there is no default primary type.

Proposal: call addNode(String, String)

--- OrderByMultiTypeTest.java   (revision 428760)
+++ OrderByMultiTypeTest.java   (working copy)
@@ -43,9 +43,9 @@
      * Tests order by queries with a String property and a long property.
      */
     public void testMultipleOrder() throws Exception {
-        Node n1 = testRootNode.addNode(nodeName1);
-        Node n2 = testRootNode.addNode(nodeName2);
-        Node n3 = testRootNode.addNode(nodeName3);
+        Node n1 = testRootNode.addNode(nodeName1, testNodeType);
+        Node n2 = testRootNode.addNode(nodeName2, testNodeType);
+        Node n3 = testRootNode.addNode(nodeName3, testNodeType);
  
         n1.setProperty(propertyName1, ""aaa"");
         n1.setProperty(propertyName2, 3);
"
1,"Multipart post is brokenI tried to do HttpPost request with MultipartEntity, this request was encoded to wire with 3 line separators after header and not processed correctly by http server.
MultipartEntry add 1 extra line separator before write itself to wire. I'm not sure about standards, but it is at least not ""browser compatible"".

"
1,"Jcr-server: DeltaVResource lists MKWORKSPACE in the method constant.RFC 3253 requires REPORT to be supported by all DeltaV compliant resources.
The method constant therefore should list REPORT only."
1,Mandatory jcr:activities node missing after upgradeThe mandatory node is only created when the repository is initially empty. The node is missing when an existing repository instance is upgraded. 
1,"IndexWriter memory leak when large docs are indexedSpinoff from the java-user thread ""IndexWriter and memory usage""...

IndexWriter has had a long standing memory leak, since LUCENE-843.

When the byte/char/int blocks are recycled to the common pool, the
per-thread DW classes incorrectly still hold a reference to them.

This normally is not a problem, since these buffers will be re-used
again.

But, if you index a massive document, causing IW to allocate more than
the RAM buffer allocated to it, then the leak happens.  So you could
have a 16 MB RAM buffer set, but if a huge doc required allocation of
200 MB worth of arrays, those 200 MB are never freed (well, until you
close the IW and deref it from the app).

It's even worse if you use multiple threads: if each thread has ever
had to index a massive document, then that thread incorrectly holds
onto the extra arrays.
"
1,"Repository Home locked not released despite RepositoryException being thrown.When an exception is thrown when calling RepositoryImpl.create(...) a .lock file is created in the repository home directory and not removed despite there no longer being an active connection.  If the user attempts to create the repository again (e.g recover from the exception because the url of the repository was temporarily unavailable) a RepositoryException is thrown again indicating that the repository home is locked by another process because there is a .lock file.  If a Repository is not successfully created then the repository home should not be locked.

The lock is only released when the repository is shutdown but in this case the Repository object is never created successfully for that method to be called.

"
1,"NullPointerException thrown when invalid header encounteredIf a server returns a header with no name but with a value (ie: an invalid line in the headers), HttpClient throws a NullPointerException instead of just skipping that header line or perhaps treating it as a continuation of the previous header (need to consult the RFC to confirm this).

Problem reported by Eduardo Francos on the commons-user list.

A good test URL for this problem is:

http://www.pc.ibm.com/us/accessories/monitors/p_allmodelos.html

which should return a 404 error but throws the NullPointerException instead."
1,"SQL query with jcr:path LIKE '/foo/%' only selects childrenA query like: 

SELECT * FROM nt:base WHERE jcr:path LIKE '/foo/%'

only selects the children of /foo instead off all descendants of /foo."
1,"{XML|Object}PersistenceManager.destroy(*) may failThe destroy methods of the ObjectPersistenceManager class try to delete their files without checking for their existence. This may result in a FileSystemException being thrown because according to the specification of FileSystem.deleteFile() a FileSystemException is thrown ""if this path does not denote a file or if another error occurs.""

While the Jackrabbit LocalFileSystem implementation silently ignores a request to delete a non-existing file, our internal implementation of the interface throws a FileSystemException in this case, which cause destroy to fail.

I suggest all destroy methods should be extended to first check for the existence of the file to prevent from being thrown.

Note: This not only applies to ObjectPersistenceManager but also to XMLPersistenceManager."
1,"XML import should not access external entitiesWith current Jackrabbit the following XML document can not be imported:

    <!DOCTYPE foo SYSTEM ""http://invalid.address/""><foo/>

Even if the DTD address (or some other external resource referenced in the XML document) is correct, I don't think importXML() should even try resolving those references."
1,"ClassDescriptor.hasIdField() fails if id is declared in upper classorg.apache.jackrabbit.ocm.mapper.model.ClassDescriptor.hasIdField() looks up only current class and not the whole hierarchy, so it fails when the id field is declared in a upper class.

hasIdField should use getIdFieldDescriptor and not access idFieldDescriptor field directly, as follows :

    public boolean hasIdField() {
   		return (this.getIdFieldDescriptor() != null && this
    				.getIdFieldDescriptor().isId());
    }

Please find patch enclosed.

Sincerely yours,

Stphane Landelle"
1,"NTLM Authentication No Longer Working In Latest ReleaseOur application has been working fine using NTLM auth with HttpClient for 3 years.   We were most recently on 4.0.3.    Upon upgrading to 4.1.2, NTLM stopped working.

I tried both the new for 4.1 built-in NTLM and the ""old way"" of using JCIFS: client.getAuthSchemes().register(""ntlm"", new NTLMSchemeFactory()); 

Using wireshark I can see that NTLM auth is not even attempted using 4.1.2.    Rolling back to 4.0.3 immediately resolved this problem."
1,"Bundle binding deserialization problemI'm trying to upgrade from 1.3.x to jackrabbit 1.4.x (branch)  and have problems with existing repostories (probaly the same issue is with 1.5.x)

Caused by: org.apache.jackrabbit.core.state.ItemStateException: failed to read bundle: deadbeef-face-babe-cafe-babecafebabe: java.lang.IllegalArgumentException: invalid namespaceURI specified
 at org.apache.jackrabbit.core.persistence.bundle.BundleDbPersistenceManager.loadBundle(BundleDbPersistenceManager.java:1229)
 at org.apache.jackrabbit.core.persistence.bundle.BundleDbPersistenceManager.loadBundle(BundleDbPersistenceManager.java:1161)

It looks that issue was introduced by resolving JCR-1632"
1,"SQL-2 query returns more than the requested columnIf I do :

SELECT alias.[jcr:title] FROM [jnt:mainContent] as alias

and then iterate through the returned columns of the rows, I get the same result as for :

SELECT * FROM [jnt:mainContent] 

which is ALL the properties defined for jnt:mainContent.

Only if I use

SELECT alias.[jcr:title] as title FROM [jnt:mainContent] as alias

the result is limited to the title column."
1,"ObjectConverterImpl.getObject(Session, Class, String) may not resolve mapping correctly for incompletely described mappingsWhen a node is mapped by calling the ObjectConverter.getObject(Session, Class, String) method and no discriminator property is configured the ObjectConverterImpl class tries to find a ""best"" mapping for the effective node. This is done by walking the class descriptor hierarchy starting at the descriptor for the selected class until a mapping for the node type is found.

In case the class descriptor hierarchy is incomplete because an improperly defined class descriptor would actually perfectly map the node but is not declared to extend (or implement) its parent classes/interfaces, the hierarchy walk down will not find the mapping and thus in the end, the originally requested class will be instantiated. If the class is abstract or an interface this of course fails.

If an exact class descriptor for the node type would be looked up directly, the mapping might be found immediately and the class of the descriptor can be verified it actually is assignement compatible with the requested class. If this would fail, we could still walk the hierarchy to see, whether we find another classdescriptor.

To clarify the issue consider the following example of an abstract base class and a concrete extension class with their node types

   AbstractBaseClass maps abstractly to AbstractBaseType
   BaseClass (extends AbstractBaseClass) maps to BaseType ( with supertype AbstractBaseType )

Note, that the BaseClass mapping does not declare to extend the AbstractBaseClass.

When calling ObjectConverterImpl.getObject(session, AbstractBaseClass.class, aBaseTypeNode), the descriptor fore the AbstractBaseClass is inspected agains the node and then it is decided to check the class descriptor hierarchy. Node mapping can be found by walking the hierarchy and hence the AbstractBaseClass is instantiated, which of course fails.

If the BaseClass mapping would be declared as extending the AbstractBaseClass mapping, everything would be fine."
1,"Parameters 'idleTime' and 'queryClass' cause QueryHandler to failThis issue does not occur in a released jackrabbit-core version. With the changes from JCR-1462 jackrabbit now fails to startup if there is an unknown parameter in a bean configuration.

The parameters 'idleTime' and 'queryClass' are not used by the QueryHandler but by the SearchManager, which instantiates the QueryHandler. Therefore the parameters do not show up in the QueryHandler.

I suggest we introduce them in the common base class AbstractQueryHandler."
1,"FVH: slow performance on very large queriesThe change from HashSet to ArrayList for flatQueries in LUCENE-3019 resulted in very significant slowdown in some of our e-discovery queries after upgrade from 3.4.0 to 3.5.0. Our queries sometime contain tens of thousands of terms. As a result, major portion of execution time for such queries is now spent in the flatQueries.contains( sourceQuery ) method calls."
1,"TaxonomyReader.refresh() is broken, replace its logic with reopen(), following IR.reopen patternWhen recreating the taxonomy index, TR's assumption that categories are only added does not hold anymore.
As result, calling TR.refresh() will be incorrect at best, but usually throw an AIOOBE."
1,"Finding Newest Segment In Empty IndexWhile extending the index writer, I discovered that its newestSegment method does not check to see if there are any segments before accessing the segment infos vector. Specifically, if you call the IndexWriter#newestSegment method on a brand-new index which is essentially empty, then it throws an java.lang.ArrayIndexOutOfBoundsException exception.

The proposed fix is to return null if no segments exist, as shown below:

--- lucene/src/java/org/apache/lucene/index/IndexWriter.java	(revision 930788)
+++ lucene/src/java/org/apache/lucene/index/IndexWriter.java	(working copy)
@@ -4587,7 +4587,7 @@
 
   // utility routines for tests
   SegmentInfo newestSegment() {
-    return segmentInfos.info(segmentInfos.size()-1);
+    return segmentInfos.size() > 0 ? segmentInfos.info(segmentInfos.size()-1) : null;
   }
"
1,"nonce-count in digest auth should not be quotedIn 3.0rc3 nonce-count (nc) is enclosed in quote marks. According to rfc2617 this
is wrong, nonce-count shouldn't be enclosed in quote marks.

> 3.2.2 The Authorization Request Header
> 
>    The client is expected to retry the request, passing an Authorization
>    header line, which is defined according to the framework above,
>    utilized as follows.
> 
>        credentials      = ""Digest"" digest-response
>        digest-response  = 1#( username | realm | nonce | digest-uri
>                        | response | [ algorithm ] | [cnonce] |
>                        [opaque] | [message-qop] |
>                            [nonce-count]  | [auth-param] )
> 
>        username         = ""username"" ""="" username-value
>        username-value   = quoted-string
>        digest-uri       = ""uri"" ""="" digest-uri-value
>        digest-uri-value = request-uri   ; As specified by HTTP/1.1
>        message-qop      = ""qop"" ""="" qop-value
>        cnonce           = ""cnonce"" ""="" cnonce-value
>        cnonce-value     = nonce-value
>        nonce-count      = ""nc"" ""="" nc-value
>        nc-value         = 8LHEX
>        response         = ""response"" ""="" request-digest
>        request-digest = <""> 32LHEX <"">
>        LHEX             =  ""0"" | ""1"" | ""2"" | ""3"" |
>                            ""4"" | ""5"" | ""6"" | ""7"" |
>                            ""8"" | ""9"" | ""a"" | ""b"" |
>                            ""c"" | ""d"" | ""e"" | ""f"""
1,"CircularRedirectException encountered when using a proxy, but not when reaching the target directlyA CircularRedirectException is encountered when using a proxy (tinyproxy on a remote machine), whereas everything is fine when using no proxy. The target is a URL such as http://www.seoconsultants.com/w3c/status-codes/301.asp which has a 301 redirection.

The issue can be fixed by using ALLOW_CIRCULAR_REDIRECTS set to true (client params), but I can't consider this a ""real"" fix.

Here is a snippet of code that exemplifies the problem (use your own proxy):

---
String proxyHost = ""xyz.webfactional.com"";
int proxyPort = 7295;

DefaultHttpClient httpclient = new DefaultHttpClient();
// without a proxy it's OK!
httpclient.getParams().setParameter(ConnRoutePNames.DEFAULT_PROXY,
        new HttpHost(proxyHost, proxyPort, ""http""));

HttpParams params = httpclient.getParams();
HttpClientParams.setRedirecting(params, true);
HttpProtocolParams.setUserAgent(params,
        ""Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.5; en-US; rv:1.9.0.10) Gecko/2009042315 Firefox/3.0.10"");

// OK, this fixes the problem, but at what cost / other problems ?
//httpclient.getParams().setParameter(ClientPNames.ALLOW_CIRCULAR_REDIRECTS, true);

String url = ""http://www.seoconsultants.com/w3c/status-codes/301.asp"";

HttpUriRequest request;
HttpResponse response;

request = new HttpGet(url);
System.out.println(""request = "" + request.getRequestLine());
response = httpclient.execute(request);
System.out.println(""status = "" + response.getStatusLine());
System.out.println(""headers = "" + Arrays.asList(response.getAllHeaders()));
---"
1,"webdav's PropertyDefinitionImpl's toXML doesn't seem to attach query operators element to the returned domPropertyDefinitionImpl.toXML does

        // JCR 2.0 extension
        Element qopElem = document.createElement(AVAILABLE_QUERY_OPERATORS_ELEMENT);
        String[] qops = getAvailableQueryOperators();
        for (int i = 0; i < qops.length; i++) {
            Element opElem = document.createElement(AVAILABLE_QUERY_OPERATOR_ELEMENT);
            DomUtil.setText(opElem, qops[i]);
            qopElem.appendChild(opElem);
        }

        return elem;

which doesn't attach the qopElem to the returned dom."
1,"ConnectException not handled in DefaultHttpMethodRetryHandlerCopied from my mailing list post, Oleg suggested I post it to JIRA for 4.0 fix.

i am using commons-httpclient.3.0.1 and I am sending some requests
through https protocol. I have a problem with a long creation of
connection if ip address of remote service is not existing. I think
problem is in the situation when https connection is not created and
ConnectException is thrown after connection timeout. This exception is
catched in HttpMethodDirector.java in method executeWithRetry. Then
the DefaultHttpMethodRetryHandler is called to recognize whether
connection creation will be repeated or not.
I think, that special handling for ConnectException is missing in
retryMethod of DefaultHttpMethodRetryHandler, because exception is not
recognized and connetions are created again.
On the other hand, ConnectTimeoutException is thrown after connection
timeout for HTTP. This exception is handled in
DefaultHttpMethodRetryHandler and call is stopped.

These lines of code handle ConnectTimeoutException in retryMethod of
DefaultHttpMethodRetryHandler:
if (exception instanceof InterruptedIOException) {
            // Timeout
            return false;
        }

Probably this is missing for ConnectException:
if (exception instanceof InterruptedIOException || exception
instanceof ConnectException) {
            // Timeout
            return false;
        }

"
1,"JNDIDatabaseJournal doesn't work with ""oracle"" schema (or: unable to use OracleDatabaseJournal with a jndi datasource)Database journal works fine on oracle when using the OracleDatabaseJournal implementation; but when you need to use a jndi datasource you actually need to use org.apache.jackrabbit.core.journal.JNDIDatabaseJournal which doesn't work fine with the ""oracle"" schema.

With the following configuration:
<Cluster id=""node1"" syncDelay=""10"">
    <Journal class=""org.apache.jackrabbit.core.journal.JNDIDatabaseJournal"">
      <param name=""schema"" value=""oracle"" />

jackrabbit crashes at startup with a not well defined sql error. Investigating on the problem I see that the ""oracle.ddl"" file contains a ""tablespace"" variable that is replaced only by the OracleDatabaseJournal implementation.

As a workaround users can create a different ddl without a tablespace variable, but this should probably work better out of the box.

WDYT about one of the following solutions?
- make the base DatabaseJournal implementation support jndi datasource just like PersistenceManagers do (without a specific configuration property but specifying a jndi location in the url property)
- move the replacement of the tablespace variable (and maybe: add a generic replacement of *any* parameter found in the databaseJournal configuration) to the main DatabaseJournal implementation. This could be handy and it will make the OracleDatabaseJournal extension useless, but I see that at the moment there can be a problem with the MsSql implementation, since it adds ""on "" to the tablespace name only when it's not set to an empty string.





"
1,"leading wildcard's don't work with trailing wildcardAs reported by Antony Bowesman, leading wildcards don't work when there is a trailing wildcard character -- instead a PrefixQuery is constructed.


http://www.nabble.com/QueryParser-bug--tf3270956.html"
1,"[PATCH] Loosing first matching document in BooleanQueryThis patch fixes loosing of first matching document when BooleanQuery
with BooleanClause.Occur.SHOULD is added to another BooleanQuery."
1,ChainedTermEnum omits initial termsThis is a regression caused by JCR-2393.
1,"primaryItemName is not inheritedif no primaryItemName is defined for a nodetype definition, it should be inherited from one of the supertypes. the spec is unclear about this, though it seems to be the natural behaviour.

for example when extending nt:resource, the subtype should not be force to redefine the jcr:data as primaryItemName.

"
1,"Intermittent failure in TestIndexWriterMergePolicy.testMaxBufferedDocsChangeLast night's build failed from it: http://hudson.zones.apache.org/hudson/job/Lucene-trunk/1019/changes

Here's the exc:

{code}
    [junit] Testcase: testMaxBufferedDocsChange(org.apache.lucene.index.TestIndexWriterMergePolicy):	FAILED
    [junit] maxMergeDocs=2147483647; numSegments=11; upperBound=10; mergeFactor=10
    [junit] junit.framework.AssertionFailedError: maxMergeDocs=2147483647; numSegments=11; upperBound=10; mergeFactor=10
    [junit] 	at org.apache.lucene.index.TestIndexWriterMergePolicy.checkInvariants(TestIndexWriterMergePolicy.java:234)
    [junit] 	at org.apache.lucene.index.TestIndexWriterMergePolicy.testMaxBufferedDocsChange(TestIndexWriterMergePolicy.java:164)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:208)
{code}

Test doesn't fail if I run on opensolaris nor os X machines..."
1,"Property.getValue() throws RepositoryException with internal errorRunning ConcurrentReadWriteTest (NUM_NODES=5, NUM_THREADS=3, RUN_NUM_SECONDS=120) resulted in a RepositoryException calling Property.getValue():

javax.jcr.RepositoryException: Internal error while retrieving value of b3fc1ea8-3364-4236-bcc7-dea0baf90640/{}test: null: null

Debugging shows that it is a NullPointerException:

java.lang.NullPointerException
	at org.apache.jackrabbit.core.PropertyImpl.getValue(PropertyImpl.java:481)
	at org.apache.jackrabbit.core.ConcurrentReadWriteTest$1$1.execute(ConcurrentReadWriteTest.java:68)
	at org.apache.jackrabbit.core.AbstractConcurrencyTest$Executor.run(AbstractConcurrencyTest.java:110)
	at java.lang.Thread.run(Thread.java:619)

It's probably the state which has been discarded after the sanityCheck()."
1,"JCR2SPI: remove node operation missing in submitted SPI batchIn JCR2SPI, the following sequence of operations seems to lead to an incorrect SPI batch being submitted:

1) remove ""/a""
2) add ""/a""
3) add ""/a/b""
4) session.save()

This seems to create an SPI batch where the first remove operation is missing.

Note that the problem only seems to occur when step 3 is part of the sequence.

Full Java source for test:

    try {
      if (session.getRepository().getDescriptor(Repository.LEVEL_2_SUPPORTED).equals(""true"")) {
        Node testnode;
        String name = ""delete-test"";
          
        Node root = session.getRootNode();
        
        // make sure it's there
        if (! root.hasNode(name)) {
          root.addNode(name, ""nt:folder"");
          session.save();
        }
        
        // now test remove/add in one batch
        if (root.hasNode(name)) {
          testnode = root.getNode(name);
          testnode.remove();
          // session.save(); // un-commenting this makes the test pass
        }
        
        testnode = root.addNode(name, ""nt:folder"");
        // add one child
        testnode.addNode(name, ""nt:folder""); // commenting this out makes the test pass
        
        session.save();
      }
    } finally {
      session.logout();
    }
    
    "
1,"Creating AccessControlEntryImpl from a base entry results in wrong restrictionsduring creation of a new AccessControlEntryImpl using a base entry the restrictions of the base entry are
not copied to the new instance."
1,"recovery tool does not recover when version history can be instantiated, but root version can notJCR-2551 introduced a recovery mode which tries to instantiate the version history, and if this fails, disconnects the VH (version history) and makes the node unversioned.

However, it appears it can happen that the persistence is damaged such as getting the VH does indeed work, but subsequent operations fail due to other problems. One problem that has been seen is a missing frozenNode property of the root version (or a missing frozenNode itself).

As a quick fix, we may want to change the checker so that it actually also tries to get the rootVersion and it's frozenNode. Long term, depending on how frequent this problem is, we may have to think about a less drastic recovery than disconnecting the VH."
1,"DOMException: NAMESPACE_ERR thrown when exporting document viewWhen I try to export some nodes with ExportDocumentView I get a DOMException with Jackrabbit 1.5.2. Version 1.4.6 works fine. Xerces version was 2.8.1.

Code:

Document document = documentBuilder.newDocument();
Element exportElement = (Element) document.appendChild(document.createElement(""Export""));
Result result = new DOMResult(exportElement);
TransformerHandler transformerHandler = saxTransformerFactory.newTransformerHandler();
transformerHandler.setResult(result);
session.exportDocumentView(workflowNode.getPath(), transformerHandler, true, false);

Exception:

org.w3c.dom.DOMException: NAMESPACE_ERR: An attempt is made to create or change an object in a way which is incorrect with regard to namespaces.
	at org.apache.xerces.dom.CoreDocumentImpl.checkDOMNSErr(Unknown Source)
	at org.apache.xerces.dom.AttrNSImpl.setName(Unknown Source)
	at org.apache.xerces.dom.AttrNSImpl.<init>(Unknown Source)
	at org.apache.xerces.dom.CoreDocumentImpl.createAttributeNS(Unknown Source)
	at org.apache.xerces.dom.ElementImpl.setAttributeNS(Unknown Source)
	at com.sun.org.apache.xalan.internal.xsltc.trax.SAX2DOM.startElement(SAX2DOM.java:194)
	at com.sun.org.apache.xml.internal.serializer.ToXMLSAXHandler.closeStartTag(ToXMLSAXHandler.java:204)
	at com.sun.org.apache.xml.internal.serializer.ToSAXHandler.flushPending(ToSAXHandler.java:277)
	at com.sun.org.apache.xml.internal.serializer.ToXMLSAXHandler.startElement(ToXMLSAXHandler.java:646)
	at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerHandlerImpl.startElement(TransformerHandlerImpl.java:263)
	at org.apache.jackrabbit.commons.xml.Exporter.startElement(Exporter.java:438)
	at org.apache.jackrabbit.commons.xml.DocumentViewExporter.exportNode(DocumentViewExporter.java:76)
	at org.apache.jackrabbit.commons.xml.Exporter.exportNode(Exporter.java:298)
	at org.apache.jackrabbit.commons.xml.Exporter.exportNodes(Exporter.java:214)
	at org.apache.jackrabbit.commons.xml.DocumentViewExporter.exportNode(DocumentViewExporter.java:77)
	at org.apache.jackrabbit.commons.xml.Exporter.exportNode(Exporter.java:295)
	at org.apache.jackrabbit.commons.xml.Exporter.export(Exporter.java:144)
	at org.apache.jackrabbit.commons.AbstractSession.export(AbstractSession.java:461)
	at org.apache.jackrabbit.commons.AbstractSession.exportDocumentView(AbstractSession.java:241)"
1,"Clustering: race condition may cause duplicate entries in search indexThere seems to be a race condition that may cause duplicate search index entries. It is reproducible as follows (Jackrabbit 1.3):
1) Start clusternode 1 that just adds a single node of node type clustering:test.
2) Shutdown clusternode 1.
3) Start clusternode 2 with an empty search index.
4) Execute the query  //element(*, clustering:test).
4) Print the result of the query (UUIDs of nodes in the result set).

When I just run clusternode 2, then there is one node in the resultset, as expected. However, when I debug clusternode 2 and have a breakpoint (i.e., a pause of a few seconds at line 306 of RepositoryImpl.java - just before the clusternode is started), then the resultset contains two results, both with the same UUID.
"
1,"ParallelReader crashes when trying to merge into a new indexParallelReader causes a NullPointerException in
org.apache.lucene.index.ParallelReader$ParallelTermPositions.seek(ParallelReader.java:318)
when trying to merge into a new index.

See test case and sample output:

$ svn diff
Index: src/test/org/apache/lucene/index/TestParallelReader.java
===================================================================
--- src/test/org/apache/lucene/index/TestParallelReader.java    (revision 179785)
+++ src/test/org/apache/lucene/index/TestParallelReader.java    (working copy)
@@ -57,6 +57,13 @@
 
   }
  
+  public void testMerge() throws Exception {
+    Directory dir = new RAMDirectory();
+    IndexWriter w = new IndexWriter(dir, new StandardAnalyzer(), true);
+    w.addIndexes(new IndexReader[] { ((IndexSearcher)
parallel).getIndexReader() });
+    w.close();
+  }
+
   private void queryTest(Query query) throws IOException {
     Hits parallelHits = parallel.search(query);
     Hits singleHits = single.search(query);
$ ant -Dtestcase=TestParallelReader test
Buildfile: build.xml
[...]
test:
    [mkdir] Created dir:
/Users/skirsch/text/lectures/da/thirdparty/lucene-trunk/build/test
    [junit] Testsuite: org.apache.lucene.index.TestParallelReader
    [junit] Tests run: 2, Failures: 0, Errors: 1, Time elapsed: 1.993 sec

    [junit] Testcase: testMerge(org.apache.lucene.index.TestParallelReader):  
Caused an ERROR
    [junit] null
    [junit] java.lang.NullPointerException
    [junit]     at
org.apache.lucene.index.ParallelReader$ParallelTermPositions.seek(ParallelReader.java:318)
    [junit]     at
org.apache.lucene.index.ParallelReader$ParallelTermDocs.seek(ParallelReader.java:294)
    [junit]     at
org.apache.lucene.index.SegmentMerger.appendPostings(SegmentMerger.java:325)
    [junit]     at
org.apache.lucene.index.SegmentMerger.mergeTermInfo(SegmentMerger.java:296)
    [junit]     at
org.apache.lucene.index.SegmentMerger.mergeTermInfos(SegmentMerger.java:270)
    [junit]     at
org.apache.lucene.index.SegmentMerger.mergeTerms(SegmentMerger.java:234)
    [junit]     at
org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:96)
    [junit]     at
org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:596)
    [junit]     at
org.apache.lucene.index.TestParallelReader.testMerge(TestParallelReader.java:63)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit]     at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit]     at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)


    [junit] Test org.apache.lucene.index.TestParallelReader FAILED

BUILD FAILED
/Users/skirsch/text/lectures/da/thirdparty/lucene-trunk/common-build.xml:188:
Tests failed!

Total time: 16 seconds
$"
1,checkindex fails if docfreq >= skipInterval and term is indexed more than once at same positionThis is a bad check in the skipping verification logic
1,"TransientFileFactory may throw ConcurrentModificationException on shutdownWhen Jackrabbit is stopped the shutdown hook of the TransientFileFactory iterates over all tracked temp files and deletes them. At the same time the reaper thread may still remove file references from the list of tracked temp files. This may lead to a ConcurrentModificationException in the shutdown hook:

java.util.ConcurrentModificationException
	at java.util.AbstractList$Itr.checkForComodification(Unknown Source)
	at java.util.AbstractList$Itr.next(Unknown Source)
	at org.apache.jackrabbit.util.TransientFileFactory$1.run(TransientFileFactory.java:86)
"
1,"JCR2SPI: Workspace.getImportHandler creates a handler which doesn't work properly under JDK 1.4.JCR2SPI returns an import handler which delegates work to a SAXTransformerHandler. In JDK, that one has a known issue not processing namespace prefix mappings properly (will attach a separate test case).

Proposals:

- drop JDK 1.4 support
- tune the JCR2SPI handler to create namespace attributes when needed
- use an entirely different serializer

My personal preference would be just to drop JDK 1.4 support, but that may not be acceptable for everyone.
"
1,"Weird BooleanQuery behaviorHere's a simple OR-connected query.

T:files T:deleting C:thanks C:exists

The query above hits 1 document. But following *same* query only
with parenthesis results nothing.

(T:files T:deleting) (C:thanks C:exists)

Another combinations of MUST and SHOULD.

""T:files T:deleting +C:production +C:optimize"" hits 1 document.
""(T:files T:deleting) (+C:production +C:optimize)"" hits 1 document."
1,CharsRef#append broken on trunk & 3.xCurrent impl. for append on CharsRef is broken - it overrides the actual content rather than append. its used in many places especially in solr so we might have some broken 
1,"DefaultRedirectHandler does not access correct HttpParamsIn the getLocationURI(HttpResponse, HttpContext) method, the HttpParams for determining REJECT_RELATIVE_REDIRECT and ALLOW_CIRCULAR_REDIRECTS are retrieved with:

HttpParams params = response.getParams();

The response HttpParams do not contain these values, however the request HttpParams do. The correct implementation is:

HttpRequest request = (HttpRequest) context.getAttribute(HttpExecutionContext.HTTP_REQUEST);
HttpParams params = request.getParams();

"
1,"jackrabbit-server.war is missing the slf4j-log4j12 libraryReported by Martin Perez:

But I found a bug on the .war file. It is missing the slf4j-log4j12-1.0.jar. It's in someway tricky to detect it because if you do not include it a ClassNotFoundException will be thrown but poiting to the JCR class with the log statement. Anyways, if you include the .jar file on the WEB-INF/lib directory, then the exception goes to exception's hell.

"
1,"StandardQueryParser ignores AND operator for tokenized query termsThe standard query parser uses the default query operator for query clauses that are created from tokenization in the query parser instead of the actual operator for the source term.

here is an example:
{code}
StandardQueryParser parser = new StandardQueryParser(new StandardAnalyzer(Version.LUCENE_34));
parser.setDefaultOperator(Operator.OR);
System.out.println(((BooleanQuery)parser.parse(""_deleted:true AND title:"", ""f"")));
{code}

this should yield:
+_deleted:true +(title: title:)

as our former core query parser does but actually yields:
+_deleted:true title: title:

seems like a bug to me, looking at the tests seems we don't test for this kind of queries in the standard query parser tests too.
"
1,"SearchIndex class contains garbled StringSomehow during the switch to SL4J also a String literal in the SearchIndex class got garbled.

See:
http://svn.apache.org/viewcvs.cgi/incubator/jackrabbit/trunk/jackrabbit/src/main/java/org/apache/jackrabbit/core/query/lucene/SearchIndex.java?rev=385280&r1=378221&r2=385280

Since this is a low risk change I would like to get this included into the 1.0 branch."
1,"Concurrent locking operations failI prepared simple test which tries to lock/unlock single node from many
threads. I expected only LockExceptions thrown by some threads which can
occur if node is already locked by other thread.

But I get incorrect effect sporadically. It looks like some thread
managed to acquire lock, but then can't release it.

Following exception is thrown then :

javax.jcr.InvalidItemStateException:
7c198c7b-76c8-47c8-96a8-d9dfefd4b387 has been modified externally
    at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1193)
    at org.apache.jackrabbit.core.NodeImpl.unlock(NodeImpl.java:3790)
    at JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:95)

additionally warning appears in log

org.apache.jackrabbit.core.lock.LockManagerImpl$LockInfo.loggingOut(LockManagerImpl.java:892)
- Unable to unlock session-scoped lock on node
'7c198c7b-76c8-47c8-96a8-d9dfefd4b387-W': Unable to unlock node. Node
has pending changes: /folder

In consequence node is left in locked state. It looks like a bug.
If one thread locked node successfully, then none other can modify it,
and the same thread should release lock without any problems.

Shouldn't be lock operation atomic itself ?

Przemo


package com.oyster.mom.contentserver.jcr.transaction;

import javax.jcr.Node;
import javax.jcr.RepositoryException;
import javax.jcr.Session;
import javax.jcr.SimpleCredentials;
import javax.jcr.lock.LockException;

import org.apache.jackrabbit.core.RepositoryImpl;
import org.apache.jackrabbit.core.config.RepositoryConfig;

public class JrTestConcurrentLocks extends Thread {

   private static final org.apache.commons.logging.Log log = org.apache.commons.logging.LogFactory.getLog(JrTestConcurrentLocks.class);

   public static String REPOSITORY_HOME = ""d:/repo/jackrabbit/"";

   public static String REPOSITORY_CONFIG = REPOSITORY_HOME + ""repository.xml"";

   public static void main(String[] args) throws Exception {

       JrTestConcurrentLocks test = new JrTestConcurrentLocks(-1);
       test.startup();

       JrTestConcurrentLocks tests[] = new JrTestConcurrentLocks[3];

       for (int i = 0; i < tests.length; i++) {
           JrTestConcurrentLocks x = new JrTestConcurrentLocks(i);
           x.setSession(repository.login(new SimpleCredentials(""admin"", ""admin"".toCharArray())));
           x.start();
           tests[i] = x;
       }

       for (int i = 0; i < tests.length; i++) {
           tests[i].join();
           tests[i].getSession().logout();
       }

       test.shutdown();
   }

   private static RepositoryImpl repository;

   private int id;

   private Session session;

   public void setSession(Session session) {
       this.session = session;
   }

   public Session getSession() {
       return this.session;
   }

   public JrTestConcurrentLocks(int i) {
       this.id = i;
   }

   public void startup() throws Exception {
       System.setProperty(""java.security.auth.login.config"", ""c:/jaas.config"");

       RepositoryConfig config = RepositoryConfig.create(REPOSITORY_CONFIG, REPOSITORY_HOME);
       repository = RepositoryImpl.create(config);

       Session session = repository.login(new SimpleCredentials(""admin"", ""admin"".toCharArray()));
       Node rootNode = session.getRootNode();
       if (!rootNode.hasNode(""folder"")) {
           Node folder = rootNode.addNode(""folder"");
           folder.addMixin(""mix:versionable"");
           folder.addMixin(""mix:lockable"");
           rootNode.save();
       }
       session.logout();
   }

   public void shutdown() throws RepositoryException {
       repository.shutdown();
   }

   public Node getFolder(Session session) throws RepositoryException {
       return session.getRootNode().getNode(""folder"");
   }

   public void run() {

       for (int i = 0; i < 10; i++) {
           log.info(""START id:"" + id + "", i="" + i);

           try {
               session.refresh(false);

               Node folder = getFolder(session);
               folder.lock(false, true);
               folder.unlock();

               log.info(""SUCCESS id:"" + id + "", i="" + i);
           }
           catch (LockException e) {
               log.info(""FAIL:"" + id + "", i="" + i);
           }
           catch (Exception e) {
               log.warn(""ERROR:"" + id + "", i="" + i, e);
           }


       }

   }
}


15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=0
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=0
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=0
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:0, i=0
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=1
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:0, i=1
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:1, i=0
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=1
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:1, i=1
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=2
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:1, i=2
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=3
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:1, i=3
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=4
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=2
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:0, i=2
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=3
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:0, i=3
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=4
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:0, i=4
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=5
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:97) - SUCCESS id:1, i=4
15:46:17 WARN  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:103) - ERROR:0, i=5
javax.jcr.ItemNotFoundException: 7c198c7b-76c8-47c8-96a8-d9dfefd4b387/{http://www.jcp.org/jcr/1.0}lockOwner
       at org.apache.jackrabbit.core.ItemManager.createItemInstance(ItemManager.java:463)
       at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:319)
       at org.apache.jackrabbit.core.NodeImpl.getProperty(NodeImpl.java:1436)
       at org.apache.jackrabbit.core.NodeImpl.getOrCreateProperty(NodeImpl.java:428)
       at org.apache.jackrabbit.core.NodeImpl.internalSetProperty(NodeImpl.java:1267)
       at org.apache.jackrabbit.core.NodeImpl.lock(NodeImpl.java:3740)
       at JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:94)
15:46:17 WARN  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:103) - ERROR:2, i=0
javax.jcr.InvalidItemStateException: 7c198c7b-76c8-47c8-96a8-d9dfefd4b387 has been modified externally
       at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1193)
       at org.apache.jackrabbit.core.NodeImpl.unlock(NodeImpl.java:3790)
       at JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:95)
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=5
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=6
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=1
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:1, i=5
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:0, i=6
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:2, i=1
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=6
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=7
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=2
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:1, i=6
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:0, i=7
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:2, i=2
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=7
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=8
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=3
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:1, i=7
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:0, i=8
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:2, i=3
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=8
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=9
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=4
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:1, i=8
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:0, i=9
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:2, i=4
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=9
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=5
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:2, i=5
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=6
15:46:18 WARN  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:103) - ERROR:1, i=9
javax.jcr.InvalidItemStateException: /folder: the node cannot be saved because it has been modified externally.
       at org.apache.jackrabbit.core.NodeImpl.makePersistent(NodeImpl.java:908)
       at org.apache.jackrabbit.core.ItemImpl.persistTransientItems(ItemImpl.java:682)
       at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1173)
       at org.apache.jackrabbit.core.NodeImpl.lock(NodeImpl.java:3744)
       at JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:94)
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:2, i=6
15:46:18 WARN  org.apache.jackrabbit.core.lock.LockManagerImpl$LockInfo.loggingOut(LockManagerImpl.java:892) - Unable to unlock session-scoped lock on node '7c198c7b-76c8-47c8-96a8-d9dfefd4b387-W': Unable to unlock node. Node has pending changes: /folder
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=7
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:2, i=7
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=8
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:2, i=8
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=9
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:2, i=9

"
1,"Deadlock in DefaultISMLockingThere seems to be a bug in DefaultISMLocking which was detected as part of JCR-2746.

1) The main thread gets a read lock.

2) The ObservationManager thread tries to lock for writing, which is blocked because there is still a read lock.

3) Then the main thread tries to get a second read lock, which is blocked because there is a waiting write lock.

The bug was introduced as part of JCR-2089 (Use java.util.concurrent), revisions 995411 and 995412. I think the safe solution is to revert those to commits, and add a test case. If the DefaultISMLocking is changed later on, more test cases are required. An efficient solution is relatively complicated.
"
1,"A failure to connect to a MySQL database when JackRabbit starts a session leaves a .lock file in the repository. Subsequent sessions cannot be created by the same thread.I investigating the robustness of JackRabbit in the face of unexpected database errors, such as the database being unavailable. In my particular case, I am attempting to start a JackRabbit session using a TransientRepository while the database is not yet running. This correctly fails. However, if I attempt to create another session within the same thread after a short while, an exception occurs saying that the repository has already been locked. I would expect the repository folder not to be locked. Maybe the code meant to remove the .lock file was not triggered because of an uncaught exception.

Please see the attached files:
-a test class to reproduce the problem
-my repository.xml config
-the log file quantel.txt with details about the stack trace.
"
1,"DbDatastore: Problems indexing pdf fileAs reported by Claus Kll:

When importing a pdf file into a repository configured with a DbDataStore the following exception occurs. This happens only when using the DbDataStore with copyWhenReading=true

java.io.IOException: Stream closed
       at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:156)
       at java.io.BufferedInputStream.read(BufferedInputStream.java:315)
       at org.apache.jackrabbit.core.data.db.TempFileInputStream.read(TempFileInputStream.java:107)
       at java.io.BufferedInputStream.read1(BufferedInputStream.java:265)
       at java.io.BufferedInputStream.read(BufferedInputStream.java:324)
       at java.io.BufferedInputStream.fill(BufferedInputStream.java:229)
       at java.io.BufferedInputStream.read(BufferedInputStream.java:246)
       at java.io.FilterInputStream.read(FilterInputStream.java:89)
       at java.io.PushbackInputStream.read(PushbackInputStream.java:141)
       at org.pdfbox.io.PushBackInputStream.peek(PushBackInputStream.java:71)
       at org.pdfbox.io.PushBackInputStream.isEOF(PushBackInputStream.java:88)
       at org.pdfbox.pdfparser.PDFParser.parseObject(PDFParser.java:370)
       at org.pdfbox.pdfparser.PDFParser.parse(PDFParser.java:176)"
1,"o.a.j.spi.commons.query.sql2.ParserTest uses platform encoding with non-ASCII charactersThe ParserTest class loads a series of test SQL statements from test.sql2.txt, which contains a few non-ASCII characters (good to test those!). Unfortunately the file is read using the default platform encoding, which breaks the Linux-based test builds.

I'll recode the file to UTF-8 and explicitly specify the encoding when the file is read."
1,JCA build failure with J2EE 1.3The fix to JCR-736 introduced a similar problem as was previously reported in JCR-413. The fix in JCR-413 should apply also to this case.
1,"Benchmark does not close its Reader when OpenReader/CloseReader are not usedOnly the Searcher is closed, but because the reader is passed to the Searcher, the Searcher does not close the Reader, causing a resource leak."
1,"leak in MultiThreadedHttpConnectionManager.ConnectionPool.mapHostsOnce entries are added to MultiThreadedHttpConnectionManager.ConnectionPool.mapHosts, they are never cleaned up unless MultiThreadedHttpConnectionManager is shutdown."
1,"InternalValue.createCopy for binary properties (jcr:data) causes problemsRunning 1.4 with no data store configured, and option org.jackrabbit.useDataStore not set (i.e true), the following code gives 0 for the property length.

Node n = root.getNode(relPath);
session.getWorkspace().copy(n.getPath(), destPath);
Node contentNode = n.getNode(JcrConstants.JCR_CONTENT);
Property p = contentNode.getProperty(JcrConstants.JCR_DATA);
System.out.println(""length = ""+p.getLength());

InternalValue.createCopy checks USE_DATA_STORE and returns the same value for the source node's state. BundleBinding.writeState() calls BLOBInMemory.discard() when persisting the new node. This has now changed the value of the existing nodes property. Setting the option org.jackrabbit.useDataStore to false works fine. Possibly the check for binary property type in InternalValue.createCopy should be done first?"
1,"wrong eval order of access control entries within a single node (node-based ac)it seems to me that with the node-based access control the ac entries within a given node are currently collected in the wrong order.
if i remember correctly this worked before and i removed at some point (for reasons i don't recall exactly but have the vague idea that it
was related to the allow-only for groups).

anyway:
while playing around with the permission in our CRX recently i found, that the evaluation of the following setup didn't work as I would
have expected:

- user A is member of group B and C
- for both groups an ACE exists on a given node /a/b/c
- the acl looks like  { deny for B, allow for C }

I would have expected that the allow for C would have reverted the previous deny for B since - in the GUI - I read the ace eval order from first entry to last entry... in the order I added them."
1,"RMI problems prevent proper startup of the Jackrabbit webappA trouble in binding the repository to a RMI registry will prevent the entire Jackrabbit webapp from starting properly. Since RMI is seldom the primary function of the webapp, it's more appropriate to simply log a warning in such cases."
1,"URI path resolution problems.URI does not completely conform to RFC 2396.  In particular it does not handle the following 
relative URIs correctly:

../../../g
../../../../g"
1,"IndexWriter.numDocs doesn't take into account applied but not flushed deletesThe javadoc states that buffered deletes are not taken into account and so you must call commit first.

But, if you do that, and you're using CMS, and you're unlucky enough to have a background merge commit just after you call commit but before you call .numDocs, you can still get a wrong count back.

The fix is trivial -- numDocs should also consult any pooled readers for their current del count.

This is causing an intermittent failure in the new TestNRTThreads.
"
1,"DatabaseJournal improperly finds tables in external schemas when used on OracleThe DatabaseJournal currently calls database metadata to determine if the journal table has already been created.  It uses the following code to do so:

ResultSet rs = metaData.getTables(null, null, tableName, null);

The Oracle driver sometimes will return the table if it is in another schema on the same database.  Other DBMS code within JackRabbit has a specific Oracle version that handles this case.  In order for the journal table to be properly created, Oracle databases will need the schema name included in the getTables() call."
1,"NPE in MultiReader.isCurrent() and getVersion()I'm attaching a fix for the NPE in MultiReader.isCurrent() plus a testcase. For getVersion(), we should throw a better exception that NPE. I will commit unless someone objects or has a better idea."
1,"TransientRepository with LocalFileSystem eventually causes Repository data to be stored at path '/'I'm using a TransitoryRepository for my unit testing, with the repository's file system specified as:

    <FileSystem class=""org.apache.jackrabbit.core.fs.local.LocalFileSystem"">
        <param name=""path"" value=""${rep.home}/repository""/>
    </FileSystem>

I noticed today that when I run my unit tests Jackrabbit is creating four directories at the root of my hard drive: ""meta"", ""namespaces"", ""nodetypes"", and ""data"". I tracked the problem the fact that when a LocalFileSystem is closed, it sets the ""root"" to null - an invalid state. But when using a TransitoryRepository, the invalid state is never discovered because the LocalFileSystem object itself is not released, or re-initialized. It is simply used to create BasedFileSystem objects in RepositoryImpl. Calls to BasedFileSystem defer to the LocalFileSystem object that now has a null root. Inside the LocalFileSystem, all the calls to Java's io.File constructor have a ""null"" parent parameter, causing File to fall back to its single argument constructor which sees the path ""/meta"" and happily creates files at the root of the disk.

I'm not sure what the best solution is, but some thoughts I've had are:
- don't set the ""root"" property to null when closing a LocalFileSystem
- make RepositoryConfig re-init the FileSystem variable when it is accessed.
- don't cache the RepositoryConfig in TransitoryRepository (this might also require a new constructor that takes a class-path resource for the repository configuration file)"
1,"Abort Before Execute & Various Other Times FailsWith svn commit #639506, a few more scenarios become testable & can be fixed.  These are: aborting before HttpClient.execute is called, aborting between setting the connection request for aborting and setting the connection release trigger, and aborting after a redirected route uses a new connection request.  As of r639506, those three scenarios fail to abort correctly."
1,"jcr:successors property not persisted correctly within a transactionDuring a transaction, if you create a new version then read the version history the ""jcr:successors"" property is not updated. Note that ""jcr:predecessors"" is updated properly.

Also, the version history is sometimes not propertly read. During the transaction, it might appear empty. This behavior in not consistent from one execution to another.

After a restart of the repository, the version history and the ""jcr:successors"" property is read properly.

* Tests cases will follow shortly.

Thanks, 

Nicolas"
1,"IndexWriter.addIndexes(IndexReader[]) fails to create compound filesEven if no exception is thrown while writing the compound file at the end of the 
addIndexes() call, the transaction is rolled back and the successfully written cfs 
file deleted. The fix is simple: There is just the 
{code:java}
success = true;
{code}
statement missing at the end of the try{} clause.

All tests pass. I'll commit this soon to trunk and 2.3.2."
1,"In case of SocketTimeoutException and using HttpRequestRetryHandler the execution is always +1If my request encounter a SocketTimeoutException, the HttpRequestRetryHandler#retryRequest will be called with an executionCount with a value +1."
1,"Add the org.apache.jackrabbit.rmi.jackrabbit package to the rmic generation From the UnicastRemoteObject's (ServerJackrabbitNodeTypeManager, ServerJackrabbitWorkspace) should be stubs generated.
"
1,"Malformed excerpt if content contains markup and no highlights foundAny markup in content that is used in an excerpt is encoded with corresponding entity references. However, this process is broken when there are no highlights in the excerpt. In this case, the content is provided as is in the excerpt, which may lead to malformed HTML/XML."
1,"cannot PUT changes to a resource in the simple webdav serverwhen using the simple webdav server to PUT a resource, the ""versionable"" mixin node type is assigned to the new node without regard to whether the node type is already assigned to the node. this causes PUT requests that change existing resources to fail with 403 errors.

the fix is to augment AddMixinCommand to not try to add the mixin node type if the node already has it.
"
1,ItemInfoBuilder fails to set correct path on propertiesThis only happens if the parent node's nodeId is id based (in contrast to path based). The build() method should not rely on the nodeId providing the full path. Instead it should us the parent node's getPath() method to construct the full path. 
1,"ContentEncodingHttpClient.execute(HttpGet, ResponseHandler<T>) throws IOException when reading compressed responseThe following snippet:

    String url = ""http://yahoo.com"";
    HttpClient httpClient = new ContentEncodingHttpClient();
    HttpGet get = new HttpGet(url);
    String content = httpClient.execute(get, new BasicResponseHandler());

throws:

java.io.IOException: Attempted read from closed stream.
	at org.apache.http.impl.io.ChunkedInputStream.read(ChunkedInputStream.java:126)
	at java.util.zip.CheckedInputStream.read(CheckedInputStream.java:42)
	at java.util.zip.GZIPInputStream.readUByte(GZIPInputStream.java:205)
	at java.util.zip.GZIPInputStream.readUShort(GZIPInputStream.java:197)
	at java.util.zip.GZIPInputStream.readHeader(GZIPInputStream.java:136)
	at java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:58)
	at java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:68)
	at org.apache.http.client.entity.GzipDecompressingEntity.getContent(GzipDecompressingEntity.java:63)
	at org.apache.http.conn.BasicManagedEntity.getContent(BasicManagedEntity.java:88)
	at org.apache.http.util.EntityUtils.consume(EntityUtils.java:65)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:974)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:919)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:910)
	at tv.adap.service.HttpPoolTest.testChunkedGzip(HttpPoolTest.java:41)

whereas the following snippet runs fine:

    String url = ""http://yahoo.com"";
    HttpClient httpClient = new ContentEncodingHttpClient();
    HttpGet get = new HttpGet(url);
    HttpResponse response = httpClient.execute(get);
    HttpEntity entity = response.getEntity();
    String content = EntityUtils.toString(entity);

These two snippets should be functionally the same (putting the entity body into content). Creating a JIRA per the recommendation of Oleg from httpclient-users."
1,"New socket timeout value wont have effect if connection is reusedReported by Teemu Tingander <Teemu.Tingander at tecnomen.fi> on The Jakarta
Commons HttpClient Developer List:

<snip>
Changing read timeout ()wont affect after successful method execution using
same connection.. 

This seems to be a bug in HttpClient class method
executeMethod(HostConfiguration ...)..

The problematic section seems to be if section checking if connection is
open
	
		method.setStrictMode(strictMode);
        		        
            if (!connection.isOpen()) {                
                connection.setConnectionTimeout(connectionTimeout);
-->		    connection.setSoTimeout(soTimeout);
                connection.open();
                if (connection.isProxied() && connection.isSecure()) {
                    method = new ConnectMethod(method);
                }
            }
 
Problem can be solved by moving the line out of if section

		method.setStrictMode(strictMode);

		connection.setSoTimeout(soTimeout);	
        		        
            if (!connection.isOpen()) {                
                connection.setConnectionTimeout(connectionTimeout);
                connection.open();
                if (connection.isProxied() && connection.isSecure()) {
                    method = new ConnectMethod(method);
                }
            }
</snip>"
1,"Check for correct content-type in URLEncodedUtils not working for encoding-suffixesDear DEV-Team,

i am developing an application with the httpclient. Today i found a small problem, related to URLEncodedUtils.

Our Tomcat-Server deliveres for Server-Requests, the HTTP-Header: ""Content-Type=application/x-www-form-urlencoded;charset=UTF-8"", but the httpclient only checks for: ""Content-Type=application/x-www-form-urlencoded"". This failing check results in an empty result of call to the method: URLEncodedUtils.parse(entity);

Following source-code causes the prob: 

public class URLEncodedUtils {

    /**
     * Returns true if the entity's Content-Type header is
     * <code>application/x-www-form-urlencoded</code>.
     */
    public static boolean isEncoded (final HttpEntity entity) {
        final Header contentType = entity.getContentType();
        return (contentType != null && contentType.getValue().equalsIgnoreCase(CONTENT_TYPE));
    }
}

IMO the method should be changed to:


public class URLEncodedUtils {

    /**
     * Returns true if the entity's Content-Type header is
     * <code>application/x-www-form-urlencoded</code>.
     */
    public static boolean isEncoded (final HttpEntity entity) {
        final Header contentType = entity.getContentType();
        return (contentType != null && contentType.getValue().startsWith(CONTENT_TYPE + "";""));
    }
}

Best Regards,"
1,GC resources in TermInfosReader when exception occurs in its constructorI replaced IndexModifier with IndexWriter in test case TestStressIndexing and noticed the test failed from time to time because some .tis file is still open when MockRAMDirectory.close() is called. It turns out it is because .tis file is not closed if an exception occurs in TermInfosReader's constructor.
1,"BundleDbPersistenceManager consistencyFix doesn't fix missing non system childnode  entries of the root nodeThe bundle check/fix mechanism completely skips the checks on the root node, but the root node can also have non system child node entries which can be broken/missing. The attached patch makes the check only check the non system child node entries of the root node. It would be nice if this patch (if/when accepted) could also be backported to the 1.5 and 1.6 branches.
"
1,"WebDAV server should treat non-wellformed XML in request bodies as errorThe WebDAV server should treat non-wellformed XML request bodies as errors (instead of treating the request as if the request body was missing).

(causes Litmus test suite failure in test case propfind_invalid)"
1,"ConcurrentModificationException thrown in MultiThreaded codeNow seeing this error.  This is with default cookie settings.  Happening rarely, however the web sites we're talking to do not use cookies very much.


java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
        at java.util.AbstractList$Itr.next(AbstractList.java:343)
        at java.util.Collections$UnmodifiableCollection$1.next(Collections.java:1010)
        at org.apache.http.client.protocol.RequestAddCookies.process(RequestAddCookies.java:152)
        at org.apache.http.protocol.BasicHttpProcessor.process(BasicHttpProcessor.java:290)
        at org.apache.http.protocol.HttpRequestExecutor.preProcess(HttpRequestExecutor.java:160)
        at org.apache.http.impl.client.DefaultClientRequestDirector.execute(DefaultClientRequestDirector.java:355)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:501)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:456)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:422)
        at com.hi5.os.Hi5RemoteContentFetcher.fetch(Hi5RemoteContentFetcher.java:279)"
1,"spi2dav: Overwrite header T specified for MOVE and COPY causes failure if some API testsfailing tests are:

org.apache.jackrabbit.test.api.WorkspaceCopySameNameSibsTest#testCopyNodesNodeExistsAtDestPath
org.apache.jackrabbit.test.api.WorkspaceMoveSameNameSibsTest#testMoveNodesNodeExistsAtDestPath

those would be fixed by setting the overwrite header to F(alse)... however, this doesn't fit those cases where same-same
siblings would be allowed and the copy/move to a destination with existing item would succeed in JCR."
1,"Jackrabbit logs a NullPointerException on shutdown if the version manager wasn't initializedIf opening the repository fails, and the version manager was not initialized, then the shutdown method logs a NullPointerException when trying to close the version manager. This is a nuisance."
1,"MultiThreadedHttpConnectionManager setMaxTotalConnections() method doesn't workThe deprecated setMaxTotalConnections() method in the
MultiThreadedHttpConnectionManager seems like it has no effect:

Here is the source code in the current version:

    public void setMaxTotalConnections(int maxTotalConnections) {
        this.params.getMaxTotalConnections();
    }

Shouldn't it look more like this?

    public void setMaxTotalConnections(int maxTotalConnections) {
        this.params.setMaxTotalConnections(maxTotalConnections);
    }"
1,"DateUtil#formatDate uses default locale instead of USProblem reported by Yannick <yannick at meudal.net> on the httpclient-user list

==================================================================
Hello,

This is a bug report.

I'm using Commons HTTPClient (rc2) for generating HTTP requests. I put in 
headers some specific header, like the If-Modified-Since attribute. 
When I generate the date through DateUtil.formatDate method, I get a 
localized date, in french. Example: 
If-Modified-Since: dim., 10 avr. 2005 05:04:08 CEST

I get problems on my http server during parsing the received date. This is 
not a RFC 2616 compliant date format. It should be:
If-Modified-Since: Sun, 10 Apr 2005 05:04:08 CEST

A patch should be applied, by creating a new SimpleDateFormat(pattern, 
Locale.US) instead of SimpleDateFormat(pattern) (like it is done in the 
parse method, line #159).

org.apache.commons.httpclient.util.DateUtil, line #205:

    public static String formatDate(Date date, String pattern) {
        if (date == null) throw new IllegalArgumentException(""date is 
null"");
        if (pattern == null) throw new IllegalArgumentException(""pattern 
is null"");
 
        SimpleDateFormat formatter = new SimpleDateFormat(pattern, 
Locale.US);
        return formatter.format(date);
    }


Regards,

Yannick."
1,"String properties with invalid XML characters export as invalid XMLAs noted in the current JCR 1.0.1 maintenance draft, sections 6.4.1,
6.4.2.6, XML export of string properties that contain invalid XML
characters isn't well-defined currently, since those characters are
not permissible in XML.  The proposed fix is to use base64
encoding for such values in System View.

Most characters below #x20 are examples of this.  Currently, these
are escaped numerically in output (such as (amp)#0; )  but
such escape sequences can't be parsed by the XML
import methods.

The current behavior is particularly problematic, because the user
doesn't know the output is corrupt until later, when they try to import it
and get InvalidSerializedDataException.

If for some reason the base64 option is delayed, it might
make sense, as an interim solution, to fail on export
or to somehow patch import to relax its parsing and allow
these escape codes."
1,"Escape colon in statement of jcr:containsThe colon is a special character in the lucene query parser and allows to prefix query terms with an optional field name. JCR does not specify such a feature, thus a colon in the fulltext statement should be treated as a regular character. "
1,InstantiatedIndexReader does not implement getFieldNames properlyCauses error in org.apache.lucene.index.SegmentMerger.mergeFields
1,"BasicCookieStore treats cookies of the same name from the same host as duplicates, even if they have different pathsThe DefaultHttpClient is not handling cookies correctly when a single host returns multiple cookies of the same name but with separate paths.  For example, if a single instance of the client is used to access two different webapps on the same server, it may receive two different JSESSIONID cookies:

Cookie: [version: 0][name: JSESSIONID][value: F832C01D23F501CE5EEB296B602700C1][domain: lglom139.example.com][path: /msa-adrenalina][expiry: null]
Cookie: [version: 0][name: JSESSIONID][value: 0FC660347391B93267168F84F2B520F5][domain: lglom139.example.com][path: /maps][expiry: null]

Because the CookieIdentityComparator class does not test the cookie path when determining equality, each new JSESSIONID received replaces the previous one instead of adding a new cookie to the store.  This results in ""disconnecting"" the client from its sessions on the prior webapps.

I've confirmed that adding a path test to CookieIdentityComparator resolves this problem."
1,"Enabling wire logging changes isEof/isStale behaviorIf you enable wire logging, DefaultClientConnection wraps the SocketInputBuffer with a LoggingSessionInputBuffer. This hides the EofSensor interface implemented by SocketInputBuffer (but not LoggingSessionInputBuffer), which makes at least AbstractHttpClientConnection.isEof() and isStale() methods behave differently.

(That is, stale connection checks won't really work as intended if wire logging is enabled. Which makes it a bit difficult to debug problems related to stale connections...)

Proposed fix: implement EofSensor interface in LoggingSessionInputBuffer (delegating it to wrapped buffer).
"
1,"Workspace.clone() fails the second time, if cloning referenceablesthe following testcode fails with the 2nd clone. please note, that if the 'folder' node is not made
referenceable, the test passes (copied an adapted from test in WorkspaceCloneTest).

    public void testCloneNodesTwice() throws RepositoryException {
        // clone referenceable node below non-referenceable node
        String dstAbsPath = node2W2.getPath() + ""/"" + node1.getName();

        Node folder = node1.addNode(""folder"");
        folder.addMixin(""mix:referenceable"");
        node1.save();
        workspaceW2.clone(workspace.getName(), node1.getPath(), dstAbsPath, true);
        workspaceW2.clone(workspace.getName(), node1.getPath(), dstAbsPath, true);

        // there should not be any pending changes after clone
        assertFalse(superuserW2.hasPendingChanges());
    }

"
1,"Problem importing node with binary property in a repository configured with datastoreUsing the importXML method of workspace to import some node containing binary properties the nodes are imported correctly and the value of the binary data property is imported
However the binary data goes to the db (persistenceManager) an not to the datastore.

Creating a new node of the same type using the api, the binary data go to the datastore."
1,"IndexReader.indexExists sometimes returns true when an index isn't presentIf you open a writer on a new dir and prepareCommit but don't finish the commit, IndexReader.indexExists incorrectly returns true, because it just checks for whether a segments_N file is present and not whether it can be successfully read."
1,"Findbugs reports and fixesRan findbugs 0.94.rc1 on 3.0RC4. 
Fixed a few of the obvious ones (patches to follow) and made notes on the 
remainder - see the //TODO markers in code.
Also created a findbugs target in build.xml - see appropriate patch file"
1,"MinPayloadFunction returns 0 when only one payload is presentIn some experiments with payload scoring through PayloadTermQuery, I'm seeing 0 returned when using MinPayloadFunction.  I believe there is a bug there.  No time at the moment to flesh out a unit test, but wanted to report it for tracking."
1,"MultiReader does not propagate readerFinishedListeners to clones/reopened readersWhile working on refactoring MultiReader/DirectoryReader in trunk, I found out that MultiReader does not correctly pass readerFinishedListeners to its clones and reopened readers."
1,"IndexReader.open(String|File) may incorrectly throw AlreadyClosedExceptionSpinoff from here:

    http://www.nabble.com/Runtime-exception-when-creating-IndexSearcher-to20226279.html

If you open an IndexSearcher/Reader, passing in String or File, then
closeDirectory is set to true in the reader.

If the index has a single segment, then SegmentReader.get is used to
open the index.  If an IOException is hit in there, the SegmentReader
closes itself and then closes the directory since closeDirectory is
true.

The problem is, the retry logic in SegmentInfos (to look for another
segments_N to try) kicks in and hits an AlreadyClosedException,
masking the original root cause.

Workaround is to separately get the Directory using
FSDirectory.getDirectory, and then instantiate IndexSearcher/Reader
from that.

This manifests as masking the root cause of a corrupted single-segment
index with a confusing AlreadyClosedException.  You could also hit
the false exception if the writer was in the process of committing
(ie, a retry was really needed) or if there is some transient IO
problem opening the index (eg too many open files).
"
1,"Incorrect sort by Numeric values for documents missing the sorting fieldWhile sorting results over a numeric field, documents which do not contain a value for the sorting field seem to get 0 (ZERO) value in the sort. (Tested against Double, Float, Int & Long numeric fields ascending and descending order).
This behavior is unexpected, as zero is ""comparable"" to the rest of the values. A better solution would either be allowing the user to define such a ""non-value"" default, or always bring those document results as the last ones.

Example scenario:
Adding 3 documents, 1st with value 3.5d, 2nd with -10d, and 3rd without any value.
Searching with MatchAllDocsQuery, with sort over that field in descending order yields the docid results of 0, 2, 1.

Asking for the top 2 documents brings the document without any value as the 2nd result - which seems as a bug?"
1,"Using MultiSearcher and ParallelMultiSearcher can change the sort order.When using multiple sort criteria the first criterium that indicates a difference should be used.
When a field does not exist for a given document, special rules apply.
From what I see in the code, it is sorted as 0 for integer and float fields, and null Strings are sorted before others.

This works correctly in both Lucene 1.4.3 and in trunk as long as you use a single IndexSearcher (except perhaps in special cases, see other bug reports like LUCENE-374).

However, in MultiSearcher and ParallelMultiSearcher, the results of the separate IndexSearchers are merged and there an error occurs.
The bug is located in FieldDocSortedHitQueue.

It can even be demonstrated by passing a single indexSearcher to a MultiSearcher.

TestCase and patch follow."
1,FilteredQuery ignores boostFiltered query ignores it's own boost.
1,"Registering nodetypes with empty namespace prefix causes a namespace exception in sync nodeRegistering a nodetype with empty namespace prefix causes a namespace exception in sync node. Stacktrace looks as follows:

03.03.2008 15:33:50 *ERROR* ClusterNode: Unable to read revision '10618'. (ClusterNode.java, line 1051)
o.a.j.core.journal.JournalException: Parse error while reading node type definition.
        at o.a.j.core.journal.AbstractRecord.readNodeTypeDef(AbstractRecord.java:256)
        at o.a.j.core.cluster.ClusterNode.consume(ClusterNode.java:1026)
        at o.a.j.core.journal.AbstractJournal.doSync(AbstractJournal.java:198)
        at o.a.j.core.journal.AbstractJournal.sync(AbstractJournal.java:173)
        at o.a.j.core.cluster.ClusterNode.sync(ClusterNode.java:303)
        at o.a.j.core.cluster.ClusterNode.run(ClusterNode.java:274)
        at java.lang.Thread.run(Thread.java:595)
Caused by: o.a.j.core.nodetype.compact.ParseException: Error while parsing 'bla' ((internal), line 3)
        at o.a.j.core.nodetype.compact.Lexer.fail(Lexer.java:152)
        at o.a.j.core.nodetype.compact.CompactNodeTypeDefReader.toQName(CompactNodeTypeDefReader.java:653)
        at o.a.j.core.nodetype.compact.CompactNodeTypeDefReader.doNodeTypeName(CompactNodeTypeDefReader.java:265)
        at o.a.j.core.nodetype.compact.CompactNodeTypeDefReader.parse(CompactNodeTypeDefReader.java:215)
        at o.a.j.core.nodetype.compact.CompactNodeTypeDefReader.<init>(CompactNodeTypeDefReader.java:178)
        at o.a.j.core.nodetype.compact.CompactNodeTypeDefReader.<init>(CompactNodeTypeDefReader.java:162)
        at o.a.j.core.journal.AbstractRecord.readNodeTypeDef(AbstractRecord.java:248)
        ... 6 more
Caused by: javax.jcr.NamespaceException: No URI for pefix '' declared.
        at o.a.j.spi.commons.namespace.NamespaceMapping.getURI(NamespaceMapping.java:74)
        at o.a.j.spi.commons.conversion.NameParser.parse(NameParser.java:116)
        at o.a.j.spi.commons.conversion.ParsingNameResolver.getQName(ParsingNameResolver.java:62)
        at o.a.j.spi.commons.conversion.DefaultNamePathResolver.getQName(DefaultNamePathResolver.java:61)
        at o.a.j.core.nodetype.compact.CompactNodeTypeDefReader.toQName(CompactNodeTypeDefReader.java:646)
        ... 11 more

"
1,Incorrect results from joins on multivalued propertiesIt looks like join conditions on multivalued properties only use one of the multiple values for the comparison.
1,"Custom LoginModule configurations broken in 1.5.0Upgrading Jackrabbit from 1.4.5 to 1.5 has created an LDAP exception.  The configuration file which has not changed (except for the adding the new SimpleSecurityManager as required) is the default with the following substituted for the LoginModule:

        <LoginModule class=""com.sun.security.auth.module.LdapLoginModule"">
            <param name=""userProvider"" value=""ldap://localhost/ou=people,dc=example,dc=com"" />
            <param name=""userFilter"" value=""(&amp;(uid={USERNAME})(objectClass=inetOrgPerson))"" />
            <param name=""authzIdentity"" value=""{USERNAME}"" />
            <param name=""debug"" value=""true"" />
        </LoginModule>

This configuration worked correctly and I was able to authenticate properly with Jackrabbit 1.4.5
The same configuration with 1.5 throws the following exception:

javax.jcr.LoginException: com.sun.security.auth.module.LdapLoginModule does not support 'userProvider: com.sun.security.auth.module.LdapLoginModule does not support 'userProvider: com.sun.security.auth.module.LdapLoginModule does not support 'userProvider
        at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1414)
        at org.apache.jackrabbit.jca.JCAManagedConnectionFactory.openSession(JCAManagedConnectionFactory.java:140)
        at org.apache.jackrabbit.jca.JCAManagedConnectionFactory.createManagedConnection(JCAManagedConnectionFactory.java:176)
        at org.apache.jackrabbit.jca.JCAManagedConnectionFactory.createManagedConnection(JCAManagedConnectionFactory.java:168)
        at com.sun.enterprise.resource.ConnectorAllocator.createResource(ConnectorAllocator.java:136)
        at com.sun.enterprise.resource.AbstractResourcePool.createSingleResource(AbstractResourcePool.java:891)
        at com.sun.enterprise.resource.AbstractResourcePool.createResourceAndAddToPool(AbstractResourcePool.java:1752)
        at com.sun.enterprise.resource.AbstractResourcePool.createResources(AbstractResourcePool.java:917)
        at com.sun.enterprise.resource.AbstractResourcePool.initPool(AbstractResourcePool.java:225)
        at com.sun.enterprise.resource.AbstractResourcePool.internalGetResource(AbstractResourcePool.java:516)
        at com.sun.enterprise.resource.AbstractResourcePool.getResource(AbstractResourcePool.java:443)
        at com.sun.enterprise.resource.PoolManagerImpl.getResourceFromPool(PoolManagerImpl.java:248)
        at com.sun.enterprise.resource.PoolManagerImpl.getResource(PoolManagerImpl.java:176)
        at com.sun.enterprise.connectors.ConnectionManagerImpl.internalGetConnection(ConnectionManagerImpl.java:337)
        at com.sun.enterprise.connectors.ConnectionManagerImpl.allocateConnection(ConnectionManagerImpl.java:189)
        at com.sun.enterprise.connectors.ConnectionManagerImpl.allocateConnection(ConnectionManagerImpl.java:165)
        at com.sun.enterprise.connectors.ConnectionManagerImpl.allocateConnection(ConnectionManagerImpl.java:158)
        at org.apache.jackrabbit.jca.JCARepositoryHandle.login(JCARepositoryHandle.java:98)
        at org.apache.jackrabbit.jca.JCARepositoryHandle.login(JCARepositoryHandle.java:89)
        at org.apache.jackrabbit.jca.JCARepositoryHandle.login(JCARepositoryHandle.java:73)
        at com.threesl.Sapphire.CradleJCR.login(CradleJCR.java:44)   

 try {
            InitialContext ctx = new InitialContext();
            repository = (Repository) ctx.lookup(""jcr/repository"");
            session = repository.login(credentials);
        } catch (Exception e) {

        at com.threesl.Sapphire.CradleWS.doLogin(CradleWS.java:68)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jersey.impl.model.method.dispatch.EntityParamDispatchProvider$TypeOutInvoker._dispatch(EntityParamDispatchProvider.java:136)
        at com.sun.jersey.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:85)
        at com.sun.jersey.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:123)
        at com.sun.jersey.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:111)
        at com.sun.jersey.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:71)
        at com.sun.jersey.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:111)
        at com.sun.jersey.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:63)
        at com.sun.jersey.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:722)
        at com.sun.jersey.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:692)
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:344)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:831)
        at org.apache.catalina.core.ApplicationFilterChain.servletService(ApplicationFilterChain.java:411)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:290)
        at org.apache.catalina.core.StandardContextValve.invokeInternal(StandardContextValve.java:271)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:202)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:632)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:577)
        at com.sun.enterprise.web.WebPipeline.invoke(WebPipeline.java:94)
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:206)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:632)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:577)
        at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:571)
        at org.apache.catalina.core.ContainerBase.invoke(ContainerBase.java:1080)
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:150)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:632)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:577)
        at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:571)
        at org.apache.catalina.core.ContainerBase.invoke(ContainerBase.java:1080)
        at org.apache.coyote.tomcat5.CoyoteAdapter.service(CoyoteAdapter.java:272)
        at com.sun.enterprise.web.connector.grizzly.DefaultProcessorTask.invokeAdapter(DefaultProcessorTask.java:637)
        at com.sun.enterprise.web.connector.grizzly.DefaultProcessorTask.doProcess(DefaultProcessorTask.java:568)
        at com.sun.enterprise.web.connector.grizzly.DefaultProcessorTask.process(DefaultProcessorTask.java:813)
        at com.sun.enterprise.web.connector.grizzly.DefaultReadTask.executeProcessorTask(DefaultReadTask.java:341)
        at com.sun.enterprise.web.connector.grizzly.DefaultReadTask.doTask(DefaultReadTask.java:263)
        at com.sun.enterprise.web.connector.grizzly.DefaultReadTask.doTask(DefaultReadTask.java:214)
        at com.sun.enterprise.web.connector.grizzly.TaskBase.run(TaskBase.java:265)
        at com.sun.enterprise.web.connector.grizzly.ssl.SSLWorkerThread.run(SSLWorkerThread.java:106)
Caused by: javax.security.auth.login.LoginException: com.sun.security.auth.module.LdapLoginModule does not support 'userProvider
        at org.apache.jackrabbit.core.security.authentication.LocalAuthContext.login(LocalAuthContext.java:68)
        at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1407)
        ... 62 more
javax.security.auth.login.LoginException: com.sun.security.auth.module.LdapLoginModule does not support 'userProvider
        at org.apache.jackrabbit.core.security.authentication.LocalAuthContext.login(LocalAuthContext.java:68)
        at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1407)
        at org.apache.jackrabbit.jca.JCAManagedConnectionFactory.openSession(JCAManagedConnectionFactory.java:140)
        at org.apache.jackrabbit.jca.JCAManagedConnectionFactory.createManagedConnection(JCAManagedConnectionFactory.java:176)
        at org.apache.jackrabbit.jca.JCAManagedConnectionFactory.createManagedConnection(JCAManagedConnectionFactory.java:168)
        at com.sun.enterprise.resource.ConnectorAllocator.createResource(ConnectorAllocator.java:136)
        at com.sun.enterprise.resource.AbstractResourcePool.createSingleResource(AbstractResourcePool.java:891)
        at com.sun.enterprise.resource.AbstractResourcePool.createResourceAndAddToPool(AbstractResourcePool.java:1752)
        at com.sun.enterprise.resource.AbstractResourcePool.createResources(AbstractResourcePool.java:917)
        at com.sun.enterprise.resource.AbstractResourcePool.initPool(AbstractResourcePool.java:225)
        at com.sun.enterprise.resource.AbstractResourcePool.internalGetResource(AbstractResourcePool.java:516)
        at com.sun.enterprise.resource.AbstractResourcePool.getResource(AbstractResourcePool.java:443)
        at com.sun.enterprise.resource.PoolManagerImpl.getResourceFromPool(PoolManagerImpl.java:248)
        at com.sun.enterprise.resource.PoolManagerImpl.getResource(PoolManagerImpl.java:176)
        at com.sun.enterprise.connectors.ConnectionManagerImpl.internalGetConnection(ConnectionManagerImpl.java:337)
        at com.sun.enterprise.connectors.ConnectionManagerImpl.allocateConnection(ConnectionManagerImpl.java:189)
        at com.sun.enterprise.connectors.ConnectionManagerImpl.allocateConnection(ConnectionManagerImpl.java:165)
        at com.sun.enterprise.connectors.ConnectionManagerImpl.allocateConnection(ConnectionManagerImpl.java:158)
        at org.apache.jackrabbit.jca.JCARepositoryHandle.login(JCARepositoryHandle.java:98)
        at org.apache.jackrabbit.jca.JCARepositoryHandle.login(JCARepositoryHandle.java:89)
        at org.apache.jackrabbit.jca.JCARepositoryHandle.login(JCARepositoryHandle.java:73)
        at com.threesl.Sapphire.CradleJCR.login(CradleJCR.java:44)
        at com.threesl.Sapphire.CradleWS.doLogin(CradleWS.java:68)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jersey.impl.model.method.dispatch.EntityParamDispatchProvider$TypeOutInvoker._dispatch(EntityParamDispatchProvider.java:136)
        at com.sun.jersey.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:85)
        at com.sun.jersey.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:123)
        at com.sun.jersey.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:111)
        at com.sun.jersey.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:71)
        at com.sun.jersey.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:111)
        at com.sun.jersey.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:63)
        at com.sun.jersey.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:722)
        at com.sun.jersey.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:692)
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:344)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:831)
        at org.apache.catalina.core.ApplicationFilterChain.servletService(ApplicationFilterChain.java:411)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:290)
        at org.apache.catalina.core.StandardContextValve.invokeInternal(StandardContextValve.java:271)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:202)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:632)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:577)
        at com.sun.enterprise.web.WebPipeline.invoke(WebPipeline.java:94)
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:206)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:632)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:577)
        at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:571)
        at org.apache.catalina.core.ContainerBase.invoke(ContainerBase.java:1080)
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:150)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:632)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:577)
        at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:571)
        at org.apache.catalina.core.ContainerBase.invoke(ContainerBase.java:1080)
        at org.apache.coyote.tomcat5.CoyoteAdapter.service(CoyoteAdapter.java:272)
        at com.sun.enterprise.web.connector.grizzly.DefaultProcessorTask.invokeAdapter(DefaultProcessorTask.java:637)
        at com.sun.enterprise.web.connector.grizzly.DefaultProcessorTask.doProcess(DefaultProcessorTask.java:568)
        at com.sun.enterprise.web.connector.grizzly.DefaultProcessorTask.process(DefaultProcessorTask.java:813)
        at com.sun.enterprise.web.connector.grizzly.DefaultReadTask.executeProcessorTask(DefaultReadTask.java:341)
        at com.sun.enterprise.web.connector.grizzly.DefaultReadTask.doTask(DefaultReadTask.java:263)
        at com.sun.enterprise.web.connector.grizzly.DefaultReadTask.doTask(DefaultReadTask.java:214)
        at com.sun.enterprise.web.connector.grizzly.TaskBase.run(TaskBase.java:265)
        at com.sun.enterprise.web.connector.grizzly.ssl.SSLWorkerThread.run(SSLWorkerThread.java:106)
RAR5117 : Failed to obtain/create connection from connection pool [ jackrabbit-connection-pool ]. Reason : Failed to create session: com.sun.security.auth.module.LdapLoginModule does not support 'userProvider: com.sun.security.auth.module.LdapLoginModule does not support 'userProvider

"
1,"AccessManager + CachingHierarchyManager problemThe problem we have is the implementation of the CachingHierarchyManager,
to which the SimpleAccessManager holds a reference.

Let's consider following example:
i add 3 subnodes (a,b,c) to a node and after that i reorder b and c ..
so i have a,c,b. in the process of reordering (using the function
orderBefore of javax.jcr.Node) our AccessManager is called several times to check the permissions of the nodes. In this AccessManager we use some
functions of the CachingHierarchyManager, f.ex.

Path itemPath = hierMgr.getPath(id);
return itemPath.denotesRoot();

or

Path itemPath = hierMgr.getPath(itemId);
Path parentPath = itemPath.getAncestor(1);
return hierMgr.resolvePath(parentPath);

the problem is, that when calling the methods of the
CachingHierarchyManager the nodes i ask for will be cached in the idCache in a wrong state (i. e.: before actually reordering the elements).
so if i want f.ex. delete the node b after reordering, the node will
be looked up in the idCache. in the cache the index of node b is still 2
(actually it should be 3) and so the wrong node will be deleted! "
1,"IndexWriter does not release its write lock when trying to open an index which does not yet existIn version 2.0.0, the private IndexWriter constructor does not properly remove its write lock in the event of an error. This can be seen when one attempts to open (not create) an index in a directory which exists, but in which there is no segments file. Here is the offending code:

    247   private IndexWriter(Directory d, Analyzer a, final boolean create, boolean closeDir)
    248     throws IOException {
    249       this.closeDir = closeDir;
    250       directory = d;
    251       analyzer = a;
    252 
    253       Lock writeLock = directory.makeLock(IndexWriter.WRITE_LOCK_NAME);
    254       if (!writeLock.obtain(writeLockTimeout)) // obtain write lock
    255         throw new IOException(""Index locked for write: "" + writeLock);
    256       this.writeLock = writeLock;                   // save it
    257 
    258       synchronized (directory) {        // in- & inter-process sync
    259         new Lock.With(directory.makeLock(IndexWriter.COMMIT_LOCK_NAME), commitLockTimeout) {
    260             public Object doBody() throws IOException {
    261               if (create)
    262                 segmentInfos.write(directory);
    263               else
    264                 segmentInfos.read(directory);
    265               return null;
    266             }
    267           }.run();
    268       }
    269   }

On line 254, a write lock is obtained by the constructor. If an exception is raised inside the doBody() method (on line 260), then that exception is propagated, the constructor will fail, but the lock is not released until the object is garbage collected. This is typically an issue except when using the IndexModifier class.

As of the filing of this bug, this has not yet been fixed in the trunk (IndexWriter.java#472959):

    251   private IndexWriter(Directory d, Analyzer a, final boolean create, boolean closeDir)
    252     throws IOException {
    253       this.closeDir = closeDir;
    254       directory = d;
    255       analyzer = a;
    256 
    257       Lock writeLock = directory.makeLock(IndexWriter.WRITE_LOCK_NAME);
    258       if (!writeLock.obtain(writeLockTimeout)) // obtain write lock
    259         throw new IOException(""Index locked for write: "" + writeLock);
    260       this.writeLock = writeLock;                   // save it
    261 
    262       synchronized (directory) {        // in- & inter-process sync
    263         new Lock.With(directory.makeLock(IndexWriter.COMMIT_LOCK_NAME), commitLockTimeout) {
    264             public Object doBody() throws IOException {
    265               if (create)
    266                 segmentInfos.write(directory);
    267               else
    268                 segmentInfos.read(directory);
    269               return null;
    270             }
    271           }.run();
    272       }
    273   }"
1,"JCR2SPI: potential race condition in event listener registrationThere's a potential race condition when the first event listener is registered (ObservationManager.addEventListener). The observation manager should only start listening for events after the new SPI event filter has been created.

(Note there's a related problem when an *additional* event listener is getting registered, while a RepositoryService.getEvents call is already in progress).
"
1,"CVE-2009-0026: Cross site scripting issues in webappSome of the jackrabbit-webapp forms don't properly escape user input when displaying it in the resulting HTML page. This leads to potential cross site scripting issues. For example:

    search.jsp?q=%25%22%3Cscript%3Ealert(1)%3C/script%3E
    swr.jsp?q=%25""<script>alert(1)</script>&swrnum=1

The CVE id for this issue is CVE-2009-0026. This issue was reported by the Red Hat Security Response Team."
1,"CacheBehaviour Observation brokenWhile trying to fix JCR-2293 I discovered that CacheBehaviour Observation is broken:

- HierarchyEventListener.onEvent ignores local event (despite the comment saying otherwise). Not sure which way it should be. However with local events being ignored, JCR-2293 will most probably also occur with CacheBehaviour Observation. 

- NodeEntryImpl.refresh(Event) does not set its child node entries to incomplete when a node/property was added.

- After tentatively fixing above issues, I discovered that NodeEntryImpl.refresh(Event) and my own event listener operate on different NodeEntryImpl and ChildNodeEntryImpl instances. That is, even though I set childNodeEntries.complete to false in NodeEntryImpl.refresh(Event), when my own event listener retrieves that node (entry), it gets a different instance which has childNodeEntries.complete still set to true.
"
1,"jcr-server should respect child node definition of jcr:contentWhen creating a new file, jcr:content defaults to nt:unstructured. This causes file creation to fail when the underlying persistent store (i.e. SPI implementation) does not support nt:unstructured for jcr:content. 

I suggest to check whether the underlying implementation provides its own node type for jcr:content and use that one. If not, default to nt:unstructured."
1,"ArrayIndexOutOfBoundsException in NodeTypeDefDiffIt appears that the code for building diffs in child node definitions loops incorrectly, opening the possibility for an ArrayIndexOutOfBounds exception. The offending portion is in the ""buildChildNodeDefDiffs"" method:

<<
NodeDef[] cnda2 = newDef.getChildNodeDefs();
HashMap defs2 = new HashMap();
for (int i = 0; i < cnda1.length; i++) {
    defs2.put(cnda2[i].getId(), cnda2[i]);
}
>>

It seems like simply changing the length check to be cnda2 (as it is in ""buildPropDefsDiff"") would suffice."
1,"BundleDbPersistenceManager.checkConsistency() only fixes inconsistency if consistencyFix is enabled in configurationThe method has a parameter that explicitly tells whether an inconsistency should be fixed, thus the configuration parameter should be ignored.

Suggested patch:

Index: BundleDbPersistenceManager.java
===================================================================
--- BundleDbPersistenceManager.java	(revision 648657)
+++ BundleDbPersistenceManager.java	(working copy)
@@ -864,7 +864,7 @@
         }
 
         // repair collected broken bundles
-        if (consistencyFix && !modifications.isEmpty()) {
+        if (fix && !modifications.isEmpty()) {
             log.info(name + "": Fixing "" + modifications.size() + "" inconsistent bundle(s)..."");
             Iterator iterator = modifications.iterator();
             while (iterator.hasNext()) {
"
1,"NoSuchItemStateException on removing node (no versioning)I'm using jackrabbit 1.2.1
with no versioning
with a very simple SimpleAccessManager (this try to compute the path of the passed ItemId and verify permissions over that path)

when I remove a node (nt:file or nt:folder),  calling session.save() I obtains the exception reported below.

is it really a bug or am i wrong?
thanks

the following is the code I'm using to build the path
----------------------------------------- CODE START
public String getStringPath(ItemId id) throws ItemNotFoundException, RepositoryException, NoPrefixDeclaredException
	{
		String p = """";
		NamespaceResolver nsResolver = ((HierarchyManagerImpl) hierMgr).getNamespaceResolver();
		Path path = hierMgr.getPath(id);
		PathElement[] pe = path.getElements();
		for (int i = 0; i < pe.length; i++)
		{
			if (pe[i].denotesName())
				p += ""/"" + pe[i].toJCRName(nsResolver);
		}
		return p;
	}
----------------------------------------- CODE END


----------------------------------------- START
javax.jcr.ItemNotFoundException: failed to build path of d688e92f-26ae-4f7c-aba7-aaff1df62c2c: d688e92f-26ae-4f7c-aba7-aaff1df62c2c: d688e92f-26ae-4f7c-aba7-aaff1df62c2c
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getPath(HierarchyManagerImpl.java:362)
	at org.apache.jackrabbit.core.CachingHierarchyManager.getPath(CachingHierarchyManager.java:224)
	at it.unict.faq.jackrabbit.SimpleAccessManager.getStringPath(SimpleAccessManager.java:238)
	at it.unict.faq.jackrabbit.SimpleAccessManager.controllo(SimpleAccessManager.java:215)
	at it.unict.faq.jackrabbit.SimpleAccessManager.isGranted(SimpleAccessManager.java:183)
	at org.apache.jackrabbit.core.ItemImpl.validateTransientItems(ItemImpl.java:645)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1162)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:821)
	at it.unict.faq.driver.manager.impl.DAO.JcrDAO.CancellaNodo(JcrDAO.java:638)
	at it.unict.faq.driver.manager.impl.DocumentServerManager.ds_del(DocumentServerManager.java:58)
	at elearn.portal.action.ds_del_portal.execute(ds_del_portal.java:28)
	at org.apache.struts.action.RequestProcessor.processActionPerform(RequestProcessor.java:421)
	at org.apache.struts.action.RequestProcessor.process(RequestProcessor.java:226)
	at org.apache.struts.action.ActionServlet.process(ActionServlet.java:1158)
	at org.apache.struts.action.ActionServlet.doPost(ActionServlet.java:415)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:709)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:252)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:173)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:264)
	at org.acegisecurity.intercept.web.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:107)
	at org.acegisecurity.intercept.web.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:72)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.ui.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:110)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.providers.anonymous.AnonymousProcessingFilter.doFilter(AnonymousProcessingFilter.java:125)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.ui.rememberme.RememberMeProcessingFilter.doFilter(RememberMeProcessingFilter.java:142)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.wrapper.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:81)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.ui.AbstractProcessingFilter.doFilter(AbstractProcessingFilter.java:217)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.ui.logout.LogoutFilter.doFilter(LogoutFilter.java:108)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.context.HttpSessionContextIntegrationFilter.doFilter(HttpSessionContextIntegrationFilter.java:193)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.util.FilterChainProxy.doFilter(FilterChainProxy.java:148)
	at org.acegisecurity.util.FilterToBeanProxy.doFilter(FilterToBeanProxy.java:98)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:202)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:173)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:213)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:178)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:126)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:105)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:107)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:148)
	at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:869)
	at org.apache.coyote.http11.Http11BaseProtocol$Http11ConnectionHandler.processConnection(Http11BaseProtocol.java:664)
	at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(PoolTcpEndpoint.java:527)
	at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt(LeaderFollowerWorkerThread.java:80)
	at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadPool.java:684)
	at java.lang.Thread.run(Thread.java:595)
Caused by: org.apache.jackrabbit.core.state.NoSuchItemStateException: d688e92f-26ae-4f7c-aba7-aaff1df62c2c
	at org.apache.jackrabbit.core.state.SessionItemStateManager.getTransientItemState(SessionItemStateManager.java:323)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:154)
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getItemState(HierarchyManagerImpl.java:120)
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getPath(HierarchyManagerImpl.java:357)
	... 52 more
org.apache.jackrabbit.core.state.NoSuchItemStateException: d688e92f-26ae-4f7c-aba7-aaff1df62c2c
	at org.apache.jackrabbit.core.state.SessionItemStateManager.getTransientItemState(SessionItemStateManager.java:323)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:154)
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getItemState(HierarchyManagerImpl.java:120)
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getPath(HierarchyManagerImpl.java:357)
	at org.apache.jackrabbit.core.CachingHierarchyManager.getPath(CachingHierarchyManager.java:224)
	at it.unict.faq.jackrabbit.SimpleAccessManager.getStringPath(SimpleAccessManager.java:238)
	at it.unict.faq.jackrabbit.SimpleAccessManager.controllo(SimpleAccessManager.java:215)
	at it.unict.faq.jackrabbit.SimpleAccessManager.isGranted(SimpleAccessManager.java:183)
	at org.apache.jackrabbit.core.ItemImpl.validateTransientItems(ItemImpl.java:645)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1162)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:821)
	at it.unict.faq.driver.manager.impl.DAO.JcrDAO.CancellaNodo(JcrDAO.java:638)
	at it.unict.faq.driver.manager.impl.DocumentServerManager.ds_del(DocumentServerManager.java:58)
	at elearn.portal.action.ds_del_portal.execute(ds_del_portal.java:28)
	at org.apache.struts.action.RequestProcessor.processActionPerform(RequestProcessor.java:421)
	at org.apache.struts.action.RequestProcessor.process(RequestProcessor.java:226)
	at org.apache.struts.action.ActionServlet.process(ActionServlet.java:1158)
	at org.apache.struts.action.ActionServlet.doPost(ActionServlet.java:415)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:709)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:252)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:173)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:264)
	at org.acegisecurity.intercept.web.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:107)
	at org.acegisecurity.intercept.web.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:72)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.ui.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:110)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.providers.anonymous.AnonymousProcessingFilter.doFilter(AnonymousProcessingFilter.java:125)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.ui.rememberme.RememberMeProcessingFilter.doFilter(RememberMeProcessingFilter.java:142)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.wrapper.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:81)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.ui.AbstractProcessingFilter.doFilter(AbstractProcessingFilter.java:217)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.ui.logout.LogoutFilter.doFilter(LogoutFilter.java:108)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.context.HttpSessionContextIntegrationFilter.doFilter(HttpSessionContextIntegrationFilter.java:193)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.util.FilterChainProxy.doFilter(FilterChainProxy.java:148)
	at org.acegisecurity.util.FilterToBeanProxy.doFilter(FilterToBeanProxy.java:98)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:202)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:173)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:213)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:178)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:126)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:105)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:107)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:148)
	at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:869)
	at org.apache.coyote.http11.Http11BaseProtocol$Http11ConnectionHandler.processConnection(Http11BaseProtocol.java:664)
	at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(PoolTcpEndpoint.java:527)
	at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt(LeaderFollowerWorkerThread.java:80)
	at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadPool.java:684)
	at java.lang.Thread.run(Thread.java:595)
----------------------------------------- END"
1,"derelativizing of relative URIs with a scheme is incorrectURI constructor ""public URI(URI base, URI relative) throws URIException"" assumes that if given 'relative' URI has a scheme, it should provide an authority and complete path to the constructed URI. However, a URI can have a scheme but still be relative, requiring the authority and base path of the 'base' URI. 

Demonstration code:

URI base = new URI(""http://www.example.com/some/page"");
URI rel = new URI(""http:boo"");
URI derel = new URI(base,rel);
derel.toString();
(java.lang.String) http:boo

In fact, derel should be ""http://www.example.com/some/boo"". 

RFC2396 is a little confused about this; section 3.1 states """"Relative URI references are distinguished from absolute URI in that they do not begin with a scheme name."" But, in section 5, there are several sentences talking about relative URIs that begin with schemes (and how this prevents using relative URIs that have leading path segments that look like scheme identifiers). 

RFC3896, which supercedes RFC2396, removes the implication a relative URI cannot begin with a scheme, leaving the other text explcitly discussing relative URIs with schemes. 

Both Firefox (1.5) and IE (6.0) treat ""http:boo"" the same as ""boo"" for purposes of derelativization against an HTTP base URI, which would give the final URI ""http://www.example.com/some/boo"" in the example above. 

Even relative URIs like ""http:../../boo"" are explicitly legal. 

"
1,"PUT method blocks against older serversTo reproduce, attempt a PUT request against an appropriate servlet under TC3.2
(yes I know that needs an upgrade - sigh)

RFC 2616 says:
""Because of the presence of older implementations, the protocol allows ambiguous
situations in which a client may send ""Expect: 100- continue"" without receiving
either a 417 (Expectation Failed) status or a 100 (Continue) status. Therefore,
when a client sends this header field to an origin server (possibly via a proxy)
from which it has never seen a 100 (Continue) status, the client SHOULD NOT wait
for an indefinite period before sending the request body.""

This isn't how HttpClient behaves. After sending the headers,
PutMethod.writeRequestBody() returns false. HttpMethodBase then calls
readStatusCode(), which blocks waiting for a read (or I guess you could time out
the whole request). Right now this makes it impossible to use HttpClient to PUT
to older Http 1.1 implementations.

A suggested resolution: since the spec allows for clients to avoid waiting if
they know the 100 response will not arrive, why not simply provide a boolean
flag to allow the 'wait for 100' behaviour in PutMethod.writeResponseBody() to
be turned off, on a per-request basis? This solution puts the burden of knowing
""origin server[s]...from which it has never seen a 100 (Continue) status"" on the
user of HttpClient. Less than perfect as you can only find out that this has
happened by trial and error.

A more correct solution, is to maintain a list of servers that ignore the Expect
header in PutMethod, and override PutMethod.readStatusCode() to time out, send
the body, remember this server is buggy, and read the status code again."
1,"DatabaseFileSystem's logger references the wrong classIn DatabaseFileSystem, the logger is constructed as
private static Logger log = LoggerFactory.getLogger(DbFileSystem.class);

It should be constructed as:
private static Logger log = LoggerFactory.getLogger(DatabaseFileSystem.class);"
1,"HTTPS Post Does Not WorkUsing Java 1.4.1_01 on Windows 2000. An HTTPS Post results in HTTP/100-Continue 
messages. The same code posting to a non HTTPS URL works. The code populates 
the request body using a NameValuePair array."
1,"TimeLimitingCollector's TimeExceededException contains useless relative docidWe found another bug with the RandomIndexWriter: When TimeLimitingCollector breaks collection after timeout, it records the last/next collected docid. It does this without rebasing, so the docid is useless. TestTimeLimitingCollector checks the docid, but correctly rebases it (as only this makes sense). Because the RandomIndexWriter uses different merge settings, the index is now sometimes not optimized and so the test fails (which is correct, as the docid is useless for non-optimized index).

Attached is a patch that fixes this. Please tell me if I should backport to 2.9 and 3.0!"
1,"javax.jcr.RepositoryException when a JOIN SQL2 query is send via Davex and has resultssee the following thread for details:
http://www.mail-archive.com/users@jackrabbit.apache.org/msg17975.html

assuming a data structure as follows:
/foo [nt:unstructured]
/foo/bar [nt:unstructured]
/foo/bar@lala = huii (lala is string property of bar)
/ding [nt:unstructured]
/ding@dong = ##barUUID### (dong is a property of type ""Reference"")

then the following code will throw an exception:

DavexClient Client = new DavexClient(url);
Repository repo = Client.getRepository();
Credentials sc = new SimpleCredentials(""admin"",""admin"".toCharArray());
Session s = repo.login(sc,workspace);

QueryManager qm = s.getWorkspace().getQueryManager();

String sql = ""SELECT data.* FROM [nt:unstructured] AS data WHERE data.lala= 'huii'"";
sql = ""SELECT * FROM [nt:unstructured] AS data INNER JOIN [nt:unstructured] AS referring ON referring.[dong] = data.[jcr:uuid] WHERE data.lala = 'huii'"";
sql = ""SELECT * FROM [nt:unstructured] AS data INNER JOIN [nt:unstructured] AS referring ON ISDESCENDANTNODE(data, referring) WHERE data.lala = 'huii'"";
Query query = qm.createQuery(sql, Query.JCR_SQL2);
QueryResult qr = query.execute();

The first query works just fine and I can iterate over the result. Neither the second nor the third query works.
In both cases I end up with a javax.jcr.RepositoryException. Note the exception only happens if the query returns results. Aka a join will work just fine if it matches no rows."
1,"need to ensure that sims that use collection-level stats (e.g. sumTotalTermFreq) handle non-existent fieldBecause of things like queryNorm, unfortunately similarities have to handle the case where they are asked to computeStats() for a term, where the field does not exist at all.
(Note they will never have to actually score anything, but unless we break how queryNorm works for TFIDF, we have to deal with this case).

I noticed this while doing some benchmarking, so i created a test to test some cases like this across all the sims."
1,MSSql and MySQL bunlde PM schemas missing definition for name indexthe mssql and mysql ddl files of the respective bundle persistence managers are missing the definitions for the name index.
1,"IW.optimize() can do too many merges at the very endThis was fixed on trunk in LUCENE-1044 but I'd like to separately
backport it to 2.3.

With ConcurrentMergeScheduler there is a bug, only when CFS is on,
whereby after the final merge of an optimize has finished and while
it's building its CFS, the merge policy may incorrectly ask for
another merge to collapse that segment into a compound file.  The net
effect is optimize can spend many extra iterations unecessarily
merging a single segment to collapse it to compound file.

I believe the case is rare (hard to hit), and maybe only if you have
multiple threads calling optimize at once (the TestThreadedOptimize
test can hit it), but it's a low-risk fix so I plan to commit to 2.3
shortly.

"
1,"[patch] fix uppercase/lowercase handling for not equal tocode is missing breaks in switch statements, which causes both uppercase and lowercase terms to the not equal to lucene search. patch fixes."
1,FilterIndexReader should overwrite isOptimized()A call of FilterIndexReader.isOptimized() results in a NPE because FilterIndexReader does not overwrite isOptimized().
1,"o.a.l.analysis.de.GermanStemmer crashes on some inputsSee the tests from LUCENE-2560. 

GermanAnalyzer no longer uses this stemmer by default, but we should fix it."
1,"Test failures with spi2jcr in AddEventListenerTestTwo tests fail:

- AddEventListenerTest.testUUID
- AddEventListenerTest.testNodeType"
1,"trunk tests hang/deadlock TestIndexWriterWithThreadstrunk tests have been hanging often lately in hudson, this time i was careful to kill and get a good stacktrace:"
1,"ClassCastException org.apache.xerces.parsers.XIncludeAwareParserConfiguration cannot be cast to org.apache.xerces.xni.parser.XMLParserConfiguration when deploying in JBoss 5.1I tried to follow the steps given on http://wiki.apache.org/jackrabbit/JackrabbitOnJBoss
To get over an exception I had to use jcr-2.0.jar (instead of jcr-1.0.jar)
The following exception happens when the jboss server is started.
=======================================================================

2009-10-02 11:49:05,630 ERROR [org.jboss.web.tomcat.service.deployers.JBossContextConfig] (main) XML error parsing: context.xml
org.jboss.xb.binding.JBossXBRuntimeException: Failed to create a new SAX parser
	at org.jboss.xb.binding.UnmarshallerFactory$UnmarshallerFactoryImpl.newUnmarshaller(UnmarshallerFactory.java:100)
	at org.jboss.web.tomcat.service.deployers.JBossContextConfig.processContextConfig(JBossContextConfig.java:549)
	at org.jboss.web.tomcat.service.deployers.JBossContextConfig.init(JBossContextConfig.java:536)
	at org.apache.catalina.startup.ContextConfig.lifecycleEvent(ContextConfig.java:279)
	at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:117)
	at org.apache.catalina.core.StandardContext.init(StandardContext.java:5436)
	at org.apache.catalina.core.StandardContext.start(StandardContext.java:4148)
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeployInternal(TomcatDeployment.java:310)
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeploy(TomcatDeployment.java:142)
	at org.jboss.web.deployers.AbstractWarDeployment.start(AbstractWarDeployment.java:461)
	at org.jboss.web.deployers.WebModule.startModule(WebModule.java:118)
	at org.jboss.web.deployers.WebModule.start(WebModule.java:97)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java:157)
	at org.jboss.mx.server.Invocation.dispatch(Invocation.java:96)
	at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)
	at org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:264)
	at org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:668)
	at org.jboss.system.microcontainer.ServiceProxy.invoke(ServiceProxy.java:206)
	at $Proxy38.start(Unknown Source)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:42)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:37)
	at org.jboss.dependency.plugins.action.SimpleControllerContextAction.simpleInstallAction(SimpleControllerContextAction.java:62)
	at org.jboss.dependency.plugins.action.AccessControllerContextAction.install(AccessControllerContextAction.java:71)
	at org.jboss.dependency.plugins.AbstractControllerContextActions.install(AbstractControllerContextActions.java:51)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.system.microcontainer.ServiceControllerContext.install(ServiceControllerContext.java:286)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.ServiceController.doChange(ServiceController.java:688)
	at org.jboss.system.ServiceController.start(ServiceController.java:460)
	at org.jboss.system.deployers.ServiceDeployer.start(ServiceDeployer.java:163)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:99)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:46)
	at org.jboss.deployers.spi.deployer.helpers.AbstractSimpleRealDeployer.internalDeploy(AbstractSimpleRealDeployer.java:62)
	at org.jboss.deployers.spi.deployer.helpers.AbstractRealDeployer.deploy(AbstractRealDeployer.java:50)
	at org.jboss.deployers.plugins.deployers.DeployerWrapper.deploy(DeployerWrapper.java:171)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doDeploy(DeployersImpl.java:1439)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1157)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1178)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.install(DeployersImpl.java:1098)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.process(DeployersImpl.java:781)
	at org.jboss.deployers.plugins.main.MainDeployerImpl.process(MainDeployerImpl.java:702)
	at org.jboss.system.server.profileservice.repository.MainDeployerAdapter.process(MainDeployerAdapter.java:117)
	at org.jboss.system.server.profileservice.repository.ProfileDeployAction.install(ProfileDeployAction.java:70)
	at org.jboss.system.server.profileservice.repository.AbstractProfileAction.install(AbstractProfileAction.java:53)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.install(AbstractProfileService.java:361)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.activateProfile(AbstractProfileService.java:306)
	at org.jboss.system.server.profileservice.ProfileServiceBootstrap.start(ProfileServiceBootstrap.java:271)
	at org.jboss.bootstrap.AbstractServerImpl.start(AbstractServerImpl.java:461)
	at org.jboss.Main.boot(Main.java:221)
	at org.jboss.Main$1.run(Main.java:556)
	at java.lang.Thread.run(Thread.java:619)
Caused by: org.jboss.xb.binding.JBossXBException: Failed to create a new SAX parser
	at org.jboss.xb.binding.parser.sax.SaxJBossXBParser.<init>(SaxJBossXBParser.java:97)
	at org.jboss.xb.binding.UnmarshallerImpl.<init>(UnmarshallerImpl.java:56)
	at org.jboss.xb.binding.UnmarshallerFactory$UnmarshallerFactoryImpl.newUnmarshaller(UnmarshallerFactory.java:96)
	... 73 more
Caused by: java.lang.ClassCastException: org.apache.xerces.parsers.XIncludeAwareParserConfiguration cannot be cast to org.apache.xerces.xni.parser.XMLParserConfiguration
	at org.apache.xerces.parsers.SAXParser.<init>(Unknown Source)
	at org.apache.xerces.parsers.SAXParser.<init>(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser.<init>(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserImpl.<init>(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserFactoryImpl.newSAXParser(Unknown Source)
	at org.jboss.xb.binding.parser.sax.SaxJBossXBParser.<init>(SaxJBossXBParser.java:92)
	... 75 more
2009-10-02 11:49:05,645 ERROR [org.jboss.web.tomcat.service.deployers.JBossContextConfig] (main) XML error parsing: jboss.web/localhost/context.xml.default
org.jboss.xb.binding.JBossXBRuntimeException: Failed to create a new SAX parser
	at org.jboss.xb.binding.UnmarshallerFactory$UnmarshallerFactoryImpl.newUnmarshaller(UnmarshallerFactory.java:100)
	at org.jboss.web.tomcat.service.deployers.JBossContextConfig.processContextConfig(JBossContextConfig.java:549)
	at org.jboss.web.tomcat.service.deployers.JBossContextConfig.init(JBossContextConfig.java:537)
	at org.apache.catalina.startup.ContextConfig.lifecycleEvent(ContextConfig.java:279)
	at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:117)
	at org.apache.catalina.core.StandardContext.init(StandardContext.java:5436)
	at org.apache.catalina.core.StandardContext.start(StandardContext.java:4148)
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeployInternal(TomcatDeployment.java:310)
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeploy(TomcatDeployment.java:142)
	at org.jboss.web.deployers.AbstractWarDeployment.start(AbstractWarDeployment.java:461)
	at org.jboss.web.deployers.WebModule.startModule(WebModule.java:118)
	at org.jboss.web.deployers.WebModule.start(WebModule.java:97)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java:157)
	at org.jboss.mx.server.Invocation.dispatch(Invocation.java:96)
	at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)
	at org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:264)
	at org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:668)
	at org.jboss.system.microcontainer.ServiceProxy.invoke(ServiceProxy.java:206)
	at $Proxy38.start(Unknown Source)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:42)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:37)
	at org.jboss.dependency.plugins.action.SimpleControllerContextAction.simpleInstallAction(SimpleControllerContextAction.java:62)
	at org.jboss.dependency.plugins.action.AccessControllerContextAction.install(AccessControllerContextAction.java:71)
	at org.jboss.dependency.plugins.AbstractControllerContextActions.install(AbstractControllerContextActions.java:51)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.system.microcontainer.ServiceControllerContext.install(ServiceControllerContext.java:286)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.ServiceController.doChange(ServiceController.java:688)
	at org.jboss.system.ServiceController.start(ServiceController.java:460)
	at org.jboss.system.deployers.ServiceDeployer.start(ServiceDeployer.java:163)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:99)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:46)
	at org.jboss.deployers.spi.deployer.helpers.AbstractSimpleRealDeployer.internalDeploy(AbstractSimpleRealDeployer.java:62)
	at org.jboss.deployers.spi.deployer.helpers.AbstractRealDeployer.deploy(AbstractRealDeployer.java:50)
	at org.jboss.deployers.plugins.deployers.DeployerWrapper.deploy(DeployerWrapper.java:171)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doDeploy(DeployersImpl.java:1439)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1157)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1178)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.install(DeployersImpl.java:1098)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.process(DeployersImpl.java:781)
	at org.jboss.deployers.plugins.main.MainDeployerImpl.process(MainDeployerImpl.java:702)
	at org.jboss.system.server.profileservice.repository.MainDeployerAdapter.process(MainDeployerAdapter.java:117)
	at org.jboss.system.server.profileservice.repository.ProfileDeployAction.install(ProfileDeployAction.java:70)
	at org.jboss.system.server.profileservice.repository.AbstractProfileAction.install(AbstractProfileAction.java:53)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.install(AbstractProfileService.java:361)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.activateProfile(AbstractProfileService.java:306)
	at org.jboss.system.server.profileservice.ProfileServiceBootstrap.start(ProfileServiceBootstrap.java:271)
	at org.jboss.bootstrap.AbstractServerImpl.start(AbstractServerImpl.java:461)
	at org.jboss.Main.boot(Main.java:221)
	at org.jboss.Main$1.run(Main.java:556)
	at java.lang.Thread.run(Thread.java:619)
Caused by: org.jboss.xb.binding.JBossXBException: Failed to create a new SAX parser
	at org.jboss.xb.binding.parser.sax.SaxJBossXBParser.<init>(SaxJBossXBParser.java:97)
	at org.jboss.xb.binding.UnmarshallerImpl.<init>(UnmarshallerImpl.java:56)
	at org.jboss.xb.binding.UnmarshallerFactory$UnmarshallerFactoryImpl.newUnmarshaller(UnmarshallerFactory.java:96)
	... 73 more
Caused by: java.lang.ClassCastException: org.apache.xerces.parsers.XIncludeAwareParserConfiguration cannot be cast to org.apache.xerces.xni.parser.XMLParserConfiguration
	at org.apache.xerces.parsers.SAXParser.<init>(Unknown Source)
	at org.apache.xerces.parsers.SAXParser.<init>(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser.<init>(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserImpl.<init>(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserFactoryImpl.newSAXParser(Unknown Source)
	at org.jboss.xb.binding.parser.sax.SaxJBossXBParser.<init>(SaxJBossXBParser.java:92)
	... 75 more
2009-10-02 11:49:05,645 ERROR [org.jboss.web.tomcat.service.deployers.JBossContextConfig] (main) XML error parsing: WEB-INF/context.xml
org.jboss.xb.binding.JBossXBRuntimeException: Failed to create a new SAX parser
	at org.jboss.xb.binding.UnmarshallerFactory$UnmarshallerFactoryImpl.newUnmarshaller(UnmarshallerFactory.java:100)
	at org.jboss.web.tomcat.service.deployers.JBossContextConfig.processContextConfig(JBossContextConfig.java:549)
	at org.jboss.web.tomcat.service.deployers.JBossContextConfig.init(JBossContextConfig.java:540)
	at org.apache.catalina.startup.ContextConfig.lifecycleEvent(ContextConfig.java:279)
	at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:117)
	at org.apache.catalina.core.StandardContext.init(StandardContext.java:5436)
	at org.apache.catalina.core.StandardContext.start(StandardContext.java:4148)
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeployInternal(TomcatDeployment.java:310)
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeploy(TomcatDeployment.java:142)
	at org.jboss.web.deployers.AbstractWarDeployment.start(AbstractWarDeployment.java:461)
	at org.jboss.web.deployers.WebModule.startModule(WebModule.java:118)
	at org.jboss.web.deployers.WebModule.start(WebModule.java:97)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java:157)
	at org.jboss.mx.server.Invocation.dispatch(Invocation.java:96)
	at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)
	at org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:264)
	at org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:668)
	at org.jboss.system.microcontainer.ServiceProxy.invoke(ServiceProxy.java:206)
	at $Proxy38.start(Unknown Source)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:42)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:37)
	at org.jboss.dependency.plugins.action.SimpleControllerContextAction.simpleInstallAction(SimpleControllerContextAction.java:62)
	at org.jboss.dependency.plugins.action.AccessControllerContextAction.install(AccessControllerContextAction.java:71)
	at org.jboss.dependency.plugins.AbstractControllerContextActions.install(AbstractControllerContextActions.java:51)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.system.microcontainer.ServiceControllerContext.install(ServiceControllerContext.java:286)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.ServiceController.doChange(ServiceController.java:688)
	at org.jboss.system.ServiceController.start(ServiceController.java:460)
	at org.jboss.system.deployers.ServiceDeployer.start(ServiceDeployer.java:163)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:99)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:46)
	at org.jboss.deployers.spi.deployer.helpers.AbstractSimpleRealDeployer.internalDeploy(AbstractSimpleRealDeployer.java:62)
	at org.jboss.deployers.spi.deployer.helpers.AbstractRealDeployer.deploy(AbstractRealDeployer.java:50)
	at org.jboss.deployers.plugins.deployers.DeployerWrapper.deploy(DeployerWrapper.java:171)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doDeploy(DeployersImpl.java:1439)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1157)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1178)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.install(DeployersImpl.java:1098)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.process(DeployersImpl.java:781)
	at org.jboss.deployers.plugins.main.MainDeployerImpl.process(MainDeployerImpl.java:702)
	at org.jboss.system.server.profileservice.repository.MainDeployerAdapter.process(MainDeployerAdapter.java:117)
	at org.jboss.system.server.profileservice.repository.ProfileDeployAction.install(ProfileDeployAction.java:70)
	at org.jboss.system.server.profileservice.repository.AbstractProfileAction.install(AbstractProfileAction.java:53)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.install(AbstractProfileService.java:361)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.activateProfile(AbstractProfileService.java:306)
	at org.jboss.system.server.profileservice.ProfileServiceBootstrap.start(ProfileServiceBootstrap.java:271)
	at org.jboss.bootstrap.AbstractServerImpl.start(AbstractServerImpl.java:461)
	at org.jboss.Main.boot(Main.java:221)
	at org.jboss.Main$1.run(Main.java:556)
	at java.lang.Thread.run(Thread.java:619)
Caused by: org.jboss.xb.binding.JBossXBException: Failed to create a new SAX parser
	at org.jboss.xb.binding.parser.sax.SaxJBossXBParser.<init>(SaxJBossXBParser.java:97)
	at org.jboss.xb.binding.UnmarshallerImpl.<init>(UnmarshallerImpl.java:56)
	at org.jboss.xb.binding.UnmarshallerFactory$UnmarshallerFactoryImpl.newUnmarshaller(UnmarshallerFactory.java:96)
	... 73 more
Caused by: java.lang.ClassCastException: org.apache.xerces.parsers.XIncludeAwareParserConfiguration cannot be cast to org.apache.xerces.xni.parser.XMLParserConfiguration
	at org.apache.xerces.parsers.SAXParser.<init>(Unknown Source)
	at org.apache.xerces.parsers.SAXParser.<init>(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser.<init>(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserImpl.<init>(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserFactoryImpl.newSAXParser(Unknown Source)
	at org.jboss.xb.binding.parser.sax.SaxJBossXBParser.<init>(SaxJBossXBParser.java:92)
	... 75 more
2009-10-02 11:49:05,786 ERROR [org.apache.catalina.startup.ContextConfig] (main) Marking this application unavailable due to previous error(s)
2009-10-02 11:49:05,786 ERROR [org.apache.catalina.core.StandardContext] (main) Context [/jackrabbit-webapp-2.0-alpha9] startup failed due to previous errors
2009-10-02 11:49:06,473 ERROR [org.jboss.kernel.plugins.dependency.AbstractKernelController] (main) Error installing to Start: name=jboss.web.deployment:war=/jackrabbit-webapp-2.0-alpha9 state=Create mode=Manual requiredState=Installed
org.jboss.deployers.spi.DeploymentException: URL file:/C:/applications/jboss-5.1.0.GA/server/default/tmp/ahn1p-6tv4p6-g0bacatm-1-g0bageil-9t/jackrabbit-webapp-2.0-alpha9.war/ deployment failed
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeployInternal(TomcatDeployment.java:331)
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeploy(TomcatDeployment.java:142)
	at org.jboss.web.deployers.AbstractWarDeployment.start(AbstractWarDeployment.java:461)
	at org.jboss.web.deployers.WebModule.startModule(WebModule.java:118)
	at org.jboss.web.deployers.WebModule.start(WebModule.java:97)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java:157)
	at org.jboss.mx.server.Invocation.dispatch(Invocation.java:96)
	at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)
	at org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:264)
	at org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:668)
	at org.jboss.system.microcontainer.ServiceProxy.invoke(ServiceProxy.java:206)
	at $Proxy38.start(Unknown Source)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:42)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:37)
	at org.jboss.dependency.plugins.action.SimpleControllerContextAction.simpleInstallAction(SimpleControllerContextAction.java:62)
	at org.jboss.dependency.plugins.action.AccessControllerContextAction.install(AccessControllerContextAction.java:71)
	at org.jboss.dependency.plugins.AbstractControllerContextActions.install(AbstractControllerContextActions.java:51)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.system.microcontainer.ServiceControllerContext.install(ServiceControllerContext.java:286)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.ServiceController.doChange(ServiceController.java:688)
	at org.jboss.system.ServiceController.start(ServiceController.java:460)
	at org.jboss.system.deployers.ServiceDeployer.start(ServiceDeployer.java:163)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:99)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:46)
	at org.jboss.deployers.spi.deployer.helpers.AbstractSimpleRealDeployer.internalDeploy(AbstractSimpleRealDeployer.java:62)
	at org.jboss.deployers.spi.deployer.helpers.AbstractRealDeployer.deploy(AbstractRealDeployer.java:50)
	at org.jboss.deployers.plugins.deployers.DeployerWrapper.deploy(DeployerWrapper.java:171)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doDeploy(DeployersImpl.java:1439)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1157)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1178)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.install(DeployersImpl.java:1098)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.process(DeployersImpl.java:781)
	at org.jboss.deployers.plugins.main.MainDeployerImpl.process(MainDeployerImpl.java:702)
	at org.jboss.system.server.profileservice.repository.MainDeployerAdapter.process(MainDeployerAdapter.java:117)
	at org.jboss.system.server.profileservice.repository.ProfileDeployAction.install(ProfileDeployAction.java:70)
	at org.jboss.system.server.profileservice.repository.AbstractProfileAction.install(AbstractProfileAction.java:53)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.install(AbstractProfileService.java:361)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.activateProfile(AbstractProfileService.java:306)
	at org.jboss.system.server.profileservice.ProfileServiceBootstrap.start(ProfileServiceBootstrap.java:271)
	at org.jboss.bootstrap.AbstractServerImpl.start(AbstractServerImpl.java:461)
	at org.jboss.Main.boot(Main.java:221)
	at org.jboss.Main$1.run(Main.java:556)
	at java.lang.Thread.run(Thread.java:619)
2009-10-02 11:49:06,598 ERROR [org.jboss.kernel.plugins.dependency.AbstractKernelController] (main) Error installing to Real: name=vfszip:/C:/applications/jboss-5.1.0.GA/server/default/deploy/jackrabbit-webapp-2.0-alpha9.war/ state=PreReal mode=Manual requiredState=Real
org.jboss.deployers.spi.DeploymentException: URL file:/C:/applications/jboss-5.1.0.GA/server/default/tmp/ahn1p-6tv4p6-g0bacatm-1-g0bageil-9t/jackrabbit-webapp-2.0-alpha9.war/ deployment failed
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeployInternal(TomcatDeployment.java:331)
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeploy(TomcatDeployment.java:142)
	at org.jboss.web.deployers.AbstractWarDeployment.start(AbstractWarDeployment.java:461)
	at org.jboss.web.deployers.WebModule.startModule(WebModule.java:118)
	at org.jboss.web.deployers.WebModule.start(WebModule.java:97)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java:157)
	at org.jboss.mx.server.Invocation.dispatch(Invocation.java:96)
	at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)
	at org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:264)
	at org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:668)
	at org.jboss.system.microcontainer.ServiceProxy.invoke(ServiceProxy.java:206)
	at $Proxy38.start(Unknown Source)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:42)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:37)
	at org.jboss.dependency.plugins.action.SimpleControllerContextAction.simpleInstallAction(SimpleControllerContextAction.java:62)
	at org.jboss.dependency.plugins.action.AccessControllerContextAction.install(AccessControllerContextAction.java:71)
	at org.jboss.dependency.plugins.AbstractControllerContextActions.install(AbstractControllerContextActions.java:51)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.system.microcontainer.ServiceControllerContext.install(ServiceControllerContext.java:286)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.ServiceController.doChange(ServiceController.java:688)
	at org.jboss.system.ServiceController.start(ServiceController.java:460)
	at org.jboss.system.deployers.ServiceDeployer.start(ServiceDeployer.java:163)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:99)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:46)
	at org.jboss.deployers.spi.deployer.helpers.AbstractSimpleRealDeployer.internalDeploy(AbstractSimpleRealDeployer.java:62)
	at org.jboss.deployers.spi.deployer.helpers.AbstractRealDeployer.deploy(AbstractRealDeployer.java:50)
	at org.jboss.deployers.plugins.deployers.DeployerWrapper.deploy(DeployerWrapper.java:171)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doDeploy(DeployersImpl.java:1439)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1157)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1178)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.install(DeployersImpl.java:1098)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.process(DeployersImpl.java:781)
	at org.jboss.deployers.plugins.main.MainDeployerImpl.process(MainDeployerImpl.java:702)
	at org.jboss.system.server.profileservice.repository.MainDeployerAdapter.process(MainDeployerAdapter.java:117)
	at org.jboss.system.server.profileservice.repository.ProfileDeployAction.install(ProfileDeployAction.java:70)
	at org.jboss.system.server.profileservice.repository.AbstractProfileAction.install(AbstractProfileAction.java:53)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.install(AbstractProfileService.java:361)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.activateProfile(AbstractProfileService.java:306)
	at org.jboss.system.server.profileservice.ProfileServiceBootstrap.start(ProfileServiceBootstrap.java:271)
	at org.jboss.bootstrap.AbstractServerImpl.start(AbstractServerImpl.java:461)
	at org.jboss.Main.boot(Main.java:221)
	at org.jboss.Main$1.run(Main.java:556)
	at java.lang.Thread.run(Thread.java:619)
"
1,"When using QueryImpl.setLimit() and QueryImpl.setOffset(), then NodeIterator.getSize() reports wrong sizeWhen using QueryImpl.setLimit() and QueryImpl.setOffset(), then NodeIterator.getSize() reports wrong size. Returned size seems to be allways the same as the limit."
1,"No equals operation for Credentials implementationsI tripped across a scenario where I wanted to compare credentials, so I could
know to discard connection state (and thus any associated cookies).

Patch to follow shortly."
1,"TopFieldCollector throws AIOOBE if numHits is 0See solr-user thread ""ArrayIndexOutOfBoundsException for query with rows=0 and sort param"".

I think we should just create a null collector (only tallies up totalHits) if numHits is 0?"
1,"ExportSysViewTest fails with: System property org.xml.sax.driver not specifiedThe ExportSysViewTest class uses the XMLReaderFactory.createXMLReader() method that depends on the system property ""org.xml.sax.driver"" being specified. Apparently using a TransformerFactory works around this issue in some way, as the problem only appeared once we changed the XML export feature to use a custom serializer class instead of a JAXP Transformer for serialization (see JCR-1952).

The current workaround is to explicitly force a Transformer to be loaded, but we really should fix the cause of this issue for example by replacing the XMLReader instance with a SAXParser."
1,"when opening the merged SegmentReader, IW attempts to open store files that were deletedThe issue happens when a merge runs that does not merge the doc stores, those doc stores are still being written to, IW is using CFS, and while the merge is running the doc stores get closed and turned into a cfx file.

When we then try to open the reader (for warming), which as of LUCENE-2311 will now [correctly] open the doc stores, we hit FNFE because the SegmentInfo for the merge does not realize that the doc stores were turned into  a cfx.

This issue does affect trunk; if you crank up the #docs in the test, it happens consistently (I will tie this to _TestUtil.getRandomMultiplier!)."
1,"Tika regressions in 0.8There are a few notable problems in Tika 0.8, namely TIKA-548 and TIKA-556, that may adversely affect users of Jackrabbit 2.2.

Since we don't have a Tika 0.9 release available yet, I'll add workarounds for these issues in Jackrabbit."
1,"sysview import does not resolve referenceswhen importing a sysview with references, those are not resolved against the new uuids of the mix:referenceable nodes"
1,"SpellChecker.clearIndex calls unlock inappropriatelyAs noted in LUCENE-1050, fixing a bug in SimpleLockFactory related to not reporting success/filure of lock file deletion has surfaced bad behavior in SpellChecker.clearIndex...

Grant...
{quote}
It seems the SpellChecker is telling the IndexReader to delete the lockFile, but the lockFile doesn't exist.
  ...
I don't know much about the locking mechanism, but it seems like this should check to see if the lockFile exists before trying to delete it.
{quote}

Hoss...
{quote}
Grant: my take on this is that SpellChecker.clearIndex is in the wrong. it shouldn't be calling unlock unless it has reason to think there is a ""stale lock"" that needs to be closed - ie: this is a bug in SpellChecker that you have only discovered because this bug LUCENE-1050 was fixed.

I would suggest a new issue for tracking, and a patch in which SpellChecker.clearIndex doesn't call unlock unless isLocked returns true. Even then, it might make sense to catch and ignore LockReleaseFailedException and let whatever resulting exception may originate from ""new IndexWriter"" be returned.
{quote}

marking for 2.3 since it seems like a fairly trivial fix, and if we don't deal with it now it will be a bug introduced in 2.3.

"
1,"Problems with custom nodes in journalI have an application that uses custom node types and I am having problems in a clustered configuration.

Issue 1: the following definition in a nodetype is incorrectly read from the journal:
  + * (nt:hierarchyNode) version

The * is stored in the journal as _x002a_ since it should be a QName and it gets escaped.
When read, the code ...core.nodetype.compact.CompactNodeTypeDefReader.doChildNodeDefinition does the following test:

        if (currentTokenEquals('*')) {
            ndi.setName(ItemDef.ANY_NAME); 
        } else {
            ndi.setName(toQName(currentToken));
        }

Since currentToken is _x002a_ and not * toQName(currentToken) is called but it fails.
I changed the test to:
        if (currentTokenEquals('*') || currentTokenEquals(""_x002a_""))
            ....
and that fixes the problem.

Issue 2: when storing a nodeType in the journal the superclass nt:base is not store, but when reading I get an error saying the node should be a subclass of nt:base.

The code in...core.nodetype.compact.CompactNodeTypeDefWriter.writeSupertypes skips nt:base when writing the node.

When reading the nodetype definition from the journal the following exception is thrown:

Unable to deliver node type operation: [{http://namespace/app/repository/1.0}resource] all primary node types except nt:base itself must be (directly or indirectly) derived from nt:base

probably because nt:base is not re-added to the nodetype definition

 "
1,"HttpClient does not compile 'out of the box' in IBM's VisualAge IDEThis was observed with IBM VisualAge 3.5, which runs JDK1.2.2:

Importing the HTTPClient source code into the IDE brings up a
compilation error in 

org.apache.commons.httpclient.HttpMethodBase.

The initialization of ""private ResponseConsumedWatcher m_responseWatcher""
using an anyonymous inner class seems to cause some trouble. Implicated code:

private ResponseConsumedWatcher m_responseWatcher = new ResponseConsumedWatcher
() {
	public void responseConsumed() {
		responseBodyConsumed();
	}
};

The error message is: ""Field initialization: The constructor invoked to create
org.apache.commons.httpclient.HttpMethodBase$1 with arguments () is not defined""

...but only in the context of HttpMethodBase(String uri) constructor, i.e.
the HttpMethodBase() constructor *can* be compiled, HttpMethodBase(String uri)
*cannot* be compiled with error ""Cannot create constructor due to incorrect
field initialization"".

I interpret this to mean that the compiler is looking for a parameterless
constructor for the anonymous class in the context of 
HttpMethodBase(String uri). The message did not really make sense to me. 
Checked the syntax, checked in the Language Definition whether setting up an
anonymous class like that is permitted; found nothing obviously wrong.

Fix:

The code above is equivalent to constructing the instance at the beginning
of each constructor of the enclosing class. A copy and paste of the
construction code into each of the two constructors fixes things...until
the next update."
1,"Text.unescape(""%"") throws a StringIndexOutOfBoundsExceptionYou get the following exception:

java.lang.StringIndexOutOfBoundsException: String index out of range: 3
	at java.lang.String.substring(String.java:1935)
	at org.apache.jackrabbit.util.Text.unescape(Text.java:407)
	at org.apache.jackrabbit.util.Text.unescape(Text.java:438)

It would be better if it failed with IllegalArgumentException."
1,"it is not possible to register an event listener which listens to mixin nodetypesit would be a nice enhancement if one could as well define mixin nodetypes to be listened:
...
om.addEventListener(this,
                        Event.PROPERTY_ADDED | Event.PROPERTY_CHANGED | Event.PROPERTY_REMOVED,
                        ""/"",
                        true,
                        null,
                        new String[]{""mix:Custom""},
                        false);
..."
1,CompactNodeTypeDefReader does not recognise MIXIN ORDERABLE sequencethe code in 'doOptions' misses to set the setOrderableChildNodes flag if the order of the tokens is MIXIN ORDERABLE.
1,"SpellChecker does not work properly on calling indexDictionary after clearIndexWe have to call clearIndex and indexDictionary to rebuild dictionary from fresh. The call to SpellChecker clearIndex() function does not reset the searcher. Hence, when we call indexDictionary after that, many entries that are already in the stale searcher will not be indexed.

Also, I see that IndexReader reader is used for the sole purpose of obtaining the docFreq of a given term in exist() function. This functionality can also be obtained by using just the searcher by calling searcher.docFreq. Thus, can we get away completely with reader and code associated with it like
      if (IndexReader.isLocked(spellIndex)){
	IndexReader.unlock(spellIndex);
      }
and the reader related code in finalize?

"
1,"intermittent failure in TestIndexWriter. testRandomIWReaderRarely, this test (which was added with LUCENE-1516) fails in MockRAMDirectory.close because some files were not closed, eg:
{code}
   [junit] NOTE: random seed of testcase 'testRandomIWReader' was: -5001333286299627079
   [junit] ------------- ---------------- ---------------
   [junit] Testcase: testRandomIWReader(org.apache.lucene.index.TestStressIndexing2):        Caused an ERROR
   [junit] MockRAMDirectory: cannot close: there are still open files: {_cq.tvx=3, _cq.fdx=3, _cq.tvf=3, _cq.tvd=3, _cq.fdt=3}
   [junit] java.lang.RuntimeException: MockRAMDirectory: cannot close: there are still open files: {_cq.tvx=3, _cq.fdx=3, _cq.tvf=3, _cq.tvd=3, _cq.fdt=3}
   [junit]     at org.apache.lucene.store.MockRAMDirectory.close(MockRAMDirectory.java:292)
   [junit]     at org.apache.lucene.index.TestStressIndexing2.testRandomIWReader(TestStressIndexing2.java:66)
   [junit]     at org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:88)
{code}"
1,"NullPointerException when iterating over propertiesRunning ConcurrentReadWriteTest (NUM_NODES=5, NUM_THREADS=3, RUN_NUM_SECONDS=120) resulted in a NullPointerException:

Exception in thread ""Thread-11"" java.lang.NullPointerException
	at org.apache.commons.collections.map.AbstractReferenceMap$ReferenceEntry.getValue(AbstractReferenceMap.java:596)
	at org.apache.commons.collections.map.AbstractReferenceMap.containsKey(AbstractReferenceMap.java:204)
	at org.apache.jackrabbit.core.state.ItemStateMap.contains(ItemStateMap.java:66)
	at org.apache.jackrabbit.core.state.ItemStateReferenceCache.isCached(ItemStateReferenceCache.java:91)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.hasItemState(LocalItemStateManager.java:173)
	at org.apache.jackrabbit.core.state.XAItemStateManager.hasItemState(XAItemStateManager.java:252)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:174)
	at org.apache.jackrabbit.core.ItemManager.createItemInstance(ItemManager.java:495)
	at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:326)
	at org.apache.jackrabbit.core.LazyItemIterator.prefetchNext(LazyItemIterator.java:90)
	at org.apache.jackrabbit.core.LazyItemIterator.next(LazyItemIterator.java:203)
	at org.apache.jackrabbit.core.LazyItemIterator.nextProperty(LazyItemIterator.java:118)
	at org.apache.jackrabbit.core.ConcurrentReadWriteTest$1$1.execute(ConcurrentReadWriteTest.java:64)
	at org.apache.jackrabbit.core.AbstractConcurrencyTest$Executor.run(AbstractConcurrencyTest.java:110)
	at java.lang.Thread.run(Thread.java:619)

The cache is not synchronized and is accessed at the same time by the current thread and another thread that notified ItemStates about changes."
1,"IndexMerger blocks client threads when obsolete index segments are deletedWhen index segments have been merged, the obsolete indexes are replaced with the new one an deleted afterwards. Currently deleting the obsolete segments is inside a MultiIndex synchronized block, which may block other threads from updating the index concurrently."
1,"bad normalization in sorted search returning TopDocsFieldSortedHitQueue.maxscore is maintained in the lessThan method (which never gets called if a single document is added to the queue).

I've checked in a test to TestSort.testTopDocsScores() with the final assertion commented out."
1,"Formatting error in ReportTask in contrib/benchmarkI am building a new Task, AnalyzerTask, that lets you change the Analyzer in the loop, thus allowing for the comparison of the same Analyzers over the set of documents.

My algorithm declaration looks like:
NewAnalyzer(WhitespaceAnalyzer, SimpleAnalyzer, StopAnalyzer, standard.StandardAnalyzer)

And it could be longer.

The exception is:
Error: cannot execute the algorithm! String index out of range: 85
java.lang.StringIndexOutOfBoundsException: String index out of range: 85
	at java.lang.String.substring(String.java:1765)
	at org.apache.lucene.benchmark.byTask.utils.Format.format(Format.java:85)
	at org.apache.lucene.benchmark.byTask.tasks.ReportTask.tableTitle(ReportTask.java:85)
	at org.apache.lucene.benchmark.byTask.tasks.ReportTask.genPartialReport(ReportTask.java:140)
	at org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask.reportSumByName(RepSumByNameTask.java:77)
	at org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask.doLogic(RepSumByNameTask.java:39)
	at org.apache.lucene.benchmark.byTask.tasks.PerfTask.runAndMaybeStats(PerfTask.java:83)
	at org.apache.lucene.benchmark.byTask.tasks.TaskSequence.doSerialTasks(TaskSequence.java:112)
	at org.apache.lucene.benchmark.byTask.tasks.TaskSequence.doLogic(TaskSequence.java:93)
	at org.apache.lucene.benchmark.byTask.utils.Algorithm.execute(Algorithm.java:228)
	at org.apache.lucene.benchmark.byTask.Benchmark.execute(Benchmark.java:73)
	at org.apache.lucene.benchmark.byTask.Benchmark.main(Benchmark.java:109)

The error seems to be caused by the fact that ReportTask uses the OP (operation) column for the String, but then uses the length of the algorithm declaration to index into the String, resulting in the index out of bounds exception.

The line in question is:
return (s + padd).substring(0, col.length());

And probably should be changed to something like:
    String s1 = (s + padd);
    return s1.substring(0, Math.min(col.length(), s1.length()));

Either that or the column should be trimmed.  The workaround is to explicitly name the task.

If no objections, I will make the change, tomorrow.  "
1,"Version.isSame(Object) not workingVersion interface is implemented (on the frontend) by the VersionImpl class (extending NodeWrapper), which delegates to an internal NodeImpl class, which in turn extends ItemImpl.

Say you have :
      Node node = // at Version 1.0
      Version version = // retrieved as 1.0 for the node
      Version baseVersion = node.getBaseVersion()

You now expect
      baseVersion.isSame(version)
even if
      baseVersion != version

This fails, because VersionImpl delegates the isSame call to its delegatee, thus above call becomes
      ((VersionImpl) baseVersion).delegatee.isSame(version)
where this method is implemented by the ItemImpl class from which the delegatee NodeImpl extends.

That latter implementation ItemImpl.isSame() only returns true if the other is an ItemImpl, too. But this is not the case because VersionImpl is a Version, NodeWrapper, Node but not an ItemImpl.

Probably the best solution would be for NodeImpl.isSame() to check whether the otherItem is a NodeWrapper und use ((NodeWrapper) otherItem).delegatee as the otherItem for the delegatee call.

On another track: ItemImpl.isSame() should probably do a fast check whether the otherItem is actually the same instance to prevent type checks..."
1,"NullPointerException in ItemManagerWe have a lot of these occurring:
java.lang.NullPointerException
	at org.apache.jackrabbit.core.ItemManager.getDefinition(ItemManager.java:206)
	at org.apache.jackrabbit.core.ItemData.getDefinition(ItemData.java:99)
	at org.apache.jackrabbit.core.AbstractNodeData.getNodeDefinition(AbstractNodeData.java:73)
	at org.apache.jackrabbit.core.NodeImpl.getDefinition(NodeImpl.java:2430)
	at org.apache.jackrabbit.core.ItemValidator.isProtected(ItemValidator.java:373)
	at org.apache.jackrabbit.core.ItemValidator.checkCondition(ItemValidator.java:273)
	at org.apache.jackrabbit.core.ItemValidator.checkRemove(ItemValidator.java:254)
	at org.apache.jackrabbit.core.ItemRemoveOperation.perform(ItemRemoveOperation.java:63)
	at org.apache.jackrabbit.core.session.SessionState.perform(SessionState.java:200)
	at org.apache.jackrabbit.core.ItemImpl.perform(ItemImpl.java:91)
	at org.apache.jackrabbit.core.ItemImpl.remove(ItemImpl.java:322)
	at org.apache.jackrabbit.core.NPEandCMETest$TestTask.run(NPEandCMETest.java:87)
	at java.lang.Thread.run(Thread.java:679)

I'll attach a junit test to reproduce this exception."
1,"DirectoryReader ignores NRT SegmentInfos in #isOptimized()DirectoryReader  only takes shared (with IW) SegmentInfos into account in DirectoryReader#isOptimized(). This can return true even if the actual realtime reader sees more than one segments. 

{code}
public boolean isOptimized() {
    ensureOpen();
   // if segmentsInfos changes in IW this can return false positive
    return segmentInfos.size() == 1 && !hasDeletions();
  }
{code}

DirectoryReader should check if this reader has a non-nul segmentInfosStart and use that instead"
1,"Calling size method of a ManageableArrayList causes NullPointerExceptionWhen using the NTCollectionConverterImpl with proxy=""true"" a call on the size () method of a ManageableArrayList causes a NullPointerException if there is no underlying List. LazyCollectionLoader doLoad returns null because there is are no children.

The ManageableArrayList is created because the isNull method of the NTCollectionConverterImpl class always returns false. 
According to the comment line this is done because the getCollectionNodes always returns a list. 
But after the fix for JCR-882 this is not correct anymore.

The attached fix corrects this. 

The only question remaining is how to differ between an empty list and a null-value for the field containing the list."
1,"Query parser builds invalid parse treeCalling org.apache.jackrabbit.spi.commons.query.QueryParser.parse on 

SELECT prop1 FROM nt:unstructured WHERE prop1 IS NOT NULL ORDER BY prop1 ASC 

results in the following parse tree

+ Select properties: {}prop1
 + PathQueryNode
   + LocationStepQueryNode:  NodeTest=* Descendants=true Index=NONE
     + RelationQueryNode: Op: NOT NULL Prop=@{}prop1 Type=STRING Value=%
     + NodeTypeQueryNode:  Prop={http://www.jcp.org/jcr/1.0}primaryType Value={http://www.jcp.org/jcr/nt/1.0}unstructured
 + OrderQueryNode
   {}prop1 asc=true

The RelationQueryNode should not have a second operand since the NOT NULL operator is unary.

"
1,Update monitor is not releasedWhen the timer thread in MultiIndex commits the volatile index after some idle time it does not release / reset the updateInProgress flag. This results in queries that hang until another thread writes to the workspace.
1,"IndexWriter.synced  field accumulates data leading to a Memory LeakI am running into a strange OutOfMemoryError. My small test application does
index and delete some few files. This is repeated for 60k times. Optimization
is run from every 2k times a file is indexed. Index size is 50KB. I did analyze
the HeapDumpFile and realized that IndexWriter.synced field occupied more than
half of the heap. That field is a private HashSet without a getter. Its task is
to hold files which have been synced already.

There are two calls to addAll and one call to add on synced but no remove or
clear throughout the lifecycle of the IndexWriter instance.

According to the Eclipse Memory Analyzer synced contains 32618 entries which
look like file names ""_e065_1.del"" or ""_e067.cfs""

The index directory contains 10 files only.

I guess synced is holding obsolete data "
1,"Yet another race in IW#nrtIsCurrentIn IW#nrtIsCurrent looks like this:

{code}
  synchronized boolean nrtIsCurrent(SegmentInfos infos) {
    ensureOpen();
    return infos.version == segmentInfos.version && !docWriter.anyChanges() && !bufferedDeletesStream.any();
  }
{code}

* the version changes once we checkpoint the IW
* docWriter has changes if there are any docs in ram or any deletes in the delQueue
* bufferedDeletes contain all frozen del packages from the delQueue

yet, what happens is 1. we decrement the numDocsInRam in DWPT#doAfterFlush (which is executed during DWPT#flush) but before we checkpoint. 2. if we freeze deletes (empty the delQueue) we put them in the flushQueue to maintain the order.  This means they are not yet in the bufferedDeleteStream.

Bottom line, there is a window where we could see IW#nrtIsCurrent returning true if we check within this particular window. Phew, I am not 100% sure if that is the reason for our latest failure in SOLR-2861 but from what the logs look like this could be what happens. If we randomly hit low values for maxBufferedDocs & maxBufferedDeleteTerms this is absolutely possible."
1,"PriorityQueue is inheriently broken if subclass attempts to use ""heap"" w/generic T bound to anything other then ""Object""as discovered in SOLR-2410 the fact that the protected ""heap"" variable in PriorityQueue is initialized using an Object[] makes it impossible for subclasses of PriorityQueue to exist and access the ""heap"" array unless they bind the generic to Object."
1,"org.apache.lucene.analysis.cn.ChineseTokenizer missing offset decrementApparently, in ChineseTokenizer, offset should be decremented like bufferIndex
when Character is OTHER_LETTER.  This directly affects startOffset and endOffset
values.

This is critical to have Highlighter working correctly because Highlighter marks
matching text based on these offset values."

label,summmarydescription
1,"fix analyzer bugs found by MockTokenizerIn LUCENE-3064, we beefed up MockTokenizer with assertions, and I've switched over the analysis tests to use MockTokenizer for better coverage.

However, this found a few bugs (one of which is LUCENE-3106):
* incrementToken() after it returns false in CommonGramsQueryFilter, HyphenatedWordsFilter, ShingleFilter, SynonymFilter
* missing end() implementation for PrefixAwareTokenFilter
* double reset() in QueryAutoStopWordAnalyzer and ReusableAnalyzerBase
* missing correctOffset()s in MockTokenizer itself.

I think it would be nice to just fix all the bugs on one issue... I've fixed everything except Shingle and Synonym"
0,"Remove getSafeJCRPath methods in HierarchyManagerImplThe getSafeJCRPath utility methods in the HierarchyManagerImpl class have not been used since revision 485720, but their presence still causes the hierarchy managers to depend on namespace mapping information. I'll remove the methods to simplify things."
1,"bugs in ByteArrayDataInputByteArrayDataInput has a byte[] ctor, but it doesn't actually work (some things like readVint will work, others will fail due to asserts).

The problem is it doesnt set things like limit in the ctor... I think the ctor should call reset()
Most code using this passes null to the ctor to initialize it, then uses reset(), instead they could just call ByteArrayInput(BytesRef.EMPTY_BYTES) if they want to do that.
finally, reset()'s limit looks like it should be offset + len"
0,"JSR 283: Repository ComplianceJSR 283 defines a huge set of new (or changed) repository descriptors as well as a couple of new methods on the
Repository interface that allows the API consumer to determine the feature set exposed by an implementation.

The new methods are

- Repository.isStandardDescriptor(String key)
- Repository.isSingleValueDescriptor(String key)
- Repository.getDescriptorValue(String key) Value
- Repository.getDescriptorValues(String key) Value[]"
0,"Clicking on the ""More Results"" link in luceneweb.war demo results in ArrayIndexOutOfBoundsExceptionSummary says it all."
0,"Precompile JavaCC parsers in jackrabbit-spi-commonsThe JavaCC-generated Java source files in jackrabbit-spi-commons require special configuration when importing Jackrabbit sources to an IDE like Eclipse. To make IDE integration smoother it would be nice if precompiled copies of the Java files existed the src/main/java folder.

Precompiling the sources would also allow us to avoid the JavaCC processing step during each Jackrabbit build. Instead we could have a separate profile for explicitly recompiling the JavaCC sources when they have been modified. In the past three years that has happened only once (JCR-952), so I think a bit of extra complexity there is justified by the simplification we can achieve in normal builds and IDE integration."
0,"Adding Event interface and isLocal()when a repository cluster is used, it seems that a common problem many people have is to detect if an observation event is send because of changes on the local instance or a remote instance of the cluster.

This is especially important if you want to do post processing of data
based on observation (the post processing should only be done by one instance in the cluster).

A current solution is to cast the jcr event object to the EventImpl of jackrabbit core which is obviously not a nice solution :)

So what about adding an event interface to jackrabbit api which extends the jcr event interface and adds the isLocal() method? "
0,"randomize skipInterval in testswe probably don't test the multi-level skipping very well, but skipInterval etc is now private to the codec, so for better test coverage we should parameterize it to the postings writers, and randomize it via mockrandomcodec."
0,"TCK: PropertyReadMethodsTest#testGetValues fails if it cannot find a single valued STRING propertyIf the JCR repository being tested does not contain any single valued property of type STRING, PropertyReadMethodsTest#testGetValues fails with the following exception:

java.lang.NullPointerException at org.apache.jackrabbit.test.api.PropertyReadMethodsTest.testGetValues(PropertyReadMethodsTest.java:273) at org.apache.jackrabbit.test.AbstractJCRTest.run(AbstractJCRTest.java:401)

The test should either try to find a single valued property of any type (it is guaranted that it will at least find jcr:primaryType) or should throw NotExecutableException if is does not find the property it needs."
0,"yank SegmentReader.norm out of SegmentReader.javaWhile working on flex scoring branch and LUCENE-3012, I noticed it was difficult to navigate 
the norms handling in SegmentReader's code.

I think we should yank this inner class out into a separate file as a start."
0,"remove unused code in SmartChineseAnalyzer hmm pkgthere is some unused code in the hmm package.

I would like to remove it before I supply a fix for LUCENE-1817.

only after this can we refactor any of this analyzer, otherwise we risk breaking custom dictionary support."
1,"TestIndexWriterExceptions reproducible AOOBE in MockVariableIntBlockCodec{code}
  [junit] Testsuite: org.apache.lucene.index.TestIndexWriterExceptions
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.739 sec
    [junit]
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterExceptions -Dtestmethod=testDocumentsWriterAbort -Dtests.seed=4579947455
682149564:-7960989923752018504
    [junit] NOTE: test params are: codec=RandomCodecProvider: {content=MockVariableIntBlock(baseBlockSize=32)}, locale=bg_BG, timezone=Brazil
/Acre
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestIndexWriterExceptions]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=94363216,total=125632512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testDocumentsWriterAbort(org.apache.lucene.index.TestIndexWriterExceptions):      Caused an ERROR
    [junit] 66
    [junit] java.lang.ArrayIndexOutOfBoundsException: 66
    [junit]     at org.apache.lucene.index.codecs.mockintblock.MockVariableIntBlockCodec$MockIntFactory$2.add(MockVariableIntBlockCodec.java:
114)
    [junit]     at org.apache.lucene.index.codecs.intblock.VariableIntBlockIndexOutput.close(VariableIntBlockIndexOutput.java:118)
    [junit]     at org.apache.lucene.index.codecs.sep.SepPostingsWriterImpl.close(SepPostingsWriterImpl.java:320)
    [junit]     at org.apache.lucene.index.codecs.BlockTermsWriter.close(BlockTermsWriter.java:137)
    [junit]     at org.apache.lucene.index.PerFieldCodecWrapper$FieldsWriter.close(PerFieldCodecWrapper.java:81)
    [junit]     at org.apache.lucene.index.FreqProxTermsWriter.flush(FreqProxTermsWriter.java:103)
    [junit]     at org.apache.lucene.index.TermsHash.flush(TermsHash.java:118)
    [junit]     at org.apache.lucene.index.DocInverter.flush(DocInverter.java:80)
    [junit]     at org.apache.lucene.index.DocFieldProcessor.flush(DocFieldProcessor.java:75)
    [junit]     at org.apache.lucene.index.DocumentsWriterPerThread.flush(DocumentsWriterPerThread.java:457)
    [junit]     at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:417)
    [junit]     at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:309)
    [junit]     at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:381)
    [junit]     at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1469)
    [junit]     at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1229)
    [junit]     at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1210)
    [junit]     at org.apache.lucene.index.TestIndexWriterExceptions.testDocumentsWriterAbort(TestIndexWriterExceptions.java:555)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1333)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1251)
    [junit]
    [junit]
    [junit] Test org.apache.lucene.index.TestIndexWriterExceptions FAILED
{code}

trunk: r1127871"
1,"Search with sort fails when a document has a missing valueTesting on version: lucene-1.4-rc2

Call in question: IndexSearcher.search(Query query, Filter filter, int nDocs, 
Sort sort) 

Description: I'm making a call to search with a sort field - in my case I'm 
sorting by date. If any document in the results set (Hits) has a missing value 
in the sort field, the entire call throws an [uncaught] exception during the 
sorting process with no results returned. 

This is an undesireable result, and the prospects for patching this problem 
outside the search classes are ugly, e.g. trying to patch the index itself.

This is actually a critical function in my application. Thank you for 
addressing it.

-Dan"
1,"PostingsConsumer#merge does not call finishDocWe discovered that the current merge function in PostingsConsumer is not calling the #finishDoc method. This does not have consequences for the standard codec (since the lastPosition is set to 0 in #startDoc, and its #finishDoc method is empty), but for the SepCodec, this results in position file corruption (the lastPosition is set to 0 in #finishDoc for the SepCodec)."
0,"Lazy Atomic Loading Stopwords in SmartCN The default constructor in SmartChineseAnalyzer loads the default (jar embedded) stopwords each time the constructor is invoked. 
This should be atomically loaded only once in an unmodifiable set.

"
0,"Added New Token API impl for ASCIIFoldingFilterI added an implementation of incrementToken to ASCIIFoldingFilter.java and extended the existing  testcase for it.
I will attach the patch shortly.
Beside this improvement I would like to start up a small discussion about this filter. ASCIIFoldingFitler is meant to be a replacement for ISOLatin1AccentFilter which is quite nice as it covers a superset of the latter. I have used this filter quite often but never on a as it is basis. In the most cases this filter does the correct thing (replace a special char with its ascii correspondent) but in some cases like for German umlaut it does not return the expected result. A german umlaut  like 'ä' does not translate to a but rather to 'ae'. I would like to change this but I'n not 100% sure if that is expected by all users of that filter. Another way of doing it would be to make it configurable with a flag. This would not affect performance as we only check if such a umlaut char is found. 
Further it would be really helpful if that filter could ""inject"" the original/unmodified token with the same position increment into the token stream on demand. I think its a valid use-case to index the modified and unmodified token. For instance, the german word ""süd"" would be folded to ""sud"". In a query q:(süd) the filter would also fold to sud and therefore find sud which has a totally different meaning. Folding works quite well but for special cases would could add those options to make users life easier. The latter could be done in a subclass while the umlaut problem should be fixed in the base class.

simon "
0,"client cache does not respect 'Cache-Control: no-store' on requests""The purpose of the no-store directive is to prevent the inadvertent release or retention of sensitive information (for example, on backup tapes). The no-store directive applies to the entire message, and MAY be sent either in a response or in a request. If sent in a request, a cache MUST NOT store any part of either this request or any response to it.""

http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.2

The current implementation will incorrectly cache responses to requests containing 'Cache-Control: no-store'."
0,"Skip sync delay when changes are foundThe cluster synchronization on a slave does always wait for some time (as specified in the sync delay) before fetching changes. If a lot of changes are being written to the master, a slave will considerably fall behind the master in term of revisions, which may endanger the integrity of the cluster if the master will crash. I therefore suggest that a slave should rather immediately contact the master again after some changes have been found, until it sees no more changes."
0,"Replacement for TermAttribute+Impl with extended capabilities (byte[] support, CharSequence, Appendable)For flexible indexing terms can be simple byte[] arrays, while the current TermAttribute only supports char[]. This is fine for plain text, but e.g NumericTokenStream should directly work on the byte[] array.
Also TermAttribute lacks of some interfaces that would make it simplier for users to work with them: Appendable and CharSequence

I propose to create a new interface ""CharTermAttribute"" with a clean new API that concentrates on CharSequence and Appendable.
The implementation class will simply support the old and new interface working on the same term buffer. DEFAULT_ATTRIBUTE_FACTORY will take care of this. So if somebody adds a TermAttribute, he will get an implementation class that can be also used as CharTermAttribute. As both attributes create the same impl instance both calls to addAttribute are equal. So a TokenFilter that adds CharTermAttribute to the source will work with the same instance as the Tokenizer that requested the (deprecated) TermAttribute.

To also support byte[] only terms like Collation or NumericField needs, a separate getter-only interface will be added, that returns a reusable BytesRef, e.g. BytesRefGetterAttribute. The default implementation class will also support this interface. For backwards compatibility with old self-made-TermAttribute implementations, the indexer will check with hasAttribute(), if the BytesRef getter interface is there and if not will wrap a old-style TermAttribute (a deprecated wrapper class will be provided): new BytesRefGetterAttributeWrapper(TermAttribute), that is used by the indexer then."
0,"Exception when missing namespace in CND file should have clearer messageUsing the attached CND file, when calling CndImporter.registerNodeTypes(..) the following message is in the returned exception:

""Unable to parse CND Input: Error setting name of sling:resourceType to sling:resourceType (bundle://23.0:0/SLING-INF/nodetypes/types.cnd, line 24)""

The issue with line 24 is that the ""sling"" namespace has not been included.  The message should state that a namespace is missing and what prefix is not understood."
0,"Let the AbstractISMLockingTest tests fail properlyThe tests in the AbstractISMLockingTest class call junit.framework.Assert.fail() on threads that are not managed by the JUnit framework. Therefore, such calls to fail are not interpreted as test failures, but are merely logged to the console and the build succeeds. This is easy to see with a stub implementation of the ISMLocking type which returns non-null references from the two acquire methods and the downgrade method: many of the following stacktraces appear, but the build succeeds.

Exception in thread ""Thread-1"" junit.framework.AssertionFailedError: acquireWriteLock must block
	at junit.framework.Assert.fail(Assert.java:47)
	at org.apache.jackrabbit.core.state.AbstractISMLockingTest.checkBlocking(AbstractISMLockingTest.java:214)
	at org.apache.jackrabbit.core.state.AbstractISMLockingTest$1.run(AbstractISMLockingTest.java:88)
	at java.lang.Thread.run(Thread.java:613)


"
1,"TestIndexWriter#testThreadInterruptDeadlock fails with OOM Selckin reported a repeatedly failing test that throws OOM Exceptions. According to the heapdump the MockDirectoryWrapper#createdFiles HashSet takes about 400MB heapspace containing 4194304 entries. Seems kind of way too many though :)

{noformat}
 [junit] java.lang.OutOfMemoryError: Java heap space
    [junit] Dumping heap to /tmp/java_pid25990.hprof ...
    [junit] Heap dump file created [520807744 bytes in 4.250 secs]
    [junit] Testsuite: org.apache.lucene.index.TestIndexWriter
    [junit] Testcase: testThreadInterruptDeadlock(org.apache.lucene.index.TestIndexWriter):	FAILED
    [junit] 
    [junit] junit.framework.AssertionFailedError: 
    [junit] 	at org.apache.lucene.index.TestIndexWriter.testThreadInterruptDeadlock(TestIndexWriter.java:2249)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1282)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1211)
    [junit] 
    [junit] 
    [junit] Testcase: testThreadInterruptDeadlock(org.apache.lucene.index.TestIndexWriter):	FAILED
    [junit] Some threads threw uncaught exceptions!
    [junit] junit.framework.AssertionFailedError: Some threads threw uncaught exceptions!
    [junit] 	at org.apache.lucene.util.LuceneTestCase.tearDown(LuceneTestCase.java:557)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1282)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1211)
    [junit] 
    [junit] 
    [junit] Tests run: 67, Failures: 2, Errors: 0, Time elapsed: 3,254.884 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] FAILED; unexpected exception
    [junit] java.lang.OutOfMemoryError: Java heap space
    [junit] 	at org.apache.lucene.store.RAMFile.newBuffer(RAMFile.java:85)
    [junit] 	at org.apache.lucene.store.RAMFile.addBuffer(RAMFile.java:58)
    [junit] 	at org.apache.lucene.store.RAMOutputStream.switchCurrentBuffer(RAMOutputStream.java:132)
    [junit] 	at org.apache.lucene.store.RAMOutputStream.copyBytes(RAMOutputStream.java:171)
    [junit] 	at org.apache.lucene.store.MockIndexOutputWrapper.copyBytes(MockIndexOutputWrapper.java:155)
    [junit] 	at org.apache.lucene.index.CompoundFileWriter.copyFile(CompoundFileWriter.java:223)
    [junit] 	at org.apache.lucene.index.CompoundFileWriter.close(CompoundFileWriter.java:189)
    [junit] 	at org.apache.lucene.index.SegmentMerger.createCompoundFile(SegmentMerger.java:138)
    [junit] 	at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3344)
    [junit] 	at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:2959)
    [junit] 	at org.apache.lucene.index.SerialMergeScheduler.merge(SerialMergeScheduler.java:37)
    [junit] 	at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1763)
    [junit] 	at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1758)
    [junit] 	at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1754)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1373)
    [junit] 	at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1230)
    [junit] 	at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1211)
    [junit] 	at org.apache.lucene.index.TestIndexWriter$IndexerThreadInterrupt.run(TestIndexWriter.java:2154)
    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testThreadInterruptDeadlock -Dtests.seed=7183538093651149:3431510331342554160
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testThreadInterruptDeadlock -Dtests.seed=7183538093651149:3431510331342554160
    [junit] The following exceptions were thrown by threads:
    [junit] *** Thread: Thread-379 ***
    [junit] java.lang.RuntimeException: MockDirectoryWrapper: cannot close: there are still open files: {_3r1n_0.tib=1, _3r1n_0.frq=1, _3r1n_0.pos=1, _3r1m.cfs=1, _3r1n_0.doc=1, _3r1n.tvf=1, _3r1n.tvd=1, _3r1n.tvx=1, _3r1n.fdx=1, _3r1n.fdt=1, _3r1q.cfs=1, _3r1o.cfs=1, _3r1n_0.skp=1, _3r1n_0.pyl=1}
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:448)
    [junit] 	at org.apache.lucene.index.TestIndexWriter$IndexerThreadInterrupt.run(TestIndexWriter.java:2217)
    [junit] Caused by: java.lang.RuntimeException: unclosed IndexOutput
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.createOutput(MockDirectoryWrapper.java:367)
    [junit] 	at org.apache.lucene.index.FieldInfos.write(FieldInfos.java:563)
    [junit] 	at org.apache.lucene.index.DocFieldProcessor.flush(DocFieldProcessor.java:82)
    [junit] 	at org.apache.lucene.index.DocumentsWriterPerThread.flush(DocumentsWriterPerThread.java:381)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:378)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:505)
    [junit] 	at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:2621)
    [junit] 	at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:2598)
    [junit] 	at org.apache.lucene.index.IndexWriter.prepareCommit(IndexWriter.java:2464)
    [junit] 	at org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:2537)
    [junit] 	at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2519)
    [junit] 	at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2503)
    [junit] 	at org.apache.lucene.index.TestIndexWriter$IndexerThreadInterrupt.run(TestIndexWriter.java:2156)
    [junit] NOTE: test params are: codec=RandomCodecProvider: {=MockVariableIntBlock(baseBlockSize=105), f6=MockFixedIntBlock(blockSize=1372), f7=Pulsing(freqCutoff=11), f8=MockRandom, f9=MockVariableIntBlock(baseBlockSize=105), f1=MockSep, f0=Pulsing(freqCutoff=11), f3=Pulsing(freqCutoff=11), f2=MockFixedIntBlock(blockSize=1372), f5=MockVariableIntBlock(baseBlockSize=105), f4=MockRandom, f=Standard, c=Pulsing(freqCutoff=11), termVector=MockFixedIntBlock(blockSize=1372), d9=MockVariableIntBlock(baseBlockSize=105), d8=MockRandom, d5=MockSep, d4=Pulsing(freqCutoff=11), d7=MockFixedIntBlock(blockSize=1372), d6=MockVariableIntBlock(baseBlockSize=105), d25=SimpleText, d0=Pulsing(freqCutoff=11), c29=SimpleText, d24=MockSep, d1=MockSep, c28=MockVariableIntBlock(baseBlockSize=105), d23=MockRandom, d2=MockVariableIntBlock(baseBlockSize=105), c27=MockRandom, d22=Standard, d3=MockFixedIntBlock(blockSize=1372), d21=MockVariableIntBlock(baseBlockSize=105), d20=MockRandom, c22=SimpleText, c21=MockSep, c20=MockRandom, d29=MockFixedIntBlock(blockSize=1372), c26=MockFixedIntBlock(blockSize=1372), d28=MockVariableIntBlock(baseBlockSize=105), c25=MockVariableIntBlock(baseBlockSize=105), d27=MockSep, c24=MockSep, d26=Pulsing(freqCutoff=11), c23=Pulsing(freqCutoff=11), e9=MockSep, e8=Standard, e7=SimpleText, e6=MockVariableIntBlock(baseBlockSize=105), e5=MockRandom, c17=MockSep, e3=SimpleText, d12=Pulsing(freqCutoff=11), c16=Pulsing(freqCutoff=11), e4=Standard, d11=MockFixedIntBlock(blockSize=1372), c19=MockFixedIntBlock(blockSize=1372), e1=MockRandom, d14=MockVariableIntBlock(baseBlockSize=105), c18=MockVariableIntBlock(baseBlockSize=105), e2=MockVariableIntBlock(baseBlockSize=105), d13=MockRandom, e0=MockFixedIntBlock(blockSize=1372), d10=MockSep, d19=Pulsing(freqCutoff=11), c11=MockVariableIntBlock(baseBlockSize=105), c10=MockRandom, d16=MockRandom, c13=MockRandom, c12=Standard, d15=Standard, d18=SimpleText, c15=SimpleText, d17=MockSep, c14=MockSep, b3=SimpleText, b2=MockSep, b5=Pulsing(freqCutoff=11), b4=MockFixedIntBlock(blockSize=1372), b7=MockFixedIntBlock(blockSize=1372), b6=MockVariableIntBlock(baseBlockSize=105), d50=Pulsing(freqCutoff=11), b9=MockRandom, b8=Standard, d43=MockRandom, d42=Standard, d41=MockFixedIntBlock(blockSize=1372), d40=MockVariableIntBlock(baseBlockSize=105), d47=MockSep, d46=Pulsing(freqCutoff=11), b0=MockFixedIntBlock(blockSize=1372), d45=Standard, b1=Pulsing(freqCutoff=11), d44=SimpleText, d49=Pulsing(freqCutoff=11), d48=MockFixedIntBlock(blockSize=1372), c6=MockRandom, c5=Standard, c4=MockFixedIntBlock(blockSize=1372), c3=MockVariableIntBlock(baseBlockSize=105), c9=Pulsing(freqCutoff=11), c8=Standard, c7=SimpleText, d30=SimpleText, d32=Pulsing(freqCutoff=11), d31=MockFixedIntBlock(blockSize=1372), c1=Standard, d34=MockFixedIntBlock(blockSize=1372), c2=MockRandom, d33=MockVariableIntBlock(baseBlockSize=105), d36=MockRandom, c0=MockFixedIntBlock(blockSize=1372), d35=Standard, d38=Standard, d37=SimpleText, d39=Pulsing(freqCutoff=11), e92=MockFixedIntBlock(blockSize=1372), e93=Pulsing(freqCutoff=11), e90=MockSep, e91=SimpleText, e89=Pulsing(freqCutoff=11), e88=Standard, e87=SimpleText, e86=MockRandom, e85=Standard, e84=MockFixedIntBlock(blockSize=1372), e83=MockVariableIntBlock(baseBlockSize=105), e80=MockVariableIntBlock(baseBlockSize=105), e81=SimpleText, e82=Standard, e77=MockFixedIntBlock(blockSize=1372), e76=MockVariableIntBlock(baseBlockSize=105), e79=MockRandom, e78=Standard, e73=SimpleText, e72=MockSep, e75=Pulsing(freqCutoff=11), e74=MockFixedIntBlock(blockSize=1372), binary=Pulsing(freqCutoff=11), f98=MockSep, f97=Pulsing(freqCutoff=11), f99=MockVariableIntBlock(baseBlockSize=105), f94=MockRandom, f93=Standard, f96=SimpleText, f95=MockSep, e95=MockSep, e94=Pulsing(freqCutoff=11), e97=MockFixedIntBlock(blockSize=1372), e96=MockVariableIntBlock(baseBlockSize=105), e99=MockVariableIntBlock(baseBlockSize=105), e98=MockRandom, id=MockRandom, f34=SimpleText, f33=MockSep, f32=MockRandom, f31=Standard, f30=MockVariableIntBlock(baseBlockSize=105), f39=MockRandom, f38=MockFixedIntBlock(blockSize=1372), f37=MockVariableIntBlock(baseBlockSize=105), f36=MockSep, f35=Pulsing(freqCutoff=11), f43=MockSep, f42=Pulsing(freqCutoff=11), f45=MockFixedIntBlock(blockSize=1372), f44=MockVariableIntBlock(baseBlockSize=105), f41=SimpleText, f40=MockSep, f47=MockVariableIntBlock(baseBlockSize=105), f46=MockRandom, f49=Standard, f48=SimpleText, content=MockFixedIntBlock(blockSize=1372), e19=Standard, e18=SimpleText, e17=MockRandom, f12=Standard, e16=Standard, f11=SimpleText, f10=MockVariableIntBlock(baseBlockSize=105), e15=MockFixedIntBlock(blockSize=1372), e14=MockVariableIntBlock(baseBlockSize=105), f16=Pulsing(freqCutoff=11), e13=Pulsing(freqCutoff=11), f15=MockFixedIntBlock(blockSize=1372), e12=MockFixedIntBlock(blockSize=1372), e11=SimpleText, f14=SimpleText, e10=MockSep, f13=MockSep, f19=Standard, f18=MockFixedIntBlock(blockSize=1372), f17=MockVariableIntBlock(baseBlockSize=105), e29=MockFixedIntBlock(blockSize=1372), e26=Standard, f21=SimpleText, e25=SimpleText, f20=MockSep, e28=MockSep, f23=Pulsing(freqCutoff=11), e27=Pulsing(freqCutoff=11), f22=MockFixedIntBlock(blockSize=1372), f25=MockFixedIntBlock(blockSize=1372), e22=MockFixedIntBlock(blockSize=1372), f24=MockVariableIntBlock(baseBlockSize=105), e21=MockVariableIntBlock(baseBlockSize=105), f27=MockRandom, e24=MockRandom, f26=Standard, e23=Standard, f29=Standard, f28=SimpleText, e20=Pulsing(freqCutoff=11), field=MockRandom, string=Pulsing(freqCutoff=11), e30=MockSep, e31=SimpleText, a98=MockRandom, e34=MockVariableIntBlock(baseBlockSize=105), a99=MockVariableIntBlock(baseBlockSize=105), e35=MockFixedIntBlock(blockSize=1372), f79=MockVariableIntBlock(baseBlockSize=105), e32=Pulsing(freqCutoff=11), e33=MockSep, b97=Pulsing(freqCutoff=11), f77=MockFixedIntBlock(blockSize=1372), e38=SimpleText, b98=MockSep, f78=Pulsing(freqCutoff=11), e39=Standard, b99=MockVariableIntBlock(baseBlockSize=105), f75=MockSep, e36=MockRandom, f76=SimpleText, e37=MockVariableIntBlock(baseBlockSize=105), f73=SimpleText, f74=Standard, f71=MockRandom, f72=MockVariableIntBlock(baseBlockSize=105), f81=MockFixedIntBlock(blockSize=1372), f80=MockVariableIntBlock(baseBlockSize=105), e40=MockSep, e41=MockVariableIntBlock(baseBlockSize=105), e42=MockFixedIntBlock(blockSize=1372), e43=MockRandom, e44=MockVariableIntBlock(baseBlockSize=105), e45=SimpleText, e46=Standard, f86=MockVariableIntBlock(baseBlockSize=105), e47=MockSep, f87=MockFixedIntBlock(blockSize=1372), e48=SimpleText, f88=Standard, e49=MockFixedIntBlock(blockSize=1372), f89=MockRandom, f82=MockSep, f83=SimpleText, f84=MockFixedIntBlock(blockSize=1372), f85=Pulsing(freqCutoff=11), f90=MockVariableIntBlock(baseBlockSize=105), f92=Standard, f91=SimpleText, str=MockFixedIntBlock(blockSize=1372), a76=MockVariableIntBlock(baseBlockSize=105), e56=MockRandom, f59=MockRandom, a77=MockFixedIntBlock(blockSize=1372), e57=MockVariableIntBlock(baseBlockSize=105), a78=Standard, e54=MockFixedIntBlock(blockSize=1372), f57=MockFixedIntBlock(blockSize=1372), a79=MockRandom, e55=Pulsing(freqCutoff=11), f58=Pulsing(freqCutoff=11), e52=Pulsing(freqCutoff=11), e53=MockSep, e50=SimpleText, e51=Standard, f51=Standard, f52=MockRandom, f50=MockFixedIntBlock(blockSize=1372), f55=Pulsing(freqCutoff=11), f56=MockSep, f53=SimpleText, e58=Standard, f54=Standard, e59=MockRandom, a80=MockVariableIntBlock(baseBlockSize=105), e60=MockRandom, a82=Standard, a81=SimpleText, a84=SimpleText, a83=MockSep, a86=Pulsing(freqCutoff=11), a85=MockFixedIntBlock(blockSize=1372), a89=Pulsing(freqCutoff=11), f68=Standard, e65=Standard, f69=MockRandom, e66=MockRandom, a87=SimpleText, e67=MockSep, a88=Standard, e68=SimpleText, e61=MockFixedIntBlock(blockSize=1372), e62=Pulsing(freqCutoff=11), e63=MockRandom, e64=MockVariableIntBlock(baseBlockSize=105), f60=SimpleText, f61=Standard, f62=Pulsing(freqCutoff=11), f63=MockSep, e69=Pulsing(freqCutoff=11), f64=MockFixedIntBlock(blockSize=1372), f65=Pulsing(freqCutoff=11), f66=MockRandom, f67=MockVariableIntBlock(baseBlockSize=105), f70=MockRandom, a93=Pulsing(freqCutoff=11), a92=MockFixedIntBlock(blockSize=1372), a91=SimpleText, e71=MockSep, a90=MockSep, e70=Pulsing(freqCutoff=11), a97=MockRandom, a96=Standard, a95=MockFixedIntBlock(blockSize=1372), a94=MockVariableIntBlock(baseBlockSize=105), c58=MockFixedIntBlock(blockSize=1372), a63=Pulsing(freqCutoff=11), a64=MockSep, c59=Pulsing(freqCutoff=11), c56=MockSep, d59=MockSep, a61=SimpleText, c57=SimpleText, a62=Standard, c54=SimpleText, c55=Standard, a60=MockRandom, c52=MockRandom, c53=MockVariableIntBlock(baseBlockSize=105), d53=MockVariableIntBlock(baseBlockSize=105), d54=MockFixedIntBlock(blockSize=1372), d51=Pulsing(freqCutoff=11), d52=MockSep, d57=SimpleText, b62=Standard, d58=Standard, b63=MockRandom, d55=MockRandom, b60=MockVariableIntBlock(baseBlockSize=105), d56=MockVariableIntBlock(baseBlockSize=105), b61=MockFixedIntBlock(blockSize=1372), b56=MockSep, b55=Pulsing(freqCutoff=11), b54=Standard, b53=SimpleText, d61=SimpleText, b59=MockRandom, d60=MockSep, b58=Pulsing(freqCutoff=11), b57=MockFixedIntBlock(blockSize=1372), c62=MockFixedIntBlock(blockSize=1372), c61=MockVariableIntBlock(baseBlockSize=105), a59=MockRandom, c60=MockSep, a58=Standard, a57=MockVariableIntBlock(baseBlockSize=105), a56=MockRandom, a55=Pulsing(freqCutoff=11), a54=MockFixedIntBlock(blockSize=1372), a72=MockFixedIntBlock(blockSize=1372), c67=MockVariableIntBlock(baseBlockSize=105), a73=Pulsing(freqCutoff=11), c68=MockFixedIntBlock(blockSize=1372), a74=MockRandom, c69=Standard, a75=MockVariableIntBlock(baseBlockSize=105), c63=MockSep, c64=SimpleText, a70=Pulsing(freqCutoff=11), c65=MockFixedIntBlock(blockSize=1372), a71=MockSep, c66=Pulsing(freqCutoff=11), d62=MockRandom, d63=MockVariableIntBlock(baseBlockSize=105), d64=SimpleText, b70=MockRandom, d65=Standard, b71=SimpleText, d66=MockSep, b72=Standard, d67=SimpleText, b73=Pulsing(freqCutoff=11), d68=MockFixedIntBlock(blockSize=1372), b74=MockSep, d69=Pulsing(freqCutoff=11), b65=Pulsing(freqCutoff=11), b64=MockFixedIntBlock(blockSize=1372), b67=MockVariableIntBlock(baseBlockSize=105), b66=MockRandom, d70=MockSep, b69=MockRandom, b68=Standard, d72=MockFixedIntBlock(blockSize=1372), d71=MockVariableIntBlock(baseBlockSize=105), c71=MockVariableIntBlock(baseBlockSize=105), c70=MockRandom, a69=Pulsing(freqCutoff=11), c73=Standard, c72=SimpleText, a66=MockRandom, a65=Standard, a68=SimpleText, a67=MockSep, c32=Standard, c33=MockRandom, c30=MockVariableIntBlock(baseBlockSize=105), c31=MockFixedIntBlock(blockSize=1372), c36=Pulsing(freqCutoff=11), a41=MockSep, c37=MockSep, a42=SimpleText, a0=MockSep, c34=SimpleText, c35=Standard, a40=MockRandom, b84=SimpleText, d79=MockSep, b85=Standard, b82=MockRandom, d77=Standard, c38=MockFixedIntBlock(blockSize=1372), b83=MockVariableIntBlock(baseBlockSize=105), d78=MockRandom, c39=Pulsing(freqCutoff=11), b80=MockVariableIntBlock(baseBlockSize=105), d75=MockRandom, b81=MockFixedIntBlock(blockSize=1372), d76=MockVariableIntBlock(baseBlockSize=105), d73=MockFixedIntBlock(blockSize=1372), d74=Pulsing(freqCutoff=11), d83=MockSep, a9=Standard, d82=Pulsing(freqCutoff=11), d81=Standard, d80=SimpleText, b79=MockVariableIntBlock(baseBlockSize=105), b78=Pulsing(freqCutoff=11), b77=MockFixedIntBlock(blockSize=1372), b76=SimpleText, b75=MockSep, a1=SimpleText, a35=MockFixedIntBlock(blockSize=1372), a2=Standard, a34=MockVariableIntBlock(baseBlockSize=105), a3=Pulsing(freqCutoff=11), a33=MockSep, a4=MockSep, a32=Pulsing(freqCutoff=11), a5=MockFixedIntBlock(blockSize=1372), a39=Standard, c40=Pulsing(freqCutoff=11), a6=Pulsing(freqCutoff=11), a38=SimpleText, a7=MockRandom, a37=MockVariableIntBlock(baseBlockSize=105), a8=MockVariableIntBlock(baseBlockSize=105), a36=MockRandom, c41=SimpleText, c42=Standard, c43=Pulsing(freqCutoff=11), c44=MockSep, c45=MockFixedIntBlock(blockSize=1372), a50=Pulsing(freqCutoff=11), c46=Pulsing(freqCutoff=11), a51=MockSep, c47=MockRandom, a52=MockVariableIntBlock(baseBlockSize=105), c48=MockVariableIntBlock(baseBlockSize=105), a53=MockFixedIntBlock(blockSize=1372), b93=MockSep, d88=Pulsing(freqCutoff=11), c49=Standard, b94=SimpleText, d89=MockSep, b95=MockFixedIntBlock(blockSize=1372), b96=Pulsing(freqCutoff=11), d84=Standard, b90=MockVariableIntBlock(baseBlockSize=105), d85=MockRandom, b91=SimpleText, d86=MockSep, b92=Standard, d87=SimpleText, d92=Pulsing(freqCutoff=11), d91=MockFixedIntBlock(blockSize=1372), d94=MockVariableIntBlock(baseBlockSize=105), d93=MockRandom, b87=MockFixedIntBlock(blockSize=1372), b86=MockVariableIntBlock(baseBlockSize=105), d90=MockSep, b89=MockRandom, b88=Standard, a44=MockVariableIntBlock(baseBlockSize=105), a43=MockRandom, a46=Standard, a45=SimpleText, a48=SimpleText, a47=MockSep, c51=MockRandom, a49=MockFixedIntBlock(blockSize=1372), c50=Standard, d98=MockRandom, d97=Standard, d96=MockFixedIntBlock(blockSize=1372), d95=MockVariableIntBlock(baseBlockSize=105), d99=SimpleText, a20=Standard, c99=MockSep, c98=Pulsing(freqCutoff=11), c97=Standard, c96=SimpleText, b19=Standard, a16=Standard, a17=MockRandom, b17=MockVariableIntBlock(baseBlockSize=105), a14=MockVariableIntBlock(baseBlockSize=105), b18=MockFixedIntBlock(blockSize=1372), a15=MockFixedIntBlock(blockSize=1372), a12=MockFixedIntBlock(blockSize=1372), a13=Pulsing(freqCutoff=11), a10=MockSep, a11=SimpleText, b11=SimpleText, b12=Standard, b10=MockVariableIntBlock(baseBlockSize=105), b15=MockFixedIntBlock(blockSize=1372), b16=Pulsing(freqCutoff=11), a18=SimpleText, b13=MockSep, a19=Standard, b14=SimpleText, b30=Standard, a31=Pulsing(freqCutoff=11), a30=MockFixedIntBlock(blockSize=1372), b28=SimpleText, a25=SimpleText, b29=Standard, a26=Standard, a27=Pulsing(freqCutoff=11), a28=MockSep, a21=MockVariableIntBlock(baseBlockSize=105), a22=MockFixedIntBlock(blockSize=1372), a23=Standard, a24=MockRandom, b20=MockSep, b21=SimpleText, b22=MockFixedIntBlock(blockSize=1372), b23=Pulsing(freqCutoff=11), a29=MockFixedIntBlock(blockSize=1372), b24=MockVariableIntBlock(baseBlockSize=105), b25=MockFixedIntBlock(blockSize=1372), b26=Standard, b27=MockRandom, b41=MockVariableIntBlock(baseBlockSize=105), b40=MockRandom, c77=SimpleText, c76=MockSep, c75=MockRandom, c74=Standard, c79=MockSep, c78=Pulsing(freqCutoff=11), c80=MockSep, c83=MockRandom, c84=MockVariableIntBlock(baseBlockSize=105), c81=MockFixedIntBlock(blockSize=1372), b39=MockRandom, c82=Pulsing(freqCutoff=11), b37=MockVariableIntBlock(baseBlockSize=105), b38=MockFixedIntBlock(blockSize=1372), b35=Pulsing(freqCutoff=11), b36=MockSep, b33=MockSep, b34=SimpleText, b31=Standard, b32=MockRandom, str2=Standard, b50=MockRandom, b52=SimpleText, str3=Pulsing(freqCutoff=11), b51=MockSep, c86=MockSep, tvtest=Pulsing(freqCutoff=11), c85=Pulsing(freqCutoff=11), c88=MockFixedIntBlock(blockSize=1372), c87=MockVariableIntBlock(baseBlockSize=105), c89=MockRandom, c90=MockRandom, c91=MockVariableIntBlock(baseBlockSize=105), c92=Standard, c93=MockRandom, c94=MockSep, c95=SimpleText, content1=SimpleText, b46=MockRandom, b47=MockVariableIntBlock(baseBlockSize=105), content3=MockRandom, b48=SimpleText, content4=Standard, b49=Standard, content5=MockVariableIntBlock(baseBlockSize=105), b42=Pulsing(freqCutoff=11), b43=MockSep, b44=MockVariableIntBlock(baseBlockSize=105), b45=MockFixedIntBlock(blockSize=1372)}, locale=it_CH, timezone=Europe/Chisinau
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestMergeSchedulerExternal, TestToken, TestCodecs, TestFieldInfos, TestFlushByRamOrCountsPolicy, TestIndexReaderReopen, TestIndexWriter]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=275548088,total=309395456
{noformat}"
1,"LockableFileRevision not thread-safeLockableFileRevision works well across process boundaries, but does not within the same JVM. The methods lock() and unlock() must be synchronized similar to DatabaseRevision."
0,"Add myqsql ddl for clustering (DatabaseJournal)the default ddl for clustering does't work with mysql, so it would be nice to include a mysql specific ddl also for clustering."
0,"FST should differentiate between final vs non-final stop nodesI'm breaking out this one improvement from LUCENE-2948...

Currently, if a node has no outgoing edges (a ""stop node"") the FST
forcefully marks this as a final node, but it need not do this.  Ie,
whether that node is final or not should be orthogonal to whether it
has arcs leaving or not.
"
1,"Calling PropertyDef.getDefaultValue() via RMI results in Exceptionhi jukka

30.03.2005 15:28:23 *MARK * servletengine: Servlet threw exception: 
org.apache.jackrabbit.rmi.client.RemoteRuntimeException: java.rmi.UnmarshalException: error unmarshalling return; nested exception is: 
	java.io.WriteAbortedException: writing aborted; java.io.NotSerializableException: javax.jcr.BooleanValue
	at org.apache.jackrabbit.rmi.client.ClientPropertyDef.getDefaultValues(ClientPropertyDef.java:76)

[...]

regards
angela"
0,"Expert API to specify indexing chainIt would be nice to add an expert API to specify an indexing chain, so that
we can make use of Mike's nice LUCENE-1301 feature.

This patch simply adds a package-protected expert API to IndexWriter and 
DocumentsWriter. It adds a inner, abstract class to DocumentsWriter called 
IndexingChain, and a default implementation that is the currently used one.

This might not be the final solution, but a nice way to play with different
modules in the indexing chain.

Could you take a look at the patch, Mike? "
0,Fix for deprecations in contrib/surroundFix for deprecations in contrib/surround.
1,"FieldSortedHitQueue - subsequent String sorts with different locales sort identicallyFrom my own post to the java-user list. I have looked into this further and am sure it's a bug.

---

It seems to me that there's a possible bug in FieldSortedHitQueue, specifically in getCachedComparator(). This is showing up on our 1.4.3 install, but it seems from source code inspection that if it's a bug, it's in 1.9.1 also.

The issue shows up when you need to sort results from a given IndexReader multiple times, using different locales. On line 180 (all line numbers from the 1.9.1 code), we have this:

ScoreDocComparator comparator = lookup (reader, fieldname, type, factory);

Then, if no comparator is found in the cache, a new one is created (line 193) and then stored in the cache (line 202). HOWEVER, both the cache lookup() and store() do NOT take into account locale; if we, on the same index reader, try to do one search sorted by Locale.FRENCH and one by Locale.ITALIAN, the first one will result in a cache miss, a new French comparator will be created, and stored in the cache. Second time through, lookup() finds the cached French comparator -- even though this time, the locale parameter to getCachedComparator() is an Italian locale. Therefore, we don't create a new comparator and we use the wrong one to sort the results.

It looks to me (unless I'm mistaken) that the FieldCacheImpl.Entry class should have an additional property, .locale, to ensure that different locales get different comparators.

---

Patch (well, most of one) to follow immediately."
1,"304 response status handlingI have an IBM WebSphere server that returns 304 responses with a Content-
Length header set to something other than 0 and the server is not closing the 
connection.  According to the HTTP RFC 
(http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html#sec10.3.5):

""The 304 response MUST NOT contain a message-body, and thus is always 
terminated by the first empty line after the header fields.""

Obviously, the web server is returning a bad response but the HTTPClient 
blocks waiting on data in the response even though there shouldn't be any.  
Other HTTP clients (browsers) do not have this issue and seem to ignore the 
fact that the server set an invalid Content-Length in the response."
0,"Support only-if-cached directiveAdd support for only-if-cached Cache-Control directive- If the request is not servable from the cache, return a 504 Gateway Timeout.  See http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.4"
1,"Readers wrapping other readers don't prevent usage if any of their subreaders was closedOn recent trunk test we got this problem:
org.apache.lucene.index.TestReaderClosed.test
fails because the inner reader is closed but the wrapped outer ones are still open.

I fixed the issue partially for SlowCompositeReaderWrapper and ParallelAtomicReader but it failed again. The cool thing with this test is the following:

The test opens an DirectoryReader and then creates a searcher, closes the reader and executes a search. This is not an issue, if the reader is closed that the search is running on. This test uses LTC.newSearcher(wrap=true), which randomly wraps the passed Reader with SlowComposite or ParallelReader - or with both!!! If you then close the original inner reader, the close is not detected when excuting search. This can cause SIGSEGV when MMAP is used.

The problem in (in Slow* and Parallel*) is, that both have their own Fields instances thats are kept alive until the reader itsself is closed. If the child reader is closed, the wrapping reader does not know and still uses its own Fields instance that delegates to the inner readers. On this step no more ensureOpen checks are done, causing the failures.

The first fix done in Slow and Parallel was to call ensureOpen() on the subReader, too when requesting fields(). This works fine until you wrap two times: ParallelAtomicReader(SlowCompositeReaderWrapper(StandardDirectoryReader(segments_1:3:nrt _0(4.0):C42)))

One solution would be to make ensureOpen also check all subreaders, but that would do the volatile checks way too often (with n is the total number of subreaders and m is the number of hierarchical levels this is n^m) - we cannot do this. Currently we only have n*m which is fine.

The proposal how to solve this (closing subreaders under the hood of parent readers is to use the readerClosedListeners. Whenever a composite or slow reader wraps another readers, it registers itself as interested in readerClosed events. When a subreader is then forcefully closed (e.g by a programming error or this crazy test), we automatically close the parents, too.

We should also fix this in 3.x, if we have similar problems there (needs investigation)."
1,"[PATCH] BooleanScorer2 ArrayIndexOutOfBoundsException + alternative NearSpansFrom Erik's post at java-dev: 
 
>      [java] Caused by: java.lang.ArrayIndexOutOfBoundsException: 4 
>      [java]     at org.apache.lucene.search.BooleanScorer2  
> $Coordinator.coordFactor(BooleanScorer2.java:54) 
>      [java]     at org.apache.lucene.search.BooleanScorer2.score  
> (BooleanScorer2.java:292) 
... 
 
and my answer: 
 
Probably nrMatchers is increased too often in score() by calling score() 
more than once."
1,"2.4.x index cannot be opened with 2.9-devSorry for the lack of proper testcase.

In 2.4.1, if you created an index with the (stupid) options below, then it will not create a .prx file. 2.9 expects this file and will not open the index.
The reason i used these stupid options is because i changed the field from indexed=yes to indexed=no, but forgot to remove the .setOmitTf()

{code}
public class Testcase {
	public static void main(String args[]) throws Exception {
		/* run this part with lucene 2.4.1 */
		IndexWriter iw = new IndexWriter(""test"", new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
		iw.setUseCompoundFile(false);
		Document doc = new Document();
		Field field1 = new Field(""field1"", ""foo"", Field.Store.YES, Field.Index.NO);
		field1.setOmitTf(true); // 2.9 will create a 0-byte .prx file, but 2.4.x will NOT. This is the problem. 2.9 expects this file!
		doc.add(field1);
		iw.addDocument(doc);
		iw.close(); 
		/* run this with lucene 2.9 */
		IndexReader ir = IndexReader.open(FSDirectory.getDirectory(""test""), true); 
	}
}
{code}"
0,"Improve maven artifactsThere are a couple of things we can improve for the next release:
- ""*pom.xml"" files should be renamed to ""*pom.xml.template""
- artifacts ""lucene-parent"" should extend ""apache-parent""
- add source jars as artifacts
- update <generate-maven-artifacts> task to work with latest version of maven-ant-tasks.jar
- metadata filenames should not contain ""local"""
0,"LuceneTestCase.afterClass does not print enough information if a temp-test-dir fails to deleteI've hit an exception from LTC.afterClass when _TestUtil.rmDir failed (on write.lock, as if some test did not release resources). However, I had no idea which test caused that (i.e. opened the temp directory and did not release resources).

I think we should do the following:
* Track in LTC a map from dirName -> StackTraceElement
* In afterClass if _TestUtil.rmDir fails, print the STE of that particular dir, so we know where was this directory created from
* Make tempDirs private and create accessor method, so that we control the inserts to this map (today the Set is updated by LTC, _TestUtils and TestBackwards !)"
1,"weird error when adding a node using an abstract/mixin nodetypewhen trying to add a node ""files"" with an abstract nodetype, i.e. nt:base, the following error is reported:

javax.jcr.nodetype.ConstraintViolationException: {}files is abstract  be used as primary node type.

the correct wording could be:

javax.jcr.nodetype.ConstraintViolationException: not allowed to add node {}files: {http://www.jcp.org/jcr/nt/1.0}base is abstract and cannot be used as primary node type.
"
0,Add settings to IWC to optimize IDV indices for CPU or RAM respectivlyspinnoff from LUCENE-3496 - we are seeing much better performance if required bits for PackedInts are rounded up to a 8/16/32/64. We should add this option to IWC and default to round up ie. more RAM & faster lookups.
1,"Typo in the deploy/jboss/4.x/jcr-ds.xml file
The datasource descriptor in the jboss 4.x example xml file has a type in this line:

<config-property name=""bindSessionToTrasaction"" type=""java.lang.Boolean"">true</config-property>

bindSessionToTrasaction ought to be bindSessionToTransaction - there is an 'n' missing from Transaction.

Found this on the tagged release in the subversion repo."
0,"Pass a context struct to Weight#scorer instead of naked booleansWeight#scorer(AtomicReaderContext, boolean, boolean) is hard to extend if another boolean like ""needsScoring"" or similar flags / information need to be passed to Scorers. An immutable struct would make such an extension trivial / way easier. "
0,"Authentication does not respond to stale nonceWhen using digest authentication, HTTP allows the server to mark the nonce value
as stale. The client then must re-authenticate with a new nonce value provided
by the server. Currently, HttpClient does not support this functionality. I've
created a patch that allows HttpClient to support stale nonce values. It is
attached below. The patch should be applied to HttpMethodBase.java


***
/home/scohen/downloads/httpclient-src/commons-httpclient-2.0-rc1/src/java/org/apache/commons/httpclient/HttpMethodBase.java
2003-07-31 22:15:26.000000000 -0400
--- org/apache/commons/httpclient/HttpMethodBase.java   2003-08-20
17:22:52.000000000 -0400
***************
*** 1351,1384 ****
       *
       * @throws IOException when errors occur reading or writing to/from the
       *         connection
       * @throws HttpException when a recoverable error occurs
       */
!     protected void addAuthorizationRequestHeader(HttpState state,
!                                                  HttpConnection conn)
!     throws IOException, HttpException {
!         LOG.trace(""enter HttpMethodBase.addAuthorizationRequestHeader(""
!                   + ""HttpState, HttpConnection)"");
   
          // add authorization header, if needed
!         if (getRequestHeader(HttpAuthenticator.WWW_AUTH_RESP) == null) {
!             Header[] challenges = getResponseHeaderGroup().getHeaders(
!                                                HttpAuthenticator.WWW_AUTH);
!             if (challenges.length > 0) {
!                 try {
!                     AuthScheme authscheme =
HttpAuthenticator.selectAuthScheme(challenges);
                      HttpAuthenticator.authenticate(authscheme, this, conn, state);
!                 } catch (HttpException e) {
!                     // log and move on
!                     if (LOG.isErrorEnabled()) {
!                         LOG.error(e.getMessage(), e);
!                     }
                  }
              }
          }
      }
                                                                                
      /**
       * Adds a <tt>Content-Length</tt> or <tt>Transfer-Encoding: Chunked</tt>
       * request header, as long as no <tt>Content-Length</tt> request header
       * already exists.
       *
--- 1351,1391 ----
       *
       * @throws IOException when errors occur reading or writing to/from the
       *         connection
       * @throws HttpException when a recoverable error occurs
       */
!     protected void addAuthorizationRequestHeader(HttpState state,
HttpConnection conn)
!         throws IOException, HttpException {
!         LOG.trace(""enter HttpMethodBase.addAuthorizationRequestHeader("" +
""HttpState, HttpConnection)"");
                                                                                
          // add authorization header, if needed
!
!         Header[] challenges =
getResponseHeaderGroup().getHeaders(HttpAuthenticator.WWW_AUTH);
!         if (challenges.length > 0) {
!
!             try {
!                 AuthScheme authscheme =
HttpAuthenticator.selectAuthScheme(challenges);
!                 if (getRequestHeader(HttpAuthenticator.WWW_AUTH_RESP) == null
!                     || isNonceStale(authscheme) ) {
                      HttpAuthenticator.authenticate(authscheme, this, conn, state);
!                 }
!             } catch (HttpException e) {
!                 // log and move on
!                 if (LOG.isErrorEnabled()) {
!                     LOG.error(e.getMessage(), e);
                  }
              }
          }
      }
                                                                                
+
+     private boolean isNonceStale(AuthScheme authscheme) {
+         return authscheme.getSchemeName().equalsIgnoreCase(""digest"")
+             && ""true"".equalsIgnoreCase(authscheme.getParameter(""stale""));
+     }
+
+
      /**
       * Adds a <tt>Content-Length</tt> or <tt>Transfer-Encoding: Chunked</tt>
       * request header, as long as no <tt>Content-Length</tt> request header
       * already exists.
       *
***************
*** 2419,2430 ****
                  buffer.append(port);
              }
              buffer.append('#');
              buffer.append(authscheme.getID());
              String realm = buffer.toString();
!
              if (realmsUsed.contains(realm)) {
                  if (LOG.isInfoEnabled()) {
                      LOG.info(""Already tried to authenticate to \""""
                               + realm + ""\"" but still receiving ""
                               + statusCode + ""."");
                  }
--- 2426,2442 ----
                  buffer.append(port);
              }
              buffer.append('#');
              buffer.append(authscheme.getID());
              String realm = buffer.toString();
!
!                       // check to see if the server has made our nonce stale.
!                       // if it has, re-auth
              if (realmsUsed.contains(realm)) {
+               if ( isNonceStale(authscheme)) {
+                       return false;
+               }
                  if (LOG.isInfoEnabled()) {
                      LOG.info(""Already tried to authenticate to \""""
                               + realm + ""\"" but still receiving ""
                               + statusCode + ""."");
                  }"
0,Caching client has a class for common headers that was not being used consistently in the codeThe HttpCachingClient has a class called HeaderConstants that contains all the cache interesting headers that are used in the code base.  This class of string constants was not being used consistently in the code base.  The attached patch cleans this up.
0,"Hostname verification:  turn off wildcards when CN is an IP addressHostname verification:   turn off wildcards when CN is an IP address.  This is a further improvement on HTTPCLIENT-613 and HTTPCLIENT-614.

Example - don't allow:
CN=*.114.102.2

I'm thinking of grabbing the substring following the final dot, and running it through ""Integer.parseInt()"".  If the NumberFormatException isn't thrown (so Integer.parseInt() actually worked!), then I'll turn off wildcard matching.  Notice that this won't be a problem with IP6 addresses, since they don't use dots.  It's only a problem with IP4, where the meaning of the dots clashes with dots in domain names.

Note:  when I turn off wildcard matching, I still attempt an exact match with the hostname.  If through some weird mechanism the client is actually able to use a hostname such as ""https://*.114.102.2/"", then they will be okay if that's what the certificate on the server contains."
0,"Spatial Filters not SerializableI am using Lucene in a distributed setup. 

The Filters in the spatial project aren't Serializable even though it inherits it from Filter. Filter is a Serializable class. 

DistanceFilter contains the non-Serializable class WeakHashMap.
CartesianShapeFilter contains the non-Serializable class java.util.logging.Logger
"
1,Several DocsEnum / DocsAndPositionsEnum return wrong docID when next() / advance(int) return NO_MORE_DOCSDuring work on LUCENE-2878 I found some minor problems in PreFlex and Pulsing Codec - they are not returning NO_MORE_DOCS but the last docID instead from DocsEnum#docID() when next() or advance(int) returned NO_MORE_DOCS. The JavaDoc clearly says that it should return NO_MORE_DOCS.
0,"HttpURLConnection wrapperInitial comments from Vincent Massol for this feature:

I am moving Jakarta Cactus from using the JDK HttpURLConnection to
Commons HttpClient. However, I have some public interface that return
HttpURLConnection and I cannot break that contract with Cactus users.

I propose to write a HttpURLConnection wrapper for HttpMethod (I have
actually already written it but I am currently testing it on Cactus and
will make a proper donation once I am sure it works - i.e all the Cactus
tests pass as before ... ).

I attach a preview of it for those interested.

What do you think of including it in HttpClient distribution ?

Thanks
-Vincent"
0,Remove deprecated RangeQuery classesRemove deprecated RangeQuery classes
0,"AdministratorTest.testAdminNodeCollidingWithRandomNode failureI see the following test failure with the latest trunk. It seems to affect also Sébastien as commented in JCR-2389. However, it doesn't break the Hudson build or Angela's checkout.

I'm filing this as a bug and will disable the test for now to be able to cut the 2.0-beta3 release. We can look at this later in more detail.

The detailed failure message is:

-------------------------------------------------------------------------------
Test set: org.apache.jackrabbit.core.security.user.TestAll
-------------------------------------------------------------------------------
Tests run: 144, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 4.078 sec <<< FAILURE!
testAdminNodeCollidingWithRandomNode(org.apache.jackrabbit.core.security.user.AdministratorTest)  Time elapsed: 0.072 sec  <<< FAILURE!
junit.framework.AssertionFailedError
        at junit.framework.Assert.fail(Assert.java:47)
        at junit.framework.Assert.assertTrue(Assert.java:20)
        at junit.framework.Assert.assertFalse(Assert.java:34)
        at junit.framework.Assert.assertFalse(Assert.java:41)
        at org.apache.jackrabbit.core.security.user.AdministratorTest.testAdminNodeCollidingWithRandomNode(AdministratorTest.java:205)
"
0,"Clone supportIt would be nice to have a clone method for some of the classes that don't have getters & setters exposed for all of their fields. Where relevant, the clone method could be in the interface, so that it doesn't matter which implementing class is being used. The main interfaces that I would like to clone are HttpRequest and Cookie. I know that HttpRequest is technically part of HttpCore, but the primary implementations of it are in HttpClient, so I thought I would post it here. 

Thanks,
David Byrne"
1,jcr2spi: wrong status change upon conflicting removal (CacheBehaviour.OBSERVATION)with CacheBehaviour.OBSERVATION the external removal of a transiently removed item results in wrong status change that brings it back to life.
0,"jackrabbit JCA pom.xmldo not see a way to add attachments, so here it is below inline.
Note, need to move the src/rar/META-INF/ra.xml to src/main/rar/META-INF/ra.xml (which is the default location with maven rar packager).
==========================================
<?xml version=""1.0"" encoding=""UTF-8""?>

<!--
   Licensed to the Apache Software Foundation (ASF) under one or more
   contributor license agreements.  See the NOTICE file distributed with
   this work for additional information regarding copyright ownership.
   The ASF licenses this file to You under the Apache License, Version 2.0
   (the ""License""); you may not use this file except in compliance with
   the License.  You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an ""AS IS"" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
  -->

<project xmlns=""http://maven.apache.org/POM/4.0.0""
         xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0
                             http://maven.apache.org/maven-v4_0_0.xsd "">
  <modelVersion>4.0.0</modelVersion>

<!-- ====================================================================== -->
<!-- P R O J E C T  D E S C R I P T I O N                                   -->
<!-- ====================================================================== -->
  <groupId>org.apache.jackrabbit</groupId>
  <artifactId>jackrabbit-jca</artifactId>
  <packaging>rar</packaging>
  <name>Jackrabbit JCA</name>
  <version>1.1-SNAPSHOT</version>
  <!--
    Keep the description on a single line. Otherwise Maven might generate
    a corrupted MANIFEST.MF (see http://jira.codehaus.org/browse/MJAR-4)
   -->
  <description>
A resource adapter for Jackrabbit as specified by JCA 1.0.
</description>
  <url>http://jackrabbit.apache.org/</url>
  <prerequisites>
    <maven>2.0</maven>
  </prerequisites>
  <issueManagement>
    <system>Jira</system>
    <url>http://issues.apache.org/jira/browse/JCR</url>
  </issueManagement>
  <inceptionYear>2005</inceptionYear>




<!-- ====================================================================== -->
<!-- M A I L I N G   L I S T S                                              -->
<!-- ====================================================================== -->
  <mailingLists>
    <mailingList>
      <name>Jackrabbit Announce List</name>
      <subscribe>announce-subscribe@jackrabbit.apache.org</subscribe>
      <unsubscribe>announce-unsubscribe@jackrabbit.apache.org</unsubscribe>
      <archive>http://mail-archives.apache.org/mod_mbox/jackrabbit-announce/</archive>
    </mailingList>
    <mailingList>
      <name>Jackrabbit Users List</name>
      <subscribe>users-subscribe@jackrabbit.apache.org</subscribe>
      <unsubscribe>users-unsubscribe@jackrabbit.apache.org</unsubscribe>
      <post>users at jackrabbit.apache.org</post>
      <archive>http://mail-archives.apache.org/mod_mbox/jackrabbit-users/</archive>
      <otherArchives>
        <otherArchive>
          http://dir.gmane.org/gmane.comp.apache.jackrabbit.user
        </otherArchive>
        <otherArchive>
          http://www.mail-archive.com/users@jackrabbit.apache.org/
        </otherArchive>
      </otherArchives>
    </mailingList>
    <mailingList>
      <name>Jackrabbit Development List</name>
      <subscribe>dev-subscribe@jackrabbit.apache.org</subscribe>
      <unsubscribe>dev-unsubscribe@jackrabbit.apache.org</unsubscribe>
      <post>dev at jackrabbit.apache.org</post>
      <archive>http://mail-archives.apache.org/mod_mbox/jackrabbit-dev/</archive>
      <otherArchives>
        <otherArchive>
          http://dir.gmane.org/gmane.comp.apache.jackrabbit.devel
        </otherArchive>
        <otherArchive>
          http://www.mail-archive.com/dev@jackrabbit.apache.org/
        </otherArchive>
        <otherArchive>
          http://www.mail-archive.com/jackrabbit-dev@incubator.apache.org/
        </otherArchive>
      </otherArchives>
    </mailingList>
    <mailingList>
      <name>Jackrabbit Source Control List</name>
      <subscribe>commits-subscribe@jackrabbit.apache.org</subscribe>
      <unsubscribe>commits-unsubscribe@jackrabbit.apache.org</unsubscribe>
      <archive>http://mail-archives.apache.org/mod_mbox/jackrabbit-commits/</archive>
    </mailingList>
  </mailingLists>


  <licenses>
    <license>
      <name>The Apache Software License, Version 2.0</name>
      <url>http://www.apache.org/licenses/LICENSE-2.0</url>
      <distribution>repo</distribution>
    </license>
  </licenses>
  <scm>
    <connection>scm:svn:http://svn.apache.org/repos/asf/jackrabbit/trunk/jca</connection>
    <developerConnection>scm:svn:https://svn.apache.org/repos/asf/jackrabbit/trunk/jca</developerConnection>
    <url>http://svn.apache.org/viewvc/jackrabbit/trunk/jca</url>
  </scm>
  <organization>
    <name>The Apache Software Foundation</name>
    <url>http://www.apache.org/</url>
  </organization>
  <build>
    <resources>
      <resource>
        <directory>src/java</directory>
      </resource>
    </resources>
<!-- 
   <testResources>
      <testResource>
        <directory>applications/test</directory>
        <includes>
          <include>*.properties</include>
          <include>*.xml</include>
        </includes>
      </testResource>
      <testResource>
        <directory>src/test/java</directory>
        <includes>
          <include>**/*.xml</include>
          <include>**/*.txt</include>
        </includes>
      </testResource>
    </testResources>
-->
    <plugins>
      <plugin>
        <artifactId>maven-compiler-plugin</artifactId>
        <configuration>
          <target>1.4</target>
          <source>1.4</source>
        </configuration>
      </plugin>
<!-- 
      <plugin>
        <artifactId>maven-surefire-plugin</artifactId>
        <configuration>
          <excludes>
            <exclude>**/init/*</exclude>
          </excludes>
          <includes>
            <include>**/*TestAll.java</include>
          </includes>
          <forkMode>once</forkMode>
          <argLine>-Xmx128m -enableassertions</argLine>
          <systemProperties>
            <property>
              <name>derby.system.durability</name>
              <value>test</value>
            </property>
            <property>
              <name>known.issues</name>
              <value>org.apache.jackrabbit.core.xml.DocumentViewTest#testMultiValue org.apache.jackrabbit.value.BinaryValueTest#testBinaryValueEquals</value>
            </property>
          </systemProperties>          
        </configuration>
      </plugin>
-->
    </plugins>
  </build>

  <dependencies>
  
  	<dependency>
	    <groupId>org.apache.jackrabbit</groupId>
	    <artifactId>jackrabbit-core</artifactId>
	    <version>1.1-SNAPSHOT</version>
	</dependency>
    <dependency>
      <groupId>concurrent</groupId>
      <artifactId>concurrent</artifactId>
      <version>1.3.4</version>
    </dependency>
    <dependency>
      <groupId>commons-collections</groupId>
      <artifactId>commons-collections</artifactId>
      <version>3.1</version>
    </dependency>
    <dependency>
      <groupId>org.apache.geronimo.specs</groupId>
      <artifactId>geronimo-jta_1.0.1B_spec</artifactId>
      <version>1.0.1</version>
    </dependency>
    <dependency>
      <groupId>javax.jcr</groupId>
      <artifactId>jcr</artifactId>
      <version>1.0</version>
    </dependency>
    <dependency>
      <groupId>log4j</groupId>
      <artifactId>log4j</artifactId>
      <version>1.2.8</version>
    </dependency>
    <dependency>
      <groupId>org.slf4j</groupId>
      <artifactId>slf4j-log4j12</artifactId>
      <version>1.0</version>
    </dependency>
    <dependency>
      <groupId>lucene</groupId>
      <artifactId>lucene</artifactId>
      <version>1.4.3</version>
    </dependency>
    <dependency>
      <groupId>org.apache.derby</groupId>
      <artifactId>derby</artifactId>
      <version>10.1.1.0</version>
      <optional>true</optional>
    </dependency>
    <dependency>
      <groupId>junit</groupId>
      <artifactId>junit</artifactId>
      <version>3.8.1</version>
      <scope>test</scope>
    </dependency>
  </dependencies>

  <distributionManagement>
    <repository>
      <id>apache.releases</id>
      <name>Apache Repository for PMC approved releases</name>
      <url>scp://people.apache.org/www/www.apache.org/dist/maven-repository/</url>
    </repository>
    <snapshotRepository>
      <id>apache.snapshots</id>
      <name>Apache Development Repository</name>
      <url>scp://people.apache.org/www/cvs.apache.org/maven-snapshot-repository</url>
    </snapshotRepository>
    <site>
      <id>website</id>
      <url>scp://people.apache.org/www/jackrabbit.apache.org/</url>
    </site>
  </distributionManagement>

</project>"
0,"Handle conditional requests in cacheReturn 304 if incoming request has ""If-None-Match"" or ""If-Modified-Since"" headers and can be served from cache.  Currently we return a 200 which is correct but not optimal."
0,"Optimize the core tokenizers/analyzers & deprecate Token.termTextThere is some ""low hanging fruit"" for optimizing the core tokenizers
and analyzers:

  - Re-use a single Token instance during indexing instead of creating
    a new one for every term.  To do this, I added a new method ""Token
    next(Token result)"" (Doron's suggestion) which means TokenStream
    may use the ""Token result"" as the returned Token, but is not
    required to (ie, can still return an entirely different Token if
    that is more convenient).  I added default implementations for
    both next() methods in TokenStream.java so that a TokenStream can
    choose to implement only one of the next() methods.

  - Use ""char[] termBuffer"" in Token instead of the ""String
    termText"".

    Token now maintains a char[] termBuffer for holding the term's
    text.  Tokenizers & filters should retrieve this buffer and
    directly alter it to put the term text in or change the term
    text.

    I only deprecated the termText() method.  I still allow the ctors
    that pass in String termText, as well as setTermText(String), but
    added a NOTE about performance cost of using these methods.  I
    think it's OK to keep these as convenience methods?

    After the next release, when we can remove the deprecated API, we
    should clean up Token.java to no longer maintain ""either String or
    char[]"" (and the initTermBuffer() private method) and always use
    the char[] termBuffer instead.

  - Re-use TokenStream instances across Fields & Documents instead of
    creating a new one for each doc.  To do this I added an optional
    ""reusableTokenStream(...)"" to Analyzer which just defaults to
    calling tokenStream(...), and then I implemented this for the core
    analyzers.

I'm using the patch from LUCENE-967 for benchmarking just
tokenization.

The changes above give 21% speedup (742 seconds -> 585 seconds) for
LowerCaseTokenizer -> StopFilter -> PorterStemFilter chain, tokenizing
all of Wikipedia, on JDK 1.6 -server -Xmx1024M, Debian Linux, RAID 5
IO system (best of 2 runs).

If I pre-break Wikipedia docs into 100 token docs then it's 37% faster
(1236 sec -> 774 sec), I think because of re-using TokenStreams across
docs.

I'm just running with this alg and recording the elapsed time:

  analyzer=org.apache.lucene.analysis.LowercaseStopPorterAnalyzer
  doc.tokenize.log.step=50000
  docs.file=/lucene/wikifull.txt
  doc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker
  doc.tokenized=true
  doc.maker.forever=false

  {ReadTokens > : *

See this thread for discussion leading up to this:

  http://www.gossamer-threads.com/lists/lucene/java-dev/51283

I also fixed Token.toString() to work correctly when termBuffer is
used (and added unit test).
"
0,"Upgrade commons-codec 1.4 -> 1.6commons-codec 1.4 is buggy, see for example https://issues.apache.org/jira/browse/CODEC-99"
0,"use reusable collation keys in ICUCollationFilterICUCollationFilter need not create a new CollationKey object for each token.
In ICU there is a mechanism to use a reusable key.
"
1,"NullPointerException in ServerRowA NullPointerException occurs in ServerRow.getValues() when the underlying Value array contains a null reference. See http://www.nabble.com/exception-after-calling-webdav-search-command-tf2826750.html for the details.

java.lang.NullPointerException
     at org.apache.jackrabbit.rmi.value.StatefulValueAdapter.getType(StatefulValueAdapter.java:98)
     at org.apache.jackrabbit.rmi.value.SerialValue.<init>(SerialValue.java:65)
     at org.apache.jackrabbit.rmi.value.SerialValueFactory.makeSerialValue(SerialValueFactory.java:100)
     at org.apache.jackrabbit.rmi.value.SerialValueFactory.makeSerialValueArray(SerialValueFactory.java:77)
     at org.apache.jackrabbit.rmi.server.ServerRow.getValues(ServerRow.java:58)

The best solution would be to explicitly handle nulls in SerialValueFactory.makeSerialValue()."
1,"when you clone or reopen an IndexReader with pending changes, the new reader doesn't commit the changesWhile working on LUCENE-1647, I came across this issue... we are failing to carry over hasChanges, norms/deletionsDirty, etc, when cloning the new reader."
0,Benchmark: Improve transparency of test resultsas discussed in JCR-1501.
1,"IndexReader.setTermInfosIndexDivisor doesn't carry over to reopened readersWhen you reopen a reader, some segments are shared (and thus properly inherit the index divisor) but others are newly opened and use the default index divisor.  You then have no way to change the index divisor of those newly opened ones.  The only workaround is to not use reopen (always open a new reader).

I'd like to make termInfosDivisor an up-front param to IndexReader, anyway, for LUCENE-1609, so likely I'll fix both of these issues there."
1,"Bug with textfilters and classloadersI'm having problems with text filter service. I built the contrib/textfilters package and I included the resulting jackrabbit-textfilters-1.0-SNAPSHOT.jar in my application classpath. The problem is that TextFilterService class is unable to find any filters, even though that a services/org...TextFilterService file is wihin the META-INF jar's directory.

I think that this must to be with Eclipse RCP classloader mechanism, but the fact is that it does not work. I find a little bit strange this way to load services, and as you can see, it seems problematic in some scenarios.

----

Marcel Reutegger 	
<marcel.reutegger@gmx.net> to jackrabbit-dev
	 More options	  11:01 am (55 minutes ago)
Hi Martin,

we had a similar problem with the query languages, but I solved that one
by telling the registry to use a specific classloader. this seemed to work.
I'm not sure this will also work for the text filters, because the jar
file might be in another classloader.

could you please post a jira bug? I'll then change the discovery
mechanism to use good old xml config ;)
"
0,"Remove deprecated methods in BooleanQueryRemove deprecated methods setUseScorer14 and getUseScorer14 in BooleanQuery, and adapt javadocs."
0,"implement reusableTokenStream for all contrib analyzersmost contrib analyzers do not have an impl for reusableTokenStream

regardless of how expensive the back compat reflection is for indexing speed, I think we should do this to mitigate any performance costs. hey, overall it might even be an improvement!

the back compat code for non-final analyzers is already in place so this is easy money in my opinion."
1,"Possible hidden exception on SegmentInfos commitI am not sure if this is that big of a deal, but I just ran into it and thought I might mention it.

SegmentInfos.commit removes the Segments File if it hits an exception. If it cannot remove the Segments file (because its not there or on Windows something has a hold of it), another Exception is thrown about not being able to delete the Segments file. Because of this, you lose the first exception, which might have useful info, including why the segments file might not be there to delete.

- Mark"
1,HttpClient treats URI fragments in redirect URIs incosistentlyHttpClient treats URI fragments in redirect URIs incosistently. It strips fragments from relative URIs but leaves absolute ones unchanges.  
0,"Persian Arabic Analyzer cleanupWhile browsing through the code I found some places for minor improvements in the new Arabic / Persian Analyzer code. 

- prevent default stopwords from being loaded each time a default constructor is called
- replace if blocks with a single switch
- marking private members final where needed
- changed protected visibility to final in final class.

"
0,Add HTMLStripReader and WordDelimiterFilter from SOLRSOLR has two classes HTMLStripReader and WordDelimiterFilter which are very useful for a wide variety of use cases.  It would be good to place them into core Lucene.
0,make spi query code compatible with JCR 2.0SPI-Commons currently has it's own outdated copy of the new JCR 2.0 query interfaces.
0,"Add log information when node/property type determination failsgetQNodeDefinition() and getQPropertyDefinition() of o.a.j.jcr2spi.nodetype.ItemDefinitionProviderImpl silently ignore errors which might occur on determination of node and property types. Instead these methods use RepositoryService.getNodeDefinition() and RepositoryService.getPropertyDefinition(), respectively to determine the types. This might lead to difficult to track down problems when the RepositoryService call occurs because of an error in the node type definition. I suggest to add logging statements to these methods. "
0,"JSP page compilation errors when depoyed using oc4jAn error in the Welcome.jsp was produced as follows:

cannot find symbol symbol : method log(java.lang.String,java.lang.Throwable)

In a response from the user group it was determined that there should be no expectation in Jackrabbit that the JspPage implementation will inherit from the GeneralServlet base class.
"
0,"add shortcut method to CndImporter which makes it easier to rereigster node typesCndImporter has a two argument registerNodeTypes method which is a nice shortcut, but in order to rereigster node types, you have to use the non-shortcut method. Attached patch adds a three-argument method which provide a shortcut for doing reregistration."
0,"contrib/benchmark config does not play nice with doubles with the flush.by.ram valueIn the o.a.l.benchmark.byTask.utils.Config.java file, the nextRound and various other methods do not handle doubles in the ""round"" property configuration syntax.

To replicate this, copy the micro-standard.alg and replace 
merge.factor=mrg:10:100:10:100
max.buffered=buf:10:10:100:100

with

ram.flush.mb=ram:32:40:48:56

and you will get various ClassCastExceptions in Config (one in newRound() and, when that is fixed, in getColsValuesForValsByRound.

The fix seems to be to just to mirror the handling of int[].

The fix seems relatively minor.  Patch shortly and will plan to commit tomorrow evening."
1,"NodeTypeDefDiff.PropDefDiff.init() constraints change check bugsTwo bugs have been found in NodeTypeDefDiff.PropDefDiff.init() when try to modify property constraints of an already registered node type:

1) according to the java doc it should be possible to remove all constraints from a property, but it is not (marked as a MAJOR change).
 
2) it's allowed (TRIVIAL) to set a constraint to a property that had no constraint at all before, which is wrong, because it could affect the consistency of existing repository content."
0,Database Data Store: support database type 'mssql'MS SQL Server is referred to with the schema name 'mssql' in the persistence managers and the cluster journal. For the DbDataStore it is called 'sqlserver'. This is not consistent.
0,"Revise PagedBytes#fillUsingLengthPrefix* methods namesPagedBytes has 3 different variants of fillUsingLengthPrefix. We need better names for that since CSFBranch already added a 4th one.


here are some suggestions:

{code}
/** Reads length as 1 or 2 byte vInt prefix, starting @ start */
    public BytesRef fillLengthAndOffset(BytesRef b, long start) 
//    was: public BytesRef fillUsingLengthPrefix(BytesRef b, long start) 


 /** @lucene.internal  Reads length as 1 or 2 byte vInt prefix, starting @ start.  Returns the block number of the term. */
    public int getBlockAndFill(BytesRef b, long start) 
//    was: public BytesRef fillUsingLengthPrefix2(BytesRef b, long start) 

/** @lucene.internal  Reads length as 1 or 2 byte vInt prefix, starting @ start. 
     * Returns the start offset of the next part, suitable as start parameter on next call
     * to sequentially read all BytesRefs. */
    public long getNextOffsetAndFill(BytesRef b, long start) 
//    was: public BytesRef fillUsingLengthPrefix3(BytesRef b, long start) 

{code}"
1,"ArrayIndexOutOfBoundsException in HttpStatus.getStatusText(508)Try the following:

    System.out.println(""Status text = "" + HttpStatus.getStatusText(507));
    try {
      System.out.println(""Status text = "" + HttpStatus.getStatusText(508));
    }
    catch (Exception ex) {
      System.err.println(""Exception! msg = "" + ex.getMessage());
      ex.printStackTrace();
    }
    System.out.println(""Status text = "" + HttpStatus.getStatusText(509));

507 -> returns a message as expected
508 -> ArrayIndexOutOfBoundsException
509 -> null as expected"
0,"Do not use deletable anymoreThe query handler implementation currently uses a deletable file to keep track of index segments that are not needed anymore and can be deleted. In general index segments are deleted right away when they are not needed anymore, but it may happen that index readers are still open (because of a time consuming query) and the index segment cannot be deleted at the moment. In this case the index segment name is written to the deletable file and the index periodically tries to delete the segment later.

The implementation should rather infer from the indexes file on startup, which segments are still needed and in use."
0,New method on NodeTypeManagerImpl to reregister nodetypesAdd a method to NodeTypeManagerImpl to allow reregistering of existing nodetypes. The method takes an inputstream in either XML or CND format and registers all new nodetypes and reregisters existing nodetypes.
0,"Some typos in the English Manualin section 2.8.4
Per default this implementation will create no more than than 2 concurrent connections per given route and no more 20 connections in total.
Here are 2 ""than"" in this statement.

in section 3.1
Netscape engineers used to refer to it as as a ""magic cookie"" and the name stuck.
Also, here are 2 ""as"" in the sentence.

in section 5.2 'http.protocol.handle-redirects'
If this parameter is not HttpClient will handle redirects automatically.
here, a ""set"" should be put after not

in section 6.1
In certain situations it may be necessary to customize the way HTTP messages get transmitted across
the wire beyond what is possible possible using HTTP parameters in order to be able to deal nonstandard,
non-compliant behaviours.
here are 2 ""possible""."
1,"SearchManager might throw when handling cluster eventWhen handling events that are generated from another node in the cluster, the SearchManager might try to index an index that does no longer exist. This results in an error message or even a NPE. The scenario looks as follows (A and B are nodes in a repository cluster)

1: A adds node N
2: A saves changes
3: A removes node N
4: A saves changes

Upon receiving the event of a newly created node, B starts indexing node N. If this process hasn't been concluded before step 3 above, it will throw.
"
0,"replacing an extended mixin with it's supertype is problematicnode.addMixin() / node.removeMixin() have some checks to avoid redundant mixin settings on a node and not only when the node is saved.

eg: have 2 mixins: mix:A and mix:AA where mix:AA > mix:A and a node (N with mix:AA) on it.

then, N.addMixin(mix:A)  has no effect, since it's regarded as redundant.  so you have to remove mix:AA first and then add mix:A.
there is the first problem when applying mixin types programmatically, just be sure to remove them first before adding new ones.

the 2nd problem occurs when mix:A has a mandatory property. then somehow when downgrading from mix:AA to mix:A, some information is lost, and a save call results in

Unable to save node 'N': javax.jcr.nodetype.ConstraintViolationException: /test/A: mandatory property {}prop does not exist.
you need to ""touch"" the property, otherwise it will not work.

so only this works:

N.removeMixin(""mix:AA"");
N.addMixin(""mix:A"");
N.setProperty(""prop"", N.getProperty(""prop"").getValue());
session.save();



"
1,"Exception handling in HttpClient requires redesignWhen I use httpclient2.0-alpha3 and setTimeout(60000), after the specified 
time, I would like to see InterruptedIOException thrown, but I got 
HttpRecoverableException instead, which is pretty general. I would like to see 
the original exception. Thanks"
1,"hrefs in dav responses should be url-escapedthe url in an href element of a dav response should be url-escaped. currently at least one webdav client (os x's webdavfs) chokes on unescaped urls (such as /home/bcm/file with spaces in its name.txt).
"
0,"improve test coverage of multi-segment indicesSimple patch that adds a test-only helper class, RandomIndexWriter, that lets you add docs, but it will randomly do things like use a different merge policy/scheduler, flush by doc count instead of RAM, flush randomly (so we get multi-segment indices) but also randomly optimize in the end (so we also sometimes test single segment indices)."
0,"Make the extraction of Session UserIDs from Subjects configurableThe SessionImpl class must extract a string name from the Prinicpals in a Subject to use as the Session userID.  In 1.4 the SessionImpl class directly selects the first available Principal.  In 1.5, this is delegated to the SecurityManager, which chooses the first  non-group principal.

It would be useful to be able to configure specific selection criteria for the Principal used for the Session userID.  A simple mechanism would involve specifying a Principal implementation classname in the configuration, and the first instance of that class found in the Subject would be used for the userID.  One way to implement this in 1.4 would be to extend AuthContext to include a method getSessionPrincipal() which encapsulates the selection logic, and adding an option the LoginModuleConfig to specify the class name of the Principal to select.

A particular use case is using the LDAP LoginModule from Sun JDK 6 with the repository.  The first Principal LdapLoginModule populates into the Subject is an instance of LdapPrincipal, which renders the userID as the full DN of the user.  The LoginModule also adds an instance of UserPrincipal, whose name is the simple username/uid attribute, which would be more appropriate as the Session userId since it corresponds to the username provided by the user to application authentication mechanisms (the provided username is expanded into the full DN prior to authentication by the login module).  If the above configuration mechanism were available, one could configure the LdapLoginModule, and specify that the userID be extracted from the first instance of com.sun.security.auth.UserPrincipal.  Since rewriting LoginModules is not always possible or desirable, this change would enable the stable integration of 3rd-party login modules that may populate the Subject with several principals."
0,"Performance improvement for merging stored, compressed fieldsHello everyone,

currently the merging of stored, compressed fields is not optimal for the following reason: every time a stored, compressed field is being merged, the FieldsReader uncompresses the data, hence the FieldsWriter has to compress it again when it writes the merged fields data (.fdt) file. The uncompress/compress step is unneccessary and slows down the merge performance significantly.

This patch improves the merge performance by avoiding the uncompress/compress step. In the following I give an overview of the changes I made:
   * Added a new FieldSelectorResult constant named ""LOAD_FOR_MERGE"" to org.apache.lucene.document.FieldSelectorResult
   * SegmentMerger now uses an FieldSelector to get stored fields from the FieldsReader. This FieldSelector's accept() method returns the FieldSelectorResult ""LOAD_FOR_MERGE"" for every field.
   * Added a new inner class to FieldsReader named ""FieldForMerge"", which extends  org.apache.lucene.document.AbstractField. This class holds the field properties and its data. If a field has the FieldSelectorResult ""LOAD_FOR_MERGE"", then the FieldsReader creates an instance of ""FieldForMerge"" and does not uncompress the field's data.
   * FieldsWriter checks if the field it is about to write is an instanceof FieldsReader.FieldForMerge. If true, then it does not compress the field data.


To test the performance I index about 350,000 text files and store the raw text in a stored, compressed field in the lucene index. I use a merge factor of 10. The final index has a size of 366MB. After building the index, I optimize it to measure the pure merge performance.

Here are the performance results:

old version:
   * Time for Indexing:  36.7 minutes
   * Time for Optimizing: 4.6 minutes

patched version:
   * Time for Indexing:  20.8 minutes
   * Time for Optimizing: 0.5 minutes

The results show that the index build time improved by about 43%, and the optimizing step is more than 8x faster. 

A diff of the final indexes (old and patched version) shows, that they are identical. Furthermore, all junit testcases succeeded with the patched version. 

Regards,
  Michael Busch"
0,"TestIndexWriter.testOptimizeTempSpaceUsage fails w/ SimpleText codec{noformat}
   [junit] Testsuite: org.apache.lucene.index.TestIndexWriter
   [junit] Testcase: testOptimizeTempSpaceUsage(org.apache.lucene.index.TestIndexWriter):      FAILED
   [junit] optimized used too much temporary space: starting usage was 117867 bytes; max temp usage was 363474 but should have been 353601 (= 3X starting usage)
   [junit] junit.framework.AssertionFailedError: optimized used too much temporary space: starting usage was 117867 bytes; max temp usage was 363474 but should have been 353601 (= 3X starting usage)
   [junit]     at org.apache.lucene.index.TestIndexWriter.testOptimizeTempSpaceUsage(TestIndexWriter.java:662)
   [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)
   [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)
   [junit]
   [junit]
   [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 5.284 sec
   [junit]
   [junit] ------------- Standard Output ---------------
   [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testOptimizeTempSpaceUsage -Dtests.seed=-3299990090561349208:2824386407253661541
   [junit] NOTE: test params are: codec=SimpleText, locale=el_GR, timezone=Africa/Dar_es_Salaam
{noformat}

It's not just SimpleText (because -Dtests.codec=SimpleText, alone, sometimes passes)... there must be something else about the RIWC settings.
"
1,Fix exception handling and thread safety in realtime branchSeveral tests are currently failing in the realtime branch - most of them due to thread safety problems (often exceptions in ConcurrentMergeScheduler) and in tests that test for aborting and non-aborting exceptions.
0,"Hudson build doesn't detect Java 5 class referencesDue to the fact that the Maven 2 support in Hudson only works on Java 5, our current CI build at http://hudson.zones.apache.org/hudson/job/Jackrabbit-trunk/ doesn't detect references to classes and methods that are only available in the Java 5 class library."
0,"Rename contrib/queryparser project to queryparser-contribMuch like with contrib/queries, we should differentiate the contrib/queryparser from the queryparser module.  No directory structure changes will be made, just ant and maven."
0,"Support more queries (other than just title) in Trec quality pkgNow that we can properly parse descriptions and narratives from TREC queries (LUCENE-2210), it would be nice to allow the user to easily run quality evaluations on more than just ""Title""

This patch adds an optional commandline argument to QueryDriver (the default is Title as before), where you can specify something like:
T: Title-only
D: Description-only
N: Narrative-only
TD: Title + Description,
TDN: Title+Description+Narrative,
DN: Description+Narrative

The SimpleQQParser has an additional constructor that simply accepts a String[] of these fields, forming a booleanquery across all of them.
"
1,"disk full can cause index corruption in certain casesRobert uncovered this nasty bug, in adding more randomness to
oal.index tests...

I got a standalone test to show the issue; the corruption path is
as follows:

  * The merge hits an initial exception (eg disk full when merging the
    postings).

  * In handling this exception, IW closes all the sub-readers,
    suppressing any further exceptions.

  * If one of these sub-readers has pending deletions, which happens
    if readers are pooled in IW, it will flush them.  If that flush
    hits a 2nd exception (eg disk full), then SegmentReader
    [incorrectly] leaves the SegmentInfo's delGen advanced by 1,
    referencing a corrupt file, yet the SegmentReader is still
    forcefully closed.

  * If enough disk frees up such that a later IW.commit/close
    succeeds, the resulting segments file will reference an invalid
    deletions file.
"
0,"TCK: AddNodeTest requires implementation to support one-parameter addNode method on test nodeThis test requires a repository to support addNode(String) [one argument] on the test node.  However, JSR-170 does not require an implementation to have at least one child node definition with a default primary type.  For such repositories, this test will fail, regardless of configuration.

Proposal: introduce a configuration property which, if set, causes calls to addNode(String) to be replaced with addNode(String, String)."
0,"JCR2SPI: Move test execution to SPI2JCRproposed patches see  issue JCR-1629

this allows to
- remove dependency to jackrabbit-spi2jcr and jackrabbit-core from jcr2spi
- remove duplicated tests from sandbox/spi"
0,"TextFilters get called three times within checkin() methodIf you want to add a PDF document to a repository using a PdfTextFilter, and you do the following steps:

session.save()
node.checkin();

The method PdfTextFilter.doFilter() gets called 4 times!!!

session's save method calls doFilter one time. This is normal

But checkin method calls doFilter three times. Is this normal? I do not see the sense.

------------------

		
Marcel Reutegger 	
<marcel.reutegger@gmx.net> to jackrabbit-dev
	 More options	  11:43 am (13 minutes ago)
Hi Martin,

this is unfortunate and should be improved. the reason why this happens
is the following:
the search index implementation always indexes a node as a whole to
improve query performance. that means even if a single property changes
the parent node with all its properties is re-indexed.

unfortunately the checkin method sets properties in three separate
'transactions', causing the search to re-index the according node three
times.

usually this is not an issue, because the index implementation keeps a
buffer for pending index work. that is, if you change the same property
several times and save after each setProperty() call, it won't actually
get re-indexed several times. but text filters behave differently here,
because they extract the text even though the text will never be used.

eventually this will improve without any change to the search index
implementation, because as soon as versioning participates properly in
transactions there will only be one call to index a node on checkin().

as a quick fix we could improve the text filter classes to only parse
the binary when the returned reader is acutally used."
0,"OracleFileSystem uses getClass().getResourceAsStream to load schema fileorg.apache.jackrabbit.core.fs.db.OracleFileSystem loads the schema via getClass().getResourceAsStream(...).
This makes it impossible to extend the class without either copying the schema ddl file, or overwriting checkSchema(...),
as the schema file is not accessible.

The solution is to use OracleFilesystem.class.getResourceAsStream(...).

See JCR-595 which fixed this already for org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager."
1,"[PATCH] PerFieldAnalyzerWrapper fails to implement getPositionIncrementGap()The attached patch causes PerFieldAnalyzerWrapper to delegate calls to getPositionIncrementGap() to the analyzer that is appropriate for the field in question.  The current behavior without this patch is to always use the default value from Analyzer, which is a bug because PerFieldAnalyzerWrapper should behave just as if it was the analyzer for the selected field.
"
0,"IP address of the server of a HttpConnectionAFAIK it's not possible to get the IP address of the server of a HttpConnection.

I propose to add a getServerAddress() method to the HttpConnection class that returns the IP address of the server, if the connection has been opened.
And either returns null or throws an Exception if the IP address is not available, i.e. the connection is not open.

Below is a workaround for getting the IP address in current versions.

-----------------------
package org.apache.commons.httpclient;

import java.io.IOException;
import java.net.InetAddress;

public class InetAddressFetcher {
	private HttpConnection hc;

	public InetAddressFetcher(HttpConnection hc) {
		this.hc = hc;
	}

	public InetAddress getInetAddress() throws IOException {
		if (!hc.isOpen()) {
			hc.open();
		}
		return hc.getSocket().getInetAddress();
	}
}"
0,"http.connection-manager.timeout is a LONG not an INTEGERDocumentation is wrong.

Table in Preference Architecture page states http.connection-manager.timeout 
is an Integer.

Doing:

setParameter(""http.connection-manager.timeout"", new Integer(n));

Causes:

java.lang.ClassCastException
	at 
org.apache.commons.httpclient.params.DefaultHttpParams.getLongParameter
(DefaultHttpParams.java:171)
	at 
org.apache.commons.httpclient.params.HttpClientParams.getConnectionManagerTimeo
ut(HttpClientParams.java:143)
	at org.apache.commons.httpclient.HttpMethodDirector.executeMethod
(HttpMethodDirector.java:161)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:437)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:324)"
0,"Improve indexing performance by increasing internal buffer sizesIn working on LUCENE-843, I noticed that two buffer sizes have a
substantial impact on overall indexing performance.

First is BufferedIndexOutput.BUFFER_SIZE (also used by
BufferedIndexInput).  Second is CompoundFileWriter's buffer used to
actually build the compound file.  Both are now 1 KB (1024 bytes).

I ran the same indexing test I'm using for LUCENE-843.  I'm indexing
~5,500 byte plain text docs derived from the Europarl corpus
(English).  I index 200,000 docs with compound file enabled and term
vector positions & offsets stored plus stored fields.  I flush
documents at 16 MB RAM usage, and I set maxBufferedDocs carefully to
not hit LUCENE-845.  The resulting index is 1.7 GB.  The index is not
optimized in the end and I left mergeFactor @ 10.

I ran the tests on a quad-core OS X 10 machine with 4-drive RAID 0 IO
system.

At 1 KB (current Lucene trunk) it takes 622 sec to build the index; if
I increase both buffers to 8 KB it takes 554 sec to build the index,
which is an 11% overall gain!

I will run more tests to see if there is a natural knee in the curve
(buffer size above which we don't really gain much more performance).

I'm guessing we should leave BufferedIndexInput's default BUFFER_SIZE
at 1024, at least for now.  During searching there can be quite a few
of this class instantiated, and likely a larger buffer size for the
freq/prox streams could actually hurt search performance for those
searches that use skipping.

The CompoundFileWriter buffer is created only briefly, so I think we
can use a fairly large (32 KB?) buffer there.  And there should not be
too many BufferedIndexOutputs alive at once so I think a large-ish
buffer (16 KB?) should be OK.
"
0,"apply delete-by-Term and docID immediately to newly flushed segmentsSpinoff from LUCENE-2324.

When we flush deletes today, we keep them as buffered Term/Query/docIDs that need to be deleted.  But, for a newly flushed segment (ie fresh out of the DWPT), this is silly, because during flush we visit all terms and we know their docIDs.  So it's more efficient to apply the deletes (for this one segment) at that time.

We still must buffer deletes for all prior segments, but these deletes don't need to map to a docIDUpto anymore; ie we just need a Set.

This issue should wait until LUCENE-1076 is in since that issue cuts over buffered deletes to a transactional stream."
0,add missing name constants for mix:title
0,"connection wrapper prevents GC of TSCCMEven if a connection is released back to the ThreadSafeClientConnManager, a hard reference to the connection wrapper will prevent GC of the TSCCM.
Make sure the connection wrapper is properly detached on release. Then update TestTSCCMWithServer.testConnectionManagerGC() accordingly. 
"
1,"DirectIOLinuxDirectory hardwires buffer size and creates files with invalid permissionsTestDemo fails if I use the DirectIOLinuxDirectory (using Robert's new -Dtests.directory=XXX), because when it O_CREATs a file, it fails to specify the mode, so [depending on C stack!] you can get permission denied.

Also, we currently hardwire the buffer size to 1 MB (Mark found this)... I plan to add a ""forcedBufferSize"" to the DirectIOLinuxDir's ctor, to optionally override lucene's default buffer sizes (which are way too small for direct IO to get barely OK performance).  If you pass 0 for this then you get Lucene's default buffer sizes..."
0,"Support contains queries with wildcard prefixThe current implementation only allows wildcards in the middle or at the end of a search term. 

The following queries work right now:
//*[jcr:contains(., 'foo*')
//*[jcr:contains(., 'fo?o')

But the following does not:
//*[jcr:contains(., '*bar')

There was already a thread in the mailing list on this topic:
http://thread.gmane.org/gmane.comp.apache.jackrabbit.devel/3304"
0,"orm-persistence package doesn't compile against cvs head.Corrected QName import (which I guess was moved). 
This patch has no meaning besides making the src/java compile and to update the dependencies in the project.xml.
"
1,"Term's equals() throws ClassCastException if passed something other than a TermTerm.equals(Object) does a cast to Term without checking if the other object is a Term.

It's unlikely that this would ever crop up but it violates the implied contract of Object.equals()."
0,"WebDAV: Allow for Extensions of MimeResolver in the Configuration.Currently mime type detection is done using the content type header or (if missing) using a static MimeResolver instance in 
the IOUtil class. The MimeResolver itself reads from a properties file, that obviously does not list all possible extensions and
mimetypes.

This could be improved by:

- extending the resource configuration.
- extend the ImportContext and ExportContext interfaces
- replacing the current usages of IOUti#MIMERESOLVER by the corresponding calls on the Context classes which 
  themselves get a MimeResolver that is retrieved from the resource configuration."
0,"remove unused benchmark dependenciesBenchmark has a huge number of jar files in its lib/ (some of which even have different versions than the same libs used in e.g. solr)

But the worst thing is, most of these it doesn't even use.
* commons-collection: unused
* commons-beanutils: unused
* commons-logging: unused
* commons-digetser: unused

"
1,"VersionHistory.removeVersion() does not throw ReferentialIntegrityExceptionInside an XATransaction immediately removing a version that was created by a checkin succeeds, even though it should fail because referential integrity is violated. The reason seems to be that the created version does not return any references.

In the end the transaction fails because referential integrity is checked again in the SharedItemStateManager, which is correct. But IMO removeVersion() should fail first.

Added test case: org.apache.jackrabbit.core.version.CheckinRemoveVersionTest"
1,"QueryNodeImpl throws ConcurrentModificationException on add(List<QueryNode>)on adding a List of children to a QueryNodeImplemention a ConcurrentModificationException is thrown.
This is due to the fact that QueryNodeImpl instead of iteration over the supplied list, iterates over its internal clauses List.

Patch:
Index: QueryNodeImpl.java
===================================================================
--- QueryNodeImpl.java    (revision 911642)
+++ QueryNodeImpl.java    (working copy)
@@ -74,7 +74,7 @@
           .getLocalizedMessage(QueryParserMessages.NODE_ACTION_NOT_SUPPORTED));
     }
 
-    for (QueryNode child : getChildren()) {
+    for (QueryNode child : children) {
       add(child);
     }
 "
0,"URI.parseUriReference treats strings with leading ':' as absolute URIs with zero-length schemeURI.parseUriReference treats strings with leading ':' as absolute URIs with a
zero-length scheme. If you then try to derelativize such a URI against a base
URI, you just get the same URI with leading ':'. 

IE and Firefox treat URI strings with a leading ':' as relative URIs. For
example, an HREF of "":foo"" in the context of base URI
""http://www.example.com/path/page"" would derelativize as
""http://www.example.com/path/:foo"". (Only if another character comes before the
colon is it interpreted as a URI scheme.)

It'd be desirable for HTTPClient URI to do the same thing.

Example code to demonstrate:

import org.apache.commons.httpclient.URI;
URI base = new URI(""http://www.example.com/path/page"");
URI rel1 = new URI("":foo/boo"");
System.out.println((new URI(base,rel1)).toString()); // displays just "":foo""

A potential fix would be for URI.parseUriReference() to avoid interpreting a ':'
in the zero position as indicating a zero-length scheme:

-       if (atColon < 0 || (atSlash >= 0 && atSlash < atColon)) {
+       if (atColon <= 0 || (atSlash >= 0 && atSlash < atColon)) {

and

-        if (at < length && tmp.charAt(at) == ':') {
+        if (at > 0 && at < length && tmp.charAt(at) == ':') {"
0,"Rethrow exception with cause in BundleDbPersistenceManagerAn exception forwarded from SQL should have a cause for better diagnosis.

Index: jackrabbit-core/src/main/java/org/apache/jackrabbit/core/persistence/bundle/BundleDbPersistenceManager.java
===================================================================
--- jackrabbit-core/src/main/java/org/apache/jackrabbit/core/persistence/bundle/BundleDbPersistenceManager.java (revision 585555)
+++ jackrabbit-core/src/main/java/org/apache/jackrabbit/core/persistence/bundle/BundleDbPersistenceManager.java (working copy)
@@ -604,7 +604,7 @@
             }
             return nameIndex;
         } catch (Exception e) {
-            throw new IllegalStateException(""Unable to create nsIndex: "" + e);
+            throw new IllegalStateException(""Unable to create nsIndex"", e);
         }
     }
 "
0,"Create ScoreNode on demand in SortedLuceneQueryHitsScoreNodes are current created for the full result fetch. Instead, the ScoreNodes should be created on demand when requested in nextScoreNode()."
0,javacc ant task for contrib/misc precedence query parserAdd a javacc task in contrib/misc for the precedence query parser.
0,"Fix StandardAnalyzer to not mis-identify HOST as ACRONYM by defaultComing out of the discussion around back compatibility, it seems best to default StandardAnalyzer to properly fix LUCENE-1068, while preserving the ability to get the back-compatible behavior in the rare event that it's desired.

This just means changing the replaceInvalidAcronym = false with = true, and, adding a clear entry to CHANGES.txt that this very slight non back compatible change took place.

Spinoff from here:

    http://www.gossamer-threads.com/lists/lucene/java-dev/57517#57517

I'll commit that change in a day or two."
0,"Provide an convenience AttributeFactory that implements all default attributes with TokenI found some places in contrib tests, where the Token.class was added using addAttributeImpl(). The problem here is, that you cannot be sure, that the attribute is really added and you may fail later (because you only update your local instance). The tests in contrib will partially fail with 3.0 without backwards layer (because the backwards layer uses Token/TokenWrapper internally and copyTo() will work.

The correct way to achieve this is using an AttributeFactory. The AttributeFactory is currently private in SingleTokenTokenStream. I want to move it to Token.java as a static class / static member. In this case the tests can be rewritten.

I also want to mark addAttributeImpl() as EXPERT, because you must really know whats happening and what are the traps."
1,"fix more position corrumptions in 4.0 codecsSpinoff of LUCENE-3876.

Some codecs have invalid asserts, wrong shift operators etc.

If a position exceeds Integer.MAX_VALUE/2 and then also has a payload,
it will produce corrumpt indexes or other strange errors.

Easiest way to trigger the bugs is to sometimes add a payload to the test from LUCENE-3876."
0,"Use Iterable<? extends UrlEncodedFormEntity> instead of List<? extends UrlEncodedFormEntity> in URLEncodedUtils.format and UrlEncodedFormEntityUrlEncodedFormEntity requires a List<? extends UrlEncodedFormEntity> to pass it to URLEncodedUtils.format. It would be nice to use Iterable<? extends UrlEncodedFormEntity> to be able to use other collections, e.g. a Set<? extends UrlEncodedFormEntity>"
1,"StatusLine IndexOutOfBoundsReported by Sam Berlin on the developers mailing list:

I'm not sure if this problem is still on CVS HEAD, but we're seeing it  
against 2.0rc2.  In StatusLine (line 139 in my version), when it walks  
through the spaces, it is possible that the entire line was spaces (and  
thus a malformed response).  The code will throw an  
StringIndexOutOfBoundsException now instead of the correct  
HttpException.  See the following bug:

http://bugs.limewire.com/bugs/ 
searching.jsp?disp1=l&disp2=c&disp3=o&disp4=j&l=151&c=204&m=416_205

Thanks,
  Sam"
0,"optionally support naist-jdic for kuromojiThis is an alternative dictionary, somewhat larger (~25%).

we can support it in build.xml so if a user wants to build with it, they can (the resulting jar file will be 500KB larger)"
0,"Add a simple FST impl to Lucene
I implemented the algo described at
http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.24.3698 for
incrementally building a finite state transducer (FST) from sorted
inputs.

This is not a fully general FST impl -- it's only able to build up an
FST incrementally from input/output pairs that are pre-sorted.

Currently the inputs are BytesRefs, and the outputs are pluggable --
NoOutputs gets you a simple FSA, PositiveIntOutputs maps to a long,
ByteSequenceOutput maps to a BytesRef.

The implementation has a low memory overhead, so that it can handle a
fairly large set of terms.  For example, it can build the FSA for the
9.8M terms from a 10M document wikipedia index in ~8 seconds (on
beast), using ~256 MB peak RAM, resulting in an FSA that's ~60 MB.

It packs the FST as-it-builds into a compact byte[], and then exposes
the API to read nodes/arcs directly from the byte[].  The FST can be
quickly saved/loaded to/from a Directory since it's just a big byte[].

The format is similar to what Morfologik uses
(http://sourceforge.net/projects/morfologik/).

I think there are a number of possible places we can use this in
Lucene.  For example, I think many apps could hold the entire terms
dict in RAM, either at the multi-reader level or maybe per-segment
(mapping to file offset or to something else custom to the app), which
may possibly be a good speedup for certain MTQs (though, because the
format is packed into a byte[], there is a decode cost when visiting
arcs).

The builder can also prune as it goes, so you get a prefix trie pruned
according to how many terms run through the nodes, which makes it
faster and even less memory consuming.  This may be useful as a
replacement for our current binary search terms index since it can
achieve higher term density for the same RAM consumption of our
current index.

As an initial usage to make sure this is exercised, I cutover the
SimpleText codec, which currently fully loads all terms into a
TreeMap (and has caused intermittent OOME in some tests), to use an FST
instead.  SimpleText uses a PairOutputs which is able to ""pair up"" any
two other outputs, since it needs to map each input term to an int
docFreq and long filePosition.

All tests pass w/ SimpleText forced codec, and I think this is
committable except I'd love to get some help w/ the generics
(confession to the policeman: I had to add
@SuppressWarnings({""unchecked""})) all over!!  Ideally an FST is
parameterized by its output type (Integer, BytesRef, etc.).

I even added a new @nightly test that makes a largeish set of random
terms and tests the resulting FST on different outputs :)

I think it would also be easy to make a variant that uses char[]
instead of byte[] as its inputs, so we could eg use this during analysis
(Robert's idea).  It's already be easy to have a CharSequence
output type since the outputs are pluggable.

Dawid Weiss (author of HPPC -- http://labs.carrotsearch.com/hppc.html -- and
Morfologik -- http://sourceforge.net/projects/morfologik/)
was very helpful iterating with me on this (thank you!).
"
0,"Optimize refresh operations With the current implementation (recursive) refresh operations cause a full traversal of the sub-tree rooted at the item causing the refresh. This is potentially expensive. 

Instead of invalidating each item in the respective sub-tree I propose to mark the root of the sub-tree as invalidated. Such a mark would include a time stamp. Also individual items would be time stamped with their resolution time. When an item is accessed, it would check if its resolution time stamp is older than the latest invalidation time stamp. If so, it checks whether the invalidation applies to it at all (by traversing up the path) and if so it would re-resolve itself. In any case its resolution time stamp will be updated.

This approach would make invalidation much cheaper without putting much additional load to simple item access. Moreover most of the additional load (traversing up the path) only applies when an invalidation is pending."
0,"[PATCH] Remove equals() from internal Comparator of ConjunctionScorerAs written, the equals() method is not used. 
The docs of java.util.Comparator have an equals() with a single 
arg to compare the Comparator itself to another one, which is 
hardly ever useful. 
Patch follows"
1,"MatchAllDocsQuery, MultiSearcher and a custom HitCollector throwing exceptionI have encountered an issue with lucene1.9.1. It involves MatchAllDocsQuery, MultiSearcher and a custom HitCollector. The following code throws  java.lang.UnsupportedOperationException.

If I remove the MatchAllDocsQuery  condition (comment whole //1 block), or if I dont use the custom hitcollector (ms.search(mbq); instead of ms.search(mbq, allcoll);) the exception goes away. By stepping into the source I can see it seems due to MatchAllDocsQuery no implementing extractTerms()....


           Searcher searcher = new
IndexSearcher(""c:\\projects\\mig\\runtime\\index\\01Aug16\\"");
           Searchable[] indexes = new IndexSearcher[1];
           indexes[0] = searcher;
           MultiSearcher ms = new MultiSearcher(indexes);

           AllCollector allcoll = new AllCollector(ms);

           BooleanQuery mbq = new BooleanQuery();
           mbq.add(new TermQuery(new Term(""body"", ""value1"")),
BooleanClause.Occur.MUST_NOT);
// 1
           MatchAllDocsQuery alld = new MatchAllDocsQuery();
           mbq.add(alld, BooleanClause.Occur.MUST);
//

           System.out.println(""Query: "" + mbq.toString());

           // 2
           ms.search(mbq, allcoll);
           //ms.search(mbq);"
0,"IndexWriter.addIndexes can make any incoming segment into CFS if it isn't alreadyToday, IW.addIndexes(Directory) does not modify the CFS-mode of the incoming segments. However, if IndexWriter's MP wants to create CFS (in general), there's no reason why not turn the incoming non-CFS segments into CFS. We anyway copy them, and if MP is not against CFS, we should create a CFS out of them.

Will need to use CFW, not sure it's ready for that w/ current API (I'll need to check), but luckily we're allowed to change it (@lucene.internal).

This should be done, IMO, even if the incoming segment is large (i.e., passes MP.noCFSRatio) b/c like I wrote above, we anyway copy it. However, if you think otherwise, speak up :).

I'll take a look at this in the next few days."
0,"CacheManager interval between recalculation of cache sizes should be configurableCurrently interval between recaluclation of cahce size is hard coded to 1000 ms. Resizing/recalculation of cache size is quite expensive method (especially getMemoryUsed on MLRUItemStateCache is time consuming)

Depending on the configuration, we realized that under some load up to 10-15% percent of CPU time (profiler metrics) could be spend doing such recalculations. It does not seem to be needed to resize cache every second. Best this interval should be configurable in external config. file with other cache settings (like memory sizes)."
0,"Move common validation checks to a single placecheck for effective locks, nodes being checked-in, protection of item definitions etc. are abundant throughout jackrabbit-core. now that in addition retention and holds will be added and need to be checked as well, i suggest to move those checks to a common utility class and pass item and a list of checks to be performed.

batcheditemoperations already provides a similar pattern for the operations on item states.
i therefore suggest to move the flags to its base (ItemValidator) and add the utility methods needed."
0,"Allow database as backend for clusteringCurrently, clustering (see JCR-623) uses a shared file system folder in order to store modifications and synchronize all nodes in the cluster. Alternatively, a database backend should be available."
0,"RFC 2965 Support (Port sensitive cookies)RFC 2109 doesn't consider port numbers when matching and filtering cookies. RFC
2965 does. Modify the Cookie class so that it (optionally?) supports RFC 2965,
while maintaining support for RFC 2109-style (portless) cookies."
0,"[PATCH] TermVectorReader and TermVectorWriterTermVectorReader.close() closes all streams now under any condition. If an
excpetion is catched, it is remembered an thrown when all streams are closed.
Unnecessary variable assignment removed from code. 
Fix typo in TermVectorReader and TermVectorWriter."
0,"Replace spatial contrib module with LSP's spatial-lucene moduleI propose that Lucene's spatial contrib module be replaced with the spatial-lucene module within Lucene Spatial Playground (LSP).  LSP has been in development for approximately 1 year by David Smiley, Ryan McKinley, and Chris Male and we feel it is ready.  LSP is here: http://code.google.com/p/lucene-spatial-playground/  and the spatial-lucene module is intuitively in svn/trunk/spatial-lucene/.

I'll add more comments to prevent the issue description from being too long."
0,"Webdav: Review usage of command chainsi'd like to review the usage of command chains for import/export within the simple webdav server.

while the concept of command chains offers a lot of flexibility, it showed that the implementation generates some drawbacks. a new mechanism should take advantage of the experiences made with the command chains.

from my point of view the following issues should be taken into consideration:

- provide means to extend and modify the import/export logic with minimal effort

- consistent import/export functionality for both collections and non-collections
  > export/import should not be completely separated.
  > interfaces should encourage consistency
  > increase maintainability, reduce no of errors

- distinction of collections and non-collections for import/export behaviour
  > PUT must result in non-coll, MKCOL in collection

- allow to defined a set of import/export-handlers with a given order.

- the different handlers must not rely on each other.

- an import/export should be completed after the first handler indicates success. there 
  should not be other classes involved in order to complete the import/export.

- avoid huge configuration files and if possible, avoid program flow being defined outside of java code.

- avoid duplicate configuration (e.g. resource-filtering), duplicate code, duplicate logic, that is 
  hard to maintain.

- additonal logic should be defined within a given import/export handler.
  however, in case of webdav i see limited value of using extra logic such as addMixin or checkin, 
  that are covered by  webdav methods (such as LOCK, VERSION-CONTROL or CHECKIN).

regards
angela


"
1,"Cached Item could be lost in cachesince items and itemstate are cache in week/soft reference maps, they can disappear from the caches after a GC cycle, and code like this:

if (isCached(id)) {
  return retrieveItem(id);
}

has potential to fail. log entries like:

failed to build path of c2eeecbe-6126-45a2-a38a-002361095107: 3334d748-2790-4004-8bfa-09463624c7c4 has no child entry for 4897c961-f36f-4d46-87bd-f24f152138a4

are the result."
1,"PATCH MultiLevelSkipListReader NullPointerException When Reconstructing Document Using Luke Tool, received NullPointerException.

java.lang.NullPointerException
        at org.apache.lucene.index.MultiLevelSkipListReader.loadSkipLevels(MultiLevelSkipListReader.java:188)
        at org.apache.lucene.index.MultiLevelSkipListReader.skipTo(MultiLevelSkipListReader.java:97)
        at org.apache.lucene.index.SegmentTermDocs.skipTo(SegmentTermDocs.java:164)
        at org.getopt.luke.Luke$2.run(Unknown Source)

Luke version 0.7.1

I emailed with Luke author Andrzej Bialecki and he suggested the attached patch file which fixed the problem.
"
0,"On missing child node, automatically remove the entry (when a session attribute is set)If a node points to a non-existing child node (which is a repository inconsistency), currently this child node is silently ignored for read operations (as far as I can tell). However, when trying to add another child node with the same name, an exception is thrown with a message saying a child node with this name already exists. Also, the parent node can't be removed.

One solution is to remove the bad child node entry, but only if the session attribute ""org.apache.jackrabbit.autoFixCorruptions"" is set (so by default the repository is not changed 'secretly'):

    SimpleCredentials cred = new SimpleCredentials(...);
    cred.setAttribute(""org.apache.jackrabbit.autoFixCorruptions"", ""true"");
    rep.login(cred);

It's not a perfect solution, but it might be better than throwing an exception and basically preventing changes.

Another solution (not implemented) would be to rename the missing child node entry when trying to add a child node with the same name (for example add the current date/time, or a random digit until there is no conflict), and then continue with adding the new child node.

"
0,"remove Byte/CharBuffer wrapping for collation key generationWe can remove the overhead of ByteBuffer and CharBuffer wrapping in CollationKeyFilter and ICUCollationKeyFilter.

this patch moves the logic in IndexableBinaryStringTools into char[],int,int and byte[],int,int based methods, with the previous Byte/CharBuffer methods delegating to these.
Previously, the Byte/CharBuffer methods required a backing array anyway.
"
0,Use executor service from repository for index mergingThe index merger currently starts its own threads for index merges. Using the repository wide executor service would simplify things and make configuration easier.
0,"Update site level documentationugg - a fun one - my brain is sliding to the bottom of my skull from excitement.

Must update all of the site level pages to current API usage."
0,"Remove dependency on Jackrabbit-coreWe should remove the dependency on Jackrabit core in the OCM subprojects ""jcr-mapping"" and ""annotation"". We can use Jackrabbit core only for the unit tests. 

We can also split the jcr-nodemanagement into several subprojects (one per JCR repo impl).  We will have only one subproject for Jackrabbit but contributions for other JCR repo impl are welcome.  A specific jcr-nodemanagement jar can be produce for each JCR repo impl. When the JCR will support the node creation, we can refactor the jcr-nodemanagement. 

"
0,"Occasional testDataStoreGarbageCollection test failuresIn the past few days our Hudson build started failing every now and then with the following jackrabbit-core test failure:

javax.jcr.NoSuchWorkspaceException: security
	at org.apache.jackrabbit.core.RepositoryImpl.getWorkspaceInfo(RepositoryImpl.java:786)
	at org.apache.jackrabbit.core.RepositoryImpl.getSystemSession(RepositoryImpl.java:985)
	at org.apache.jackrabbit.core.RepositoryImpl.getSecurityManager(RepositoryImpl.java:471)
	at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1496)
	at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:380)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:123)
	at org.apache.jackrabbit.core.data.DataStoreAPITest.testDataStoreGarbageCollection(DataStoreAPITest.java:55)"
1,"EventConsumer.canRead() should rely on AccessManager.isGranted()The current implementation of EventConsumer.canRead() uses
AccessManager.canRead(), which might cause issues if the item
does not exist anymore. AccessManager.isGranted() explicitly
mentions and supports checks on paths for items that do not
yet exist or not exist anymore.

See also JCR-3271."
0,"ConjunctionScorer - more tuneup(See also: #LUCENE-443)
I did some profile testing with the new ConjuctionScorer in 2.1 and discovered a new bottleneck in ConjunctionScorer.sortScorers. The java.utils.Arrays.sort method is cloning the Scorers array on every sort, which is quite expensive on large indexes because of the size of the 'norms' array within, and isn't necessary. 

Here is one possible solution:

  private void sortScorers() {
// squeeze the array down for the sort
//    if (length != scorers.length) {
//      Scorer[] temps = new Scorer[length];
//      System.arraycopy(scorers, 0, temps, 0, length);
//      scorers = temps;
//    }
    insertionSort( scorers,length );
    // note that this comparator is not consistent with equals!
//    Arrays.sort(scorers, new Comparator() {         // sort the array
//        public int compare(Object o1, Object o2) {
//          return ((Scorer)o1).doc() - ((Scorer)o2).doc();
//        }
//      });
  
    first = 0;
    last = length - 1;
  }
  private void insertionSort( Scorer[] scores, int len)
  {
      for (int i=0; i<len; i++) {
          for (int j=i; j>0 && scores[j-1].doc() > scores[j].doc();j-- ) {
              swap (scores, j, j-1);
          }
      }
      return;
  }
  private void swap(Object[] x, int a, int b) {
    Object t = x[a];
    x[a] = x[b];
    x[b] = t;
  }
 
The squeezing of the array is no longer needed. 
We also initialized the Scorers array to 8 (instead of 2) to avoid having to grow the array for common queries, although this probably has less performance impact.

This change added about 3% to query throughput in my testing.

Peter
"
0,"Clean caches in node type registry on session logoutWhen running the JCR tests the memory consumption increases steadily. At the end of the test run it consumes about 300 Mb on my machine. There's not really a memory leak in jcr2spi, because the JUnit tests keep references to Session and Node instances until the end of the test run, but  it would be nice if those instances were a bit more lightweight."
0,"SPI: Introduce NodeInfo.getChildInfos()Improvement suggested by Marcel:

ChildInfo is basically a stripped down NodeInfo. With little effort it would even be possible to have NodeInfo extends ChildInfo. Not sure how useful that is, but since we don't have that inheritance in code and at the same time nearly a 100% overlap it makes me suspicious.

Here's another idea:

introduce a method ChildInfo[] NodeInfo.getChildInfos(). The method either returns:

- all child infos, which also gives the correct number of child nodes. this may also mean that an empty array is returned to indicate there are no child nodes.
- null, to indicate that there are *lots* of child nodes and the method RepositoryService.getChildInfos() with the iterator should be used. 


"
0,Missing third party notices and license infoThe components that bundle external libraries (jackrabbit-webapp and jackrabbit-jca) should come with appropriate copyright notices and license information.
0,"Add CharArrayMap to lucene and make CharAraySet an proxy on the keySet() of itThis patch adds a CharArrayMap<V> to Lucene's analysis package as compagnon of CharArraySet. It supports fast retrieval of char[] keys like CharArraySet does. This is important for some stemmers and other places in Lucene.

Stemers generally use CharArrayMap<String>, which has then get(char[]) returning String. Strings are compact and can be easily copied into termBuffer. A Map<String,String> would be slow as the termBuffer would be first converted to String, then looked up. The return value as String is perfectly legal, as it can be copied easily into termBuffer.

This class borrows lots of code from Solr's pendant, but has additional features and more consistent API according to CharArraySet. The key is always <?>, because as of CharArraySet, anything that has a toString() representation can be used as key (of course with overhead). It also defines a unmodifiable map and correct iterators (returning the native char[]).

CharArraySet was made consistent and now returns for matchVersion>=3.1 also an iterator on char[]. CharArraySet's code was almost completely copied to CharArrayMap and removed in the Set. CharArraySet is now a simple proxy on the keySet().

In future we can think of making CharArraySet/CharArrayMap/CharArrayCollection an interface so the whole API would be more consistent to the Java collections API. But this would be a backwards break. But it would be possible to use better impl instead of hashing (like prefix trees)."
0,"Logging per test caseThanks to the switch to Logback a while ago (JCR-2584) we can start taking advantage of some nice new features like the one described in [1]. With this trick we'll be able to split the currently pretty large jcr.log test log file we have to separate log files per each test case. This will make it much easier to review the logs written during any particular test.

[1] http://www.nalinmakar.com/2010/07/28/logging-tests-to-separate-files/"
0,"SimpleJbossAccessManagerhttp://wiki.apache.org/jackrabbit/SimpleJbossAccessManager

Code contribution
"
0,"Java 5 port phase II LUCENE-1257 addresses the public API changes ( generics , mainly ) and other j.u.c. package changes related to the API .  The changes are frozen and closed for 3.0 . This would be a placeholder JIRA for 3.0+ version to address the pending changes ( tests for generics etc.) and any other internal API changes as necessary. "
0,"New PostSOAP example (for src/examples)I have a slightly modified version of PostXML which invokes SOAP requests. The
only difference to PostXML is that PostSOAP takes the SOAPAction as an extra
commndline arg and adds that as a header into the request."
1,"NullPointerException in GarbageCollector.scan() if no DataStore configured
I am running the garbage collector in a separate thread every 5 minutes.

            GarbageCollector gc = ((SessionImpl)mSession).createDataStoreGarbageCollector();
            gc.scan();
            gc.stopScan();
            int du = gc.deleteUnused();

When using Jackrabbit v1.5.2 I get sometimes a null pointer exception;

java.lang.NullPointerException
        at org.apache.jackrabbit.core.data.GarbageCollector.scan(GarbageCollector.java:153)



"
0,"Replacing mixin type doesn't preserve propertiesNodeImpl.setPrimaryType(String) attempts to ""redefine"" nodes and properties that were defined by the previous node type if they also appear in the new type. If there is no matching definition for a node/property in the new type - or value conversion for matched node/property fails - only then are children removed. For example, say I have a node ""harry"", with a primary type ""Human"" that defines a ""bloodgroup"" property. If I set the primary type to be an unrelated type ""Animal"" that has a similar ""bloodgroup"" property, then its property value will survive the call to setPrimaryType(""Animal"").

The same is apparently not possible with mixins. NodeImpl.removeMixin(Name) immediately removes all children that were defined by the mixin (strictly, those that are not present in the effective node type resulting from the mixin being removed). In addition, NodeImpl.addMixin(Name) immediately throws a NodeTypeConflictException if you attempt to add a mixin defining an identically-named property prior to calling removeMixin. For example, say I have a node ""matrix"", with a mixin type ""movie"" that defines a ""title"" property. If I wish to replace the ""movie"" mixin on that node with another ""jcr:title"" mixin type, the existing ""title"" property will be deleted.

This occurs regardless of the order in which removeMixin and addMixin are called, and without session.save() being called between them. One option for coding this is to defer validation (and possible node/property removal) until session.save() is called.

This is not strictly a bug, as JSR-283 seems to leave the details of assigning node types (section 5.5) unspecified. However, it does say for Node.removeMixin(String) that ""Both the semantic change in effective node type and the persistence of the
change to the jcr:mixinTypes property occur on save"" and ideally we could emulate the nice behaviour in NodeImpl.setPrimaryType(String) for mixin types."
1,"OOM erros with CheckIndex with indexes containg a lot of fields with normsAll index readers have a cache of the last used norms (SegmentReader, MultiReader, MultiSegmentReader,...). This cache is never cleaned up, so if you access norms of a field, the norm's byte[maxdoc()] array is not freed until you close/reopen the index.

You can see this problem, if you create an index with many fields with norms (I tested with about 4,000 fields) and many documents (half a million). If you then call CheckIndex, that calls norms() for each (!) field in the Segment and each of this calls creates a new cache entry, you get OutOfMemoryExceptions after short time (I tested with the above index: I was not able to do a CheckIndex even with ""-Xmx 16GB"" on 64bit Java).

CheckIndex opens and then tests each segment of a index with a separate SegmentReader. The big index with the OutOfMemory problem was optimized, so consisting of one segment with about half a million docs and about 4,000 fields. Each byte[] array takes about a half MiB for this index. The CheckIndex funtion created the norm for 4000 fields and the SegmentReader cached them, which is about 2 GiB RAM. So OOMs are not unusal.

In my opinion, the best would be to use a Weak- or better a SoftReference so norms.bytes gets java.lang.ref.SoftReference<byte[]> and used for caching. With proper synchronization (which is done on the norms cache in SegmentReader) you can do the best with SoftReference, as this reference is garbage collected only when an OOM may happen. If the byte[] array is freed (but it is only freed if no other references exist), a lter call to getNorms() creates a new array. When code is hard referencing the norms array, it will not be freed, so no problem. The same could be done for the other IndexReaders.

Fields without norm() do not have this problem, as all these fields share a one-time allocated dummy norm array. So the same index without norms enabled for most of the fields checked perfectly.

I will prepare a patch tomorrow.

Mike proposed another quick fix for CheckIndex:
bq. we could do something first specifically for CheckIndex (eg it could simply use the 3-arg non-caching bytes method instead) to prevent OOM errors when using it.
"
0,JSR 283: JCR Names
0,"Use NIO positional read to avoid synchronization in FSIndexInputAs suggested by Doug, we could use NIO pread to avoid synchronization on the underlying file.
This could mitigate any MT performance drop caused by reducing the number of files in the index format."
0,Jcr2Spi: configuration entry for size of ItemCachein order to make the size of the ItemCache configurable (see TODO in jcr2spi SessionImpl) i'd like to extend the jcr2spi RepositoryConfig and have a default value being provided with AbstractRepositoryConfig in the tests section.
0,ExtendableQueryParser should allow extensions to access the toplevel parser settings/ propertiesBased on the latest discussions in LUCENE-2039 this issue will expose the toplevel parser via the ExtensionQuery so that ExtensionParsers can access properties like getAllowLeadingWildcards() from the top level parser.
0,"Factor maxMergeSize into findMergesForOptimize in LogMergePolicyLogMergePolicy allows you to specify a maxMergeSize in MB, which is taken into consideration in regular merges, yet ignored by findMergesForOptimze. I think it'd be good if we take that into consideration even when optimizing. This will allow the caller to specify two constraints: maxNumSegments and maxMergeMB. Obviously both may not be satisfied, and therefore we will guarantee that if there is any segment above the threshold, the threshold constraint takes precedence and therefore you may end up w/ <maxNumSegments (if it's not 1) after optimize. Otherwise, maxNumSegments is taken into consideration.

As part of this change, I plan to change some methods to protected (from private) and members as well. I realized that if one wishes to implement his own LMP extension, he needs to either put it under o.a.l.index or copy some code over to his impl.

I'll attach a patch shortly."
0,"Unable to get the status line from a http method objectThe status line (typically the first line returned from a http connection read)
is hidden inside httpclient with no way for client code to retrieve it intact.
readStatusLine() in HttpMethodBase is where the status line is
read, but it is never stored and is not available outside the method.

We could store the status line as a string and add a getStatusLine
method to the HttpMethod interface and HttpMethodBase class.  Alternatively, we
could create a header for it with the name StatusLine (or perhaps just null) so
that it could be retrieved with getHeader(""StatusLine"").  This would preserve
the interface but would be a bit of a kludge."
0,"Allow TaskSequence to run for certain timeTo help the perf testing for LUCENE-1483, I added simple ability to specify a fixed run time (seconds) for a task sequence, eg:
{code}
{ ""XSearchWithSort"" SearchWithSort(doctitle:string) > : 2.7s
{code}
iterates on that subtask until 2.7 seconds have elapsed, and then sets the repetition count to how many iterations were done.  This is useful when you are running searches whose runtime may vary drastically."
0,Move Content-Type to the RequestEntityThe content type is really a property of the RequestEntity.  It should be moved there.
0,"Remove RepositoryService.getRootId()For consistency reasons jcr2spi should use idFactory.createNodeId((String) null, pathFactory.getRootPath()) everywhere to build the NodeId of the root node. having two separate methods is confusing."
0,More javadocs for WeightFrom Doug's reply of 21 Feb 2005 in bug 31841
0,"Mavenize ProjectMavenize project pending mailing list feedback.

I'm attaching an out-dated zip that has some useful ""stuff"" to review or build upon.

Proposed flow:

1) Review the maven specific elements in the zip:
- project.xml, header.txt, license.txt, notice.txt, checkstyle.xml, project.properties, xdocs directory. NOTE: I know the license.txt and maybe header.txt are not correct per earlier discussions.
2) Once new package layout/code is in svn - we can refactor this and move the maven related files into place.

Thoughts,
  a) One of the artifacts generated is the bat file and supporting directory with the jar to run the sample app - could use some ideas on how maven should generate this.
  b) How the local maven repo gets the jcr-api.jar? My current thinking is 'same as any non-redistributable artifact'. The user is responsible for downloading it to a local repository."
0,"TestCachingSpanFilter sometimes failsif I run 
{noformat} 
ant test -Dtestcase=TestCachingSpanFilter -Dtestmethod=testEnforceDeletions -Dtests.seed=5015158121350221714:-3342860915127740146 -Dtests.iter=100
{noformat} 

I get two failures on my machine against current trunk
{noformat} 

junit-sequential:
    [junit] Testsuite: org.apache.lucene.search.TestCachingSpanFilter
    [junit] Testcase: testEnforceDeletions(org.apache.lucene.search.TestCachingSpanFilter):	FAILED
    [junit] expected:<2> but was:<3>
    [junit] junit.framework.AssertionFailedError: expected:<2> but was:<3>
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)
    [junit] 	at org.apache.lucene.search.TestCachingSpanFilter.testEnforceDeletions(TestCachingSpanFilter.java:101)
    [junit] 
    [junit] 
    [junit] Testcase: testEnforceDeletions(org.apache.lucene.search.TestCachingSpanFilter):	FAILED
    [junit] expected:<2> but was:<3>
    [junit] junit.framework.AssertionFailedError: expected:<2> but was:<3>
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)
    [junit] 	at org.apache.lucene.search.TestCachingSpanFilter.testEnforceDeletions(TestCachingSpanFilter.java:101)
    [junit] 
    [junit] 
    [junit] Tests run: 100, Failures: 2, Errors: 0, Time elapsed: 2.297 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestCachingSpanFilter -Dtestmethod=testEnforceDeletions -Dtests.seed=5015158121350221714:-3342860915127740146
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestCachingSpanFilter -Dtestmethod=testEnforceDeletions -Dtests.seed=5015158121350221714:-3342860915127740146
    [junit] NOTE: test params are: codec=MockVariableIntBlock(baseBlockSize=43), locale=fr, timezone=Africa/Bangui
    [junit] ------------- ---------------- ---------------
    [junit] Test org.apache.lucene.search.TestCachingSpanFilter FAILED
{noformat}

not sure what it is but it seems likely to be a WeakRef / GC issue in the cache. "
1,ChangeLogRecord throws NullPointerExceptionHappens when the userId of the session that created the events is null.
1,"DigestScheme.authenticate returns invalid authorization string when algorithm is nullDigestScheme.authenticate returns invalid authorization string when algorithm 
is null. 
I traced the bug and found the following from the method call:
authenticate(Credentials credentials, String method, String uri) calls
authenticate(UsernamePasswordCredentials credentials,
            Map params) calls
createDigest(String uname, String pwd,
            Map params)
  and properly defaults algorithm to MD5 if null
but the final call to createDigestHeader(String uname, Map params,
            String digest) does not default algorithm to MD5 if null"
0,"Update namespace uri for prefix fnThe SearchManager class still uses an outdated namespace uri for the 'fn' prefix: http://www.w3.org/2004/10/xpath-functions

The prefix should be re-mapped to the now offical namespace: http://www.w3.org/2005/xpath-functions

See: http://www.w3.org/TR/xquery-operators/#namespace-prefixes

To keep a minimum of backward compatibility, the existing namespace uri should still exist in the namespace registry, but refer to another prefix. E.g. fn_old."
0,"short circuit FuzzyQuery.rewrite when input token length is small compared to minSimilarityI found this (unreplied to) email floating around in my Lucene folder from during the holidays...

{noformat}
From: Timo Nentwig
To: java-dev
Subject: Fuzzy makes no sense for short tokens
Date: Mon, 31 Dec 2007 16:01:11 +0100
Message-Id: <200712311601.12255.lucene@nitwit.de>

Hi!

it generally makes no sense to search fuzzy for short tokens because changing
even only a single character of course already results in a high edit
distance. So it actually only makes sense in this case:

           if( token.length() > 1f / (1f - minSimilarity) )

E.g. changing one character in a 3-letter token (foo) results in an edit
distance of 0.6. And if minSimilarity (which is by default: 0.5 :-) is higher
we can save all the expensive rewrite() logic.
{noformat}

I don't know much about FuzzyQueries, but this reasoning seems sound ... FuzzyQuery.rewrite should be able to completely skip all TermEnumeration in the event that the input token is shorter then some simple math on the minSimilarity.  (i'm not smart enough to be certain that the math above is right however ... it's been a while since i looked at Levenstein distances ... tests needed)
"
1,"If the NRT reader hasn't changed then IndexReader.openIfChanged should return nullI hit a failure in TestSearcherManager (NOTE: doesn't always fail):

{noformat}
  ant test -Dtestcase=TestSearcherManager -Dtestmethod=testSearcherManager -Dtests.seed=459ac99a4256789c:-29b8a7f52497c3b4:145ae632ae9e1ecf
{noformat}

It was tripping the assert inside SearcherLifetimeManager.record,
because two different IndexSearcher instances had different IR
instances sharing the same version.  This was happening because
IW.getReader always returns a new reader even when there are no
changes.  I think we should fix that...

Separately I found a deadlock in
TestSearcherManager.testIntermediateClose, if the test gets
SerialMergeScheduler and needs to merge during the second commit.
"
1,"WebDAV method invocation trying to create a new resource should fail with 409 (Conflict) if parent resource does not existThis is Litmus test case copy_nodestcoll. An attempt is made to COPY an existing resource to a new location, where the parent collection of the resource-to-be-created does not exist. RFC2518 asks for status code 409 (Conflict) instead of 403 (Forbidden) in this case.
"
1,"restore sometime throws error about missing tmp filesCaused by: javax.jcr.RepositoryException: file backing binary value not
found: /server/apache-tomcat-5.5.15/temp/bin4435.tmp (No such file or
directory): /server/apache-tomcat-5.5.15/temp/bin4435.tmp (No such file
or directory)
   at
org.apache.jackrabbit.core.value.BLOBFileValue.getStream(BLOBFileValue.java:454)
   at
org.apache.jackrabbit.core.state.util.Serializer.serialize(Serializer.java:197)"
1,"DefaultHttpRequestRetryHandler#retryRequest should not retry aborted requestsDefaultHttpRequestRetryHandler#retryRequest incorrectly retries aborted requests; I have seen the following log messages in JMeter:

org.apache.http.impl.client.DefaultHttpClient: I/O exception (java.net.SocketException) caught when processing request: socket closed
org.apache.http.impl.client.DefaultHttpClient: Retrying request

and

org.apache.http.impl.client.DefaultHttpClient: I/O exception (java.net.BindException) caught when connecting to the target host: Address already in use: connect
org.apache.http.impl.client.DefaultHttpClient: Retrying connect

The abort() method sets the isAborted() flag, but the retry handler does not check it."
1,"FSDirectory doesn't detect double-close nor usage after closeFSDirectory.close implements logic to ensure only a single instance of FSDirectory per canonical directory exists.  This means code that synchronizes on the FSDirectory instance is also synchronized against that canonical directory.  I think only IndexModifier (now deprecated) actually makes use of this, but I'm not certain. 

But, the close() method doesn't detect double close, and doesn't catch usage after being closed, and so one can easily get two instances of FSDirectory for the same canonical directory."
1,"Problem with IndexWriter.mergeFinishI'm getting a (very) infrequent assert in IndexWriter.mergeFinish from TestIndexWriter.testAddIndexOnDiskFull. The problem occurs during the rollback when the merge hasn't been registered. I'm not 100% sure this is the correct fix, because it's such an infrequent event. 

{code:java}
  final synchronized void mergeFinish(MergePolicy.OneMerge merge) throws IOException {
    
    // Optimize, addIndexes or finishMerges may be waiting
    // on merges to finish.
    notifyAll();

    if (merge.increfDone)
      decrefMergeSegments(merge);

    assert merge.registerDone;

    final SegmentInfos sourceSegments = merge.segments;
    final int end = sourceSegments.size();
    for(int i=0;i<end;i++)
      mergingSegments.remove(sourceSegments.info(i));
    mergingSegments.remove(merge.info);
    merge.registerDone = false;
  }
{code}

Should  be something like:

{code:java}
  final synchronized void mergeFinish(MergePolicy.OneMerge merge) throws IOException {
    
    // Optimize, addIndexes or finishMerges may be waiting
    // on merges to finish.
    notifyAll();

    if (merge.increfDone)
      decrefMergeSegments(merge);

    if (merge.registerDone) {
      final SegmentInfos sourceSegments = merge.segments;
      final int end = sourceSegments.size();
      for(int i=0;i<end;i++)
        mergingSegments.remove(sourceSegments.info(i));
      mergingSegments.remove(merge.info);
      merge.registerDone = false;
    }
  }
{code}"
1,"AccessManager asks for property (jcr:created) permissions before the actual creation of the object
When implementing a custom AccessManager for Jackrabbit v1.5+ there a bug when creating a new object.

I perform an addNode() and my own accessmanager the isGranted() method is override'd and performs the following code;

            ......
            String perm = null;
            NodeId     nodeId = mHierMgr.resolveNodePath( pPath );
            PropertyId propId = null;
            if (nodeId==null) {
              propId = mHierMgr.resolvePropertyPath( pPath );
     System.out.println(""path = "" + pPath.toString() );
              // **** TODO is this ok?... it happens when a new object is created and the accessmanager ask for read access on a property.
//              if (propId==null) return true;
              nodeId = propId.getParentId();
            }

this is the System.out.println result

path = {}        {}JeCARS        {}default        {}Groups        {}testGroup        {http://www.jcp.org/jcr/1.0}created


and the stacktrace


	at org.jecars.CARS_AccessManager.isGranted(CARS_AccessManager.java:844)
	at org.jecars.CARS_AccessManager.isGranted(CARS_AccessManager.java:806)
	at org.apache.jackrabbit.core.ItemManager.canRead(ItemManager.java:339)
	at org.apache.jackrabbit.core.ItemManager.canRead(ItemManager.java:326)
	at org.apache.jackrabbit.core.ItemManager.createItemData(ItemManager.java:696)
	at org.apache.jackrabbit.core.ItemManager.getItemData(ItemManager.java:291)
	at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:228)
	at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:493)
	at org.apache.jackrabbit.core.NodeImpl.createChildProperty(NodeImpl.java:479)
	at org.apache.jackrabbit.core.NodeImpl.createChildNode(NodeImpl.java:535)
	at org.apache.jackrabbit.core.NodeImpl.internalAddChildNode(NodeImpl.java:795)
	at org.apache.jackrabbit.core.NodeImpl.internalAddNode(NodeImpl.java:729)
	at org.apache.jackrabbit.core.NodeImpl.internalAddNode(NodeImpl.java:677)
	at org.apache.jackrabbit.core.NodeImpl.addNode(NodeImpl.java:2110)


It seems that READ permission for the jcr:created property is requested before the object is actually created


"
1,"LOWER operand with nested LOCALNAME operand not work with SQL2Below query was running successful using Query.SQL languange:
SELECT * FROM nt:file WHERE (CONTAINS(*, 'Jon') OR  LOWER(fn:name()) LIKE '%jon%') AND jcr:path LIKE '/Resources/%' ORDER BY jcr:score()

But equivalent next query in Query.JCR_SQL2 will fail with exception UnsupportedRepositoryOperationException():
SELECT * FROM [nt:file] WHERE (CONTAINS([nt:file].*, 'Jon') OR  LOWER(LOCALNAME()) LIKE '%jon%') AND ISDESCENDANTNODE('/Resources') ORDER BY SCORE()

From my investigation seems LOWER function will not work with nested function LOCALNAME. According to section ""6.7.32 LowerCase"" JCR 2.0 Specs, LOWER operand able to work on DynamicOperand argument."
0,"[PATCH] demo HTML parser corrupts foreign charactersWe are using HTML parser for parsing English and other NL documents in 
Eclipse.  Post Lucene 1.2 there has been a regression in the parser.  
Characters coming from Reader (obtained from getReader() ) are corrupted.  
Only the characters that can be encoded using the default machine encoding go 
through correctly.  For example, parsing Chinese document on an English 
machine results with all characters, except the few English words, corrupted."
1,"SPI2DAVex: HttpClient StringPart uses charset US-ASCII by defaultif the diff is sent as multipart instead of a urlencoded post string properties may be garbeled.
reason: instances of httpclient StringPart are created without specifying the charset in which case US-ASCII is used by default."
1,"Incorrect ShingleFilter behavior when outputUnigrams == falseShingleFilter isn't working as expected when outputUnigrams == false. In particular, it is outputting unigrams at least some of the time when outputUnigrams==false.

I'll attach a patch to ShingleFilterTest.java that adds some test cases that demonstrate the problem.

I haven't checked this, but I hypothesize that the behavior for outputUnigrams == false got changed when the class was upgraded to the new TokenStream API?"
0,"Creation of JavaDoc fails on jackrabbit-jcr-serverThe creation of JavaDoc fails on project jackrabbit-jcr-server if the command ""mvn javadoc:javadoc"" is called on the latest checkout of jackrabbit. See attached log..."
0,"Correctly handle concurrent calls to addIndexes, optimize, commitSpinoff from here:

    http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200807.mbox/%3Cc7b302c50807111018j58b6d08djd56b5889f6b3780d@mail.gmail.com%3E"
0,"Add a document describing the HttpClient release processThe commons release process (http://jakarta.apache.org/commons/releases.html) is
a good starting place, but is out of date.  When we do our own releases, there
are some other steps particular to maven, the test-local and the webapp tests
that must be documented."
1,"NPE in StopFilter caused by StandardAnalyzer(boolean replaceInvalidAcronym) constructorI think that I found a problem with the new code (https://issues.apache.org/jira/browse/LUCENE-1068).
Usage of the new constructor StandardAnalyzer(boolean replaceInvalidAcronym) causes NPE in
StopFilter:

java.lang.NullPointerException
        at org.apache.lucene.analysis.StopFilter.<init>(StopFilter.java:74)
        at org.apache.lucene.analysis.StopFilter.<init>(StopFilter.java:86)
        at
org.apache.lucene.analysis.standard.StandardAnalyzer.tokenStream(StandardAnalyzer.java:151)
        at
org.apache.lucene.queryParser.QueryParser.getFieldQuery(QueryParser.java:452)
        at
org.apache.lucene.queryParser.QueryParser.Term(QueryParser.java:1133)
        at
org.apache.lucene.queryParser.QueryParser.Clause(QueryParser.java:1020)
        at
org.apache.lucene.queryParser.QueryParser.Query(QueryParser.java:948)
        at
org.apache.lucene.queryParser.QueryParser.Clause(QueryParser.java:1024)
        at
org.apache.lucene.queryParser.QueryParser.Query(QueryParser.java:948)
        at
org.apache.lucene.queryParser.QueryParser.TopLevelQuery(QueryParser.java:937)
        at
org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:147)

The reason is that new constructor forgets to initialize the stopSet field:
  public StandardAnalyzer(boolean replaceInvalidAcronym) {
    this.replaceInvalidAcronym = replaceInvalidAcronym;
  }

I guess this should be changed to something like this:
  public StandardAnalyzer(boolean replaceInvalidAcronym) {
    this(STOP_WORDS);
    this.replaceInvalidAcronym = replaceInvalidAcronym;
  }

The bug is present in RC3. Fix is one line, it'll be great to have it in 2.3
release.
"
0,"[PATCH] Differently configured Lucene 'instances' in same JVMCurrently Lucene can be configured using system properties. When running multiple 'instances' of Lucene for different purposes in the same JVM, it is not possible to use different settings for each 'instance'.

I made changes to some Lucene classes so you can pass a configuration to that class. The Lucene 'instance' will use the settings from that configuration. The changes do not effect the API and/or the current behavior so are backwards compatible.

In addition to the changes above I also made the SegmentReader and SegmentTermDocs extensible outside of their package. I would appreciate the inclusion of these changes but don't mind creating a separate issue for them.

"
1,"Journal log file rotation overwrites old filesJournal log files are rotated as follows:

  journal.log.N -> journal.log.(N+1)

Because the list of files to be rotated is created with alphanumeric sort order
(descending), it may destroy files in the following situation:

  journal.log.9  -> journal.log.10
  ..
  journal.log.2  -> journal.log.3
  journal.log.10 -> journal.log.11 (!)
  journal.log.1  -> journal.log.2

i.e. journal.log.10 is overwritten by journal.log.9 and then moved."
0,"trappy ignoreCase behavior with StopFilter/ignoreCaseSpinoff from LUCENE-3751:

{code}
* If <code>stopWords</code> is an instance of {@link CharArraySet} (true if
* <code>makeStopSet()</code> was used to construct the set) it will be
* directly used and <code>ignoreCase</code> will be ignored since
* <code>CharArraySet</code> directly controls case sensitivity.
{code}

This is really confusing and trappy... we need to change something here."
0,"MultiThreadedHttpConnectionManager never reclaims unused connectonsThere is no limit on the number of connections that will get created by the
MultiThreadedHttpConnectionManager.  Unused connections are never destroyed."
0,"Minor changes to SimpleHTMLFormatterI'd like to make few minor changes to SimpleHTMLFormatter.

1. Define DEFAULT_PRE_TAG and DEFAULT_POST_TAG and use them in the default constructor. This will not trigger String lookups by the JVM whenever the highlighter is instantiated.

2. Create the StringBuffer in highlightTerm with the right number of characters from the beginning. Even though StringBuffer's default constructor allocates 16 chars, which will probably be enough for most highlighted terms (pre + post tags are 7 chars, which leaves 9 chars for terms), I think it's better to allocate SB with the right # of chars in advance, to avoid char[] allocations in the middle."
0,"Speed up hierarchy cache initializationInitializing a workspace can take quite a long time if there is a big number of nodes and some search indexes involved. The reason is that the setup of the CachingIndexReader is processed using chunks of a certain size (actually 400K) in order to reduce the memory footprint. As soon as the number of documents exceeds this limit some operations (actually traversing complete indexes) are performed again and again.

It seems that the current algorithm ""initializeParents"" in the CachingIndexReader class can't be optimized without increasing the memory consumption. Therefore it should be a promising approach to persist the ""state"" of this class (actually it's main member array and map) and reload it on startup.

The ""load"" of the state can be done implicitly in the initializing phase of the cache. This is obvious. The correct point of time to call the ""save"" operation isn't obvious at all. I tried the ""doClose"" method of the class and it seems sufficient."
1,Index migration fails for property names that are prefixes of othersThe automatic index migration (JCR-1363) introduced in Jackrabbit version 1.5.0 replaces the separator char for named term text in PROPERTIES field. Util 1.4.x '\uFFFF' was used and after migration '[' is used. This changes the overall order of  PROPERTIES terms and leads to assertion failures. See attached test.
0,"Preserve whitespace in <code> sections in the Changes.html generated from CHANGES.txt by changes2html.plThe Trunk section of CHANGES.txt sports use of a new feature: <code> sections, for the two mentions of LUCENE-1575.

This looks fine in the text rendering, but looks crappy in the HTML version, since changes2html.pl escapes HTML metacharacters to appear as-is in the HTML rendering, but the newlines in the code are converted to a single space. 

I think this should be fixed by modifying changes2html.pl to convert <code> and </code> into (unescaped) <code><pre> and </pre></code>, respectively, since just passing through <code> and </code>, without </?pre>, while changing the font to monospaced (nice), still collapses whitespace (not nice). 

See the java-dev thread that spawned this issue here: http://www.nabble.com/CHANGES.txt-td23102627.html"
0,"Provide access to port of Host headerWe use a load balancer that connects to the HTTP server and the HTTP server
connects to the application server. We use port translation in our load
balancer. So when e.g. a client connects to 90 of the load balancer, the load
balancer connects to port 100 of the HTTP server. The load balancer doesn't
change the Host request header, so in the host request header is still the
original virtual host name and port, in this case port 90. For this reason, the
virtual hosts of the HTTP server and application server are configured based on
the external port numbers, so in this case port 90.
 
For test purposes, we sometimes want to connect directly to the HTTP server or
the application server, bypassing the load balancer. To do this, we need to
connect to the same port as the load balancer would, in this example port 100,
but the host header of this request should be the same as if the request would
go through the load balancer, so in this example port 90, because the HTTP
server and application server's virtual hosts are configured for this port.

The attached patch adds the possibility to specify the port number for virtual
hosts.

Here's a code snippet that uses the patched code:

HttpClient httpClient = new HttpClient();
HttpMethod method = new GetMethod();
HostConfiguration hostConfiguration = new HostConfiguration();
hostConfiguration.setHost(""localhost"", 80, ""http"");
HostParams params = new HostParams();
params.setVirtualHost(""localhost"");
params.setVirtualHostPort(100);
hostConfiguration.setParams(params);
httpClient.executeMethod(hostConfiguration, method);
System.out.println(method.getResponseBodyAsString());
method.releaseConnection();"
0,"Avoid creation of more than one jackrabbit instance with the same configurationbased on the mailing list archive, it seems new users often run more than one jackrabbit instance with the same configuration. I propose to lock the repository by creating an empty file called "".lock"" at the repository home on startup and remove it on shutdown.
If the lock file is found on jackrabbit startup the following message will be logged:
""The repository home at "" + home.getAbsolutePath() + "" appears to be in use. If you are sure it's not in use please delete the file at  "" + lock.getAbsolutePath() + "". Probably the repository was not shutdown properly.""
"
0,"refactoring of Similarity.sloppyFreq() and Similarity.scorePayloadCurrently these are top-level, but they only affect the SloppyDocScorer.
So it makes more sense to put these into the SloppyDocScorer api, this gives you additional flexibility
(e.g. combining payloads with CSF or whatever the hell you want to do), and is cleaner.

Furthermore, there are the following confusing existing issues:
* scorePayload should take bytesref
* PayloadTermScorer passes a *null* byte[] array to the sim if there are no payloads. I don't think it should do this, and its inconsistent with PayloadNearQuery, which does not do this. Its an undocumented conditional you need to have in the scoring algorithm which we should remove.
* there is an unused constant for scorepayload (NO_DOC_ID_PROVIDED), which is a documented, but never used anywhere. I think we should remove this conditional too, because its not possible to have a payload without a docid, and we shouldn't be passing fake document ids (-1) to our scoring APIs anyway.
"
1,"ClassCastException MultiReader(See original message below)
Sure.  'Bugzilla it', please.

Otis
P.S.
That line 274 should be line 273 in the CVS HEAD as of now.

--- Rasik Pandey <rasik.pandey@ajlsm.com> wrote:
> Howdy,
> 
> This exception was thrown with 1.4rc3. Do you need a test case for 
> this one?
> 
> java.lang.ClassCastException
>         at
> org.apache.lucene.index.MultiTermEnum.<init>(MultiReader.java:274)
>         at
> org.apache.lucene.index.MultiReader.terms(MultiReader.java:187)
> 
> 
> Regards,
> RBP
> 
> 
> 
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: lucene-dev-unsubscribe@jakarta.apache.org
> For additional commands, e-mail: lucene-dev-help@jakarta.apache.org
> 


---------------------------------------------------------------------
To unsubscribe, e-mail: lucene-dev-unsubscribe@jakarta.apache.org
For additional commands, e-mail: lucene-dev-help@jakarta.apache.org"
1,"'OR' in XPath query badly interpretedexecuting query: //*[@a=1 and @b=2 or @c=3] leads to creating wrong query tree. The builded tree looks like for query: //*[@a=1 and @b=2 and @c=3](see attachement). using brackets resolves the problem, but without brackets output query is different from input query. When AND and OR are switched(so the OR is in first palce - //*[@a=1 or @b=2 and @c=3]) everything is ok."
1,"Explanation.toHtml outputs invalid HTMLIf you want an HTML representation of an Explanation, you might call the toHtml() method.  However, the output of this method looks like the following:

<ul>
  <li>some value = some description</li>
  <ul>
    <li>some nested value = some description</li>
  </ul>
</ul>

As it is illegal in HTML to nest a UL directly inside a UL, this method will always output unparseable HTML if there are nested explanations.

What Lucene probably means to output is the following, which is valid HTML:

<ul>
  <li>some value = some description
    <ul>
      <li>some nested value = some description</li>
    </ul>
  </li>
</ul>
"
0,"Replace commons-logging with jcl-over-slf4j in jackrabbit-webdavIn JCR-1631 we moved the exclusion rule against the transitive commons-logging dependency from commons-httpclient to higher level components like jackrabbit-jca and jackrabbit-webapp.

However, I'm having some trouble with commons-logging showing up in other downstream projects that depend directly on jackrabbit-webapp or jackrabbit-jcr-server. Thus I'd like to revert the JCR-1631 solution and push the exclusion rule back to jackrabbit-webapp and replace the commons-logging dependency with jcl-over-slf4j. Downstream projects that already use commons-logging can still exclude the jcl-over-slf4j dependency."
0,"Logging documentation refers to nonexisting level ERROR in java.util.logging sectionPage ""Logging Practices""  Section ""java.util.logging Examples"" 
(http://hc.apache.org/httpcomponents-client/logging.html) 
says:
""org.apache.http.level = FINEST
org.apache.http.wire.level = ERROR""

However, there is no such thing as java.util.logging.Level.ERROR

Did you mean SEVERE as in
Logger.getLogger(""org.apache.http.wire"").setLevel(Level.SEVERE);

Thanks"
0,"ManageableCollectionUtil should not throw ""unsupported"" JcrMapping exceptionMany times, the object model'd code cannot be altered for ocm.

To avoid the ""unsupported"" exception in almost all such cases, use a delegating wrapper class to encapsulate a Collection.    The wrapper class implements MaangeableCollection.

Since delegation is a performance hit, make the test below the last resort for *object*  conversion in the method:
public static ManageableCollection getManageableCollection(Object object) 

Proposed ""catchall"" test and program action:

            if (object instanceof Collection) {
                return new ManageableCollectionImpl((Collection)object);
            }

"
0,HttpMethodBase Javadoc patchesClarify that HTTP 1.1 is the default. Attaching.
0,"Create OSGi Bundle Manifest HeadersTo be able to easily uses libraries from Jackrabbit inside an OSGi framework, for example in Apache Sling, it would be very helpfull if some of the Jackrabbit libraries include OSGi Bundle Manifest headers. It will of course not be possible to define such manifest header definition for all libraries, but jackrabbit-api, jackrabbit-jcr-commons and jackrabbit-jcr-rmi are certainly good candidates."
0,VFS backed file systemFile System implementation backed by commons VFS. 
0,"tweak AppendingCodec to write segments_N compatible with 'normal' LuceneJust an easy improvement from LUCENE-3490:

Currently AppendingCodec writes a different segments_N format (it writes no checksum at all in commit())
If you don't configure your codecprovider correctly in IndexReader, you will get read past EOF.
(we have some proposed fixes for this stuff in LUCENE-3490 branch)

But besides this, all it really needs to do is no-op prepareCommit(), it can still write the 'final' checksum
which is a good thing."
1,"[Lock] weird number for ""infinite""(this is a follow-up of JCR-3205)

i am surprised by the davex reply to a lock request with infinite timeout (before and after the fix from JCR-3205):


<D:timeout>Second-2147483</D:timeout>

this number is
2^21+50331

which seems pretty random to me. coincidally, this number is exactly 2^31 - 1 (2147483647) without the last 3 digits. can it be that there are some weird string operations happening on server side?
"
0,"TermsFilter: reuse TermDocsTermsFilter currently calls termDocs(Term) once per term in the TermsFilter.  If we sort the terms it's filtering on, this can be optimised to call termDocs() once and then skip(Term) once per term, which should significantly speed up this filter.
"
1,"FastVectorHighlighter SimpleBoundaryScanner does not work well when highlighting at the beginning of the text The SimpleBoundaryScanner still breaks text not based on characters provided when highlighting text that end up scanning to the beginning of the text to highlight. In this case, just use the start of the text as the offset."
0,"importXML still depends on XercesPrzemo Pakulski commented on JCR-367:
> Jackrabbit-core is still dependent on Xerces directly during runtime, SessionImpl.importWorkspace,
> Workspacempl.importWorkspace methods contains folliwng lines :
>
>             XMLReader parser =
>                     XMLReaderFactory.createXMLReader(""org.apache.xerces.parsers.SAXParser"");
>
> It works in maven1 probably because maven1 itself needs xerces to run test goal.
>
> I suggest reopening the issue.

Creating a new issue since JCR-367 is already closed after the 1.1 release."
1,"MultiPhraseQuery has incorrect hashCode() implementation - Leads to Solr Cache missesI found this while hunting for the cause of Solr Cache misses.

The MultiPhraseQuery class hashCode() implementation is non-deterministic. It uses termArrays.hashCode() in the computation. The contents of that ArrayList are actually arrays themselves, which return there reference ID as a hashCode instead of returning a hashCode which is based on the contents of the array. I would suggest an implementation involving the Arrays.hashCode() method.

I will try to submit a patch soon, off for today."
0,"Problematic exception handling in Jackrabbit WebAppIn this project, the cause of the exception is often ignored, and only the message of the cause is used, as in:

} catch (Exception e) {
    log.error(""Error in configuration: {}"", e.toString());
    throw new ServletException(""Error in configuration: "" + e.toString());
}

An additional problem is that when using ServletException(String message, Throwable rootCause), the rootCause is not used in printStackTrace(), that means the cause is not logged. See also: http://closingbraces.net/2007/11/27/servletexceptionrootcause/

It is therefore better to convert 
  throw new ServletException(""Unable to create RMI repository. jcr-rmi.jar might be missing."", e);
to
  ServletException s = new ServletException(""Unable to create RMI repository. jcr-rmi.jar might be missing."");
  s.initCause(e);
  throw s;




"
1,StackOverflowError if too many versions of a node are createdIn org.apache.jackrabbit.core.version.VersionIteratorImpl addVersion() is called recursively which can cause StackOverflowErrors if there are too many versions.
0,"Proxy authentication does not handle multiple multiple authentication schemesMy proxy server returns the following header lines in the response:

    Proxy-Authenticate: NTLM
    Proxy-Authenticate: Basic realm=""10.105.20.201""

i.e., it returns two Proxy-Authenticate header lines. Unfortunately this does 
not work. In line 253 of class Authenticator (method: authenticate(HttpMethod, 
HttpState, Header, String)) I see this comment:

    // FIXME: Note that this won't work if there is more than one realm within 
the challenge

so it looks like this is something that isn't yet implemented. In the log, I 
can see that the Authenticator attempts to parse the realm, but it looks like
this is not being done correctly:

   411 DEBUG Attempting to authenticate challenge: Proxy-Authenticate: NTLM, 
Basic realm=""10.105.20.201""

   411 DEBUG Parsed realm ""ealm=""10.105.20.201"" from challenge ""NTLM, Basic 
realm=""10.105.20.201"""".
   421 WARN  Exception thrown authenticating
java.lang.UnsupportedOperationException: Authentication type ""NTLM,"" is not 
recognized.
    at org.apache.commons.httpclient.Authenticator.authenticate
(Authenticator.java:274)
    at org.apache.commons.httpclient.Authenticator.authenticateProxy
(Authenticator.java:178)
    at 
org.apache.commons.httpclient.HttpMethodBase.processAuthenticationResponse
(HttpMethodBase.java:580)
    at org.apache.commons.httpclient.HttpMethodBase.execute
(HttpMethodBase.java:668)
    at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:355)
    at com.cmg.httptest.Main.main(Main.java:34)

It looks wrong to me that the realm name seems to be parsed as: 
ealm=""10.105.20.201

I understand that Authenticator does not know what NTLM is but I would like it 
to use Basic authentication in this case.

If there are more authentication methods possible, how can I specify which one 
I want to use?

Jesper de Jong"
0,"optional normsFor applications with many indexed fields, the norms cause memory problems both during indexing and querying.
This patch makes norms optional on a per-field basis, in the same way that term vectors are optional per-field.

Overview of changes:
 - Field.omitNorms that defaults to false
 - backward compatible lucene file format change: FieldInfos.FieldBits has a bit for omitNorms
 - IndexReader.hasNorms() method
 - During merging, if any segment includes norms, then norms are included.
 - methods to get norms return the equivalent 1.0f array for backward compatibility

The patch was designed for backward compatibility:
 - all current unit tests pass w/o any modifications required
 - compatible with old indexes since the default is omitNorms=false
 - compatible with older/custom subclasses of IndexReader since a default hasNorms() is provided
 - compatible with older/custom users of IndexReader such as Weight/Scorer/explain since a norm array is produced on demand, even if norms were not stored

If this patch is accepted (or if the direction is acceptable), performance for scoring  could be improved by assuming 1.0f when hasNorms(field)==false.
"
0,"Site search powered by Lucene/SolrFor a number of years now, the Lucene community has been criticized for not eating our own ""dog food"" when it comes to search. My company has built and hosts a site search (http://www.lucidimagination.com/search) that is powered by Apache Solr and Lucene and we'd like to donate it's use to the Lucene community. Additionally, it allows one to search all of the Lucene content from a single place, including web, wiki, JIRA and mail archives. See also http://www.lucidimagination.com/search/document/bf22a570bf9385c7/search_on_lucene_apache_org

You can see it live on Mahout, Tika and Solr

Lucid has a fault tolerant setup with replication and fail over as well as monitoring services in place. We are committed to maintaining and expanding the search capabilities on the site.

The following patch adds a skin to the Forrest site that enables the Lucene site to search Lucene only content using Lucene/Solr. When a search is submitted, it automatically selects the Lucene facet such that only Lucene content is searched. From there, users can then narrow/broaden their search criteria.


I plan on committing in a 3 or 4 days."
0,"bad assumptions/error handling in SetValueVersionExceptionTest:SetValueVersionExceptionTest makes several assumptions that may not be true in all repositories:

- nodes can be created without specifying a node type
- nodes are not referenceable by default (thus addMixin fails)

Also, if a repository does not allow creating a reference property, the associated test should be aborted with NotExecutableException.
"
0,"add getFinalOffset() to TokenStreamIf you add multiple Fieldable instances for the same field name to a document, and you then index those fields with TermVectors storing offsets, it's very likely the offsets for all but the first field instance will be wrong.

This is because IndexWriter under the hood adds a cumulative base to the offsets of each field instance, where that base is 1 + the endOffset of the last token it saw when analyzing that field.

But this logic is overly simplistic.  For example, if the WhitespaceAnalyzer is being used, and the text being analyzed ended in 3 whitespace characters, then that information is lost and then next field's offsets are then all 3 too small.  Similarly, if a StopFilter appears in the chain, and the last N tokens were stop words, then the base will be 1 + the endOffset of the last non-stopword token.

To fix this, I'd like to add a new getFinalOffset() to TokenStream.  I'm thinking by default it returns -1, which means ""I don't know so you figure it out"", meaning we fallback to the faulty logic we have today.

This has come up several times on the user's list."
0,ConsistencyCheck uses too much memoryA consistency check loads all lucene documents into memory. On a large repository this may lead to an OutOfMemoryError. The consistency check should rather read the lucene documents on demand and discard them afterwards.
0,"Demo HTML parser doesn't work for international documentsJavacc assumes ASCII so it won't work with, say, japanese documents. Ideally it would read the charset from the HTML markup, but that can by tricky. For now assuming unicode would do the trick:

Add the following line marked with a + to HTMLParser.jj:

options {
  STATIC = false;
  OPTIMIZE_TOKEN_MANAGER = true;
  //DEBUG_LOOKAHEAD = true;
  //DEBUG_TOKEN_MANAGER = true;
+  UNICODE_INPUT = true;
}
"
0,"Provide general HTTP date parsingAdd generally accessible support for parsing HTTP dates as used in headers/cookies.

Initially submitted to HttpClient dev by Chris Brown."
0,"improve efficiency of snowballfiltersnowball stemming currently creates 2 new strings and 1 new stringbuilder for every word.

all of this is unnecessary, so don't do it.
"
1,o.a.j.spi.commons.nodetype.NodeTypeDefinitionFactory does not set required typeNodeTypeDefinitionFactory does not set required type for property definitions.
0,"Add numDeletedDocs to IndexReaderAdd numDeletedDocs to IndexReader. Basically, the implementation is as simple as doing:
public int numDeletedDocs() {
  return deletedDocs == null ? 0 : deletedDocs.count();
}
in SegmentReader.
Patch to follow to include in all IndexReader extensions."
0,"Add utitily class to manage NRT reopeningI created a simple class, NRTManager, that tries to abstract away some
of the reopen logic when using NRT readers.

You give it your IW, tell it min and max nanoseconds staleness you can
tolerate, and it privately runs a reopen thread to periodically reopen
the searcher.

It subsumes the SearcherManager from LIA2.  Besides running the reopen
thread, it also adds the notion of a ""generation"" containing changes
you've made.  So eg it has addDocument, returning a long.  You can
then take that long value and pass it back to the getSearcher method
and getSearcher will return a searcher that reflects the changes made
in that generation.

This gives your app the freedom to force ""immediate"" consistency (ie
wait for the reopen) only for those searches that require it, like a
verifier that adds a doc and then immediately searches for it, but
also use ""eventual consistency"" for other searches.

I want to also add support for the new ""applyDeletions"" option when
pulling an NRT reader.

Also, this is very new and I'm sure buggy -- the concurrency is either
wrong over overly-locking.  But it's a start...
"
1,"Thai token type() bugWhile adding tests for offsets & type to ThaiAnalyzer, i discovered it does not type Thai numeric digits correctly.
ThaiAnalyzer uses StandardTokenizer, and this is really an issue with the grammar, which adds the entire [:Thai:] block to ALPHANUM.

i propose that alphanum be described a little bit differently in the grammar.
Instead, [:letter:] should be allowed to have diacritics/signs/combining marks attached to it.

this would allow the [:thai:] hack to be completely removed, would allow StandardTokenizer to parse complex writing systems such as Indian languages, and would fix LUCENE-1545.
"
0,"deprecate IndexWriter.addIndexes(Directory[])Since addIndexesNoOptimize accomplishes the same thing, more efficiently, and you can always then call optimize() if you really wanted to, I think we should deprecate the older addIndexes(Directory[])."
0,"Token implementation needs improvementsThis was discussed in the thread (not sure which place is best to reference so here are two):
http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200805.mbox/%3C21F67CC2-EBB4-48A0-894E-FBA4AECC0D50@gmail.com%3E
or to see it all at once:
http://www.gossamer-threads.com/lists/lucene/java-dev/62851

Issues:
1. JavaDoc is insufficient, leading one to read the code to figure out how to use the class.
2. Deprecations are incomplete. The constructors that take String as an argument and the methods that take and/or return String should *all* be deprecated.
3. The allocation policy is too aggressive. With large tokens the resulting buffer can be over-allocated. A less aggressive algorithm would be better. In the thread, the Python example is good as it is computationally simple.
4. The parts of the code that currently use Token's deprecated methods can be upgraded now rather than waiting for 3.0. As it stands, filter chains that alternate between char[] and String are sub-optimal. Currently, it is used in core by Query classes. The rest are in contrib, mostly in analyzers.
5. Some internal optimizations can be done with regard to char[] allocation.
6. TokenStream has next() and next(Token), next() should be deprecated, so that reuse is maximized and descendant classes should be rewritten to over-ride next(Token)
7. Tokens are often stored as a String in a Term. It would be good to add constructors that took a Token. This would simplify the use of the two together.
"
0,"TCK: XPathQueryLevel2Test uses optional column specifier syntaxTest assumes the implementation uses a terminal attribute step as the column specifier.  This is allowed, but not required, by JCR.

Proposal: remove column specifier and process results using getNodes instead of getRows.

--- XPathQueryLevel2Test.java   (revision 422074)
+++ XPathQueryLevel2Test.java   (working copy)
@@ -85,7 +85,7 @@
         checkResult(result, 1);
  
         // evaluate result
-        checkValue(result.getRows(), propertyName1, ""b"");
+        checkValue(result.getNodes(), propertyName1, ""b"");
     }
  
     /**
@@ -101,7 +101,7 @@
         checkResult(result, 1);
  
         // evaluate result
-        checkValue(result.getRows(), propertyName1, ""existence"");
+        checkValue(result.getNodes(), propertyName1, ""existence"");
     }
  
     /**
@@ -147,7 +147,6 @@
         tmp.append(jcrRoot).append(testRoot);
         tmp.append(""/*[@"").append(propertyName2).append("" = 'two'"");
         tmp.append("" and @"").append(propertyName1).append("" = 'existence']"");
-        tmp.append(""/@"").append(propertyName1);
         return new Statement(tmp.toString(), Query.XPATH);
     }
  
@@ -161,7 +160,7 @@
         tmp.append(propertyName1);
         tmp.append("" <= 'b' and @"");
         tmp.append(propertyName1);
-        tmp.append("" > 'a']/@"").append(propertyName1);
+        tmp.append("" > 'a']"");
         return new Statement(tmp.toString(), Query.XPATH);
     }
 }

--- AbstractQueryLevel2Test.java        (revision 422074)
+++ AbstractQueryLevel2Test.java        (working copy)
@@ -19,6 +19,7 @@
 import org.apache.jackrabbit.test.NotExecutableException;
  
 import javax.jcr.nodetype.NodeType;
+import javax.jcr.NodeIterator;
 import javax.jcr.query.RowIterator;
 import javax.jcr.query.Row;
 import javax.jcr.Value;
@@ -115,4 +116,16 @@
                     expectedValue, value.getString());
         }
     }
+
+    protected void checkValue(NodeIterator itr,
+                              String propertyName,
+                              String expectedValue) throws RepositoryException {
+        while (itr.hasNext()) {
+            Node node = itr.nextNode();
+            // check fullText
+            Value value = node.getProperty(propertyName).getValue();
+            assertEquals(""Value in query result row does not match expected value"",
+                    expectedValue, value.getString());
+        }
+    }
 }
"
0,"Add fake charfilter to BaseTokenStreamTestCase to find offsets bugsRecently lots of issues have been fixed about broken offsets, but it would be nice to improve the
test coverage and test that they work across the board (especially with charfilters).

in BaseTokenStreamTestCase.checkRandomData, we can sometimes pass the analyzer a reader wrapped
in a ""MockCharFilter"" (the one in the patch sometimes doubles characters). If the analyzer does
not call correctOffsets or does incorrect ""offset math"" (LUCENE-3642, etc) then eventually
this will create offsets and the test will fail.

Other than tests bugs, this found 2 real bugs: ICUTokenizer did not call correctOffset() in its end(),
and ThaiWordFilter did incorrect offset math."
0,"Large fetch sizes have potentially deleterious effects on VM memory requirements when using OracleSince Release 10g, Oracle JDBC drivers use the fetch size to allocate buffers for caching row data.
cf. http://www.oracle.com/technetwork/database/enterprise-edition/memory.pdf

r1060431 hard-codes the fetch size for all ResultSet-returning statements to 10,000. This value has significant, potentially deleterious, effects on the heap space required for even moderately-sized repositories. For example, the BUNDLE table (from 'oracle.ddl') has two columns -- NODE_ID raw(16) and BUNDLE_DATA blob -- which require 16 b and 4 kb of buffer space, respectively. This requires a buffer of more than 40 mb [(16+4096) * 10000 = 41120000].

If the issue described in JCR-2832 is truly specific to PostgreSQL, I think its resolution should be moved to a PostgreSQL-specific ConnectionHelper subclass. Failing that, there should be a way to override this hard-coded value in OracleConnectionHelper."
1,"IndexWriter applies wrong deletes during concurrent flush-allYonik uncovered this with the TestRealTimeGet test: if a flush-all is
underway, it is possible for an incoming update to pick a DWPT that is
stale, ie, not yet pulled/marked for flushing, yet the DW has cutover
to a new deletes queue.  If this happens, and the deleted term was
also updated in one of the non-stale DWPTs, then the wrong document is
deleted and the test fails by detecting the wrong value.

There's a 2nd failure mode that I haven't figured out yet, whereby 2
docs are returned when searching by id (there should only ever be 1
doc since the test uses updateDocument which is atomic wrt
commit/reopen).

Yonik verified the test passes pre-DWPT, so my guess is (but I
have yet to verify) this test also passes on 3.x.  I'll backport
the test to 3.x to be sure.
"
1,"Destination header not containing URI scheme causes NPEIn WebDAVRequestImpl. getDestinationLocator assumes that URI.getAuthority is always non-null.

In RFC2518, a full URI is indeed required, but the NPE causes a status of 500, instead of 400 as expected.

In RFC4918, an absolute path is allowed.

Proposal: delegate to gethrefLocator, which already does the right thing.
"
0,Improved javadocs for PriorityQueue#lessThanIt kills me that I have to inspect the code every time I implement a PriorityQueue. :)
0,"Add tool to upgrade all segments of an index to last recent supported index format without optimizingCurrently if you want to upgrade an old index to the format of your current Lucene version, you have to optimize your index or use addIndexes(IndexReader...) [see LUCENE-2893] to copy to a new directory. The optimize() approach fails if your index is already optimized.

I propose to add a custom MergePolicy to upgrade all segments to the last format. This MergePolicy could simply also ignore all segments already up-to-date. All segments in prior formats would be merged to a new segment using another MergePolicy's optimize strategy.

This issue is different from LUCENE-2893, as it would only support upgrading indexes from previous Lucene versions in-place using the official path. Its a tool for the end user, not a developer tool.

This addition should also go to Lucene 3.x, as we need to make users with pre-3.0 indexes go the step through 3.x, else they would not be able to open their index with 4.0. With this tool in 3.x the users could safely upgrade their index without relying on optimize to work on already-optimized indexes."
1,"PredefinedNodeTypeTest..getNodeTypeSpec handling unknown super typesThis method tries to filter out custom super types, but produces a broken spec when *all* super types are custom (in which case it should emit ""[]"", but doesn't).
"
0,"Searchable.java: The info in the @deprecated tags do not refer to the search(Weight, etc...) versions...E.g.

The javadoc for 
          void search(Query query, Filter filter, HitCollector results)
states:
          Deprecated. use search(Query, Filter, HitCollector) instead.
instead of:
          Deprecated. use search(Weight, Filter, HitCollector) instead.
"
0,"QueryParser should use reusable token streamsJust like indexing, the query parser should use reusable token streams"
0,"Implement getDistance() on DirectSpellChecker.INTERNAL_LEVENSHTEINDirectSpellChecker.INTERNAL_LEVENSHTEIN is currently not a full-fledged implementation of StringDistance.  But an full implementation is needed for Solr's SpellCheckComponent.finishStage(), and also would be helpful for those trying to take the advice given in LIA 2nd ed section sect8.5.3."
0,"Refactor ObservationManagerFactoryThe current o.a.j.core.observation.ObservationManagerFactory class has two main responsibilities:

    1) Create new ObservationManagerImpl instances as an observation manager factory
    2) Manage event consumers and dispatch events within a workspace

These two responsibilities are quite unrelated and the factory responsibility essentially boils down to the following method that is only ever invoked within WorkspaceImpl.getObservationManager():

    public ObservationManagerImpl createObservationManager(SessionImpl session, ItemManager itemMgr) {
        return new ObservationManagerImpl(this, session, itemMgr);
    }

To simplify the design I'd inline this method and rename ObservationManagerFactory to ObservationDispatcher to better reflect the one remaining responsibility."
0,"change jdk & icu collation to use byte[]Now that term is byte[], we should switch collation to use byte[] instead of 'indexablebinarystring'.

This is faster and results in much smaller sort keys.

I figure we can work it out here, and fix termrangequery to use byte in parallel, but we can already test sorting etc now."
1,"InputStream.read return value is ignored.RepositoryImpl.loadRootNodeId reads from an InputStreamReader using read(...) and ignores the return values.

This results in a problem if the input stream doesn't read all bytes."
0,"Very inefficient implementation of MultiTermDocs.skipToIn our application anytime the index was unoptimized/contained more than one segment there was a sharp drop in performance, which amounted to over 50ms per search on average.  We would consistently see this drop anytime an index went from an optimized state to an unoptimized state.

I tracked down the issue to the implementation of MultiTermDocs.skipTo function (found in MultiReader.java).  Optimized indexes do not use this class during search but unoptimized indexes do.  The comment on this function even explicitly states 'As yet unoptimized implementation.'  It was implemented just by calling 'next' over and over so even if it knew it could skip ahead hundreds of thousands of hits it would not.

So I re-implemented the function very similar to how the MultiTermDocs.next function was implemented and tested it out on or application for correctness and performance and it passed all our tests and the performance penalty of having multiple segments vanished.  We have already put the new jar onto our production machines.

Here is my implementation of skipTo, which closely mirrors the accepted implementation of 'next', please feel free to test it and commit it.

  /** Much more optimized implementation. Could be
   * optimized fairly easily to skip entire segments */
  public boolean skipTo(int target) throws IOException {
    if (current != null && current.skipTo(target-base)) {
      return true;
    } else if (pointer < readers.length) {
      base = starts[pointer];
      current = termDocs(pointer++);
      return skipTo(target);
    } else
      return false;
  }"
0,"DataStore: gc.stopScan() should be optionalData Store garbage collection currently works like this:

gc.scan();
gc.stopScan();
gc.deleteUnused();

Currently, if stopScan() is not called, an exception is thrown. This is not user friendly. Instead, stopScan() should be optional, and should be allowed any time. It may be used to indicate garbage collection has finished:

try {
  gc.scan();
  ...
  gc.deleteUnused();
} finally {
  gc.stopScan();
}

Or when sharing the repository:

gc1.scan();
gc2.scan();
gc1.deleteUnused();
gc2.stopScan();
"
1,"OutOfMemoryError When repeat login and the logout many timesWhen repeat login and the logout many times, I encountered?OutOfMemoryError.

javax.jcr.RepositoryException: Cannot instantiate persistence manager org.apache.jackrabbit.core.state.db.DerbyPersistenceManager: Java exception: 'Java heap space: java.lang.OutOfMemoryError'.: Java exception: 'Java heap space: java.lang.OutOfMemoryError'.
	at org.apache.jackrabbit.core.RepositoryImpl.createPersistenceManager(RepositoryImpl.java:1095)
	at org.apache.jackrabbit.core.RepositoryImpl.createVersionManager(RepositoryImpl.java:300)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:245)
	at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:498)
	at org.apache.jackrabbit.core.TransientRepository$2.getRepository(TransientRepository.java:245)
	at org.apache.jackrabbit.core.TransientRepository.startRepository(TransientRepository.java:265)
	at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:333)
	at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:388)
	at Test.tryLoginAndLogout(Test.java:19)
	at Test.test1(Test.java:13)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:164)
	at junit.framework.TestCase.runBare(TestCase.java:130)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:120)
	at junit.framework.TestSuite.runTest(TestSuite.java:230)
	at junit.framework.TestSuite.run(TestSuite.java:225)
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:128)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)
Caused by: SQL Exception: Java exception: 'Java heap space: java.lang.OutOfMemoryError'.
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.javaException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.<init>(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement20.<init>(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement30.<init>(Unknown Source)
	at org.apache.derby.jdbc.Driver30.newEmbedPreparedStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.prepareStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.prepareStatement(Unknown Source)
	at org.apache.jackrabbit.core.state.db.DatabasePersistenceManager.init(DatabasePersistenceManager.java:224)
	at org.apache.jackrabbit.core.RepositoryImpl.createPersistenceManager(RepositoryImpl.java:1091)
	... 27 more
java.lang.OutOfMemoryError: Java heap space

"
0,"better payload testing with mockanalyzerMockAnalyzer currently always indexes some fixed-length payloads.

Instead it should decide for each field randomly (and remember it for that field):
* if the field should index no payloads at all
* field should index fixed length payloads
* field should index variable length payloads.
"
0,The DisjunctionMaxQuery lacks an implementation of extractTerms().The DisjunctionMaxQuery lacks an implementation of extractTerms(). 
1,"ArrayIndexOutofBoundException while setting a reference propertyI have a node whith a multivalued reference property.
I try to add a reference as follows (the spec is outdated at this point, so I'm not sure if I use the right approach to add a reference):

  ReferenceValue rv = new ReferenceValue(rn.getNode(""pages/mjo:page""));
  Value[] values = {rv};
  tstN.setProperty(""mjo:testCon"",values);
  session.save();

This results in a 

2004-12-09 16:11:43,614 WARN org.apache.jackrabbit.core.ItemManager - node at /pages/mjo:page has invalid definitionId (1512950840)
java.lang.ArrayIndexOutOfBoundsException: 0
	at org.apache.jackrabbit.core.ItemImpl.validateTransientItems(ItemImpl.java:626)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1148)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:633)
	at de.freaquac.test.JCRTest.main(JCRTest.java:174)

ArrayIndexOutOfBoundsException is never a good sign, so I assume it's a bug. I should say that crx throws the same exception if I try it there.
"
0,"Use hash codes instead of sequence numbers for string indexesWe use index numbers instead of namespace URIs or other strings in many places. The two-way mapping between namespace URIs and index numbers is by default stored in the repository-global ns_idx.properties file, and the index numbers are allocated using a linear sequence. The problem with this approach is that two repositories will easily end up with different string index mappings, which makes it practically impossible to make low-level copies of workspace content across repositories.

The ultimate solution for this problem would be to store the namespace URIs closer to the stored content, ideally as an implementation detail of a persistence manager.

An easier short-term solution would be to decrease the chances of two repositories having different string index mappings. A simple (and backwards-compatible) way to do this is to use the hash code of a namespace URI as the basis of allocating a new index number. Hash collisions are fairly unlikely, and can be handled by incrementing the intial hash code until the collision is avoided. In the common case of no collisions (with a uniform hash function the chance of a collision is less than 1% even with tousands of registered namespaces) this solution allows workspaces to be copied between repositories without worrying about the namespace index mappings."
0,"Upload Lucene 2.0 artifacts in the Maven 1 repositoryThe Lucene 2.0 artifacts can be found in the Maven 2 repository, but not in the Maven 1 repository. There are still projects using Maven 1 who might be interested in upgrading to Lucene 2, so having the artifacts also in the Maven 1 repository would be very helpful."
0,Improve aggregate node indexing codeCurrently the aggregate nodes indexing code uses a sub-optimal way of copying and sorting the aggregated fields.
0,"move log4j initialization out of RepositoryStartupServletthe RepositoryStartupServlet initializes/configures the Log4J environment. this might not be desirable since other applications might already done so.

"
1,"KeywordTokenizer/Analyzer cannot be re-used
The new reusableTokenStream API in KeywordAnalyzer fails to reset the tokenizer when it re-uses it.

This issue came from this thread:

    http://www.gossamer-threads.com/lists/lucene/java-dev/55929

Thanks to Hideaki Takahashi for finding this!"
1,"NodeIdImpl is not really serializable I've been trying to get jcr2spi - rmi - spi2jcr to work. 

The error I'm seeing is reported as:
java.io.NotSerializableException: org.apache.jackrabbit.spi.commons.identifier.IdFactoryImpl

I believe I tracked this down.  It is because NodeIdImpl is implicitly referencing its containing instance IdFactoryImpl which is not serializable.

NodeIdImpl is attempted to be serialized, in my case, with the following stack:

at org.apache.jackrabbit.spi.rmi.client.ClientRepositoryService.getItemInfos(ClientRepositoryService.java:258)
at org.apache.jackrabbit.jcr2spi.state.WorkspaceItemStateFactory.createNodeState(WorkspaceItemStateFactory.java:94)
at org.apache.jackrabbit.jcr2spi.state.TransientISFactory.createNodeState(TransientISFactory.java:99)
at org.apache.jackrabbit.jcr2spi.hierarchy.NodeEntryImpl.doResolve(NodeEntryImpl.java:972)
at org.apache.jackrabbit.jcr2spi.hierarchy.HierarchyEntryImpl.resolve(HierarchyEntryImpl.java:95)
at org.apache.jackrabbit.jcr2spi.hierarchy.HierarchyEntryImpl.getItemState(HierarchyEntryImpl.java:212)
at org.apache.jackrabbit.jcr2spi.ItemManagerImpl.getItem(ItemManagerImpl.java:170)
at org.apache.jackrabbit.jcr2spi.SessionImpl.getRootNode(SessionImpl.java:216)

I think I must be doing something wrong, because it seems like this is a fundamental problem with doing jcr2spi - rmi - spi2jcr, and looking at the SVN history I don't see how this ever could have worked.  
So either session.getRootNode() has never been tested using jcr2spi - rmi - spi2jcr, or I've got something setup wrong."
1,"Duplicate request headers when connect through proxiesWhen negotiating proxy servers or during write-failure retries, the httpClient 
adds duplicate request headers to each retry.  The result is that each header 
is duplicated multiple times (number of retries).  This only impact the headers 
that allow multiple values (the others were prevented byt the code).  In  
Particular, it affects ""cookie"" header.  It happens more often when going 
through proxy server with tunnelling connections (https), but also happens on 
http on NTLM proxy server (need multiple round trips to get authenticated).

Steps to Reproduce:
Setup a client application to go to a website that requires going through a 
proxy server that supports tunnelling for https connections and authenticate 
users (Basic and/or NTLM).  The website also need to support keep-alive and 
support https. Initilize the HttpState with a cookie. Turn httpClient 
logging ""wire, debug and trace"" logging on.  Set the proxy credential with 
valid user id and password.

Then run the application against any url on the website.

Test Results and fixes:
1. When connect to https through a (Netscape/Basic auth) proxy that does 
the ""tunnelling"", the initial ""CONNECT"" adds the cookie header once.  The proxy 
returns the 407.  Then the code will use the proxy credential to do 
the ""CONNECT"" again.  After successful connection (200), the code will do the 
proper ""POST"".  The httpClient code adds the same cookie one more time in here.

This case was caused by the wrapper class ConnectMethod using the wrapped 
method ""addRequestHeaders"" to build headers for the ""CONENCT"".  It can be fixed 
by having the ConnectMethod only adds headers it needs (""addHostRequestHeader"" 
and ""addProxyAuthorizationRequestHeader"").

2. When connect to http through proxy.  The client first sends a ""POST"" without 
proxy credentials, and gets a 407 back.  The cookie header is added before that 
happens.  Then (loop in the HttpMethodBase.execute) the client will retry 
the ""POST"" with the credentials (the logic require to have a response header to 
submit credentials).  In the retry, the cookie is added again 
(addRequestHeaders is called inside the writeRequest).

3. When using kee-alive with no-proxy on http, if the connection times out, the 
next call of the method will get a socket error on write.  The retry loop in 
the processRequest method will retry the request again.  It will add the cookie 
again.  To deal with both case 2 and 3, the addRequestHeadres call need to be 
moved up to the beginning of the HttpMethodBase.execute.  We tested this 
approach and it worked for us.

4. When negotiating NTLM proxy for https, multiple round trips are needed to 
get user authenticated.  The same ConnectMethod instance is used.  So the 
adding of the proxy authenticate headers need to be done with the ConnectMethod 
instance (vs. the wrapped method for Host header).  Otherwise, the 
Authenticator class would not be able to find the information for 
authenticating user.


I will send in our suggested fixes later.

Build: This is based on 0307 nightly build.  We run into some other problems 
when trying the 0410 nightly build."
0,"improve BaseTokenStreamTestCase to test end()If offsetAtt/end() is not implemented correctly, then there can be problems with highlighting: see LUCENE-2207 for an example with CJKTokenizer.

In my opinion you currently have to write too much code to test this.

This patch does the following:
* adds optional Integer finalOffset (can be null for no checking) to assertTokenStreamContents
* in assertAnalyzesTo, automatically fill this with the String length()

In my opinion this is correct, for assertTokenStreamContents the behavior should be optional, it may not even have a Tokenizer. If you are using assertTokenStreamContents with a Tokenizer then simply provide the extra expected value to check it.

for assertAnalyzesTo then it is implied there is a tokenizer so it should be checked.

the tests pass for core but there are failures in contrib even besides CJKTokenizer (apply Koji's patch from LUCENE-2207, it is correct). Specifically ChineseTokenizer has a similar problem.
"
0,Refrase javadoc 1st sentence for IndexReader.deleteDocuments
1,"NullPointerException may be thrown when trying to enumerate observation event listenersWhen calling the ObservationManager.getRegisteredEventListeners() a NullPointerException may be thrown if no event listener has been registered (yet). The reason for this is, that in the ClientObservationManager.getRegisteredEventListener method an internal field is access directly, which is created on demand and thus may be null."
0,Remove unnecessary memory barriers in DWPTCurrently DWPT still uses AtomicLong to count the bytesUsed. Each write access issues an implicite memory barrier which is totally unnecessary since we doing everything single threaded on that level. This might be very minor but we shouldn't issue unnecessary memory barriers causing processors to lock their instruction pipeline for no reason.
0,"relax the per-segment max unique term limitLucene can't handle more than 2.1B (limit of signed 32 bit int) unique terms in a single segment.

But I think we can improve this to termIndexInterval (default 128) * 2.1B.  There is one place (internal API only) where Lucene uses an int but should use a long."
0,use AtomicInteger/Boolean to track IR.refCount and IW.closedLess costly than synchronized methods we have now...
0,Add link to irc channel #lucene on the websiteWe should add a link to #lucene IRC channel on chat.freenode.org. 
1,"DefaultSimilarity.queryNorm() should never return InfinityCurrently DefaultSimilarity.queryNorm() returns Infinity if sumOfSquaredWeights=0.
This can result in a score of NaN (e. g. in TermScorer) if boost=0.0f.

A simple fix would be to return 1.0f in case zero is passed in.

See LUCENE-698 for discussions about this."
1,"dead lock while locking or unlocking nodesJackRabbit is still hanging on the Node.lock() or Node.unlock() function.

... everything fine until here...
s13: 4
s13: 5
s13: 6
s13: 7   -> unlock()
s14: started.
s14: 1   -> session.getRootNode()
s15: started.
s15: 1
s16: started.

I just find this failure during the first run (emtpy repository home directory). 2nd and 3th run are fine after killing the vm from first run, but with already initialized repository directory these time.

1. rm -rf repository.home
2. run -> hang
3. kill
4. run -> ok
5. run -> ok
"
0,"org.apache.lucene.ant.HtmlDocument added Tidy config file passthrough availabilityParsing HTML documents using the org.apache.lucene.ant.HtmlDocument.Document method resulted in many error messages such as this:

    line 152 column 725 - Error: <as-html> is not recognized!
    This document has errors that must be fixed before
    using HTML Tidy to generate a tidied up version.

The solution is to configure Tidy to accept these abnormal tags by adding the tag name to the ""new-inline-tags"" option in the Tidy config file (or the command line which does not make sense in this context), like so:

    new-inline-tags: as-html

Tidy needs to know where the configuration file is, so a new constructor and Document method can be added.  Here is the code:

{code}
    /**                                                                                                                                                                                            
     *  Constructs an <code>HtmlDocument</code> from a {@link                                                                                                                                      
     *  java.io.File}.                                                                                                                                                                             
     *                                                                                                                                                                                             
     *@param  file             the <code>File</code> containing the                                                                                                                                
     *      HTML to parse                                                                                                                                                                          
     *@param  tidyConfigFile   the <code>String</code> containing                                                                                                                                  
     *      the full path to the Tidy config file                                                                                                                                                  
     *@exception  IOException  if an I/O exception occurs                                                                                                                                          
     */
    public HtmlDocument(File file, String tidyConfigFile) throws IOException {
        Tidy tidy = new Tidy();
        tidy.setConfigurationFromFile(tidyConfigFile);
        tidy.setQuiet(true);
        tidy.setShowWarnings(false);
        org.w3c.dom.Document root =
                tidy.parseDOM(new FileInputStream(file), null);
        rawDoc = root.getDocumentElement();
    }

    /**                                                                                                                                                                                            
     *  Creates a Lucene <code>Document</code> from a {@link                                                                                                                                       
     *  java.io.File}.                                                                                                                                                                             
     *                                                                                                                                                                                             
     *@param  file                                                                                                                                                                                 
     *@param  tidyConfigFile the full path to the Tidy config file                                                                                                                                 
     *@exception  IOException                                                                                                                                                                      
     */
    public static org.apache.lucene.document.Document
        Document(File file, String tidyConfigFile) throws IOException {

        HtmlDocument htmlDoc = new HtmlDocument(file, tidyConfigFile);

        org.apache.lucene.document.Document luceneDoc = new org.apache.lucene.document.Document();

        luceneDoc.add(new Field(""title"", htmlDoc.getTitle(), Field.Store.YES, Field.Index.ANALYZED));
        luceneDoc.add(new Field(""contents"", htmlDoc.getBody(), Field.Store.YES, Field.Index.ANALYZED));

        String contents = null;
        BufferedReader br =
            new BufferedReader(new FileReader(file));
        StringWriter sw = new StringWriter();
        String line = br.readLine();
        while (line != null) {
            sw.write(line);
            line = br.readLine();
        }
        br.close();
        contents = sw.toString();
        sw.close();

        luceneDoc.add(new Field(""rawcontents"", contents, Field.Store.YES, Field.Index.NO));

        return luceneDoc;
    }
{code}

I am using this now and it is working fine.  The configuration file is being passed to Tidy and now I am able to index thousands of HTML pages with no more Tidy tag errors.

"
0,"Make Authorizable.setProperty more noisy in case of failuressetProperty fails with an unspecific warning, when there's an exception, but it doesn' print any useful information from this exception."
0,"IntelliJ IDEA and Eclipse setupSetting up Lucene/Solr in IntelliJ IDEA or Eclipse can be time-consuming.

The attached patches add a new top level directory {{dev-tools/}} with sub-dirs {{idea/}} and {{eclipse/}} containing basic setup files for trunk, as well as top-level ant targets named ""idea"" and ""eclipse"" that copy these files into the proper locations.  This arrangement avoids the messiness attendant to in-place project configuration files directly checked into source control.

The IDEA configuration includes modules for Lucene and Solr, each Lucene and Solr contrib, and each analysis module.  A JUnit run configuration per module is included.

The Eclipse configuration includes a source entry for each source/test/resource location and classpath setup: a library entry for each jar.

For IDEA, once {{ant idea}} has been run, the only configuration that must be performed manually is configuring the project-level JDK.  For Eclipse, once {{ant eclipse}} has been run, the user has to refresh the project (right-click on the project and choose Refresh).

If these patches is committed, Subversion svn:ignore properties should be added/modified to ignore the destination IDEA and Eclipse configuration locations.

Iam Jambour has written up on the Lucene wiki a detailed set of instructions for applying the 3.X branch patch for IDEA: http://wiki.apache.org/lucene-java/HowtoConfigureIntelliJ"
0,"2.1 Locking documentation in ""Apache Lucene - Index File Formats"" section ""6.2 Lock File"" out datedI am in the process to migrate from Lucene 2.0 to Lucene 2.1.

From reading the Changes document I understand that the write locks are now written into the index folder instead of the java.io.tmpdir. 

In the ""Apache Lucene - Index File Formats"" document in section ""6.2 Lock File"" I read that there is a write lock used to indicate that another process is writing into the index and that this file is stored in the java.io.tempdir.

This is confusing to me.  I had the impression all lock files go into the index folder now.  And using the the java.io.tempdir is only local and does not support access to shared index folders.

Do I miss something here or is the documentation not updated?
"
0,"SecureProtocolFactoryWrapper class for using the socket factory created by Java Web StartAs smartcards and SSL are becoming more and more prevelant, Java Web Start has started to become better equiped to handle these situations.  When running an app within webstart, it can access the browser's keystore, which (at least in our case) accessed the users smartcard to make the SSL connection.

I wanted to start using HttpClient, but needed a way to do so while still mainaining access to the browser's keystore.

My initial tests show that getting the default socket factory from the java.net.HttpURLConnection and wrapping it in a class that implements org.apache.commons.httpclient.protocol.SecureProtocolSocketFactory is sufficient."
0,"Rename GenericRepositoryFactory to JndiRepositoryFactoryThe GenericRepositoryFactory class introduced in JCR-2360 has since been refactored so that most of its functionality is now distributed among the more implementation-specific RepositoryFactory classes. Now the GenericRepositoryFactory only contains support for looking the repository up in JNDI, so it would be better to rename the class to JndiRepositoryFactory.

The only troublesome part of the rename is the GenericRepositoryFactory.URI constant that was for a while documented on our wiki as a part of the canonical code snippet for accessing a remote repository based on the repository URI. The latest recommendation is to use the JcrUtils.getRepository(String uri) method so the constant is no longer needed in client code, but for backwards compatibility with earlier Jackrabbit 2.0 betas it may be good to keep the deprecated constant for at least the next beta release."
0,"Index nodes in parallelCPUs with multiple cores are now standard and Jackrabbit should make use of it where it makes sense. Analyzing content while a node is indexed is quite costly, but can be broken easily into task that can be executed in parallel. E.g. index multiple nodes in parallel."
0,Inner classes of FilterAtomicReader (trunk) / FilterIndexReader (3.x) do not override all methods to be filteredThis issue adds missing checks in the FilterReader test to also check overridden methods in the enum implementations (inner classes) similar to the checks added by Shai Erea.
1,"Node.getWeakReferences throws UnsupportedOperationException if not referenceable.... while Node.getReferences() doesn't and returns an empty propertyiterator.

From my point of view both methods should behave the same way and i prefer not throwing.

"
0,"When resolving deletes, IW should resolve in term sort orderSee java-dev thread ""IndexWriter.updateDocument performance improvement""."
0,lucene jars should include LiCENSE and NOTICEThe Lucene jars created by the build should include the LICENSE and NOTICE files in META-INF.
0,"New node type namespaces should be automatically registeredA user currently needs to explicitly register any new namespaces used in new node types before registering the node types. See for example the problem on the mailing list:

    http://mail-archives.apache.org/mod_mbox/incubator-jackrabbit-dev/200603.mbox/%3c1142091097.13136.0.camel@localhost.localdomain%3e

The node type registration should be changed so that new namespaces are automatically registered."
0,"javadocs very very ugly if you generate with java7Java7 changes its javadocs to look much nicer, but this involves different CSS styles.

Lucene overrides the CSS with stylesheet+prettify.css which is a combination of java5/6 stylesheet + google prettify:
but there are problems because java7 has totally different styles.

So if you generate javadocs with java7, its like you have no stylesheet at all.

A solution might be to make stylesheet7+prettify.css and conditionalize a property in ant based on java version."
0,"Add IndexReader.flush(commitUserData)IndexWriter offers a commit(String commitUserData) method.
IndexReader can commit as well using the flush/close methods and so
needs an analogous method that accepts commitUserData."
1,"RegexQuery matches terms the input regex doesn't actually matchI was writing some unit tests for our own wrapper around the Lucene regex classes, and got tripped up by something interesting.

The regex ""cat."" will match ""cats"" but also anything with ""cat"" and 1+ following letters (e.g. ""cathy"", ""catcher"", ...)  It is as if there is an implicit .* always added to the end of the regex.

Here's a unit test for the behaviour I would expect myself:

    @Test
    public void testNecessity() throws Exception {
        File dir = new File(new File(System.getProperty(""java.io.tmpdir"")), ""index"");
        IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(), true);
        try {
            Document doc = new Document();
            doc.add(new Field(""field"", ""cat cats cathy"", Field.Store.YES, Field.Index.TOKENIZED));
            writer.addDocument(doc);
        } finally {
            writer.close();
        }

        IndexReader reader = IndexReader.open(dir);
        try {
            TermEnum terms = new RegexQuery(new Term(""field"", ""cat."")).getEnum(reader);
            assertEquals(""Wrong term"", ""cats"", terms.term());
            assertFalse(""Should have only been one term"", terms.next());
        } finally {
            reader.close();
        }
    }

This test fails on the term check with terms.term() equal to ""cathy"".

Our workaround is to mangle the query like this:

    String fixed = String.format(""(?:%s)$"", original);
"
0,"Provide a clean mechanism to attatch user define attributes to connectionsIt would be nice to have a way to attach user defined attributes to a connection.
Ideally it'd be nice if such support could be added to HttpClientConnection, but understandably this may not be possible due to back-compatibility issues.
So, we could have something like HttpConnectionContext perhaps (or similar) with:

HttpConnectionContext#setAttribute(String name, Object value)
Object HttpConnectionContext#getAttribute(String name)

This would be made available in the HttpContext of a request (like the connection is today):

HttpConnectionContext connectionContext = (HttpConnectionContext) httpContext.getAttribute(ExecutionContext.HTTP_CONNECTION_CONTEXT);

This would make a few things much cleaner to implement than they are today: The most obvious being my current use case of wanting connection isolated cookies.

Currently to achieve this goal we need to provide custom client connection + connection operator + connection manager implementations. Then there is no clean way to currently obtain the actual connection instance created by a custom operator in the HttpContext: As it's wrapped by the connection pool and #getWrappedConnection is protected - so we need to resort to reflection in interceptors.

Providing a clean mechanism for attaching user defined attributes to a connection instance as described above would make such implementations far far simpler.
"
0,Remove unused method RedoLog.clear()This method is not used anymore and can be removed.
0,Make ItemInfoBuilder name space awareCurrently there is no way to to have NodeInfoBuilder/PropertyInfoBuilder build NodeInfos/ItemInfos on a name space other than the default one. I suggest to add appropriate methods to NodeInfoBuilder/PropertyInfoBuilder to set the name space.
0,Webdav: Drop xerces dependencyjukka is the final assignee :) thanks for taking over.
1,"HttpState.setCookiePolicy() is completely ignoredThough this method is deprecated, it currently has no effect and gives no warning that it does nothing.  
A patch that fixes this problem is coming shortly.

Mike"
0,PrivilegeHandlerTest fails on WindowsThe test fails on Windows because there are differences in line breaks between expected and actual results.
1,"DocumentsWriter.abort fails to clear docStoreOffsetI hit this in working on LUCENE-1044.

If you disk full event during flush, then DocumentsWriter will abort
(clear all buffered docs).  Then, if you then add another doc or two,
and then close your writer, and this time succeed in flushing (say
because it's only a couple buffered docs so the resulting segment is
smaller), you can flush a corrupt segment (that incorrectly has a
non-zero docStoreOffset).

I modified the TestConcurrentMergeScheduler test to show this bug.
I'll attach a patch shortly.
"
0,"Extract the public API interfaces from o.a.j.core to o.a.j.apiTo better document and track the public JCR extensions and component API provided by Jackrabbit and to allow more room for refactoring within the Jackrabbit core, we shoud move (or create) the supported API interfaces to a new org.apache.jackrabbit.api package.

At least the following interfaces should be moved along with any supporting implementation-independent classes:

    * PersistenceManager
    * FileSystem
    * AccessManager
    * QueryHandler
    * TextFilter

Possible dependencies to implementation-specific classes should preferably be abstracted using extra interfaces.

Also the workspace and node type administration methods should be published as Jackrabbit-specific extensions to the JCR API interfaces.
"
0,"Catch Throwables while calling TextExtractorsThere are different Exception Handlings in the current TextExtractors.
The Method Signature throws IOException but the internal Handling is different.

For example in the MsExcelTextExtractor there will be RuntimeException's catched but not in all Extractors.
@see JCR-574

I think we should catch Throwables in the NodeIndexer to prevent OutOfMemoryErros while indexing a node."
0,"Modify LazyQueryResultImpl to allow resultFetchSize to be set programmaticallyIn our application we have a search which only shows part of a query result. We always know which part of the result needs to be shown. This means we know in advance how many results need to be fetched. I would like to be able to programmatically set resultFetchSize to minimize the number of loaded lucene docs and therefore improve the performance.
I know it is already possible to the set the resultFetchSize via the index configuration, but this number is fixed and doesn't work well in environments where you use paging for your results because if you set this number too low the query will be executed multiple times and if you set it too high too many lucene docs are loaded."
0,"Slightly more readable code in Token/TermAttributeImplNo big deal. 

growTermBuffer(int newSize) was using correct, but slightly hard to follow code. 

the method was returning null as a hint that the current termBuffer has enough space to the upstream code or reallocated buffer.

this patch simplifies logic   making this method to only reallocate buffer, nothing more.  
It reduces number of if(null) checks in a few methods and reduces amount of code. 
all tests pass.

This also adds tests for the new basic attribute impls (copies of the Token tests)."
0,move wordnet based synonym code out of contrib/memory and into contrib/wordnet (or somewhere else)see LUCENE-387 ... some synonym related code has been living in contrib/memory for a very long time ... it should be refactored out.
1,"InternalVersionManager deadlockThe changes in JCR-2753 exposed the InternalVersionManager classes to the following deadlock scenario:

   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x0000000085edb5a0> (a org.apache.jackrabbit.core.state.DefaultISMLocking)
	at java.lang.Object.wait(Object.java:485)
	at org.apache.jackrabbit.core.state.DefaultISMLocking.acquireReadLock(DefaultISMLocking.java:92)
	- locked <0x0000000085edb5a0> (a org.apache.jackrabbit.core.state.DefaultISMLocking)
	at org.apache.jackrabbit.core.version.InternalVersionManagerBase.acquireReadLock(InternalVersionManagerBase.java:192)
	at org.apache.jackrabbit.core.version.InternalVersionManagerImpl.getItem(InternalVersionManagerImpl.java:324)
	at org.apache.jackrabbit.core.version.InternalVersionManagerBase.createInternalVersionItem(InternalVersionManagerBase.java:761)
	at org.apache.jackrabbit.core.version.InternalVersionManagerImpl.getItem(InternalVersionManagerImpl.java:329)
	- locked <0x0000000085edb770> (a org.apache.commons.collections.map.ReferenceMap)
	at org.apache.jackrabbit.core.version.InternalVersionManagerBase.getVersionHistory(InternalVersionManagerBase.java:130)
	at org.apache.jackrabbit.core.version.InternalVersionManagerImpl.getVersionHistory(InternalVersionManagerImpl.java:70)
	at org.apache.jackrabbit.core.version.InternalVersionManagerImpl$4.run(InternalVersionManagerImpl.java:415)
	at org.apache.jackrabbit.core.version.InternalVersionManagerImpl$DynamicESCFactory.doSourced(InternalVersionManagerImpl.java:720)
	at org.apache.jackrabbit.core.version.InternalVersionManagerImpl.checkin(InternalVersionManagerImpl.java:407)
	at org.apache.jackrabbit.core.version.InternalXAVersionManager.checkin(InternalXAVersionManager.java:251)
	at org.apache.jackrabbit.core.version.VersionManagerImplBase.checkoutCheckin(VersionManagerImplBase.java:190)
	at org.apache.jackrabbit.core.VersionManagerImpl.access$100(VersionManagerImpl.java:72)
	at org.apache.jackrabbit.core.VersionManagerImpl$1.perform(VersionManagerImpl.java:121)
	at org.apache.jackrabbit.core.VersionManagerImpl$1.perform(VersionManagerImpl.java:114)
	at org.apache.jackrabbit.core.session.SessionState.perform(SessionState.java:200)
	at org.apache.jackrabbit.core.VersionManagerImpl.perform(VersionManagerImpl.java:95)
	at org.apache.jackrabbit.core.VersionManagerImpl.checkin(VersionManagerImpl.java:114)
	at org.apache.jackrabbit.core.VersionManagerImpl.checkin(VersionManagerImpl.java:100)

   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.jackrabbit.core.version.InternalVersionManagerImpl.getItem(InternalVersionManagerImpl.java:327)
	- waiting to lock <0x0000000085edb770> (a org.apache.commons.collections.map.ReferenceMap)
	at org.apache.jackrabbit.core.version.InternalXAVersionManager.getItem(InternalXAVersionManager.java:442)
	at org.apache.jackrabbit.core.version.InternalVersionManagerBase.getVersionHistory(InternalVersionManagerBase.java:130)
	at org.apache.jackrabbit.core.version.InternalXAVersionManager.getVersionHistory(InternalXAVersionManager.java:58)
	at org.apache.jackrabbit.core.version.VersionHistoryImpl.getInternalVersionHistory(VersionHistoryImpl.java:78)
	at org.apache.jackrabbit.core.version.VersionHistoryImpl.isSame(VersionHistoryImpl.java:278)
	at org.apache.jackrabbit.core.version.VersionHistoryImpl.checkOwnVersion(VersionHistoryImpl.java:326)
	at org.apache.jackrabbit.core.version.VersionHistoryImpl.getVersionLabels(VersionHistoryImpl.java:218)

The problem is the ReferenceMap synchronization (object 0x0000000085edb770) that now interferes with the more general read/write locking mechanism."
0,"PersistenceManager API change breaks backward compatibilityPersistence Manager API change introduced in JCR-1428 breaks backward compatibility. although this is not a public visible API it renders 3rd party PMs invalid that do not extend from AbstractPersistenceManager.
at least for the 1.4.3 patch release, we should not do this.

suggest to revert the API change for the next 1.4.4 release, but leave the method on the abstract pm, and introduce it only for 1.5.
"
0,"additional conditional compliance tests for the caching module for Content-Encoding, Content-Location, Date, Expires, Server, Transfer-Encoding, and Vary headersPatch is forthcoming."
0,"AttributeSource's methods for accessing attributes should be final, else its easy to corrupt the internal statesThe methods that operate and modify the internal maps of AttributeSource should be final, which is a backwards break. But anybody that overrides such methods simply creates a buggy AS either case.

I want to makeall impls final (in general the class should be final at all, but it is made for extension in TokenStream). So its important that the implementations are final!"
1,Prevent persistence of faulty back-referencesThe SharedItemStateManager updates reference data. Sometimes the back-references to reference properties are not updated correctly with the result that nodes cannot removed anymore. The attached patch contains JUnit test cases and a possible solution.
1,"shouldn't automatically set Content-Length in request headercurrently, httpclient automatically add Content-Length: 0 in the request 
header, this is causing problems with some web servers, particularly, with

ar.atwola.com

Try the following URL
http://ar.atwola.com/file/adsWrapper.js

It will block indefinitely. This problem can be fixed by not sending the 
Content-Length header, this is the browser's behavior. I'm not sure why this 
casue problem, but let's conform to a standard browser's practice and avoid 
troubles."
0,"Further steps towards flexible indexingI attached a very rough checkpoint of my current patch, to get early
feedback.  All tests pass, though back compat tests don't pass due to
changes to package-private APIs plus certain bugs in tests that
happened to work (eg call TermPostions.nextPosition() too many times,
which the new API asserts against).

[Aside: I think, when we commit changes to package-private APIs such
that back-compat tests don't pass, we could go back, make a branch on
the back-compat tag, commit changes to the tests to use the new
package private APIs on that branch, then fix nightly build to use the
tip of that branch?o]

There's still plenty to do before this is committable! This is a
rather large change:

  * Switches to a new more efficient terms dict format.  This still
    uses tii/tis files, but the tii only stores term & long offset
    (not a TermInfo).  At seek points, tis encodes term & freq/prox
    offsets absolutely instead of with deltas delta.  Also, tis/tii
    are structured by field, so we don't have to record field number
    in every term.
.
    On first 1 M docs of Wikipedia, tii file is 36% smaller (0.99 MB
    -> 0.64 MB) and tis file is 9% smaller (75.5 MB -> 68.5 MB).
.
    RAM usage when loading terms dict index is significantly less
    since we only load an array of offsets and an array of String (no
    more TermInfo array).  It should be faster to init too.
.
    This part is basically done.

  * Introduces modular reader codec that strongly decouples terms dict
    from docs/positions readers.  EG there is no more TermInfo used
    when reading the new format.
.
    There's nice symmetry now between reading & writing in the codec
    chain -- the current docs/prox format is captured in:
{code}
FormatPostingsTermsDictWriter/Reader
FormatPostingsDocsWriter/Reader (.frq file) and
FormatPostingsPositionsWriter/Reader (.prx file).
{code}
    This part is basically done.

  * Introduces a new ""flex"" API for iterating through the fields,
    terms, docs and positions:
{code}
FieldProducer -> TermsEnum -> DocsEnum -> PostingsEnum
{code}
    This replaces TermEnum/Docs/Positions.  SegmentReader emulates the
    old API on top of the new API to keep back-compat.
    
Next steps:

  * Plug in new codecs (pulsing, pfor) to exercise the modularity /
    fix any hidden assumptions.

  * Expose new API out of IndexReader, deprecate old API but emulate
    old API on top of new one, switch all core/contrib users to the
    new API.

  * Maybe switch to AttributeSources as the base class for TermsEnum,
    DocsEnum, PostingsEnum -- this would give readers API flexibility
    (not just index-file-format flexibility).  EG if someone wanted
    to store payload at the term-doc level instead of
    term-doc-position level, you could just add a new attribute.

  * Test performance & iterate.
"
1,"NPE in PredefinedNodeTypeTest.getPropertyDefSpecOccurs when PropertyDefinition.getValueConstraints returns null, which is allowed by the spec.
"
1,AbstractSession should not synchronize on the session instanceThe local namespace mapping methods in AbstractSession are synchronized to protect against concurrent access. That's troublesome since our observation delivery needs to be able to get those mappings even when the session is synchronized to do something else.
0,"DerbyPersistenceManager only usable for embedded databasesDerbyPersistenceManager always shuts down the database on exit, which makes it unusable for standalone databases."
0,Contrib CharTokenizer classes should be instantiated using their new Version based ctorsContrib CharTokenizer classes should be instantiated using their new Version based ctors introduced by LUCENE-2183 and LUCENE-2240
0,"Two-stage state expansion for the FST: distance-from-root and child-count criteria.In the current implementation FST states are expanded into a binary search on labels (from a vector of transitions) when the child count of a state exceeds a given predefined threshold (NUM_ARCS_FIXED_ARRAY). This threshold affects automaton size and traversal speed (as it turns out when benchmarked). For some degenerate (?) data sets, close-to-the-root nodes could have a small number of children (below the threshold) and yet be traversed on every single seek.

A fix of this is to introduce two control thresholds: 
  EXPAND state if (distance-from-root <= MIN_DISTANCE || children-count >= NUM_ARCS_FIXED_ARRAY)

My plan is to create a data set that will prove this first and then to implement the workaround above."
1,"Auto Reconnect for RMI RepositoryIf i bind the RepositoryAccessServlet to a RMI Repository and then reboot the Repository i get a
NullpointerException. 
Stack :
java.lang.NullPointerException
	at org.apache.jackrabbit.webdav.jcr.JcrDavException.<init>(JcrDavException.java:111)
	at org.apache.jackrabbit.webdav.simple.DavSessionProviderImpl.attachSession(DavSessionProviderImpl.java:99)
	at org.apache.jackrabbit.server.AbstractWebdavServlet.service(AbstractWebdavServlet.java:181)

If i deploy jackrabbit in a Model 3 Environment this Situation can happen very often.
thanks
claus"
1,"IndexWriter should never pool readers for external segmentsEG when addIndexes is called, it enrolls external segment infos, which are then merged.  But merging will simply ask the pool for the readers, and if writer is pooling (NRT reader has been pooled) it incorrectly pools these readers.

It shouldn't break anything but it's a waste because these readers are only used for merging, once, and they are not opened by NRT reader."
0,"Strengthen CheckIndex a bitA few small improvements to CheckIndex to detect possible ""docs out of order"" cases."
0,Migrate to maven 2 the other OCM subprojectsMigrate to maven 2 the other OCM subprojects : jcr-nodemanagement & spring. 
1,WeightedHighlighter does not encode XML markup charactersSee JCR-2611; the same problem applies to WeightedHighlighter.
0,"AppendRecord writes single bytes to diskThe AppendRecord initially buffers writes in memory and starts
to write it to a temp file as soon as it occupies more than
64k heap. After switching to the temp file, data is written
unbuffered."
1,"Deadlock on concurrent commitsAs reported in the followup to JCR-1979, there's a case where two transactions may be concurrently inside a commit. This is bad as it breaks the main assumption in http://jackrabbit.apache.org/concurrency-control.html about all transactions first acquiring the versioning write lock.

Looking deeper into this I find that the versioning write lock is only acquired if the transaction being committed contains versioning operations. This is incorrect as all transactions in any case need to access the version store when checking for references."
0,"provide an ehcache implementation for HttpCacheProvide an implementation of the HttpCache interface that stores cache entries in ehcache.
"
1,"Path is not indexed when inserting a new node with SNSUsing Jackrabbit OCM, when inserting two nodes with the same path, the second node's path is not indexed. 
Both nodes have the same path, and a search by path retrieves the first node only. 

The node mapping included the following annotations:

@Node(jcrMixinTypes=""mix:referenceable,mix:lockable,mix:versionable"") 
public class Article { 

        @Field(uuid=true) 
        private String id = null; 
        
        @Field(path=true) 
        private String path = null; 

        ....
}"
0,"authentication order has changed from 1.4.x to 1.5.xIn 1.4.x inside RepositoryImpl.login(...) at first the local configuration is checked for configured LoginModules and after it was unsuccessful, the JAAS component is asked:

          AuthContext authCtx;
            LoginModuleConfig lmc = repConfig.getLoginModuleConfig();
            if (lmc == null) {
                        authCtx = new AuthContext.JAAS(repConfig.getAppName(), credentials);
            } else {
...

With 1.5.x this behaviour has moved to SimpleSecurityManager.init(..) and is changed:
        LoginModuleConfig loginModConf = config.getLoginModuleConfig();
        authCtxProvider = new AuthContextProvider(config.getAppName(), loginModConf);
        if (authCtxProvider.isJAAS()) {
            log.info(""init: using JAAS LoginModule configuration for "" + config.getAppName());
        } else if (authCtxProvider.isLocal()) {
...

The problem is with JBoss JAAS implemantation, that authCtxProvider.isJAAS()  is always true.
Because for any reason, the result of Configuration.getAppConfigurationEntry(appName) is never empty,
when a jaas.config is specified for Liferay. Using different appName takes no effect, always the configuration inside the jaas.config is used.

I think still first the local configuration should be concerned, before using JAAS."
1,"Deadlock due different Thread access while prepare and commit in same TransactionSince we have configured a j2c resource adapter any modification to the repository ends
with a deadlock."
0,"SimpleHttpConnectionManager is used incorrectly by tutorial codeUsing pretty well standard (from the tutorial) code causes the 
SimpleHttpConnectionManager to print its ""being used incorrectly"" warning if 
the connection times out (or other I/O exception occurs).

I will attach a simple test I made to demonstrate this."
1,"NumericRangeQuery errors with endpoints near long min and max valuesThis problem first reported in Solr:

http://lucene.472066.n3.nabble.com/range-query-on-TrieLongField-strange-result-tt970974.html#a970974"
0,"Automatic repository shutdownCurrently Jackrabbit relies on two mechanisms for safely shutting down a repository:

    1) client application invoking RepositoryImpl.shutdown(), or
    2) the shutdown hook installed by RepositoryImpl being run

Both of these mechanisms have problems:

    1) The shutdown() method is not a part of the JCR API, thus making the client application depend on a Jackrabbit-specific feature
    2) In some cases the shutdown hook is not properly run (see issues JCR-120 and JCR-233)

I think the JCR spec thinks of the Repository and Session interfaces as being somewhat similar to the JDBC DataSource and Connection interfaces. The Repository instances have no real lifecycle methods while the Session instances have clearly specified login and logout steps. (DataSource.getConnection() = Repository.login(), Session.logout() = Connection.close()) However the Jackrabbit implementation defines an explicit lifecycle for the RepositoryImpl instances.

This causes problems especially for container environments (JNDI, Spring) where it is hard or even impossible to specify a shutdown mechanism for resource factories like the Repository instances. The current solution for such environments is to use a shutdown hook, but as reported this solution does not work perfectly in all cases.

How about if we bound the RepositoryImpl lifecycle to the lifecycles of the instantiated Sessions. A RepositoryImpl instance could initialize (and lock) the repository when the first session is opened and automatically shut down when the last session has logged out. As long as the sessions are properly logged out (or finalized by the garbage collector) there would be no need for an explicitly RepositoryImpl.shutdown() call. The current behaviour of pre-initializing the repository and shutting down during a shutdown hook could be enabled with a configuration option for environments (like global JNDI resources) in which the shutdown hooks work well.
"
1,"SSLSocketFactory.connectSocket() possible NPE - or use of wrong variable?SSLSocketFactory.connectSocket() has a possible NPE at line 324:

            sock.connect(remoteAddress, connTimeout);

Or perhaps this should really be:

            sslsock.connect(remoteAddress, connTimeout);"
1,"Node.restore fails with multiple mixin typesRestoring a node that has more than one mixin type causes the exception below. 

java.lang.NullPointerException
	at org.apache.jackrabbit.core.NodeImpl.restoreFrozenState(NodeImpl.java:3286)
	at org.apache.jackrabbit.core.NodeImpl.internalRestore(NodeImpl.java:3243)
	at org.apache.jackrabbit.core.NodeImpl.internalRestore(NodeImpl.java:3210)
	at org.apache.jackrabbit.core.NodeImpl.restore(NodeImpl.java:2821)
	at com.gtnet.jcr.VersionedNodeTest.testNodeVersionAndRestore(VersionedNodeTest.java:311)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at com.intellij.rt.execution.junit2.JUnitStarter.main(JUnitStarter.java:31)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:78)
"
1,Incorrect iterator position in JCR-RMI when skipping large number of entriesThe positionOfBuffer variable in ClientIterator gets out of sync more than ServerAdapter.bufferLength items.
1,"indexing doesn't reset token stateIndexWriter (DocumentsWriter) forgets to reset the token state resulting in incorrect positionIncrements, payloads, and token types."
1,"if index is too old you should hit an exception saying soIf you create an index in 2.3.x (I used demo's IndexFiles) and then try to read it in 4.0.x (I used CheckIndex), you hit a confusing exception like this:
{noformat}
java.io.IOException: read past EOF
        at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:154)
        at org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:39)
        at org.apache.lucene.store.ChecksumIndexInput.readByte(ChecksumIndexInput.java:40)
        at org.apache.lucene.store.DataInput.readInt(DataInput.java:76)
        at org.apache.lucene.index.SegmentInfo.<init>(SegmentInfo.java:171)
        at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:230)
        at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:269)
        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:649)
        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:484)
        at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:265)
        at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:308)
        at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:287)
        at org.apache.lucene.index.CheckIndex.main(CheckIndex.java:930)
{noformat}

I think instead we should throw an IndexTooOldException or something like that?"
1,"RAMDirectory reports incorrect EOF on seekIf you create a file whose length is a multiple of 1024 (BUFFER_SIZE),
and then try to seek to the very end of the file, you hit
EOFException.

But this is actually ""legal"" as long as you don't try to read any
bytes at that point.

I'm hitting this (rarely) with the bulk-merging logic for term vectors
(LUCENE-1120), which can seek to the very end of the file but not read
any bytes if conditions are right.

"
0,"Some tests try to add new nodes without specifying the node typeSome tests try to add new nodes without specifying the node type, which may not be supported by a repository.

In particular:

- NodeAddMixinTest.testAddNonExisting
- NodeCanAddMixinTest.testNonExisting
- ValueFactoryTest.testValueType
- ValueFactoryTest.testValueFormatException

Proposal: update test cases to obtain the node type from the test config.
"
0,"Unique ID for org.apache.jackrabbit.value.BinaryValueBinaryValue should have a method get the unique identifier (if one is available). That way an application may not have to read the stream if that value is already processed.

When the DataStore is used, a unique identifier is available, so probably this feature is quite simple to implement.

See also http://www.nabble.com/Workspace.copy()-Question-...-td20435164.html (but please don't reply to this thread from now on - instead add comments to this issue).

Another feature is getFileName() to get the file name if it is stored in the file system. This method may need a security mechanism, for example getFileName(Session s) so that the system can check it. In any case the file should not be modified, but maybe knowing the file name is already too dangerous in some cases."
0,"Add a TypeTokenFilterIt would be convenient to have a TypeTokenFilter that filters tokens by its type, either with an exclude or include list. This might be a stupid thing to provide for people who use Lucene directly, but it would be very useful to later expose it to Solr and other Lucene-backed search solutions."
0,"Improve CachingWrapperFilter to optionally also cache acceptDocs, if identical to liveDocsSpinoff from LUCENE-1536: This issue removed the different cache modes completely and always applies the acceptDocs using BitsFilteredDocIdSet.wrap(), the cache only contains raw DocIdSet without any deletions/acceptDocs. For IndexReaders that are seldom reopened, this might not be as performant as it could be. If the acceptDocs==IR.liveDocs, those DocIdSet could also be cached with liveDocs applied."
1,"Using transactions leads to memory leakThere is global static map in XASessionImpl class which stores all Xids and TransactionContexts

    /**
     * Global transactions
     */
    private static final Map txGlobal = new HashMap();

It looks like this map is never cleared, even after end of transaction. It leads to memory leak because TransactionContexts and all nested objects (including XASessionImpl) are still referenced and couldn't be freed.

Proposed solution : Is it posssible to add just single line which will remove TransactionContext from static map at the end of transaction ?

      if (flags == TMSUCCESS || flags == TMFAIL) {
            associate(null);
-->       txGlobal.remove(xid);
        } else  if (flags == TMSUSPEND) {
            associate(null);
        } else {
            throw new XAException(XAException.XAER_INVAL);
        }

If this is not acceptable, then we have to unreference TransactionContext in another way."
1,"JCR-Server: Allow header misses colong (RootCollection, WorkspaceResourceImpl)List of supported dav methods misses some colons."
1,"Dead code in SpellChecker.java (branch never executes)SpellChecker contains the following lines of code:

    final int goalFreq = (morePopular && ir != null) ? ir.docFreq(new Term(field, word)) : 0;
    // if the word exists in the real index and we don't care for word frequency, return the word itself
    if (!morePopular && goalFreq > 0) {
      return new String[] { word };
    }

The branch will never execute: the only way for goalFreq to be greater than zero is if morePopular is true, but if morePopular is true, the expression in the if statement evaluates to false.

"
0,"make frozenbuffereddeletes more efficient for termswhen looking at LUCENE-3340, I thought its also ridiculous how much ram we use for delete by term.

so we can save a lot of memory, especially object overhead by being a little more efficient."
0,"Customizable Cookie PolicyEven if the server is not complying with the cookie specification, sometimes 
you still need to talk to it.

It would be nice that when setting the Cookie Policy for the HttpMethod, you 
could specify a custom policy that implements the CookieSpecBase but may 
handle certain problems more leniently.  This would be instead of specifying 
one of the three hard-coded policies."
1,"Restoring a node fails (partially) if done within a XA transactionA problem occurs with the following sequence of steps: 

1) Create a versionable node that has a child and a grandchild.
2) Perform a check-in of the versionable node and give a version-label.
3) Perform a restore by using the version-label.
4) Access the grandchild.

Step 4 fails, if step 3 is executed within a transaction. If no transaction is used, then step 4 succeeds. 
The test-case attached below can be executed within XATest.java (http://svn.apache.org/repos/asf/jackrabbit/trunk/jackrabbit-core/src/test/java/org/apache/jackrabbit/core/XATest.java).


public void testRestore() throws Exception {
        Session session = null;
        try {
            session = getHelper().getSuperuserSession();

            // make sure that 'testNode' does not exist at the beginning of the test
            for (NodeIterator ni = session.getRootNode().getNodes(); ni.hasNext();) {
                Node aNode = ni.nextNode();
                if (aNode.getName().equals(""testNode"")) {
                    aNode.remove();
                }
            }

            // 1) create 'testNode' that has a child and a grandchild
            session.getRootNode().addNode(""testNode"").addMixin(NodeType.MIX_VERSIONABLE);
            session.getRootNode().getNode(""testNode"").addNode(""child"").addNode(""grandchild"");
            session.save();

            // 2) check in 'testNode' and give a version-label
            Version version = session.getWorkspace().getVersionManager().checkin(
                    session.getRootNode().getNode(""testNode"").getPath());
            session.getWorkspace().getVersionManager().getVersionHistory(
                    session.getRootNode().getNode(""testNode"").getPath()).addVersionLabel(version.getName(),
                    ""testLabel"", false);

            // 3) do restore by label
            UserTransaction utx = new UserTransactionImpl(session);
            utx.begin();
            session.getWorkspace().getVersionManager().restoreByLabel(
                    session.getRootNode().getNode(""testNode"").getPath(), ""testLabel"", true);
            utx.commit();

            // 4) try to get the grandchild (fails if the restoring has been done within a transaction)
            session.getRootNode().getNode(""testNode"").getNode(""child"").getNode(""grandchild"");
        } finally {
            if (session != null) {
                session.logout();
            }
        }
    } "
1,"InvalidQueryException thrown for a SQL query using WHERE CONTAINS(., 'someword')The following SQL query:
SELECT * FROM es:document WHERE CONTAINS(., 'software')
throws an InvalidQueryException exception.

javax.jcr.query.InvalidQueryException: Encountered ""."" at line 1, column 42.
Was expecting one of:
    ""BY"" ...
    ""IN"" ...
    ""OR"" ...
    ""IS"" ...
    ""AND"" ...
    ""LIKE"" ...
    ""NULL"" ...
    ""FROM"" ...
    ""ORDER"" ...
    ""WHERE"" ...
    ""SELECT"" ...
    ""BETWEEN"" ...
    ""*"" ...
    <REGULAR_IDENTIFIER> ...
    <DELIMITED_IDENTIFIER> ...
    
This syntax seems correct according to the latest jcr spec (1.0.1).
Using an asterisk (*) instead of a dot (.) as the first parameter of CONTAINS() works fine."
0,Introduce spellchecker functionality based on content in the workspaceProvide a way to spell check a fulltext statement based on the content present in the workspace.
1,"offset gap should be added regardless of existence of tokens in DocInverterPerFieldProblem: If a multiValued field which contains a stop word (e.g. ""will"" in the following sample) only value is analyzed by StopAnalyzer when indexing, the offsets of the subsequent tokens are not correct.

{code:title=indexing a multiValued field}
doc.add( new Field( F, ""Mike"", Store.YES, Index.ANALYZED, TermVector.WITH_OFFSETS ) );
doc.add( new Field( F, ""will"", Store.YES, Index.ANALYZED, TermVector.WITH_OFFSETS ) );
doc.add( new Field( F, ""use"", Store.YES, Index.ANALYZED, TermVector.WITH_OFFSETS ) );
doc.add( new Field( F, ""Lucene"", Store.YES, Index.ANALYZED, TermVector.WITH_OFFSETS ) );
{code}

In this program (soon to be attached), if you use WhitespaceAnalyzer, you'll get the offset(start,end) for ""use"" and ""Lucene"" will be use(10,13) and Lucene(14,20). But if you use StopAnalyzer, the offsets will be use(9,12) and lucene(13,19). When searching, since searcher cannot know what analyzer was used at indexing time, this problem causes out of alignment of FVH.

Cause of the problem: StopAnalyzer filters out ""will"", anyToken flag set to false then offset gap is not added in DocInverterPerField:

{code:title=DocInverterPerField.java}
if (anyToken)
  fieldState.offset += docState.analyzer.getOffsetGap(field);
{code}

I don't understand why the condition is there... If always the gap is added, I think things are simple."
0,"Upgrade to latest Apache parent POMWe're quite a bit behind the latest and greatest of the Apache parent POMs (org.apache:apache), mostly since we're inheriting it through the now mostly unused org.apache.jackrabbit:parent POM.

I'd like to move things back from the o.a.j:parent POM to the jackrabbit-parent POM that's located inside trunk. This will allow us to upgrade to the latest Apache parent POM without the trouble of an extra release of the o.a.j:parent POM."
0,"Czech StemmerCurrently, the CzechAnalyzer is merely stopwords, and there isn't a czech stemmer in snowball.

This patch implements the light stemming algorithm described in: http://portal.acm.org/citation.cfm?id=1598600

In their measurements, it improves MAP 42%

The analyzer does not use this stemmer if LUCENE_VERSION <= 3.0, for back compat.
"
0,"QueryObjectModelImpl should execute queries as SessionOperation(s)QueryObjectModelImpl doesn't leverage the SessionOperation closure approach (like the QueryImpl does). 

Switching to this style of running a query yields some gains in speed (I ran 50 queries per test):
 - #1. old style   (no code change)       avg was 14.26 ms
 - #2. new style (as session operation) avg was 12.14 ms
 - #3. new style (as session operation) avg was   6.44 ms
 - #4. new style (as session operation) avg was   6.68 ms
 - #5. old style  (no code change)        avg was 11.62 ms
 - #6. old style  (no code change)        avg was 11.66 ms"
1,"SpanNotQuery.hashCode ignores excludefiling as bug for tracking/refrence...

On May 16, 2006, at 3:33 AM, Chris Hostetter wrote:

> SpanNodeQuery's hashCode method makes two refrences to  
> include.hashCode(),
> but none to exclude.hashCode() ... this is a mistake yes/no?

Date: Tue, 16 May 2006 05:57:15 -0400
From: Erik Hatcher
To: java-dev@lucene.apache.org
Subject: Re: SpanNotQuery.hashCode cut/paste error?

Yes, this is a mistake.  I'm happy to fix it, but looks like you have  
other patches in progress.

"
1,spi2davex: uri-lookup not cleared after reordering referenceable same-name-siblings
1,"Exception in DocumentsWriter.addDocument can corrupt stored fields file (fdt)DocumentsWriter writes the number of stored fields, up front, into the
fdtLocal buffer.  Then, as each field is processed, it writes each
stored field into this buffer.  When the document is done, in a
finally clause, it flushes the buffer to the real fdt file in the
Directory.

The problem is, if an exception is hit, that number of stored fields
can be too high, which corrupts the fdt file.

The solution is to not write it up front, and instead write only the
number of fields we actually saw."
1,"Unsynchronized NameFactoryImpl initializationorg.apache.jackrabbit.spi.commons.name.NameFactoryImpl uses an unsafe pattern when initializing:

    private static NameFactory FACTORY;
    private NameFactoryImpl() {};
    public static NameFactory getInstance() {
        if (FACTORY == null) {
            FACTORY = new NameFactoryImpl();
        }
        return FACTORY;
    }

This is bad in a multi-threaded environment (see http://www.ibm.com/developerworks/library/j-dcl.html for details)."
1,"DbDataStore: garbage collection deadlockSometimes, the unit tests hangs with the following threads blocked. It looks like a database level deadlock caused by the DbDataStore implementation. The database used is Apache Derby.

org.apache.jackrabbit.core.data.db.DbDataStore.addRecord line=298
org.apache.jackrabbit.core.value.BLOBInDataStore.getInstance line=120
org.apache.jackrabbit.core.value.InternalValue.getBLOBFileValue line=644
org.apache.jackrabbit.core.value.InternalValue.create line=123
org.apache.jackrabbit.core.PropertyImpl.setValue line=609
org.apache.jackrabbit.core.PropertyImpl.setValue line=525
org.apache.jackrabbit.core.NodeImpl.setProperty line=2312
org.apache.jackrabbit.core.data.CopyValueTest.doTestCopy line=64
org.apache.jackrabbit.core.data.CopyValueTest.testCopyStream line=45

org.apache.jackrabbit.core.data.db.DbDataStore.updateLastModifiedDate line=641
org.apache.jackrabbit.core.data.db.DbDataStore.touch line=631
org.apache.jackrabbit.core.data.db.DbDataStore.getRecord line=484
org.apache.jackrabbit.core.value.BLOBInDataStore.getDataRecord line=136
org.apache.jackrabbit.core.value.BLOBInDataStore.getLength line=92
org.apache.jackrabbit.core.data.GarbageCollector.scanPersistenceManagers
org.apache.jackrabbit.core.data.GarbageCollector.scan line=161
org.apache.jackrabbit.core.data.GCThread.run line=52"
0,"DateTools.stringToDate() can cause lock contention under loadLoad testing our application (the JIRA Issue Tracker) has shown that threads spend a lot of time blocked in DateTools.stringToDate().

The stringToDate() method uses a singleton SimpleDateFormat object to parse the dates.
Each call to SimpleDateFormat.parse() is *synchronized* because SimpleDateFormat is not thread safe.

"
0,"Transaction-safe versioningI've been working on a partial fix to JCR-630. Instead of implementing fully transactional versioning (i.e. a checkin will disappear when a transactin is rolled back), I'm ensuring that all versioning operations within a transaction will leave the version store in a consistent state even if the transaction otherwise fails at any point."
1,"LuceneTestCase's check for uncaught exceptions in threads causes collateral damage?Eg see these failures:

    https://hudson.apache.org/hudson/job/Lucene-3.x/214/

Multiple test methods failed in TestIndexWriterOnDiskFull, but, I think only 1 test had a real failure but somehow our ""thread hit exc"" tracking incorrectly blames the other 3 cases?

I'm not sure about this but it seems like something like that is going on...

So, one problem is that LuceneTestCase.tearDown fails on any thread excs, but if CMS had also hit a failure, then fails to clear CMS's thread failures.  I think we should just remove CMS's thread failure tracking?  (It's static so it can definitely bleed across tests).  Ie, just rely on LuceneTestCase's tracking."
0,"Trunk fails tests, FSD.open() - related    [junit] Testcase: testReadAfterClose(org.apache.lucene.index.TestCompoundFile):	FAILED
    [junit] expected readByte() to throw exception
    [junit] junit.framework.AssertionFailedError: expected readByte() to throw exception
    [junit] 	at org.apache.lucene.index.TestCompoundFile.demo_FSIndexInputBug(TestCompoundFile.java:345)
    [junit] 	at org.apache.lucene.index.TestCompoundFile.testReadAfterClose(TestCompoundFile.java:313)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:88)

This one is a non-bug, if you ask me. The test should fail on SimpleFSD, but on my system FSD.open() creates MMapD and that one cannot be closed, so the read succeeds.

    [junit] ------------- Standard Output ---------------
    [junit] Thread[Thread-34,5,main]: exc
    [junit] java.nio.BufferUnderflowException
    [junit] 	at java.nio.Buffer.nextGetIndex(Buffer.java:474)
    [junit] 	at java.nio.DirectByteBuffer.get(DirectByteBuffer.java:229)
    [junit] 	at org.apache.lucene.store.MMapDirectory$MMapIndexInput.readByte(MMapDirectory.java:67)
    [junit] 	at org.apache.lucene.store.ChecksumIndexInput.readByte(ChecksumIndexInput.java:36)
    [junit] 	at org.apache.lucene.store.IndexInput.readInt(IndexInput.java:70)
    [junit] 	at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:238)
    [junit] 	at org.apache.lucene.index.DirectoryIndexReader$1.doBody(DirectoryIndexReader.java:106)
    [junit] 	at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:699)
    [junit] 	at org.apache.lucene.index.DirectoryIndexReader.open(DirectoryIndexReader.java:126)
    [junit] 	at org.apache.lucene.index.IndexReader.open(IndexReader.java:374)
    [junit] 	at org.apache.lucene.index.IndexReader.open(IndexReader.java:260)
    [junit] 	at org.apache.lucene.search.IndexSearcher.<init>(IndexSearcher.java:76)
    [junit] 	at org.apache.lucene.index.TestStressIndexing$SearcherThread.doWork(TestStressIndexing.java:109)
    [junit] 	at org.apache.lucene.index.TestStressIndexing$TimedThread.run(TestStressIndexing.java:52)
    [junit] NOTE: random seed of testcase 'testStressIndexAndSearching' was: -7374705829444180151
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testStressIndexAndSearching(org.apache.lucene.index.TestStressIndexing):	FAILED
    [junit] null
    [junit] junit.framework.AssertionFailedError
    [junit] 	at org.apache.lucene.index.TestStressIndexing.runStressTest(TestStressIndexing.java:155)
    [junit] 	at org.apache.lucene.index.TestStressIndexing.testStressIndexAndSearching(TestStressIndexing.java:178)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:88)

This one suceeds sometimes, sometimes (mostly) fails. Is obviously linked with switch to MMapD, but what is the real cause - I don't know.

    [junit] ------------- Standard Output ---------------
    [junit] NOTE: random seed of testcase 'testSetBufferSize' was: 8481546620770090440
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testSetBufferSize(org.apache.lucene.store.TestBufferedIndexInput):	Caused an ERROR
    [junit] org.apache.lucene.store.MMapDirectory$MMapIndexInput cannot be cast to org.apache.lucene.store.BufferedIndexInput
    [junit] java.lang.ClassCastException: org.apache.lucene.store.MMapDirectory$MMapIndexInput cannot be cast to org.apache.lucene.store.BufferedIndexInput
    [junit] 	at org.apache.lucene.store.TestBufferedIndexInput$MockFSDirectory.tweakBufferSizes(TestBufferedIndexInput.java:226)
    [junit] 	at org.apache.lucene.store.TestBufferedIndexInput.testSetBufferSize(TestBufferedIndexInput.java:181)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:88)

Broken assumptions.
"
1,"LazyField use of IndexInput not thread safeHypothetical problem: IndexInput.clone() of an active IndexInput could result in a corrupt copy.
LazyField clones the FieldsReader.fieldsStream, which could be in use via IndexReader.document()"
0,"remove IndexDocValuesFieldIts confusing how we present CSF functionality to the user, its actually not a ""field"" but an ""attribute"" of a field like  STORED or INDEXED.

Otherwise, its really hard to think about CSF because there is a mismatch between the APIs and the index format."
0,"Document the problem with MS impl of digest authentication with older JREs and stale connection checkIt seems like digest authentication, when used with a username of the format:
domain\username fails in httpclient-3.0-rc2.

I did confirm that digest authentication does work by connecting to a local
Apache HTTP 2.0 server, using just a username and password (no domain\username).
However, it does not support the MD5-sess algorithm, and the server I am getting
the failure from is using MD5-sess. 

It may turn out the username is not causing the problem, but one thing is
consistent--I can connect to the site in the logs below using httpclient-2.0.2.
It fails when I use identical Java code, with the addition of AuthScope, when
using httpclient-3.0-rc2. I will also attach Java code that reproduces the problem.

The following are wire and debug logs from httpclient-2.0.2 and
httpclient-3.0-rc2 respectively. The first one connects and gets an 'HTTP 200'
response. The second one, using 3.0-rc2 fails with an 'HTTP 401'.

LOGS
===============================================================
commons-httpclient-2.0.2 (works):
2005/05/13 11:05:15:185 EDT [DEBUG] HttpClient - Java version: 1.3.1
2005/05/13 11:05:15:185 EDT [DEBUG] HttpClient - Java vendor: IBM Corporation
2005/05/13 11:05:15:185 EDT [DEBUG] HttpClient - Java class path: 
2005/05/13 11:05:15:205 EDT [DEBUG] HttpClient - Operating system name: Windows XP
2005/05/13 11:05:15:205 EDT [DEBUG] HttpClient - Operating system architecture: x86
2005/05/13 11:05:15:205 EDT [DEBUG] HttpClient - Operating system version: 5.1
2005/05/13 11:05:15:205 EDT [DEBUG] HttpClient - SUN 1.2: SUN (DSA key/parameter
generation; DSA signing; SHA-1, MD5 digests; SecureRandom; X.509 certificates;
JKS keystore)
2005/05/13 11:05:15:205 EDT [DEBUG] HttpClient - SunJCE 1.22: SunJCE Provider
(implements DES, Triple DES, Blowfish, PBE, Diffie-Hellman, HMAC-MD5, HMAC-SHA1)
2005/05/13 11:05:15:205 EDT [DEBUG] HttpClient - SunJSSE 1.0303: Sun JSSE
provider(implements RSA Signatures, PKCS12, SunX509 key/trust factories, SSLv3,
TLSv1)
2005/05/13 11:05:20:893 EDT [DEBUG] HttpConnection - HttpConnection.setSoTimeout(0)
2005/05/13 11:05:20:893 EDT [DEBUG] HttpMethodBase - Execute loop try 1
2005/05/13 11:05:20:913 EDT [DEBUG] header - >> ""GET
/CustomerData-30/CustomerDataService.asmx HTTP/1.1[\r][\n]""
2005/05/13 11:05:20:913 EDT [DEBUG] HttpMethodBase - Adding Host request header
2005/05/13 11:05:20:913 EDT [DEBUG] header - >> ""User-Agent: Jakarta
Commons-HttpClient/2.0.2[\r][\n]""
2005/05/13 11:05:20:913 EDT [DEBUG] header - >> ""Host:
mappoint-css.partners.extranet.microsoft.com[\r][\n]""
2005/05/13 11:05:21:173 EDT [DEBUG] header - >> ""[\r][\n]""
2005/05/13 11:05:21:273 EDT [DEBUG] header - << ""HTTP/1.1 401 Unauthorized[\r][\n]""
2005/05/13 11:05:21:273 EDT [DEBUG] header - << ""Content-Length: 1656[\r][\n]""
2005/05/13 11:05:21:283 EDT [DEBUG] header - << ""Content-Type: text/html[\r][\n]""
2005/05/13 11:05:21:283 EDT [DEBUG] header - << ""Server: Microsoft-IIS/6.0[\r][\n]""
2005/05/13 11:05:21:283 EDT [DEBUG] header - << ""WWW-Authenticate: Digest
qop=""auth"",algorithm=MD5-sess,nonce=""b2a83a38cd57c501af3ad2c91f189512060524424ffc2b818c9920db15cd247a9d47cf5a789d63c6"",opaque=""1704373a505e74c4ec692978e5c1a539"",charset=utf-8,realm=""Digest""[\r][\n]""
2005/05/13 11:05:21:283 EDT [DEBUG] header - << ""X-Powered-By: ASP.NET[\r][\n]""
2005/05/13 11:05:21:283 EDT [DEBUG] header - << ""Date: Fri, 13 May 2005 15:05:37
GMT[\r][\n]""
2005/05/13 11:05:21:283 EDT [DEBUG] HttpMethodBase - Authorization required
2005/05/13 11:05:21:283 EDT [DEBUG] HttpAuthenticator - Authenticating with the
'Digest' authentication realm at mappoint-css.partners.extranet.microsoft.com
2005/05/13 11:05:21:283 EDT [DEBUG] DigestScheme - Using qop method auth
2005/05/13 11:05:21:283 EDT [DEBUG] HttpMethodBase - HttpMethodBase.execute():
Server demanded authentication credentials, will try again.
2005/05/13 11:05:21:293 EDT [DEBUG] HttpMethodBase - Resorting to protocol
version default close connection policy
2005/05/13 11:05:21:293 EDT [DEBUG] HttpMethodBase - Should NOT close
connection, using HTTP/1.1.
2005/05/13 11:05:21:293 EDT [DEBUG] HttpMethodBase - Execute loop try 2
2005/05/13 11:05:21:293 EDT [DEBUG] header - >> ""GET
/CustomerData-30/CustomerDataService.asmx HTTP/1.1[\r][\n]""
2005/05/13 11:05:21:293 EDT [DEBUG] HttpMethodBase - Request to add Host header
ignored: header already added
2005/05/13 11:05:21:293 EDT [DEBUG] header - >> ""User-Agent: Jakarta
Commons-HttpClient/2.0.2[\r][\n]""
2005/05/13 11:05:21:293 EDT [DEBUG] header - >> ""Host:
mappoint-css.partners.extranet.microsoft.com[\r][\n]""
2005/05/13 11:05:21:293 EDT [DEBUG] header - >> ""Authorization: Digest
username=""domain\user"", realm=""Digest"",
nonce=""b2a83a38cd57c501af3ad2c91f189512060524424ffc2b818c9920db15cd247a9d47cf5a789d63c6"",
uri=""/CustomerData-30/CustomerDataService.asmx"", qop=""auth"",
algorithm=""MD5-sess"", nc=00000001, cnonce=""393a8abf65cd20f85ffdf46a9273b28b"",
response=""854bf54261112caf2e86652276cb2ce6"",
opaque=""1704373a505e74c4ec692978e5c1a539""[\r][\n]""
2005/05/13 11:05:21:293 EDT [DEBUG] header - >> ""[\r][\n]""
2005/05/13 11:05:21:994 EDT [DEBUG] header - << ""HTTP/1.1 200 OK[\r][\n]""HTTP
result: 200

===========================================================

commons-httpclient-3.0-rc2 (does not work):
2005/05/13 11:16:54:881 EDT [DEBUG] DefaultHttpParams - -Set parameter
http.useragent = Jakarta Commons-HttpClient/3.0-rc2
2005/05/13 11:16:54:881 EDT [DEBUG] DefaultHttpParams - -Set parameter
http.protocol.version = HTTP/1.1
2005/05/13 11:16:54:881 EDT [DEBUG] DefaultHttpParams - -Set parameter
http.connection-manager.class = class
org.apache.commons.httpclient.SimpleHttpConnectionManager
2005/05/13 11:16:54:891 EDT [DEBUG] DefaultHttpParams - -Set parameter
http.protocol.cookie-policy = rfc2109
2005/05/13 11:16:54:891 EDT [DEBUG] DefaultHttpParams - -Set parameter
http.protocol.element-charset = US-ASCII
2005/05/13 11:16:54:891 EDT [DEBUG] DefaultHttpParams - -Set parameter
http.protocol.content-charset = ISO-8859-1
2005/05/13 11:16:54:891 EDT [DEBUG] DefaultHttpParams - -Set parameter
http.method.retry-handler =
org.apache.commons.httpclient.DefaultHttpMethodRetryHandler@5048d78c
2005/05/13 11:16:54:891 EDT [DEBUG] DefaultHttpParams - -Set parameter
http.dateparser.patterns = [EEE, dd MMM yyyy HH:mm:ss zzz, EEEE, dd-MMM-yy
HH:mm:ss zzz, EEE MMM d HH:mm:ss yyyy, EEE, dd-MMM-yyyy HH:mm:ss z, EEE,
dd-MMM-yyyy HH-mm-ss z, EEE, dd MMM yy HH:mm:ss z, EEE dd-MMM-yyyy HH:mm:ss z,
EEE dd MMM yyyy HH:mm:ss z, EEE dd-MMM-yyyy HH-mm-ss z, EEE dd-MMM-yy HH:mm:ss
z, EEE dd MMM yy HH:mm:ss z, EEE,dd-MMM-yy HH:mm:ss z, EEE,dd-MMM-yyyy HH:mm:ss
z, EEE, dd-MM-yyyy HH:mm:ss z]
2005/05/13 11:16:54:911 EDT [DEBUG] HttpClient - -Java version: 1.3.1
2005/05/13 11:16:54:911 EDT [DEBUG] HttpClient - -Java vendor: IBM Corporation
2005/05/13 11:16:54:911 EDT [DEBUG] HttpClient - -Java class path: 
2005/05/13 11:16:54:941 EDT [DEBUG] HttpClient - -Operating system name: Windows XP
2005/05/13 11:16:54:941 EDT [DEBUG] HttpClient - -Operating system architecture: x86
2005/05/13 11:16:54:941 EDT [DEBUG] HttpClient - -Operating system version: 5.1
2005/05/13 11:16:54:941 EDT [DEBUG] HttpClient - -SUN 1.2: SUN (DSA
key/parameter generation; DSA signing; SHA-1, MD5 digests; SecureRandom; X.509
certificates; JKS keystore)
2005/05/13 11:16:54:951 EDT [DEBUG] HttpClient - -SunJCE 1.22: SunJCE Provider
(implements DES, Triple DES, Blowfish, PBE, Diffie-Hellman, HMAC-MD5, HMAC-SHA1)
2005/05/13 11:16:54:951 EDT [DEBUG] HttpClient - -SunJSSE 1.0303: Sun JSSE
provider(implements RSA Signatures, PKCS12, SunX509 key/trust factories, SSLv3,
TLSv1)
2005/05/13 11:16:54:961 EDT [DEBUG] HttpConnection - -Open connection to
mappoint-css.partners.extranet.microsoft.com:443
2005/05/13 11:17:00:629 EDT [DEBUG] header - ->> ""GET
/CustomerData-30/CustomerDataService.asmx HTTP/1.1[\r][\n]""
2005/05/13 11:17:00:629 EDT [DEBUG] HttpMethodBase - -Adding Host request header
2005/05/13 11:17:00:639 EDT [DEBUG] header - ->> ""User-Agent: Jakarta
Commons-HttpClient/3.0-rc2[\r][\n]""
2005/05/13 11:17:00:639 EDT [DEBUG] header - ->> ""Host:
mappoint-css.partners.extranet.microsoft.com[\r][\n]""
2005/05/13 11:17:00:639 EDT [DEBUG] header - ->> ""[\r][\n]""
2005/05/13 11:17:00:989 EDT [DEBUG] header - -<< ""HTTP/1.1 401 Unauthorized[\r][\n]""
2005/05/13 11:17:00:999 EDT [DEBUG] header - -<< ""Content-Length: 1656[\r][\n]""
2005/05/13 11:17:00:999 EDT [DEBUG] header - -<< ""Content-Type: text/html[\r][\n]""
2005/05/13 11:17:00:999 EDT [DEBUG] header - -<< ""Server: Microsoft-IIS/6.0[\r][\n]""
2005/05/13 11:17:00:999 EDT [DEBUG] header - -<< ""WWW-Authenticate: Digest
qop=""auth"",algorithm=MD5-sess,nonce=""c66759cace57c5016cf5645c6dee5b649ed29067f652939d6aaf7239310bb333eeb0153783ae445f"",opaque=""e7e259c137b65766c971d6cfc4115789"",charset=utf-8,realm=""Digest""[\r][\n]""
2005/05/13 11:17:00:999 EDT [DEBUG] header - -<< ""X-Powered-By: ASP.NET[\r][\n]""
2005/05/13 11:17:00:999 EDT [DEBUG] header - -<< ""Date: Fri, 13 May 2005
15:16:51 GMT[\r][\n]""
2005/05/13 11:17:00:999 EDT [DEBUG] HttpMethodDirector - -Authorization required
2005/05/13 11:17:01:009 EDT [DEBUG] AuthChallengeProcessor - -Supported
authentication schemes in the order of preference: [ntlm, digest, basic]
2005/05/13 11:17:01:009 EDT [DEBUG] AuthChallengeProcessor - -Challenge for ntlm
authentication scheme not available
2005/05/13 11:17:01:009 EDT [INFO] AuthChallengeProcessor - -digest
authentication scheme selected
2005/05/13 11:17:01:009 EDT [DEBUG] AuthChallengeProcessor - -Using
authentication scheme: digest
2005/05/13 11:17:01:009 EDT [DEBUG] AuthChallengeProcessor - -Authorization
challenge processed
2005/05/13 11:17:01:009 EDT [DEBUG] HttpMethodDirector - -Authentication scope:
DIGEST 'Digest'@mappoint-css.partners.extranet.microsoft.com:443
2005/05/13 11:17:01:009 EDT [DEBUG] HttpMethodDirector - -Retry authentication
2005/05/13 11:17:01:019 EDT [DEBUG] HttpMethodBase - -Resorting to protocol
version default close connection policy
2005/05/13 11:17:01:019 EDT [DEBUG] HttpMethodBase - -Should NOT close
connection, using HTTP/1.1
2005/05/13 11:17:01:019 EDT [DEBUG] HttpConnection - -Connection is locked. 
Call to releaseConnection() ignored.
2005/05/13 11:17:01:019 EDT [DEBUG] HttpMethodDirector - -Authenticating with
DIGEST 'Digest'@mappoint-css.partners.extranet.microsoft.com:443
2005/05/13 11:17:01:019 EDT [DEBUG] HttpMethodParams - -Credential charset not
configured, using HTTP element charset
2005/05/13 11:17:01:019 EDT [DEBUG] DigestScheme - -Using qop method auth
2005/05/13 11:17:01:019 EDT [DEBUG] HttpConnection - -Connection is stale,
closing...
2005/05/13 11:17:01:019 EDT [DEBUG] HttpConnection - -Open connection to
mappoint-css.partners.extranet.microsoft.com:443
2005/05/13 11:17:01:110 EDT [DEBUG] header - ->> ""GET
/CustomerData-30/CustomerDataService.asmx HTTP/1.1[\r][\n]""
2005/05/13 11:17:01:110 EDT [DEBUG] HttpMethodBase - -Adding Host request header
2005/05/13 11:17:01:110 EDT [DEBUG] header - ->> ""User-Agent: Jakarta
Commons-HttpClient/3.0-rc2[\r][\n]""
2005/05/13 11:17:01:110 EDT [DEBUG] header - ->> ""Authorization: Digest
username=""domain\user"", realm=""Digest"",
nonce=""c66759cace57c5016cf5645c6dee5b649ed29067f652939d6aaf7239310bb333eeb0153783ae445f"",
uri=""/CustomerData-30/CustomerDataService.asmx"",
response=""9cab4fcdb2d09f57523aec80d7b51e95"", qop=""auth"", nc=00000001,
cnonce=""b27507ee79c880b2bb565d363598ce07"", algorithm=""MD5-sess"",
opaque=""e7e259c137b65766c971d6cfc4115789""[\r][\n]""
2005/05/13 11:17:01:120 EDT [DEBUG] header - ->> ""Host:
mappoint-css.partners.extranet.microsoft.com[\r][\n]""
2005/05/13 11:17:01:120 EDT [DEBUG] header - ->> ""[\r][\n]""
2005/05/13 11:17:01:540 EDT [DEBUG] header - -<< ""HTTP/1.1 401 Unauthorized[\r][\n]""
2005/05/13 11:17:01:540 EDT [DEBUG] header - -<< ""Content-Length: 1539[\r][\n]""
2005/05/13 11:17:01:540 EDT [DEBUG] header - -<< ""Content-Type: text/html[\r][\n]""
2005/05/13 11:17:01:550 EDT [DEBUG] header - -<< ""Server: Microsoft-IIS/6.0[\r][\n]""
2005/05/13 11:17:01:550 EDT [DEBUG] header - -<< ""WWW-Authenticate: Digest
qop=""auth"",algorithm=MD5-sess,nonce=""3296b0dace57c5012fe314f8c6f8cafd10abc7a61c09484b2be5c7ef19ecb3c080da1f82c3f5a532"",opaque=""87578a7f0871280654aed868cb9497fb"",charset=utf-8,realm=""Digest""[\r][\n]""
2005/05/13 11:17:01:550 EDT [DEBUG] header - -<< ""X-Powered-By: ASP.NET[\r][\n]""
2005/05/13 11:17:01:550 EDT [DEBUG] header - -<< ""Date: Fri, 13 May 2005
15:17:19 GMT[\r][\n]""
2005/05/13 11:17:01:550 EDT [DEBUG] HttpMethodDirector - -Authorization required
2005/05/13 11:17:01:550 EDT [DEBUG] AuthChallengeProcessor - -Using
authentication scheme: digest
2005/05/13 11:17:01:550 EDT [DEBUG] AuthChallengeProcessor - -Authorization
challenge processed
2005/05/13 11:17:01:550 EDT [DEBUG] HttpMethodDirector - -Authentication scope:
DIGEST 'Digest'@mappoint-css.partners.extranet.microsoft.com:443
2005/05/13 11:17:01:550 EDT [DEBUG] HttpMethodDirector - -Credentials required
HTTP result: 401
2005/05/13 11:17:01:550 EDT [DEBUG] HttpMethodDirector - -Credentials provider
not available
2005/05/13 11:17:01:580 EDT [INFO] HttpMethodDirector - -Failure authenticating
with DIGEST 'Digest'@mappoint-css.partners.extranet.microsoft.com:443
2005/05/13 11:17:01:580 EDT [DEBUG] HttpMethodBase - -Buffering response body
2005/05/13 11:17:01:580 EDT [DEBUG] HttpMethodBase - -Resorting to protocol
version default close connection policy
2005/05/13 11:17:01:580 EDT [DEBUG] HttpMethodBase - -Should NOT close
connection, using HTTP/1.1
2005/05/13 11:17:01:580 EDT [DEBUG] HttpConnection - -Releasing connection back
to connection manager."
0,"IOUtils.closeSafely should log suppressed Exceptions in stack trace of original Exception (a new feature of Java 7)I was always against Java 6 support, as it brings no really helpful new features into Lucene. But there are several things that make life easier in coming Java 7 (hopefully on July 28th, 2011). One of those is simplier Exception handling and suppression on Closeable, called ""Try-With-Resources"" (see http://docs.google.com/View?id=ddv8ts74_3fs7483dp, by the way all Lucene classes support these semantics in Java 7 automatically, the cool try-code below would work e.g. for IndexWriter, TokenStreams,...).

We already have this functionality in Lucene since adding the IOUtils.closeSafely() utility (which can be removed when Java 7 is the minimum requirement of Lucene - maybe in 10 years):

{code:java}
try (Closeable a = new ...; Closeable b = new ...) {
  ... use Closeables ...
} catch (Exception e) {
  dosomething;
  throw e;
}
{code}

This code will close a and b in an autogenerated finally block and supress any exception. This is identical to our IOUtils.closeSafely:

{code:java}
Exception priorException = null;
Closeable a,b;
try (Closeable a = new ...; Closeable b = new ...) {
  a = new ...;
  b = new ...
  ... use Closeables ...
} catch (Exception e) {
  priorException = e;
  dosomething;
} finally {
  IOUtils.closeSafely(priorException, a, b);
}
{code}

So this means we have the same functionality without Java 7, but there is one thing that makes logging/debugging much nicer:
The above Java 7 code also adds maybe suppressed Exceptions in those Closeables to the priorException, so when you print the stacktrace, it not only shows the stacktrace of the original Exception, it also prints all Exceptions that were suppressed to throw this Exception (all Closeable.close() failures):

{noformat}
org.apache.lucene.util.TestIOUtils$TestException: BASE-EXCEPTION
    at org.apache.lucene.util.TestIOUtils.testSuppressedExceptions(TestIOUtils.java:61)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:601)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1486)
    at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1404)
    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
    at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
    at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:768)
    Suppressed: java.io.IOException: TEST-IO-EXCEPTION-1
            at org.apache.lucene.util.TestIOUtils$BrokenCloseable.close(TestIOUtils.java:36)
            at org.apache.lucene.util.IOUtils.closeSafely(IOUtils.java:58)
            at org.apache.lucene.util.TestIOUtils.testSuppressedExceptions(TestIOUtils.java:62)
            ... 26 more
    Suppressed: java.io.IOException: TEST-IO-EXCEPTION-2
            at org.apache.lucene.util.TestIOUtils$BrokenCloseable.close(TestIOUtils.java:36)
            at org.apache.lucene.util.IOUtils.closeSafely(IOUtils.java:58)
            at org.apache.lucene.util.TestIOUtils.testSuppressedExceptions(TestIOUtils.java:62)
            ... 26 more
{noformat}

For this in Java 7 a new method was added to Throwable, that allows logging such suppressed Exceptions (it is called automatically by the synthetic bytecode emitted by javac). This patch simply adds this functionality conditionally to IOUtils, so it ""registers"" all suppressed Exceptions, if running on Java 7. This is done by reflection: once it looks for this method in Throwable.class and if found, it invokes it in closeSafely, so the exceptions thrown on Closeable.close() don't get lost.

This makes debugging much easier and logs all problems that may occur.

This patch does *not* change functionality or behaviour, it just adds more nformation to the stack trace in a Java-7-way (similar to the way how Java 1.4 added causes). It works here locally on Java 6 and Java 7, but only Java 7 gets the additional stack traces. For Java 6 nothing changes. Same for Java 5 (if we backport to 3.x).

This would be our first Java 7 improvement (a minor one). Next would be NIO2... - but thats not easy to do with reflection only, so we have to wait 10 years :-)"
0,"FieldCacheImpl cache gets rebuilt every timeFieldCacheImpl uses WeakHashMap to store the cached objects, but since 
there is no other reference to this cache it is getting released every time."
1,"GData TestDateFormater (sic) fails when the Date returned is: Sun, 23 Sep 2007 01:29:06 GMT+00:00TestDateFormater.testFormatDate fails when the Date returned is Sun, 23 Sep 2007 01:29:06 GMT+00:00

The issue lies with the +00:00 at the end of the string.  

The question is, though, is that a valid date for GData?

This is marked as major b/c it is causing nightly builds to fail."
0,"[PATCH] Contribution: A QueryParser which passes wildcard and prefix queries to analyzerLucenes built-in QueryParser class does not analyze prefix nor wildcard queries.
Attached is a subclass which passes these queries to the analyzer as well."
0,"core analyzers should not produce tokens > N (100?) characters in lengthDiscussion that led to this:

  http://www.gossamer-threads.com/lists/lucene/java-dev/56103

I believe nearly any time a token > 100 characters in length is
produced, it's a bug in the analysis that the user is not aware of.

These long tokens cause all sorts of problems, downstream, so it's
best to catch them early at the source.

We can accomplish this by tacking on a LengthFilter onto the chains
for StandardAnalyzer, SimpleAnalyzer, WhitespaceAnalyzer, etc.

Should we do this in 2.3?  I realize this is technically a break in
backwards compatibility, however, I think it must be incredibly rare
that this break would in fact break something real in the application?"
0,FAQ documentCollect the frequently discussed issues from the mailing list and the wiki into an FAQ document.
0,SPI2DAV: setup automated test executiontask copied from JCR-1877
1,FreqProxTermsWriter leaks file handles if exceptions are thrown during flush()FreqProxTermsWriter leaks open file handles if exceptions are thrown during flush. Code needs to be protected by try-finally clauses.
0,"3.x backwards tests are using Version.LUCENE_CURRENT: aren't testing backwards!The 3.x backwards tests are mostly all using Version.LUCENE_CURRENT, therefore they don't always test the behavior as they should.

I added TEST_VERSION_CURRENT = 3.0 to the backwards/LuceneTestCase, and I think we should fix all backwards tests to use TEST_VERSION_CURRENT instead."
0,"[PATCH] UpdateTest has two typosUpdateTest has a typo where it doesn't grab an element out of an array, but uses the array itself to do comparisons. This patch fixes this."
0,"Allow easy extensions of TopDocCollectorTopDocCollector's members and constructor are declared either private or package visible. It makes it hard to extend it as if you want to extend it you can reuse its *hq* and *totatlHits* members, but need to define your own. It also forces you to override getTotalHits() and topDocs().
By changing its members and constructor (the one that accepts a PQ) to protected, we allow users to extend it in order to get a different view of 'top docs' (like TopFieldCollector does), but still enjoy its getTotalHits() and topDocs() method implementations."
0,"jackrabbit-jcr-tests should still be based on Java 1.4The JCR 2.0 TCK needs to be runnable on Java 1.4, so even though we've upgraded to Java 5 as the base platform for Jackrabbit 2.0, the jackrabbit-jcr-tests component needs to still be based on Java 1.4."
1,"NullPointerException in VirtualNodeTypeStateManager.nodeTypeRegisteredI am working on a custom persistence manager which requires an additional node type being registered. For performance reasons, the existence of this node type is verified during PersistenceManager.init.

Unfortunately this does not seem to work as the VirtualNodeTypeStateManager is not prepared to handle this situation at that point in time - the systemSession field seems to still be null."
0,"if a filter can support random access API, we should use itI ran some performance tests, comparing applying a filter via
random-access API instead of current trunk's iterator API.

This was inspired by LUCENE-1476, where we realized deletions should
really be implemented just like a filter, but then in testing found
that switching deletions to iterator was a very sizable performance
hit.

Some notes on the test:

  * Index is first 2M docs of Wikipedia.  Test machine is Mac OS X
    10.5.6, quad core Intel CPU, 6 GB RAM, java 1.6.0_07-b06-153.

  * I test across multiple queries.  1-X means an OR query, eg 1-4
    means 1 OR 2 OR 3 OR 4, whereas +1-4 is an AND query, ie 1 AND 2
    AND 3 AND 4.  ""u s"" means ""united states"" (phrase search).

  * I test with multiple filter densities (0, 1, 2, 5, 10, 25, 75, 90,
    95, 98, 99, 99.99999 (filter is non-null but all bits are set),
    100 (filter=null, control)).

  * Method high means I use random-access filter API in
    IndexSearcher's main loop.  Method low means I use random-access
    filter API down in SegmentTermDocs (just like deleted docs
    today).

  * Baseline (QPS) is current trunk, where filter is applied as iterator up
    ""high"" (ie in IndexSearcher's search loop)."
0,"Add CopyMoveHanlder so that the copy/move behavior can be customized (as this is the case for the IOHandler and PropertyHandler)The IOHandler impls let you define a specific import/export behavior either for specific nodetypes/locations/etc which is just great. this works well while you create or modify a web dav resource but does not work for the copy/move use case.
the attached patch provides a proposal for an additional CopyMoveHandler."
1,"hotspot bug in readvint gives wrong resultsWhen testing the 3.1-RC1 made by Yonik on the PANGAEA (www.pangaea.de) productive system I figured out that suddenly on a large segment (about 5 GiB) some stored fiels suddenly produce a strange deflate decompression problem (CompressionTools) although the stored fields are no longer pre-3.0 compressed. It seems that the header of the stored field is read incorrectly at the buffer boundary in MultiMMapDir and then FieldsReader just incorrectly detects a deflate-compressed field (CompressionTools).

The error occurs reproducible on CheckIndex with MMapDirectory, but not with NIODir or SimpleDir. The FDT file of that segment is 2.6 GiB, on Solaris the chunk size is Integer.MAX_VALUE, so we have 2 MultiMMap IndexInputs.

Robert and me have the index ready as a tar file, we will do tests on our local machines and hopefully solve the bug, maybe introduced by Robert's recent changes to MMap."
0,"Deprecation of autoCommit in 2.4 leads to compile problems, when autoCommit should be falseI am currently changing my code to be most compatible with 2.4. I switched on deprecation warnings and got a warning about the autoCommit parameter in IndexWriter constructors.

My code *should* use autoCommit=false, so I want to use the new semantics. The default of IndexWriter is still autoCommit=true. My problem now: How to disable autoCommit whithout deprecation warnings?

Maybe, the ""old"" constructors, that are deprecated should use autoCommit=true. But there are new constructors with this ""IndexWriter.MaxFieldLength mfl"" in it, that appear new in 2.4 but are deprecated:

IndexWriter(Directory d, boolean autoCommit, Analyzer a, boolean create, IndexDeletionPolicy deletionPolicy, IndexWriter.MaxFieldLength mfl) 
          Deprecated. This will be removed in 3.0, when autoCommit will be hardwired to false. Use IndexWriter(Directory,Analyzer,boolean,IndexDeletionPolicy,MaxFieldLength) instead, and call commit() when needed.

What the hell is meant by this, a new constructor that is deprecated? And the hint is wrong. If I use the other constructor in the warning, I get autoCommit=true.

There is something completely wrong.

It should be clear, which constructors set autoCommit=true, which set it per default to false (perhaps new ones), and the Deprecated text is wrong, if autoCommit does not default to false."
1,"FilterIndexReader does not override all of IndexReader methodsFilterIndexReader does not override all of IndexReader methods. We've hit an error in LUCENE-3573 (and fixed it). So I thought to write a simple test which asserts that FIR overrides all methods of IR (and we can filter our methods that we don't think that it should override). The test is very simple (attached), and it currently fails over these methods:
{code}
getRefCount
incRef
tryIncRef
decRef
reopen
reopen
reopen
reopen
clone
numDeletedDocs
document
setNorm
setNorm
termPositions
deleteDocument
deleteDocuments
undeleteAll
getIndexCommit
getUniqueTermCount
getTermInfosIndexDivisor
{code}

I didn't yet fix anything in FIR -- if you spot a method that you think we should not override and delegate, please comment."
0,"The cluster syncDelay attribute is millisecondsThe repository DTDs document the cluster syncDelay attribute as being the sync interval in seconds, when it really is the interval in milliseconds."
0,"UserManagement: Don't read cached memberships if session has pending (group) changesfor backwards compatibility reasons reading group membership should not access the overall cache in case of pending (group) change.
the current implememation (always reading from cache) caused a regression in test-case we have @day that relied on accurate group
membership information with having unsaved group-member changes."
0,"Occasional JCA test failuresAs discussed in JCR-2870, the JCA packaging tests are occasionally failing with assertion failures in TransientRepository.startRepository(). We haven't seen this before JCR-2870, but it doesn't look like that change could possibly trigger this failure except perhaps by subtly affecting garbage collection. Thus I'm opening this new issue to track this as a separate problem.

Based on my analysis so far it looks likely that this problem has something to do with the ReferenceMap used by TransientRepository to track open sessions. The fact that the problem occurs only occasionally and on just some systems supports the assumption that this is related to some non-deterministic process like garbage collection."
1,"Problems with maxMergeDocs parameterI found two possible problems regarding IndexWriter's maxMergeDocs value. I'm using the following code to test maxMergeDocs:

{code:java} 
  public void testMaxMergeDocs() throws IOException {
    final int maxMergeDocs = 50;
    final int numSegments = 40;
    
    MockRAMDirectory dir = new MockRAMDirectory();
    IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true);      
    writer.setMergePolicy(new LogDocMergePolicy());
    writer.setMaxMergeDocs(maxMergeDocs);

    Document doc = new Document();
    doc.add(new Field(""field"", ""aaa"", Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
    for (int i = 0; i < numSegments * maxMergeDocs; i++) {
      writer.addDocument(doc);
      //writer.flush();      // uncomment to avoid the DocumentsWriter bug
    }
    writer.close();
    
    new SegmentInfos.FindSegmentsFile(dir) {

      protected Object doBody(String segmentFileName) throws CorruptIndexException, IOException {

        SegmentInfos infos = new SegmentInfos();
        infos.read(directory, segmentFileName);
        for (int i = 0; i < infos.size(); i++) {
          assertTrue(infos.info(i).docCount <= maxMergeDocs);
        }
        return null;
      }
    }.run();
  }
{code} 
  
- It seems that DocumentsWriter does not obey the maxMergeDocs parameter. If I don't flush manually, then the index only contains one segment at the end and the test fails.

- If I flush manually after each addDocument() call, then the index contains more segments. But still, there are segments that contain more docs than maxMergeDocs, e. g. 55 vs. 50. The javadoc in IndexWriter says:
{code:java}
   /**
   * Returns the largest number of documents allowed in a
   * single segment.
   *
   * @see #setMaxMergeDocs
   */
  public int getMaxMergeDocs() {
    return getLogDocMergePolicy().getMaxMergeDocs();
  }
{code}"
1,"BindVariable not registered in JCR-SQL2 CONTAINSThe following fails with a ""java.lang.IllegalArgumentException: not a valid variable in this query:""

Query query = qm.createQuery(""SELECT * FROM [my:document] AS document WHERE CONTAINS(document.original, $x)"", Query.JCR_SQL2);
query.bindVariable(""x"", vf.createValue(""moo""));

And query.getBindVariableNames() returns an empty array.

The FullTextSearchExpression _is_ however correctly parsed as a BindVariableValueImpl:
((FullTextSearch) ((QueryObjectModelImpl) query).getConstraint()).getFullTextSearchExpression() instanceof BindVariableValue
"
1,"Enable the use of NoMergePolicy and NoMergeScheduler by BenchmarkBenchmark allows one to set the MP and MS to use, by defining the class name and then use reflection to instantiate them. However NoMP and NoMS are singletons and therefore reflection does not work for them. Easy fix in CreateIndexTask. I'll post a patch soon."
0,SimpleAnalyzer and WhitespaceAnalyzer should have Version ctorsDue to the Changes to CharTokenizer ( LUCENE-2183 ) WhitespaceAnalyzer and SimpleAnalyzer need a Version ctor. Default ctors must be deprecated
0,"Deprecate and replace TestWebapp with the SimpleHttpServer based testing frameworkBasically TestWebapp based testcases test functionality of Tomcat, rather than
that of HttpClient. They tend to get broken with every major release of Tomcat
and have proven more of a burden than any good"
0,"When authentication is invalidated during redirection, proxy authentication also should be invalidatedThis was discovered during use by Lucene Connector Framework, on 3.1.

When a document is fetched through a proxy authenticated with NTLM, and
that document is a redirection (301 or 302), the httpclient fails to
properly use the right proxy credentials on the subsequent document
fetch. This leads to 407 errors on these kinds of documents.

I've attached a proposed patch.
"
1,JcrValueType#typeFromContentType throws IllegalArgumentException for type weak-ref and uri
1,"HttpState cannot differentiate credentials for different hosts with same Realm namesIt seems that one needs a separate HttpState per client per host: from the 
javadocs, if (by coincidence or by design) more than one host uses the same 
realm name, such as ""Private"", then there's an unresolvable conflict, as 
HttpState can only store one set of credentials for a given name...

According to Oleg Kalnichevski, it is plausible just to extend the HttpState 
class with additional methods that would require host to be specified along the
authentication realm when dealing with credentials.

See postings on ""Commons HttpClient Project"" mailing list for more info (dated 
21/03/2003)."
0,"jcr-server: add possibility to PROPFIND the JCR_NODETYPES_CND_LN propertypatch by uwe jaeger see JCR-2454

in didn't include in the resolution of JCR-2454 since i would like to have JCR-2946 fixed as a prerequisite."
0,"BooleanQuery add public method that returns number of clauses this queryBooleanQuery add public method getClausesCount() that returns number of clauses this query.

current ways of getting clauses count are:
1).
 int clausesCount  = booleanQuery.getClauses().length;

or 

"
0,"non mantatory revision property in the Journal configurationAn exception is thrown if property 'revision' is not set. I think it would be great to save the revision file in the repository home dir 
when the property is not set."
1,"Incorrect SegmentInfo.delCount when IndexReader.flush() is usedWhen deleted documents are flushed using IndexReader.flush() the delCount in SegmentInfo is updated based on the current value and SegmentReader.pendingDeleteCount (introduced by LUCENE-1267). It seems that pendingDeleteCount is not reset after the commit, which means after a second flush() or close() of an index reader the delCount in SegmentInfo is incorrect. A subsequent IndexReader.open() call will fail with an error when assertions are enabled. E.g.:

java.lang.AssertionError: delete count mismatch: info=3 vs BitVector=2
	at org.apache.lucene.index.SegmentReader.loadDeletedDocs(SegmentReader.java:405)
[...]"
1,"Inconsistency between Session.getProperty and Node.getProperty for binary valuesthere an inconsistency in the binary handling between the batch-reading facility and those cases where a property is directly
accessed without having accessed the parent node before.

this issue came up with timothee maret running into performance issues when retrieving the length of a binary property:

if the property-entry has been created in the run of a batch-read operation the corresponding property-data object
contains internal values that contain the length of the binary (such as transported with the json response) and only
read the data from the server if the value stream is explicitly requested.
however, if the property is accessed directly (e.g. Session.getProperty or Node.getProperty with a relative path) 
a GET request is made to the corresponding dav resource and the stream is read immediately.

possible solution:

if RepositoryService#getItemInfos(SessionInfo, ItemId) is called with a PropertyId the implementation
should not result in a GET request to the corresponding resource by calling super.getPropertyInfo(sessionInfo, (PropertyId) itemId).
instead it should be consistent with the batch-read and only make a PROPFIND request for the property
length. the returned PropertyInfo object would in that case be identical to the one generated by the batch-read functionality.
"
1,"HostConfiguration.setHost(String) causes NullPointerExceptionCalling setHost(String) on a HostConfiguration object causes a null pointer
exception.

As far as I can tell, this is due to it incorrectly calling the deprecated
setHost(String, String, int, Protocol) method, rather than setHost(String, int,
Protocol)

So:

public synchronized void setHost(final String host) {
  Protocol defaultProtocol = Protocol.getProtocol(""http""); 
  setHost(host, null, defaultProtocol.getDefaultPort(), defaultProtocol);
}

should become :

public synchronized void setHost(final String host) {
    Protocol defaultProtocol = Protocol.getProtocol(""http""); 
    setHost(host, defaultProtocol.getDefaultPort(), defaultProtocol);
}"
0,Change log level for text extractor timeoutCurrently an info message is written to the log when a text extractor job times out. Because this may happen quite frequently this should be changed to log level debug.
0,"Remove jcr-commons dependency from jackrabbit-webdavwhile looking at JCR-2896 i just happen to see that jackrabbit-webdav contains a dependency to jcr-commons.
this was never intended to be and i want to get rid of it again... the webdav library should not have any dependency to JCR."
1,LuceneTestCase#newFSDirectoryImpl misses to set LockFactory if ctor call throws exceptionselckin reported on IRC that if you run ant test -Dtestcase=TestLockFactory -Dtestmethod=testNativeFSLockFactoryPrefix -Dtests.directory=FSDirectory the test fails. Since FSDirectory is an abstract class it can not be instantiated so our code falls back to FSDirector.open. yet we miss to set the given lockFactory though.
0,"fix or deprecate TermsEnum.skipToThis method is a trap: it looks legitimate but it has hideously poor performance (simple linear scan implemented in the TermsEnum base class since none of the concrete impls override it with a more efficient implementation).

The least we should do for 2.9 is deprecate the method with  a strong warning about its performance.

See here for background: http://www.lucidimagination.com/search/document/77dc4f8e893d3cf3/possible_terminfosreader_speedup

And, here for historical context: 

http://www.lucidimagination.com/search/document/88f1b95b404ebf16/remove_termenum_skipto_term_target"
0," The jackrabbit-ocm DTD 1.5 is missing and has to be publish
The jackrabbit-ocm DTD 1.5 is missing and it should be made available for reference on the Jackrabbit web site."
1,"Invalid cookie causing IllegalArgumentExceptionThe bug reported by Oliver KÃ¶ll <listen at quimby.de> on HttpClient mailing list

<quote>
I'm dealing with a site that serves invalid Cookies in various kind of  
ways. In some cases the Cookie values contain "","" characters, which  
really confuses the Header/Cookie parsers and eventually leads to  
IllegalArgumentExceptions thrown by the Cookie constructor:

java.lang.IllegalArgumentException: Cookie name may not be blank
   at org.apache.commons.httpclient.Cookie.<init>(Cookie.java:142)
   at  
org.apache.commons.httpclient.cookie.CookieSpecBase.parse(CookieSpecBase 
.java:192)
   at  
org.apache.commons.httpclient.cookie.CookieSpecBase.parse(CookieSpecBase 
.java:256)
   at  
org.apache.commons.httpclient.HttpMethodBase.processResponseHeaders(Http 
MethodBase.java:1826)
   at  
org.apache.commons.httpclient.HttpMethodBase.readResponse(HttpMethodBase 
.java:1939)
   at  
org.apache.commons.httpclient.HttpMethodBase.processRequest(HttpMethodBa 
se.java:2631)
   at  
org.apache.commons.httpclient.HttpMethodBase.execute(HttpMethodBase.java 
:1085)
   at  
org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:6 
74)
   at  
org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:5 
29)
   at my.code.Test.getHttp(Test.java:114)

What bothers me, is that these IllegalArgumentExceptions are never  
caught in the HttpClient code, making it effectivily impossible to  
handle these responses.

</quote>"
0,"extend ConsistencyChecker API to allow adoption of orphaned nodes to a to-be-specified parent nodeThe optional ConsistencyChecker API on persistence managers allows analyzing and fixing storage inconsistencies. The current fixup code though does not attempt to ""adopt"" orphaned nodes."
0,"contrib.benchmark.quality package improvementsFew fixes and improvements for the search quality benchmark package:
- flush report and logger at the end (otherwise long submission reports might miss last lines).
- add run-tag-name to submission report (API change).
- add control over max-#queries to run (useful at debugging a quality evaluation setup).
- move control over max-docs-to-retrieve from benchmark constructor to a setter method (API change).
- add computation of Mean Reciprocal Rank (MRR) in QualityStats.
- QualityStats fixed to not fail if there are no results to average.
- Add a TREC queries reader adequate for the 1MQ track (track started 2007).

All tests pass, will commit this in 1-2 days if there is no objection.
"
1,"SimpleFSLockFactory ignores error on deleting the lock fileSpinoff from here:

    http://www.gossamer-threads.com/lists/lucene/java-user/54438

The Lock.release for SimpleFSLockFactory ignores the return value of lockFile.delete().  I plan to throw a new LockReleaseFailedException, subclassing from IOException, when this returns false.  This is a very minor change to backwards compatibility because all methods in Lucene that release a lock already throw IOException."
1,"double encoding of URLsIn HttpMethodBase.generateRequestLine(HttpConnection connection, String name, 
String reqPath, String qString, String protocol)

the path is always encoded using URIUtil.encode(reqPath,URIUtil.pathSafe()). 
However, if the path already contains an encoding space, i.e. %20, the % will 
be encoded again, so we get %2520. This behavior is not correct. We shouldn't 
encode any % signs."
0,"Replace NodeReferencesId with NodeIdThe NodeReferencesId class simply wraps a NodeId and forwards all essential method calls to it.

The main (only?) benefit of having NodeReferencesId as a separate class is the ability to distinguish between the overloaded exists() and load() method signatures on PersistenceManager. The downside is the need to instantiate all the NodeReferencesId wrapper objects whenever accessing the references to a node.

I propose to rename the overloaded methods to hasReferencesTo(NodeId) and getReferencesTo(NodeId) and to replace the NodeReferencesId with just the target NodeId wherever used.
"
0,"NodeTypeManagerImpl.hasNodeType should allow unknown prefixesThe current implementation of NodeTypeImpl.hasNodeType(String) throws an exception if the given name uses an unknown prefix. A better alternative would be to just return false, as by definition a node type in an unknown namespace can not exist."
0,"set jcr:encoding when importing files into simple webdav serverattached is a patch that sets the jcr:encoding property when importing files into the simple webdav server. it also strips parameters from the content type before setting the jcr:mimetype property.
"
1,"Workspace.copy(src, dest) throws unexpected RepositoryException (""Invalid path"")when using the davex remoting layer (jcr2spi->spi2davex), 
the following code fragment causes an unexpected RepositoryException:

<snip>
    Node testNode1 = session.getRootNode().addNode(""test"", ""nt:folder"");

    Node copyDestination = testNode1.addNode(""CopyDestination"", ""nt:folder"");
    testNode1.addNode(""CopySource"", ""nt:folder"").addNode(""testCopyCommand"", ""nt:folder"").addNode(""abc"", ""nt:folder"");
    session.save();
    copyDestination.addMixin(""mix:referenceable"");
    session.save();

    session.getWorkspace().copy(""/test/CopySource/testCopyCommand"", ""/test/CopyDestination/testCopyCommand"");
</snip>

==> Caused by: javax.jcr.RepositoryException: Invalid path:/test/CopyDestination//testCopyCommand
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.apache.jackrabbit.spi2dav.ExceptionConverter.generate(ExceptionConverter.java:69)
	at org.apache.jackrabbit.spi2dav.ExceptionConverter.generate(ExceptionConverter.java:51)
	at org.apache.jackrabbit.spi2dav.RepositoryServiceImpl.execute(RepositoryServiceImpl.java:482)
	at org.apache.jackrabbit.spi2dav.RepositoryServiceImpl.copy(RepositoryServiceImpl.java:1307)
	at org.apache.jackrabbit.spi2davex.RepositoryServiceImpl.copy(RepositoryServiceImpl.java:326)
	at org.apache.jackrabbit.jcr2spi.WorkspaceManager$OperationVisitorImpl.visit(WorkspaceManager.java:889)
	at org.apache.jackrabbit.jcr2spi.operation.Copy.accept(Copy.java:48)
	at org.apache.jackrabbit.jcr2spi.WorkspaceManager$OperationVisitorImpl.execute(WorkspaceManager.java:848)
	at org.apache.jackrabbit.jcr2spi.WorkspaceManager$OperationVisitorImpl.access$400(WorkspaceManager.java:793)
	at org.apache.jackrabbit.jcr2spi.WorkspaceManager.execute(WorkspaceManager.java:581)
	at org.apache.jackrabbit.jcr2spi.WorkspaceImpl.copy(WorkspaceImpl.java:149)
	[...]  


however, the following slightly altered code fragment works as expected:


<snip>
    Node testNode1 = session.getRootNode().addNode(""test"", ""nt:folder"");
/*
    Node copyDestination = testNode1.addNode(""CopyDestination"", ""nt:folder"");
    testNode1.addNode(""CopySource"", ""nt:folder"").addNode(""testCopyCommand"", ""nt:folder"").addNode(""abc"", ""nt:folder"");
    session.save();
    copyDestination.addMixin(""mix:referenceable"");
    session.save();
*/
    testNode1.addNode(""CopyDestination"", ""nt:folder"").addMixin(NodeType.MIX_REFERENCEABLE);
    Node n = testNode1.addNode(""CopySource"", ""nt:folder"").addNode(""testCopyCommand"", ""nt:folder"").addNode(""abc"", ""nt:folder"");
    session.save();

    session.getWorkspace().copy(""/test/CopySource/testCopyCommand"", ""/test/CopyDestination/testCopyCommand"");
</snip>
"
0,"[API Doc] Compile new preference architecture and HTTP parameterization guideDocument the new preference architecture based on the hierarchy of HttpParams
collections, as well as available parameters and options"
0,"Provide a JackrabbitNode#setMixins(String[] mixinNames) methodassume the following scenario:

- mixin A declares the mandatory property p
- mixin A' extends from A
- node n has mixin A'
- we'd like to migrate/downgrade node n from mixin A' to A

currently there's no easy way of replacing the assigned mixins.

assigning A first results in a NOP since A would be redundant.
removing A' first removes the mandatory property p.

a new method setMixins(String[]) would allow to migrate
a node from mixin A' to A while preserving 'shared' content.
the semantics of setMixins(String[]) would be similar to
Node.setPrimaryType(String)."
1,"DictionaryCompoundWordTokenFilter does not properly add tokens from the end compound word.Due to an off-by-one error, a subword placed at the end of a compound word will not get a token added to the token stream.


For example (from the unit test in the attached patch):
Dictionary: {""ab"", ""cd"", ""ef""}
Input: ""abcdef""
Created tokens: {""abcdef"", ""ab"", ""cd""}
Expected tokens: {""abcdef"", ""ab"", ""cd"", ""ef""}


Additionally, it could produce tokens that were shorter than the minSubwordSize due to another off-by-one error. For example (again, from the attached patch):


Dictionary: {""abc"", ""d"", ""efg""}
Minimum subword length: 2
Input: ""abcdefg""
Created tokens: {""abcdef"", ""abc"", ""d"", ""efg""}
Expected tokens: {""abcdef"", ""abc"", ""efg""}
"
0,"[PATCH] HTMLParser doesn't parse hexadecimal character referencesI recently inherited a project from an ex-colleague; it uses Lucene and in
particular the HTML Parser.  I've found that she had made an amendment to the
parser to allow it to parse and decode hexadecimal character references, which
we depend on, but had not reported a bug.  If she had, someone might have
pointed out that her correction was wrong ...

I don't seem to be able to attach the (fairly trivial) patch to an initial bug
report (and in any case I've failed to find the instructions for generating a
diff file in the right format, even though I'm sure I've seen it somewhere)."
0,"NodeImpl.checkout() calls save() two timesSimilar to JCR-975, The version related properties on a versionable node that is checked out are saved individually. There is no need to save them individually because checkd in node must not have pending changes and save() can be called safely on the node itself."
1,"ParallelReader fails on deletes and on seeks of previously unused fieldsIn using ParallelReader I've hit two bugs:

1.  ParallelReader.doDelete() and doUndeleteAll() call doDelete() and doUndeleteAll() on the subreaders, but these methods do not set hasChanges.  Thus the changes are lost when the readers are closed.  The fix is to call deleteDocument() and undeleteAll() on the subreaders instead.

2.  ParallelReader discovers the fields in each subindex by using IndexReader.getFieldNames() which only finds fields that have occurred on at least one document.  In general a parallel index is designed with assignments of fields to sub-indexes and term seeks (including searches) may be done on any of those fields, even if no documents in a particular state of the index have yet had an assigned field.  Seeks/searches on fields that have not yet been indexed generated an NPE in ParallelReader's various inner class seek() and next() methods because fieldToReader.get() returns null on the unseen field.  The fix is to extend the add() methods to supply the correct list of fields for each subindex.

Patch that corrects both of these issues attached.
"
0,"benchmark for collationSteven Rowe attached a contrib/benchmark-based benchmark for collation (both jdk and icu) under LUCENE-2084, along with some instructions to run it... 

I think it would be a nice if we could turn this into a committable patch and add it to benchmark.
"
1,"Manually set 'Cookie' & 'Authorization' headers get discardedHttpClient discards all the 'Cookie' & 'Authorization' headers including those
manually set when populating request header collection with automatically
generated headers."
1,"NumericUtils.floatToSortableInt/doubleToSortableLong does not sort certain NaN ranges correctly and NumericRangeQuery produces wrong results for NaNs with half-open rangesThe current implementation of floatToSortableInt does not account for different NaN ranges which may result in NaNs sorted before -Infinity and after +Infinity. The default Java ordering is: all NaNs after Infinity.

A possible fix is to make all NaNs canonic ""quiet NaN"" as in:
{code}
// Canonicalize NaN ranges. I assume this check will be faster here than 
// (v == v) == false on the FPU? We don't distinguish between different
// flavors of NaNs here (see http://en.wikipedia.org/wiki/NaN). I guess
// in Java this doesn't matter much anyway.
if ((v & 0x7fffffff) > 0x7f800000) {
  // Apply the logic below to a canonical ""quiet NaN""
  return 0x7fc00000 ^ 0x80000000;
}
{code}

I don't commit because I don't know how much of the existing stuff relies on this (nobody should be keeping different NaNs  in their indexes, but who knows...)."
0,"Unnecessary null check in EffectiveNodeType.getApplicableChildNodeDef()This is just a trivial thing I noticed this while inspecting the code. getApplicableChildNodeDef() says:

        // try named node definitions first
        ItemDef[] defs = getNamedItemDefs(name);
        if (defs != null) {

but getNamedItemDefs() is currently defined to not return null:

    public ItemDef[] getNamedItemDefs(Name name) {
        List defs = (List) namedItemDefs.get(name);
        if (defs == null || defs.size() == 0) {
            return ItemDef.EMPTY_ARRAY;
        }
        return (ItemDef[]) defs.toArray(new ItemDef[defs.size()]);
    }

I didn't check to see if there were any other unnecessary null checks."
1,"MemoryIndex doesn't call TokenStream.reset() and TokenStream.end()MemoryIndex from contrib/memory does not honor the contract for a consumer of a TokenStream

will work up a patch right quick"
1,"SSL Tunneling does not work with MultiThreadedHttpConnectionManagerThe HttpConnection is released prematurely when doing SSL tunneling with the
MultiThreadedHttpConnectionManager.  The ConnectMethod releases the connection
in responseBodyConsumed() before it can be used by the real method."
1,"Invalid query results when using jcr:like with a case transform function and a pattern not starting with a wildcardIf the repository contains nodes with the following value for the property name :
john
JOhn
joe
Joey

and we run the following query :
//element(*, document)/*[jcr:like(fn:lower-case(@name), 'joh%')]"")
then all the previous nodes will match especially the last 2 nodes.

The reason is the use of two range scans from the lucene term index:
..._name_jOH
..................
..._name_joh_

and

..._name_JOH
..................
..._name_Joh_

The first range will contains ..._name_joe property and the second will contains ..._name_Joey.
But the pattern 'joh%' and so the regexp '.*' because of the range scan will match
the substring values of the properties ('' in the first range and 'y' in the second range).

The solution is to use the full pattern (ie 'joh.*') for matching each properties."
0,"Rename ""Version Managers""currently there is a VersionManager interface and VersionManagerImpl class that operate on the version storage.
new for JSR283, the is a javax.jcr.version.VersionManager and its implementation JcrVersionManager.

in order to avoid confusion, i would like to rename the following classes:

o.a.j.core.version.VersionManager - > o.a.j.core.version.InternalVersionManager
o.a.j.core.version.VersionManagerImpl -> o.a.j.core.version.InternalVersionManagerImpl
o.a.j.core.version.AbstractVersionManager -> o.a.j.core.version.AbstractInternalVersionManager
o.a.j.core.version.XAVersionManager -> o.a.j.core.version.XAInternalVersionManager

o.a.j.core.version.JcrVersionManagerImpl -> o.a.j.core.version.VersionManagerImpl"
0,"Add unsigned packed int impls in oal.utilThere are various places in Lucene that could take advantage of an
efficient packed unsigned int/long impl.  EG the terms dict index in
the standard codec in LUCENE-1458 could subsantially reduce it's RAM
usage.  FieldCache.StringIndex could as well.  And I think ""load into
RAM"" codecs like the one in TestExternalCodecs could use this too.

I'm picturing something very basic like:
{code}
interface PackedUnsignedLongs  {
  long get(long index);
  void set(long index, long value);
}
{code}

Plus maybe an iterator for getting and maybe also for setting.  If it
helps, most of the usages of this inside Lucene will be ""write once""
so eg the set could make that an assumption/requirement.

And a factory somewhere:

{code}
  PackedUnsignedLongs create(int count, long maxValue);
{code}

I think we should simply autogen the code (we can start from the
autogen code in LUCENE-1410), or, if there is an good existing impl
that has a compatible license that'd be great.

I don't have time near-term to do this... so if anyone has the itch,
please jump!
"
0,"RAMDirectory not SerializableThe current implementation of RAMDirectory throws a NotSerializableException when trying to serialize, due to the inner class KeySet of HashMap not being serializable (god knows why)

java.io.NotSerializableException: java.util.HashMap$KeySet
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1081)

Caused by line 43:

private Set fileNames = fileMap.keySet();

EDIT:

while we're at it: same goes for inner class Values 

java.io.NotSerializableException: java.util.HashMap$Values
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1081)

Collection files = fileMap.values();
"
0,"rename KeywordMarkerTokenFilterI would like to rename KeywordMarkerTokenFilter to KeywordMarkerFilter.
We havent released it yet, so its a good time to keep the name brief and consistent."
1,"ClassCastExeption when executing union queriesThe XPathQueryBuilder throws a ClassCastException on line 322 in release 0.9 when executing syntactically valid union queries. An example query would be ""//element(*, nt:file) or //element(*, mix:lockable)"". It appears that in the invocation of the visit method the SimpleNode id indicates a type of JJTROOTDESCENDANTS at a certain point but the data is actually an OrQueryNode and hence the cast to a PathQueryNode fails."
0,"Compressed entities are not being cached properlyorg.apache.http.impl.client.cache.CacheValidityPolicy.contentLengthHeaderMatchesActualLength() returns false for entities decompressed by ContentEncodingHttpClient, because the length of decompressed entity stored in cache will be different from the length specified in the response header.
Consequently, gzipped/deflated entities will never be satisfied from the cache.

Proposed fix: introduce new field in HttpCacheEntry() - actualContentLength, and populate it with the actual content length rigth before the cache entry is stored in the cache. Change the org.apache.http.impl.client.cache.CacheValidityPolicy.contentLengthHeaderMatchesActualLength() method to compare
entry.getResource().length() with entry.getActualContentLength()
"
0,"reorder arguments of Field constructor to be more intuitiveI think Field should take (name, value, type) not (name, type, value) ?

This seems more intuitive and consistent with previous releases

Take this change to some code I had for example:
{noformat}
-    d1.add(new Field(""foo"", ""bar"", Field.Store.YES, Field.Index.ANALYZED));
+    d1.add(new Field(""foo"", TextField.TYPE_STORED, ""bar""));
{noformat}

I think it would be better if it was
{noformat}
document.add(new Field(""foo"", ""bar"", TextField.TYPE_STORED));
{noformat}"
0,MatchAllDocsQuery to return all documentsIt would be nice to have a type of query just return all documents from an index.
0,"Absorb NIOFSDirectory into FSDirectoryI think whether one uses java.io.* vs java.nio.* or eventually
java.nio2.*, or some other means, is an under-the-hood implementation
detail of FSDirectory and doesn't merit a whole separate class.

I think FSDirectory should be the core class one uses when one's index
is in the filesystem.

So, I'd like to deprecate NIOFSDirectory, absorbing it into
FSDirectory, and add a setting ""useNIO"" to FSDirectory.  It should
default to ""true"" for non-Windows OSs, because it gives far better
concurrent performance on all platforms but Windows (due to known Sun
JRE issue http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6265734).
"
0,"Make IndexReader.decRef() call refCount.decrementAndGet instead of getAndDecrementIndexReader.decRef() has this code:

{code}
    final int rc = refCount.getAndDecrement();
    if (rc == 1) {
{code}

I think it will be clearer if it was written like this:

{code}
    final int rc = refCount.decrementAndGet();
    if (rc == 0) {
{code}

It's a very simple change, which makes reading the code (at least IMO) easier. Will post a patch shortly."
0,"DbDataStore: improve error message when init failsWhen initialization of the database data store fails, the error message does not
contain enough data to analyze the problem:

Driver: Oracle JDBC driver / 10.2.0.1.0
could not execute statement, reason: ORA-00902: invalid datatype, state/code: 42000/902
Can not init data store, driver=oracle.jdbc.OracleDriver url=jdbc:oracle:thin:@localhost:1521:orcl user=JACKRABBIT

Additionally the create table statement should be logged, and the table name."
0,"All Analysis Consumers should use reusableTokenStreamWith Analyzer now using TokenStreamComponents, theres no reason for Analysis consumers to use tokenStream() (it just gives bad performance).  Consequently all consumers will be moved over to using reusableTokenStream().  The only challenge here is that reusableTokenStream throws an IOException which many consumers are not rigged to deal with.

Once all consumers have been moved, we can rename reusableTokenStream() back to tokenStream()."
1,"deadlock in TestIndexWriterExceptions    [junit] 2012-01-18 18:18:16
    [junit] Full thread dump Java HotSpot(TM) 64-Bit Server VM (19.1-b02 mixed mode):
    [junit] 
    [junit] ""Indexer 3"" prio=10 tid=0x0000000041b9b800 nid=0x6291 waiting for monitor entry [0x00007f7e8868f000]
    [junit]    java.lang.Thread.State: BLOCKED (on object monitor)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushQueue.innerPurge(DocumentsWriterFlushQueue.java:118)
    [junit] 	- waiting to lock <0x00000000e40ff2a8> (a org.apache.lucene.index.DocumentsWriterFlushQueue)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushQueue.tryPurge(DocumentsWriterFlushQueue.java:141)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:439)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:317)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:390)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1534)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1506)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread.run(TestIndexWriterExceptions.java:187)
    [junit] 
    [junit] ""Indexer 2"" prio=10 tid=0x0000000041b9b000 nid=0x6290 waiting on condition [0x00007f7e8838c000]
    [junit]    java.lang.Thread.State: WAITING (parking)
    [junit] 	at sun.misc.Unsafe.park(Native Method)
    [junit] 	- parking to wait for  <0x00000000e4103100> (a org.apache.lucene.index.DocumentsWriterStallControl$Sync)
    [junit] 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    [junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)
    [junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireShared(AbstractQueuedSynchronizer.java:941)
    [junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireShared(AbstractQueuedSynchronizer.java:1261)
    [junit] 	at org.apache.lucene.index.DocumentsWriterStallControl.waitIfStalled(DocumentsWriterStallControl.java:115)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushControl.waitIfStalled(DocumentsWriterFlushControl.java:591)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.preUpdate(DocumentsWriter.java:302)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:362)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1534)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1506)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread.run(TestIndexWriterExceptions.java:187)
    [junit] 
    [junit] ""Indexer 1"" prio=10 tid=0x0000000042500000 nid=0x628f waiting for monitor entry [0x00007f7e8858e000]
    [junit]    java.lang.Thread.State: BLOCKED (on object monitor)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushQueue.addSegment(DocumentsWriterFlushQueue.java:84)
    [junit] 	- waiting to lock <0x00000000e40ff2a8> (a org.apache.lucene.index.DocumentsWriterFlushQueue)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:424)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:317)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:390)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1534)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1506)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread.run(TestIndexWriterExceptions.java:187)
    [junit] 
    [junit] ""Indexer 0"" prio=10 tid=0x0000000041508000 nid=0x628d waiting on condition [0x00007f7e8848d000]
    [junit]    java.lang.Thread.State: WAITING (parking)
    [junit] 	at sun.misc.Unsafe.park(Native Method)
    [junit] 	- parking to wait for  <0x00000000e414b408> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
    [junit] 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    [junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)
    [junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:842)
    [junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1178)
    [junit] 	at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:186)
    [junit] 	at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:262)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushQueue.forcePurge(DocumentsWriterFlushQueue.java:130)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushQueue.addDeletesAndPurge(DocumentsWriterFlushQueue.java:50)
    [junit] 	- locked <0x00000000e40ff2a8> (a org.apache.lucene.index.DocumentsWriterFlushQueue)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.applyAllDeletes(DocumentsWriter.java:179)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:460)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:317)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:390)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1534)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1506)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread.run(TestIndexWriterExceptions.java:187)
    [junit] 
    [junit] ""Low Memory Detector"" daemon prio=10 tid=0x00007f7e84025800 nid=0x6003 runnable [0x0000000000000000]
    [junit]    java.lang.Thread.State: RUNNABLE
    [junit] 
    [junit] ""CompilerThread1"" daemon prio=10 tid=0x00007f7e84022800 nid=0x6002 waiting on condition [0x0000000000000000]
    [junit]    java.lang.Thread.State: RUNNABLE
    [junit] 
    [junit] ""CompilerThread0"" daemon prio=10 tid=0x00007f7e8401f800 nid=0x6001 waiting on condition [0x0000000000000000]
    [junit]    java.lang.Thread.State: RUNNABLE
    [junit] 
    [junit] ""Signal Dispatcher"" daemon prio=10 tid=0x00007f7e8401d800 nid=0x6000 waiting on condition [0x0000000000000000]
    [junit]    java.lang.Thread.State: RUNNABLE
    [junit] 
    [junit] ""Finalizer"" daemon prio=10 tid=0x00007f7e84001000 nid=0x5ffa in Object.wait() [0x00007f7e8961b000]
    [junit]    java.lang.Thread.State: WAITING (on object monitor)
    [junit] 	at java.lang.Object.wait(Native Method)
    [junit] 	- waiting on <0x00000000e2ece380> (a java.lang.ref.ReferenceQueue$Lock)
    [junit] 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:118)
    [junit] 	- locked <0x00000000e2ece380> (a java.lang.ref.ReferenceQueue$Lock)
    [junit] 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:134)
    [junit] 	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:159)
    [junit] 
    [junit] ""Reference Handler"" daemon prio=10 tid=0x000000004101e000 nid=0x5ff9 in Object.wait() [0x00007f7e8971c000]
    [junit]    java.lang.Thread.State: WAITING (on object monitor)
    [junit] 	at java.lang.Object.wait(Native Method)
    [junit] 	- waiting on <0x00000000e2ece318> (a java.lang.ref.Reference$Lock)
    [junit] 	at java.lang.Object.wait(Object.java:485)
    [junit] 	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:116)
    [junit] 	- locked <0x00000000e2ece318> (a java.lang.ref.Reference$Lock)
    [junit] 
    [junit] ""main"" prio=10 tid=0x0000000040fb2000 nid=0x5fe2 in Object.wait() [0x00007f7e8ecc1000]
    [junit]    java.lang.Thread.State: WAITING (on object monitor)
    [junit] 	at java.lang.Object.wait(Native Method)
    [junit] 	- waiting on <0x00000000e4105080> (a org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread)
    [junit] 	at java.lang.Thread.join(Thread.java:1186)
    [junit] 	- locked <0x00000000e4105080> (a org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread)
    [junit] 	at java.lang.Thread.join(Thread.java:1239)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions.testRandomExceptionsThreads(TestIndexWriterExceptions.java:286)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit] 	at java.lang.reflect.Method.invoke(Method.java:597)
    [junit] 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit] 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit] 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit] 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit] 	at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$3$1.evaluate(LuceneTestCase.java:528)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:165)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)
    [junit] 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    [junit] 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    [junit] 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    [junit] 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    [junit] 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    [junit] 	at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:743)
    [junit] 
    [junit] ""VM Thread"" prio=10 tid=0x0000000041017800 nid=0x5fef runnable 
    [junit] 
    [junit] ""GC task thread#0 (ParallelGC)"" prio=10 tid=0x0000000040fc5000 nid=0x5fe3 runnable 
    [junit] 
    [junit] ""GC task thread#1 (ParallelGC)"" prio=10 tid=0x0000000040fc7000 nid=0x5fe4 runnable 
    [junit] 
    [junit] ""GC task thread#2 (ParallelGC)"" prio=10 tid=0x0000000040fc9000 nid=0x5fe5 runnable 
    [junit] 
    [junit] ""GC task thread#3 (ParallelGC)"" prio=10 tid=0x0000000040fca800 nid=0x5fe6 runnable 
    [junit] 
    [junit] ""GC task thread#4 (ParallelGC)"" prio=10 tid=0x0000000040fcc800 nid=0x5fe7 runnable 
    [junit] 
    [junit] ""GC task thread#5 (ParallelGC)"" prio=10 tid=0x0000000040fce800 nid=0x5fe8 runnable 
    [junit] 
    [junit] ""GC task thread#6 (ParallelGC)"" prio=10 tid=0x0000000040fd0000 nid=0x5fe9 runnable 
    [junit] 
    [junit] ""GC task thread#7 (ParallelGC)"" prio=10 tid=0x0000000040fd2000 nid=0x5fea runnable 
    [junit] 
    [junit] ""VM Periodic Task Thread"" prio=10 tid=0x00007f7e84030000 nid=0x6004 waiting on condition 
    [junit] 
    [junit] JNI global references: 1578
    [junit] 
    [junit] 
    [junit] Found one Java-level deadlock:
    [junit] =============================
    [junit] ""Indexer 3"":
    [junit]   waiting to lock monitor 0x0000000041477498 (object 0x00000000e40ff2a8, a org.apache.lucene.index.DocumentsWriterFlushQueue),
    [junit]   which is held by ""Indexer 0""
    [junit] ""Indexer 0"":
    [junit]   waiting for ownable synchronizer 0x00000000e414b408, (a java.util.concurrent.locks.ReentrantLock$NonfairSync),
    [junit]   which is held by ""Indexer 3""
    [junit] 
    [junit] Java stack information for the threads listed above:
    [junit] ===================================================
    [junit] ""Indexer 3"":
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushQueue.innerPurge(DocumentsWriterFlushQueue.java:118)
    [junit] 	- waiting to lock <0x00000000e40ff2a8> (a org.apache.lucene.index.DocumentsWriterFlushQueue)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushQueue.tryPurge(DocumentsWriterFlushQueue.java:141)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:439)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:317)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:390)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1534)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1506)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread.run(TestIndexWriterExceptions.java:187)
    [junit] ""Indexer 0"":
    [junit] 	at sun.misc.Unsafe.park(Native Method)
    [junit] 	- parking to wait for  <0x00000000e414b408> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
    [junit] 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    [junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)
    [junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:842)
    [junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1178)
    [junit] 	at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:186)
    [junit] 	at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:262)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushQueue.forcePurge(DocumentsWriterFlushQueue.java:130)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushQueue.addDeletesAndPurge(DocumentsWriterFlushQueue.java:50)
    [junit] 	- locked <0x00000000e40ff2a8> (a org.apache.lucene.index.DocumentsWriterFlushQueue)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.applyAllDeletes(DocumentsWriter.java:179)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:460)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:317)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:390)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1534)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1506)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread.run(TestIndexWriterExceptions.java:187)
    [junit] 
    [junit] Found 1 deadlock.
    [junit] 
    [junit] Heap
    [junit]  PSYoungGen      total 67136K, used 4647K [0x00000000f5560000, 0x00000000fbc60000, 0x0000000100000000)
    [junit]   eden space 65792K, 5% used [0x00000000f5560000,0x00000000f58a5e10,0x00000000f95a0000)
    [junit]   from space 1344K, 96% used [0x00000000f9740000,0x00000000f98840a0,0x00000000f9890000)
    [junit]   to   space 19840K, 0% used [0x00000000fa900000,0x00000000fa900000,0x00000000fbc60000)
    [junit]  PSOldGen        total 171392K, used 66868K [0x00000000e0000000, 0x00000000ea760000, 0x00000000f5560000)
    [junit]   object space 171392K, 39% used [0x00000000e0000000,0x00000000e414d080,0x00000000ea760000)
    [junit]  PSPermGen       total 21248K, used 14733K [0x00000000dae00000, 0x00000000dc2c0000, 0x00000000e0000000)
    [junit]   object space 21248K, 69% used [0x00000000dae00000,0x00000000dbc635a8,0x00000000dc2c0000)
    [junit] 
"
0,"CompactNodeTypeDefReader could auto-provide default namespace mappings if omittedthe default namespaces for 'jcr', 'nt', 'mix', '' should  be automatically registered during parsing if needed."
0,"Reducing buffer sizes for TermDocs.From java-dev: 
 
On Friday 09 September 2005 00:34, Doug Cutting wrote: 
> Paul Elschot wrote: 
> > I suppose one of these cases are when many terms are used in a query.  
> > Would it be easily possible to make the buffer size for a term iterator 
> > depend on the numbers of documents to be iterated? 
> > Many terms only occur in a few documents, so this could be a  
> > nice win on total buffer size for the many terms case. 
>  
> This would not be too difficult. 
>  
> Look in SegmentTermDocs.java.  The buffer may be allocated when the  
> parent's stream is first cloned, but clone() won't allocate a buffer if  
> the source hasn't had a buffer allocated yet, and nothing should perform  
> i/o directly on the parent's freqStream, so in practice a buffer should  
> not be allocated until the first read is performed on the clone. 
 
I tried delaying the buffer allocation in BufferedIndexInput by 
using this clone() method: 
 
  public Object clone() { 
    BufferedIndexInput clone = (BufferedIndexInput)super.clone(); 
    clone.buffer = null; 
    clone.bufferLength = 0; 
    clone.bufferPosition = 0; 
    clone.bufferStart = getFilePointer();  
    return clone; 
  } 
 
With this all term document iterators seem to be empty, no 
query in the test cases gives any results, for example TestDemo 
and TestBoolean2. 
As far as I can see, this delaying should work, but it doesn't and 
I have no idea why. 
 
End of quote from java-dev. 
 
Doug replied that at a glance this clone method looks good. 
Without this delayed buffer allocation, a reduced buffer size 
for TermDocs cannot be implemented easily."
0,"Use parallel arrays instead of PostingList objectsThis is Mike's idea that was discussed in LUCENE-2293 and LUCENE-2324.

In order to avoid having very many long-living PostingList objects in TermsHashPerField we want to switch to parallel arrays.  The termsHash will simply be a int[] which maps each term to dense termIDs.

All data that the PostingList classes currently hold will then we placed in parallel arrays, where the termID is the index into the arrays.  This will avoid the need for object pooling, will remove the overhead of object initialization and garbage collection.  Especially garbage collection should benefit significantly when the JVM runs out of memory, because in such a situation the gc mark times can get very long if there is a big number of long-living objects in memory.

Another benefit could be to build more efficient TermVectors.  We could avoid the need of having to store the term string per document in the TermVector.  Instead we could just store the segment-wide termIDs.  This would reduce the size and also make it easier to implement efficient algorithms that use TermVectors, because no term mapping across documents in a segment would be necessary.  Though this improvement we can make with a separate jira issue."
1,"[patch] QValueFactoryImpl.equals doesn't do compare correctlyequals compares it's uri to it's own uri, as poosed to the other one.

                 // for both the value has not been loaded yet
                 if (!initialized) {
                     if (other.uri != null) {
-                        return uri.equals(uri);
+                        return other.uri.equals(uri);
                     } else {
                         // need to load the binary value in order to be able
                         // to compare the 2 values."
0,"DocumentsWriter.applyDeletes should not create TermDocs or IndexSearcher if not neededDocumentsWriter.applyDeletes(IndexReader, int) always creates TermDocs and IndexSearcher, even if there were no deletes by Term or by Query. The attached patch wraps those creations w/ checks on whether there were any deletes by these two. Additionally, the searcher wasn't closed in a finally block, so I fixed that as well.

I'll attach a patch shortly."
0,Move FilterIterator and SizedIterator from package flat to package iteratorI suggest to move said classes from package org.apache.jackrabbit.commons.flat to package org.apache.jackrabbit.commons.iterator. 
0,"Extend Codec to handle also stored fields and term vectorsCurrently Codec API handles only writing/reading of term-related data, while stored fields data and term frequency vector data writing/reading is handled elsewhere.

I propose to extend the Codec API to handle this data as well."
1,"NTLM authentication failed due to closing of connectionDescription:

When dealing with a NTLM proxy server that sends response back with lines:

14:51:27:750 << HTTP/1.0 407 Proxy Authentication Required
14:51:27:796 << Date: Mon, 14 Apr 2003 19:52:43GMT[\r][\n]
14:51:27:796 << Content-Length: 257[\r][\n]
14:51:27:796 << Content-Type: text/html[\r][\n]
14:51:27:796 << Server: NetCache appliance (NetApp/5.3.1R1)[\r][\n]
14:51:27:796 << Connection: keep-alive[\r][\n]
14:51:27:796 << Proxy-Authenticate: NTLM 
TlRMTVNTUAACAAAABgAGACgAAAAGggEAtOoNy4M0g0EAAAAAAAAAAEdMT0JBTA==[\r][\n]

The httpClient code is using the ""HTTP/1.0"" as clue for closing the connection 
and ignored the ""Connection: keep-alive"".  That caused the NTLM authentication 
to fail as the NTLM requires the response to the challenge to be sent back on 
the same connection.

Proposed Fix:

Our fix is to add a flag inProxyAuthenticationRetry (in HttpMethodBase) to 
indicate that the method is doing proxy authentication retry.  When the flag is 
true, in ""HttpMethodBase.shouldCloseConnection"", check the ""Connection: keep-
alive"" before determining to close the connection."
0,"New utility: Journal walker for journal filesGiven the cluster record access provided by JCR-1789, add a journal walker utiltity that provides descriptive information about the contents of journal records."
1,"FieldCache keeps hard references to readers, doesn't prevent multiple threads from creating same instance"
1,"SQL2 parser: identifiers should be case sensitiveCurrently the SQL2 parser converts the query to uppercase before parsing. However the identifiers should be kept case sensitive.

Instead of converting the query to uppercase, String.equalsIgnoreCase should be used to compare against keywords.
"
1,webapp: troubleshooting.jsp fails
1,"IndexingQueue not checked on initial index creationWith a default value of 100 for extractorBackLogSize and lots of text extractions that time out, the temp folder gets filled with extractor*.tmp files. This is because the IndexingQueue.getFinishedDocuments() is not called during the initial index creation. This is not an issue during regular operation because the method is called periodically from a timer thread."
0,"Remove duplicate code in InternalValueFactoryAfter JCR-2245 has been applied some of the duplicate code in InternaValueFactory can be removed.

Namely:
- create(String, int)
- all other create methods that are non-binary ?"
0,"QueryManagerImpl hardwires supported query languagesQueryManagerImpl hardwires supported query languages. This seems to be sub-optimal, given the fact that Jackrabbit has a pluggable architecture for additional query languages.
"
1,"Lucene RAM Directory doesn't work for Index Size > 8 GBfrom user list - http://www.gossamer-threads.com/lists/lucene/java-user/50982

Problem seems to be casting issues in RAMInputStream.

Line 90:
      bufferStart = BUFFER_SIZE * currentBufferIndex;
both rhs are ints while lhs is long.
so a very large product would first overflow MAX_INT, become negative, and only then (auto) casted to long, but this is too late. 

Line 91: 
     bufferLength = (int) (length - bufferStart);
both rhs are longs while lhs is int.
so the (int) cast result may turn negative and the logic that follows would be wrong.
"
1,"Warning while building DAV:parent-set for root-node resourcethe following warning is generated when calculating DAV:parent-set for the resource representing the root-node:

05.12.2008 11:49:50 *WARN * DavResourceImpl: unable to calculate parent set (DavResourceImpl.java, line 955)
javax.jcr.ItemNotFoundException: root node doesn't have a parent
        at org.apache.jackrabbit.core.NodeImpl.getParent(NodeImpl.java:2078)
        at org.apache.jackrabbit.webdav.simple.DavResourceImpl.getParentElements(DavResourceImpl.java:949)
        at org.apache.jackrabbit.webdav.simple.DavResourceImpl.initProperties(DavResourceImpl.java:393)

this could simply be avoided by slightly modifying the method getParentElements (starting at line 937) and adding a test asserting that node.getParent() is not called for the root node.

"
1,"memory leak in MultiThreadedHttpConnectionManagerMultiThreadedHttpConnectionManager.getConnectionsInPool(hostConfiguration) will create HostConnectionPool entries that will not be cleaned up unless they are later used for communication. This should be changed to not create pools in that method, but rather return 0 for a non-existent pool.
"
0,"Next steps towards flexible indexingIn working on LUCENE-1410 (PFOR compression) I tried to prototype
switching the postings files to use PFOR instead of vInts for
encoding.

But it quickly became difficult.  EG we currently mux the skip data
into the .frq file, which messes up the int blocks.  We inline
payloads with positions which would also mess up the int blocks.
Skipping offsets and TermInfo offsets hardwire the file pointers of
frq & prox files yet I need to change these to block + offset, etc.

Separately this thread also started up, on how to customize how Lucene
stores positional information in the index:

  http://www.gossamer-threads.com/lists/lucene/java-user/66264

So I decided to make a bit more progress towards ""flexible indexing""
by first modularizing/isolating the classes that actually write the
index format.  The idea is to capture the logic of each (terms, freq,
positions/payloads) into separate interfaces and switch the flushing
of a new segment as well as writing the segment during merging to use
the same APIs.
"
0,Allow pseudo properties in query relationThe XPath query parser does not allow using a function name as part of a relation in a query.
0,"Typo in repository.xmlThere's another typo in repository.xml. This is basically the same as JCR-1460, but for the SearchIndex element at the bottom of the repository.xml."
0,"TCK: SetPropertyCalendarTest compares Calendar objectsSetPropertyCalendarTest# testNewCalendarPropertySession
SetPropertyCalendarTest# testModifyCalendarPropertySession
SetPropertyCalendarTest# testNewCalendarPropertyParent
SetPropertyCalendarTest# testModifyCalendarPropertyParent

Tests compare Calendar objects.  Calendar.equals(Object) is a stronger test than JSR-170 specifies for Value.equals(Object), leading to false failures.  For the purpose of these tests, even Value.equals(Object) is too strong an equality test, since some repositories may normalize date/time values across a save/read roundtrip (for example, converting ""Z"" to ""+00:00"", or adding/removing trailing zeros in fractional seconds).

Proposal: compare the getTimeInMillis() values.

--- SetPropertyCalendarTest.java        (revision 422074)
+++ SetPropertyCalendarTest.java        (working copy)
@@ -52,8 +52,8 @@
         testNode.setProperty(propertyName1, c1);
         superuser.save();
         assertEquals(""Setting property with Node.setProperty(String, Calendar) and Session.save() not working"",
-                c1,
-                testNode.getProperty(propertyName1).getDate());
+                c1.getTimeInMillis(),
+                testNode.getProperty(propertyName1).getDate().getTimeInMillis());
     }
  
     /**
@@ -66,8 +66,8 @@
         testNode.setProperty(propertyName1, c2);
         superuser.save();
         assertEquals(""Modifying property with Node.setProperty(String, Calendar) and Session.save() not working"",
-                c2,
-                testNode.getProperty(propertyName1).getDate());
+                c2.getTimeInMillis(),
+                testNode.getProperty(propertyName1).getDate().getTimeInMillis());
     }
  
     /**
@@ -78,8 +78,8 @@
         testNode.setProperty(propertyName1, c1);
         testRootNode.save();
         assertEquals(""Setting property with Node.setProperty(String, Calendar) and parentNode.save() not working"",
-                c1,
-                testNode.getProperty(propertyName1).getDate());
+                c1.getTimeInMillis(),
+                testNode.getProperty(propertyName1).getDate().getTimeInMillis());
     }
  
     /**
@@ -92,8 +92,8 @@
         testNode.setProperty(propertyName1, c2);
         testRootNode.save();
         assertEquals(""Modifying property with Node.setProperty(String, Calendar) and parentNode.save() not working"",
-                c2,
-                testNode.getProperty(propertyName1).getDate());
+                c2.getTimeInMillis(),
+                testNode.getProperty(propertyName1).getDate().getTimeInMillis());
     }
"
0,"Log at debug level rather that info in CacheManagerPlease change org.apache.jackrabbit.core.state.CacheManager#resizeAll to log at DEBUG level rather that INFO.
15:50:01,058 INFO  [CacheManager] resizeAll size=8

--- jackrabbit-core/src/main/java/org/apache/jackrabbit/core/state/CacheManager.java    (revision 565102)
+++ jackrabbit-core/src/main/java/org/apache/jackrabbit/core/state/CacheManager.java    (working copy)
@@ -122,7 +122,7 @@
      * Re-calcualte the maximum memory for each cache, and set the new limits.
      */
     private void resizeAll() {
-        log.info(""resizeAll size="" + caches.size());
+        log.debug(""resizeAll size="" + caches.size());
         // get strong references
         // entries in a weak hash map may disappear any time
         // so can't use size() / keySet() directly

"
1,"max connections per host setting does not workWhen using the MultiThreadedHttpConnectionManager the default maximal
connections per host/port cannot be exceeded (allowed maximum is 2 by default).
Attempts to exceed this by manually setting the max connections using
HttpConnectionManagerParams#setMaxConnectionsPerHost fail. This is caused by a
bug in the MultiThreadedHttpConnectionManager."
1,"More query classes with missing extractTerms()The query classes DerefQuery, RangeQuery and WildcardQuery do not overwrite the method extractTerms()."
0,"Referenced derby library behaves buggy on FreeBSDThe derby library referenced on dependencies page (http://jackrabbit.apache.org/dependencies.html) behaves buggy on FreeBSD. This is the same issue like the one with Magnolia CMS: http://jira.magnolia.info/browse/MAGNOLIA-818. Version derby-10.1.3.1 works fine.
"
0,"Remove unused ""numSlotsFull"" from FieldComparator.setNextReaderThis param is a relic from older optimizations that we've since turned off, and it's quite confusing.  I don't think we need it, and we haven't released the API yet so we're free to remove it now."
0,jcr mapping layer does not expose node move and node copy via PersistenceManager.javaThe PersistenceManagerImpl.java  in jcr-apping layer does not implement move and copy methods for a node.  
1,Scorer.skipTo() doesn't always work if called before next()skipTo() doesn't work for all scorers if called before next().
0,"A replacement for AsciiFoldingFilter that does a more thorough job of removing diacritical marks or non-spacing modifiers.The ISOLatin1AccentFilter takes Unicode characters that have diacritical marks and replaces them with a version of that character with the diacritical mark removed.  For example é becomes e.  However another equally valid way of representing an accented character in Unicode is to have the unaccented character followed by a non-spacing modifier character (like this:  é  )    The ISOLatin1AccentFilter doesn't handle the accents in decomposed unicode characters at all.    Additionally there are some instances where a word will contain what looks like an accented character, that is actually considered to be a separate unaccented character  such as  Ł  but which to make searching easier you want to fold onto the latin1  lookalike  version   L  .   

The UnicodeNormalizationFilter can filter out accents and diacritical marks whether they occur as composed characters or decomposed characters, it can also handle cases where as described above characters that look like they have diacritics (but don't) are to be folded onto the letter that they look like ( Ł  -> L )"
0,"Inefficient growth of OpenBitSetHi, I found a potentially serious efficiency problem with OpenBitSet.

One typical (I think) way to build a bit set is to set() the bits one by one -
e.g., have a HitCollector set() the bit for each matching document.
The underlying array of longs needs to grow as more as more bits are set, of
course.

But looking at the code, it appears to me that the array grows very
ineefficiently - in the worst case (when doc ids are sorted, as they would
normally be in the HitCollector case for example), copying the array again
and again for every added bit... The relevant code in OpenBitSet.java is:

  public void set(long index) {
    int wordNum = expandingWordNum(index);
    ...
  }

  protected int expandingWordNum(long index) {
    int wordNum = (int)(index >> 6);
    if (wordNum>=wlen) {
      ensureCapacity(index+1);
    ...
  }
  public void ensureCapacityWords(int numWords) {
    if (bits.length < numWords) {
      long[] newBits = new long[numWords];
      System.arraycopy(bits,0,newBits,0,wlen);
      bits = newBits;
    }
  }

As you can see, if the bits array is not long enough, a new one is
allocated at exactly the right size - and in the worst case it can grow
just one word every time...

Shouldn't the growth be more exponential in nature, e.g., grow to the maximum
of index+1 and twice the existing size?

Alternatively, if the growth is so inefficient, this should be documented,
and it should be recommended to use the variant of the constructor with the
correct initial size (e.g., in the HitCollector case, the number of documents
in the index). and the fastSet() method instead of set().

Thanks,
Nadav.
"
1,"Node.setProperty(String, String) does not convert valueswhen setting the value of a defined property via the Node.setProperty(String, String) method, a ConstraintViolationException is thrown. but the string value should be converted, or a ValueFormatException must be thrown.

"
1,"Long values not properly storedWhen a long value assigned to a property is too big, when restarting the server the value become 0 !! 

The test pass with versions 1.6.4 and 2.0"
0,Performance improvement in OpenBitSetDISI.inPlaceAnd()
0,"Create merge policy that doesn't periodically inadvertently optimizeThe current merge policy, at every maxBufferedDocs *
power-of-mergeFactor docs added, will do a fully cascaded merge, which
is the same as an optimize.

I think this is not good because at that ""optimization poin"", the
particular addDocument call is [surprisingly] very expensive.  While,
amortized over all addDocument calls, the cost is low, the cost is
paid ""up front"" and in a very ""bunched up"" manner.

I think of this as ""pay it forward"": you are paying the full cost of
an optimize right now on the expectation / hope that you will be
adding a great many more docs.  But, if you don't add that many more
docs, then, the amortized cost for your index is in fact far higher
than it should have been.  Better to ""pay as you go"" instead.

So we could make a small change to the policy by only merging the
first mergeFactor segments once we hit 2X the merge factor.  With
mergeFactor=10, when we have created the 20th level 0 (just flushed)
segment, we merge the first 10 into a level 1 segment.  Then on
creating another 10 level 0 segments, we merge the second set of 10
level 0 segments into a level 1 segment, etc.

With this new merge policy, an index that's a bit bigger than a
current ""optimization point"" would then have a lower amortized cost
per document.  Plus the merge cost is less ""bunched up"" and less ""pay
it forward"": instead you pay for what you are actually using.

We can start by creating this merge policy (probably, combined with
with the ""by size not by doc count"" segment level computation from
LUCENE-845) and then later decide whether we should make it the
default merge policy.
"
0,Speed up NodeIndexer.isIndexed() checkThe isIndexed() method is called for every value in a multi-valued property. This may be quite expensive when there are a lot of values.
1,WebDAV: LocatorFactoryImplEx doesn't properly evaluate resource path...... if the item path happens to start with the workspace name.
0,Incorrect link on Apache Jackrabbit Welcome homepageLink to JSR283 in welcome text points to http://jcp.org/en/jsr/detail?id=170 instead of http://jcp.org/en/jsr/detail?id=283
1,"Avoid exceptions during shutting repository down if several PMs/FSs use same DBAccording to docs and forum discussions, it's legal to use same DB for different FileSystems/Persistence Managers. Such configurations seem to work fine, but when repository is stopped, exceptions are produced like following:

SEVERE: Error while closing Version Manager.
java.sql.SQLNonTransientConnectionException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.getMetaData(Unknown Source)
	at org.apache.jackrabbit.core.persistence.db.DerbyPersistenceManager.closeConnection(DerbyPersistenceManager.java:109)
	at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.close(DatabasePersistenceManager.java:261)
	at org.apache.jackrabbit.core.version.VersionManagerImpl.close(VersionManagerImpl.java:201)
	at org.apache.jackrabbit.core.RepositoryImpl.doShutdown(RepositoryImpl.java:1000)
	at org.apache.jackrabbit.core.RepositoryImpl.shutdown(RepositoryImpl.java:948)
	at org.apache.jackrabbit.core.TransientRepository.stopRepository(TransientRepository.java:275)
	at org.apache.jackrabbit.core.TransientRepository.loggedOut(TransientRepository.java:427)
	at org.apache.jackrabbit.core.SessionImpl.notifyLoggedOut(SessionImpl.java:574)
	at org.apache.jackrabbit.core.SessionImpl.logout(SessionImpl.java:1247)
	at org.apache.jackrabbit.core.XASessionImpl.logout(XASessionImpl.java:403)
	at com.blandware.tooling.jcrplugin.ExportMojo.execute(ExportMojo.java:81)
	at org.apache.maven.plugin.DefaultPluginManager.executeMojo(DefaultPluginManager.java:447)
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoals(DefaultLifecycleExecutor.java:539)
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeStandaloneGoal(DefaultLifecycleExecutor.java:493)
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoal(DefaultLifecycleExecutor.java:463)
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoalAndHandleFailures(DefaultLifecycleExecutor.java:311)
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeTaskSegments(DefaultLifecycleExecutor.java:278)
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.execute(DefaultLifecycleExecutor.java:143)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:333)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:126)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:282)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.classworlds.Launcher.launchEnhanced(Launcher.java:315)
	at org.codehaus.classworlds.Launcher.launch(Launcher.java:255)
	at org.codehaus.classworlds.Launcher.mainWithExitCode(Launcher.java:430)
	at org.codehaus.classworlds.Launcher.main(Launcher.java:375)
Caused by: java.sql.SQLException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)
	... 35 more
"
0,"Remove or deprecate contrib/similarityClasses under contrib/similarity seem to be duplicates of classes under contrib/queries.
I'd like to remove *.java from contrib/similarity without bothering with deprecation, since the same functionality exists in contrib/queries.
Anyone minds?
"
0,Make it possible to configure Lucene Analyzer for SearchIndexJackrabbit does not support the specification of a Lucene analyzer class (org.apache.lucene.analysis.Analyzer) to be used by a SearchIndex (or other Lucene based indices) declared in an XML configuration file. Custom analyzer is useful for indexing language specific content.
1,"BooleanFilter changed behavior in 3.5, no longer acts as if ""minimum should match"" set to 1The change LUCENE-3446 causes a change in behavior in BooleanFilter. It used to work as if minimum should match clauses is 1 (compared to BQ lingo), but now, if no should clauses match, then the should clauses are ignored, and for example, if there is a must clause, only that one will be used and returned.

For example, a single must clause and should clause, with the should clause not matching anything, should not match anything, but, it will match whatever the must clause matches.

The fix is simple, after iterating over the should clauses, if the aggregated bitset is null, return null."
0,"NumericField should be stored in binary format in index (matching Solr's format)(Spinoff of LUCENE-3001)

Today when writing stored fields we don't record that the field was a NumericField, and so at IndexReader time you get back an ""ordinary"" Field and your number has turned into a string.  See https://issues.apache.org/jira/browse/LUCENE-1701?focusedCommentId=12721972&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-12721972

We have spare bits already in stored fields, so, we should use one to record that the field is numeric, and then encode the numeric field in Solr's more-compact binary format.

A nice side-effect is we fix the long standing issue that you don't get a NumericField back when loading your document."
1,"InternalXAResource.rollback() can be called twice and without prepareduring the prepare phase in the transaction context, each resource is 'prepared'. if one of them fails to prepare, the rest is rolledback, and later all of them are rolledback again. this can cause that:
- a resource that is never prepared is rolled back (which is ok, since is may need to cleanup stuff)
- a resource's rollback() may be called twice (which i don't know, if it's ok)

however, some of the resources are buggy and can't handle neither case correctly."
1,"Exception during IndexWriter.close() prevents release of the write.lockAfter encountering a case of index corruption - see http://issues.apache.org/jira/browse/LUCENE-140 - when the close() method encounters an exception in the flushRamSegments() method, the index write.lock is not released (ie. it is not really closed).

The writelock is only released when the IndexWriter is GC'd and finalize() is called."
0,provide a memcached implementation for HttpCacheThe feature here would be an implementation of the HttpCache interface that stored cache entries in memcached.
1,document field lengths count analyzer synonym overlaysUsing a synonym expansion analyzer to add tokens with zero offset from the substituted token should not extend the length of the field in the document (for scoring purposes)
1,SpanScorer fails when sloppyFreq() returns 0I think we should fix this for 2.4 (now back to 10)?
0,"Unnecessary hasItemState() call in SessionItemStateManagerAt the end of  SessionItemStateManager.getItemState(ItemId) the underlying item state manager is first asked whether it contains the item and then it is retrieved. In case the item state manager does not know the item a NoSuchItemStateException is thrown.

The initial check is unnecessary because getItemState() on the underlying manager will also throw the exception if the item does not exist."
1,"BlockJoinCollector only allows retrieving groups for only one BlockJoinQuerySpinoff from Mark Harwood's email (subject ""BlockJoin concerns"") to
dev list.

It's fine to use multiple nested joins in a single query, and
BlockJoinCollector should let you retrieve the top groups for all of
them.

But currently it always returns null after the first query's groups
have been retrieved, because of a silly bug.
"
1,"NodeReferencesId.equals() is not symetricNodeReferencesId.equals() is not symetric when equality is tested against a NodeId.

Code example:
UUID uuid = UUID.randomUUID();
NodeId id = new NodeId(uuid);
NodeReferencesId refId = new NodeReferencesId(uuid);
id.equals(refId); // will return true
refId.equals(id); // will return false

NodeReferencesId should be decouled from the ItemId hierarchy. The class NodeReferences already does not extend from NodeState which makes perfectly sense. So, the same should apply to the identifier of NodeReferences.

The attached patch to NodeReferencesId also requires minor changes to some of the persistence managers."

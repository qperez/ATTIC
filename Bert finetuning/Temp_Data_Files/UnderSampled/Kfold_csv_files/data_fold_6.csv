label,summmarydescription
0,"QDefinitionBuilderFactory should auto-subtype from nt:baseSimilar to JCR-2066, the QNodeTypeDefinitions build by QDefinitionBuilderFactory should auto subtype from nt:base. "
0,"filter jcr properties in jcr-serverattached is a patch that implements jcr property filtering in jcr-server in the same way that nodes and resources are filtered. with the default filter configuration, this has the effect of filtering jcr:created, jcr:mixinTypes, and jcr:primaryType from nt:folder and nt:file nodes. 

this is likely the expected default behavior for most webdav servers - they want to return the normal dav properties, live properties defined themselves, and dead properties defined by clients, but not jcr-internal properties which are for all intents and purposes implementation-specific."
0,"ProtocolSocketFactory equals and hashCode don't support subclassingIn the implemenation of equals and hashCode for the classes
org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory
org.apache.commons.httpclient.protocol.SSLProtocolSocketFactory

The implementation of equals and hashCode attempts to make all instances of the classes equal.  However, the manner in which the methods are coded makes it necessary for any subclass to implement equals and hashCode themselves.  A minor change to the methods in these classes will make possible to subclass these factories without re-implementing the equals and hashCode.  The method equals should be written as

        return ((obj != null) && obj.getClass().equals(getClass()));

rather than

        return ((obj != null) && obj.getClass().equals(DefaultProtocolSocketFactory.class));

And similarly, the hashCode method should be

        return getClass().hashCode();

rather than

        return DefaultProtocolSocketFactory.class.hashCode();"
1,"Node.checkin() throws ArrayIndexOutOfBoundsExceptionI get an ArrayIndexOutOfBoundsException for index 0 when checking-in a node. After drilling into the code I found, that during checkin, the jcr:uuid property (defined as OPV INITIALIZE) is not copied from the node to the frozen node.

After checkin though the implementation tries to access the string value of the jcr:uuid property, which is not existing, hence the internal property implementation throws the exception when accessing the first element in the empty value array.

As a workaround I currently the set jcr:uuid property to OPV=COPY in the mixin:referenceable node type. But I could imagine, that this might be incorrect according to the spec, yet it works in my use case."
0,Improve performance of simple path queriesQueries with simple path constraints can be quite slow because of the way they are implemented. The current implementation basically does a hierarchical join with the context nodes and the set of nodes with the name of the next location step. When the specified path is quite selective the implementation should   rather resolve the path expression using the item state manager (similar to how regular paths are resolved in the JCR API).
0,"Rename IndexReader.reopen to make it clear that reopen may not happenSpinoff from LUCENE-3454 where Shai noted this inconsistency.

IR.reopen sounds like an unconditional operation, which has trapped users in the past into always closing the old reader instead of only closing it if the returned reader is new.

I think this hidden maybe-ness is trappy and we should rename it (maybeReopen?  reopenIfNeeded?).

In addition, instead of returning ""this"" when the reopen didn't happen, I think we should return null to enforce proper usage of the maybe-ness of this API."
0,"Preserving UUID and document version history on repository migrationI have been working I an migration utility for OpenKM and I performed some changes in jackrabit-core to enable version import, preserving
the modification date. Also modified org.apache.jackrabbit.core.NodeImpl to preserve UUID in the migration process.

This migration process is needed because there are changes in repository node definition, and Jackrabbit can't deal with this actually.

I've attache a PDF with the changes needed in Jackrabbit-core. It works and there was no problems with the migrated repository."
0,"httpClient does not support installation of different SSLSocketFactoryDescription:

The SSLProtocolSocketFactory class had hard-
coded ""javax.net.ssl.SSLSocketFactory"" as the socket factory.  It does not 
support installation of other socket factory.

Proposed Fix:

We added a setDefaultSSLSocketFactory method to the SSLProtocolSocketFactory 
and modified the code to use the factory it it is set.  The code falls back on 
using ""javax.net.ssl.SSLSocketFactory"" if a default is not set."
1,jackrabbit-jcr-client tests fail (and are disabled in pom)I suggest to enable the tests and fix the issues causing them to fail. 
0,"Document is partially indexed on an unhandled exceptionWith LUCENE-843, it's now possible for a subset of a document's
fields/terms to be indexed or stored when an exception is hit.  This
was not the case in the past (it was ""all or none"").

I plan to make it ""all or none"" again by immediately marking a
document as deleted if any exception is hit while indexing it.

Discussion leading up to this:

  http://www.gossamer-threads.com/lists/lucene/java-dev/56103
"
1,"Bundle cache is not rolled back when the storage of a ChangeLog failsThe bundle cache in the bundle persistence managers is not restored to its old state when the AbstractBundlePersistenceManager.store(ChangeLog changeLog) method throws an exception. If, for instance, the storage of references fails then the AbstractBundlePersistenceManager.putBundle(NodePropBundle bundle) method has already been called for all modified bundles. Because of the connection rollback, the bundle cache will be out-of-sync with the persistent state. As a result, the SharedItemStateManager will have an incorrect view of the persistent state.
Furthermore, if the blockOnConnectionLoss property is set to true, then the BundleDbPersistenceManager can be caught in an infinite loop because of invalid SQL inserts because of an incorrect bundle cache; see attached stacktrace."
0,"PhraseQuery/TermQuery/SpanQuery use IndexReader specific stats in their explainsPhraseQuery uses IndexReader in explainfor top level stats - as mentioned by Mike McCandless in LUCENE-1837.
TermQuery uses IndexReader in explain for top level stats

Always been a bug with MultiSearcher, but per segment search makes it worse.

"
0,Include the WebDAV litmus tests in the Jackrabbit integration testsIt would be great to integrate the litmus tests (http://www.webdav.org/neon/litmus/) to our integration test suite.
0,"Add TopDocs.merge to merge multiple TopDocsIt's not easy today to merge TopDocs, eg produced by multiple shards,
supporting arbitrary Sort.
"
0,"Make JCAManagedConnectionFactory non final, so it can be extendedHello,

Is there a reason why JCAManagedConnectionFactory is final?
I need to build my own one and I'd rather reuse some code of yours.
"
1,"Large distances in Spatial go beyond Prime MEridianhttp://amidev.kaango.com/solr/core0/select?fl=*&json.nl=map&wt=json&radius=5000&rows=20&lat=39.5500507&q=honda&qt=geo&long=-105.7820674

Get an error when using Solr when distance is calculated for the boundary box past 90 degrees.


Aug 4, 2009 1:54:00 PM org.apache.solr.common.SolrException log
SEVERE: java.lang.IllegalArgumentException: Illegal lattitude value 93.1558669413734
        at org.apache.lucene.spatial.geometry.FloatLatLng.<init>(FloatLatLng.java:26)
        at org.apache.lucene.spatial.geometry.shape.LLRect.createBox(LLRect.java:93)
        at org.apache.lucene.spatial.tier.DistanceUtils.getBoundary(DistanceUtils.java:50)
        at org.apache.lucene.spatial.tier.CartesianPolyFilterBuilder.getBoxShape(CartesianPolyFilterBuilder.java:47)
        at org.apache.lucene.spatial.tier.CartesianPolyFilterBuilder.getBoundingArea(CartesianPolyFilterBuilder.java:109)
        at org.apache.lucene.spatial.tier.DistanceQueryBuilder.<init>(DistanceQueryBuilder.java:61)
        at com.pjaol.search.solr.component.LocalSolrQueryComponent.prepare(LocalSolrQueryComponent.java:151)
        at org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:174)
        at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:131)
        at org.apache.solr.core.SolrCore.execute(SolrCore.java:1328)
        at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:341)
        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:244)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:128)
        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102)
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)
        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:286)
        at org.apache.coyote.http11.Http11AprProcessor.process(Http11AprProcessor.java:857)
        at org.apache.coyote.http11.Http11AprProtocol$Http11ConnectionHandler.process(Http11AprProtocol.java:565)
        at org.apache.tomcat.util.net.AprEndpoint$Worker.run(AprEndpoint.java:1509)
        at java.lang.Thread.run(Thread.java:619)


"
0,"Default KuromojiAnalyzer to use search modeKuromoji supports an option to segment text in a way more suitable for search,
by preventing long compound nouns as indexing terms.

In general 'how you segment' can be important depending on the application 
(see http://nlp.stanford.edu/pubs/acl-wmt08-cws.pdf for some studies on this in chinese)

The current algorithm punishes the cost based on some parameters (SEARCH_MODE_PENALTY, SEARCH_MODE_LENGTH, etc)
for long runs of kanji.

Some questions (these can be separate future issues if any useful ideas come out):
* should these parameters continue to be static-final, or configurable?
* should POS also play a role in the algorithm (can/should we refine exactly what we decompound)?
* is the Tokenizer the best place to do this, or should we do it in a tokenfilter? or both?
  with a tokenfilter, one idea would be to also preserve the original indexing term, overlapping it: e.g. ABCD -> AB, CD, ABCD(posInc=0)
  from my understanding this tends to help with noun compounds in other languages, because IDF of the original term boosts 'exact' compound matches.
  but does a tokenfilter provide the segmenter enough 'context' to do this properly?

Either way, I think as a start we should turn on what we have by default: its likely a very easy win.
"
0,"Signature changes in AttributeSource for better Generics support of AddAttribute/getAttributeThe last update of Attribute API using AttributeImpl as implementation oif Attributes changed the API a little bit. This change leads to the fact, that in Java 1.5 using generics we are no longer able to add Attributes without casting. addAttribute and getAttribute should return the Attribute interface because the implementation of the attribute is not interesting to the caller. By that in 1.5 using generics, one could add a TermAttribute without casting using:
{code}
TermAttribute termAtt = addAttribute(TermAttribute.class);
{code}
The signature to do this is:
{code}
public <T extends Attribute> T addAttribute(Class<T>)
{code}

The attached patch applies the mentioned change to the signature (without generic, only returning Attribute). No other code changes are needed, as current code always casts the result to the requested interface. I also added the 1.5 method signature for all these methods to the javadocs.

All tests pass."
0,"NewAnalyzerTaskNewAnalyzerTask (patch to follow) allows a contrib/benchmark algorithm to change Analyzers during a run.  This is useful when comparing Analyzers

{""NewAnalyzer"" NewAnalyzer(WhitespaceAnalyzer, SimpleAnalyzer, StopAnalyzer, standard.StandardAnalyzer) >

is a sample declaration in an algorithm file."
0,"WindowsDirectoryWe can use Windows' overlapped IO to do pread() and avoid the performance problems of SimpleFS/NIOFSDir.
"
0,"check all tests that use FSDirectory.openIn LUCENE-2471 we were discussing the copyBytes issue, and Shai and I had a discussion about how we could prevent such bugs in the future.

One thing that lead to the bug existing in our code for so long, was that it only happened on windows (e.g. never failed in hudson!)
This was because the bug only happened if you were copying from SimpleFSDirectory, and the test used FSDirectory.open

Today the situation is improving: most tests use newDirectory() which is random by default and never use FSDir.open,
it always uses SimpleFS or NIOFS so that the same random seed will reproduce across both windows and unix.

So I think we need to review all uses of FSDirectory.open in our tests, and minimize these.
In general tests should use newDirectory().
If the test comes with say a zip-file and wants to explicitly open stuff from disk, I think it should open the contents with say SimpleFSDir,
and then call newDirectory(Directory) to copy into a new ""random"" implementation for actual testing. This method already exists:
{noformat}
  /**
   * Returns a new Dictionary instance, with contents copied from the
   * provided directory. See {@link #newDirectory()} for more
   * information.
   */
  public static MockDirectoryWrapper newDirectory(Directory d) throws IOException {
{noformat}
"
1,"Uncaught AbstractMethodError exception in in DomUtil.createFactory()DomUtil.createFactory() throws an uncaught AbstractMethodError exception when xerces is on the classpath and the jackrabbit webdav module is used. 

This can render the class unusable when used in conjunction with the xerces library. 
"
1,"NullPointerException in BooleanFilter BooleanFilter getDISI() method used with QueryWrapperFilter occur NullPointerException,
if any QueryWrapperFilter not match terms in IndexReader.

---------------------------------------------------
java.lang.NullPointerException
	at org.apache.lucene.util.OpenBitSetDISI.inPlaceAnd(OpenBitSetDISI.java:66)
	at org.apache.lucene.search.BooleanFilter.getDocIdSet(BooleanFilter.java:102)
	at org.apache.lucene.search.IndexSearcher.searchWithFilter(IndexSearcher.java:551)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:532)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:463)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:433)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:356)
	at test.BooleanFilterTest.main(BooleanFilterTest.java:50)
---------------------------------------------------

null-check below lines.
---------------------------------------------------
res = new OpenBitSetDISI(getDISI(shouldFilters, i, reader), reader.maxDoc());
res.inPlaceOr(getDISI(shouldFilters, i, reader));
res = new OpenBitSetDISI(getDISI(notFilters, i, reader), reader.maxDoc());
res.inPlaceNot(getDISI(notFilters, i, reader));
res = new OpenBitSetDISI(getDISI(mustFilters, i, reader), reader.maxDoc());
res.inPlaceAnd(getDISI(mustFilters, i, reader));
---------------------------------------------------"
0,Do not log warning when coercing value in query is not possibleThe LuceneQueryBuilder currently logs a warning when a String literal cannot be coerced into a type derived from information provided by the node type manager. The log level should be lowered to debug.
0,"QueryParser support for MatchAllDocsIt seems like there really should be QueryParser support for MatchAllDocsQuery.
I propose *:* (brings back memories of DOS :-)
"
0,Remove deprecated Filter.bits() and make Filter.getDocIdSet() abstract.
1,"Using WildcardQuery with MultiSearcher, and Boolean MUST_NOT clauseWe are searching across multiple indices using a MultiSearcher. There seems to be a problem when we use a WildcardQuery to exclude documents from the result set. I attach a set of unit tests illustrating the problem.

In these tests, we have two indices. Each index contains a set of documents with fields for 'title',  'section' and 'index'. The final aim is to do a keyword search, across both indices, on the title field and be able to exclude documents from certain sections (and their subsections) using a
WildcardQuery on the section field.
 
 e.g. return documents from both indices which have the string 'xyzpqr' in their title but which do not lie
 in the news section or its subsections (section = /news/*).
 
The first unit test (testExcludeSectionsWildCard) fails trying to do this.
 If we relax any of the constraints made above, tests pass:
 
* Don't use WildcardQuery, but pass in the news section and it's child section to exclude explicitly (testExcludeSectionsExplicit)</li>
* Exclude results from just one section, not it's children too i.e. don't use WildcardQuery(testExcludeSingleSection)</li>
* Do use WildcardQuery, and exclude a section and its children, but just use one index thereby using the simple
   IndexReader and IndexSearcher objects (testExcludeSectionsOneIndex).
* Try the boolean MUST clause rather than MUST_NOT using the WildcardQuery i.e. only include results from the /news/ section
   and its children."
1,"Extend mimetype list of text extractorsDo you think it would be possible to extend the mimetype list of the
MsPowerpoint and MsExcel textextractors with ""application/powerpoint"" and
""application/excel""? 

It just took me half an hour to figure out why my
documents didn't turn up in a jackrabbit fulltext-search and maybe other
users might run into the same problem...

I'm not sure if there is some kind of standard which lists the possible
default mimetypes but after a quick google search it seems to me that they
are not that uncommon.
"
1,"Fix and simplify CryptedSimpleCredentialsthe credentials retrieved from UserImpl and used to validate the simplecredentials passed to the repository login is overly complex
and buggy as it tries to match all kind credentials variants with and without hashed password.
in particular it contains the following problems:
- simplecredentials containing the hashed pw are considered valid
- passwords startign with {something} cause inconsistencies and may even prevent the user from login

it should be improved as follows:
- simplecredentials are always expected to contain the plain text password both for creation and
  comparison with the cryptedsimplecredentials.
- creating cryptedsimplecredentials from uid/pw however is left unchanged: the specified pw is
  hashed with the default algorithm if it turns out not to be in the hashed format.
- in addition the pw should also be hashed if it has the form {something}whatever but something
  is an invalid algorithm.
"
0,"Testing for indexable properties should check the default indexable properties firstorg.apache.jackrabbit.core.query.lucene.NodeIndexer#addValue, uses the following condition for a PropertyType.NAME type of property:

if (isIndexed(name)
                    || name.equals(NameConstants.JCR_PRIMARYTYPE)
                    || name.equals(NameConstants.JCR_MIXINTYPES)) {
                addNameValue(doc, fieldName, value.getQName());
}

It'd be more efficient to test the default properties first (which are on every node anyway) than to query the custom indexing rules every time. "
0,"Adding DerbyDataStore to handle proper close of the embedded databaseWhen using embedded Derby in conjunction with DbDataStore, the Derby database is never shutdown, as it requires special code to be executed (creating a Connection with "";shutdown=true"")
We may provide a DerbyDataStore extending standard DbDataStore for handling that."
1,"KeywordTokenizer does not properly set the end offsetKeywordTokenizer sets the Token's term length attribute but appears to omit the end offset. The issue was discovered while using a highlighter with the KeywordAnalyzer. KeywordAnalyzer delegates to KeywordTokenizer propagating the bug. 

Below is a JUnit test (source is also attached) that exercises various analyzers via a Highlighter instance. Every analyzer but the KeywordAnazlyzer successfully wraps the text with the highlight tags, such as ""<b>thetext</b>"". When using KeywordAnalyzer the tags appear before the text, for example: ""<b></b>thetext"". 

Please note NewKeywordAnalyzer and NewKeywordTokenizer classes below. When using NewKeywordAnalyzer the tags are properly placed around the text. The NewKeywordTokenizer overrides the next method of the KeywordTokenizer setting the end offset for the returned Token. NewKeywordAnalyzer utilizes KeywordTokenizer to produce proper token.

Unless there is an objection I will gladly post a patch in the very near future . 

-----------------------------
package lucene;

import java.io.IOException;
import java.io.Reader;

import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.analysis.KeywordAnalyzer;
import org.apache.lucene.analysis.KeywordTokenizer;
import org.apache.lucene.analysis.SimpleAnalyzer;
import org.apache.lucene.analysis.StopAnalyzer;
import org.apache.lucene.analysis.Token;
import org.apache.lucene.analysis.TokenStream;
import org.apache.lucene.analysis.Tokenizer;
import org.apache.lucene.analysis.WhitespaceAnalyzer;
import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.search.highlight.Highlighter;
import org.apache.lucene.search.highlight.QueryScorer;
import org.apache.lucene.search.highlight.SimpleHTMLFormatter;
import org.apache.lucene.search.highlight.WeightedTerm;
import org.junit.Test;
import static org.junit.Assert.*;

public class AnalyzerBug {

	@Test
	public void testWithHighlighting() throws IOException {
		String text = ""thetext"";
		WeightedTerm[] terms = { new WeightedTerm(1.0f, text) };

		Highlighter highlighter = new Highlighter(new SimpleHTMLFormatter(
				""<b>"", ""</b>""), new QueryScorer(terms));

		Analyzer[] analazers = { new StandardAnalyzer(), new SimpleAnalyzer(),
				new StopAnalyzer(), new WhitespaceAnalyzer(),
				new NewKeywordAnalyzer(), new KeywordAnalyzer() };

		// Analyzers pass except KeywordAnalyzer
		for (Analyzer analazer : analazers) {
			String highighted = highlighter.getBestFragment(analazer,
					""CONTENT"", text);
			assertEquals(""Failed for "" + analazer.getClass().getName(), ""<b>""
					+ text + ""</b>"", highighted);
			System.out.println(analazer.getClass().getName()
					+ "" passed, value highlighted: "" + highighted);
		}
	}
}

class NewKeywordAnalyzer extends KeywordAnalyzer {

	@Override
	public TokenStream reusableTokenStream(String fieldName, Reader reader)
			throws IOException {
		Tokenizer tokenizer = (Tokenizer) getPreviousTokenStream();
		if (tokenizer == null) {
			tokenizer = new NewKeywordTokenizer(reader);
			setPreviousTokenStream(tokenizer);
		} else
			tokenizer.reset(reader);
		return tokenizer;
	}

	@Override
	public TokenStream tokenStream(String fieldName, Reader reader) {
		return new NewKeywordTokenizer(reader);
	}
}

class NewKeywordTokenizer extends KeywordTokenizer {
	public NewKeywordTokenizer(Reader input) {
		super(input);
	}

	@Override
	public Token next(Token t) throws IOException {
		Token result = super.next(t);
		if (result != null) {
			result.setEndOffset(result.termLength());
		}
		return result;
	}
}
"
1,"workspace.copy causes 2 nodes in the same workspace to have the same version historyworkspace.copy creates a copy of a versionable node with a new uuid which share the same version. ""In a given workspace, there is at most one versionable node per version history"" (4.11 spec)"
0,"[PATCH] remove code stutterMethod calls getQName for no reason


public String getName() throws RepositoryException {
        checkStatus();
        Name qName = getQName();
        return session.getNameResolver().getJCRName(getQName());
    }

patch fixes it."
0,"Enforce TokenStream impl / Analyzer finalness by an assertionAs noted in LUCENE-1753 and other issues, TokenStream and Analyzers are based on the decorator pattern. At least all TokenStream and Analyzer implementations in Lucene and Solr should be final.

The attached patch adds an assertion to the ctors of both classes that does the corresponding checks:
- Analyzers must be final or private classes or anonymous inner classes
- TokenStreams must be final or private classes or anonymous inner classes or have a final incrementToken()

I will commit this after robert have fixed solr streams."
0,"Move FuzzyQuery rewrite as separate RewriteMode into MTQ, was: Highlighter fails to highlight FuzzyQueryAs FuzzyQuery does not allow to change the rewrite mode, highlighter fails with UOE in flex since LUCENE-2110, because it changes the rewrite mode to Boolean query. The fix is: Allow MTQ to change rewrite method and make FUZZY_REWRITE public for that.

The rewrite mode will live in MTQ as TOP_TERMS_SCORING_BOOLEAN_REWRITE. Also the code will be refactored to make heavy reuse of term enumeration code and only plug in the PQ for filtering the top terms."
1,"need a test that uses termsenum.seekExact() (which returns true), then calls next()i tried to do some seekExact (where the result must exist) then next()ing in the faceting module,
and it seems like there could be a bug here.

I think we should add a test that mixes seekExact/seekCeil/next like this, to ensure that
if seekExact returns true, that the enum is properly positioned."
0,"changes-to-html: better handling of bulleted lists in CHANGES.txt- bulleted lists
- should be rendered
- as such
- in output HTML"
0,"Add JMX support to register a JCR RMI Server into Jboss I added two classes and one descriptor file to the jcr-rmi project. These files provide support to make the generated jar deployable into a Jboss server. 

 The deployment descriptor contains two parameters, the address of the local repository instance, and the target address where the rmi server should be registered. 

e.g.

<server>
 <mbean code=""org.apache.jackrabbit.rmi.server.jmx.JCRServer""
     name=""Jackrabbit.services:RMIServer = JCR RMI Server"">
    <attribute name=""Local"">java:jcr/local</attribute>
    <attribute name=""Target"">jnp://localhost:1099/jcrServer</attribute>	
<depends>jboss.jca:service=ManagedConnectionFactory,name=jcr/local</depends>					
  </mbean>
</server>	

this configuration registers an RMI server at /jcrServer that wraps the local repository at java:jcr/local.

br,
Edgar"
1,"Authentication fails with proxied SSL ConnectionsWhen connecting through a proxy, using SSL and authentication HttpClient winds 
up sending a GET request to the proxy after the initial auth required response, 
the proxy then obviously responds with a not implemented response since it 
can't handle a GET request to an SSL URL.  In essence the following is 
happening:

1. HttpClient sends Connect response.
2. Proxy responds 200 Connect OK
3. HttpClient uses SSL connection to send the request to the web server.
4. Web server responds with not authorized and closes the connection.
5. HttpClient opens a new connection to the proxy and issues a GET request for 
the SSL URL.
6. Proxy returns 501 not implemented.

I'll attach a full log to this bug.

This is likely to be hard to fix since the retry is performed in HttpMethodBase 
but the Connect method is executed by HttpClient so a fix for this may be best 
waiting for 2.1.  This looks very similar to HTTPCLIENT-195 except that that bug is 
marked as fixed and this one still doesn't work, this also applies to 
authentication schemes other than NTLM (testing NTLM and basic).

My best evaluation is that the web server returns Connection: close when it 
rejects the authorization attempt and then HttpMethodBase is incapable of 
creating a new SSL connection through the proxy.  The only thing I can think of 
that could be done prior to 2.1 to fix this is to send a Connection: keep-alive 
as well as the Proxy-Connection: Keep-Alive we're already sending with the 
original request."
1,"Removal of a node with shared subnodes failsA simple testcase:

Set up (first transaction):
Node a1 = testRootNode.addNode(""a1"");
Node a2 = testRootNode.addNode(""a2"");
a2.addMixin(""mix:shareable"");
session.save();
// now we have a shareable node N with path a2

Workspace workspace = session.getWorkspace();
String path = a1.getPath() + ""/b1"";
workspace.clone(workspace.getName(), a2.getPath(), path, false);
session.save();
// now we have another shareable node N' in the same shared set as N with path a1/b1

Test(second transaction):
testRootNode.remove(""a1"");
session.save();

At least in a transactional repository the node will not be removed, an error will be thrown instead."
0,Favour QValue.getPath() over getString() where appropriateTo avoid extra conversion round trips QValue.getPath() should be used instead of  QValue.getString() where appropriate.
1,"testIWondiskfull unreferenced files failureNOTE: reproduce with: ant test -Dtestcase=TestIndexWriterOnDiskFull -Dtestmethod=testAddDocumentOnDiskFull -Dtests.seed=aff9b14dd518cfb:4d2f112726e2947f:-2b03094a43a947ee -Dtests.multiplier=3 -Dargs=""-Dfile.encoding=ISO8859-1""

Reproduces some of the time..."
1,"save() might create new transient propertiesIt seems that when a new node is saved through the parent node, new properties might get created, which are not saved. To persist those properties the new node must be saved again.

Example:

(Consider a mixin type ""extVer"" extending the standard type mix:versionable.)

      Node node = parent.addNode(""newNode"", ""nt:base"");
      node.addMixin(""extVer"");
      // ""mix:versionable"" properties do not exist here
      
      // save the new node
      parent.save();

      // now ""mix:versionable"" properties like ""jcr:isCheckedOut""
      // exist in the ""node"" but:
      //    node.getProperty(""jcr:isCheckedOut"").isNew() == true
      // fix:
      node.save();

If the last node.save() opertation would not be done, a RepositoryException would result if a node.checkIn() would be done immediately after parent.save().

This seems counterintuitive and seems like an error. I wonder whether the properties should not be added upon ""node.addMixin"" ? At least ""parent.save()"" should (or might I say must ?) not only add the properties but also save them."
0,"ValueFormat should provide method getJCRStringIn order to retrieve the JCR String representation of a QValue currently the following calls are required:

ValueFormat.getJCRValue(QValue, NamePathResolver, ValueFactory)
Value.getString()

This could be simplified if the ValueFormat would provide

ValueFormat.getJCRString(QValue, NamePathResolver)

"
0,"throw exception for fieldcache on a non-atomic readerIn Lucene 4.0, we go through a lot of effort to prevent slow uses of non-atomic readers:

DirectoryReader/MultiReader etc throw exception if you don't try to access postings or docvalues apis per-segment, etc.

But the biggest trap of all is still too easy to fall into, we don't do the same for FieldCache.

I think we should throw exception, forcing the user to either change their code or use a SlowMultiReaderWrapper.
"
0,ConfigurationException constructors are package privateConfigurationException constructors are package private which prevents reusing them in other packages. eg. when extending the configuration.
0,"Contrib queryparser should not use CharSequence as Map keyToday, contrib query parser uses Map<CharSequence,...> in many different places, which may lead to problems, since CharSequence interface does not enforce the implementation of hashcode and equals methods. Today, it's causing a problem with QueryTreeBuilder.setBuilder(CharSequence,QueryBuilder) method, that does not works as expected."
1,"contrib/benchmark assumes Locale.US for parsing dates in Reuters collectionSimpleDateFormat used for parsing dates in Reuters documents is instantiated without specifying a locale. So it is using the default locale. If that happens to be US, it will work. But for another locale a parse exception is likely.

Affects both StandardBenchmarker and ReutersDocMaker.

Fix is trivial - specify Locale.US for SimpleDateFormat's constructor.
"
0,"Jackrabbit utilitiesAttached are two utilities for Jackrabbit:

The first one is a DataStore implementation that uses Amazon S3 for storage.
This is fairly straightforward. It is configured by adding a DataStore
section to the repository.xml file, e.g.:
   <DataStore class=""org.jcrutil.S3DataStore"">
       <param name=""awsAccessKey"" value="""" />
       <param name=""awsSecretKey"" value="""" />
       <param name=""bucketName"" value="""" />
       <param name=""minModifiedDate"" value=""0"" />
       <param name=""minRecordLength"" value=""0"" />
   </DataStore>

The second utility is a JCR based Commons VFS filesystem provider. This
allows you to access a JCR repository (nt:file and nt:folder nodes) using
the Commons VFS API. I've also used this with MINA FTP Server and Dctm VFS
(http://dctmvfs.sourceforge.net/) to provide FTP access to a Jackrabbit
repository.
"
0,"Unnecessary parsing of Name valueWhen a Name value is created for a call like Property.getValue() the internal QName if formatted, parsed and formatted again."
0,"cache entry resource management should be extracted from CachingHttpClientAs we have built in support for stream-based management of cached response bodies, the CachingHttpClient class has its fingers in too many pies and is involved in resource management but not storage of the actual HttpCacheEntries.

I have a patch forthcoming. :)
"
0,"cache should invalidate obsoleted entries mentioned in Content-LocationFrom http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.6:

If a cache receives a successful response whose Content-Location field matches that of an existing cache entry for the same Request-URI, whose entity-tag differs from that of the existing entry, and whose Date is more recent than that of the existing entry, the existing entry SHOULD NOT be returned in response to future requests and SHOULD be deleted from the cache.

Current caching module doesn't do this (yet). As this is a recommendation (SHOULD) and not a requirement (MUST) I am marking this as an improvement rather than a bug.
"
1,"Changes from Session.move() to a top-level node aren't seen in a second sessionI'll attach a test case, but basically...

* Create two sessions
* Create a top-level node in the first session and save it.
* Move the top-level node using the first session
* In the second session, try itemExists() for the path of the node. It returns true when it should be false."
0,"Move MemoryJournal from test to mainRunning our tests with the FileJournal implementation on a windows box can be quite slow because of the many FileDescriptor.sync() calls.

I'd like to move the MemoryJournal in jackrabbit-core test to the main sources. That way we can use it in other test setups."
0,"Jackrabbit depends on Oracle driver for BLOB support in Oracle versions previous than 10.2In Oracle versions previous to 10.2, Jackrabbit explicitly uses a class from the Oracle driver to provide BLOB support (see OracleFileSystem.init()). This special handling is no longer necesary for Oracle 10.2+, so we should provide a new implementation. As discussed on the list, we can create a new class for Oracle 10.2+, make it inherit from DbFileSystem, and override the createSchema(), and table space related methods, which are the ones that need special handling. Furthermore, we could refactor the current OracleFileSystem and break it into two clases, one of them to keep the current behavior and a new one to keep the common code (which we could rename to OracleBaseFileSystem or similar, to maintain compatiblity with code that uses OracleFileSystem for versions previous to 10.2). Then we make the Oracle10FileSystem inherit from the latter."
0,"CharFilter - normalize characters before tokenizerThis proposes to import CharFilter that has been introduced in Solr 1.4.

Please see for the details:
- SOLR-822
- http://www.nabble.com/Proposal-for-introducing-CharFilter-to20327007.html"
0,"Pre-analyzed fieldsAdds the possibility to set a TokenStream at Field constrution time, available as tokenStreamValue in addition to stringValue, readerValue and binaryValue.

There might be some problems with mixing stored fields with the same name as a field with tokenStreamValue."
1,"Intermitted failure on DocValues branchI lately ran into two random failures on the CSF branch that seem not to be related to docValues but I can't reproduce them neither on docvalues branch nor on trunk.

{code}
jError Message

IndexFileDeleter doesn't know about file _1e.tvx
Stacktrace

junit.framework.AssertionFailedError: IndexFileDeleter doesn't know about file _1e.tvx
	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:979)
	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:917)
	at org.apache.lucene.index.IndexWriter.filesExist(IndexWriter.java:3633)
	at org.apache.lucene.index.IndexWriter.startCommit(IndexWriter.java:3699)
	at org.apache.lucene.index.IndexWriter.prepareCommit(IndexWriter.java:2407)
	at org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:2478)
	at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2460)
	at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2444)
	at org.apache.lucene.index.TestIndexWriterExceptions.testRandomExceptionsThreads(TestIndexWriterExceptions.java:213)
Standard Output

NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterExceptions -Dtestmethod=testRandomExceptionsThreads -Dtests.seed=-6528669668419768890:4860241142852689334 -Dtests.codec=randomPerField -Dtests.multiplier=3
NOTE: test params are: codec=PreFlex, locale=sv, timezone=Atlantic/South_Georgia
Standard Error

NOTE: all tests run in this JVM:
[TestDemo, TestToken, TestBinaryDocument, TestCodecs, TestDirectoryReader, TestIndexInput, TestIndexWriterExceptions]
{code}

and

{code}

[junit] Testsuite: org.apache.lucene.index.TestIndexReaderReopen
    [junit] Testcase: testThreadSafety(org.apache.lucene.index.TestIndexReaderReopen):	Caused an ERROR
    [junit] MockDirectoryWrapper: cannot close: there are still open files: {_4_3.frq=1, _4_3.pos=1, _4_0.frq=1, _4_0.prx=1, _4.pst=1, _4_3.pyl=1, _4_3.skp=1, _4_0.tis=1, _4_3.doc=1, _4_3.tis=1}
    [junit] java.lang.RuntimeException: MockDirectoryWrapper: cannot close: there are still open files: {_4_3.frq=1, _4_3.pos=1, _4_0.frq=1, _4_0.prx=1, _4.pst=1, _4_3.pyl=1, _4_3.skp=1, _4_0.tis=1, _4_3.doc=1, _4_3.tis=1}
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:387)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen.testThreadSafety(TestIndexReaderReopen.java:859)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:979)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:917)
    [junit] Caused by: java.lang.RuntimeException: unclosed IndexInput
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.openInput(MockDirectoryWrapper.java:342)
    [junit] 	at org.apache.lucene.store.Directory.openInput(Directory.java:122)
    [junit] 	at org.apache.lucene.index.codecs.standard.StandardPostingsReader.<init>(StandardPostingsReader.java:49)
    [junit] 	at org.apache.lucene.index.codecs.standard.StandardCodec.fieldsProducer(StandardCodec.java:87)
    [junit] 	at org.apache.lucene.index.PerFieldCodecWrapper$FieldsReader.<init>(PerFieldCodecWrapper.java:119)
    [junit] 	at org.apache.lucene.index.PerFieldCodecWrapper.fieldsProducer(PerFieldCodecWrapper.java:211)
    [junit] 	at org.apache.lucene.index.SegmentReader$CoreReaders.<init>(SegmentReader.java:137)
    [junit] 	at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:532)
    [junit] 	at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:509)
    [junit] 	at org.apache.lucene.index.DirectoryReader.<init>(DirectoryReader.java:238)
    [junit] 	at org.apache.lucene.index.DirectoryReader.doReopen(DirectoryReader.java:500)
    [junit] 	at org.apache.lucene.index.DirectoryReader.access$000(DirectoryReader.java:48)
    [junit] 	at org.apache.lucene.index.DirectoryReader$2.doBody(DirectoryReader.java:493)
    [junit] 	at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:623)
    [junit] 	at org.apache.lucene.index.DirectoryReader.doReopenNoWriter(DirectoryReader.java:488)
    [junit] 	at org.apache.lucene.index.DirectoryReader.doReopen(DirectoryReader.java:446)
    [junit] 	at org.apache.lucene.index.DirectoryReader.reopen(DirectoryReader.java:406)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen$9.run(TestIndexReaderReopen.java:770)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen$ReaderThread.run(TestIndexReaderReopen.java:897)
    [junit] 
    [junit] 
    [junit] Tests run: 17, Failures: 0, Errors: 1, Time elapsed: 13.766 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexReaderReopen -Dtestmethod=testThreadSafety -Dtests.seed=-5455993123574190959:-1935535300313439968 -Dtests.codec=randomPerField -Dtests.multiplier=3
    [junit] NOTE: test params are: codec=RandomCodecProvider: {field5=MockVariableIntBlock(baseBlockSize=29), id=Standard, fielda=Standard, field4=MockFixedIntBlock(blockSize=924), field3=Standard, field2=SimpleText, id2=Standard, field6=MockSep, field1=Pulsing(freqCutoff=8)}, locale=zh_CN, timezone=Asia/Hovd
{code}

I haven't seen those before - let me know if you have!"
0,"CacheClient Javadoc and Constants usage cleanupCacheClient has some empty public java doc on methods that are not get/set.  These should have some body.  

Also the HeaderConstants Class has some overlap with the existing HTTP class for header values.  These need cleaning up."
1,"Be consistent about negative vInt/vLongToday, write/readVInt ""allows"" a negative int, in that it will encode and decode correctly, just horribly inefficiently (5 bytes).

However, read/writeVLong fails (trips an assert).

I'd prefer that both vInt/vLong trip an assert if you ever try to write a negative number... it's badly trappy today.  But, unfortunately, we sometimes rely on this... had we had this assert in 'since the beginning' we could have avoided that.

So, if we can't add that assert in today, I think we should at least fix readVLong to handle negative longs... but then you quietly spend 9 bytes (even more trappy!)."
0,"Enable MultiTermQuery's constant score mode to also use BooleanQuery under the hoodWhen MultiTermQuery is used (via one of its subclasses, eg
WildcardQuery, PrefixQuery, FuzzyQuery, etc.), you can ask it to use
""constant score mode"", which pre-builds a filter and then wraps that
filter as a ConstantScoreQuery.

If you don't set that, it instead builds a [potentially massive]
BooleanQuery with one SHOULD clause per term.

There are some limitations of this approach:

  * The scores returned by the BooleanQuery are often quite
    meaningless to the app, so, one should be able to use a
    BooleanQuery yet get constant scores back.  (Though I vaguely
    remember at least one example someone raised where the scores were
    useful...).

  * The resulting BooleanQuery can easily have too many clauses,
    throwing an extremely confusing exception to newish users.

  * It'd be better to have the freedom to pick ""build filter up front""
    vs ""build massive BooleanQuery"", when constant scoring is enabled,
    because they have different performance tradeoffs.

  * In constant score mode, an OpenBitSet is always used, yet for
    sparse bit sets this does not give good performance.

I think we could address these issues by giving BooleanQuery a
constant score mode, then empower MultiTermQuery (when in constant
score mode) to pick & choose whether to use BooleanQuery vs up-front
filter, and finally empower MultiTermQuery to pick the best (sparse vs
dense) bit set impl.
"
0,spi2dav: JSR 283 NodeType Management
0,"TCK: SetValueDateTest compares Calendar objectsSetValueDateTest#testDateSession
SetValueDateTest#testDateParent

Tests compare Calendar objects.  Calendar.equals(Object) is a stronger test than JSR-170 specifies for Value.equals(Object), leading to false failures.  For the purpose of these tests, even Value.equals(Object) is too strong an equality test, since some repositories may normalize date/time values across a save/read roundtrip (for example, converting ""Z"" to ""+00:00"", or adding/removing trailing zeros in fractional seconds).

Proposal: compare the getTimeInMillis() values.

--- SetValueDateTest.java       (revision 422074)
+++ SetValueDateTest.java       (working copy)
@@ -79,7 +80,8 @@
     public void testDateSession() throws RepositoryException {
         property1.setValue(value);
         superuser.save();
-        assertEquals(""Date node property not saved"", value.getDate(), property1.getValue().getDate());
+        assertEquals(""Date node property not saved"",
+          value.getDate().getTimeInMillis(), property1.getDate().getTimeInMillis());
     }
  
     /**
@@ -89,7 +91,8 @@
     public void testDateParent() throws RepositoryException {
         property1.setValue(value.getDate());
         node.save();
-        assertEquals(""Date node property not saved"", value.getDate(), property1.getValue().getDate());
+        assertEquals(""Date node property not saved"",
+          value.getDate().getTimeInMillis(), property1.getDate().getTimeInMillis());
     }
"
0,"Lucene needs to ship the JUnit jar for testingIn order for Hudson builds, etc. to work properly, Lucene needs to ship the JUnit jar and have it made available in the testing classpath.  Our system reqs say 3.8.1, but I have 3.8.2 laying around, so I will update the system requirements, too."
1,"smartcn analyzer throw NullPointer exception when the length of analysed text over 32767That's all because of org.apache.lucene.analysis.cn.smart.hhmm.SegGraph's makeIndex() method:
  public List<SegToken> makeIndex() {
    List<SegToken> result = new ArrayList<SegToken>();
    int s = -1, count = 0, size = tokenListTable.size();
    List<SegToken> tokenList;
    short index = 0;
    while (count < size) {
      if (isStartExist(s)) {
        tokenList = tokenListTable.get(s);
        for (SegToken st : tokenList) {
          st.index = index;
          result.add(st);
          index++;
        }
        count++;
      }
      s++;
    }
    return result;
  }

here 'short index = 0;' should be 'int index = 0;'. And that's reported here http://code.google.com/p/imdict-chinese-analyzer/issues/detail?id=2 and http://code.google.com/p/imdict-chinese-analyzer/issues/detail?id=11, the author XiaoPingGao have already fixed this bug:http://code.google.com/p/imdict-chinese-analyzer/source/browse/trunk/src/org/apache/lucene/analysis/cn/smart/hhmm/SegGraph.java"
0,Move hasVectors() & hasProx() responsibility out of SegmentInfo to FieldInfos Spin-off from LUCENE-2881 which had this change already but due to some random failures related to this change I remove this part of the patch to make it more isolated and easier to test. 
0,"Improve BenchmarkBenchmark can be improved by incorporating recent suggestions posted
on java-dev. M. McCandless' Python scripts that execute multiple
rounds of tests can either be incorporated into the codebase or
converted to Java."
0,"add @experimental javadocs tagThere are a lot of things marked experimental, api subject to change, etc. in lucene.

this patch simply adds a @experimental tag to common-build.xml so that we can use it, for more consistency.
"
0,"Remove query handler idleTimeThe changes included in JCR-415 revealed a synchronization issue with the query handler idle timer task.

See thread on dev-list: http://thread.gmane.org/gmane.comp.apache.jackrabbit.devel/10199

We could either fix the synchronization issue in the SearchManager class or remove the functionality all together.

Because the repository also supports a idle time parameter for the whole workspace (maxIdleTime in Workspaces element) the query handler idle time should be removed."
1,"Chunked Stream Encoding Problems Fails to throw ExceptionsUsing the HttpClient 2.0.1 with Sun's JDK 1.4.1_01 and connecting to a site 
that appareantly has problems generating proper chunked output causes the http 
client to catch and log an exception then return null data. Ideally the http 
client should throw the IOException to the calling class so that it can be 
handled by the programmer. It's not a problem that an exception is being 
generated it is a bug that the exception is being trapped in the somewhere in 
the httpclient code.

2004-08-27 21:19:01,013 main HttpMethodBase [ERROR]: I/O failure reading 
response body
java.io.IOException: chunked stream ended unexpectedly
        at 
org.apache.commons.httpclient.ChunkedInputStream.getChunkSizeFromInputStream
(ChunkedInputStream.java:234)
        at org.apache.commons.httpclient.ChunkedInputStream.nextChunk
(ChunkedInputStream.java:205)
        at org.apache.commons.httpclient.ChunkedInputStream.read
(ChunkedInputStream.java:160)
        at java.io.FilterInputStream.read(FilterInputStream.java:111)
        at org.apache.commons.httpclient.AutoCloseInputStream.read
(AutoCloseInputStream.java:110)
        at java.io.FilterInputStream.read(FilterInputStream.java:90)
        at org.apache.commons.httpclient.AutoCloseInputStream.read
(AutoCloseInputStream.java:129)
        at org.apache.commons.httpclient.HttpMethodBase.getResponseBody
(HttpMethodBase.java:685)
        at com.algorim.ei.cets.EmailPreProcessor.processMessage
(EmailPreProcessor.java:565)
        at com.algorim.ei.cets.EmailUpdate.run(EmailUpdate.java:332)
        at com.algorim.ei.cets.EmailUpdate.main(EmailUpdate.java:89)

Request and response that are causing the error:
GET /aeq.aspx?k=32226&k=sb1313@xcorp5.com HTTP/1.1
User-Agent: Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.0; .NET CLR 
1.1.4322)
Host: 38.117.227.56

HTTP/1.1 200 OK
Date: Fri, 27 Aug 2004 20:55:27 GMT
Server: Microsoft-IIS/6.0
X-Powered-By: ASP.NET
X-AspNet-Version: 1.1.4322
Transfer-Encoding: chunked
Cache-Control: private
Content-Type: text/html; charset=utf-8

179
<html><head><META HTTP-EQUIV=Refresh CONTENT=""1; 
URL=http://www.datingresults.com/default.asp?
p=7090&PRM=38664""</head><body><script>win2=win
dow.open('http://m.qmct.com/images/d.html?
a=1', 'newwin','toolbar=0,width=730,height=500');if (win2 != null) win2.blur
();window.focus();wind
ow.location = 'http://www.datingresults.com/default.asp?
p=7090&PRM=38664';</script></body></html>"
1,"IndexWriter can flush too early when flushing by RAM usageThere is a silly bug in how DocumentsWriter tracks its RAM usage:
whenever term vectors are enabled, it incorrectly counts the space
used by term vectors towards flushing, when in fact this space is
recycled per document.

This is not a functionality bug.  All it causes is flushes to happen
too frequently, and, IndexWriter will use less RAM than you asked it
to.  To work around it you can simply give it a bigger RAM buffer.

I will commit a fix shortly."
1,DWPT doesn't see changes to DW#infoStreamDW does not push infostream changes to DWPT since DWPT#infoStream is final and initialized on DWPTPool initialization (at least for initial DWPT) we should push changes to infostream to DWPT too
0,JSR 283: Introduce Event.getDate()JSR 283 adds a method to an Event that returns the date when the change happened that caused the event.
0,"Change BooleanFilter to have only a single clauses ArrayList (so toString() works fine, clauses() method could be added) so it behaves more lik BooleanQueryThis is unrelated to the other BF changes, but should be done"
0,"remove MoreLikeThis's default analyzerMoreLikeThis has the following:

{code}
public static final Analyzer DEFAULT_ANALYZER = new StandardAnalyzer(Version.LUCENE_CURRENT);
{code}"
0,"SQL2 parser: Support CASTSome CAST(...) data conversions are not yet implemented, for example String to Decimal."
1,"NPE doing local sensitive sorting when sort field is missingIf you do a local sensitive sort against a field that is missing from some documents in the index an NPE will get thrown.

Attached is a patch which resolved the issue and updates the sort test case to give coverage to this issue."
0,"WebApp: Ease first access for new users looking for a WebDAV serversuggestion posted by mike oliver in the user list:

> I know that JackRabbit isn't the same as Jakarta Slide and not expecting it to be, but one thing we did right on 
> that project was create a runnable war file that doesn't require any learning curve to get started.  Install the war file, 
> create the network place and login as the root:root user and start creating content folders and documents. 
> If JackRabbit did that, then I think more people would try it and use it and then spend the time to learn how to make 
> it all it can be."
0,"Add private ctors to static utility classesDuring development in 3.x and trunk we added some new classes like IOUtils and CodecUtils that are only providing static methods, but have no ctor at all. This adds the default empty public ctor, which is wrong, the classes should never be instantiated.

We should add private dummy ctors to prevent creating instances."
1,DWFlushControl does not take active DWPT out of the loop on fullFlushWe have seen several OOM on TestNRTThreads and all of them are caused by DWFlushControl missing DWPT that are set as flushPending but can't full due to a full flush going on. Yet that means that those DWPT are filling up in the background while they should actually be checked out and blocked until the full flush finishes. Even further we currently stall on the maxNumThreadStates while we should stall on the num of active thread states. I will attach a patch tomorrow.
0,"Port to Generics - test cases in contrib LUCENE-1257 in Lucene 3.0 addressed porting to generics across public api-s . LUCENE-2065 addressed across src/test . 

This would be a placeholder JIRA for any remaining pending generic conversions across the code base. 

Please keep it open after commiting and we can close it when we are near a 3.1 release , so that this could be a placeholder ticket. 

"
0,"Deprecate and replace SimpleHttpConnection with the SimpleHttpServer based testing frameworkThanks to Christian Kohlschuetter and Odi we now have a very flexible testing
framework, which enables us to emulate pretty much all the aspects of a HTTP
server functionality including non-compliant behavior and various vendor
specific implementation quirks. 

Many, many thanks go to Christian Kohlschuetter for having contributed the
original code. 

I propose SimpleHttpConnection be deprecated and eventually be phased out. I
took the first steps toward this goal by migrating Basic authentication test
cases. I urge all committers and contributors to use SimpleHttpServer for all
the new cases from now on. Ideally in the future we should even be able to get
rid of Tomcat as a dependency for testing.

I also took liberty of tweaking the SimpleHttpServer API a little. I factored
SimpleRequest and SimpleResponse classes out and provided a new interface called
HttpService, which can be used instead of HttpRequestHandler to implement test
cases in a way very similar to writing servlets. 

I'll commit the patch shortly as it does not really touch any _productive_ code. 

Oleg"
0,"CookieIdentityComparator and CookiePathComparator could/should implement SerializableCookieIdentityComparator and CookiePathComparator could/should implement Serializable

As Findbugs suggests:

""Comparator doesn't implement Serializable

This class implements the Comparator interface. You should consider whether or not it should also implement the Serializable interface. If a comparator is used to construct an ordered collection such as a TreeMap, then the TreeMap will be serializable only if the comparator is also serializable. As most comparators have little or no state, making them serializable is generally easy and good defensive programming. ""

Neither class has any state, so implementing Serializable would be trivial.

"
0,"Poor performance race condition in FieldCacheImplA race condition exists in FieldCacheImpl that causes a significant performance degradation if multiple threads concurrently request a value that is not yet cached. The degradation is particularly noticable in large indexes and when there a many concurent requests for the cached value.

For the full discussion see the mailing list thread 'Poor performance ""race condition"" in FieldSortedHitQueue' (http://www.gossamer-threads.com/lists/lucene/java-user/38717)."
0,Precedence query parser using the contrib/queryparser frameworkExtend the current StandardQueryParser on contrib so it supports boolean precedence
1,"relative URIs with internal double-slashes ('//') misparsedURI.parseUriReference()'s heuristic for interpreting URI parts is thrown off by relative URIs which include an internal '//'. As a result, portions of the supplied relative URI (path) can be lost. 

For example:

URI rel = new URI(""foo//bar//baz"");
rel.toString();
(java.lang.String) //bar//baz

The culprit seems to be line 1961 of URI improperly concluding that two slashes later than the beginning of 'tmp' are still indicative the URI is a 'net_path'. 

A possible quick fix might be to add a '!isStartedFromPath &&' to the beginning of the line 1961 test, making the line:

            if (!isStartedFromPath && at + 2 < length && tmp.charAt(at + 1) == '/') {

... and thus preventing the misguided authority-parsing from happening when earlier analysis already identified the current string as a strictly path-oriented URI.

(It also appears the setting of the is_net_path boolean at the end of this if's block may be wrong; this code is run for hier_path URIs that are not net_paths in the 2396 syntax. For example:

URI uri = new URI(""http://www.example.com/some/page"");
uri.isNetPath();
 (boolean) true 

)"
0,"openReaderPassed not populated in CheckIndex.Status.SegmentInfoStatusWhen using CheckIndex programatically, the openReaderPassed flag on the SegmentInfoStatus is never populated (so it always comes back false)

looking at the code, its clear that openReaderPassed is defined, but never used

furthermore, it appears that not all information that is propagated to the ""InfoStream"" is available via SegmentIinfoStatus

All of the following information should be able to be gather from public properties on the SegmentInfoStatus:
test: open reader.........OK
test: fields, norms.......OK [2 fields]
test: terms, freq, prox...OK [101 terms; 133 terms/docs pairs; 133 tokens]
test: stored fields.......OK [100 total field count; avg 1 fields per doc]
test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
"
0,Add timing information to event deliveryThere should be debug messages that contain information on how long event listeners spend iterating over the delivered events.
0,java.lang.Iterable support for RangeIteratorsMake javax.jcr.RangeIterator extend java.lang.Iterable in order to enable foreach loops on implementations of RangeIterator.
1,"import of multivalue properties with single value results in incorrect property creationWhen importing a file exported with system view, a value of a multivalued property is stored as a singlevalue property. The bug seems to be that for some reason, even if PropDef.isMultiple() is true for a given property, no ValueFormatException is thrown when setting the property as single value.

Workaround:

It works if I change PropInfo.apply() line 136 to 

if (va.length == 1 && !def.isMultiple()) {
...

"
1,"XML import always throws ItemExistsException when trying to overwrite existing nodesAccording to the JCR-API, it should be possible to govern the import of XML serialized referenceable nodes in case of UUID collision. Unfortunately, the UUID conflict is handled too late during import, an ItemExistsException is always thrown beforehand due to not allowed same-name-siblings.

Simply try to import a previously exported referenceable node twice, providing either

- ImportUUIDBehavior.IMPORT_UUID_COLLISION_REMOVE_EXISTING or
- ImportUUIDBehavior.IMPORT_UUID_COLLISION_REPLACE_EXISTING.

This will fail and result in an ItemExistsException."
0,"encoding of GermanAnalyzer.java and GermanStemmer.java isn't utf-8For PyLucene, the gcj/swig - based python integration of java lucene, it would
be good if java source files didn't use encodings other than utf-8.
On Windows - and systems without iconv support in general - compiling code  
with gcj where the java source text is in another encoding than utf-8 is    
difficult if not impossible.

To change the encoding on these files:

 iconv -f iso-8859-1 -t utf-8 GermanAnalyzer.java > GermanAnalyzer.java.utf-8
 iconv -f iso-8859-1 -t utf-8 GermanStemmer.java > GermanStemmer.java.utf-8"
0,"crank up faceting module testsThe faceting module has a large set of good tests.

lets switch them over to use all of our test infra (randomindexwriter, random iwconfig, mockanalyzer, newDirectory, ...)
I don't want to address multipliers and atLeast() etc on this issue, I think we should follow up with that on a separate issue, that also looks at speed and making sure the nightly build is exhaustive.

for now, lets just get the coverage in, it will be good to do before any refactoring.
"
1,"CJKTokenizer generates tokens with incorrect offsetsIf I index a Japanese *multi-valued* document with CJKTokenizer and highlight a term with FastVectorHighlighter, the output snippets have incorrect highlighted string. I'll attach a program that reproduces the problem soon."
0,"improve automaton performance by running on byte[]Currently, when enumerating terms, automaton must convert entire terms from flex's native utf-8 byte[] to char[] first, then step each char thru the state machine.

we can make this more efficient, by allowing the state machine to run on byte[], so it can return true/false faster."
0,"Patch to JCR-RMI contribution adding Version/VersionHistory supportHi Jukka,

You contributed the famous RMI extension to Jackrabbit. Many thanks. On my way to implement an Eclipse plugin to access repositories this provides great help. Unfortunately your contribution does not include support for versioning yet.

I took the freedom to add this missing piece and provide it to you to add it to your contribution. Thanks."
0,"Restructure codec hierarchySpinoff of LUCENE-2621. (Hoping we can do some of the renaming etc here in a rote way to make progress).

Currently Codec.java only represents a portion of the index, but there are other parts of the index 
(stored fields, term vectors, fieldinfos, ...) that we want under codec control. There is also some 
inconsistency about what a Codec is currently, for example Memory and Pulsing are really just 
PostingsFormats, you might just apply them to a specific field. On the other hand, PreFlex actually
is a Codec: it represents the Lucene 3.x index format (just not all parts yet). I imagine we would
like SimpleText to be the same way.

So, I propose restructuring the classes so that we have something like:
* CodecProvider <-- dead, replaced by java ServiceProvider mechanism. All indexes are 'readable' if codecs are in classpath.
* Codec <-- represents the index format (PostingsFormat + FieldsFormat + ...)
* PostingsFormat: this is what Codec controls today, and Codec will return one of these for a field.
* FieldsFormat: Stored Fields + Term Vectors + FieldInfos?

I think for PreFlex, it doesnt make sense to expose its PostingsFormat as a 'public' class, because preflex
can never be per-field so there is no use in allowing you to configure PreFlex for a specific field.
Similarly, I think in the future we should do the same thing for SimpleText. Nobody needs SimpleText for production, it should
just be a Codec where we try to make as much of the index as plain text and simple as possible for debugging/learning/etc.
So we don't need to expose its PostingsFormat. On the other hand, I don't think we need Pulsing or Memory codecs,
because its pretty silly to make your entire index use one of their PostingsFormats. To parallel with analysis:
PostingsFormat is like Tokenizer and Codec is like Analyzer, and we don't need Analyzers to ""show off"" every Tokenizer.

we can also move the baked in PerFieldCodecWrapper out (it would basically be PerFieldPostingsFormat). Privately it would
write the ids to the file like it does today. in the future, all 3.x hairy backwards code would move to PreflexCodec. 
SimpleTextCodec would get a plain text fieldinfos impl, etc."
0,Convert Batch implementation in spi-rmi from remote object into a local oneThe current implementation of the Batch interface in spi-rmi is very simple and just uses remotes to the server side batch. This should be changed to a local object on the client and only transmit the changes in a single call to the server on save.
1,"StandardCodec sometimes supplies skip pointers past EOFPretty sure this is 4.0-only:
I added an assertion, the test to reproduce is:

ant test-core -Dtestcase=TestPayloadNearQuery -Dtestmethod=testMinFunction -Dtests.seed=4841190615781133892:3888521539169738727 -Dtests.multiplier=3

{noformat}
    [junit] Testcase: testMinFunction(org.apache.lucene.search.payloads.TestPayloadNearQuery):  FAILED
    [junit] invalid skip pointer: 404, length=337
    [junit] junit.framework.AssertionFailedError: invalid skip pointer: 404, length=337
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1127)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1059)
    [junit]     at org.apache.lucene.index.codecs.MultiLevelSkipListReader.init(MultiLevelSkipListReader.java:176)
    [junit]     at org.apache.lucene.index.codecs.standard.DefaultSkipListReader.init(DefaultSkipListReader.java:50)
    [junit]     at org.apache.lucene.index.codecs.standard.StandardPostingsReader$SegmentDocsAndPositionsAndPayloadsEnum.advance(StandardPostingsReader.java:742)
    [junit]     at org.apache.lucene.search.spans.TermSpans.skipTo(TermSpans.java:72)
{noformat}"
1,"HttpConnectionParams.setConnectionTimeout(int) has no effect if host unreachableI have just modified MultiThreadedExample.java by adding
httpClient.getHttpConnectionManager().getParams().setConnectionTimeout(5000); in
order to set a connection timeout on the client side. Then I have added a LAN
url to urisToGet array. The ip of this url (""http://192.168.254.1/"") is not
assigned to any computer.

After running the client, I get the expected message ( error:
org.apache.commons.httpclient.ConnectTimeoutException: The host did not accept
the connection within timeout of 5000 ms) but only after 20 seconds.

I use java version ""1.5.0_04"". This is not a JVM bug since normal connection
procedure times out after 5 seconds as expected:
        SocketAddress addr = new InetSocketAddress(""192.168.254.1"", 80);
        try {
            
            SocketChannel channel = SocketChannel.open();
            channel.socket().connect(addr, 5000);            
            System.out.println(""connected"");
            
        } catch (Exception e) {
            e.printStackTrace();
        }"
0,"Correct copy-paste victim CommentCorrect the doc-comment of FieldsProducer (being a copy-paste victim of FieldsConsumer).
""consumes"" replaced with ""produces"".

One word change to avoid confusion: safe to commit.
"
1,"QueryParser.getFieldQuery(String,String) doesn't set default slop on MultiPhraseQuerythere seems to have been an oversight in calling mph.setSlop(phraseSlop) in QueryParser.getFieldQuery(String,String).  The result being that in some cases, the ""default slop"" value doesnt' get set right (sometimes, ... see below).

when i tried amending TestMultiAnalyzer to demonstrate the problem, I discovered that the grammer aparently always calls getFieldQuery(String,String,int) -- even if no ""~slop"" was specified in the text being parsed, in which case it passes the default as if it were specified.
(just to clarify: i haven't comfirmed this from a detailed reading of the grammer/code, it's just what i've deduced based on observation of the test)

The problem isn't entirely obvious unless you have a subclasses of QueryParser and try to call getFieldQuery(String,String) directly.   

In my case, I had overridden getFieldQuery(String,String) to call super.getFieldQuery(String,String) and wrap the result in a DisjunctionMaxQuery ... I don't care about supporting the ~slop syntax, but i do care about the default slop and i wasn't getting lucky the way QueryParser does, because getFieldQuery(String,String,int) wasn't getting back something it could call setSlop() with the (default) value it got from the javacc generated code.

My description may not make much sense, but hopefull the test patch i'm about to attach will.  The fix is also in the patch, and is fairly trivial.

(disclaimer: i don't have javacc installed, so I tested this patch by manually making the change to both QueryParser.java ... it should only be commited by someone with javacc who can regen the java file and confirm that my jj change doesn't have some weird bug in it)



"
1,"Redirection of a POST methodI execute a PostMethod to an URL which redirects me to a HTML page. If I set 
follow redirects to true the HttpClient wants to execute once more a POST. Of 
course a POST is not allowed to HTML pages. I think the HttpClient should 
exectue a GET method instead. That's also what is in the RFC2616:

10.3 Redirection 3xx

   This class of status code indicates that further action needs to be
   taken by the user agent in order to fulfill the request.  The action
   required MAY be carried out by the user agent without interaction
   with the user if and only if the method used in the second request is
   GET or HEAD. A client SHOULD detect infinite redirection loops, since
   such loops generate network traffic for each redirection.

      Note: previous versions of this specification recommended a
      maximum of five redirections. Content developers should be aware
      that there might be clients that implement such a fixed
      limitation."
0,Promote ItemInfo builder classes from GetItemsTest to top level classesorg.apache.jackrabbit.jcr2spi.GetItem test contains builders for ItemInfo and NodeInfo instances. These should be generalized and promoted to spi-commons. 
0,"Enhance SnapshotDeletionPolicy to allow taking multiple snapshotsA spin off from here: http://www.gossamer-threads.com/lists/lucene/java-dev/99161?do=post_view_threaded#99161

I will:
# Replace snapshot() with snapshot(String), so that one can name/identify the snapshot
# Add some supporting methods, like release(String), getSnapshots() etc.
# Some unit tests of course.

This is mostly written already - I want to contribute it. I've also written a PersistentSDP, which persists the snapshots on stable storage (a Lucene index in this case) to support opening an IW with existing snapshots already, so they don't get deleted. If it's interesting, I can contribute it as well.

Porting my patch to the new API. Should post it soon."
0,"Support DateTools in QueryParserThe QueryParser currently uses the deprecated class DateField to create RangeQueries with date values. However, most users probably use DateTools to store date values in their indexes, because this is the recommended way since DateField has been deprecated. In that case RangeQueries with date values produced by the QueryParser won't work with those indexes.

This patch replaces the use of DateField in QueryParser by DateTools. Because DateTools can produce date values with different resolutions, this patch adds the following methods to QueryParser:

  /**
   * Sets the default date resolution used by RangeQueries for fields for which no
   * specific date resolutions has been set. Field specific resolutions can be set
   * with {@link #setDateResolution(String, DateTools.Resolution)}.
   *  
   * @param dateResolution the default date resolution to set
   */
  public void setDateResolution(DateTools.Resolution dateResolution);
  
  /**
   * Sets the date resolution used by RangeQueries for a specific field.
   *  
   * @param field field for which the date resolution is to be set 
   * @param dateResolution date resolution to set
   */
  public void setDateResolution(String fieldName, DateTools.Resolution dateResolution);

(I also added the corresponding getter methods).

Now the user can set a default date resolution used for all fields or, with the second method, field specific date resolutions.
The initial default resolution, which is used if the user does not set a different resolution, is DateTools.Resolution.DAY. 

Please let me know if you think we should use a different resolution as default.

I extended TestQueryParser to test this new feature.

All unit tests pass.
"
1,"literal plus (+) character in path components of HttpURL is not preserved.When a literal plus character is included in the path component of an URL, it is
not encoded, but get decoded during getPath() to a space.

Reproducible with the following:

HttpURL httpURL = new HttpURL(""http://localhost/test+test"");
System.out.println(httpURL.getPath());

Output:
""test test""

The following path fixes the issue (This patch does not appear to break anything
 else):

Patch against SVN Repo:
URL: http://svn.apache.org/repos/asf/jakarta/commons/proper/httpclient/trunk
Repository UUID: 13f79535-47bb-0310-9956-ffa450edef68
Revision: 405803

Index: src/java/org/apache/commons/httpclient/URI.java
===================================================================
--- src/java/org/apache/commons/httpclient/URI.java (revision 405803)
+++ src/java/org/apache/commons/httpclient/URI.java (working copy)
@@ -1552,6 +1552,7 @@
         allowed_abs_path.or(abs_path);
         // allowed_abs_path.set('/');  // aleady included
         allowed_abs_path.andNot(percent);
+        allowed_abs_path.clear('+');
     }


@@ -1563,6 +1564,7 @@
     static {
         allowed_rel_path.or(rel_path);
         allowed_rel_path.clear('%');
+        allowed_rel_path.clear('+');
     }"
0,"version.propertiesIf we're not going to split it, there should be only one version.properties in module-client.
module-httpmime is currently missing a version.properties file.
"
0,"Multiple tests test for locking instead of versioningMultiple tests claim to check whether versioning is supported, but in reality check for locking.  Patch included."
0,"UUID generation: SecureRandom should be used by defaultCurrently, the UUID generation used the regular java.util.Random implementation to generate random UUIDs. The seed value of Random is initialized using System.currentTimeMillis(); for Windows, the resolution is about 15 milliseconds. That means two computer that start creating UUIDs with Jackrabbit within the same 15 millisecond interval will generate the same UUIDs. In a clustered environment the nodes could be started automatically at the same time (for example after a backup).

Also, the Random class uses a 48-bit seed, which is much less than the number of random bits in UUID (122). This is not secure. See also:

http://en.wikipedia.org/wiki/UUID
Random UUID probability of duplicates
""The probability [of duplicates] also depends on the quality of the random number generator. A cryptographically secure pseudorandom number generator must be used to generate the values, otherwise the probability of duplicates may be significantly higher.""

Therefore, I suggest to change VersionFourGenerator to use the SecureRandom implementation in by default."
0,"Cached filter for a single term fieldThese classes implement inexpensive range filtering over a field containing a single term. They do this by building an integer array of term numbers (storing the term->number mapping in a TreeMap) and then implementing a fast integer comparison based DocSetIdIterator.

This code is currently being used to do age range filtering, but could also be used to do other date filtering or in any application where there need to be multiple filters based on the same single term field. I have an untested implementation of single term filtering and have considered but not yet implemented term set filtering (useful for location based searches) as well. 

The code here is fairly rough; it works but lacks javadocs and toString() and hashCode() methods etc. I'm posting it here to discover if there is other interest in this feature; I don't mind fixing it up but would hate to go to the effort if it's not going to make it into Lucene.

"
0,"[PATCH] FSDirectory create() method deletes all fileshi all,

the current implementation of FSDirectory.create(...) silently deletes all files
(even empty directories) within the index directory when setting up a new index
with create option enabled. Lucene doesn't care when deleting files in the index
directory if they  belong to lucene or not. I don't think that this is a real
bug, but it can be a pain if somebody whants to store some private information
in the lucene index directory, e.g some configuration files.

Therefore i implemented a FileFilter which knows about the internal lucene file
extensions, so that all other files would never get touched when creating a new
index. The current patch is an enhancement in FSDirectory only. I don't think
that there is a need to make it available in the Directory class and change all
it's depending classes.

regards
Bernhard"
1,"TestSimpleExplanations failure{noformat}
ant test -Dtestcase=TestSimpleExplanations -Dtestmethod=testDMQ8 -Dtests.seed=-7e984babece66153:3e3298ae627b33a9:3093059db62bcc71
{noformat}

fails w/ this on current trunk... looks like silly floating point precision issue:

{noformat}

    [junit] Testsuite: org.apache.lucene.search.TestSimpleExplanations
    [junit]   1.4508595 = (MATCH) sum of:
    [junit]     1.4508595 = (MATCH) weight(field:yy in 2) [DefaultSimilarity], result of:
    [junit]       1.4508595 = score(doc=2,freq=1.0 = termFreq=1
    [junit] ), product of:
    [junit]         1.287682 = queryWeight, product of:
    [junit]           1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]           1.0 = queryNorm
    [junit]         1.1267219 = fieldWeight in 2, product of:
    [junit]           1.0 = tf(freq=1.0), with freq of:
    [junit]             1.0 = termFreq=1
    [junit]           1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]           0.875 = fieldNorm(doc=2)
    [junit]   145085.95 = (MATCH) weight(field:xx^100000.0 in 2) [DefaultSimilarity], result of:
    [junit]     145085.95 = score(doc=2,freq=1.0 = termFreq=1
    [junit] ), product of:
    [junit]       128768.2 = queryWeight, product of:
    [junit]         100000.0 = boost
    [junit]         1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]         1.0 = queryNorm
    [junit]       1.1267219 = fieldWeight in 2, product of:
    [junit]         1.0 = tf(freq=1.0), with freq of:
    [junit]           1.0 = termFreq=1
    [junit]         1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]         0.875 = fieldNorm(doc=2)
    [junit]  expected:<145086.66> but was:<145086.69>)
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.544 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestSimpleExplanations -Dtestmethod=testDMQ8 -Dtests.seed=144152895b276837:eb7ba4953db943f:33373b79a971db02
    [junit] NOTE: test params are: codec=PreFlex, sim=RandomSimilarityProvider(queryNorm=false,coord=false): {field=DefaultSimilarity, alt=DFR I(ne)LZ(0.3), KEY=IB LL-D2}, locale=en_IN, timezone=Pacific/Samoa
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestSimpleExplanations]
    [junit] NOTE: Linux 2.6.33.6-147.fc13.x86_64 amd64/Sun Microsystems Inc. 1.6.0_21 (64-bit)/cpus=24,threads=1,free=130426744,total=189988864
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testDMQ8(org.apache.lucene.search.TestSimpleExplanations):	FAILED
    [junit] ((field:yy field:w5^100.0) | field:xx^100000.0)~0.5: score(doc=2)=145086.66 != explanationScore=145086.69 Explanation: 145086.69 = (MATCH) max plus 0.5 times others of:
    [junit]   1.4508595 = (MATCH) sum of:
    [junit]     1.4508595 = (MATCH) weight(field:yy in 2) [DefaultSimilarity], result of:
    [junit]       1.4508595 = score(doc=2,freq=1.0 = termFreq=1
    [junit] ), product of:
    [junit]         1.287682 = queryWeight, product of:
    [junit]           1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]           1.0 = queryNorm
    [junit]         1.1267219 = fieldWeight in 2, product of:
    [junit]           1.0 = tf(freq=1.0), with freq of:
    [junit]             1.0 = termFreq=1
    [junit]           1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]           0.875 = fieldNorm(doc=2)
    [junit]   145085.95 = (MATCH) weight(field:xx^100000.0 in 2) [DefaultSimilarity], result of:
    [junit]     145085.95 = score(doc=2,freq=1.0 = termFreq=1
    [junit] ), product of:
    [junit]       128768.2 = queryWeight, product of:
    [junit]         100000.0 = boost
    [junit]         1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]         1.0 = queryNorm
    [junit]       1.1267219 = fieldWeight in 2, product of:
    [junit]         1.0 = tf(freq=1.0), with freq of:
    [junit]           1.0 = termFreq=1
    [junit]         1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]         0.875 = fieldNorm(doc=2)
    [junit]  expected:<145086.66> but was:<145086.69>
    [junit] junit.framework.AssertionFailedError: ((field:yy field:w5^100.0) | field:xx^100000.0)~0.5: score(doc=2)=145086.66 != explanationScore=145086.69 Explanation: 145086.69 = (MATCH) max plus 0.5 times others of:
    [junit]   1.4508595 = (MATCH) sum of:
    [junit]     1.4508595 = (MATCH) weight(field:yy in 2) [DefaultSimilarity], result of:
    [junit]       1.4508595 = score(doc=2,freq=1.0 = termFreq=1
    [junit] ), product of:
    [junit]         1.287682 = queryWeight, product of:
    [junit]           1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]           1.0 = queryNorm
    [junit]         1.1267219 = fieldWeight in 2, product of:
    [junit]           1.0 = tf(freq=1.0), with freq of:
    [junit]             1.0 = termFreq=1
    [junit]           1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]           0.875 = fieldNorm(doc=2)
    [junit]   145085.95 = (MATCH) weight(field:xx^100000.0 in 2) [DefaultSimilarity], result of:
    [junit]     145085.95 = score(doc=2,freq=1.0 = termFreq=1
    [junit] ), product of:
    [junit]       128768.2 = queryWeight, product of:
    [junit]         100000.0 = boost
    [junit]         1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]         1.0 = queryNorm
    [junit]       1.1267219 = fieldWeight in 2, product of:
    [junit]         1.0 = tf(freq=1.0), with freq of:
    [junit]           1.0 = termFreq=1
    [junit]         1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]         0.875 = fieldNorm(doc=2)
    [junit]  expected:<145086.66> but was:<145086.69>
    [junit] 	at org.apache.lucene.search.CheckHits.verifyExplanation(CheckHits.java:324)
    [junit] 	at org.apache.lucene.search.CheckHits$ExplanationAsserter.collect(CheckHits.java:494)
    [junit] 	at org.apache.lucene.search.Scorer.score(Scorer.java:60)
    [junit] 	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:580)
    [junit] 	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:363)
    [junit] 	at org.apache.lucene.search.CheckHits.checkExplanations(CheckHits.java:302)
    [junit] 	at org.apache.lucene.search.QueryUtils.checkExplanations(QueryUtils.java:92)
    [junit] 	at org.apache.lucene.search.QueryUtils.check(QueryUtils.java:126)
    [junit] 	at org.apache.lucene.search.QueryUtils.check(QueryUtils.java:122)
    [junit] 	at org.apache.lucene.search.QueryUtils.check(QueryUtils.java:106)
    [junit] 	at org.apache.lucene.search.CheckHits.checkHitCollector(CheckHits.java:89)
    [junit] 	at org.apache.lucene.search.TestExplanations.qtest(TestExplanations.java:99)
    [junit] 	at org.apache.lucene.search.TestSimpleExplanations.testDMQ8(TestSimpleExplanations.java:224)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$2$1.evaluate(LuceneTestCase.java:611)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:148)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:50)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.search.TestSimpleExplanations FAILED
{noformat}"
1,"XMLTextFilter does not extract text elementsXMLTextFilter only returns the text from attributes, not the content of text elements,"
1,"Jenkins builds hang quite often in TestIndexWriterWithThreads.testCloseWithThreadsLast hung test run: [https://builds.apache.org/job/Lucene-Solr-tests-only-trunk/10638/console]

{noformat}
[junit] ""main"" prio=5 tid=0x0000000801ef3800 nid=0x1965c waiting on condition [0x00007fffffbfd000]
[junit]    java.lang.Thread.State: WAITING (parking)
[junit] 	at sun.misc.Unsafe.park(Native Method)
[junit] 	- parking to wait for  <0x0000000825d853a8> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
[junit] 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
[junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:838)
[junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:871)
[junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1201)
[junit] 	at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)
[junit] 	at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)
[junit] 	at org.apache.lucene.index.DocumentsWriterFlushControl.markForFullFlush(DocumentsWriterFlushControl.java:403)
[junit] 	at org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:557)
[junit] 	- locked <0x0000000825d81998> (a org.apache.lucene.index.DocumentsWriter)
[junit] 	at org.apache.lucene.index.IndexWriter.prepareCommit(IndexWriter.java:2776)
[junit] 	- locked <0x0000000825d7d840> (a java.lang.Object)
[junit] 	at org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:2904)
[junit] 	- locked <0x0000000825d7d830> (a java.lang.Object)
[junit] 	at org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:1156)
[junit] 	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1099)
[junit] 	at org.apache.lucene.index.TestIndexWriterWithThreads.testCloseWithThreads(TestIndexWriterWithThreads.java:200)
[junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
[junit] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[junit] 	at java.lang.reflect.Method.invoke(Method.java:616)
[junit] 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
[junit] 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
[junit] 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
[junit] 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
[junit] 	at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
[junit] 	at org.apache.lucene.util.LuceneTestCase$2$1.evaluate(LuceneTestCase.java:611)
[junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
[junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
[junit] 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
[junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:148)
[junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:50)
[junit] 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
[junit] 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
[junit] 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
[junit] 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
[junit] 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
[junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
[junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
[junit] 	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
[junit] 	at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
[junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
[junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
[junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:743)
{noformat}"
0,"deprecate ChineseAnalyzerThe ChineseAnalyzer, ChineseTokenizer, and ChineseFilter (not the smart one, or CJK) indexes chinese text as individual characters and removes english stopwords, etc.

In my opinion we should simply deprecate all of this in favor of StandardAnalyzer, StandardTokenizer, and StopFilter, which does the same thing."
0,"Stored-only fields automatically enable norms and tf when added to documentDuring updating my internal components to the new TrieAPI, I have seen the following:

I index a lot of numeric fields with trie encoding omitting norms and term frequency. This works great. Luke shows that both is omitted.

As I sometimes also want to have the components of the field stored and want to use the same field name for it. So I add additionally the field again to the document, but stored only (as the Field c'tor using a TokenStream cannot additionally store the field). As it is stored only, I thought, that I can left out explicit setting of omitNorms and omitTermFreqAndPositions. After adding the stored-only-without-omits field, Luke shows all fields with norms enabled. I am not sure, if the norms/tf were really added to the index, but Luke shows a value for the norms and FieldInfo has it enabled.

In my opinion, this is not intuitive, o.a.l.document.Field  should switch both omit* options on when storing fields only (and also disable other indexing-only options). Alternatively the internal FieldInfo.update(boolean isIndexed, boolean storeTermVector, boolean storePositionWithTermVector, boolean storeOffsetWithTermVector, boolean omitNorms, boolean storePayloads, boolean omitTermFreqAndPositions) should only change the omit* and other options, if the isIndexed parameter (not this.isIndexed) is also true, elsewhere leave it as it is.

In principle, when adding a stored-only field, any indexing-specific options should not be changed in FieldInfo. If the field was indexed with norms before, norms should stay enabled (but this would be the default as it is)."
1,"Cluster Node ID should be trimmedIf the cluster node ID is not configured in repository.xml, it is read from the file cluster_node.id instead. In case this file is edited by hand, some editors (e.g. vi) insert a trailing newline character (""\n""). This leads to the cluster node ID to contain a blank character. While I don't expect this to cause any issues, it is inconvenient for debugging and also introduces line-breaks in log files. I suggest to trim the cluster node ID, so only non-blank characters are used."
0,"Searchability settings in PropertyDefinitionRelated to JCR-1591, the new JCR 2.0 property definitions contain settings for searchability of properties.

I'm not sure how deeply we want to implement these settings (perhaps we should just hard-code the values), but in any case the relevant methods need to be implemented."
0,"Move text extraction into a background threadEven though text extraction is not done right on save() most of the extraction work is later done by a client thread. There is a mechanism in place that commits the deferred work in a background thread. But the background thread is only triggered by a timer and does not constantly write back pending index changes. For regular index changes this is done on purpose and should not be changed. However text extraction work should be moved completely into a background thread because it often takes a fair amount of time to index a large document.

Outline of a possible solution:
- all text filtering is tasks are put into a work queue
- the work queue is processed by a background thread
- basic indexing of nt:resource without text filtering takes place
- the background thread updates the index when text filtering completed for a nt:resource

There should be a configuration parameter that allows to execute text filtering without the background thread. This way it is possible to get the existing behaviour of Jackrabbit: the fulltext index is always up-to-date and can be used.
With the background process this is no longer the case."
1,"JCA will not compile with J2EE1.3 classesIn JCAManagedConnectionFactory, the constructor invoked to throw ResourceException does not exist under J2EE1.3 / JCA1.1 classes.
 throw new ResourceException(e)  -  line 136 and line 277.

Instead the code needs to do something like:

            ResourceException exception = new ResourceException(""Failed to create session"");
            exception.setLinkedException(e);
            throw exception;

This will allow it to compile/run under J2EE1.3
"
0,Add DataInput/DataOutput subclasses that delegate to an InputStream/OutputStream.Such classes would be handy for FST serialization/deserialization.
0,"IndexOutput.writeString() should write length in bytesWe should change the format of strings written to indexes so that the length of the string is in bytes, not Java characters.  This issue has been discussed at:

http://www.mail-archive.com/java-dev@lucene.apache.org/msg01970.html

We must increment the file format number to indicate this change.  At least the format number in the segments file should change.

I'm targetting this for 2.1, i.e., we shouldn't commit it to trunk until after 2.0 is released, to minimize incompatible changes between 1.9 and 2.0 (other than removal of deprecated features)."
0,"Coarser granularity of node type unregistration notificationsWhen unregistering multiple node types at a time, the internal notification methods are called separately for each type. This causes some problems as the first notifications triggers the regeneration of the full virtual node type tree, and later notification calls will fail (logging an error) in VirtualNodeTypeStateManager because the removed type is no longer there. A better approach would be to send the names of all the unregistered node types as a collection."
1,"session.move() throws ItemExistsException despite same name siblingscode to reproduce:

            Session session = r.login(new SimpleCredentials(""johndoe"", """".toCharArray()), wspName);
            Node root = session.getRootNode();

            // setup test case
            if (!root.hasNode(""foo"")) {
                root.addNode(""foo"");
                root.save();
            }
            if (!root.hasNode(""bar"")) {
                root.addNode(""bar"");
                root.save();
            }

            session.move(""/foo"", ""/bar"");    // ==> ItemExistsException
"
0,Use type StaticOperand for fullTextSearchExpressionSee: https://jsr-283.dev.java.net/issues/show_bug.cgi?id=691
0,"Simplified Repository URI format for JNDI lookupsThe JndiRepositoryFactory class (together with JcrUtils) currently supports the following repository URI formats:

    JcrUtils.getRepository(""jndi:name-of-repository"");
    JcrUtils.getRepository(""jndi://ignored?org.apache.jackrabbit.repository.jndi.name=name-of-repository&other-parameters"");

The first uri formats allows no extra JNDI environment settings to be passed in, and the second one is pretty verbose and simply ignores the authority and path parts of the URI.

I'd like to add support for the following simplified format that makes it easy to provide the repository name along with the initial context factory from which the name is to be looked up:

    JcrUtils.getRepository(""jndi://initial-context-factory/name-of-repository"");

Extra JNDI environment settings could still be included as additional query parameters. Backwards compatibility with the previous formats would be guaranteed based on the presence or absence of the org.apache.jackrabbit.repository.jndi.name parameter in hierarchical URIs."
1,"NullPointerException when accessing the about.jsp page because of missing /META-INF/NOTICE.TXTAccessing http://localhost:8080/about.jsp produces:

java.lang.NullPointerException
	at org.apache.jsp.about_jsp.output(org.apache.jsp.about_jsp:39)
	at org.apache.jsp.about_jsp._jspService(org.apache.jsp.about_jsp:103)
	at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:109)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:389)
	at org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:486)
	at org.apache.jasper.servlet.JspServlet.service(JspServlet.java:380)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:502)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:363)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:417)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.handler.RequestLogHandler.handle(RequestLogHandler.java:49)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:324)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:534)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:864)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:533)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:207)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:403)
	at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:522)

This is because the jar misses the following file:
/META-INF/NOTICE.TXT
but there is /NOTICE.TXT and /META-INF/NOTICE

This problem is not reproducible with jackrabbit-webapp-2.0-beta1.war.
"
0,"Enable setting hits queue size in Search*Task in contrib/benchmarkIn testing for LUCENE-1483, I'd like to try different collector queue
sizes during benchmarking.  But currently contrib/benchmark uses
deprecated Hits with hardwired ""top 100"" queue size.  I'll switch it to
the TopDocs APIs."
0,"LocalNamespaceMappings does not make use of NameCache in NamespaceRegistryImplThis basically means that the NameCache in NamespaceRegistryImpl is never used.

The LocalNamespaceMappings should also implement NameCache and forward calls to the NamespaceRegistryImpl for names
with namespace URIs that are not locally remapped. See proposed patch."
1,"DocumentsWriter blocks flushes when applyDeletes takes forever - memory not releasedIn DocumentsWriter we have a safety check that applies all deletes if the deletes consume too much RAM to prevent too-frequent flushing of a long tail of tiny segments. If we enter applyAllDeletes we essentially lock on IW -> BufferedDeletes which is fine since this usually doesn't take long and doesn't keep DWPTs from indexing. Yet, if that takes long and at the same time a semgent is flushed and subsequently published to the IW we take the lock on the ticket queue and the IW. Now this prevents all other threads to append to the ticketQueue which is done BEFORE we actually flush the segment concurrently and free up the RAM.

Essentially its ok to block on the IW lock but we should not keep concurrent flushed from execution just because we apply deletes. The threads will block once they try to execute maybeMerge after the segment is flushed so we don't pile up subsequent memory but we should actually allow the DWPT to be flushed since we actually try to get rid of memory.

I ran into this by accident due to a coding bug using delete queries instead of terms for each document. This thread dump show the problem:

{noformat}
""Application Worker Thread"" prio=10 tid=0x00007fdda0238000 nid=0x3256 waiting for
monitor entry [0x00007fddad3c2000]
  java.lang.Thread.State: BLOCKED (on object monitor)
       at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:424)
       - waiting to lock <0x00007fddb74ff990> (a
org.apache.lucene.index.DocumentsWriter$TicketQueue)
       at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:320)
       at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:393)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1484)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)

""Application Worker Thread"" prio=10 tid=0x00007fdda0236000 nid=0x3255 waiting for
monitor entry [0x00007fddad4c3000]
  java.lang.Thread.State: BLOCKED (on object monitor)
       at org.apache.lucene.index.IndexWriter.updatePendingMerges(IndexWriter.java:1854)
       - waiting to lock <0x00007fddb74fe350> (a
org.apache.solr.update.SolrIndexWriter)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1848)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1843)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1493)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)

""Application Worker Thread"" prio=10 tid=0x00007fdda0234000 nid=0x3254 waiting for
monitor entry [0x00007fddad5c4000]
  java.lang.Thread.State: BLOCKED (on object monitor)
       at org.apache.lucene.index.IndexWriter.updatePendingMerges(IndexWriter.java:1854)
       - waiting to lock <0x00007fddb74fe350> (a
org.apache.solr.update.SolrIndexWriter)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1848)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1843)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1493)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)

""Application Worker Thread"" prio=10 tid=0x00007fdda0232000 nid=0x3253 waiting for
monitor entry [0x00007fddad6c5000]
  java.lang.Thread.State: BLOCKED (on object monitor)
       at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:424)
       - waiting to lock <0x00007fddb74ff990> (a
org.apache.lucene.index.DocumentsWriter$TicketQueue)
       at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:320)
       at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:393)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1484)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)

""Application Worker Thread"" prio=10 tid=0x00007fdda0230800 nid=0x3252 waiting for
monitor entry [0x00007fddad7c6000]
  java.lang.Thread.State: BLOCKED (on object monitor)
       at org.apache.lucene.index.IndexWriter.updatePendingMerges(IndexWriter.java:1854)
       - waiting to lock <0x00007fddb74fe350> (a
org.apache.solr.update.SolrIndexWriter)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1848)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1843)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1493)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)

""Application Worker Thread"" prio=10 tid=0x00007fdda022e800 nid=0x3251 runnable
[0x00007fddad8c6000]
  java.lang.Thread.State: RUNNABLE
       at java.nio.Bits.copyToArray(Bits.java:715)
       at java.nio.DirectByteBuffer.get(DirectByteBuffer.java:233)
       at org.apache.lucene.store.MMapDirectory$MMapIndexInput.readBytes(MMapDirectory.java:319)
       at org.apache.lucene.index.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum$Frame.loadBlock(BlockTreeTermsReader.java:2283)
       at org.apache.lucene.index.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum.seekExact(BlockTreeTermsReader.java:1600)
       at org.apache.lucene.util.TermContext.build(TermContext.java:97)
       at org.apache.lucene.search.TermQuery.createWeight(TermQuery.java:180)
       at org.apache.lucene.search.BooleanQuery$BooleanWeight.<init>(BooleanQuery.java:186)
       at org.apache.lucene.search.BooleanQuery.createWeight(BooleanQuery.java:423)
       at org.apache.lucene.search.IndexSearcher.createNormalizedWeight(IndexSearcher.java:583)
       at org.apache.lucene.search.QueryWrapperFilter.getDocIdSet(QueryWrapperFilter.java:55)
       at org.apache.lucene.index.BufferedDeletesStream.applyQueryDeletes(BufferedDeletesStream.java:431)
       at org.apache.lucene.index.BufferedDeletesStream.applyDeletes(BufferedDeletesStream.java:268)
       - locked <0x00007fddb751e1e8> (a
org.apache.lucene.index.BufferedDeletesStream)
       at org.apache.lucene.index.IndexWriter.applyAllDeletes(IndexWriter.java:2852)
       - locked <0x00007fddb74fe350> (a org.apache.solr.update.SolrIndexWriter)
       at org.apache.lucene.index.DocumentsWriter.applyAllDeletes(DocumentsWriter.java:188)
       at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:470)
       at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:320)
       at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:393)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1484)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)
       

""Application Worker Thread"" prio=10 tid=0x00007fdda022d800 nid=0x3250 waiting for
monitor entry [0x00007fddad9c8000]
  java.lang.Thread.State: BLOCKED (on object monitor)
       at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:424)
       - waiting to lock <0x00007fddb74ff990> (a
org.apache.lucene.index.DocumentsWriter$TicketQueue)
       at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:320)
       at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:393)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1484)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)
   
""Application Worker Thread"" prio=10 tid=0x00007fdda022d000 nid=0x324f waiting for
monitor entry [0x00007fddadac9000]
  java.lang.Thread.State: BLOCKED (on object monitor)
       at org.apache.lucene.index.IndexWriter.useCompoundFile(IndexWriter.java:2274)
       - waiting to lock <0x00007fddb74fe350> (a
org.apache.solr.update.SolrIndexWriter)
       at org.apache.lucene.index.IndexWriter.prepareFlushedSegment(IndexWriter.java:2156)
       at org.apache.lucene.index.DocumentsWriter.publishFlushedSegment(DocumentsWriter.java:526)
       at org.apache.lucene.index.DocumentsWriter.finishFlush(DocumentsWriter.java:506)
       at org.apache.lucene.index.DocumentsWriter.applyFlushTickets(DocumentsWriter.java:483)
       - locked <0x00007fddb74ff990> (a
org.apache.lucene.index.DocumentsWriter$TicketQueue)
       at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:449)
       at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:320)
       at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:393)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1484)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)

{noformat}"
1,"Lower-Case Search-Function works with Upper-Case Searchstringif you perform a query like this
testroot/*[jcr:like(fn:lower-case(@prop1), 'FO%')]
you get valid results even though the value in the property has the ""foo"" value
The search works with lower and upper-case search strings."
0,Add args to test-macroAdd passing args to JUnit.  (Like Solr and mainly for debugging).  
1,"TestFSTs.testRandomWords failureWas running some while(1) tests on the docvalues branch (r1103705) and the following test failed:

{code}
    [junit] Testsuite: org.apache.lucene.util.automaton.fst.TestFSTs
    [junit] Testcase: testRandomWords(org.apache.lucene.util.automaton.fst.TestFSTs):	FAILED
    [junit] expected:<771> but was:<TwoLongs:771,771>
    [junit] junit.framework.AssertionFailedError: expected:<771> but was:<TwoLongs:771,771>
    [junit] 	at org.apache.lucene.util.automaton.fst.TestFSTs$FSTTester.verifyUnPruned(TestFSTs.java:540)
    [junit] 	at org.apache.lucene.util.automaton.fst.TestFSTs$FSTTester.doTest(TestFSTs.java:496)
    [junit] 	at org.apache.lucene.util.automaton.fst.TestFSTs$FSTTester.doTest(TestFSTs.java:359)
    [junit] 	at org.apache.lucene.util.automaton.fst.TestFSTs.doTest(TestFSTs.java:319)
    [junit] 	at org.apache.lucene.util.automaton.fst.TestFSTs.testRandomWords(TestFSTs.java:940)
    [junit] 	at org.apache.lucene.util.automaton.fst.TestFSTs.testRandomWords(TestFSTs.java:915)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1282)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1211)
    [junit] 
    [junit] 
    [junit] Tests run: 7, Failures: 1, Errors: 0, Time elapsed: 7.628 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: Ignoring nightly-only test method 'testBigSet'
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestFSTs -Dtestmethod=testRandomWords -Dtests.seed=-269475578956012681:0
    [junit] NOTE: test params are: codec=PreFlex, locale=ar, timezone=America/Blanc-Sablon
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestToken, TestCodecs, TestIndexReaderReopen, TestIndexWriterMerging, TestNoDeletionPolicy, TestParallelReaderEmptyIndex, TestParallelTermEnum, TestPerSegmentDeletes, TestSegmentReader, TestSegmentTermDocs, TestStressAdvance, TestTermVectorsReader, TestSurrogates, TestMultiFieldQueryParser, TestAutomatonQuery, TestBooleanScorer, TestFuzzyQuery, TestMultiTermConstantScore, TestNumericRangeQuery64, TestPositiveScoresOnlyCollector, TestPrefixFilter, TestQueryTermVector, TestScorerPerf, TestSloppyPhraseQuery, TestSpansAdvanced, TestWindowsMMap, TestRamUsageEstimator, TestSmallFloat, TestUnicodeUtil, TestFSTs]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=137329960,total=208207872
    [junit] ------------- ---------------- ---------------
    [junit] TEST org.apache.lucene.util.automaton.fst.TestFSTs FAILED
{code}

I am not able to reproduce"
0,"Initial size of ConcurrentCache depends on number of segments (available processors)This causes a build failure on my machine. Tests run into an OOME because the initial memory footprint of a ConcurrentCache on my machine is 8k. Many of the tests keep references to some kind of repository objects (node, session, x-manager), which means ConcurrentCache instances  cannot be garbage collected immediately after a test run.

I think the overall initial size of the cache should be independent of the number of segments. See proposed patch."
1,"CachingHierarchyManager synchronization problemThe method CachingHierarchyManager.resolveNodePath(..) does not synchronize on the cacheMonitor object. This can result in an endless loop in cache(), in NullPointerException or in other unexpected behavior in CachingHierarchyManager."
0,"Add a ContextAwareAuthScheme that has access to the HttpContext in the authenticate methodThe interface to be added would be:

/**
 * This interface represents an extended  authentication scheme
 * that requires access to {@link HttpContext} in order to
 * generate an authorization string.
 *
 * @since 4.1
 */

public interface ContextAwareAuthScheme extends AuthScheme {

    /**
     * Produces an authorization string for the given set of
     * {@link Credentials}.
     *
     * @param credentials The set of credentials to be used for athentication
     * @param request The request being authenticated
     * @param context HTTP context
     * @throws AuthenticationException if authorization string cannot
     *   be generated due to an authentication failure
     *
     * @return the authorization string
     */
    Header authenticate(
            Credentials credentials,
            HttpRequest request,
            HttpContext context) throws AuthenticationException;

}

Binary compatibility can be maintained by doing an instanceof check at the location where AuthScheme.authenticate() is called at the moment, and calling the context aware version if available.

This interface is necessary for the NegotiateScheme authentication scheme because the service names for the authentication tickets are based on the hostname of the target host or proxy host, depending on whether it's normal or proxy authentication, and this information is only available from the HttpContext.

Without the HttpContext there is a workaround that works most of the time, which looks like this:

	String host;
	if (isProxy()) {
		// FIXME this should actually taken from the HttpContext.
		HttpHost proxy = ConnRouteParams.getDefaultProxy(request.getParams());
		host = proxy.getHostName();
	} else {
		host = request.getLastHeader(""Host"").getValue();
	}

"
0,"Allow query results with unknown sizeTo further optimize certain queries the query implementation should be changed to allow for unknown result sizes. Currently there is only one query ( //* ) where the query result returns an unknown size and a special query result implementation is returned. At the same time, this should be fixed that only one implementation is used."
0,"XMLReader logs fatal error to system outSome test cases check if an appropriate exception is thrown when invalid XML is supplied, in that case the build in XMLReader in Java 1.5 logs a fatal error to system out.

This seems to be caused by a missing error handler on the XMLReader."
0,"DbDataStore: delete temporary files using finalize()Currently, reading from the DbDataStore creates a temporary file by default. If the application doesn't fully read or close the input stream, the file is not deleted. The best solution is to use finally { in.close() } in the application, but this is easily forgotten.

I suggest to delete the temp file using finalize(). There is a small performance penalty when creating the temporary object, but compared to I/O it is very small. Note that FileInputStream and FileOutputStream also use finalize()."
1,RepositoryConfig created by Jcr2spiRepositoryFactory should always return same RepositoryService instanceThe Jcr2spiRepositoryFactory uses a default implementation of RepositoryConfig if none is passed to it by the user. Currently this default implementation returns a new RepositoryService instance on each call to getRepositoryService(). This is not correct since the consumer of the RepositoryConfig instance expects the same RepositoryService instance on every call. 
1,"TokenStream.next(Token) reuse 'policy': calling Token.clear() should be responsibility of producer.Tokenizers which implement the reuse form of the next method:
    next(Token result) 
should reset the postionIncrement of the returned token to 1."
1,"SQL2 Left Outer JoinCreate this nodes.
def n1 = root.addNode(""node1"", ""sling:SamplePage"");
n1.setProperty(""n1prop1"", ""page1"");
def n2 = n1.addNode(""node2"", ""sling:SampleContent"");
n2.setProperty(""n2prop1"", ""content1"");

Execute this Query:
Select * from [sling:SamplePage] as page left outer join [sling:SampleContent] as content on ISDESCENDANTNODE(content,page) where page.n1prop1 = 'page1' and content.n2prop1 = 'content1';
The resultset have 1 row with 2 Nodes. This OK.

Then execute this:
Select * from [sling:SamplePage] as page left outer join [sling:SampleContent] as content on ISDESCENDANTNODE(content,page) where page.n1prop1 = 'page1' and content.n2prop1 = 'XXXXX';

The resultset has 1 row with 1 node.
This wrong. The result should be 0 rows.

Old Versions, prior 2.2.2 have also 0 rows as result.

Also, if nodes ""n2"" not exists, jackrabbit reports 1 row as result.

"
0,"Configure the maven build for IDE project generation for IDEA and EclipseCan we add a plugin configuration for the maven-idea-plugin and maven-eclipse-plugin, with JDK version set for IDEA and configured source download of dependencies?

Simplifies project regeneration and working with IDEA or Eclipse.

I'll add a patch.

Thanks!


 "
0,"Things to be done now that Filter is independent from BitSet(Aside: where is the documentation on how to mark up text in jira comments?)

The following things are left over after LUCENE-584 :

For Lucene 3.0  Filter.bits() will have to be removed.

There is a CHECKME in IndexSearcher about using ConjunctionScorer to have the boolean behaviour of a Filter.

I have not looked into Filter caching yet, but I suppose there will be some room for improvement there.
Iirc the current core has moved to use OpenBitSetFilter and that is probably what is being cached.
In some cases it might be better to cache a SortedVIntList instead.

Boolean logic on DocIdSetIterator is already available for Scorers (that inherit from DocIdSetIterator) in the search package. This is currently implemented by ConjunctionScorer, DisjunctionSumScorer,
ReqOptSumScorer and ReqExclScorer.
Boolean logic on BitSets is available in contrib/misc and contrib/queries

DisjunctionSumScorer calls score() on its subscorers before the score value actually needed.
This could be a reason to introduce a DisjunctionDocIdSetIterator, perhaps as a superclass of DisjunctionSumScorer.

To fully implement non scoring queries a TermDocIdSetIterator will be needed, perhaps as a superclass of TermScorer.

The javadocs in org.apache.lucene.search using matching vs non-zero score:
I'll investigate this soon, and provide a patch when necessary.

An early version of the patches of LUCENE-584 contained a class Matcher,
that differs from the current DocIdSet in that Matcher has an explain() method.
It remains to be seen whether such a Matcher could be useful between
DocIdSet and Scorer.

The semantics of scorer.skipTo(scorer.doc()) was discussed briefly.
This was also discussed at another issue recently, so perhaps it is wortwhile to open a separate issue for this.

Skipping on a SortedVIntList is done using linear search, this could be improved by adding multilevel skiplist info much like in the Lucene index for documents containing a term.

One comment by me of 3 Dec 2008:

A few complete (test) classes are deprecated, it might be good to add the target release for removal there.
"
0,"Improve exception handling in observation (ChangePolling)Currently when an (internal) event listener throws an exception, all further event listeners are skipped. This happens for move events where the HierarchyListener throws an UnsupportedOperationException. I suggest to move the exception handler up the call chain such that exceptions are caught and logged per listener instead of for all listeners. "
0,"Lazy field loading breaks backward compatDocument.getField() and Document.getFields() have changed in a non backward compatible manner.
Simple code like the following no longer compiles:
 Field x = mydoc.getField(""x"");"
0,"java.util.logging configuration examples does not work as intendedjava.util.logging configuration examples do not work as intended. Those can be found here: http://jakarta.apache.org/httpcomponents/httpclient-3.x/logging.html

Steps to reproduce:
1. Create a simple project using HttpClient (see listing below) and JDK 1.6 (I suppose it is JDK 1.4 or higher, but I did not test anything other than 1.6). Without log4j in the classpath and without any commons-logging system properties set, java.util.logging is automatically selected by commons-logging.
2. Create logging.properties file as shown in any of the java.util.logging examples
3. Run a program, passing -Djava.util.logging.config.file=logging.properties argument to the JVM

Expected results:
Quite a few log messages should be sent to System.err

Actual results:
Unless there is an I/O error encountered, no log messages are sent to System.err

The problem, as far as I can see, is caused by the default logging level of java.util.logging.ConsoleHandler, which is set to INFO. In order for any log messages to go through, the log hadler log level needs to be lower than logged messages log level. Adding the following line to all java.util.logging examples should fix the problem:

java.util.logging.ConsoleHandler.level = ALL

--- Get.java -----------------------------------------

import java.io.IOException;
import java.io.InputStreamReader;
import java.io.Reader;

import org.apache.commons.httpclient.HostConfiguration;
import org.apache.commons.httpclient.HttpClient;
import org.apache.commons.httpclient.HttpException;
import org.apache.commons.httpclient.HttpMethodBase;
import org.apache.commons.httpclient.methods.GetMethod;


public class Get {

	/**
	 * @param args
	 */
	public static void main(String[] args) {
		
 		HttpClient client = new HttpClient();
		HttpMethodBase get = new GetMethod(""http://www.apache.org"");
		
		try {
			int code = client.executeMethod(get);
			System.out.println(""Status code: "" + code);
			
			String csn = get.getResponseCharSet();
			System.out.println(""Charset is: "" + csn);
			
			long len = get.getResponseContentLength();
			System.out.println(""Length is: "" + len);
			len = len < 0 ? 200 : len;
			
			StringBuilder buf = new StringBuilder((int)len);
			Reader r = new InputStreamReader(get.getResponseBodyAsStream(), csn);
			for (int c = r.read(); c >= 0; c = r.read()) {
				buf.append((char)c);
			}
			
			System.out.println(""Body:"");
			System.out.println(buf.toString());			
			
		} catch (HttpException e) {
			e.printStackTrace();
		} catch (IOException e) {
			e.printStackTrace();
		} finally {
			get.releaseConnection();
		}				
	}
}

--- logging.properties ----- From examples ----------------------

.level=INFO

handlers=java.util.logging.ConsoleHandler
java.util.logging.ConsoleHandler.formatter = java.util.logging.SimpleFormatter

httpclient.wire.header.level=FINEST
org.apache.commons.httpclient.level=FINEST

--------------------------------------------------------------------------------
"
0,Perform random operation testsAs discussed on the mailing list and in other jira issues it makes sense to execute tests that perform random operations on the repository. This helps us detect concurrency issues in jackrabbit and increase code coverage.
0,"NodeTypeRegistry.registerNodetypes(Collection) should not register a partial setthe javadoc says:

     * Note that in the case an exception is thrown, some node types might have
     * been nevertheless successfully registered.

the problem hereby is, that it cannot be determined easily, what nodetypes could be registered, and which couldnt. i would rather prefer a all-or-nothing behaviour."
1,"SpellChecker not working because of stale IndexSearcherThe SpellChecker unit test did not work, because of a stale IndexReader and IndexSearcher instance after calling indexDictionary(Dictionary)."
1,"SessionImpl.createSession uses same Subject/LoginContextSessionImpl.createSession(String) uses the same loginctx/subject to create a new session.
this will cause problems if Session.logout() is called on the original instance.

i suggest to fix that by creating a new subject for the new session instance."
1,"XATest error: commit from different thread but same XID must not blockI'm seeing the following test error quite often in the CI server at work:

testDistributedThreadAccess(org.apache.jackrabbit.core.XATest)  Time elapsed: 0.213 sec  <<< ERROR!
javax.transaction.SystemException: commit from different thread but same XID must not block
	at org.apache.jackrabbit.core.UserTransactionImpl.commit(UserTransactionImpl.java:147)
	at org.apache.jackrabbit.core.XATest.testDistributedThreadAccess(XATest.java:1637)

It seems to be a system-specific issue, as I've never seen the same error locally or on Hudson."
0,"jcr:like() does not scale well on large value rangesThere are two major issues with the current WildcardQuery implementation:

1) A wildcard expression is restricted to match at most 1024 terms, otherwise a TooManyClauses exception is thrown. Similar to the RangeQuery issue: JCR-111
2) The enumeration over the terms that match the wildcard pattern is slow"
0,"TCK: NodeOrderableChildNodesTest tests node order even if node type doesn't support child node orderingNodeOrderableChildNodesTest# testOrderBeforeUnsupportedRepositoryOperationException

This test calls prepareTest, which requires getNodes() to return the child nodes in the order added, even if the node type doesn't support child node ordering.  JSR-170 (Section 4.4.2) imposes no such requirement.

Proposal: do not check child node order in this test case.
"
1,jcr:encoding not respected in NodeIndexerThe value of the jcr:encoding property is not passed to the TextFilter instances.
0,"TCK: NodeMixinUtil should exclude for mix:shareableThe mixin test NodeRemoveMixinTest#testRemoveSuccessfully tries to remove a mixin:

what it does: retrieve an addable mixin (NodeTypeUtil#getAddableMixinName), adds it and tries to remove it later on.
the addable mixins are retrieve from the complete set of mixin, testing node.canAddMixin.

However: with jackrabbit-core ""mix:shareable"" can be added but not removed.
if the test by chance gets exactly that mixin the test fails with exception (there is an explicit check the core for exactly that case).

the tck should exclude that special case, shouldn't it?

((michael found the issue)) "
0,release is not signedthere are no signatures & checksums available for your latest release
0,Configure occurrence of property value in excerptJackrabbit currently includes all indexed property values as potential content in an excerpt. This is not always desirable because there may be properties that need to be full-text indexed but should not show up in an excerpt.
1,"TestDocValuesIndexing reproducible  test failuredocvalues branch: r1131275

{code}
    [junit] Testsuite: org.apache.lucene.index.values.TestDocValuesIndexing
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.81 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestDocValuesIndexing -Dtestmethod=testAddIndexes -Dtests.seed=-3253978684351194958:-8331223747763543724
    [junit] NOTE: test params are: codec=RandomCodecProvider: {id=Standard, BYTES_VAR_STRAIGHT=Pulsing(freqCutoff=12), BYTES_FIXED_SORTED=MockRandom}, locale=es_MX, timezone=Pacific/Chatham
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestDocValuesIndexing]
    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=89168480,total=125632512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testAddIndexes(org.apache.lucene.index.values.TestDocValuesIndexing):     FAILED
    [junit] [first=BYTES_FIXED_SORTED, second=BYTES_VAR_STRAIGHT] expected:<9> but was:<10>
    [junit] junit.framework.AssertionFailedError: [first=BYTES_FIXED_SORTED, second=BYTES_VAR_STRAIGHT] expected:<9> but was:<10>
    [junit]     at org.apache.lucene.index.values.TestDocValuesIndexing.testAddIndexes(TestDocValuesIndexing.java:208)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1348)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1266)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.index.values.TestDocValuesIndexing FAILED
{code}"
0,"Improve StandardTokenizer's understanding of non ASCII punctuation and quotesIn the vein of LUCENE-1126 and LUCENE-1390, StandardTokenizerImpl.jflex should do a better job at understanding non-ASCII punctuation characters.

For example, its understanding of the single-quote character ""'"" is currently limited to that character only. It will set a token's type to APOSTROPHE only if the ""'"" was used.
In the patch attached, I added all the characters that ASCIIFoldingFilter would change into ""'"".

I'm not sure that this is the right approach so I didn't write a complete patch for all the other hardcoded characters used in jflex rules such as ""."", ""-"" which have some variants in ASCIIFoldingFilter that could be used as well.

Maybe a better approach would be to make it possible to have an ASCIIFoldingFilter-like reader as a character filter that could be in inserted in front of StandardTokenizer ?"
0,"handle multivalue headers correctlySome times, web servers send back multiple headers with the same key. e.g.

WWW-Authenticate: Negotiate
WWW-Authenticate: NTLM
WWW-Authenticate: Basic realm=""kmdc5""

To handle this correctly, we should add a method 

public java.util.Iterator getResponseHeaders(java.lang.String name)

just as in javax.servlet.http.HttpServletRequest."
0,"removing properties through SPI: two ways to do itBatch currently provides two ways to delete a property, similarly to JCR:

- Batch.remove()
- Batch.setValue(..., null)

JCR2SPI currently uses (AFAIK) Batch.remove().

Proposal:

- clarify that the QValue argument in setValue must be non-null (same for setValues)

"
1,"TermVectors corruption case when autoCommit=falseI took Yonik's awesome test case (TestStressIndexing2) and extended it to also compare term vectors, and, it's failing.

I still need to track down why, but it seems likely a separate issue."
0,"populate.jsp uses Java 1.5 methodThe method is URLConnection.setReadTimeout()
"
0,"SimpleSelectionTest assumes RowIterator.getSize() not to return -1Test case ""testSingleProperty"" assumes that RowIterator.getSize() will not return -1. This is an incorrect assumption, according to the JavaDoc for RangeIterator.

Suggested change:

        long size = result.getRows().getSize();
        if (size != -1) {
            assertEquals(""Should have only 1 result"", 1, size);
        }
"
0,"Support lower and upper case functions in ""order by"" clauseThe query languages should support lower- and upper-case functions within the ""order by"" clause.  This would provide case-insensitive ordering of query results.

Example:  Find all ""nt:base"" nodes ordered by the ""foo"" property, but ignoring case

In XPath:

//element(*,nt:base) order by fn:lower-case(@foo)

In SQL:

SELECT * FROM nt:base ORDER BY lower(foo)

"
0,Limit fields read from indexReading a lucene document from the index should be limited to only those fields that are necessary.
0,"Make shutdown hooks in TransientFileFactory removableTransientFileFactory class always registers shutdown hook. So, if jackrabbit classes were loaded by web-app classloader, they will not be released when web-app is undeployed (if jackrabbit-jcr-commons JAR is inside WAR). This causes classloader leak.
It seems to be useful to have ability to cancel TransientFileFactory's shutdown hook when application is going to be unloaded to avoid classloader leak."
0,"Revise Weight#scorer & Filter#getDocIdSet API to pass Readers contextSpinoff from LUCENE-2694 - instead of passing a reader into Weight#scorer(IR, boolean, boolean) we should / could revise the API and pass in a struct that has parent reader, sub reader, ord of that sub. The ord mapping plus the context with its parent would make several issues way easier. See LUCENE-2694, LUCENE-2348 and LUCENE-2829 to name some.

"
0,"Wrapup flexible indexingSpinoff from LUCENE-1458.

The flex branch is in fairly good shape -- all tests pass, initial search performance testing looks good, it survived several visits from the Unicode policeman ;)

But it still has a number of nocommits, could use some more scrutiny especially on the ""emulate old API on flex index"" and vice/versa code paths, and still needs some more performance testing.  I'll do these under this issue, and we should open separate issues for other self contained fixes.

The end is in sight!"
0,"RepositoryFactory implementation for jcr2spiThere should be a RepositoryFactory implementation in jcr2spi that also covers acquiring the underlying RepositoryService.

For this purpose I suggest to create:
-  a RepositoryServiceFactory in jackrabbit-spi, which encapsulates the spi implementation specifc instantiation of the RepositoryService. the factory probably just needs a single method that takes a parameters map.
- a RepositoryFactory implementation in jcr2spi, which works with a URI that contains all required information to connect/acquired the RepositoryService.

To use jcr2spi/spi2jcr/jackrabbit-core:
- jcr+file://path/to/repository/home?config=repository.xml

To use jcr2spi/spi2dav:
- jcr+dav://localhost:8080/server/repository/?br=4

To use jcr2spi/spi2davex:
- jcr+davex://localhost:8080/server/repository/

An implementation of RepositoryServiceFactory must check the scheme and decide if it can handle it and create a RepositoryService instance with it, otherwise it must return null. This means there would be a single name for the connect URI for all RepositoryServiceFactory implementations.

"
0,"Code depends on Log4J directlyThe code is written against the Log4J APIs, which forces all users of Jackarabbit to pick up log4J dependency and to juggle with JDK logging and Log4J configuration if other components of the project uses JDK 1.4 logging.
If the code is move to depend on Apache commons-logging this issue will be resolved. Also this should be a minor fix."
0,JSR 283: Binary interfaces The Binary interface replaces the deprecated methods for getting/setting the InputStream of a given JCR value and the method to create binary value (ValueFactory).
1,"index corruption autoCommit=falseIn both Lucene 2.3 and trunk, the index becomes corrupted when autoCommit=false"
1,"CloseableThreadLocal should allow null ObjectsCloseableThreadLocal does not allow null Objects in its get() method, but does nothing to prevent them in set(Object). The comment in get() before assert v != null is irrelevant - the application might have passed null.

Null is an important value for Analyzers. Since tokenStreams (a ThreadLocal private member in Analyzer) is not accessible by extending classes, the only way for an Analyzer to reset the tokenStreams is by calling setPreviousTokenStream(null).

I will post a patch w/ a test"
0,"intermittent failures of  TestTimeLimitedCollector.testTimeoutMultiThreaded in nightly testsOccasionly TestTimeLimitedCollector.testTimeoutMultiThreaded fails. e.g. with this output:

{noformat}
   [junit] ------------- Standard Error -----------------
   [junit] Exception in thread ""Thread-97"" junit.framework.AssertionFailedError: no hits found!
   [junit]     at junit.framework.Assert.fail(Assert.java:47)
   [junit]     at junit.framework.Assert.assertTrue(Assert.java:20)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.doTestTimeout(TestTimeLimitedCollector.java:152)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.access$100(TestTimeLimitedCollector.java:38)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector$1.run(TestTimeLimitedCollector.java:231)
   [junit] Exception in thread ""Thread-85"" junit.framework.AssertionFailedError: no hits found!
   [junit]     at junit.framework.Assert.fail(Assert.java:47)
   [junit]     at junit.framework.Assert.assertTrue(Assert.java:20)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.doTestTimeout(TestTimeLimitedCollector.java:152)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.access$100(TestTimeLimitedCollector.java:38)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector$1.run(TestTimeLimitedCollector.java:231)
   [junit] ------------- ---------------- ---------------
   [junit] Testcase: testTimeoutMultiThreaded(org.apache.lucene.search.TestTimeLimitedCollector):      FAILED
   [junit] some threads failed! expected:<50> but was:<48>
   [junit] junit.framework.AssertionFailedError: some threads failed! expected:<50> but was:<48>
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.doTestMultiThreads(TestTimeLimitedCollector.java:255)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.testTimeoutMultiThreaded(TestTimeLimitedCollector.java:220)
   [junit]
{noformat}

Problem either in test or in TimeLimitedCollector."
1,"Passing a null fieldname to MemoryFields#terms in MemoryIndex throws a NPEI found this when querying a MemoryIndex using a RegexpQuery wrapped by a SpanMultiTermQueryWrapper.  If the regexp doesn't match anything in the index, it gets rewritten to an empty SpanOrQuery with a null field value, which then triggers the NPE."
1,"Thread safety issue can cause index corruption when autoCommit=true and multiple threads are committingThis is only present in 2.9 trunk, but has been there since
LUCENE-1516 was committed I believe.

It's rare to hit: it only happens if multiple calls to commit() are in
flight (from different threads) and where at least one of those calls
is due to a merge calling commit (because autoCommit is true).

When it strikes, it leaves the index corrupt because it incorrectly
removes an active segment.  It causes exceptions like this:
{code}
java.io.FileNotFoundException: _1e.fnm
	at org.apache.lucene.store.MockRAMDirectory.openInput(MockRAMDirectory.java:246)
	at org.apache.lucene.index.FieldInfos.<init>(FieldInfos.java:67)
	at org.apache.lucene.index.SegmentReader.initialize(SegmentReader.java:536)
	at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:468)
	at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:414)
	at org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:641)
	at org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:627)
	at org.apache.lucene.index.DocumentsWriter.applyDeletes(DocumentsWriter.java:923)
	at org.apache.lucene.index.IndexWriter.applyDeletes(IndexWriter.java:4987)
	at org.apache.lucene.index.IndexWriter.doFlushInternal(IndexWriter.java:4165)
	at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:4025)
	at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:4016)
	at org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:2077)
	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:2040)
	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:2004)
	at org.apache.lucene.index.TestStressIndexing2.indexRandom(TestStressIndexing2.java:210)
	at org.apache.lucene.index.TestStressIndexing2.testMultiConfig(TestStressIndexing2.java:104)
{code}

It's caused by failing to increment changeCount inside the same
synchronized block where segmentInfos was changed, in commitMerge.
The fix is simple -- I plan to commit shortly.
"
0,"Support for transactions when using JCR over RMI.At this time, the sessions obtained from o.a.j.rmi.client.LocalAdapterFactory do not implement the methods for the XASession.  Therefor the RMI access layer does not support a transactional session."
0,"BrowserCompatHostnameVerifier and StrictHostnameVerifier should handle wildcards in SSL certificates betterI ran into a problem with SSL wildcard certificates in the class BrowserCompatHostnameVerifier. It handles ""*.example.org"" fine but ""server*.example.org"" fails to work correctly. The javadoc claims that it should behave the same way as curl and FireFox. In Firefox an SSL certificate for ""server*.example.org"" works fine for the host ""server.example.org"", using HttpClient it throws an exception.

Here is an example test (JUnit4):

package org.example.hb;

import javax.net.ssl.SSLException;

import org.apache.http.conn.ssl.BrowserCompatHostnameVerifier;
import org.junit.Test;

public class BrowserCompatHostnameVerifierTest {

	/**
	 * Should not throw an exeption in the verify method.
	 * @throws SSLException
	 */
	@Test
	public void testVerifyStringStringArrayStringArray() throws SSLException
	{
		BrowserCompatHostnameVerifier hv = new BrowserCompatHostnameVerifier();
		String host = ""www.example.org"";
		String[] cns = {""www*.example.org""};
		
		hv.verify(host, cns, cns);
	}

}"
0,"Improve BaseTokenStreamTestCase to uses a fake attribute to check if clearAttributes() was called correctly - found bugs in contrib/analyzersRobert had the idea to use a fake attribute inside BaseTokenStreamTestCase that records if its clear() method was called. If this is not the case after incrementToken(), asserTokenStreamContents fails. It also uses the attribute in TeeSinkTokenFilter, because there a lot of copying, captureState and restoreState() is used. By the attribute, you can track wonderful, if save/restore and clearAttributes is correctly implemented. It also verifies that *before* a captureState() it was also cleared (as the state will also contain the clear call). Because if you consume tokens in a filter, capture the consumed tokens and insert them, the capturedStates must also be cleared before.

In contrib analyzers are some test that fail to pass this additional assertion. They are not fixed in the attached patch."
0,"FileDataStore should check for lastModified error resultAccording to javadoc for File.lastModified(), the return value may indicate error: ""...or 0L if the file does not exist or if an I/O error occurs"".

Accordingly, FileDataStore should be checking for this return value, rather than treating it as an actual modification time of ""0"".

Patch to follow.
"
1,"Issue while loading list of classes at that path itself.Hi,

I cannot retrieve list of objects that are directly under the path that they were saved in. I did not know where to simulate this issue and hence I have used DigesterSimpleQueryTest. I have attached the path for the newly added test case testObjectListRetrievalAtBasePath. In case the patch is not up to the mark I have attached the modified file too.

Instead of creating Page in /test if I create it in /sample/test and search in /sample/test it returns nothing but if I search in /sample it would return the object.

Another important point here is that it is causing issues while retrieving Page class, the other test cases that are retrieving Paragraph class (embedded inside Page class) are still working fine!

Regards,

Kaizer"
0,"Configuration of CacheManager memory sizes(I already posted this as comments under JCR-619.)

The maximum size for all caches in CacheManager is hardcoded to 16 megabytes and there's no way to change that. It would be nice if this as well as other CacheManager parameters were configurable. It's just a waste running Jackrabbit on a server with gigabytes of memory and only using 16 megabytes for cache...

I have created a really simple and straightforward patch (jackrabbit-cachemanager-config.patch) which enables reaching the CacheManager instance through RepositoryImpl object and setting all three of its memory parameters. The memory parameters are no longer static constants, but instance fields getting initial values from constants (so the default behavior of the class remains the same).

(It would be even nicer if these parameters were configurable via configuration files, but that should probably be implemented by someone close to the project.)"
1,"FilteredDocIdSet does not handle a case where the inner set iterator is nullDocIdSet#iterator is allowed to return null, when used in FilteredDocIdSet, if null is returned from the inner set, the FilteredDocIdSetIterator fails since it does not allow for nulls to be passed to it.

The fix is simple, return null in FilteredDocIdSet in the iterator method is the iterator is null."
0,"Change DateTools to not create a Calendar in every call to dateToString or timeToStringDateTools creates a Calendar instance on every call to dateToString and timeToString. Specifically:

# timeToString calls Calendar.getInstance on every call.
# dateToString calls timeToString(date.getTime()), which then instantiates a new Date(). I think we should change the order of the calls, or not have each call the other.
# round(), which is called from timeToString (after creating a Calendar instance) creates another (!) Calendar instance ...

Seems that if we synchronize the methods and create the Calendar instance once (static), it should solve it."
0,"Add Warnlog on Extraction FailureIt will be fine to have a feedback if a exception occurs in the TextExtractors.
At the moment only a empty StringReader will be returned.
We had the issue that we updated the content and in the textextractor a exception occured
so the index was not updated and the document was searchable by its old content."
0,"Open up org.apache.commons.httpclient.Base64 pleaseI have had several problems lately where I needed to truck backwards and
forwards between bytes and base64. As I am using HttpClient, I know I have the
code in my proect, but I need to duplicate it into my own heirarchy to get
access rights. 

Please make appropriate changes to Base64 (can be as simple as marking the
encode and decode methods public) to allow outside use of Base64.

Would be nice to extend Base64 to deal with multi-line Base64 content too - but
I know this is outside of Base64 original intended use. But it would be useful. :)"
0,"Transparent Content Coding supportI would like to see HttpClient features brought up to parity with other libraries, both in Java and other languages. c.f. Python's httplib2 (not yet in the standard library, but many would like to see it in there). That library transparently handles gzip and compress content codings.

This issue is to capture possible solutions to providing this sort of innate functionality in HttpClient, so that users aren't required to know RFC2616 intimately. The HttpClient library should do the right thing and use the network in the most efficient manner possible."
1,"IndexOutOfBoundsException at ShingleMatrixFilter's Iterator#hasNext methodI tried to use the ShingleMatrixFilter within Solr. To test the functionality etc., I first used the built-in field analysis view.The filter was configured to be used only at query time analysis with ""_"" as spacer character and a min. and max. shingle size of 2. The generation of the shingles for query strings with this filter seems to work at this view, but by turn on the highlighting of indexed terms that will match the query terms, the exception was thrown. Also, each time I tried to query the index the exception was immediately thrown.

Stacktrace:
{code}
java.lang.IndexOutOfBoundsException: Index: 1, Size: 1
	at java.util.ArrayList.RangeCheck(Unknown Source)
	at java.util.ArrayList.get(Unknown Source)
	at org.apache.lucene.analysis.shingle.ShingleMatrixFilter$Matrix$1.hasNext(ShingleMatrixFilter.java:729)
	at org.apache.lucene.analysis.shingle.ShingleMatrixFilter.next(ShingleMatrixFilter.java:380)
	at org.apache.lucene.analysis.StopFilter.next(StopFilter.java:120)
	at org.apache.lucene.analysis.TokenStream.next(TokenStream.java:47)
	...
{code}

Within the hasNext method, there is the {{s-1}}-th Column from the ArrayList {{columns}} requested, but there isn't this entry within columns.

I created a patch that checks, if {{columns}} contains enough entries."
1,"MatchAllDocsQuery doesn't honor boost or queryNormMatchAllDocsQuery doesn't pay attention to either it's own boost, or lucene's query normalization factor."
1,"removing source parent node after session move throws on savethe following code fragment illustrates the problem:

        /**
         * create the following node tree:
         *     
         *       + A
         *         + B
         *            + C
         *         + D
         */
        Node A;
        if (root.hasNode(""A"")) {
            A = root.getNode(""A"");
        } else {
            A = root.addNode(""A"");
        }
        Node B = A.addNode(""B"");
        Node C = B.addNode(""C"");
        Node D = A.addNode(""D"");
        root.save();

        // move C under D
        session.move(""/A/B/C"", ""/A/D/C"");
        // remove B
        A.getNode(""B"").remove();
        /**
         * the expected resulting node tree:
         *     
         *       + A
         *         + D
         *            + C
         */
        A.save();


==> the last save() will throw 
javax.jcr.RepositoryException: inconsistency: failed to retrieve transient state for ...
 "
1,"303 Redirects are not handled properlyWhen the server spits back a 303 (See Other), the redirect is not handled. 
Looking at the code, I saw that the processRedirectResponse method in
HttpMethodBase does not check for SC_SEE_OTHER in the case statement. 
SC_SEE_OTHER is a redirect and should be handled appropriately.

Here is a trace from the output of the client and server.

GET http://172.30.229.75/CGI/Screenshot HTTP/1.1 
Authorization: Basic c3VwZXJ1c2VyOnJvb3Q= 
Host: 172.30.229.75 
User-Agent: Jakarta Commons-HttpClient/2.0M1 

HTTP/1.1 303 See Other 
Location: http://172.30.229.75/FS/CIP_0_5842
Content-Length: 0 
Server: *snip*"
0,"Add offsets to postings (D&PEnum)I think should explore making start/end offsets a first-class attr in the
postings APIs, and fixing the indexer to index them into postings.

This will make term vector access cleaner (we now have to jump through
hoops w/ non-first-class offset attr).  It can also enable efficient
highlighting without term vectors / reanalyzing, if the app indexes
offsets into the postings.
"
0,"replace UUID strings by UUID classes in NodeId, etc..Currently the UUIDs of the nodes are stored as Strings in the ItemIds and ItemStates and cause alot of overhead throughout jackrabbit. they should be replaced by a fast implementation of a UUID class."
1,"MSSqlFileSystem - JNDI & several configuration issuesthere are several configuration issues using the org.apache.jackrabbit.core.fs.db.MSSqlFileSystem
my (working) configuration (repository.xml) looks like:

<FileSystem class=""org.apache.jackrabbit.core.fs.db.MSSqlFileSystem"">
 <param name=""driver"" value=""javax.naming.InitialContext""/>
 <param name=""url"" value=""java:MYDatasource""/>
 <param name=""schema"" value=""mssql""/>
 <param name=""schemaObjectPrefix"" value=""MYPREFIX_""/>
 <param name=""user"" value=""MYUSERNAME""/> 
 <param name=""password"" value=""MYPASSWORD""/>
 <param name=""tableSpace"" value=""""/>
</FileSystem>

i have to unnecessarily specify username & password, because the MSSqlFileSystem presets them to an empty string instead of null. funnily enough  the tableSpace is preset to null, which leads to a nullpointer in createSchemaSql



"
0,"Remove benchmark/lib/xml-apis.jar - JVM 1.5 already contains the required JAXP 1.3 implementationOn [LUCENE-2957|https://issues.apache.org/jira/browse/LUCENE-2957?focusedCommentId=13004991&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13004991], Uwe wrote:
{quote}
xml-apis.jar is not needed with xerces-2.9 and Java 5, as Java 5 already has these interface classes (JAXP 1.3). Xerces 2.11 needs it, because it ships with Java 6's JAXP release (containing STAX & Co. not available in Java 5).
{quote}

On the #lucene IRC channel, Uwe also wrote:
{noformat}
since we are on java 5 since 3.0
we have the javax APIs already available in the JVM
xerces until 2.9.x only needs JAXP 1.3
so the only thing you need is xercesImpl.jar
and serializer.jar
serializer.jar is shared between all apache xml projects, dont know the exact version number
ok you dont need it whan you only parse xml
as soon as you want to serialize a dom tree or result of an xsl transformation you need it
[...]
but if we upgrade to latest xerces we need it [the xml-apis jar] again unless we are on java 6
so the one shipped with xerces 2.11 is the 1.4 one
because xerces 2.11 supports Stax
{noformat}"
1,"Test failures in jcr-rmi and jcr2davIntegration testing currently fails for jcr-rmi:
  testCloneNodesAccessDenied(org.apache.jackrabbit.test.api.WorkspaceCloneTest)
  testCopyNodesAccessDenied(org.apache.jackrabbit.test.api.WorkspaceCopyBetweenWorkspacesTest)
  testCopyNodesAccessDenied(org.apache.jackrabbit.test.api.WorkspaceCopyTest)
  testMoveNodesAccessDenied(org.apache.jackrabbit.test.api.WorkspaceMoveTest)
  testImpersonate(org.apache.jackrabbit.test.api.ImpersonateTest)
  testCheckPermission(org.apache.jackrabbit.test.api.CheckPermissionTest)
  testRemoveItem4(org.apache.jackrabbit.test.api.SessionRemoveItemTest)
  testReadOnlyPermission(org.apache.jackrabbit.test.api.HasPermissionTest)
  testGetPrivileges(org.apache.jackrabbit.test.api.security.RSessionAccessControlDiscoveryTest)
  testNotHasPrivileges(org.apache.jackrabbit.test.api.security.RSessionAccessControlDiscoveryTest)
  testGetApplicablePolicies(org.apache.jackrabbit.test.api.security.RSessionAccessControlPolicyTest)
  testGetPolicy(org.apache.jackrabbit.test.api.security.RSessionAccessControlPolicyTest)
  testGetEffectivePolicy(org.apache.jackrabbit.test.api.security.RSessionAccessControlPolicyTest)
  testMoveNode(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testSetProperty(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testSetValue(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testDeleteNode(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testDeleteProperty(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testWorkspaceMoveNode(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testCopyNode(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)

and jcr2dav:
  testCloneNodesAccessDenied(org.apache.jackrabbit.test.api.WorkspaceCloneTest)
  testCopyNodesAccessDenied(org.apache.jackrabbit.test.api.WorkspaceCopyBetweenWorkspacesTest)
  testCopyNodesAccessDenied(org.apache.jackrabbit.test.api.WorkspaceCopyTest)
  testMoveNodesAccessDenied(org.apache.jackrabbit.test.api.WorkspaceMoveTest)
  testCheckPermission(org.apache.jackrabbit.test.api.CheckPermissionTest)
  testRemoveItem4(org.apache.jackrabbit.test.api.SessionRemoveItemTest)
  testReadOnlyPermission(org.apache.jackrabbit.test.api.HasPermissionTest)
  testMoveNode(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testSetProperty(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testSetValue(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testDeleteNode(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testDeleteProperty(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testWorkspaceMoveNode(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testCopyNode(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
"
0,"Jcr-Server Contrib: Remove JDOM dependenciesJDOM has been replace throught the jackrabbit project except the jcr-server contrib.

"
0,"optimizer for n-gram PhraseQueryIf 2-gram is used and the length of query string is 4, for example q=""ABCD"", QueryParser generates (when autoGeneratePhraseQueries is true) PhraseQuery(""AB BC CD"") with slop 0. But it can be optimized PhraseQuery(""AB CD"") with appropriate positions.

The idea came from the Japanese paper ""N.M-gram: Implementation of Inverted Index Using N-gram with Hash Values"" by Mikio Hirabayashi, et al. (The main theme of the paper is different from the idea that I'm using here, though)"
1,"DbInputStream does not support mark()/reset() when exhausted.The DbDataStore implementation uses a DbInputStream to read binary properties from the database. When a new binary property is created, Jackrabbit attempts to index it. Tika's CharsetDetector is used in the process, which marks the input stream, reads the first 8000 bytes and then resets the stream.

This results in the stacktrace shown at the end of the issue, if the following two conditions hold true:
* the property is larger than the minRecordLength configuration of the Datastore and
* the property is smaller than 8000 bytes

The DbInputStream needs to have the following properties:
1. lazy instantiation of the underlying stream
2. auto-close underlying stream when EOF is reached
3. fully support mark()/reset() even if  the underlying stream is auto-closed due to 2.


12.03.2010 15:53:28 *WARN * LazyTextExtractorField: Failed to extract text from a binary property (LazyTextExtractorField.java, line 165)
java.io.EOFException
        at org.apache.jackrabbit.core.data.db.DbInputStream.reset(DbInputStream.java:180)
        at org.apache.tika.io.ProxyInputStream.reset(ProxyInputStream.java:156)
        at org.apache.tika.io.ProxyInputStream.reset(ProxyInputStream.java:156)
        at org.apache.tika.parser.txt.CharsetDetector.setText(CharsetDetector.java:131)
        at org.apache.tika.parser.txt.TXTParser.parse(TXTParser.java:77)
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:120)
        at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:101)
        at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:114)
        at org.apache.jackrabbit.core.query.lucene.LazyTextExtractorField$ParsingTask.run(LazyTextExtractorField.java:160)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:207)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
"
0,"TCK: SerializationTest.helpTestSaxException casts ContentHandler to DefaultHandlerthe JSR170 defines import with ContentHandler (see Session.getImportContentHandler, Workspace.getImportContentHandler)

the mentioned helper method within the TCK casts the ContentHandler returned by those methods to DefaultHandler without testing if the contenthandler is a DefaultHandler (line 273)"
0,"[PATCH] BitSetQuery, FastPrefixQuery, FastWildcardQuery and FastQueryParserFastPrefixQuery and FastWildcardQuery rewrites to BitSetQuery instead of OR'ed
BooleanQuery's.  A BitSetQuery contains a BitSet that desginates which document
should be included in the search result.  BitSetQuery cannot be used by itself
with MultiSearcher as of now."
1,"Do not launch new merges if IndexWriter has hit OOMEif IndexWriter has hit OOME, it defends itself by refusing to commit changes to the index, including merges.  But this can lead to infinite merge attempts because we fail to prevent starting a merge.

Spinoff from http://www.nabble.com/semi-infinite-loop-during-merging-td23036156.html."
0,"allow SPI implementation to compute default values for autocreated propertiesCurrently, when creating nodes in transient space, JCR2SPI uses hard-wired logic trying to populate system generated properties such as jcr:created, jcr;uuid and so on.

This is problematic as

- it doesn't scale -- it fails for autocreated properties not known to JCR2SPI, and

- the syntax for the defaults may be dependant on the back end, such as legal syntax for (UU)IDs.

Proposal:

- extend QValueFactory with something like

  QValue computeDefaultValue(QPropertyDefinition)

- use that in JCR2SPI, getting rid of the currently hard-wired logic.
"
0,IntParser and FloatParser unused by FieldCacheImplFieldCacheImpl doesn't use IntParser or FloatParser to parse values
1,"Node.orderBefore and JackrabbitNode.rename should check for ability to modify children-collection on parent nodecurrently the implementation of Node.orderBefore and JackrabbitNode.rename perform the same validation that is executed
for a move operation which includes removal of the original node. however, the methods mentioned above only include
a manipulation on the child-node-collection of the parent (subset of the current check). therefore the permission check should be 
adjusted accordingly."
0,"Fixed Spelling mailinglist.xmlJust fixed some spelling in the mailinglist.xml in /java/trunk/xdocs



"
0,"References to old repository-1.x.dtdSome components still reference old version of the repository-1.x.dtd.
All components should be upgraded to repository-1.6.dtd"
1,"NullPointerException in IndexModifier.close()We upgraded from Lucene 2.0.0. to 2.3.1 hoping this would resolve this issue.

http://jira.codehaus.org/browse/MRM-715

Trace is as below for Lucene 2.3.1:
java.lang.NullPointerException
at org.apache.lucene.index.IndexModifier.close(IndexModifier.java:576)
at org.apache.maven.archiva.indexer.lucene.LuceneRepositoryContentIndex.closeQuietly(LuceneRepositoryContentIndex.java:416)
at org.apache.maven.archiva.indexer.lucene.LuceneRepositoryContentIndex.modifyRecord(LuceneRepositoryContentIndex.java:152)
at org.apache.maven.archiva.consumers.lucene.IndexContentConsumer.processFile(IndexContentConsumer.java:169)
at org.apache.maven.archiva.repository.scanner.functors.ConsumerProcessFileClosure.execute(ConsumerProcessFileClosure.java:51)
at org.apache.commons.collections.functors.IfClosure.execute(IfClosure.java:117)
at org.apache.commons.collections.CollectionUtils.forAllDo(CollectionUtils.java:388)
at org.apache.maven.archiva.repository.scanner.RepositoryContentConsumers.executeConsumers(RepositoryContentConsumers.java:283)
at org.apache.maven.archiva.proxy.DefaultRepositoryProxyConnectors.transferFile(DefaultRepositoryProxyConnectors.java:597)
at org.apache.maven.archiva.proxy.DefaultRepositoryProxyConnectors.fetchFromProxies(DefaultRepositoryProxyConnectors.java:157)
at org.apache.maven.archiva.web.repository.ProxiedDavServer.applyServerSideRelocation(ProxiedDavServer.java:447)
at org.apache.maven.archiva.web.repository.ProxiedDavServer.fetchContentFromProxies(ProxiedDavServer.java:354)
at org.apache.maven.archiva.web.repository.ProxiedDavServer.process(ProxiedDavServer.java:189)
at org.codehaus.plexus.webdav.servlet.multiplexed.MultiplexedWebDavServlet.service(MultiplexedWebDavServlet.java:119)
at org.apache.maven.archiva.web.repository.RepositoryServlet.service(RepositoryServlet.java:155)
at javax.servlet.http.HttpServlet.service(HttpServlet.java:803)"
0,"jcr-rmi maven ""site"" target failsthe target ""site"" in jcr rmi fails because org.apache.jackrabbit.rmi.remote.SerialValue.java is empty"
1,"WorkspaceAccessManager defined with SecurityManager that keeps users per workspace must test if user existsthe WorkspaceAccessManager defined with the security manager keeping users per workspace currently returns true upon calls to grant(Set, String) if
a workspace with the given name exists.

while this is fine for the initial check upon session creation, it obviously isn't for all method calls that test for accessible workspace names, such as
Workspace#getAccessibleWorkspaceName, Workspace#clone and copy across workspaces.

instead it should test if any of the specified principals corresponds to a valid user within the workspace identified by the given workspaceName."
0,"When using QueryWrapperFilter with CachingWrapperFilter, QueryWrapperFilter returns a DocIdSet that creates a Scorer, which gets cached rather than a bit setthere is a large performance cost to this.

The old impl for this type of thing, QueryFilter, recommends :

@deprecated use a CachingWrapperFilter with QueryWrapperFilter

The deprecated QueryFilter itself also suffers from the problem because its now implemented using a CachingWrapperFilter and QueryWrapperFilter.

see http://search.lucidimagination.com/search/document/7f54715f14b8b7a/lucene_2_9_0rc4_slower_than_2_4_1"
0,"Append-only index updatesCurrently index updates modify some existing files. This is troublesome in scenarios like a backup or when an index will be shared in a cluster (though this is not yet the case).

Requirements are:

- index segments need a custom (lucene) IndexDeletionPolicy to keep index commits for a given time.
- index segments are not only referenced by their name, but also with their generation
- the segments file must now also record the generation of a segment. the file itself must be generational itself.
- purging of outdated index segment commits
"
1,"GzipDecompressingEntity (and therefore ContentEncodingHttpClient) not consistent with EntityUtils.consumeEntityInvoking EntityUtils.consume( entity ) after a previous call to entity.getContent (and subsequent processing of the content) throws a java.io.EOFException when gzip decompression support is enabled via ContentEncodingHttpClient or some similar mechanism.  I invoke EntityUtils.consume in a 'finally' block - maybe I'm not using the API correctly ... ?  

java.io.EOFException
	at java.util.zip.GZIPInputStream.readUByte(GZIPInputStream.java:207)
	at java.util.zip.GZIPInputStream.readUShort(GZIPInputStream.java:197)
	at java.util.zip.GZIPInputStream.readHeader(GZIPInputStream.java:136)
	at java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:58)
	at java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:68)
	at org.apache.http.client.entity.GzipDecompressingEntity.getContent(GzipDecompressingEntity.java:63)
	at org.apache.http.conn.BasicManagedEntity.getContent(BasicManagedEntity.java:88)
	at org.apache.http.util.EntityUtils.consume(EntityUtils.java:65)

I believe the problem is that the underlying DecompressingEntity allocates a new GzipInputStream for each call to getContent, rather than caching the stream created by the first getContent call.  
       http://svn.apache.org/repos/asf/httpcomponents/httpclient/trunk/httpclient/src/main/java/org/apache/http/client/entity/DecompressingEntity.java
The ""CustomProtocolInterceptors"" example has the same bug:  http://hc.apache.org/httpcomponents-client-ga/examples.html

I worked around the problem implementing the example with my own GzipDecompressingEntity (scala code - lazy value not evaluated till accessed):

  class GzipDecompressingEntity( entity:http.HttpEntity) extends http.entity.HttpEntityWrapper(entity) {
    private lazy val gzipStream = new GZIPInputStream( entity.getContent() )
    
    /** 
     * Wrap entity stream in GZIPInputStream
     */
    override def getContent():java.io.InputStream = gzipStream

    /**
     * Return -1 - don't know unzipped content size
     */
    override def getContentLength():Long = -1L
  }

"
0,"Minimize calls to PersistenceManagerIn some situations the PersistenceManager is called even though it is not necessary.

E.g. when new items are created the method NodeImpl.getOrCreateProperty() will always check if there is an already existing property state. If the node is new the call will always go down the full item state stack and ask the PersistenceManager if it knows the property id. This is unnessessary because there will never exist properties in the persistence manager for a new node that has not been saved yet.

I propose to add a check to the method to see if  the node is new and does not yet have a property with the given name. In that case the property can be created without further checks.

With the patch applied the time to transiently create 1000 nodes with 4 properties each drops from 1485 ms to 422 ms."
0,"Analysis package calls Java 1.5 APII found compile errors when I tried to compile trunk with 1.4 JVM.
org.apache.lucene.analysis.NormalizeCharMap
org.apache.lucene.analysis.MappingCharFilter

uses Character.valueOf() which has been added in 1.5.
I added a CharacterCache (+ testcase) with a valueOf method as a replacement for that quite useful method.

org.apache.lucene.analysis.BaseTokenTestCase

uses StringBuilder instead of the synchronized version StringBuffer (available in 1.4)

I will attach a patch shortly."
1,"Occasional NullPointerException in ItemManagerFrom time to time I see a NullPointerException in ItemManager when running ConcurrentReadWriteTest. The exception is probably caused by another session that removes the property, which has the effect that the ItemState in ItemData is set to null.

Exception in thread ""Thread-11"" java.lang.NullPointerException
	at org.apache.jackrabbit.core.ItemManager.canRead(ItemManager.java:313)
	at org.apache.jackrabbit.core.ItemManager.getItemData(ItemManager.java:293)
	at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:226)
	at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:486)
	at org.apache.jackrabbit.core.LazyItemIterator.prefetchNext(LazyItemIterator.java:111)
	at org.apache.jackrabbit.core.LazyItemIterator.<init>(LazyItemIterator.java:93)
	at org.apache.jackrabbit.core.LazyItemIterator.<init>(LazyItemIterator.java:75)
	at org.apache.jackrabbit.core.ItemManager.getChildProperties(ItemManager.java:658)
	at org.apache.jackrabbit.core.NodeImpl.getProperties(NodeImpl.java:2663)
	at org.apache.jackrabbit.core.ConcurrentReadWriteTest$1$1.execute(ConcurrentReadWriteTest.java:65)
	at org.apache.jackrabbit.core.AbstractConcurrencyTest$Executor.run(AbstractConcurrencyTest.java:206)
	at java.lang.Thread.run(Thread.java:595)

This issue does not occur in a release but only in trunk."
0,"Testcase for StandardAnalyzerAs per our discussion on lucene-user, I'm attaching a unit test for 
StandardAnalyzer.  I wrote most of the tests from reading the comments in the 
StandardTokenizer.jj grammar.  Someone familiar with the grammar (and its 
intent) should review the tests."
1,"URI.normalize() errorcode:

----------------------------
import org.apache.commons.httpclient.URI;

class Main {
  publi static void main(String[] args) throws Exception {
    URI uri = new URI(""http"", null, ""host"", -1, ""/tmp/../yo"", null, null);
    uri.normalize();
    System.out.println(uri);
  }
} /// end of Main
----------------------------

prints:

http://host/tmp/../yo

instead of

http://host/yo"
0,"Enable maven-source-pluginCurrently the maven-source-plugin is enabled by default in jackrabbit-jcr-rmi, but it would be good to enable it globally for all Jackrabbit components."
0,"Avoid unnecessary index reader calls when using aggregate definitionsSearchIndex.retrieveAggregateRoot(Set<NodeId> removedIds, Map<NodeId, NodeState> map) identifies aggregate root nodes based on removed nodes and aggregate rules defined in the indexing configuration. This process requires index lookups. The method can be optimized for the case when no nodes are removed and an unnecessary call to the index reader can be avoided."
1,RMI reference not automatically bound by the standalone serverThe RMI servlet in the 1.5.0 standalone server is only initialized (and the remote reference bound to the RMI registry) when the http://.../rmi URL is first accessed. The RMI binding should be made as soon as the standalone server starts.
0,"Contrib analyzers need testsThe analyzers in contrib need tests, preferably ones that test the behavior of all the Token 'attributes' involved (offsets, type, etc) and not just what they do with token text.

This way, they can be converted to the new api without breakage."
1,"IndexWriter.updateDocument is no longer atomicSpinoff from LUCENE-847.

Ning caught that as of LUCENE-843, we lost the atomicity of the delete
+ add in IndexWriter.updateDocument.

Ning suggested a simple fix: move the buffered deletes into
DocumentsWriter and let it do the delete + add atomically.  This has a
nice side effect of also consolidating the ""time to flush"" logic in
DocumentsWriter.

"
0,"Add ReverseStringFilteradd ReverseStringFilter and ReverseStringAnalyzer that can be used for backword much. For Example, ""*ry"", ""*ing"", ""*ber""."
1,"Concurrent Session.move() operations failurePerforming concurrent move operations may cause failures similar to the following:

javax.jcr.RepositoryException: Unable to update item: node /
       at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1147)
       at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:920)
       at ConcurrentMoveTest$1.execute(ConcurrentMoveTest.java:30)
       at org.apache.jackrabbit.core.AbstractConcurrencyTest$Executor.run(AbstractConcurrencyTest.java:209)
       at java.lang.Thread.run(Thread.java:637)
Caused by: org.apache.jackrabbit.core.state.ItemStateException: Unable
to resolve path for item: 79a0fbdb-49fd-4830-a842-5ab11842cd17
       at org.apache.jackrabbit.core.observation.EventStateCollection.getPath(EventStateCollection.java:683)
       at org.apache.jackrabbit.core.observation.EventStateCollection.createEventStates(EventStateCollection.java:268)
       at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:702)
       at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:1110)
       at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:1140)
       at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:351)
       at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:354)
       at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:326)
       at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:328)
       at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1141)
       ... 4 more
Caused by: javax.jcr.ItemNotFoundException: failed to build path of
79a0fbdb-49fd-4830-a842-5ab11842cd17:
826f0c19-9956-402a-9c0d-93089eedcc1c has no child entry for
79a0fbdb-49fd-4830-a842-5ab11842cd17
       at org.apache.jackrabbit.core.HierarchyManagerImpl.buildPath(HierarchyManagerImpl.java:291)
       at org.apache.jackrabbit.core.CachingHierarchyManager.buildPath(CachingHierarchyManager.java:198)
       at org.apache.jackrabbit.core.HierarchyManagerImpl.getPath(HierarchyManagerImpl.java:395)
       at org.apache.jackrabbit.core.CachingHierarchyManager.getPath(CachingHierarchyManager.java:232)
       at org.apache.jackrabbit.core.observation.EventStateCollection.getPath(EventStateCollection.java:678)
       ... 13 more
"
0,"Specialize BooleanQuery if all clauses are TermQueriesDuring work on LUCENE-3319 I ran into issues with BooleanQuery compared to PhraseQuery in the exact case. If I disable scoring on PhraseQuery and bypass the position matching, essentially doing a conjunction match, ExactPhraseScorer beats plain boolean scorer by 40% which is a sizeable gain. I converted a ConjunctionScorer to use DocsEnum directly but still didn't get all the 40% from PhraseQuery. Yet, it turned out with further optimizations this gets very close to PhraseQuery. The biggest gain here came from converting the hand crafted loop in ConjunctionScorer#doNext to a for loop which seems to be less confusing to hotspot. In this particular case I think code specialization makes lots of sense since BQ with TQ is by far one of the most common queries.

I will upload a patch shortly"
0,JSR 283: Evaluate Capabilities Exposed by Session.hasCapability
0,Make WeightedSpanTermExtractor extensible to handle custom query implemenationsCurrently if I have a custom query which subclasses query directly I can't use the QueryScorer for highlighting since it does explicit instanceof checks. In some cases its is possible to rewrite the query before passing it to the highlighter to obtain a primitive query. However I had the usecase where this was not possible ie. the original index was not available on the machine which highlights the results. To still use the highlighter I had to copy a bunch of code due to visibility issues in those classes. I think we can make this extensible with minor effort to allow this usecase without massive code duplication.
0,"BasicResponseHandler Javadoc Needs ClarificationThe class-level javadoc for BasicResponseHandler indicates that it reads the response body before throwing an Exception for responses with status code >= 300, which is not the case."
0,"Rename Analyzer.reusableTokenStream() to tokenStream()All Analysis consumers now use reusableTokenStream().  To finally make reuse mandatory, lets rename resuableTokenStream() to tokenStream() (removing the old tokenStream() method)."
1,"AccessControlManager#setPolicy may fail for new applicable policy despite jcr:modifyAccessControl privilege being grantedthe sequence AccessControlManager.getApplicablePolicies -> modify -> AccessControlManager#setPolicy fails
due to bug in AC evaluation if target is an AC-item but not yet existing. in this case the
wrong parent node is used for AC-evaluation."
1,"Unclosed files when aggregated property states are indexedThis is a regression caused by JCR-1990.

The lucene document for the node that contains the aggregated property may contain an extractor job that has an open file handle to the jcr:data binary property. The document must be disposed after the properties are transferred."
1,"JCR2SPI: lockmgr isn't aware about external unlock (CacheBehavior.OBSERVATION)issue occurring with CacheBehavior.OBSERVATION only:

the lock manager expects the jcr:lockIsDeep property to be created upon successful lock.
this however isn't the case since the time, we changed the Operation.persisted method to invalidate the affected states. consequently the mgr never started to listen on changes made to the jcr:lockIsDeep property and consequently wasn't aware of an external removal.

suggested fix:
force re-loading of the lock holding node."
0,"Document Vector->ArrayListDocument Vector should be changed to ArrayList.
Document is not advertised to be thread safe, and it's doubtful that anyone modifies a Document from multiple threads."
0,"Term improvementTerm is designed for reuse of the supplied filter, to minimize intern().

One of the common use patterns is to create a Term with the txt field being an empty string.

To simplify this pattern and to document it's usefulness, I suggest adding a constructor:
public Term(String fld)
with the obvious implementation
and use it throughout core and contrib as a replacement.

"
1,"org.apache.commons.httpclient.HeaderElement fail to parse cookie headerif Set-Cookie header has value such ""expires=Mon, .."".
org.apache.commons.httpclient.HeaderElement will fail to parse this header.

Cause:
In the source cord:
-------------
            try {
                /*
                 * Following to RFC 2109 and 2965, in order not to conflict
                 * with the next header element, make it sure to parse tokens.
                 * the expires date format is ""Wdy, DD-Mon-YY HH:MM:SS GMT"".
                 * Notice that there is always comma(',') sign.
                 * For the general cases, rfc1123-date, rfc850-date.
                 */
                if (tokenizer.hasMoreTokens()) {
                    String s = nextToken.toLowerCase();
                    if (nextToken.endsWith(""mon"") 
                        || s.endsWith(""tue"")
                        || s.endsWith(""wed"") 
                        || s.endsWith(""thu"")
                        || s.endsWith(""fri"")
                        || s.endsWith(""sat"")
                        || s.endsWith(""sun"")
                        || s.endsWith(""monday"") 
                        || s.endsWith(""tuesday"") 
---- snip ---
 
""if (nextToken.endsWith(""mon"") "" is wrong.
this must be ""if (s.endsWith(""mon"") "".

Source cord version:
 * $Header:
/home/cvspublic/jakarta-commons/httpclient/src/java/org/apache/commons/httpclient/HeaderElement.java,v
1.17 2003/03/08 21:30:02 olegk Exp $
 * $Revision: 1.17 $
 * $Date: 2003/03/08 21:30:02 $"
1,"replace invalid U+FFFF character during indexingIf the invalid U+FFFF character is embedded in a token, it actually causes indexing to silently corrupt the index by writing duplicate terms into the terms dict.  CheckIndex will catch the error, and merging will hit exceptions (I think).

We already replace invalid surrogate pairs with the replacement character U+FFFD, so I'll just do the same with U+FFFF."
0,"SPI: Testsuite for the SPI Interfacesnow that people start writing SPI implementations we should provide a test-suite that runs on the SPI directly in order to provide the developers a way to assert basic compliance of their implementation without having the JCR api in between.
"
0,"Use covariant clone() return types*Paul Cowan wrote in LUCENE-1257:*

OK, thought I'd jump in and help out here with one of my Java 5 favourites. Haven't seen anyone discuss this, and don't believe any of the patches address this, so thought I'd throw a patch out there (against SVN HEAD @ revision 827821) which uses Java 5 covariant return types for (almost) all of the Object#clone() implementations in core. 
i.e. this:

public Object clone() {
changes to:
public SpanNotQuery clone() {

which lets us get rid of a whole bunch of now-unnecessary casts, so e.g.

if (clone == null) clone = (SpanNotQuery) this.clone();
becomes
if (clone == null) clone = this.clone();

Almost everything has been done and all downcasts removed, in core, with the exception of

Some SpanQuery stuff, where it's assumed that it's safe to cast the clone() of a SpanQuery to a SpanQuery - this can't be made covariant without declaring ""abstract SpanQuery clone()"" in SpanQuery itself, which breaks those SpanQuerys that don't declare their own clone() 
Some IndexReaders, e.g. DirectoryReader - we can't be more specific than changing .clone() to return IndexReader, because it returns the result of IndexReader.clone(boolean). We could use covariant types for THAT, which would work fine, but that didn't follow the pattern of the others so that could be a later commit. 
Two changes were also made in contrib/, where not making the changes would have broken code by trying to widen IndexInput#clone() back out to returning Object, which is not permitted. contrib/ was otherwise left untouched.

Let me know what you think, or if you have any other questions."
0,"[PATCH] jackrabbit-webapp pom.xml patch to create an additional jar artifactModifies the jackrabbit-webapp pom.xml to create a jar artifact in addition to the existing war artifact, to allow the jackrabbit-webapp utility servlets to be reused in other modules.

The right way would be to create a separate jar module for the servlets (or move them to jackrabbit-jcr-commons?), and reuse that jar as a dependency in the jackrabbit-webapp. So I'm not sure if this patch deserves to be applied to the trunk, but it can be useful as a workaround before a cleaner solution is implemented.

See also http://mail-archives.apache.org/mod_mbox/jackrabbit-dev/200705.mbox/%3C510143ac0705151453t7a0eb4cam859a40fb106e81f5@mail.gmail.com%3E which discusses possible improvements to these jackrabbit-webapp utility servlets.

"
0,'ant javacc' in root project should also properly create contrib/surround Java filesFor consistency after LUCENE-1829 which did the same for contrib/queryparser
0,Add the new DataSource element to the repository DTDThe connection pooling feature from JCR-1456 introduced a new DataSource configuration element to Jackrabbit. It should be added to the repository config DTD.
0,"Add a way to locate full text extraction problemsFull text indexing of a binary document can fail for various reasons. Currently we just log a generic error message in such cases, which makes it difficult for the user to locate such problems for review and reindexing. We should improve this by making the logs more informative or by adding some other mechanism for locating troublesome documents."
1,"DataStore: garbage collection fails if a workspace is not initializedThe test case GCEventListenerTest fails with the following exception:

testEventListener(org.apache.jackrabbit.core.data.GCEventListenerTest)  Time elapsed: 10.235 sec  <<< ERROR!
java.lang.IllegalStateException: workspace 'test' not initialized
	at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.getPersistenceManager(RepositoryImpl.java:1703)
	at org.apache.jackrabbit.core.SessionImpl.createDataStoreGarbageCollector(SessionImpl.java:694)
	at org.apache.jackrabbit.core.data.GCEventListenerTest.doTestEventListener(GCEventListenerTest.java:75)
	at org.apache.jackrabbit.core.data.GCEventListenerTest.testEventListener(GCEventListenerTest.java:49)

"
0,"Change value for SearchIndex#DEFAULT_EXTRACTOR_BACK_LOGThe value is currently 100. This means that once 100 extractor jobs are pending in the indexing queue additional extractor jobs are executed with the current thread. I think it would be more useful to change this value to Integer.MAX_VALUE (or in other words: unbounded).

If the backlog is filled up then this indicates that the repository is very busy and we should not put additional burden on the current thread in that case."
0,fix for Document.getBoost() documentationThe attached patch fixes the javadoc to make clear that getBoost() will never return a useful value in most cases. I will commit this unless someone has a better wording or a real fix.
1,"QueryManager.createQuery() exception handlingQuery q = this.superuser.getWorkspace().getQueryManager()
                .createQuery(""SELECT * FROM nt:base"", Query.XPATH);

produces:
org.apache.jackrabbit.core.query.xpath.TokenMgrError: Lexical error at line 1, column 28.  Encountered: ""b"" (98), after : "":""
	at org.apache.jackrabbit.core.query.xpath.XPathTokenManager.getNextToken(XPathTokenManager.java:14546)
	at org.apache.jackrabbit.core.query.xpath.XPath.jj_ntk(XPath.java:9187)
	at org.apache.jackrabbit.core.query.xpath.XPath.PredicateList(XPath.java:5195)
	at org.apache.jackrabbit.core.query.xpath.XPath.AxisStep(XPath.java:4707)
	at org.apache.jackrabbit.core.query.xpath.XPath.StepExpr(XPath.java:4597)
	at org.apache.jackrabbit.core.query.xpath.XPath.RelativePathExpr(XPath.java:4511)
	at org.apache.jackrabbit.core.query.xpath.XPath.PathExpr(XPath.java:4482)
	at org.apache.jackrabbit.core.query.xpath.XPath.ValueExpr(XPath.java:4125)
	at org.apache.jackrabbit.core.query.xpath.XPath.UnaryExpr(XPath.java:4032)
	at org.apache.jackrabbit.core.query.xpath.XPath.CastExpr(XPath.java:3935)
	at org.apache.jackrabbit.core.query.xpath.XPath.CastableExpr(XPath.java:3898)
	at org.apache.jackrabbit.core.query.xpath.XPath.TreatExpr(XPath.java:3861)
	at org.apache.jackrabbit.core.query.xpath.XPath.InstanceofExpr(XPath.java:3824)
	at org.apache.jackrabbit.core.query.xpath.XPath.IntersectExceptExpr(XPath.java:3748)
	at org.apache.jackrabbit.core.query.xpath.XPath.UnionExpr(XPath.java:3672)
	at org.apache.jackrabbit.core.query.xpath.XPath.MultiplicativeExpr(XPath.java:3622)
	at org.apache.jackrabbit.core.query.xpath.XPath.AdditiveExpr(XPath.java:3510)
	at org.apache.jackrabbit.core.query.xpath.XPath.RangeExpr(XPath.java:3451)
	at org.apache.jackrabbit.core.query.xpath.XPath.ComparisonExpr(XPath.java:3353)
	at org.apache.jackrabbit.core.query.xpath.XPath.AndExpr(XPath.java:3290)
	at org.apache.jackrabbit.core.query.xpath.XPath.OrExpr(XPath.java:3227)
	at org.apache.jackrabbit.core.query.xpath.XPath.ExprSingle(XPath.java:2214)
	at org.apache.jackrabbit.core.query.xpath.XPath.ForClause(XPath.java:2337)
	at org.apache.jackrabbit.core.query.xpath.XPath.FLWORExpr(XPath.java:2233)
	at org.apache.jackrabbit.core.query.xpath.XPath.ExprSingle(XPath.java:2133)
	at org.apache.jackrabbit.core.query.xpath.XPath.Expr(XPath.java:2094)
	at org.apache.jackrabbit.core.query.xpath.XPath.QueryBody(XPath.java:2066)
	at org.apache.jackrabbit.core.query.xpath.XPath.MainModule(XPath.java:512)
	at org.apache.jackrabbit.core.query.xpath.XPath.Module(XPath.java:387)
	at org.apache.jackrabbit.core.query.xpath.XPath.QueryList(XPath.java:151)
	at org.apache.jackrabbit.core.query.xpath.XPath.XPath2(XPath.java:118)
	at org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder.<init>(XPathQueryBuilder.java:224)
	at org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder.createQuery(XPathQueryBuilder.java:255)
	at org.apache.jackrabbit.core.query.QueryParser.parse(QueryParser.java:57)
	at org.apache.jackrabbit.core.query.lucene.QueryImpl.<init>(QueryImpl.java:119)
	at org.apache.jackrabbit.core.query.lucene.SearchIndex.createExecutableQuery(SearchIndex.java:158)
	at org.apache.jackrabbit.core.query.QueryImpl.<init>(QueryImpl.java:90)
	at org.apache.jackrabbit.core.SearchManager.createQuery(SearchManager.java:192)
	at org.apache.jackrabbit.core.query.QueryManagerImpl.createQuery(QueryManagerImpl.java:87)
	at org.apache.jackrabbit.test.api.query.IllegalXPathTest.testIllegalStatement(IllegalXPathTest.java:45)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:324)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at org.apache.jackrabbit.test.AbstractJCRTest.run(AbstractJCRTest.java:401)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:474)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:342)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:194)
"
1,"rep:similar in xpath does not workThis query //*[rep:similar(., '/content/en')] produces an exception:

24.09.2009 16:56:48.156 *ERROR* [0:0:0:0:0:0:0:1%0 [1253804208093] GET /libs/cq/search/content/querydebug.html HTTP/1.1] org.apache.sling.engine.impl.SlingMainServlet service: Uncaught SlingException java.lang.ArrayIndexOutOfBoundsException: -1
	at org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder.visit(LuceneQueryBuilder.java:612)
	at org.apache.jackrabbit.spi.commons.query.RelationQueryNode.accept(RelationQueryNode.java:115)
	at org.apache.jackrabbit.spi.commons.query.NAryQueryNode.acceptOperands(NAryQueryNode.java:143)
	at org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder.visit(LuceneQueryBuilder.java:489)
	at org.apache.jackrabbit.spi.commons.query.LocationStepQueryNode.accept(LocationStepQueryNode.java:166)
	at org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder.visit(LuceneQueryBuilder.java:468)
	at org.apache.jackrabbit.spi.commons.query.PathQueryNode.accept(PathQueryNode.java:74)
	at org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder.visit(LuceneQueryBuilder.java:257)
	at org.apache.jackrabbit.spi.commons.query.QueryRootNode.accept(QueryRootNode.java:115)
	at org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder.createLuceneQuery(LuceneQueryBuilder.java:247)
	at org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder.createQuery(LuceneQueryBuilder.java:227)
	at org.apache.jackrabbit.core.query.lucene.QueryImpl.execute(QueryImpl.java:111)
	at org.apache.jackrabbit.core.query.QueryImpl.execute(QueryImpl.java:127)
"
0,"Add Highlighting benchmark support to contrib/benchmarkI would like to be able to test the performance (speed, initially) of the Highlighter in a standard way.  Patch to follow that adds the Highlighter as a dependency benchmark and adds in tasks extending the ReadTask to perform highlighting on retrieved documents."
0,Remove excessive dependencies from jcr-client module
0,"Lock-less commitsThis is a patch based on discussion a while back on lucene-dev:

    http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200608.mbox/%3c44E5B16D.4010805@mikemccandless.com%3e

The approach is a small modification over the original discussion (see
Retry Logic below).  It works correctly in all my cross-machine test
case, but I want to open it up for feedback, testing by
users/developers in more diverse environments, etc.

This is a small change to how lucene stores its index that enables
elimination of the commit lock entirely.  The write lock still
remains.

Of the two, the commit lock has been more troublesome for users since
it typically serves an active role in production.  Whereas the write
lock is usually more of a design check to make sure you only have one
writer against the index at a time.

The basic idea is that filenames are never reused (""write once""),
meaning, a writer never writes to a file that a reader may be reading
(there is one exception: the segments.gen file; see ""RETRY LOGIC""
below).  Instead it writes to generational files, ie, segments_1, then
segments_2, etc.  Besides the segments file, the .del files and norm
files (.sX suffix) are also now generational.  A generation is stored
as an ""_N"" suffix before the file extension (eg, _p_4.s0 is the
separate norms file for segment ""p"", generation 4).

One important benefit of this is it avoids files contents caching
entirely (the likely cause of errors when readers open an index
mounted on NFS) since the file is always a new file.

With this patch I can reliably instantiate readers over NFS when a
writer is writing to the index.  However, with NFS, you are still forced to
refresh your reader once a writer has committed because ""point in
time"" searching doesn't work over NFS (see LUCENE-673 ).

The changes are fully backwards compatible: you can open an old index
for searching, or to add/delete docs, etc.  I've added a new unit test
to test these cases.

All units test pass, and I've added a number of additional unit tests,
some of which fail on WIN32 in the current lucene but pass with this
patch.  The ""fileformats.xml"" has been updated to describe the changes
to the files (but XXX references need to be fixed before committing).

There are some other important benefits:

  * Readers are now entirely read-only.

  * Readers no longer block one another (false contention) on
    initialization.

  * On hitting contention, we immediately retry instead of a fixed
    (default 1.0 second now) pause.

  * No file renaming is ever done.  File renaming has caused sneaky
    access denied errors on WIN32 (see LUCENE-665 ).  (Yonik, I used
    your approach here to not rename the segments_N file(try
    segments_(N-1) on hitting IOException on segments_N): the separate
    "".done"" file did not work reliably under very high stress testing
    when a directory listing was not ""point in time"").

  * On WIN32, you can now call IndexReader.setNorm() even if other
    readers have the index open (fixes a pre-existing minor bug in
    Lucene).

  * On WIN32, You can now create an IndexWriter with create=true even
    if readers have the index open (eg see
    www.gossamer-threads.com/lists/lucene/java-user/39265) .


Here's an overview of the changes:

  * Every commit writes to the next segments_(N+1).

  * Loading the segments_N file (& opening the segments) now requires
    retry logic.  I've captured this logic into a new static class:
    SegmentInfos.FindSegmentsFile.  All places that need to do
    something on the current segments file now use this class.

  * No more deletable file.  Instead, the writer computes what's
    deletable on instantiation and updates this in memory whenever
    files can be deleted (ie, when it commits).  Created a common
    class index.IndexFileDeleter shared by reader & writer, to manage
    deletes.

  * Storing more information into segments info file: whether it has
    separate deletes (and which generation), whether it has separate
    norms, per field (and which generation), whether it's compound or
    not.  This is instead of relying on IO operations (file exists
    calls).  Note that this fixes the current misleading
    FileNotFoundException users now see when an _X.cfs file is missing
    (eg http://www.nabble.com/FileNotFound-Exception-t6987.html).

  * Fixed some small things about RAMDirectory that were not
    filesystem-like (eg opening a non-existent IndexInput failed to
    raise IOException; renames were not atomic).  I added a stress
    test against a RAMDirectory (1 writer thread & 2 reader threads)
    that uncovered these.

  * Added option to not remove old files when create=true on creating
    FSDirectory; this is so the writer can do its own [more
    sophisticated because it retries on errors] removal.

  * Removed all references to commit lock, COMMIT_LOCK_TIMEOUT, etc.
    (This is an API change).

  * Extended index/IndexFileNames.java and index/IndexFileNameFilter.java
    with logic for computing generational file names.

  * Changed index/IndexFileNameFilter.java to use a HashSet to check
    file extentsions for better performance.

  * Fixed the test case TestIndexReader.testLastModified: it was
    incorrectly (I think?) comparing lastModified to version, of the
    index.  I fixed that and then added a new test case for version.


Retry Logic (in index/SegmentInfos.java)

If a reader tries to load the segments just as a writer is committing,
it may hit an IOException.  This is just normal contention.  In
current Lucene contention causes a [default] 1.0 second pause then
retry.  With lock-less the contention causes no added delay beyond the
time to retry.

When this happens, we first try segments_(N-1) if present, because it
could be segments_N is still being written.  If that fails, we
re-check to see if there is now a newer segments_M where M > N and
advance if so.  Else we retry segments_N once more (since it could be
it was in process previously but must now be complete since
segments_(N-1) did not load).

In order to find the current segments_N file, I list the directory and
take the biggest segments_N that exists.

However, under extreme stress testing (5 threads just opening &
closing readers over and over), on one platform (OS X) I found that
the directory listing can be incorrect (stale) by up to 1.0 seconds.
This means the listing will show a segments_N file but that file does
not exist (fileExists() returns false).

In order to handle this (and other such platforms), I switched to a
hybrid approach (originally proposed by Doron Cohen in the original
thread): on committing, the writer writes to a file ""segments.gen"" the
generation it just committed.  It writes 2 identical longs into this
file.  The retry logic, on detecting that the directory listing is
stale falls back to the contents of this file.  If that file is
consistent (the two longs are identical), and, the generation is
indeed newer than the dir listing, it will use that.

Finally, if this approach is also stale, we fallback to stepping
through sequential generations (up to a maximum # tries).  If all 3
methods fail, we throw the original exception we hit.

I added a static method SegmentInfos.setInfoStream() which will print
details of retry attempts.  In the patch it's set to System.out right
now (we should turn off before a real commit) so if there are problems
we can see what retry logic had done.
"
0,"CustomScoreQuery (function query) is broken (due to per-segment searching)Spinoff from here:

  http://lucene.markmail.org/message/psw2m3adzibaixbq

With the cutover to per-segment searching, CustomScoreQuery is not really usable anymore, because the per-doc custom scoring method (customScore) receives a per-segment docID, yet there is no way to figure out which segment you are currently searching.

I think to fix this we must also notify the subclass whenever a new segment is switched to.  I think if we copy Collector.setNextReader, that would be sufficient.  It would by default do nothing in CustomScoreQuery, but a subclass could override."
1,"BooleanQuery explain with boost==0BooleanWeight.explain() uses the returned score of subweights to determine if a clause matched.
If any required clause has boost==0, the returned score will be zero and the explain for the entire BooleanWeight will be simply  Explanation(0.0f, ""match required"").

I'm not sure what the correct fix is here.  I don't think it can be done based on score alone, since that isn't how scorers work.   Perhaps we need a new method ""boolean Explain.matched()"" that returns true on a match, regardless of what the score may be? 

Related to the problem above, even if no boosts are zero, it it sometimes nice to know *why* a particular query failed to match.  It would mean a longer explanation, but maybe we should include non matching explains too?"
1,"Brazilian Analyzer doesn't remove stopwords when uppercase is givenThe order of filters matter here, just need to apply lowercase token filter before removing stopwords

	result = new StopFilter( result, stoptable );
		result = new BrazilianStemFilter( result, excltable );
		// Convert to lowercase after stemming!
		result = new LowerCaseFilter( result );

Lowercase must come before BrazilianStemFilter

At the end of day I'll attach a patch, it's straightforward"
1,Benchmark deletes.alg failsBenchmark deletes.alg fails because the index reader defaults to open readonly.  
1,"Lucene fails to close file handles under certain situationsAs a followon to LUCENE-820, I've added a further check in
MockRAMDirectory to assert that there are no open files when the
directory is closed.

That check caused a few unit tests to fail, and in digging into the
reason I uncovered these cases where Lucene fails to close file
handles:

  * TermInfosReader.close() was setting its ThreadLocal enumerators to
    null without first closing the SegmentTermEnum in there.  It looks
    like this was part of the fix for LUCENE-436.  I just added the
    call to close.

    This is somewhat severe since we could leak many file handles for
    use cases that burn through threads and/or indexes.  Though,
    FSIndexInput does have a finalize() to close itself.

  * Flushing of deletes in IndexWriter opens SegmentReader to do the
    flushing, and it correctly calls close() to close the reader.  But
    if an exception is hit during commit and before actually closing,
    it will leave open those handles.  I fixed this first calling
    doCommit() and then doClose() in a finally.  The ""disk full"" tests
    we now have were hitting this.

  * IndexWriter's addIndexes(IndexReader[]) method was opening a
    reader but not closing it with a try/finally.  I just put a
    try/finally in.

I've also changed some unit tests to use MockRAMDirectory instead of
RAMDirectory to increase testing coverage of ""leaking open file
handles"".
"
0,"OCM:Add the ability to specify name of a Collection Element through XML Mapping files.Collection elements get mapped to a node ""collection-element"" when the mappings are specified through XML config files.  We need the ability to control this name through configuration.  Without that feature querying object structures is painful.  For example I have structure as below :

class Foo{
String id;
 List<Foo> children
 List<Foo> friends
}

And I have a need to query a Foo with id : 100 .  If I am interested only in child nodes with id = 110 , I could specify through the Filter that look at only node names , ""childFoo"" ; If I have the flexibility of adding a child node name."
1,"Unreferenced sessions should get garbage collectedIf an application opens many sessions and doesn't close them, they are never garbage collected. After some time, the virtual machine will run out of memory. This code will run out of memory after a few thousand logins:

Repository rep = new TransientRepository();
for (int i = 0; ; i++) {
  rep.login(new SimpleCredentials("""", new char[0]));
}

Using a finalizer to close SessionImpl doesn't work, because it seems there are references from the (hard referenced part of the cache) to the SessionImpl objects. Maybe it is possible to remove those references, or change them to weak references.
"
1,"IndexReader.termDocs() retrieves no documentsTermDocs object returned by indexReader.termDocs() retrieves no documents, howerver, the documents are retrieved correctly when using indexReader.termDocs(Term), indexReader.termDocs(null) and indexSearcher.search(Query)."
0,Change contrib tests to use the special LuceneTestCase(J4) constant for the current version used a matchVersion parameterSub issue for contrib changes
0,New MsOutlook Message ExtractorSinse we are using poi 3.0.2 it will be useful to have a outlook message extractor
1,CompactNodeTypeDefWriter does not escaped names properlyCompactNodeTypeDefWriter does not escaped names properly. If the name includes a '-' or a '+' the names must be surrounded by single quotes.
0,HttpClient 'ParamBeans' for easier configurationAs I did for a 'core' here I would like to contribute for 'client' part as few 'ParamBeans' for easier external configuration... Any comment or improvement is very welcome...
0,"user need a way to control cookie policyUser need a way to control what cookie can be accepted and what should be 
rejected. It would be nice to provide a cookie filter interface, so the user 
can change the cookie policy by implementing his own filter."
1,"CartesianPolyFilterBuilder doesn't handle edge case around the 180 meridianTest case:  
Points all around the globe, plus two points at 0, 179.9 and 0,-179.9 (on each side of the meridian).  Then, do a Cartesian Tier filter on a point right near those two.  It will return all the points when it should just return those two.

The flawed logic is in the else clause below:
{code}
if (longX2 != 0.0) {
		//We are around the prime meridian
		if (longX == 0.0) {
			longX = longX2;
			longY = 0.0;
        	shape = getShapeLoop(shape,ctp,latX,longX,latY,longY);
		} else {//we are around the 180th longitude
			longX = longX2;
			longY = -180.0;
			shape = getShapeLoop(shape,ctp,latY,longY,latX,longX);
	}
{code}

Basically, the Y and X values are transposed.  This currently says go from longY (-180) all the way around  to longX which is the lower left longitude of the box formed.  Instead, it should go from the lower left long to -180."
1,"Escaped wildcard character in wildcard term not handled correctlyIf an escaped wildcard character is specified in a wildcard query, it is treated as a wildcard instead of a literal.
e.g., t\??t is converted by the QueryParser to t??t - the escape character is discarded."
0,"Support multiple proxiesHttpClient supports one proxy currently.
Our requirement is to suppport more than one proxy. We may need to connect more than one proxies before connects to target resource. 
I found that HttpMethodDirector creates tunnelled socket and there is no easy way to plugin our custom HttpMethodDirector class with HttpClient other than extending HttpClient to override ""public int executeMethod(HostConfiguration hostconfig, final HttpMethod method, final HttpState state"" method.


"
0,Remove deprecated DocIdSetIterator methods
0,"Introduce similarity functionThe query handler should support a similarity function that allows one to find nodes that are similar to a given existing one.

Example:

//*[rep:similar(""/foo/bar"")]

Finds nodes that are similar to node /foo/bar."
0,"SimpleWebdavServlet: avoid 404 for the root collectionin order to avoid strange 404 error when accessing the root collection in simple webdav servlet (thus missing workspace name), that request should either be redirected or handled by a create fake root that has no correspondance in the jsr170 repository."
0,Avoid using BitSets in ChildAxisQuery to minimize memory usageWhen doing ChildAxisQueries on large indexes the internal BitSet instance (hits) may consume a lot of memory because the BitSet is always as large as IndexReader.maxDoc(). In our case we had a query consisting of 7 ChildAxisQueries which combined to a total of 14MB. Since we have multiple users executing this query simultaneously this caused an out of memory error.
0,"Not getting random-seed/reproduce-with if a test fails from another threadSee https://builds.apache.org/job/Lucene-Solr-tests-only-trunk/12822/console as an example.

This is at least affecting 4.0, maybe 3.x too"
0,"move intblock/sep codecs into testThe intblock and sep codecs in core exist to make it easy for people to try different low-level algos for encoding ints.

Sep breaks docs, freqs, pos, skip data, payloads into 5 separate files (vs 2 files that standard codec uses).

Intblock further enables the docs, freqs, pos files to encode fixed-sized blocks of ints at a time.

So an app can easily ""subclass"" these codecs, using their own int encoder.

But these codecs are now concrete, and they use dummy low-level block int encoder (eg encoding 128 ints as separate vints).

I'd like to change these to be abstract, and move these dummy codecs into test.

The tests would still test these dummy codecs, by rotating them in randomly for all tests.

I'd also like to rename IntBlock -> FixedIntBlock, because I'm trying to get a VariableIntBlock working well (for int encoders like Simple9, Simple16, whose block size varies depending on the particular values).
"
0,"More Fine grained Permission FlagsIt would be fine to have one more Permission Flag on node add.
At the moment there are 3 flags. We need to know if a node will be updated or created.
This is not possible with the current implementation because on node add the permission flag 
AccessManager.WRITE will be used. This is a Problem in a  WebDav Scenario with Microsoft-Word because if i open a Node and 
try to save it i need write permissions on the parent node. this is ok. If a user trys to save the file with a other name
he can because the same PermissionFlag will be used.
Maybe there is a other solution for this problem ?
BR,
claus"
0,"Change Log-Level in DefaultIOListenerPlease change loglevel for method onEnd(IOHandler handler, IOContext ioContext, boolean success) to debug"
1,Index recovery may fail when redo log contains nodes that are part of an index aggregateSearchIndex.mergeAggregatedNodeIndexes() will throw a NullPointerException because index is not yet set. The call is made from the recovery code that is triggered in the MultiIndex constructor.
1,"LockTest.testLogout fails to refresh session before checking lock from other sessionLockTest.testLogout() fails to refresh the session before checking the lock state of a node that was locked by another session.

Proposal:

Insert 

  n1.refresh(false);

before 

  assertTrue(""node must be locked"", n1.isLocked());

"
0,Remove System.out left in SpanHighlighter codeA System.out debug was left in the code when a Query is not supported by the SpanHighlighter. This issue simply removes it.
1,"JCAResourceAdapter must implement SerializableWe are running Weblogic 10.0 servers in cluster environment.   When deploying the rar, we always get this warning from weblogic stdout.log: 

<Jan 15, 2009 2:42:10 AM PST> <Warning> <Connector> <BEA-190155> <Compliance checking/validation of the resource adapter /home/user/jackrabbit_rar/jackrabbit-jca-1.5.0.rar resulted in the following warnings:  The ra.xml <resourceadapter-class> class 'org.apache.jackrabbit.jca.JCAResourceAdapter' should implement java.io.Serializable but does not.> 

When trying to do the JNDI lookup the repository, we got the error ""No Object found: jackrabbit|null"".   The jackrabbit entry in the jndi tree is visible only as a javax.naming.reference and not as the JCARepositoryHandle due to the above warning.  Due to that, we can't deploy jackrabbit-jca in Test/Production environment.  

I'm no expert in JCA, but feel it is fairly easy to implement Serializable for  JCAResourceAdapter.  Please help us out.
"
0,"spi2davex: clear uri-lookup after removing node identified with uniqueIDsome test cases of DocumentViewImportTest fail in the setup (line 325 of AbstractImportXmlTest) since the uri resolved from the specified
nodeID still refers to the node removed during the initial import before. this would equally cause problems whenever a referenceable node was
replaced by another node and can easily be fixed by clearing the uri-cache after the removal as it was already done for move."
1,"Inflater.end() method not always called in FieldsReader
We've just found an insidious memory leak in our own application as we did not always call Deflater.end() and Inflater.end(). As documented here;

http://bugs.sun.com/view_bug.do?bug_id=4797189

The non-heap memory that the native zlib code uses is not freed in a timely manner.

FieldsWriter appears safe as no exception can be thrown between the Deflater's creation and end() as it uses a ByteArrayOutputStream

FieldsReader, however, is not safe. In the event of a DataFormatException the call to end() will not occur."
1,"Clustering: re-registration of nodetypes is not  synchronizedThe re-registration of nodetypes is not yet synchronized between clusternodes, although re-registration is already (partially) implemented in the NodeTypeRegistry."
1,"MS Excel Mime Type missing in MsExcelTextExtractor The MsExcelTextExtractor listens to mime type ""application/vnd.ms-excel"", but storing excels will result in mime type ""application/msexcel"", too. Such tagged files will not be indexed by the MsExcelTextExtractor. The class should register itself to both mime types like the MsWordTextExtractor does. "
0,"reopen support for SegmentReaderReopen for SegmentReader can be supported simply as the following:

  @Override
  public synchronized IndexReader reopen() throws CorruptIndexException,
		IOException {
	return reopenSegment(this.si,false,readOnly);
  }

  @Override
  public synchronized IndexReader reopen(boolean openReadOnly)
		throws CorruptIndexException, IOException {
	return reopenSegment(this.si,false,openReadOnly);
  }
"
1,"bogus positions create a corrumpt indexIts pretty common that positionIncrement can overflow, this happens really easily 
if people write analyzers that don't clearAttributes().

It used to be the case that if this happened (and perhaps still is in 3.x, i didnt check),
that IW would throw an exception.

But i couldnt find the code checking this, I wrote a test and it makes a corrumpt index..."
0,"loadURI compile error with Maven 1.0.2As reported on the mailing list by Ashley Martens:

----
C:\apache\jackrabbit-contrib\nt-ns-util>maven
 __  __
|  \/  |__ _Apache__ ___
| |\/| / _` \ V / -_) ' \  ~ intelligent projects ~
|_|  |_\__,_|\_/\___|_||_|  v. 1.0.2

Attempting to download jackrabbit-1.0-SNAPSHOT.jar.
Artifact /org.apache.jackrabbit/jars/jackrabbit-1.0-SNAPSHOT.jar doesn't exists in remote repository, but it exists locally
Attempting to download jackrabbit-commons-1.0-SNAPSHOT.jar.
Artifact /org.apache.jackrabbit/jars/jackrabbit-commons-1.0-SNAPSHOT.jar doesn't exists in remote repository, but it exists locally
build:start:

java:prepare-filesystem:

java:compile:
   [echo] Compiling to C:\apache\jackrabbit-contrib\nt-ns-util/target/classes
   [javac] Compiling 1 source file to C:\apache\jackrabbit-contrib\nt-ns-util\target\classes
C:\apache\jackrabbit-contrib\nt-ns-util\src\main\java\org\apache\jackrabbit\util\nodetype\SchemaConverter.java:71: cannot resolve symbol
symbol  : method loadURI (java.lang.String)
location: class org.apache.xerces.impl.xs.XMLSchemaLoader
       XSModel xsModel = loader.loadURI(uri);
                               ^
1 error

BUILD FAILED
File...... C:\Documents and Settings\ashleym\.maven\cache\maven-java-plugin-1.5\plugin.jelly
Element... ant:javac
Line...... 63
Column.... 48
Compile failed; see the compiler error output for details.
Total time: 8 seconds
Finished at: Mon Jan 02 10:40:47 EST 2006
----

Peeter Piegaze found out the problem:

----
I was able to build it without a problem using maven-1.1-beta-2 and JDK 1.4.2.

However, it sounds to me like in your case maven has set up its
on-build classpath so that it sees the older xerces-2.4.0.jar before
the new xerxesImpl.-2.6.2.jar. Maven seems to download the old
xerces-2.4.0 into its repository for internal use, while my code uses
the newer xerxesImpl-2.6.2.jar. The old jar overlaps class-wise with
the new one, but the new one implements the additional loadURI method
(among others).

I am not sure exactly why your maven build process is looking in the
wrong jar. But that is what is doing, almost certainly.
----
"
0,"Add a MBean method to programatically create a new Workspace.Would be useful to have a mbean method to create a new workspace to use if with a jmx console.

"
1,"RFC4918IfHeaderTest.testPutIfLockToken could fail with 412 Precondition FailedIn org.apache.jackrabbit.webdav.server.RFC4918IfHeaderTest:110 (webdav-test), 
the lock request is initialized with a timeout of 1800 milliseconds, which is rounded as Timeout: Second-1 (at org.apache.jackrabbit.webdav.header.TimeoutHeader:46).

The assertion in the finally block MUST fail (412, Precondition Failed) if the lock has expired (cf. RFC 4918, Section 10.4.10).

The lock request should be initialized with a higher timeout, at least several seconds."
0,"New Gump projects for HttpComponents 4.0Create new Gump definitions for the 4.0 code base, both core and client.
There are other Maven-based projects in Gump to learn from, for example Apollo and Excalibur.

"
1,"More Locale problems in LuceneThis is a followup to LUCENE-1836: I found some more Locale problems in Lucene with Date Formats. Even for simple date formats only consisting of numbers (like ISO dates), you should always give the US locale. Because the dates in DateTools should sort according to String.compare(), it is important, that the decimal digits are western ones. In some strange locales, this may be different. Whenever you want to format dates for internal formats you exspect to behave somehow, you should at least set the locale to US, which uses ASCII. Dates entered by users and displayed to users, should be formatted according to the default or a custom specified locale.
I also looked for DecimalFormat (especially used for padding numbers), but found no problems."
0,"FormBodyPart code does not agree with ContentDescriptor Javadoc wrt nullability of mimeType and transferEncodingThe FormBodyPart does not agree with ContentDescriptor Javadoc wrt nullability of mimeType and transferEncoding:

The code in FormBodyPart explicitly allows mimeType and transferEncoding to be null, in which case the relevant header is not generated.
This is useful behaviour, as the headers are not necessaruly needed.

However the bahaviour disagrees with the Javadoc in the ContentDescriptor interface - null is not allowed.
Also, AbstractContentBody does not allow mime-type to be null."
0,"Enable setting the terms index divisor used by IndexWriter whenever it opens internal readersOpening a place holder issue... if all the refactoring being discussed don't make this possible, then we should add a setting to IWC to do so.

Apps with very large numbers of unique terms must set the terms index divisor to control RAM usage.

(NOTE: flex's RAM terms dict index RAM usage is more efficient, so this will help such apps).

But, when IW resolves deletes internally it always uses default 1 terms index divisor, and the app cannot change that.  Though one workaround is to call getReader(termInfosIndexDivisor) which will pool the reader with the right divisor."
0,"Get javadoc for the similarities package in shape1. Create a package.html in the similarities package.
2. Update the javadoc of the search package (package.html mentions Similarity)?
3. Compile the javadoc to see if there are any warnings."
0,"httpclient build requires jdk 1.4 or jce in classpathCurrently when a 'ant dist'
is performed httpclient is looking for javax.crypt.* which is in jce.jar

The build.xml and build.properties.sample need to be patched
so they allow the jce.jar file to be specified
just like the jsse.jar is specified.

will attach two patch files made from todays cvs"
0,"Review pck names in the others ocm subprojectsReview package structure and graffito references in the other OCM subprojects : jcr-nodemanagement & spring. 
"
0,"improve termquery ""pk lookup"" performanceFor things that are like primary keys and don't exist in some segments (worst case is primary/unique key that only exists in 1)
we do wasted seeks.

While LUCENE-2694 tries to solve some of this issue with TermState, I'm concerned we could every backport that to 3.1 for example.

This is a simpler solution here just to solve this one problem in termquery... we could just revert it in trunk when we resolve LUCENE-2694,
but I don't think we should leave things as they are in 3.x
"
0,"Update overview example codeSee http://lucene.apache.org/java/2_4_1/api/core/overview-summary.html - need to update for non-deprecated best-practices/recommended API usage.

Also, double-check that the demo app works as documented."
1,BoostingNearQuery doesn't have hashCode/equals
1,"preflex codec returns wrong terms if you use an empty field namespinoff from LUCENE-3473.

I have a standalone test for this... the termsenum is returning a bogus extra empty-term (I assume it has no postings, i didnt try).

This causes the checkindex test in LUCENE-3473 to fail, because there are 4 terms instead of 3. 

"
0,"smartcn HHMM doc translationMy coworker Patricia Peng translated the documentation and code comments for smartcn HHMM package.
"
1,"Oracle JDBC Class Cast ExceptionWhen utilizing the OraclePersistenceManager (package org.apache.jackrabbit.core.persistence.db) (I realize this is marked as deprecated) we noticed during our migration from Jackrabbit 1.6.1 to 2.2.10/11 that when starting the application server an error message is displayed to us that indicates that the Connection object passed to the createTemporaryBlob method of the BLOB class can't be cast to oracle.jdbc.OracleConnection

Here the interesting lines from our log:
2012-03-15 17:15:47,926 ERROR [org.apache.jackrabbit.core.persistence.db.OraclePersistenceManager] failed to write node state: cafebabe-cafe-babe-cafe-babecafebabe
java.lang.ClassCastException: org.apache.commons.dbcp.PoolingDataSource$PoolGuardConnectionWrapper cannot be cast to oracle.jdbc.OracleConnection
	at oracle.sql.BLOB.createTemporary(BLOB.java:708)
	at org.apache.jackrabbit.core.persistence.db.OraclePersistenceManager.createTemporaryBlob(OraclePersistenceManager.java:375)

I want to highlight at this point that the do not see the issue when using the Oracle Bundled persistence manager, however due to the fact that we haven't used the bundled version in the past we have a lot of customers with repo layouts that can not be used by the bundled persistence manager - we ran some tests and noticed that the consistency check fails.
-> At the moment there is no good upgrade path to move a repo to the bundled structure, the paths provided thus far are shaky at best.

I did find a solution to the problem that has shown no issues thus far and wanted to share this with you:

It is a one line change that can be made before the wrapped connection is passed to the Oracle driver:
org.apache.jackrabbit.core.util.db.ConnectionFactory.unwrap(con);

This then solves the problem, I also wanted to share that we are using an XA datasource."
0,jcr:like on node nameUntil now it is only possible to do an exact match on the node name. It would be useful to also use jcr:like on the node name.
1,"Paths not correct after reordering childrenReordered, unsaved children of a node do not have the correct path. In the test case attached, the following operation is attempted with three SNS children named b[1], b[2], b[3]: the last element is ordered before the first three times, which should result in the initial children order.
"
0,HttpState should have methods for clearing all cookies and credentials 
1,"The repeats mechanism in SloppyPhraseScorer is broken when doc has tokens at same positionIn LUCENE-736 we made fixes to SloppyPhraseScorer, because it was
matching docs that it shouldn't; but I think those changes caused it
to fail to match docs that it should, specifically when the doc itself
has tokens at the same position.
"
0,"allow ResourceType dav property to have multiple valuesattached is a patch that allows the ResourceType dav property to have multiple values (useful for dav protocol extensions such as caldav). 

it is not a perfect patch, in that subclasses of ResourceType do not know about each others' resource types, but it is a decent start. one way to address this issue might be to have subclasses register extended resource types and their corresponding xml representations with ResourceType, removing the need for them to override resourceTypeToXml() and isValidResourceType().
"
0,"Default merge policy should take deletions into accountLUCENE-1634 added a calibrateSizeByDeletes; we had a TODO to default this to true as of 3.0 but we missed it.  I'll fix it now for 3.1 and 4.0.  While this is technically a change in back-compat (for 3.x), I think it's fine to make an exception here; this should be a big win for indices that have high doc turnover with time."
0,"Failures during contrib builds, when classes in core were changed without ant cleanFrom java-dev by Shai Erera:

{quote}
I've noticed that sometimes, after I run test-core and test-contrib, and then change core code, test-contrib fail on NoSuchMethodError and stuff like that. I've noticed that core.jar exists under build, and I assumed it's used by test-contrib, and probably is not recreated after core code has changed.

I verified it when looking in contrib-build.xml, which defines a property lucene.jar.present which is set to true if the jar is ... well, present. Which I believe is the reason for these failures. I've been thinking how to resolve that, and I can think of two ways:

(1) have test-core always delete that file, but that has two issues:
(1.1) It's redundant if the code hasn't changed.
(1.2) It forces you to either jar-core or test-core before you test-contrib, if you want to make sure you run w/ the latest jar.

or

(2) have test-contrib always call jar-core, which will first delete the file and then re-create it by compiling first. Compiling should not do anything if the code hasn't changed. So the only waste would be to create the .jar, but I think that's quite fast?

Does anyone, with more Ant skills than me, know of a better way to detect from test-contrib that core code has changed and only then rebuild the jar?
{quote}"
0,Only load root node definition when requiredThe root node definition is currently loaded whenever a session logs in.
0,"Change project names to start with jackrabbitAll the released projects should have artifactId's starting with ""jackrabbit""."
0,"Deploy JCA JAR file to maven repositoryPlease deploy the JCA JAR file to the maven repository (ibiblio) whenever deploying the RAR artifact.

The JAR is need for non managed usage of the Jackrabbit JCA, eg. for embedding the resource adapter in your application with Spring JCA in order to use XA for Jackrabbit.

It would be nice if this could be done starting at the current 1.3 version (and for future versions, too).

Thanks!"
1,"in trunk if you switch up omitNorms while indexing, you get a corrumpt norms filedocument 1 has 
  body: norms=true
  title: norms=true
document 2 has 
  body: norms=false
  title: norms=true

when seeing 'body' for the first time, normswriterperfield gets 'initial fieldinfo' and 
saves it away, which says norms=true

however, at flush time we dont check, so we write the norms happily anyway.
then SegmentReader reads the norms later: it skips ""body"" since it omits norms
and if you ask for the norms of 'title' it instead returns the bogus ""body"" norms.

asserting that SegmentReader ""plans to"" read the whole .nrm file exposes the bug."
1,"DefaultMethodRetryHandler bugDefaultMethodRetryHandler does not seem to test correctly for the number of
attempts to retry a given method. It seems to bail out one attempt too early:

if (executionCount >= this.retryCount) {
  // Do not retry if over max retry count
  return false;
}

For example, if I set the retryCount to 1, HttpClient does not retry the method
at all. At least that's what I'm seeing when I step through it with a debugger."
0,Reorganize Jackrabbit into 'core' 'api' and 'commons'
1,"ManageableCollectionUtil doesn't support MapsManageableCollectionUtil has two getManageableCollection methods, which do not currently return a ManageableCollection which wraps Maps. 

ManagedHashMap already exists in the codebase which I assume was created for this purpose, so both getManageableCollection methods could be modified so that they do something like:

            if (object instanceof Map){
                return new ManagedHashMap((Map)object);
            }


An alternative solution might be to modify the JCR mapping to support explicitly defining the 'ManagedXXX' class."
0,"Errors in character entities in Javadoc for HttpVersionThere are some errors in the Javadoc for the HttpVersion class. This is the
class comment:

 *  <p>HTTP version, as specified in RFC 2616.</p>
 *  <p>
 *  HTTP uses a ""&ltmajor&gt.&ltminor&gt"" numbering scheme to indicate versions
 *  of the protocol. The protocol versioning policy is intended to allow
 *  the sender to indicate the format of a message and its capacity for
 *  understanding further HTTP communication, rather than the features
 *  obtained via that communication. No change is made to the version
 *  number for the addition of message components which do not affect
 *  communication behavior or which only add to extensible field values.
 *  The &ltminor&gt number is incremented when the changes made to the
 *  protocol add features which do not change the general message parsing
 *  algorithm, but which may add to the message semantics and imply
 *  additional capabilities of the sender. The &ltmajor&gt number is
 *  incremented when the format of a message within the protocol is
 *  changed. See RFC 2145 [36] for a fuller explanation.
 *  </p>
 *  <p>
 *  The version of an HTTP message is indicated by an HTTP-Version field
 *  in the first line of the message.
 *  </p>
 *  <pre>
 *     HTTP-Version   = ""HTTP"" ""/"" 1*DIGIT ""."" 1*DIGIT
 *  </pre>
 *  <p>
 *   Note that the major and minor numbers MUST be treated as separate
 *   integers and that each MAY be incremented higher than a single digit.
 *   Thus, HTTP/2.4 is a lower version than HTTP/2.13, which in turn is
 *   lower than HTTP/12.3. Leading zeros MUST be ignored by recipients and
 *   MUST NOT be sent.
 *  </p>

Note that the character entities for less-than and greater-than are not properly
ended with a semi-colon.

I will attach a proposed fix."
0,"Default lock timeouts should have static setter/getters
We recently stopped using Java system properties to derive defaults for things like the write/commit lock timeout, and switched to getter/setter's across all classes.  See here:

    http://www.gossamer-threads.com/lists/lucene/java-dev/27447

But, in the case at least of the write lock timeout, because it's marked ""public final static"", a consumer of this API can no longer change this value before instantiating the IndexWriter.  This is because the getter/setter for this is not static, which generally makes sense so you can change the timeout for each instance of IndexWriter.  But because IndexWriter on construction uses the timeout value, some uses cases need to change the value before getting an instance of IndexWriter.

This was actually a regression, in that Lucene users lost functionality they previously had, on upgrading.

I would propose that that we add getter/setter for the default value of this timeout, which would be static.  I'll attach a patch file.

See this thread for context that led to this issue:

   http://www.gossamer-threads.com/lists/lucene/java-dev/37421"
0,"Windows specific implementation of the Digest auth schemeMicrosoft Windows 2003 implementation of digest auth scheme is essentially a
superset of RFC 2617 with Windows specific aspects:
http://www.microsoft.com/technet/prodtechnol/windowsserver2003/library/TechRef/717b450c-f4a0-4cc9-86f4-cc0633aae5f9.mspx

Provide a super class of DigestScheme with Windows 2003 specific extensions,
which can be plugged in instead of the standard Digest impl

For details see PR #34909"
1,"LuceneDictionary skips first word in enumerationThe current code for LuceneDictionary will always skip the first word of the TermEnum. The reason is that it doesn't initially retrieve TermEnum.term - its first call is to TermEnum.next, which moves it past the first term (line 76).
To see this problem cause a failure, add this test to TestSpellChecker:
similar = spellChecker.suggestSimilar(""eihgt"",2);
      assertEquals(1, similar.length);
      assertEquals(similar[0], ""eight"");

Because ""eight"" is the first word in the index, it will fail.
"
1,NotQuery does not implement extractTerms()If the not() function is used in query together with the rep:excerpt() function an UnsupportedOperationException is thrown.
0,"Mark Fieldable as allowing some changes in 2.x future releasesSee http://lucene.markmail.org/message/4k2gqs3n7coh4lmd?q=Fieldable

1. We mark Fieldable as being subject to change. We heavily advertise (on java-dev and java-user and maybe general) that in the next minor release of Lucene (2.4), Fieldable will be changing. It is also marked at the top of CHANGES.txt very clearly for all the world to see. Since 2.4 is probably at least a month away, I think this gives anyone with a pulse enough time to react. "
1,"jcr-server: DefaultItemCollection#unlock does not call DavSession#removeReferenceDefaultItemCollection#unlock does not remove the token-reference from the DavSession that has been added
before upon creating the lock. This causes pending lock-references thus the cache entries in JCRWebdavServer
will not be cleared filling up the cache although the locks have been properly released. 
"
1,"spi2davex: Batch fails to create/modify properties with non-ascii characters namesthe spi2davex batch implementation fails upon creation/modification of all property types that have their value sent as
separate stringpart or binarypart AND contain non-ascii characters in their property name.

from what i've seen this is due to a limitation in HttpClient 3.x Part#sendDispositionHeader that always writes the part name
as ascii-bytes. in a related discussion [1] specification compliance and usability were addressed.

looking at the server-side part revealed that org.apache.commons.fileupload.FileUploadBase#FileItemIteratorImpl
is prepared to receive non-ascii characters in a header value.
a simple test also showed that curl is perfectly able to send utf-8 part names.

based on this information and given the fact that spi2dav and the server-sided part are intended to communicate
with one other rather than with any kind of custom clients, i suggest to add a simple fix by patching the parts used
within spi2davex.

btw: in HttpClient 4.x there seems to be a workaround for this problem [2]

[1] http://www.mail-archive.com/httpclient-dev@jakarta.apache.org/msg04637.html
[2] https://issues.apache.org/jira/browse/HTTPCLIENT-293"
0,"Clean up old JIRA issues in component ""Other""A list of all JIRA issues in component ""Other"" that haven't been updated in 2007:

   *	 LUCENE-746  	 Incorrect error message in AnalyzingQueryParser.getPrefixQuery   
   *	LUCENE-644 	Contrib: another highlighter approach 
   *	LUCENE-574 	support for vjc java compiler, also known as J# 
   *	LUCENE-471 	gcj ant target doesn't work on windows 
   *	LUCENE-434 	Lucene database bindings 
   *	LUCENE-254 	[PATCH] pseudo-relevance feedback enhancement 
   *	LUCENE-180 	[PATCH] Language guesser contribution 
"
0,"Improve performance of CharTermAttribute(Impl) and also fully implement AppendableThe Appendable.append(CharSequence) method in CharTermAttributes is good for general use. But like StringBuilder has for some common use cases specialized methods, this does the same and adds separate append methods for String, StringBuilder and CharTermAttribute itsself. This methods enable the compiler to directly link the specialized methods and don't use the instanceof checks. The unspecialized method only does the instanceof checks for longer CharSequences (>8 chars), else it simply iterates.

This patch also fixes the required special ""null"" handling. append() methods are required by Appendable to append ""null"", if the argument is null. I dont like this, but its required. Maybe we should document, that we dont dont support it. Otherwise, JDK's formatter fails with formatting null."
0,"Restore top level disjunction performanceThis patch restores the performance of top level disjunctions. 
The introduction of BooleanScorer2 had impacted this as reported
on java-user on 21 Nov 2006 by Stanislav Jordanov.
"
1,"FileInputStream never closed in HTMLParserHTMLParser.java contains this code: 
 
  public HTMLParser(File file) throws FileNotFoundException { 
    this(new FileInputStream(file)); 
  } 
 
This FileInputStream should be closed with the close() method, as there's no 
guarantee that the garbage collection will run and do this for you. I don't 
know how to fix this without changing the API to take a FileInputStream 
instead of a File, as the call to this() must be the first thing in the 
constructor, i.e. you cannot create the stream, call this(...), and then close 
the stream."
0,Fix Hits deprecation noticeJust needs to be committed to 2.9 branch since hits is now removed.
0,"Hindi AnalyzerAn analyzer for hindi.

below are MAP values on the FIRE 2008 test collection.
QE means expansion with morelikethis, all defaults, on top 5 docs.

||setup||T||T(QE)||TD||TD(QE)||TDN||TDN(QE)||
|words only|0.1646|0.1979|0.2241|0.2513|0.2468|0.2735|
|HindiAnalyzer|0.2875|0.3071|0.3387|*0.3791**|0.3837|0.3810|
|improvement|74.67%|55.18%|51.14%|50.86%|55.47%|39.31%|

* TD was the official measurement, highest score for this collection in FIRE 2008 was 0.3487: http://www.isical.ac.in/~fire/paper/mcnamee-jhu-fire2008.pdf

needs a bit of cleanup and more tests"
0,"Change access to internal maps of HttpState to protected.To be able to serialize the conversational state of a http session access to the internal maps of HttpState is required. Currently they are all ""private"", so subclasses cannot access them. Changing the access to ""protected"" will allow any subclass to access those maps."
0,"Use a separate JFlex generated Unicode 4 by Java 5 compatible StandardTokenizerThe current trunk version of StandardTokenizerImpl was generated by Java 1.4 (according to the warning). In Java 3.0 we switch to Java 1.5, so we should regenerate the file.

After regeneration the Tokenizer behaves different for some characters. Because of that we should only use the new TokenizerImpl when Version.LUCENE_30 or LUCENE_31 is used as matchVersion."
0,"Add files generated by eclipse or maven to svn:ignoreTo make life easier for eclipse and maven users please add the following files to svn:ingore:

jackrabbit-jcr-rmi:
-------------------
.settings
.classpath
.project
jackrabbit-jcr-rmi-pom-snapshot-version
project.xml.md5

jackrabbit-core:
----------------
.settings
.classpath
.project
jackrabbit-core-pom-snapshot-version
project.xml.md5

Maybe there some files missing in this list that could help developers using IDEA?"
1,"WorkspaceManager.dispose() should wait until change feed thread is stoppedThe WorkspaceManager currently only interrupts the change feed thread, but does not wait until it stops."
0,"Data store garbage collectionCurrently the data store garbage collection needs to be run manually. It should be simpler to use (maybe tool based), or automatic."
0,"need tests to guarantee transparency of caching module on end-to-end headers""A transparent proxy SHOULD NOT modify an end-to-end header unless the definition of that header requires or specifically allows that.""

http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.5.2

This is already true of our implementation, but we should have tests to preserve that behavior.
"
1,"SSL does not seem to work at allWhenever I try to request content via https I get this exception:


Exception in thread ""main"" javax.net.ssl.SSLException: hostname in certificate didn't match: <140.211.11.131> != <*.apache.org>
	at org.apache.http.conn.ssl.AbstractVerifier.verify(AbstractVerifier.java:220)
	at org.apache.http.conn.ssl.BrowserCompatHostnameVerifier.verify(BrowserCompatHostnameVerifier.java:54)
	at org.apache.http.conn.ssl.AbstractVerifier.verify(AbstractVerifier.java:149)
	at org.apache.http.conn.ssl.AbstractVerifier.verify(AbstractVerifier.java:130)
	at org.apache.http.conn.ssl.SSLSocketFactory.createSocket(SSLSocketFactory.java:399)
	at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:143)
	at org.apache.http.impl.conn.AbstractPoolEntry.open(AbstractPoolEntry.java:149)
	at org.apache.http.impl.conn.AbstractPooledConnAdapter.open(AbstractPooledConnAdapter.java:108)
	at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:415)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:641)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:576)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:554)
	at HttpsTest.fails(HttpsTest.java:25)
	at HttpsTest.main(HttpsTest.java:12)


I can reproduce this whith the following code:


import org.apache.http.client.HttpClient;
import org.apache.http.client.methods.HttpGet;
import org.apache.http.impl.client.DefaultHttpClient;

public class HttpsTest {

    public static void main(final String[] args) throws Exception {
        final HttpClient client = new DefaultHttpClient();
        final HttpGet req = new HttpGet(""https://www.apache.org"");
        client.execute(req);
    }

}
"
0,"StringBody has incorrect default for charactersetStringBody defaults to Charset.defaultCharset() if the charset is not provided.

This means that the default depends on the current host.

The default should be US-ASCII (as was the case with StringPart in Commons HttpClient 3.1)."
0,DescendantSelfAxisQuery creates too many object instancesIn DescendantSelfAxisQuery.DescendantSelfAxisScorer.isValid() there is an ArrayList and an Integer instance created on every call. Since this method gets called really often during queries the object creation/gc affects performance.
0,"Possible Memory Leak in StoredFieldsWriterStoredFieldsWriter creates a pool of PerDoc instances

this pool will grow but never be reclaimed by any mechanism

furthermore, each PerDoc instance contains a RAMFile.
this RAMFile will also never be truncated (and will only ever grow) (as far as i can tell)

When feeding documents with large number of stored fields (or one large dominating stored field) this can result in memory being consumed in the RAMFile but never reclaimed. Eventually, each pooled PerDoc could grow very large, even if large documents are rare.

Seems like there should be some attempt to reclaim memory from the PerDoc[] instance pool (or otherwise limit the size of RAMFiles that are cached) etc
"
0,"refactor spatial contrib ""Filter"" ""Query"" classesFrom erik's comments in LUCENE-1387

    * DistanceQuery is awkwardly named. It's not an (extends) Query.... it's a POJO with helpers. Maybe DistanceQueryFactory? (but it creates a Filter also)

    * CartesianPolyFilter is not a Filter (but CartesianShapeFilter is)
"
0,Use jackrabbit-jcr-commons in jackrabbit-jcr-rmiThe jackrabbit-jcr-rmi component should leverage the general-purpose classes in jackrabbit-jcr-commons even at the expense of introducing an extra dependency.
0,"Contrib/Module-uptodate assume name matches path and jarWith adding a new 'queries' module, I am trying to change the project name of contrib/queries to queries-contrib.  However currently the contrib-uptodate assumes that the name property is used in the path and in the jar name.

By using the name in the path, I must set the value to 'queries' (since the path is contrib/queries).  However because the project name is now queries-contrib, the actual jar file will be lucene-queries-contrib-${version}.jar, not lucene-queries-${version}.jar, as is expected.

Consequently I think we need to separate the path name from the jar name properties.  For simplicity I think adding a new jar-name property will suffice, which can be optional and if omitted, is filled in with the name property."
0,"contrib/Highlighter javadoc example needs to be updatedThe Javadoc package.html example code is outdated, as it still uses QueryParser.parse.  

http://lucene.zones.apache.org:8080/hudson/job/Lucene-Nightly/javadoc/contrib-highlighter/index.html"
0,"Modify ParallelMultiSearcher to use a CompletionService instead of slowly polling for resultsRight now, the parallel multi searcher creates an array/list of Future<V> representing each of the searchables that's being concurrently searched (and its corresponding search task).

As it stands, once the tasks are all submitted to the executor, the array is iterated over, FIFO, and Future.get() is called iteratively.  This obviously works, but isn't ideal.  It's entirely possible (a situation I've run into) where one of the first searchables represents a large index that takes a long time to search, so the results of the other searchables can't be processed until the large index is done searching.  In my case, we have two indexes with several million records that get searched in front of some other indexes, the smallest of which has only a few ten thousand entries and I didn't think it was ideal for the results of the other indexes to wait.

I've modified ParallelMultiSearcher to use CompletionServices instead, so that results are processed in the order they are completed, rather than the order that they are submitted.  All the tests still pass, and to the best of my knowledge this won't break anything.  This have several advantages:
1) Speed - the thread owning the executor doesn't have to wait for the first submitted task to finish in order to process the results of the other tasks, which may have finished first
2) Removed several warnings (even if they are annotated away) due to the ugliness of typecasting generic arrays.
3) Decreased the complexity of the code in some cases, usually by removing the necessity of allocating and filling arrays.

With a primed ""cache"" of searchables, I was getting 700-1200 ms per search, and using the same phrases, with this patch, I am now getting 400-500ms per search :)

Patch is attached."
0,"package org.apache.xml.utils does not exist (JDK 1.5.0)Executing ""maven jar"" on a freshly checked out source tree fails with the following error messages when using JDK 1.5.0:

/home/jukkaz/src/jackrabbit/src/java/org/apache/jackrabbit/core/xml/DocViewSAXEventGenerator.java:24: package org.apache.xml.utils does not exist
import org.apache.xml.utils.XMLChar;
                            ^
/home/jukkaz/src/jackrabbit/src/java/org/apache/jackrabbit/core/xml/DocViewSAXEventGenerator.java:142: cannot find symbol
symbol  : variable XMLChar
location: class org.apache.jackrabbit.core.xml.DocViewSAXEventGenerator
            if (!XMLChar.isValidName(elemName)) {
                 ^
/home/jukkaz/src/jackrabbit/src/java/org/apache/jackrabbit/core/xml/DocViewSAXEventGenerator.java:162: cannot find symbol
symbol  : variable XMLChar
location: class org.apache.jackrabbit.core.xml.DocViewSAXEventGenerator
                if (!XMLChar.isValidName(attrName)) {

The same build succeeds without problems on JDK 1.4.2_06.

I found some reports about similar problems after upgrading from JDK 1.4 to 1.5. It seems that the org.apache.xml.utils.XMLChar was a part (undocumented?) of the standard JDK classpath, but that it has been dropped from JDK 1.5.

A similar (the same?) XMLChar utility class can be found in the org.apache.xerces.utils package, which is automatically included by the Xerces dependency. The following change fixes the problem on JDK 1.5.0 and seems to work fine also on JDK 1.4.2_06.

Index: src/java/org/apache/jackrabbit/core/xml/DocViewSAXEventGenerator.java
===================================================================
--- src/java/org/apache/jackrabbit/core/xml/DocViewSAXEventGenerator.java      (revision 57540)
+++ src/java/org/apache/jackrabbit/core/xml/DocViewSAXEventGenerator.java      (working copy)
@@ -21,7 +21,7 @@
 import org.apache.jackrabbit.core.state.PropertyState;
 import org.apache.jackrabbit.core.util.Base64;
 import org.apache.log4j.Logger;
-import org.apache.xml.utils.XMLChar;
+import org.apache.xerces.util.XMLChar;
 import org.xml.sax.ContentHandler;
 import org.xml.sax.SAXException;
 import org.xml.sax.helpers.AttributesImpl;

"
0,"Speedup merging of stored fields when field mapping ""matches""Robert Engels suggested the following idea, here:

  http://www.gossamer-threads.com/lists/lucene/java-dev/54217

When merging in the stored fields from a segment, if the field name ->
number mapping is identical then we can simply bulk copy the entire
entry for the document rather than re-interpreting and then re-writing
the actual stored fields.

I've pulled the code from the above thread and got it working on the
current trunk."
0,"wordlistloader is inefficientWordListLoader is basically used for loading up stopwords lists, stem dictionaries, etc.
Unfortunately the api returns Set<String> and sometimes even HashSet<String> or HashMap<String,String>

I think we should break it and return CharArraySets and CharArrayMaps (but leave the return value as generic Set,Map).

If someone objects to breaking it in 3.1, then we can do this only in 4.0, but i think it would be good to fix it both places.
The reason is that if someone does new FooAnalyzer() a lot (probably not uncommon) i think its doing a bunch of useless copying.

I think we should slap @lucene.internal on this API too, since thats mostly how its being used.
"
0,"Allow access to journal inside ClusterNodeJCR-757 added support multiple consumers/producers to be attached to the same journal. In order to access this journal, however, o.a.j.core.cluster.ClusterNode has to allow access to the journal it has created."
0,"Disable SearchManagerIn previous versions (e.g. SVN tag 0.1-spec0.14) it was possible to disable the SearchManagers by not configuring a search index path. In the current revision, a NullPointerException is thrown, if the search index configuration is missing, tough the rest of the system would support missing search index configuration as before.

I suggest to extend search index configuration interpretation in WorkspaceCfg.init as follows:

        Element srchConfig = wspElem.getChild(SEARCH_INDEX_ELEMENT);
        if (srchConfig != null) {
            String pathAttr = srchConfig.getAttributeValue(PATH_ATTRIB);
            if (pathAttr != null && pathAttr.length() > 0) {
                searchIndexDir = replaceVars(pathAttr, vars);
            }
        }

This only reads search index configuration if available.

The reason to switch of the SearchManager is, that in my use case enabling the SearchManager yields a performance degradation of a factor of 10 ! Instead of taking around 500ms (which is still too long :-) to save 3 nodes and 15 properties, it would take around 5 seconds to save the same amount of data. And I do not need the SearchManager in my use case."
1,"TestFSTs.testRealTerms produces a corrupt indexseems to be prox/skip related: the test passes, but the checkindex upon closing fails.

ant test-core -Dtestcase=TestFSTs -Dtests.seed=-4012305283315171209:0 -Dtests.multiplier=3 -Dtests.nightly=true -Dtests.linedocsfile=c:/data/enwiki.random.lines.txt.gz

Note: to get the enwiki.random.lines.txt.gz you have to fetch it from hudson (warning 1 gigabyte file).
you also have to run the test a few times to trigger it.

ill upload the index this thing makes to this issue.
"
0,"allow tests to use different Directory implsNow that all tests use MockRAMDirectory instead of RAMDirectory, they are all picky like windows and force our tests to
close readers etc before closing the directory.

I think we should do the following:
# change new MockRAMDIrectory() in tests to .newDirectory(random)
# LuceneTestCase[J4] tracks if all dirs are closed at tearDown and also cleans up temp dirs like solr.
# factor out the Mockish stuff from MockRAMDirectory into MockDirectoryWrapper
# allow a -Dtests.directoryImpl or simpler to specify the default Directory to use for tests: default being ""random""

i think theres a chance we might find some bugs that havent yet surfaced because they are easier to trigger with FSDir
Furthermore, this would be beneficial to Directory-implementors as they could run the entire testsuite against their Directory impl, just like codec-implementors can do now.
"
0,Grouping collector that computes grouped facet countsSpinoff from issue SOLR-2898. 
1,"Highlighter doesn't support NumericRangeQuery or deprecated RangeQuerySucks. Will throw a NullPointer exception. 

Only NumericRangeQuery will throw the exception.
RangeQuery just won't highlight."
1,"SampleComparable doesn't work well in contrib/remote testsAs discovered in LUCENE-1749, when using identical instances of a SortComparator you get multiple entries in the FieldCache.

demonstrating this bug currently requires the patches in LUCENE-1749.

See markmiller's comment here...
https://issues.apache.org/jira/browse/LUCENE-1749?focusedCommentId=12735190#action_12735190"
0,Consolidate Solr  & Lucene FunctionQuery into modulesSpin-off from the [dev list | http://www.mail-archive.com/dev@lucene.apache.org/msg13261.html]  
0,"Elision filter for simple french analyzingIf you don't wont to use stemming, StandardAnalyzer miss some french strangeness like elision.
""l'avion"" wich means ""the plane"" must be tokenized as ""avion"" (plane).
This filter could be used with other latin language if elision exists."
0,"comparator API for segment versionsSee LUCENE-3012 for an example.

Things get ugly if you want to use SegmentInfo.getVersion()

For example, what if we committed my patch, release 3.2, but later released 3.1.1 (will ""3.1.1"" this be whats written and returned by this function?)
Then suddenly we broke the index format because we are using Strings here without a reasonable comparator API.

In this case one should be able to compute if the version is < 3.2 safely.

If we don't do this, and we rely upon this version information internally in lucene, I think we are going to break something."
0,"Use Maven dependency managementMany of the Jackrabbit components have dependencies to each other and to external libraries,
whose versions should ideally be the same for all the Jackrabbit components. To guarantee
the use of same depedency versions and to simplify overall depedency management we should
start using the Maven depedencyManagement feature in the Jackrabbit parent pom."
0,"add TCK test for Info map of NODE_MOVED event on node reorderingadd the TCK test for this problem, and mark this as known test failure for now"
1,"contrib-spatial java.lang.UnsupportedOperationException on QueryWrapperFilter.getDocIdSetWe use in our Project (which is in the devel phase) the latest Snapshot release of lucene. After i updated to the latest Snapshot a few days ago one of our JUnit tests fails and throws the following error:

java.lang.UnsupportedOperationException
	at org.apache.lucene.search.Query.createWeight(Query.java:91)
	at org.apache.lucene.search.QueryWrapperFilter.getDocIdSet(QueryWrapperFilter.java:72)
	at org.apache.lucene.misc.ChainedFilter.getDISI(ChainedFilter.java:150)
	at org.apache.lucene.misc.ChainedFilter.initialResult(ChainedFilter.java:173)
	at org.apache.lucene.misc.ChainedFilter.getDocIdSet(ChainedFilter.java:211)
	at org.apache.lucene.misc.ChainedFilter.getDocIdSet(ChainedFilter.java:141)
	at org.apache.lucene.search.ConstantScoreQuery$ConstantScorer.<init>(ConstantScoreQuery.java:116)
	at org.apache.lucene.search.ConstantScoreQuery$ConstantWeight.scorer(ConstantScoreQuery.java:81)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:244)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:172)
	at org.apache.lucene.search.Searcher.search(Searcher.java:183)
	at org.hibernate.search.query.QueryHits.updateTopDocs(QueryHits.java:100)
	at org.hibernate.search.query.QueryHits.<init>(QueryHits.java:61)
	at org.hibernate.search.query.QueryHits.<init>(QueryHits.java:51)
	at org.hibernate.search.query.FullTextQueryImpl.getQueryHits(FullTextQueryImpl.java:373)
	at org.hibernate.search.query.FullTextQueryImpl.list(FullTextQueryImpl.java:293)
	...

I think it appeared after the Hudson build 917... and the following commit of the Query.java http://hudson.zones.apache.org/hudson/job/Lucene-trunk/917/changes#detail4 and is in connection with this JIRA issue: LUCENE-1771
I hope i'm at the right place and that you can fix it. Thanks!"
1,"CloseableThreadLocal does not work well with Tomcat thread poolingWe tracked down a large memory leak (effectively a leak anyway) caused
by how Analyzer users CloseableThreadLocal.
CloseableThreadLocal.hardRefs holds references to Thread objects as
keys.  The problem is that it only frees these references in the set()
method, and SnowballAnalyzer will only call set() when it is used by a
NEW thread.

The problem scenario is as follows:

The server experiences a spike in usage (say by robots or whatever)
and many threads are created and referenced by
CloseableThreadLocal.hardRefs.  The server quiesces and lets many of
these threads expire normally.  Now we have a smaller, but adequate
thread pool.  So CloseableThreadLocal.set() may not be called by
SnowBallAnalyzer (via Analyzer) for a _long_ time.  The purge code is
never called, and these threads along with their thread local storage
(lucene related or not) is never cleaned up.

I think calling the purge code in both get() and set() would have
avoided this problem, but is potentially expensive.  Perhaps using 
WeakHashMap instead of HashMap may also have helped.  WeakHashMap 
purges on get() and set().  So this might be an efficient way to
clean up threads in get(), while set() might do the more expensive
Map.keySet() iteration.

Our current work around is to not share SnowBallAnalyzer instances
among HTTP searcher threads.  We open and close one on every request.

Thanks,
Matt"
0,"Configure Maximum Connection LifetimesProvide a means of configuring a maximum lifetime for HttpClient connections.  Currently, it would appear as long as a connection is used it may persist indefinitely.

This would be useful for situations where HttpClient needs to react to DNS changes, such as the following situation that may occur when using DNS load balancing:
 - HttpClient maintains connections to example.com which resolves to IP A
 - Machine at IP A fails, and example.com now resolves to backup machine at IP B
 - Since IP A is failing, connections are destroyed, and new connections are made to IP B
 - Machine at IP A recovers, but HttpClient maintains connections to IP B since the connections are still healthy

The desired behavior would be that connections to IP B will reach their connection lifetime, and new connections could be created back to IP A according to the updated DNS settings."
0,Wrong javadoc on LowerCaseTokenizer.normalizeThe javadoc on LowerCaseTokenizer.normalize seems to be copy/paste from LetterTokenizer.isTokenChar.
0,"Handle date values in the far future or prevent these from being persistedSetting a date property with a value in the far future (e.g., the year 20009) and saving the session causes the index component to throw an exception (see the DateField#timeToString method). Furthermore, when the repository is restarted, the properties' value cannot be retrieved anymore because of a ValueFormatException caused by an empty value. Restarting the repository with an empty search index does not work because indexing fails. I haven't looked into the effect on queries.
"
1,"JCR-SQL2 query with multiple columns in result only returns last column when using Row.getValues()When running a query like below on an in-process repository (via TransientRepository) or via RMI access, a call to Row.getValues() only returns the last column selected:

       SELECT property1, property2 FROM [nodetype]

QueryResult.getColumnNames() returns the right set of columns.

Stepping through the code shows that org.apache.jackrabbit.core.query.lucene.join.AbstractRow has the implementation of getValues() - this creates a new Values array, then overwrites it multiple times in a for loop that iterates once per column. That doesn't sound like the desired behaviour.

Getting values via individual calls to Row.getValue(""property1"") gives the correct results.

"
0,"Little improvement for SimpleHTMLEncoderThe SimpleHTMLEncoder could be improved slightly: all characters with code >=
128 should be encoded as character entities. The reason is, that the encoder
does not know the encoding that is used for the response. Therefore it is safer
to encode all characters beyond ASCII as character entities.

Here is the necessary modification of SimpleHTMLEncoder:

       default:
         if (c < 128) {
           result.append(c);
         } else {
           result.append(""&#"").append((int)c).append("";"");
         }"
0,"Make the Payload Boosting Queries consistentBoostingFunctionTermQuery should be consistent with BoostingNearQuery -

Renaming to PayloadNearQuery and PayloadTermQuery"
0,"https should check CN of x509 certhttps should check CN of x509 cert

Since we're essentially rolling our own ""HttpsURLConnection"",  the checking provided by ""javax.net.ssl.HostnameVerifier"" is no longer in place.

I have a patch I'm about to attach which caused both createSocket() methods on o.a.h.conn.ssl.SSLSocketFactory to blowup:

test1: javax.net.ssl.SSLException: hostname in certificate didn't match: <vancity.com> != <www.vancity.com>
test2: javax.net.ssl.SSLException: hostname in certificate didn't match: <vancity.com> != <www.vancity.com>

Hopefully people agree that this is desirable.
"
0,"TCK: SessionReadMethodsTest#testIsLive calls logout() more than onceSessionReadMethodsTest#testIsLive calls logout more than once in a session (once in the test, once in tearDown).  JSR-170 doesn't prohibit an implementation from throwing an unchecked exception (such as IllegalStateException) if logout is called more than once.

Proposal: change tearDown to test isLive before calling logout.

--- SessionReadMethodsTest.java (revision 422074)
+++ SessionReadMethodsTest.java (working copy)
@@ -57,7 +57,7 @@
      * Releases the session aquired in {@link #setUp()}.
      */
     protected void tearDown() throws Exception {
-        if (session != null) {
+        if (session != null && session.isLive()) {
             session.logout();
         }
         super.tearDown();
"
1,"document view: importXML() fails on protected property jcr:primaryTypewhen trying to import an xml document where elements contain the attribute jcr:primaryType the import fails with:

javax.jcr.nodetype.ConstraintViolationException: cannot set the value of a protected property /testroot/docviewimport/doc/jcr:primaryType
	at org.apache.jackrabbit.core.PropertyImpl.setValue(PropertyImpl.java:907)
	at org.apache.jackrabbit.core.NodeImpl.setProperty(NodeImpl.java:1044)
	at org.apache.jackrabbit.core.xml.DocViewImportHandler.startElement(DocViewImportHandler.java:124)
	at org.apache.jackrabbit.core.xml.ImportHandler.startElement(ImportHandler.java:164)
	at org.apache.xerces.parsers.AbstractSAXParser.startElement(Unknown Source)
	at org.apache.xerces.impl.XMLNSDocumentScannerImpl.scanStartElement(Unknown Source)
	at org.apache.xerces.impl.XMLNSDocumentScannerImpl$NSContentDispatcher.scanRootElementHook(Unknown Source)
	at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl$FragmentContentDispatcher.dispatch(Unknown Source)
	at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanDocument(Unknown Source)
	at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
	at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
	at org.apache.xerces.parsers.XMLParser.parse(Unknown Source)
	at org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source)
	at org.apache.jackrabbit.core.SessionImpl.importXML(SessionImpl.java:836)
	at org.apache.jackrabbit.test.api.DocViewImportTest.setUp(DocViewImportTest.java:92)
	at com.intellij.rt.execution.junit2.JUnitStarter.main(JUnitStarter.java:31)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:78)

if i understand the spec correctly, the import process should take care of this attribute and determine the node type of the new nodes based on it."
0,allow AbstractFileSystemTest.getFileSystem to throw an Exception
0,"reusing connections is unreliableHttpConnection reuse is unreliable. Because of the following:

1) There is currently no way to determine if a connection is still open on the
server side.
2) If an IOException occurs while writing to a connection it cannot be reused."
0,"configurable User-Agent stringUser configurable item to set the user agent without haveing to set it on a per
HttpMethod basis."
0,"[PATCH] new method expungeDeleted() added to IndexWriterWe make use the docIDs in lucene. I need a way to compact the docIDs in segments
to remove the ""holes"" created from doing deletes. The only way to do this is by
calling IndexWriter.optimize(). This is a very heavy call, for the cases where
the index is large but with very small number of deleted docs, calling optimize
is not practical.

I need a new method: expungeDeleted(), which finds all the segments that have
delete documents and merge only those segments.

I have implemented this method and have discussed with Otis about submitting a
patch. I don't see where I can attached the patch. I will do according to the
patch guidleine and email the lucene mailing list.

Thanks

-John

I don't see a place where I can"
0,"Add oal.util.Version ctor to QueryParserThis is a followup of LUCENE-1987:

If somebody uses StandardAnalyzer with Version.LUCENE_CURRENT and then uses QueryParser, phrase queries will not work, because the StopFilter enables position Increments for stop words, but QueryParser ignores them per default. The user has to explicitely enable them.

This issue would add a ctor taking the Version constant and automatically enable this setting. The same applies to the contrib queryparser. Eventually also StopAnalyzer should add this version ctor.

To be able to remove the default ctor for 3.0 (to remove a possible trap for users of QueryParser), it must be deprecated and the new one also added to 2.9.1."
1,"DefaultLoginModule/SimpleLoginModule don't support custom PrincipalProviderWhen configuring a custom PrincipalProvider for the SimpleLoginModule or DefaultLoginModule, inside of a repository.xml file with configuration such as the following:

    <Security appName=""Jackrabbit"">
        <AccessManager
            class=""org.apache.jackrabbit.core.security.DefaultAccessManager"">
        </AccessManager>
        <LoginModule
            class=""org.apache.jackrabbit.core.security.authentication.DefaultLoginModule"">
            <param name=""principalprovider"" value=""com.foo.jcr.BasicPrincipalProvider""/>
        </LoginModule>
      <SecurityManager class=""org.apache.jackrabbit.core.DefaultSecurityManager"">         
      </SecurityManager>    
    </Security>

And that yields the following stacktrace:

javax.jcr.LoginException: org.apache.jackrabbit.core.security.authentication.DefaultLoginModule does not support 'principalprovider: org.apache.jackrabbit.core.security.authentication.DefaultLoginModule does not support 'principalprovider: org.apache.jackrabbit.core.security.authentication.DefaultLoginModule does not support 'principalprovider
	at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1353)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:53)
	at com.foo.jcr.PrincipalProviderTest.testPrincipalProvider(PrincipalProviderTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at org.junit.internal.runners.TestMethod.invoke(TestMethod.java:59)
	at org.junit.internal.runners.MethodRoadie.runTestMethod(MethodRoadie.java:98)
	at org.junit.internal.runners.MethodRoadie$2.run(MethodRoadie.java:79)
	at org.junit.internal.runners.MethodRoadie.runBeforesThenTestThenAfters(MethodRoadie.java:87)
	at org.junit.internal.runners.MethodRoadie.runTest(MethodRoadie.java:77)
	at org.junit.internal.runners.MethodRoadie.run(MethodRoadie.java:42)
	at org.junit.internal.runners.JUnit4ClassRunner.invokeTestMethod(JUnit4ClassRunner.java:88)
	at org.junit.internal.runners.JUnit4ClassRunner.runMethods(JUnit4ClassRunner.java:51)
	at org.junit.internal.runners.JUnit4ClassRunner$1.run(JUnit4ClassRunner.java:44)
	at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:27)
	at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:37)
	at org.junit.internal.runners.JUnit4ClassRunner.run(JUnit4ClassRunner.java:42)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:45)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)
Caused by: javax.security.auth.login.LoginException: org.apache.jackrabbit.core.security.authentication.DefaultLoginModule does not support 'principalprovider
	at org.apache.jackrabbit.core.security.authentication.LocalAuthContext.login(LocalAuthContext.java:68)
	at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1346)
	... 24 more
javax.security.auth.login.LoginException: org.apache.jackrabbit.core.security.authentication.DefaultLoginModule does not support 'principalprovider
	at org.apache.jackrabbit.core.security.authentication.LocalAuthContext.login(LocalAuthContext.java:68)
	at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1346)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:53)
	at com.foo.jcr.PrincipalProviderTest.testPrincipalProvider(PrincipalProviderTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at org.junit.internal.runners.TestMethod.invoke(TestMethod.java:59)
	at org.junit.internal.runners.MethodRoadie.runTestMethod(MethodRoadie.java:98)
	at org.junit.internal.runners.MethodRoadie$2.run(MethodRoadie.java:79)
	at org.junit.internal.runners.MethodRoadie.runBeforesThenTestThenAfters(MethodRoadie.java:87)
	at org.junit.internal.runners.MethodRoadie.runTest(MethodRoadie.java:77)
	at org.junit.internal.runners.MethodRoadie.run(MethodRoadie.java:42)
	at org.junit.internal.runners.JUnit4ClassRunner.invokeTestMethod(JUnit4ClassRunner.java:88)
	at org.junit.internal.runners.JUnit4ClassRunner.runMethods(JUnit4ClassRunner.java:51)
	at org.junit.internal.runners.JUnit4ClassRunner$1.run(JUnit4ClassRunner.java:44)
	at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:27)
	at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:37)
	at org.junit.internal.runners.JUnit4ClassRunner.run(JUnit4ClassRunner.java:42)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:45)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)"
1,"DBDataStore doesn't support concurrent readsMy understanding is that setting parameter copyWhenReading to true should allow concurrent reads by spooling binary property to temporary file and free database resources (connection) immediately to make it available for other threads.

After applying patch for JCR-1388, DBDataStore doesn't support concurrent reads anymore, resultSet is kept open and db connection is blocked until the stream is read and closed. When copyWhenReading is set to true db connection should be released immediately, this is the reason i guess why temporary file is used."
0,"New Analyzer for buffering tokensIn some cases, it would be handy to have Analyzer/Tokenizer/TokenFilters that could siphon off certain tokens and store them in a buffer to be used later in the processing pipeline.

For example, if you want to have two fields, one lowercased and one not, but all the other analysis is the same, then you could save off the tokens to be output for a different field.

Patch to follow, but I am still not sure about a couple of things, mostly how it plays with the new reuse API.

See http://www.gossamer-threads.com/lists/lucene/java-dev/54397?search_string=BufferingAnalyzer;#54397"
0,"back-compat tests (""ant test-tag"") should test JAR drop-in-ability
We now test back-compat with ""ant test-tag"", which is very useful for
catching breaks in back compat before committing.

However, that currently checks out ""src/test"" sources and then
compiles them against the trunk JAR, and runs the tests.  Whereas our
back compat policy:

  http://wiki.apache.org/lucene-java/BackwardsCompatibility

states that no recompilation is required on upgrading to a new JAR.
Ie you should be able to drop in the new JAR in place of your old one
and things should work fine.

So... we should fix ""ant test-tag"" to:

  * Do full checkout of core sources & tests from the back-compat-tag

  * Compile the JAR from the back-compat sources

  * Compile the tests against that back-compat JAR

  * Swap in the trunk JAR

  * Run the tests

"
0,Expose BootstrapConfig in Servletsthe RepostitoryStartup and RepositroyAccess servlets use a bootstrap config object for initialization. in order to generate diagnostics reports it would be very useful to be able to access them.
1,"The CredentialsWrapper should use a empty String as userId if custom Credentials are usedIf custom Credentials are used we get a IllegalArgumentException from the AbstractQValueFactory while executing SessionItemStateManager.computeSystemGeneratedPropertyValues().
The 2 Properties jcr:createdBy and jcr:lastModified could not be created."
0,New QueryParser should not allow leading wildcard by defaultThe current QueryParser disallows leading wildcard characters by default.
1,ItemManager registers itself as listener too earlyThis is similar to JCR-2168 but for ItemManager and SessionItemStateManager.
1,"Path returned by FileSystemBLOBStore.createId() is not absoluteHi,

I have developed my own FileSystem in which I call FileSystemPathUtil.checkFormat(path) for every operation on the file system.
When the file system is called to store a BLOB value, the path I get is always relative, resulting in a ""not an absolute path"" FileSystemException.

The problem has been traced back to org.apache.jackrabbit.core.state.util.FileSystemBLOBStore.creatId().
I think there should be a:
   sb.append(FileSystem.SEPARATOR_CHAR);
before the for loop.

Thanks."
1,"cache module produces improperly formatted Warning header when revalidation failsThe warning header currently attached to a stale response by the caching module when validation with the origin server fails is not a properly-formatted Warning header.

http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.46"
0,"ChineseFilter is inefficienttrivial patch to use CharArraySet, so it can use termBuffer() instead of term()
"
0,"Use StringBuilder instead of StringBuffer in benchmarkMinor change - use StringBuilder instead of StringBuffer in benchmark's code. We don't need the synchronization of StringBuffer in all the places that I've checked.

The only place where it _could_ be a problem is in HtmlParser's API - one method accepts a StringBuffer and it's an interface. But I think it's ok to change benchmark's API, back-compat wise and so I'd like to either change it to accept a String, or remove the method altogether -- no code in benchmark uses it, and if anyone needs it, he can pass StringReader to the other method."
0,"Optimize usage of normsThere is a very significant potential for optimizing the size of the search index.

We have seen a case where there were multiple segments with about the same number of nodes (roughly 10 million), but the size on disk was very different.
One segment was 19 GB while all others where around 3 GB. The major difference was the number of fields indexed. The large segment had significantly more fields, which resulted in a large norms file.

We should go through our implementation and see where norms are really necessary and disable tracking of norms wherever possible."
0,"Upgrade JUnit to 4.10, refactor state-machine of detecting setUp/tearDown call chaining.Both Lucene and Solr use JUnit 4.7. I suggest we move forward and upgrade to JUnit 4.10 which provides several infrastructural changes (serializable Description objects, class-level rules, various tweaks). JUnit 4.10 also changes (or fixes, depends how you look at it) the order in which @Before/@After hooks and @Rules are applied. This makes the old state-machine in LuceneTestCase fail (because the order is changed).

I rewrote the state machine and used a different, I think simpler, although Uwe may disagree :), mechanism in which the hook methods setUp/ tearDown are still there, but they are empty at the top level and serve only to detect whether subclasses chain super.setUp/tearDown properly (if they override anything).

In the long term, I would love to just get rid of public setup/teardown methods and make them private (so that they cannot be overriden or even seen by subclasses) but this will require changes to the runner itself."
0,"better handling of files inside/outside CFS by codecSince norms and deletes were moved under Codec (LUCENE-3606, LUCENE-3661),
we never really properly addressed the issue of how Codec.files() should work,
considering these files are always stored outside of CFS.

LUCENE-3606 added a hack, LUCENE-3661 cleaned up the hack a little bit more,
but its still a hack.

Currently the logic in SegmentInfo.files() is:
{code}
clearCache()

if (compoundFile) {
  // don't call Codec.files(), hardcoded CFS extensions, etc
} else {
  Codec.files()
}

// always add files stored outside CFS regardless of CFS setting
Codec.separateFiles()

if (sharedDocStores) {
  // hardcoded shared doc store extensions, etc
}
{code}

Also various codec methods take a Directory parameter, but its inconsistent
what this Directory is in the case of CFS: for some parts of the index its
the CFS directory, for others (deletes, separate norms) its not.

I wonder if instead we could restructure this so that SegmentInfo.files() logic is:
{code}
clearCache()
Codec.files()
{code}

and so that Codec is instead responsible.

instead Codec.files logic by default would do the if (compoundFile) thing, and
Lucene3x codec itself would only have the if (sharedDocStores) thing, and any
part of the codec that wants to put stuff always outside of CFS (e.g. Lucene3x separate norms, deletes) 
could just use SegmentInfo.dir. Directory parameters in the case of CFS would always
consistently be the CFSDirectory.

I haven't totally tested if this will work but there is definitely some cleanups 
we can do either way, and I think it would be a good step to try to clean this up
and simplify it.
"
1,"TestGrouping failure{noformat}
ant test -Dtestcase=TestGrouping -Dtestmethod=testRandom -Dtests.seed=295cdb78b4a442d4:-4c5d64ef4d698c27:-425d4c1eb87211ba
{noformat}

fails with this on current trunk:

{noformat}

    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestGrouping -Dtestmethod=testRandom -Dtests.seed=295cdb78b4a442d4:-4c5d64ef4d698c27:-425d4c1eb87211ba
    [junit] NOTE: test params are: codec=RandomCodecProvider: {id=MockRandom, content=MockSep, sort2=SimpleText, groupend=Pulsing(freqCutoff=3 minBlockSize=65 maxBlockSize=132), sort1=Memory, group=Memory}, sim=RandomSimilarityProvider(queryNorm=true,coord=false): {id=DFR I(F)L2, content=DFR BeL3(800.0), sort2=DFR GL3(800.0), groupend=DFR G2, sort1=DFR GB3(800.0), group=LM Jelinek-Mercer(0.700000)}, locale=zh_TW, timezone=America/Indiana/Indianapolis
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestGrouping]
    [junit] NOTE: Linux 2.6.33.6-147.fc13.x86_64 amd64/Sun Microsystems Inc. 1.6.0_21 (64-bit)/cpus=24,threads=1,free=143246344,total=281804800
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testRandom(org.apache.lucene.search.grouping.TestGrouping):	FAILED
    [junit] expected:<11> but was:<7>
    [junit] junit.framework.AssertionFailedError: expected:<11> but was:<7>
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:148)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:50)
    [junit] 	at org.apache.lucene.search.grouping.TestGrouping.assertEquals(TestGrouping.java:980)
    [junit] 	at org.apache.lucene.search.grouping.TestGrouping.testRandom(TestGrouping.java:865)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$2$1.evaluate(LuceneTestCase.java:611)
    [junit] 
    [junit] 
{noformat}

I dug for a while... the test is a bit sneaky because it compares sorted docs (by score) across 2 indexes.  Index #1 has no deletions; Index #2 has same docs, but organized into doc blocks by group, and has some deletions.  In theory (I think) even though the deletions will cause scores to differ across the two indices, it should not alter the sort order of the docs.  Here is the explain output of the docs that sorted differently:

{noformat}
#1: top hit in the ""has deletes doc-block"" index (id=239):

explain: 2.394486 = (MATCH) weight(content:real1 in 292)
[DFRSimilarity], result of:
 2.394486 = score(DFRSimilarity, doc=292, freq=1.0), computed from:
   1.0 = termFreq=1
   41.944084 = NormalizationH3, computed from:
     1.0 = tf
     5.3102274 = avgFieldLength
     2.56 = len
   102.829 = BasicModelBE, computed from:
     41.944084 = tfn
     880.0 = numberOfDocuments
     239.0 = totalTermFreq
   0.023286095 = AfterEffectL, computed from:
     41.944084 = tfn


#2: hit in the ""no deletes normal index"" (id=229)

ID=229 explain=2.382285 = (MATCH) weight(content:real1 in 225)
[DFRSimilarity], result of:
 2.382285 = score(DFRSimilarity, doc=225, freq=1.0), computed from:
   1.0 = termFreq=1
   41.765594 = NormalizationH3, computed from:
     1.0 = tf
     5.3218827 = avgFieldLength
     10.24 = len
   101.879845 = BasicModelBE, computed from:
     41.765594 = tfn
     786.0 = numberOfDocuments
     215.0 = totalTermFreq
   0.023383282 = AfterEffectL, computed from:
     41.765594 = tfn

Then I went and called explain on the ""no deletes normal index"" for
the top doc (id=239):

explain: 2.3822558 = (MATCH) weight(content:real1 in 17)
[DFRSimilarity], result of:
 2.3822558 = score(DFRSimilarity, doc=17, freq=1.0), computed from:
   1.0 = termFreq=1
   42.165264 = NormalizationH3, computed from:
     1.0 = tf
     5.3218827 = avgFieldLength
     2.56 = len
   102.8307 = BasicModelBE, computed from:
     42.165264 = tfn
     786.0 = numberOfDocuments
     215.0 = totalTermFreq
   0.023166776 = AfterEffectL, computed from:
     42.165264 = tfn
{noformat}"
1,"EventFilter misses Events for same NodetypeIf an ObservationListener registers with a NodeType-filter, 
it only gets informed about events on Sub-NodeTypes of the ones specified in the filter but not on the NodeType itself.

Example:
========
ObservationManager om = wsp.getObservationManager();
om.addEventListener(listener, Event.PROPERTY_ADDED, ""/"", true, null, new String[]{""nt:unstructured""}, true);

would receive notifications on nodes of type ""rep:root"", which is based on ""nt:unstructured"" but not of ""nt:unstructured""


"
1,"Deadlock inside XASession on WeblogicIn one of our client deployments on WebLogic 9.2 we observed JackRabbit sessions going stale in a load test. This was observed against release 1.6.1 (to which we migrated due to concurrency related issues JCR-2081 and JCR-2237). Same effect with 2.0.0.
 
I could finally reproduce this issue locally. And it seems to boil down to WLS invoking the sequence of <prepare> ... <release> ... <commit> on one XA session from multiple threads, as it seems breaking assumptions of the thread-bound java.util.concurrent-RWLock based DefaultISMLocking class.
Effectively the setActiveXid(..) method on DefaultISMLocking$RWLock fails as the old active XID was not yet cleared. With the result of more and more sessions deadlocking in below's invocation stack.

{code}
""[ACTIVE] ExecuteThread: '27' for queue: 'weblogic.kernel.Default (self-tuning)'"" daemon prio=1 tid=0x33fc3ec0 nid=0x2324 in Object.wait() [0x2156a000..0x2156beb0] at java.lang.Object.wait(Native Method) - waiting on <0x68a54698> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock) at java.lang.Object.wait(Object.java:474) at EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock.acquire(Unknown Source) - locked <0x68a54698> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock) at org.apache.jackrabbit.core.state.DefaultISMLocking$1.<init>(DefaultISMLocking.java:64) at org.apache.jackrabbit.core.state.DefaultISMLocking.acquireWriteLock(DefaultISMLocking.java:61) at org.apache.jackrabbit.core.version.AbstractVersionManager.acquireWriteLock(AbstractVersionManager.java:146) at org.apache.jackrabbit.core.version.XAVersionManager$1.prepare(XAVersionManager.java:562) at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:154) - locked <0x6dc2ad88> (a org.apache.jackrabbit.core.TransactionContext) at org.apache.jackrabbit.core.XASessionImpl.prepare(XASessionImpl.java:331) at org.apache.jackrabbit.jca.TransactionBoundXAResource.prepare(TransactionBoundXAResource.java:68) at weblogic.connector.security.layer.AdapterLayer.prepare(AdapterLayer.java:397) at weblogic.connector.transaction.outbound.XAWrapper.prepare(XAWrapper.java:297) at weblogic.transaction.internal.XAServerResourceInfo.prepare(XAServerResourceInfo.java:1276) at weblogic.transaction.internal.XAServerResourceInfo.prepare(XAServerResourceInfo.java:499) at weblogic.transaction.internal.ServerSCInfo$1.execute(ServerSCInfo.java:335) at weblogic.kernel.Kernel.executeIfIdle(Kernel.java:243) at weblogic.transaction.internal.ServerSCInfo.startPrepare(ServerSCInfo.java:326) at weblogic.transaction.internal.ServerTransactionImpl.localPrepare(ServerTransactionImpl.java:2516) at weblogic.transaction.internal.ServerTransactionImpl.globalPrepare(ServerTransactionImpl.java:2211) at weblogic.transaction.internal.ServerTransactionImpl.internalCommit(ServerTransactionImpl.java:266) at weblogic.transaction.internal.ServerTransactionImpl.commit(ServerTransactionImpl.java:227) at weblogic.transaction.internal.TransactionManagerImpl.commit(TransactionManagerImpl.java:283) at org.springframework.transaction.jta.JtaTransactionManager.doCommit(JtaTransactionManager.java:1028) at org.springframework.transaction.support.AbstractPlatformTransactionManager.processCommit(AbstractPlatformTransactionManager.java:709) at org.springframework.transaction.support.AbstractPlatformTransactionManager.commit(AbstractPlatformTransactionManager.java:678)
{code}"
1,"Workspace is shut down while creating initial indexThis only happens when a maxIdleTime is configured for the workspaces in the repository.xml and the workspace to index is not the default workspace.

The idle check considers a workspace as idle when there only a system session is open and the configured idle time elapsed. This is also the case when the workspace is initializing.

The repository should either check if a workspace is still initializing or we need to move the search manager initialization into the WorkspaceInfo.doInitialize() method.

"
0,"Database persistence managers: log database and driver name and versionDatabase related problems can be solved more easily when we know what database and driver version is used. Sometimes multiple database drivers are installed in an app server environment, and the user may not even know it. 

Currently the driver class name is logged. I suggest to log the driver and database name and version as well."
1,"ClassCastException bei unregisterNodeTypeI have a NodeType with various childnodes which I want to unregister. If I call:

    
      NodeTypeManager ndmg = session.getWorkspace().getNodeTypeManager();
      NodeTypeRegistry ntReg = ((NodeTypeManagerImpl) ndmg).getNodeTypeRegistry();
      ntReg.unregisterNodeType(new QName(""testURI"",""Page""));


I get a 

java.lang.ClassCastException
 at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.getDependentNodeTypes(NodeTypeRegistry.java:1242)
 at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.unregisterNodeType(NodeTypeRegistry.java:1120)
 at de.freaquac.test.JCRTest.main(JCRTest.java:80)

It looks to me like there are QNames in the Iterator but NodeTyeDefs are expected.
"
0,Fix SnowballAnalyzer casing behavior for Turkish LanguageLUCENE-2102 added a new TokenFilter to handle Turkish unique casing behavior correctly. We should fix the casing behavior in SnowballAnalyzer too as it supports a TurkishStemmer.
0,"Deployment of webdav servlet on Jboss problem - loggingTested two different installs of JBoss to verify problem is not related to a specific version. There is a problem with the jackrabbit-server.war when deploying on jboss.  Here are the details during deployment:

=======================
13:20:48,654 INFO  [TomcatDeployer] deploy, ctxPath=/jackrabbit-server, warUrl=.../deploy/jackrabbit-server.war/
13:20:48,857 INFO  [STDOUT] log4j:ERROR A ""org.jboss.logging.util.OnlyOnceErrorHandler"" object is not assignable to a ""o rg.apache.log4j.spi.ErrorHandler"" variable.
13:20:48,857 INFO  [STDOUT] log4j:ERROR The class ""org.apache.log4j.spi.ErrorHandler"" was loaded by
13:20:48,857 INFO  [STDOUT] log4j:ERROR [WebappClassLoader
  delegate: false
  repositories:
    /WEB-INF/classes/
----------> Parent Classloader:
java.net.FactoryURLClassLoader@19d277e
] whereas object of type
13:20:48,857 INFO  [STDOUT] log4j:ERROR ""org.jboss.logging.util.OnlyOnceErrorHandler"" was loaded by [org.jboss.system.se rver.NoAnnotationURLClassLoader@ab95e6].
13:20:48,904 INFO  [STDOUT] log4j:ERROR Could not create an Appender. Reported error follows.
13:20:48,904 INFO  [STDOUT] java.lang.ClassCastException: org.jboss.logging.appender.DailyRollingFileAppender
13:20:48,904 INFO  [STDOUT]     at org.apache.log4j.xml.DOMConfigurator.parseAppender(DOMConfigurator.java:165)
13:20:48,904 INFO  [STDOUT]     at org.apache.log4j.xml.DOMConfigurator.findAppenderByName(DOMConfigurator.java:140)
13:20:48,904 INFO  [STDOUT]     at org.apache.log4j.xml.DOMConfigurator.findAppenderByReference(DOMConfigurator.java:153
)
13:20:48,904 INFO  [STDOUT]     at org.apache.log4j.xml.DOMConfigurator.parseChildrenOfLoggerElement(DOMConfigurator.jav
a:415)
13:20:48,904 INFO  [STDOUT]     at org.apache.log4j.xml.DOMConfigurator.parseRoot(DOMConfigurator.java:384)
13:20:48,904 INFO  [STDOUT]     at org.apache.log4j.xml.DOMConfigurator.parse(DOMConfigurator.java:783)
13:20:48,904 INFO  [STDOUT]     at org.apache.log4j.xml.DOMConfigurator.doConfigure(DOMConfigurator.java:666)
13:20:48,904 INFO  [STDOUT]     at org.apache.log4j.xml.DOMConfigurator.doConfigure(DOMConfigurator.java:616)
13:20:48,904 INFO  [STDOUT]     at org.apache.log4j.xml.DOMConfigurator.doConfigure(DOMConfigurator.java:602)
13:20:48,904 INFO  [STDOUT]     at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:460)

13:20:48,904 INFO  [STDOUT]     at org.apache.log4j.LogManager.<clinit>(LogManager.java:113)
13:20:48,904 INFO  [STDOUT]     at org.apache.log4j.xml.DOMConfigurator.configure(DOMConfigurator.java:543)
13:20:48,904 INFO  [STDOUT]     at org.apache.jackrabbit.j2ee.LoggingServlet.configureXML(LoggingServlet.java:148)
13:20:48,904 INFO  [STDOUT]     at org.apache.jackrabbit.j2ee.LoggingServlet.configure(LoggingServlet.java:115)
13:20:48,904 INFO  [STDOUT]     at org.apache.jackrabbit.j2ee.LoggingServlet.init(LoggingServlet.java:86)
======================
 
Unlike most logging problems, this is having an impact during runtime - when trying to do DASL searches, will return a 500 error as the server was unable to log correctly.
"
1,"Removed version is not invalidatedwhen a version is removed, it's internal represenation is not evicted from the cache. this can leed to unexpected behaviours. XATest.removeVersion() tests this. this also happens in a non-transactional environment."
0,"Add QueryParser.newFieldQueryNote: this patch changes no behavior, just makes QP more subclassable.

Currently we have Query getFieldQuery(String field, String queryText, boolean quoted)
This contains very hairy methods for producing a query from QP's analyzer.

I propose we factor this into newFieldQuery(Analyzer analyzer, String field, String queryText, boolean quoted)
Then getFieldQuery just calls newFieldQuery(this.analyzer, field, queryText, quoted);

The reasoning is: it can be quite useful to consider the double quote as more than phrases, but a ""more exact"" search.
In the case the user quoted the terms, you might want to analyze the text with an alternate analyzer that:
doesn't produce synonyms, doesnt decompose compounds, doesn't use WordDelimiterFilter 
(you would need to be using preserveOriginal=true at index time for the WDF one), etc etc.

This is similar to the way google's double quote operator works, its not defined as phrase but ""this exact wording or phrase"".
For example compare results to a query of tests versus ""tests"".

Currently you can do this without heavy code duplication, but really only if you make a separate field (which is wasteful),
and make your custom QP lie about its field... in the examples I listed above you can do this with a single field, yet still
have a more exact phrase search.
"
1,"Cookies with ',' in the value string is not parsed correctly in some casesThis version extracts the ""Set-Cookie"" statementes of the following
HTTP response headers incorrectly.

The HTTP response is sent when executing GET method on --->
""http://my.taishinbank.com.tw/netbank/nbslogin.asp?
subFunID=https://my.taishinbank.com.tw/netbank/AccountQuery/QAccbyID.asp""

After the HttpClient extracts Set-Cookie from the response, it generates a wrong
cookie statement---->

  [INFO] wire - ->> ""Cookie: $Version=0; _mysite=520163500; 1027657033=null; 
   1027787539=null; 0=null; $Path=/; cata=11; $Path=/;   
   ASPSESSIONIDGGGQQXEU=ADLCDAGAJLKEBJEKBOMMAMOB; 
   $Path=/""

, where it shall 
be ""_mysite=520163500,1027657033,1027787539,1027787539,0;"" ,but 
not ""_mysite=520163500; 1027657033=null; 1027787539=null; 0=null;""
 

Thank you"
1,"TestStressIndexing has intermittent failuresSee http://www.gossamer-threads.com/lists/lucene/java-dev/55092 copied below:

 OK, I have seen this twice in the last two days:
Testsuite: org.apache.lucene.index.TestStressIndexing
[junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 18.58
sec
[junit]
[junit] ------------- Standard Output ---------------
[junit] java.lang.NullPointerException
[junit] at
org.apache.lucene.store.RAMInputStream.readByte(RAMInputStream.java:67)
[junit] at
org.apache.lucene.store.IndexInput.readInt(IndexInput.java:66)
[junit] at org.apache.lucene.index.SegmentInfos
$FindSegmentsFile.run(SegmentInfos.java:544)
[junit] at
org
.apache
.lucene.index.DirectoryIndexReader.open(DirectoryIndexReader.java:63)
[junit] at
org.apache.lucene.index.IndexReader.open(IndexReader.java:209)
[junit] at
org.apache.lucene.index.IndexReader.open(IndexReader.java:192)
[junit] at
org.apache.lucene.search.IndexSearcher.<init>(IndexSearcher.java:56)
[junit] at org.apache.lucene.index.TestStressIndexing
$SearcherThread.doWork(TestStressIndexing.java:111)
[junit] at org.apache.lucene.index.TestStressIndexing
$TimedThread.run(TestStressIndexing.java:55)
[junit] ------------- ---------------- ---------------
[junit] Testcase:
testStressIndexAndSearching
(org.apache.lucene.index.TestStressIndexing): FAILED
[junit] hit unexpected exception in search1
[junit] junit.framework.AssertionFailedError: hit unexpected
exception in search1
[junit] at
org
.apache
.lucene.index.TestStressIndexing.runStressTest(TestStressIndexing.java:
159)
[junit] at
org
.apache
.lucene
.index
.TestStressIndexing
.testStressIndexAndSearching(TestStressIndexing.java:187)
[junit]
[junit]
[junit] Test org.apache.lucene.index.TestStressIndexing FAILED

Subsequent runs have, however passed. Has anyone else hit this on
trunk?

I am running using ""ant clean test""

I'm on a Mac Pro 4 core, 4GB machine, if that helps at all. Not sure
how to reproduce at this point, but strikes me as a threading issue.
Oh joy!

I'll try to investigate more tomorrow to see if I can dream up a test
case.

-Grant 

"
1,"MMapDirectory chunking is buggyMMapDirectory uses chunking with MultiMMapIndexInput.
 
Because Java's ByteBuffer uses an int to address the
values, it's necessary to access a file >
Integer.MAX_VALUE in size using multiple byte buffers.

But i noticed from the clover report the entire MultiMMapIndexInput class is completely untested: no surprise since all tests make tiny indexes.
"
0,Wrong link in javadoc of QNodeTypeDefinitionThe javadoc of QNodeTypeDefinition links to javax.jcr.nodetype.NodeDefinition instead of javax.jcr.nodetype.NodeType
0,"Add API for selective bundle consistency check (Jackrabbit-specific)Add a jackrabbit-specific API for doing a selective consistencyCheck, ie. on single nodes. The current entire-workspace check can be very slow if there workspace is large enough. Also it should be easy to write a tool to invoke that feature programmatically rather than by configuration + restart (see below).

Existing Implementation:
The current bundle consistencyCheck feature is enabled by setting a bundle PM parameter and restarting Jackrabbit, it will then run upon startup (see JCR-972 for the only issue regarding bundle consistency check). This check looks for broken parent-child relationships, ie. it will remove any child node entries that reference non-existing parent nodes. For non-existing parent UUIDs and other problems in bundles it will log those.

Outlook:
An advanced consistencyCheck could also check for non-existing version nodes and vice-versa (see JCR-630), but this is not the focus of this issue and could be a later addition to the API."
0,"JSR 283: adopt CND syntax changesthe CND syntax has changed from Public Review Draft to Public Final Draft.

old and new syntax are incompatible."
0,"Demo: DeleteFiles doesn't delete files by their path namesIt appears that delete(term) fails to delete the last document containing term, which
for a unique match means that you can't remove an individual document.

Code attempting to remove document with specific 'path' (slightly modified version of demo code):

Directory directory = FSDirectory.getDirectory(""index"", false);
IndexReader reader = IndexReader.open(directory);
Term term = new Term(""path"", args[0]);  // path passed via command line arg
int deleted = reader.delete(term);
reader.close();
directory.close();

System.out.println(""deleted "" + deleted + "" documents containing "" + term);

Executing this always returns ""deleted 0 documents containing <path entered>""

In IndexReader.java, delete() has:

public final int delete(Term term) throws IOException {
  TermDocs docs = termDocs(term);
  if (docs == null) return 0;
  int n = 0;
  try {
    while (docs.next()) {
      delete(docs.doc());
      n++;
    }
  } finally {
    docs.close();
  }
  return n;
}

It appears that docs.next() always returns false when there is only one doc, hence
delete() is never called and 0 is always returned.  I assume that this also means that
if there are multiple matches, the last doc will not be deleted either, but I have not tested
that.

I modified the code as follows:

    boolean more = true;
    try {
      docs.next();
      while (more) {
        delete(docs.doc());
        n++;
        more = docs.next();
      }
    } finally {
      docs.close();
    }

and then it worked as expected (at least attempts to delete a single document from the
index succeeded whereas previously they did not)."
1,"Lock expires almost immediatelyWhen a timeoutHint other than Long.MAX_VALUE is given to the javax.jcr.lock.LockManager API:

   lock(String absPath, boolean isDeep, boolean isSessionScoped, long timeoutHint, String ownerInfo)

a timeoutTime in seconds will be computed as follows (o.a.j.core.lock.LockInfo#updateTimeoutTime):

   long now = (System.currentTimeMillis() + 999) / 1000; // round up
   this.timeoutTime = now + timeoutHint;

the TimeoutHandler in o.a.j.core.lock.LockManagerImpl running every second will then check whether the timeout has expired (o.a.j.core.lock.LockInfo#isExpired):

    public boolean isExpired() {
        return timeoutTime != Long.MAX_VALUE
            && timeoutTime * 1000 > System.currentTimeMillis();
    }

Obviously, the latter condition is true from the very beginning. Replacing '>' with '<' or '<=' should do the trick."
0,"Initializing SeededSecureRandom may be slowFor systems where reading from /dev/random is very slow (so that the alternative seed algorithm is used), initializing the org.apache.jackrabbit.core.id.SeededSecureRandom singleton may be very slow, because it is not synchronized. Each thread that calls SeededSecureRandom.getInstance() will wait up to 1 second until the singleton is initialized.

At the same time, I would like to add more entropy to the alternative seed algorithm.
"
0,"Remove remaining @author references$ find . -name \*.java | xargs grep '@author' | cut -d':' -f1 | xargs perl -pi -e 's/ \@author.*//'
"
0,"DocId.UUIDDocId should not have a string attr uuidAfter JCR-1213 will be solved, lots of DocId.UUIDDocId can be cached, and not being cleaned after every gc(). The number of cached UUIDDocId can grow very large, depending on the size of the repository.  Therefor, instead of storing the private String uuid; we can make it more memory efficient by storing 2 long's, the lsb and msb of the uuid.  Storing 1.000.000 of parent UUIDDocId might differ about 100Mb of memory. 

I even did test by removing the entire uuid string, and not use msb or lsb, because, when everything works properly (with references to index reader segments (See JCR-1213)), the uuid is never needed again: in 

UUIDDocId getDocumentNumber(IndexReader reader) throws IOException {

we could set uuid = null just before the return. It works perfectly well, because when an index reader is recreated, the CachingIndexReader will be recreated, hence DocId[] parents will be recreated. 

So, IMO, I think we might be able to remove the uuid entirely when the docNumber is found in DocId.UUIDDocId (obviously after JCR-1213)

WDOT?

"
0,"Various inner classes maintain references to owning class for no reasonVarious inner classes maintain references to their owning classes for no reason, as they are independent classes. This issue will change these classes to be static inner classes, so that their footprint decreases, they ease gc work, and potentially reduce the lifetime of the owning classes if they outlive their owner."
1,I/O exceptions can cause loss of buffered deletesSome I/O exceptions that result in segmentInfos rollback operations can cause buffered deletes that existed before the rollback creation point to be incorrectly lost when the IOException triggers a rollback.
1,"Deadlock case in IndexWriter on exception just before flushIf a document hits a non-aborting exception, eg something goes wrong
in tokenStream.next(), and, that document had triggered a flush
(due to RAM or doc count) then DocumentsWriter will deadlock because
that thread marks the flush as pending but fails to clear it on
exception.

I have a simple test case showing this, and a fix fixing it."
0,Cut Norms over to DocValuessince IR is now fully R/O and norms are inside codecs we can cut over to use a IDV impl for writing norms. LUCENE-3606 has some [ideas|https://issues.apache.org/jira/browse/LUCENE-3606?focusedCommentId=13160559&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13160559] about how this could be implemented
0,"Implement ""ignoreCookies"" CookieSpecIt would be useful to Implement an ""ignoreCookies"" CookieSpec, as was done in Commons HC 3.1

This should be registered by DefaultHttpClient.createCookieSpecRegistry().

Patch to follow."
0,"Child Axis support in order by clauseHi, 

since child axis is supported in XPath predicates, it would be nice to support it in order by clause as well

Queries of type

//element(*, type) [ foo/@bar ]  order by foo/@bar asc

can become very useful

BR, 

Savvas"
0,"Provide a Method getCredentialsProvider to the SimpleWebdavServletIt will be useful to provide a easy way to change the default CredentialsProvider (BasicCredentialsProvider) when the SessionProvider will be created.
It makes sense to let the SessionProvider return a other CredentialsProvider so that no BasicAuthentication wil be prompt.
thanks
claus"
1,"ZombieHierarchyManager can return wrong child node entries for replaced nodesThe ZombieHierarchyManager currently implements the two getChildNodeEntry methods like this:

1) look up child node in old, overlayed state, which might contain removed child nodes
2) if not found, ask the super implementation (ie. get the child node from the up-to-date list)

The purpose of the ZombieHM is to be able to return removed item ids from the attic. However, the behavior above is IMO wrong, as it should first find an existing child node with the given name (or id):

1) look up child node in super implementation (ie. get the child node from the up-to-date list)
2) if not found, look in the old, overlayed state if it might have been removed

I was able to reproduce this issue when replacing a node (but note the custom access manager in 1.4.x used as explained below): create /replaced/subnode structure, save the session, remove the replaced node and add /replaced and then /replaced/subnode again:

        Node rootNode = session.getRootNode();
        
        // 1. create structure /replaced/subnode
        Node test = rootNode.addNode(""replaced"", NT);
        test.addNode(""subnode"", NT);
        // 2. persist changes
        session.save();

        // 3. remove node and recreate it
        test.remove();
        test = rootNode.addNode(""replaced"", NT);
        
        // 4. create previous child with same name
        test.addNode(""subnode"", NT);
        
        // 5. => gives exception
        test.getNode(""subnode"").getNodes();

To complicate things further, this was only triggered by a custom access manager, and all based upon Jackrabbit 1.4.x. Back then (pre-1.5 and new security stuff era), the access manager would get a ZombieHM as its hierarchy manager. If its implementation called resolvePath() on the HM for checking read-access in the final getNodes() call, where the tree will be traversed using the getChildNdeEntry(NodeState, Name, int) method, it would get the old node id and hence fail if it would try to retrieve it from the real item state manager.

Thus with a Jackrabbit >= 1.5 and 2.0 the above code will work fine, because the ZombieHM is not used.

However, we might want to fix it for 1.4.x and also check the other uses of the ZombieHM in the current trunk, which I couldn't test. These are (explicit and implicit): ChangeLogBasedHierarchyMgr, SessionItemStateManager.getDescendantTransientItemStates(NodeId), ItemImpl.validateTransientItems(Iterable<ItemState>, Iterable<ItemState>) and SessionItemStateManager.getDescendantTransientItemStatesInAttic(NodeId).
"
1,"Wildcard query with no wildcard characters in the term throws StringIndexOutOfBounds exception
Query q1 = new WildcardQuery(new Term(""Text"", ""a""));
Hits hits = searcher.search(q1);


Caught Exception
java.lang.StringIndexOutOfBoundsException : String index out of range: -1
    at java.lang.String.substring(Unknown Source)
    at org.apache.lucene.search.WildcardTermEnum.<init>(WildcardTermEnum.java:65)
    at org.apache.lucene.search.WildcardQuery.getEnum (WildcardQuery.java:38)
    at org.apache.lucene.search.MultiTermQuery.rewrite(MultiTermQuery.java:54)
    at org.apache.lucene.search.IndexSearcher.rewrite(IndexSearcher.java:137)
    at org.apache.lucene.search.Query.weight (Query.java:92)
    at org.apache.lucene.search.Hits.<init>(Hits.java:41)
    at org.apache.lucene.search.Searcher.search(Searcher.java:44)
    at org.apache.lucene.search.Searcher.search(Searcher.java:36)
    at QuickTest.main(QuickTest.java:45)


From Erik Hatcher

Feel free to log this as a bug report in our JIRA issue tracker.  It
seems like a reasonable change to make, such that a WildcardQuery
without a wildcard character would behave like TermQuery."
1,"Http Authentication with invalid credentials causes infinite loopAt HttpMethodBase(460), a break statement is executed only if
log.isInfoEnabled(). The break statement needs to be moved outside of the if
statement so that it breaks if realms already contains foo. Patch submitted on
mailing list as per Apache site guidelines."
0,"Add reopen(IndexCommit) methods to IndexReaderAdd reopen(IndexCommit) methods to IndexReader to be able to reopen an index on any previously saved commit points with all advantages of LUCENE-1483.

Similar to open(IndexCommit) & company available in 2.4.0.
"
1,"OverlappingFileLockException with JRE 1.6Per email discussion:
On Mon, 2007-02-26 at 10:26 +0100, Marcel Reutegger wrote:
> > just my 2c, I didn't really investigated this issue in more detail...
> >
> > according to the javadoc of FileChannel.tryLock() the
> > OverlappingFileLockException is thrown if the JVM already holds a lock on the
> > channel.
> >
> > in contrast, the current check in the repository startup method primarily
> > focuses on the situation where *two* JVMs start a repository on the same home
> > directory.
> >
> > I'd say the OverlappingFileLockException is thrown because two repository
> > instances are startup within the *same* JVM using the same repository home
> > directory.
> >
> > I suggest we add a catch clause, which also covers OverlappingFileLockException
> > in addition to IOException.
> >
> > regards
> >   marcel
> >
> > Stefan Guggisberg wrote:
> > > btw, afaik OverlappingFileLockException is only thrown on linux,
> > > FileChannel#getLock on windows e.g. returns null in the same situation.
> > >
> > > you might want to test on a different platform to further isolate the
> > > issue.
> > > you could also place a breakpoint at the top of the
> > > RepositoryImpl#acquireRepositoryLock
> > > method, step through the code, verify the contents of your fs etc.
> >
>


=== Original email

On 2/19/07, Patrick Haggood <codezilla@> wrote:
I'm using Linux, Sun Java 6 and Jackrabbit 1.3 with Derby persistance.
I have a putNode(object) function that's giving the above error in unit
tests.  It always fails after the second update, even when I swap tests
(i.e. save user doc then save user).  Prior to each test, I delete the
repository directory.

Do I need to set explicit locks before/after each session.save()?

*********** Unit Test
DBConn dbc;

    public SessionUtilTest(String testName) {
        super(testName);
        dbc = new DBConn();
    }

// Note - putUser and putDocument both use putNode after determining
which rootnode will be used

   /**
     * Test of putUnityUser method, of class unityjsr170.jr.SessionUtil.
     */
    public void testPutUnityUser() {
        System.out.println(""putUnityUser"");
        UnityUser usr = usr1;
        SessionUtil instance = dbc.getSutil();
        String result = instance.putUnityUser(usr1);
        assertNotNull(result);
        usr = (UnityUser) instance.getUnityUserByID(result);
        assertEquals(usr1.getName(),usr.getName());
    }
       
    /**
     * Test of putUnityDocument method, of class
unityjsr170.jr.SessionUtil.
     */
    public void testPutUnityDocument() {
        System.out.println(""putUnityDocument"");
        UnityDocument udoc = adr1;
        SessionUtil instance = dbc.getSutil();
        String result = instance.putUnityDocument(udoc);   <---- File
Lock Error
        assertNotNull(result);
        udoc = (UnityDocument) instance.getUnityDocumentByID(result);
        assertEquals(adr1.getName(),udoc.getName());
    }


********* Here's where I setup my repository connection

    public DBConn(){
        sutil = null;
        try {
            rp = new TransientRepository();
            sutil= new SessionUtil(rp);
        } catch (IOException ex) {
            ex.printStackTrace();
        }
    }
    
    public void shutdown(){
        sutil.closeAll();
    }
    
    public SessionUtil getSutil(){
        return sutil;
    }

****************  SessionUtil

    public SessionUtil(Repository rp){
        try {
            session = rp.login(new
SimpleCredentials(""username"",""password"".toCharArray()));
            
        } catch (LoginException ex) {
            ex.printStackTrace();
        } catch (RepositoryException ex) {
            ex.printStackTrace();
        } 
        
    }
    
    public void closeAll(){
        try {
            session.logout();
        } catch (Exception ex) {
            ex.printStackTrace();
            System.out.println(""Error closing repository"");
        }
    }
    
 // Put a node on the tree under the root node, return the uuid of the
new or updated node
    private String putNode(String nodetype, UnityBaseObject ubo){
        String resultuuid =null;
        String uname = ubo.getName();
        String utype = ubo.getType();
        String objectuid = ubo.getId();
        Node pnode; //  node to add or update
        Session ses = null;
        try {
            ses = getSession();
            // Does updateable node already have node Id?
            if (objectuid==null) {
                Node rn = ses.getRootNode();
                pnode = rn.addNode(utype);
                pnode.addMixin(""mix:referenceable"");
            } else{
                // grab existing node by uuid
                pnode = ses.getNodeByUUID(objectuid);
            }
            // Did we get an updateable node?
            if (pnode!=null){
                ubo.setId(pnode.getUUID());
                String unityXML =
utrans.getXMLStringFromUnityBaseObject(ubo);
                // update all the properties
                pnode.setProperty(""name"",ubo.getName());
                pnode.setProperty(""type"",ubo.getType());
                pnode.setProperty(""xmldata"",unityXML);
                ses.save();
                resultuuid = ubo.getId();
            }
        } catch(Exception e) {
            e.printStackTrace();
        } 
        return resultuuid;
    }

    private Session getSession(){
        return session;
    }
    

************  repository.xml

 <Workspace name=""${wsp.name}"">
        <FileSystem
class=""org.apache.jackrabbit.core.fs.local.LocalFileSystem"">
            <param name=""path"" value=""${wsp.home}""/>
        </FileSystem>
        <PersistenceManager
class=""org.apache.jackrabbit.core.state.db.DerbyPersistenceManager"">
            <param name=""url"" value=""jdbc:derby:
${wsp.home}/db;create=true""/>
            <param name=""schemaObjectPrefix"" value=""${wsp.name}_""/>
        </PersistenceManager>
        <SearchIndex
class=""org.apache.jackrabbit.core.query.lucene.SearchIndex"">
            <param name=""path"" value=""${wsp.home}/index""/>
            <param name=""useCompoundFile"" value=""true""/>
            <param name=""minMergeDocs"" value=""100""/>
            <param name=""volatileIdleTime"" value=""3""/>
            <param name=""maxMergeDocs"" value=""100000""/>
            <param name=""mergeFactor"" value=""10""/>
            <param name=""bufferSize"" value=""10""/>
            <param name=""cacheSize"" value=""1000""/>
            <param name=""forceConsistencyCheck"" value=""false""/>
            <param name=""autoRepair"" value=""true""/>
            <param name=""analyzer""
value=""org.apache.lucene.analysis.standard.StandardAnalyzer""/>
        </SearchIndex>
    </Workspace>

"
0,"Enable flexible scoringThis is a first step (nowhere near committable!), implementing the
design iterated to in the recent ""Baby steps towards making Lucene's
scoring more flexible"" java-dev thread.

The idea is (if you turn it on for your Field; it's off by default) to
store full stats in the index, into a new _X.sts file, per doc (X
field) in the index.

And then have FieldSimilarityProvider impls that compute doc's boost
bytes (norms) from these stats.

The patch is able to index the stats, merge them when segments are
merged, and provides an iterator-only API.  It also has starting point
for per-field Sims that use the stats iterator API to compute boost
bytes.  But it's not at all tied into actual searching!  There's still
tons left to do, eg, how does one configure via Field/FieldType which
stats one wants indexed.

All tests pass, and I added one new TestStats unit test.

The stats I record now are:

  - field's boost

  - field's unique term count (a b c a a b --> 3)

  - field's total term count (a b c a a b --> 6)

  - total term count per-term (sum of total term count for all docs
    that have this term)

Still need at least the total term count for each field.
"
0,"[PATCH] tests use 12 for month which is invalidtests create calendar with 12 as a month, which is invalid. December is 11, so use Calendar.DECEMBER instead. - patch fixes this."
1,"Intermittent failure in TestIndexWriter.testCommitThreadSafetyMark's while(1) hudson box found this failure (and I can repro it too):

{noformat}
Error Message

MockRAMDirectory: cannot close: there are still open files: {_1m.cfs=1,
_1k.cfs=1, _14.cfs=1, _1g.cfs=1, _1h.cfs=1, _1f.cfs=1, _1n.cfs=1,
_1i.cfs=1, _1j.cfs=1, _1l.cfs=1}

Stacktrace

java.lang.RuntimeException: MockRAMDirectory: cannot close: there are
still open files: {_1m.cfs=1, _1k.cfs=1, _14.cfs=1, _1g.cfs=1,
_1h.cfs=1, _1f.cfs=1, _1n.cfs=1, _1i.cfs=1, _1j.cfs=1, _1l.cfs=1}
       at
org.apache.lucene.store.MockRAMDirectory.close(MockRAMDirectory.java:282)
       at
org.apache.lucene.index.TestIndexWriter.testCommitThreadSafety(TestIndexWriter.java:4616)
       at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:328)

Standard Output

NOTE: random codec of testcase 'testCommitThreadSafety' was: Sep

Standard Error

The following exceptions were thrown by threads:
*** Thread: Thread-1784 ***
java.lang.RuntimeException: junit.framework.AssertionFailedError: null
       at org.apache.lucene.index.TestIndexWriter$9.run(TestIndexWriter.java:4606)
Caused by: junit.framework.AssertionFailedError: null
       at junit.framework.Assert.fail(Assert.java:47)
       at junit.framework.Assert.assertTrue(Assert.java:20)
       at junit.framework.Assert.assertTrue(Assert.java:27)
       at org.apache.lucene.index.TestIndexWriter$9.run(TestIndexWriter.java:4597)
{noformat}"
1,"Unmatched right parentheses truncates queryThe query processor truncates a query when right parentheses are unmatched.
E.g.:

 secret AND illegal) AND access:confidential

will not result in a ParseException instead will run as:

 secret AND illegal"
0,"Exclude tests instead skipping themjcr2spi tests run with the spi2jcr module by default so they are configured to be skipped when jcr2spi is built. Manually running a jcr2spi test like this

mvn -Dtest=MyTest -Dmaven.test.skip=false test

does not work however. The pom configuration seems to take precedence here. 

To fix this I propose to exclude all test instead of skipping them making it possible to manually execute tests like this

mvn -Dtest=MyTest test
"
0,"Highlighter wraps caching token filters that are not CachingTokenFilter in CachingTokenFilterI figured this was fine and a rare case that you would have another caching tokenstream to feed the highlighter with - but I guess if its happening to you, especially depending on what you are doing - its not an ideal situation."
1,"new QueryParser over-increment position for MultiPhraseQueryIf the new QP is parsing a phrase, and when the analyzer runs on the text within the phrase it produces some tokens with posIncr=0, a MultiPhraseQuery is produced.  But, the positions of the added terms are over-incremented, and don't match what the current QueryParser does."
1,"CheckIndex should allow term position = -1
Spinoff from this discussion:

    http://mail-archives.apache.org/mod_mbox/lucene-java-user/200803.mbox/%3CPine.LNX.4.62.0803292323350.16762@radix.cryptio.net%3E

Right now CheckIndex claims the index is corrupt if you index a Token with -1 position, which happens if your first token has positionIncrementGap set to 0.

But, as far as I can tell, Lucene doesn't ""mind"" when this happens.

So I plan to fix CheckIndex to allow this case.  I'll backport to 2.3.2 as well.

LUCENE-1253 is one example where Lucene's core analyzers could do this."
1,queries with zero boosts don't workQueries consisting of only zero boosts result in incorrect results.
1,"JCR Server has concurrency issues on JcrWebdavServer.SessionCache internal HashMap cachesAfter doing the davex remoting performance work outlined in JCR-3026, the increased concurrency on my jcr server exposed a lot of errors related to getting and putting from the JcrWebdavServer.SessionCache's internal HashMap's.  This problem with HashMap's is a well known concurrency error and was easily fixed by upgrading these maps to ConcurrentHashMaps.  Performance seems dramatically better.  

The fix includes exposure of a tuning parameter that allows the user to set the expected concurrency level.  This is the number of concurrent requests you expect the server to be handling.  In the typical davex remoting scenario, this means you should tune this server side value to match the total max connections of all clients pointed at the server.  See JCR-3026. 

USAGE:  Set the 'concurrency-level' init param for the JcrRemotingServlet, via the web.xml of the jackabbit-webapp component.  Default value is 50.  Or you can intervene in a lower level api if appropriate."
1," inconsistent session and persistent state after ReferentialIntegrityExceptionWhen a ReferentialIntegrityException occurs in a session it seems that subsequent actions on that session may result in a inconsistent session state AND even inconsistent persistent state. The latter will even make jackrabbit fail to bootstrap an index from that persistent state.

Typical rootcause:

Caused by: org.apache.jackrabbit.core.state.NoSuchItemStateException: ddb9d3ea-59c1-4eb4-a83e-332f646d4f40
        at org.apache.jackrabbit.core.state.SharedItemStateManager.getItemState(SharedItemStateManager.java:270)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.createIndex(MultiIndex.java:1082)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.createIndex(MultiIndex.java:1088)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.createInitialIndex(MultiIndex.java:395)

Bootstrap failure:

java.io.IOException: Error indexing workspace
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.createInitialIndex(MultiIndex.java:402)
        at org.apache.jackrabbit.core.query.lucene.SearchIndex.doInit(SearchIndex.java:465)
        at org.apache.jackrabbit.core.query.AbstractQueryHandler.init(AbstractQueryHandler.java:59)
        at org.apache.jackrabbit.core.SearchManager.initializeQueryHandler(SearchManager.java:553)

"
1,"Highligter fails to include non-token at end of string to be highlightedThe following code extract show the problem


		TermQuery query= new TermQuery( new Term( ""data"", ""help"" )); 
		Highlighter hg = new Highlighter(new SimpleHTMLFormatter(), new QueryScorer( query ));
		hg.setTextFragmenter( new NullFragmenter() );
		
		String match = null;
		try {
			match = hg.getBestFragment( new StandardAnalyzer(), ""data"", ""help me [54-65]"" );
		} catch (IOException e) {
			e.printStackTrace();
		}
		System.out.println( match );


The sytsem outputs 

<B>help</B> me [54-65


would expect 

<B>help</B> me [54-65]



"
0,Introduce SessionInfo parameter for AbstractRepositoryService.createRootNodeDefinition()  SPI implementations might require access to the state of the current session in order to fulfill the contract of AbstractRepositoryService.createRootNodeDefinition(). I therefore suggest to add a SessionInfo parameter to this method. 
0,"JCR Test for Adding Node Type Tests That Abstract Nodes Can Be Added as Children, contrary to JCR 2.0 specificationWhen the TCK test method testLegalAndResidualType in the CanAddChildNodeCallWithNodeTypeTest class picks a node with a residual type, it does not filter out abstract nodes.  For example, in my local test, nt:hierarchyNode is selected for the local variable 'type'.

Since abstract node types ""cannot be directly assigned to a node,""[1] canAddChildNode(anyPropertyName, ""nt:hierarchyNode"") must return false.  However, since the test assumes that a non-abstract node type was chosen, it expects canAddChildNode(String, String) to return true.

This could be fixed if NodeTypeUtil.locateChildNodeDef(...) were extended to add an extra argument allowing or disallowing abstract types and that extra argument was used to filter the type used in testLegalAndResidualType (or if locateChildNodeDef(...) automatically excluded abstract types in the same manner that it automatically excludes protected types).

[1] - Section 3.7.1.3 of the JCR2 specification"
0,"Index File Format - Example for frequency file .frq is wrongReported by Johan Stuyts - http://www.nabble.com/Possible-documentation-error--p7012445.html - 

Frequency file example says: 

     For example, the TermFreqs for a term which occurs once in document seven and three times in document eleven would be the following sequence of VInts: 
         15, 22, 3 

It should be: 

     For example, the TermFreqs for a term which occurs once in document seven and three times in document eleven would be the following sequence of VInts: 
         15, 8, 3 


"
1,"System search manager uses a SessionItemStateManagerAs noted in JCR-2000, the system search manager (responsible for indexing the /jcr:system subtree) uses the SessionItemStateManager instance of the system session instead of the SharedItemStateManager of the underlying default workspace.

This can cause a deadlock (see the thread dumps in JCR-2000) when one thread is accessing the LockManager (that also uses the system session) while another thread is persisting versioning changes.

See the search-on-sism.patch attachment in JCR-2000 for a fix to this issue."
0,"Improved reusability of the JCA packageThe jackrabbit-jca package currently has hardcoded references to jackrabbit-core, which makes it difficult to reuse the packaging and related code with other JCR implementations. With the RepositoryFactory interface from JCR 2.0 we can avoid this hard dependency."
1,"FilterIndexReader doesn't work correctly with post-flex SegmentMergerIndexWriter.addIndexes(IndexReader...) internally uses SegmentMerger to add data from input index readers. However, SegmentMerger uses the new post-flex API to do this, which bypasses the pre-flex TermEnum/TermPositions API that FilterIndexReader implements. As a result, filtering is not applied."
0,"Add support for Map of referenced beansOCM should support the mapping of maps of referenced beans.

@Collection(collectionConverter= BeanReferenceCollectionConverterImpl.class)
private java.util.Map<String, ReferencedBean> aMap;

BeanReferenceCollectionConverterImpl (mainly the method doGetCollection) needs to be updated to support the interface ManageableMap interface.
"
0,"Decouple Filter from BitSet{code}
package org.apache.lucene.search;

public abstract class Filter implements java.io.Serializable 
{
  public abstract AbstractBitSet bits(IndexReader reader) throws IOException;
}

public interface AbstractBitSet 
{
  public boolean get(int index);
}

{code}

It would be useful if the method =Filter.bits()= returned an abstract interface, instead of =java.util.BitSet=.

Use case: there is a very large index, and, depending on the user's privileges, only a small portion of the index is actually visible.
Sparsely populated =java.util.BitSet=s are not efficient and waste lots of memory. It would be desirable to have an alternative BitSet implementation with smaller memory footprint.

Though it _is_ possibly to derive classes from =java.util.BitSet=, it was obviously not designed for that purpose.
That's why I propose to use an interface instead. The default implementation could still delegate to =java.util.BitSet=.

"
0,"Tests need to clean up after themselvesI havent run 'ant clean' for a while.

The randomly generated temporarily file names just piled up from running the tests many times... so ant clean is still running after quite a long time.

We should take the logic in the base solr test cases, and push it into LuceneTestCase, so a test cleans up all its temporary stuff.
"
1,"QNodeTypeDefinitionImpl.getSerializablePropertyDefs() might return non serializable property definitions QNodeTypeDefinitionImpl.getSerializablePropertyDefs() returns a set-version of the passed in parameter, irrespective of whether the property defs are serializable or not.

See http://markmail.org/thread/65ngqvyxnu4nn3su"
0,"Move UnInvertedField into Lucene coreSolr's UnInvertedField lets you quickly lookup all terms ords for a
given doc/field.

Like, FieldCache, it inverts the index to produce this, and creates a
RAM-resident data structure holding the bits; but, unlike FieldCache,
it can handle multiple values per doc, and, it does not hold the term
bytes in RAM.  Rather, it holds only term ords, and then uses
TermsEnum to resolve ord -> term.

This is great eg for faceting, where you want to use int ords for all
of your counting, and then only at the end you need to resolve the
""top N"" ords to their text.

I think this is a useful core functionality, and we should move most
of it into Lucene's core.  It's a good complement to FieldCache.  For
this first baby step, I just move it into core and refactor Solr's
usage of it.

After this, as separate issues, I think there are some things we could
explore/improve:

  * The first-pass that allocates lots of tiny byte[] looks like it
    could be inefficient.  Maybe we could use the byte slices from the
    indexer for this...

  * We can improve the RAM efficiency of the TermIndex: if the codec
    supports ords, and we are operating on one segment, we should just
    use it.  If not, we can use a more RAM-efficient data structure,
    eg an FST mapping to the ord.

  * We may be able to improve on the main byte[] representation by
    using packed ints instead of delta-vInt?

  * Eventually we should fold this ability into docvalues, ie we'd
    write the byte[] image at indexing time, and then loading would be
    fast, instead of uninverting
"
0,"Change default Directory impl on 64bit linux to MMapConsistently in my NRT testing on Fedora 13 Linux, 64 bit JVM (Oracle 1.6.0_21) I see MMapDir getting better search and merge performance when compared to NIOFSDir.

I think we should fix the default."
0,"spi2davex: InvalidItemStateException not properly extracted from ambiguous response errorNodeTest#testSaveInvalidStateException
SessionTest#testSaveInvalidStateException

fail with PathNotFoundException instead of InvalidItemStateException.

i remember that i already addressed that issue in spi2dav a long time ago. with the batched writing in
spi2davex it is back: the server isn't aware of the distinction and just isn't able to retrieve that removed
item... either the client side finds a way to distinguish between path-not-found and externally modified
or we have to leave this as known issue...

in spi2dav i added add quick hack: if the operation was some write operation the path-not-found is
simply converted into invaliditemstateexception."
0,"maxDoc should be explicitly stored in the index, not derived from file lengthThis is a spinoff of LUCENE-140

In general we should rely on ""as little as possible"" from the file system.  Right now, maxDoc is derived by checking the file length of the FieldsReader index file (.fdx) which makes me nervous.  I think we should explicitly store it instead.

Note that there are no known cases where this is actually causing a problem. There was some speculation in the discussion of LUCENE-140 that it could be one of the possible, but in digging / discussion there were no specifically relevant JVM bugs found (yet!).  So this would be a defensive fix at this point."
1,"QueryUtils should check that equals properly handles nullIts part of the equals contract, but many classes currently violate"
1,"jcr2spi: versionmanager#checkout(NodeState) should not forward to checkout(NodeState, NodeId)VersionManager#checkout(NodeState nodeState) is called if activity is not supported and thus should call the
corresponding SPI method instead of checkout(NodeState, NodeId activityId)"
0,"Document Package level javadocs need improvingThe document package package level javadocs could use some improving, such as:
1. Info on what a Document is, as well as Field and Fieldable
2. Examples of FieldSelectors and how to implement
3. Samples of using DateTools and NumberTools"
0,"Some contribs depend on core tests to be compiled and fail when ant clean was done beforeIf you do ""ant clean"" on the root level of Lucene and then go to e.g. contrib/queryparser (3.x only) or contrib/misc (3.x and trunk) and call ""ant test"", the build of tests fails:
- contrib/queryparser's ExtendedableQPTests extend a core TestQueryParser (3.x only, in module this works, of course)
- contrib/misc/TestIndexSplitter uses a core class to build its index

To find the root cause: We should first remove the core tests from contrib classpath, so the issue gets visible even without ""ant clean"" before. Then we can fix this."
0,"TestNLS fails with ja localeset ANT_ARGS=""-Dargs=-Duser.language=ja -Duser.country=JP""
ant test-core -Dtestcase=TestNLS

The test has 2 sets of message, one fallback, and one ja.
The tests assume if it asks for a non-ja locale, that it will get the fallback message,
but this is not how ResourceBundle.getBundle works:
{noformat}
Otherwise, the following sequence is generated from the attribute values of the specified locale 
(language1, country1, and variant1) and of the default locale (language2, country2, and variant2):

baseName + ""_"" + language1 + ""_"" + country1 + ""_"" + variant1
baseName + ""_"" + language1 + ""_"" + country1
baseName + ""_"" + language1
baseName + ""_"" + language2 + ""_"" + country2 + ""_"" + variant2
baseName + ""_"" + language2 + ""_"" + country2
baseName + ""_"" + language2
baseName
{noformat}

So in the case of ja default locale, you get a japanese message instead from the baseName + ""_"" + language2 match"
1,"Writers blocked forever when waiting on update operations  Thread 1 calls Session.save() and has a write lock.

Thread 2 is in XA prepare() and is waiting on thread 1 in FineGrainedISMLocking.acquireWriteLock().

Thread 1's save calls SharedItemStateManager.Update#end() and performs a write-lock downgrade to a read-lock, then (at the end of Update#end()) it calls readLock.release(). FineGrainedISMLocking.ReadLockImpl#release thinks activeWriterId is of the current transation and does not notify any writers (activeWriterId is not being reset on downgrade in what seems to be a related to JCR-2753).
Thread 1 waits forever."
0,"Avoid item state reads during Session.logout()This is a follow up issue for JCR-2231. There's a second CachingHierarchyManager attached to the LocalItemStateManager, which it unregistered too late and causes reads on the SharedItemStateManager on Session logout. The hierarchy manager should be unregistered as listener before the state manager is disposed."
1,"ItemStates in the ChangeLog can not be retrieved in the sequence they were created/modified/deletedThe itemstates are ordered by the hash code.
It's an issue with PersistenceManagers that check referencial integrity (e.g. rdbms)."
1,"ConnectionRecoveryManager is created twice in DBDataStore init methodIt seems that after introducing pool, old initizialization of ConnectionRecoveryManager has not been removed.

Index: DbDataStore.java
===================================================================
--- DbDataStore.java	(revision 605626)
+++ DbDataStore.java	(working copy)
@@ -479,8 +479,6 @@
             initDatabaseType();
             connectionPool = new Pool(this, maxConnections);
             ConnectionRecoveryManager conn = getConnection();
-            conn = new ConnectionRecoveryManager(false, driver, url, user, password);
-            conn.setAutoReconnect(true);
             DatabaseMetaData meta = conn.getConnection().getMetaData();
             log.info(""Using JDBC driver "" + meta.getDriverName() + "" "" + meta.getDriverVersion());
             meta.getDriverVersion();

Duplicated initialization should be removed , but i've never run this code yet."
1,"WebDav MKCOL on a directory that already exists generates a IllegalStateExceptionwhen performing a MKCOL on a resource that already exists, following is thrown.

31.03.2010 16:14:10.760 *ERROR* [127.0.0.1 [1270012450463] MKCOL /org.apache.sling.launchpad.testing-6-SNAPSHOT/apps HTTP/1.1] org.apache.sling.engine.impl.SlingMainServlet service: Uncaught Throwable java.lang.IllegalStateException: Response has already been committed
       at org.apache.sling.engine.impl.SlingHttpServletResponseImpl.checkCommitted(SlingHttpServletResponseImpl.java:398)
       at org.apache.sling.engine.impl.SlingHttpServletResponseImpl.setStatus(SlingHttpServletResponseImpl.java:265)
       at org.apache.jackrabbit.webdav.WebdavResponseImpl.setStatus(WebdavResponseImpl.java:276)
       at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.doMkCol(AbstractWebdavServlet.java:548)
       at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.execute(AbstractWebdavServlet.java:256)
       at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.service(AbstractWebdavServlet.java:196)
       at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
       at org.apache.sling.engine.impl.request.RequestData.service(RequestData.java:523)
....


I think a return after the sendError is required ?
in AbstractWebdavServlet.doMkCol(...) 


   protected void doMkCol(WebdavRequest request, WebdavResponse response,
                          DavResource resource) throws IOException, DavException {

       DavResource parentResource = resource.getCollection();
       if (parentResource == null || !parentResource.exists() || !parentResource.isCollection()) {
           // parent does not exist or is not a collection
           response.sendError(DavServletResponse.SC_CONFLICT);
           return;
       }
       // shortcut: mkcol is only allowed on deleted/non-existing resources
       if (resource.exists()) {
           response.sendError(DavServletResponse.SC_METHOD_NOT_ALLOWED);
+          return;
       }

       if (request.getContentLength() > 0 || request.getHeader(""Transfer-Encoding"") != null) {
           parentResource.addMember(resource, getInputContext(request, request.getInputStream()));
       } else {
           parentResource.addMember(resource, getInputContext(request, null));
       }
       response.setStatus(DavServletResponse.SC_CREATED);
   }



"
0,"bad java practices which affect performance (result of code inspection)IntelliJ IDEA found the following issues in the Lucense source code and tests:

1) explicit for loops where calls to System.arraycopy() should have been
2) calls to Boolean constructor (in stead of the appropriate static method/field)
3) instantiation of unnecessary Integer instances for toString, instead of calling the static one
4) String concatenation using + inside a call to StringBuffer.append(), in stead of chaining the append calls

all minor issues. patch is forthcoming.
"
0,Use bundle persistence in default configurationThe default repository configuration files in jackrabbit-core and -webapp still use the old simple database persistence. They should be updated to use bundle persistence in the 1.4 release.
1,"nt:versionedChild problemProblem occurs when both parent and child beans are versionable.  Jackrabbit creates an nt:versionedChild node that is referenced by the parent node, referencing the childs versionedHistory node of the child.  The current OCM code does not handle this correctly and produces an error:  ""Node type  'nt:versionedChild' does not match descriptor node type 'nt:unstructured'""

Below is a example code of the problem and a patch that appears to correctly resolve the problem.


 Within ObjectConverterImpl created the below method.

        public Node getActualNode(Session session,Node node) throws
 RepositoryException
        {
                NodeType type = node.getPrimaryNodeType();
                if (type.getName().equals(""nt:versionedChild""))
                {

                        String uuid =
 node.getProperty(""jcr:childVersionHistory"").getValue().getString();
                        Node actualNode = session.getNodeByUUID(uuid);
                        String name = actualNode.getName();
                        actualNode = session.getNodeByUUID(name);

                        return actualNode;
                }

                return node;
        }

 AND modified the following to call the above method


        public Object getObject(Session session, Class clazz, String path)
        {
                try {
                        if (!session.itemExists(path)) {
                                return null;
                        }

                        if (requestObjectCache.isCached(path))
                    {
                        return requestObjectCache.getObject(path);
                    }

                        ClassDescriptor classDescriptor =
 getClassDescriptor(clazz);

                        checkNodeType(session, classDescriptor);

                        Node node = (Node) session.getItem(path);
                        if (!classDescriptor.isInterface()) {
                                {
                                node = getActualNode(session,node);
                                checkCompatiblePrimaryNodeTypes(session,
 node, classDescriptor, true);
                                }
                        }

                        ClassDescriptor alternativeDescriptor = null;
                        if
 (classDescriptor.usesNodeTypePerHierarchyStrategy())
 {
                                if
 (node.hasProperty(ManagerConstant.DISCRIMINATOR_PROPERTY_NAME))
 {
                        String className =
 node.getProperty(ManagerConstant.DISCRIMINATOR_PROPERTY_NAME
 ).getValue().getString();
                        alternativeDescriptor =
 getClassDescriptor(ReflectionUtils.forName(className));
                                }
                        } else {
                                if
 (classDescriptor.usesNodeTypePerConcreteClassStrategy())
 {
                                        String nodeType =
 node.getPrimaryNodeType().getName();
                                        if
 (!nodeType.equals(classDescriptor.getJcrType()))
 {
                                            alternativeDescriptor =
 classDescriptor.getDescendantClassDescriptor(nodeType);

                                            // in case we an alternative
 could not be found by walking
                                            // the class descriptor
 hierarchy, check whether we
 would
                                            // have a descriptor for the
 node type directly (which
                                            // may the case if the class
 descriptor hierarchy is
                                            // incomplete due to missing
 configuration. See JCR-1145
                                            // for details.
                                            if (alternativeDescriptor ==
 null) {
                                                alternativeDescriptor =
 mapper.getClassDescriptorByNodeType(nodeType);
                                            }
                                        }
                                }
                        }

                        // if we have an alternative class descriptor,
 check whether its
                        // extends (or is the same) as the requested class.
                        if (alternativeDescriptor != null) {
                            Class alternativeClazz =
 ReflectionUtils.forName(alternativeDescriptor.getClassName());
                            if (clazz.isAssignableFrom(alternativeClazz)) {
                                clazz = alternativeClazz;
                                classDescriptor = alternativeDescriptor;
                            }
                        }

                        // ensure class is concrete (neither interface nor
 abstract)
                        if (clazz.isInterface() ||
 Modifier.isAbstract(clazz.getModifiers())) {
                            throw new JcrMappingException( ""Cannot
 instantiate non-concrete
 class "" + clazz.getName()
                        + "" for node "" + path + "" of type "" +
 node.getPrimaryNodeType().getName());
                        }

            Object object =
 ReflectionUtils.newInstance(classDescriptor.getClassName());

            if (! requestObjectCache.isCached(path))
            {
                          requestObjectCache.cache(path, object);
            }

            simpleFieldsHelp.retrieveSimpleFields(session,
 classDescriptor, node, object);
                        retrieveBeanFields(session, classDescriptor, node,
 path, object, false);
                        retrieveCollectionFields(session, classDescriptor,
 node, object, false);

                        return object;
                } catch (PathNotFoundException pnfe) {
                        // HINT should never get here
                        throw new
 ObjectContentManagerException(""Impossible to get
 the object
 at "" + path, pnfe);
                } catch (RepositoryException re) {
                        throw new
 org.apache.jackrabbit.ocm.exception.RepositoryException(""Impossible to
 get the object at "" + path, re);
                }
        }




>
>
>
> > I am building a test application against OCM.  I have the following
> > classes that are annotated for OCM.  The problem is that when I update
> and
> > version the root object PressRelease the Bean Author is versioned to
> > nt:versionedChild.  While the OCM is checking for node type
> compatibility
> > it is throwing the following exception.  It looks like the
> versionedChild
> > is not handled correctly.  Any suggestions?
> >
> > I also attempted to retrieve the version based on the version name for
> the
> > rootVersion but also trapped. From a Version object how should I access
> > each of the versioned entries?
> >
> > Thanks
> > Wes
> >
> > @Node (jcrMixinTypes=""mix:versionable"")
> > public class PressRelease
> > {
> >       @Field(path=true) String path;
> >       @Field String title;
> >       @Field Date pubDate;
> >       @Field String content;
> >       @Bean Author author;
> >       @Collection (elementClassName=Comment.class) List<Comment>
> comments = new
> > ArrayList<Comment>();
> >
> >       public String getPath() {
> >               return path;
> >       }
> >       public void setPath(String path) {
> >               this.path = path;
> >       }
> >       public String getContent() {
> >               return content;
> >       }
> >       public void setContent(String content) {
> >               this.content = content;
> >       }
> >       public Date getPubDate() {
> >               return pubDate;
> >       }
> >       public void setPubDate(Date pubDate) {
> >               this.pubDate = pubDate;
> >       }
> >       public String getTitle() {
> >               return title;
> >       }
> >       public void setTitle(String title) {
> >               this.title = title;
> >       }
> >       public Author getAuthor() {
> >               return author;
> >       }
> >       public void setAuthor(Author author) {
> >               this.author = author;
> >       }
> >       public List<Comment> getComments() {
> >               return comments;
> >       }
> >       public void setComments(List<Comment> comments) {
> >               this.comments = comments;
> >       }
> >
> >
> > }
> >
> > @Node (jcrMixinTypes=""mix:versionable"")
> > public class Author {
> >
> >       @Field(path=true) String path;
> >       @Field String name;
> >
> >
> >       public String getName() {
> >               return name;
> >       }
> >       public void setName(String name) {
> >               this.name = name;
> >       }
> >       public String getPath() {
> >               return path;
> >       }
> >       public void setPath(String path) {
> >               this.path = path;
> >       }
> >
> > }
> >
> > MAIN
> >
> >       while (versionIterator.hasNext())
> >       {
> >           Version version = (Version) versionIterator.next();
> >           System.out.println(""version found : ""+ version.getName() + "" -
> "" +
> >                                 version.getPath() + "" - "" +
> > version.getCreated().getTime());
> >
> >
> >           if (!version.getName().equals(""jcr:rootVersion""))
> >           {
> >
> > //      Get the object matching to the first version
> >           pressRelease = (PressRelease)
> > ocm.getObject(""/newtutorial"",version.getName());
> >
> >
> >               System.out.println(""PressRelease title : "" +
> pressRelease.getTitle());
> >               System.out.println(""             author: "" +
> > pressRelease.getAuthor().getName());
> >               System.out.println(""            content: "" +
> pressRelease.getContent());
> >               List comments = pressRelease.getComments();
> >               Iterator iterator = comments.iterator();
> >               while (iterator.hasNext())
> >               {
> >                       comment = (Comment) iterator.next();
> >                       System.out.println(""Comment : <"" + comment.getData()
> + "">"" +
> > comment.getText());
> >               }
> >           }
> >       }
> >
> >
> > CONSOLE
> > version found : jcr:rootVersion -
> >
> /jcr:system/jcr:versionStorage/fc/0b/fd/fc0bfd89-c487-4fbe-930f-d837e5dfed79/jcr:rootVersion
> > - Thu Feb 28 15:54:42 EST 2008
> > version found : 1.0 -
> >
> /jcr:system/jcr:versionStorage/fc/0b/fd/fc0bfd89-c487-4fbe-930f-d837e5dfed79/1.0
> > - Thu Feb 28 15:54:59 EST 2008
> > Exception in thread ""main""
> > org.apache.jackrabbit.ocm.exception.ObjectContentManagerException:
> Cannot
> > map object of type 'com..pc.repository.Author'. Node type
> > 'nt:versionedChild' does not match descriptor node type
> 'nt:unstructured'
> >       at
> >
> org.apache.jackrabbit.ocm.manager.objectconverter.impl.ObjectConverterImpl.checkCompatiblePrimaryNodeTypes
> (ObjectConverterImpl.java:552)
> >       at
> >
> org.apache.jackrabbit.ocm.manager.objectconverter.impl.ObjectConverterImpl.getObject
> (ObjectConverterImpl.java:361)
> >       at
> >
> org.apache.jackrabbit.ocm.manager.beanconverter.impl.DefaultBeanConverterImpl.getObject
> (DefaultBeanConverterImpl.java:80)
> >       at
> >
> org.apache.jackrabbit.ocm.manager.objectconverter.impl.ObjectConverterImpl.retrieveBeanField
> (ObjectConverterImpl.java:666)
> >       at
> >
> org.apache.jackrabbit.ocm.manager.objectconverter.impl.ObjectConverterImpl.retrieveBeanFields
> (ObjectConverterImpl.java:621)
> >       at
> >
> org.apache.jackrabbit.ocm.manager.objectconverter.impl.ObjectConverterImpl.getObject
> (ObjectConverterImpl.java:309)
> >       at
> >
> org.apache.jackrabbit.ocm.manager.impl.ObjectContentManagerImpl.getObject(
> ObjectContentManagerImpl.java:313)
> >       at com.pc.repository.Main.main(Main.java:345)






"
0,"Make DirectoryTaxonomyWriter's indexWriter member privateDirectoryTaxonomyWriter has a protected indexWriter member. As far as I can tell, for two reasons:

# protected openIndexWriter method which lets you open your own IW (e.g. with a custom IndexWriterConfig).
# protected closeIndexWriter which is a hook for letting you close the IW you opened in the previous one.

The fixes are trivial IMO:
# Modify the method to return IW, and have the calling code set DTW's indexWriter member
# Eliminate closeIW. DTW already has a protected closeResources() which lets you clean other resources you've allocated, so I think that's enough.

I'll post a patch shortly."
0,"Consolidate Lucene's QueryParsers into a moduleLucene has a lot of QueryParsers and we should have them all in a single consistent place.  

The following are QueryParsers I can find that warrant moving to the new module:

- Lucene Core's QueryParser
- AnalyzingQueryParser
- ComplexPhraseQueryParser
- ExtendableQueryParser
- Surround's QueryParser
- PrecedenceQueryParser
- StandardQueryParser
- XML-Query-Parser's CoreParser

All seem to do a good job at their kind of parsing with extensive tests.

One challenge of consolidating these is that many tests use Lucene Core's QueryParser.  One option is to just replicate this class in src/test and call it TestingQueryParser.  Another option is to convert all tests over to programmatically building their queries (seems like alot of work)."
1,"ChangeLog serialization causes cache inconsistenciesThe ordering of actions is taken into account when a ChangeLog is built through session manipulations (see, for instance,  ChangeLog.deleted(ItemState state)). When it is serialized in ClusterNode.write(Record record, ChangeLog changeLog, EventStateCollection esc), however, this implicit ordering might be changed. As a consequence,  the deserialization in ClusterNode.consume(Record record) might produce a different ChangeLog with the effect that the local caches get out-of-sync with the persistent state of the repository.

The issue should be reproducable as follows:
- Setup a clustered environment with two Jackrabbit instances, say A and B.
- On instance A add a property ""P"" with value ""x"" to some node and save the session.
- On instance B read property ""P"" -> it will have value ""x"".
- On instance A delete property P and then add it again with value ""y"" and save the session.
- On instance B read property ""P"" -> it will still have value ""x"" after the cluster sync..."
1,"incorrect definition of built-in node type nt:hierarchyNodethe property jcr:created of nt:hierarchyNode should be non-mandatory according to the specification (jcr 1.0 and jcr 1.0.1).
both the definition of the built-in node type and the related test case should be fixed accordingly."
0,"TCK: DocumentViewImportTest does not call refresh after direct-to-workspace importAfter performing a direct-to-workspace import, the test does not call refresh to ensure the transient layer doesn't contain stale data.

Proposal: call refresh(false) after performing direct-to-workspace imports.

--- DocumentViewImportTest.java (revision 422074)
+++ DocumentViewImportTest.java (working copy)
@@ -106,6 +106,12 @@
             SAXException, NotExecutableException {
  
         importXML(target, createSimpleDocument(), uuidBehaviour, withWorkspace);
+
+        if (withWorkspace)
+        {
+          session.refresh(false);
+        }
+
         performTests();
     }
  
@@ -127,6 +133,12 @@
             SAXException, IOException, NotExecutableException {
  
         importWithHandler(target, createSimpleDocument(), uuidBehaviour, withWorkspace);
+
+        if (withWorkspace)
+        {
+          session.refresh(false);
+        }
+
         performTests();
     }
"
0,"Improved XML export handlingAs mentioned in JCR-1574, the current XML export functionality is generating workarounds like the new PropertyWrapper class. I'd like to refactor and clean up the XML export stuff so that such workarounds wouldn't be needed.

An additional bonus would be to make both core and jcr2spi use the same XML export mechanism. For example the one in core already supports JSR 283 shareable nodes, but the one in jcr2spi does not."
0,"[PATCH] Clear ThreadLocal instances in close()As already found out in LUCENE-436, there seems to be a garbage collection problem with ThreadLocals at certain constellations, resulting in an OutOfMemoryError.
The resolution there was to remove the reference to the ThreadLocal value when calling the close() method of the affected classes (see FieldsReader and TermInfosReader).
For Java < 5.0, this can effectively be done by calling threadLocal.set(null); for Java >= 5.0, we would call threadLocal.remove()

Analogously, this should be done in *any* class which creates ThreadLocal values

Right now, two classes of the core API make use of ThreadLocals, but do not properly remove their references to the ThreadLocal value
1. org.apache.lucene.index.SegmentReader
2. org.apache.lucene.analysis.Analyzer

For SegmentReader, I have attached a simple patch.
For Analyzer, there currently is no patch because Analyzer does not provide a close() method (future to-do?)

"
1,"JCA Concurrent Modification Exception when JCAManagedConnection.cleanup() calledThe JCAManagedConnection.closeHandles() method causes a ConcurrentModificationException if the handles list is not empty.
This is caused by modification of the handles list by removeHandle(), while closeHandles() is iterating over the list.

Under SunOne AppServer 7 this can be caused simply by not closing the Session handle before the transaction commits.

It is probably not even necessary to send connectionClosed events during cleanup().  According to the API for connectionClosed, the event indicates that an application component has closed  the connection handle.  cleanup() is a container initiated action, and so the connectionClosed event is not applicable.


java.util.ConcurrentModificationException
    at java.util.LinkedList$ListItr.checkForComodification(LinkedList.java:552)
    at java.util.LinkedList$ListItr.next(LinkedList.java:488)
    at org.apache.jackrabbit.jca.JCAManagedConnection.closeHandles(JCAManagedConnection.java:382)
    at org.apache.jackrabbit.jca.JCAManagedConnection.cleanup(JCAManagedConnection.java:145)
    at com.sun.enterprise.resource.IASPoolObjectImp.cleanup(IASPoolObjectImp.java:243)
    at com.sun.enterprise.resource.IASGenericPoolObjects.transactionCompleted(IASGenericPoolObjects.java:794)
    at com.sun.enterprise.resource.ResourcePoolManagerImpl.transactionCompleted(ResourcePoolManagerImpl.java:347)
    at com.sun.enterprise.resource.ResourcePoolManagerImpl$SynchronizationListener.afterCompletion(ResourcePoolManagerImpl.java:644)
    at com.sun.jts.jta.SynchronizationImpl.after_completion(SynchronizationImpl.java:70)

"
0,"User-defined ProtocolSocketFactory for secure connection through proxyI use a custom socket implementation with HttpClient, and am having problems 
getting secure connections through a proxy working.

HttpClient requires that my SecureProtocolSocketFactory be able to create a 
secure socket layered over an existing insecure socket, but does not specify 
how that insecure socket is created. Currently, for secure proxied 
connections, the insecure connection is always created using a 
DefaultProtocolSocketFactory (HttpConnection.java, line 702). I wish to be 
able to override this behaviour, so that I can create the insecure socket 
using my own custom implementation.

The problem with the default behaviour is that my custom socket implementation 
is written in C++ using JNI, and the SSL implementation is handled at the 
native level. Hence, layering over an existing JDK socket will not work.

My proposed solution is to add an HTTP connection parameter to specify the 
socket factory to use, perhaps http.connection.insecuresocketfactory of type 
Class."

label,summmarydescription
0,"RangeQuery and RangeFilter should use collation to check for range inclusionSee [this java-user discussion|http://www.nabble.com/lucene-farsi-problem-td16977096.html] of problems caused by Unicode code-point comparison, instead of collation, in RangeQuery.

RangeQuery could take in a Locale via a setter, which could be used with a java.text.Collator and/or CollationKey's, to handle ranges for languages which have alphabet orderings different from those in Unicode."
1,"NodeReferences are lost when deleting and setting the same reference in the same save() cycleI've written the following snippet to illustrate the issue :

        Node root = session.getRootNode();
        
        Node a = root.addNode(""a"");
        Node b = root.addNode(""b"");
        b.addMixin(""mix:referenceable"");
        
        a.setProperty(""p"", b);
        
        root.save();
        
        System.out.println(b.getReferences().getSize());     // --> correctly returns 1
        
        a.setProperty(""p"", (Node) null);
        a.setProperty(""p"", b);
        
        root.save();
        
        System.out.println(b.getReferences().getSize());    // --> returns 0 !

When the ChangeLog is processed, added references are processed before deleted ones, so the persisted NodeReferences is finally wrong.

I've set the priority of this issue to critical, because the persisted references count is corrupted.

A simple workaround is to first remove the property, then save, then add the property again, but it not satisfying.
"
0,"Missing class JNDIDatabaseJournalWe're dealing to set up a clustered repository and run into some issues and missing features stated to be fixed in the upcoming v1.4. But while scanning the sources of the v1.4-rc1, i still can't find the class

  JNDIDatabaseJournal (org.apache.jackrabbit.core.journal.JNDIDatabaseJournal)

as a silbing to the classes 

  JNDIDatabaseFileSystem (org.apache.jackrabbit.core.fs.db.JNDIDatabaseFileSystem)

and 

  JNDIDatabasePersistenceManager (org.apache.jackrabbit.core.persistence.db.JNDIDatabasePersistenceManager)

The missing one you'll need to configure all parts of a repository handeled in an abstract way by (e.g one common) JNDI database resource. From the shortness and simplicity of the source code of the other ones, i think adding this missing feature takes just about an hour.

Thank you for support"
0,"bulk postings should be codec privateIn LUCENE-2723, a lot of work was done to speed up Lucene's bulk postings read API.

There were some upsides:
* you could specify things like 'i dont care about frequency data up front'.
  This made things like multitermquery->filter and other consumers that don't
  care about freqs faster. But this is unrelated to 'bulkness' and we have a
  separate patch now for this on LUCENE-2929.
* the buffersize for standardcodec was increased to 128, increasing performance
  for TermQueries, but this was unrelated too.

But there were serious downsides/nocommits:
* the API was hairy because it tried to be 'one-size-fits-all'. This made consumer code crazy.
* the API could not really be specialized to your codec: e.g. could never take advantage that e.g. docs and freqs are aligned.
* the API forced codecs to implement delta encoding for things like documents and positions. 
  But this is totally up to the codec how it wants to encode! Some codecs might not use delta encoding.
* using such an API for positions was only theoretical, it would have been super complicated and I doubt ever
  performant or maintainable.
* there was a regression with advance(), probably because the api forced you to do both a linear scan thru
  the remaining buffer, then refill...

I think a cleaner approach is to let codecs do whatever they want to implement the DISI
contract. This lets codecs have the freedom to implement whatever compression/buffering they want
for the best performance, and keeps consumers simple. If a codec uses delta encoding, or if it wants
to defer this to the last possible minute or do it at decode time, thats its own business. Maybe a codec
doesn't want to do any buffering at all.
"
0,"Order of stored Fields not maintainedAs noted in these threads...

http://www.nabble.com/Order-of-fields-returned-by-Document.getFields%28%29-to21034652.html
http://www.nabble.com/Order-of-fields-within-a-Document-in-Lucene-2.4%2B-to24210597.html

somewhere prior to Lucene 2.4.1 a change was introduced that prevents the Stored fields of a Document from being returned in same order that they were originally added in.  This can cause serious performance problems for people attempting to use LoadFirstFieldSelector or a custom FieldSelector with the LOAD_AND_BREAK, or the SIZE_AND_BREAK options (since the fields don't come back in the order they expect)

Speculation in the email threads is that the origin of this bug is code introduced by LUCENE-1301 -- but the purpose of that issue was refactoring, so if it really is the cause of the change this would seem to be a bug, and not a side affect of a conscious implementation change.

Someone who understands indexing internals should investigate this.  At a minimum, if it's decided that this is not actual a bug, then prior to resolving this bug the wiki docs and some of the FIeldSelector javadocs should be updated to make it clear what order Fields will be returned in.

"
1,"Bundle persistence name index not case-sensitive in MySQL and MS SQLAs reported by Martijn Hendriks on the dev mailing list (see http://www.nabble.com/Bundle-persistence-managers---db-collation-tf3571522.html), the NAME column of the NAMES table in the bundle persistence manager needs to be case-sensitive."
1,"Preemtive Auth fails whithout credentialsThe preemtive authorization causes a HttpException to be thrown in teh
Authenticator if no credentials were provided at all. This case should be
handled quietly. A test case should be added."
1,"System view export truncates carriage returnIf a string contains a carriage return (\r), this character was truncated on some platforms. "
1,"VersionManagerImplRestore internalRestoreFrozen method has identity versus equals bugIn method protected void internalRestoreFrozen(NodeStateEx state,
                                         InternalFrozenNode freeze,
                                         VersionSelector vsel,
                                         Set<InternalVersion> restored,
                                         boolean removeExisting,
                                         boolean copy)
in the VersionManagerImplRestore class line 557 the code performs an == instead of calling the NodeId.equals() method.  We ran into problems with the code that executes below this (trying to restore a folder node throws an ItemExistsException since same sibling not allowed on folder nodes)"
1,"In JCAConnectionRequestInfo, equals() and hashCode() implementations are inconsistentJCAConnectionRequestInfo behaves differently in its equals() and hashCode() methods. The former is aware about SimpleCredentials structure, so two instances of JCAConnectionRequestInfo were supplied SimpleCredentials instances with same userID, password and attributes, they are considered equal.
But JCAConnectionRequestInfo.hashCode() just delegates to SimpleCredentials.hashCode() which is same as Object's method. This breaks session pooling."
1,"consistency check fails with derbypm if bundle size exceeds 32kdue to a 'problem' in derby DERBY-1486 interleaved reads on a bundle that is larger than about 32k results in an error:
  ERROR XJ073: The data in this BLOB or CLOB is no longer available.  
               The BLOB or CLOBs transaction may be committed, or its 
               connection is closed.

this issue was already addressed in JCR-1039 but not fixed for the consistency check."
0,"Should SegmentTermPositionVector be public?I'm wondering why SegmentTermPositionVector is public. It implements the public
interface TermPositionVector. Should we remove ""public""?"
0,"Improve FilteredQuery to shortcut on wrapped MatchAllDocsQuerySince the rewrite of Lucene trunk to delegate all Filter logic to FilteredQuery, by simply wrapping in IndexSearcher.wrapFilter(), we can do more short circuits and improve query execution. A common use case it to pass MatchAllDocsQuery as query to IndexSearcher and a filter. For the underlying hit collection this is stupid and slow, as MatchAllDocsQuery simply increments the docID and checks acceptDocs. If the filter is sparse, this is a big waste. This patch changes FilteredQuery.rewrite() to short circuit and return ConstantScoreQuery, if the query is MatchAllDocs."
1,"Cluster Journal directory should be created automaticallyIf the cluster journal directory does not exist when starting the cluster, an exception is thrown: ERROR org.apache.jackrabbit.core.RepositoryImpl - failed to start Repository: Directory specified does either not exist or is not a directory: ...

As far as I know, this is not consistent with how all other components of Jackrabbit work. I think the directory should be created automatically if it does not exist:

new File(...).mkdirs();

I know you could argue this is not a bug, but in my view it is an important usability issue."
1,Use of Multi-Args URI Causes URI-Rewriting to improperly unescape charactersSee: http://www.nabble.com/unable-to-encode-reserved-characters-using-java.net.URI-multi-arg-constructors-td14954679.html for information from the httpclient-dev thread.  The basic idea is that URI's multi-arg constructors break things.
0,"optimizations for bufferedindexinputalong the same lines as LUCENE-2816:
* the readVInt/readVLong/readShort/readInt/readLong are not optimal here since they defer to readByte. for example this means checking the buffer's bounds per-byte in readVint instead of per-vint.
* its an easy win to speed this up, even for the vint case: its essentially always faster, the only slower case is 1024 single-byte vints in a row, in this case we would do a single extra bounds check (1025 instead of 1024)
"
0,"Convert NumericUtils and NumericTokenStream to use BytesRef instead of Strings/char[]After LUCENE-2302, we should use TermToBytesRefAttribute to index using NumericTokenStream. This also should convert the whole NumericUtils to use BytesRef when converting numerics."
1,"NodeTypeDef depends on supertype orderingCurrently the NodeTypeDef.setSupertypes() method simply sets the given QName array as the supertype QName array of the defined node type, thus preserving whatever ordering a node type parser or ultimately a node type definition file uses. This causes problems for example in the equals() method that uses the order-sensitive Arrays.equals() method to check for equality of the supertype QName arrays. The current implementation does therefore not consider the node type definitions ""A > B, C"" and ""A > C, B"" as equal even though they really should be so considered.

The same problem affects also child node and property definitions. The proper fix for this issue would probably be to use Sets to store and handle this information."
0,"REFERENCE properties produce duplicate strings in memoryWhen reference property is loaded from PM, Serializer.deserialize(NodeReferences, InputStream) is called, which calls PropertyId.valueOf(String), which in turn calls NameFactoryImpl.create(String) which finally splits a full property name to namespace and local name. Namespace is internalized, but local name is not (comments say that this is done to avoid perm space overfilling).
So, in the end, a new String instance is created for local name. This leads to considerable memory waste when repository has a lot of nodes with REFERENCE properties.
It seems that local name part could be internalized here too because in the most repositories it's not allowed to create properties with arbitrary names, so the danger of perm space exhaust does not seem to be an argument.

As for ways to resolve this, maybe a new NameFactory implementation could be created which would be used for properties only (and, possibly, mainly in the PropertyId.valueOf(String)) which would extend an existing NameFactoryImpl overriding its create(String) method.

What do you think about all this?"
0,"Missing support for some ""general"" relations in QueryTreeDump and xpath.QueryFormatThe two classes lack support for some of the ""general"" relations in XPath.
"
0,"benchmark pkg: allow TrecContentSource not to change the docnameTrecContentSource currently appends 'iteration number' to the docname field.
Example: if the original docname is DOC0001 then it will be indexed as DOC0001_0

this presents a problem for relevance testing, because when judging results, the expected docname will never be present.
This patch adds an option to disable this behavior, defaulting to the existing behavior (which is to append the iteration number).
"
0,"Remove Unnecessary NULL check in FindSegmentsFile - cleanupFindSegmentsFile accesses the member ""directory"" in line 579 while performing a null check in 592. The null check is unnecessary as if directory is null line 579 would throw a NPE.
I removed the null check and made the member ""directory"" final. In addition I added a null check in the constructor as If the value is null we should catch it asap. 

"
0,"Thread starvation problems in some testsIn some of the tests, a time limit is set and the tests have a ""while (inTime)"" loop. If creation of thread under heavy load is too slow, the tasks are not done. Most tests are only useful, if the task is at least done once (most would even fail).

This thread changes the loops to be do...while, so the task is run at least one time."
1,Scorer.skipTo() does not initialize hitsSome of the custom Scorer implementations in Jackrabbit do not initialize the internal hits BitSet if skipTo() is called before next().
0,"TopDocsCollector should have bounded generic <T extends ScoreDoc>TopDocsCollector was changed to be TopDocsCollector<T>. However it has methods which specifically assume the PQ stores ScoreDoc. Therefore, if someone extends it and defines a type which is not ScoreDoc, things will break.

We shouldn't put <T> on TopDocsCollector at all, but rather change its ctor to *protected TopDocsCollector(PriorityQueue<? extends ScoreDoc> pq)*. TopDocsCollector should handle ScoreDoc types. If we do this, we'll need to change FieldValueHitQueue's Entry to extend ScoreDoc as well."
0,NodeCanAddMixinTest.testCheckedIn() has wrong option checkNodeCanAddMixinTest.testCheckedIn() checks for locking option instead of versioning option.
0,"JackrabbitParser and tika 0.7 parserHi,

I was trying to implement a custom parser and found the following problem.
Since tika 0.7 it is possible to implement your custom parser and specify it into a service provider configuration file (META-INF/services/org.apache.tika.parser.Parser). In this way there would be no need to maintain a custom tika-config.xml file if you'd like to implement a custom parser.

The problem that I had was in the JackrabbitParser because I wasn't able to instantiate the AutoDetectParser with the default constructor is will be instantiated using the default TikaConfig constructor.
Basically from tika 0.7, the TikaConfig.getTikaConfig() is instantiating the TikaConfig using the default constructor instead of accessing the tika-config.xml file from withing the package, and reads the service provider configuration files and populate the parsers map.

What I'm proposing is to change the JackrabbitParser to instantiate the AutoDetectParser using the default constructor, in this way the using tika version >= 0.7 we could easily implement our own parsers and there won't be a reason to maintain the tika-config.xml, also a sort of ""backward"" compatibility would be maintained because using the AutoDetectParser default constructor the TikaConfig is instantiated using TikaConfig.getTikaConfig() wich for tika versions < 0.7 calls the TikaConfig(InputStream) constructor whcih reads the configuration directly from the package.

Basically the JackrabbitParser should look like this:

    public JackrabbitParser() {
            	parser = new AutoDetectParser();
    }
 
Thanks,
Dan"
1,"new QueryParser fails to set AUTO REWRITE for multi-term queriesThe old QueryParser defaults to constant score rewrite for Prefix,Fuzzy,Wildcard,TermRangeQuery, but the new one seems not to."
0,"Skip deployment of jackrabbit-standaloneThe jackrabbit-standalone jar currently can't be deployed to the repository.apache.org server probably because of its size. I'm not sure if there are any good use cases where you'd want to use the standalone jar as a Maven dependency, so having it on Maven central doesn't seem that important. I'd like to make this explicit by configuring the deploy plugin to skip deploying the standalone jar."
0,"Code cleanups for Java 1.5 and more.I can't resist giving code a good cleansing when I start hacking.  Here's some simple things:
- Use character constants instead of string contstants
- Use java 1.5 style for loops
- Use StringBuilder where appropriate
- Fix javadocs
- switch somestring.equals("""") to .length() == 0
-  simplify some boolean expressions
- eliminate redundant initializers
- fix some html nits
- remove final keyword from static methods
"
0,Make all classes that have a close() methods instanceof Closeable (Java 1.5)This should be simple.
0,"Can't put non-index files (e.g. CVS, SVN directories) in a Lucene index directoryLucene won't tolerate foreign files in its index directories.  This makes it impossible to keep an index in a CVS or Subversion repository.

For instance, this exception appears when creating a RAMDirectory from a java.io.File that contains a subdirectory called "".svn"".

java.io.FileNotFoundException: /home/local/ejj/ic/.caches/.search/.index/.svn
(Is a directory)
        at java.io.RandomAccessFile.open(Native Method)
        at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
        at
org.apache.lucene.store.FSIndexInput$Descriptor.<init>(FSDirectory.java:425)
        at org.apache.lucene.store.FSIndexInput.<init>(FSDirectory.java:434)
        at org.apache.lucene.store.FSDirectory.openInput(FSDirectory.java:324)
        at org.apache.lucene.store.RAMDirectory.<init>(RAMDirectory.java:61)
        at org.apache.lucene.store.RAMDirectory.<init>(RAMDirectory.java:86)"
0,"All Tokenizer implementations should have constructors that take AttributeSource and AttributeFactoryI have a TokenStream implementation that joins together multiple sub TokenStreams (i then do additional filtering on top of this, so i can't just have the indexer do the merging)

in 2.4, this worked fine.
once one sub stream was exhausted, i just started using the next stream 

however, in 2.9, this is very difficult, and requires copying Term buffers for every token being aggregated

however, if all the sub TokenStreams share the same AttributeSource, and my ""concat"" TokenStream shares the same AttributeSource, this goes back to being very simple (and very efficient)


So for example, i would like to see the following constructor added to StandardTokenizer:
{code}
  public StandardTokenizer(AttributeSource source, Reader input, boolean replaceInvalidAcronym) {
    super(source);
    ...
  }
{code}

would likewise want similar constructors added to all Tokenizer sub classes provided by lucene
"
0,"it is impossible to use a custom dictionary for SmartChineseAnalyzerit is not possible to use a custom dictionary, even though there is a lot of code and javadocs to allow this.

This is because the custom dictionary is only loaded if it cannot load the built-in one (which is of course, in the jar file and should load)
{code}
public synchronized static WordDictionary getInstance() {
    if (singleInstance == null) {
      singleInstance = new WordDictionary(); // load from jar file
      try {
        singleInstance.load();
      } catch (IOException e) { // loading from jar file must fail before it checks the AnalyzerProfile (where this can be configured)
        String wordDictRoot = AnalyzerProfile.ANALYSIS_DATA_DIR;
        singleInstance.load(wordDictRoot);
      } catch (ClassNotFoundException e) {
        throw new RuntimeException(e);
      }
    }
    return singleInstance;
  }
{code}

I think we should either correct this, document this, or disable custom dictionary support..."
0,"The terms index divisor in IW should be set via IWC not via getReaderThe getReader call gives a false sense of security... since if deletions have already been applied (and IW is pooling) the readers have already been loaded with a divisor of 1.

Better to set the divisor up front in IWC."
0,"rev. 169301: wrong directory name in build.xmlbuild.xml mentions non-existing directory contrib/WordNet/ which should read
contrib/wordnet in line 418.

below the result of svn diff against the corrected and working version of build.xml

--- build.xml   (revision 169301)
+++ build.xml   (working copy)
@@ -415,7 +415,7 @@
         <!-- TODO: find a dynamic way to do include multiple source roots -->
         <packageset dir=""src/java""/>
         <packageset dir=""contrib/analyzers/src/java""/>
-        <packageset dir=""contrib/WordNet/src/java""/>
+        <packageset dir=""contrib/wordnet/src/java""/>
         <packageset dir=""contrib/highlighter/src/java""/>
         <packageset dir=""contrib/similarity/src/java""/>
         <packageset dir=""contrib/spellchecker/src/java""/>"
0,Move NodeTypeStorage to spi-commons and provide default implementationThis facilitates the implementation of NodeTypes methods of RepositoryService.
1,"Transient Repository cannot be used more than once when configured with DataSourcesThe TransientRepository cannot be used more than once when the repository is configured with the DataSources construct. This has been verified with both Oracle and Derby configurations. Once the TransientRepository closes for the first time, the ConnectionFactory class sets a boolean value named closed to 'true'.  Thereafter, any use of the ConnectionFactory throws a runtime exception.

The following stacktrace is thrown on the second attempt to utilize the repository:

2011-01-25 08:12:14 DatabaseFileSystem [ERROR] failed to initialize file system
java.lang.IllegalStateException: this factory has already been closed
	at org.apache.jackrabbit.core.util.db.ConnectionFactory.sanityCheck(ConnectionFactory.java:213)
	at org.apache.jackrabbit.core.util.db.ConnectionFactory.getDataBaseType(ConnectionFactory.java:134)
	at org.apache.jackrabbit.core.fs.db.DbFileSystem.getDataSource(DbFileSystem.java:228)
	at org.apache.jackrabbit.core.fs.db.DatabaseFileSystem.init(DatabaseFileSystem.java:190)
	at org.apache.jackrabbit.core.config.RepositoryConfigurationParser$6.getFileSystem(RepositoryConfigurationParser.java:1057)
	at org.apache.jackrabbit.core.config.RepositoryConfig.getFileSystem(RepositoryConfig.java:892)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:284)
	at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:602)
	at org.apache.jackrabbit.core.TransientRepository$1.getRepository(TransientRepository.java:179)
	at org.apache.jackrabbit.core.TransientRepository.startRepository(TransientRepository.java:279)
	at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:375)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:123)
	...
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:115)
2011-01-25 08:12:14 RepositoryImpl [ERROR] failed to start Repository: File system initialization failure.
javax.jcr.RepositoryException: File system initialization failure.
	at org.apache.jackrabbit.core.config.RepositoryConfigurationParser$6.getFileSystem(RepositoryConfigurationParser.java:1060)
	at org.apache.jackrabbit.core.config.RepositoryConfig.getFileSystem(RepositoryConfig.java:892)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:284)
	at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:602)
	at org.apache.jackrabbit.core.TransientRepository$1.getRepository(TransientRepository.java:179)
	at org.apache.jackrabbit.core.TransientRepository.startRepository(TransientRepository.java:279)
	at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:375)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:123)
	at TransientRepositoryTest.addNodeToRepository(TransientRepositoryTest.java:32)
	at TransientRepositoryTest.shouldNotFailWhenUsingTransientRepositoryTwice(TransientRepositoryTest.java:26)
	...
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:115)
Caused by: org.apache.jackrabbit.core.fs.FileSystemException: failed to initialize file system
	at org.apache.jackrabbit.core.fs.db.DatabaseFileSystem.init(DatabaseFileSystem.java:210)
	at org.apache.jackrabbit.core.config.RepositoryConfigurationParser$6.getFileSystem(RepositoryConfigurationParser.java:1057)
	... 42 more
Caused by: java.lang.IllegalStateException: this factory has already been closed
	at org.apache.jackrabbit.core.util.db.ConnectionFactory.sanityCheck(ConnectionFactory.java:213)
	at org.apache.jackrabbit.core.util.db.ConnectionFactory.getDataBaseType(ConnectionFactory.java:134)
	at org.apache.jackrabbit.core.fs.db.DbFileSystem.getDataSource(DbFileSystem.java:228)
	at org.apache.jackrabbit.core.fs.db.DatabaseFileSystem.init(DatabaseFileSystem.java:190)
	... 43 more
2011-01-25 08:12:14 RepositoryImpl [ERROR] Error while closing Version Manager.
java.lang.NullPointerException
	at org.apache.jackrabbit.core.RepositoryImpl.doShutdown(RepositoryImpl.java:1117)
	at org.apache.jackrabbit.core.RepositoryImpl.shutdown(RepositoryImpl.java:1063)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:388)
	at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:602)
	at org.apache.jackrabbit.core.TransientRepository$1.getRepository(TransientRepository.java:179)
	at org.apache.jackrabbit.core.TransientRepository.startRepository(TransientRepository.java:279)
	at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:375)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:123)
	at TransientRepositoryTest.addNodeToRepository(TransientRepositoryTest.java:32)
	at TransientRepositoryTest.shouldNotFailWhenUsingTransientRepositoryTwice(TransientRepositoryTest.java:26)
	...
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:115)
2011-01-25 08:12:14 RepositoryImpl [ERROR] In addition to startup fail, another unexpected problem occurred while shutting down the repository again.
java.lang.NullPointerException
	at org.apache.jackrabbit.core.RepositoryImpl.doShutdown(RepositoryImpl.java:1136)
	at org.apache.jackrabbit.core.RepositoryImpl.shutdown(RepositoryImpl.java:1063)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:388)
	at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:602)
	at org.apache.jackrabbit.core.TransientRepository$1.getRepository(TransientRepository.java:179)
	at org.apache.jackrabbit.core.TransientRepository.startRepository(TransientRepository.java:279)
	at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:375)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:123)
	at TransientRepositoryTest.addNodeToRepository(TransientRepositoryTest.java:32)
	at TransientRepositoryTest.shouldNotFailWhenUsingTransientRepositoryTwice(TransientRepositoryTest.java:26)
	...
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:115)

javax.jcr.RepositoryException: File system initialization failure.
	at org.apache.jackrabbit.core.config.RepositoryConfigurationParser$6.getFileSystem(RepositoryConfigurationParser.java:1060)
	at org.apache.jackrabbit.core.config.RepositoryConfig.getFileSystem(RepositoryConfig.java:892)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:284)
	at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:602)
	at org.apache.jackrabbit.core.TransientRepository$1.getRepository(TransientRepository.java:179)
	at org.apache.jackrabbit.core.TransientRepository.startRepository(TransientRepository.java:279)
	at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:375)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:123)
	at TransientRepositoryTest.addNodeToRepository(TransientRepositoryTest.java:32)
	at TransientRepositoryTest.shouldNotFailWhenUsingTransientRepositoryTwice(TransientRepositoryTest.java:26)
	...
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:115)
Caused by: org.apache.jackrabbit.core.fs.FileSystemException: failed to initialize file system
	at org.apache.jackrabbit.core.fs.db.DatabaseFileSystem.init(DatabaseFileSystem.java:210)
	at org.apache.jackrabbit.core.config.RepositoryConfigurationParser$6.getFileSystem(RepositoryConfigurationParser.java:1057)
	... 42 more
Caused by: java.lang.IllegalStateException: this factory has already been closed
	at org.apache.jackrabbit.core.util.db.ConnectionFactory.sanityCheck(ConnectionFactory.java:213)
	at org.apache.jackrabbit.core.util.db.ConnectionFactory.getDataBaseType(ConnectionFactory.java:134)
	at org.apache.jackrabbit.core.fs.db.DbFileSystem.getDataSource(DbFileSystem.java:228)
	at org.apache.jackrabbit.core.fs.db.DatabaseFileSystem.init(DatabaseFileSystem.java:190)
	... 43 more"
0,DatabasePersistenceManager & DatabaseFileSystem: try to gracefully recover from connection loss
0,"PropertyImpl.getNode() and NamePropertyTest use different exception than documented in the JCR API JavaDocThe Property.getNode() method's JavaDoc [1] lists 3 types of exceptions: ValueFormatException, ItemNotFoundException, and RepositoryException, and that ItemNotFoundException is to be thrown when the target node could not be found.  However, the NamePropertyTest.testGetProperty() method is checking for a PathNotFoundException rather than the documented ItemNotFoundException (see [2], line 189).  Jackrabbit's implementation in PropertyImpl (see [3] line 539) delegates to Session.getNode(absolutePath) or Property.getParent().getNode(relativePath), and these methods are documented as throwing PathNotFoundException (see [4] and [5]).

Therefore, the unit test and PropertyImpl.getNode() implementation appear to be in disagreement with the JCR 2.0 API JavaDoc.

[1] http://www.day.com/maven/javax.jcr/javadocs/jcr-2.0/javax/jcr/Property.html#getNode()
[2] http://svn.apache.org/viewvc/jackrabbit/trunk/jackrabbit-jcr-tests/src/main/java/org/apache/jackrabbit/test/api/NamePropertyTest.java?revision=772352&view=markup
[3] http://svn.apache.org/viewvc/jackrabbit/trunk/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/PropertyImpl.java?revision=948827&view=markup
[4] http://www.day.com/maven/javax.jcr/javadocs/jcr-2.0/javax/jcr/Session.html#getNode(java.lang.String)
[5] http://www.day.com/maven/javax.jcr/javadocs/jcr-2.0/javax/jcr/Node.html#getNode(java.lang.String)
"
1,"addIndexesNoOptimize intermittantly throws incorrect ""segment exists in external directory..."" exceptionSpinoff from here:

    http://mail-archives.apache.org/mod_mbox/lucene-java-user/200809.mbox/%3Cba72f77f0809111418l29cf215dnd45bf679832d7d42%40mail.gmail.com%3E

Here's my response on that thread:

The bug only happens when you call addIndexesNoOptimize, and one
simple workaround would be to use SerialMergeScheduler.

I think this is already fixed in trunk (soonish to be 2.4) as a side
effect of https://issues.apache.org/jira/browse/LUCENE-1335.

In 2.3, merges that involve external segments (which are segments
folded in by addIndexesNoOptimize) are not supposed to run in a BG
thread.  This is to prevent addIndexesNoOptimize from returning until
after all external segments have been carried over (merged or copied)
into the index, so that if there is an exception (eg disk full),
addIndexesNoOptimize is able to rollback to the index to the starting
point.

The primary merge() method of CMS indeed does not BG any external
merges, but the bug is that when a BG merge finishes it then selects a
new merge to kick off and that selection is happy to pick an external
segment."
1,"DocValues merging is not associative, leading to different results depending upon how merges executerecently I cranked up TestDuelingCodecs to actually test docvalues (previously it wasn't testing it at all).

This test is simple, it indexes the same random content with 2 different indexwriters, it just allows them
to use different codecs with different indexwriterconfigs.

then it asserts the indexes are equal.

Sometimes, always on BYTES_FIXED_DEREF type, we end out with one reader that has a zero-filled byte[] for a doc,
but that same document in the other reader has no docvalues at all.
"
0,"remove MultiTermQuery get/inc/clear totalNumberOfTermsThis method is not correct if the index has more than one segment.
Its also not thread safe, and it means calling query.rewrite() modifies
the original query. 

All of these things add up to confusion, I think we should remove this 
from multitermquery, the only thing that ""uses"" it is the NRQ tests, which 
conditionalizes all the asserts anyway.
"
0,"Move Jackrabbit Query Parser from core to spi-commonsThe query parser can be used outside jackrabbit-core, for instances in other repository implementations based on JCR2SPI.

Proposal:

- move source and build infrastructure from o.a.j.core.query to o.a.j.spi.commons.query

- switch over jackrabbit.core to use spi-commons for query

- optimally, add specific test cases for the query tree generation. "
1,"FieldCache.getStringIndex should not throw exception if term count exceeds doc countSpinoff of LUCENE-2133/LUCENE-831.

Currently FieldCache cannot handle more than one value per field.
We may someday want to fix that... but until that day:

FieldCache.getStringIndex currently does a simplistic check to try to
catch when you've accidentally allowed more than one term per field,
by testing if the number of unique terms exceeds the number of
documents.

The problem is, this is not a perfect check, in that it allows false
negatives (you could have more than one term per field for some docs
and the check won't catch you).

Further, the exception thrown is the unchecked RuntimeException.

So this means... you could happily think all is good, until some day,
well into production, once you've updated enough docs, suddenly the
check will catch you and throw an unhandled exception, stopping all
searches [that need to sort by this string field] in their tracks.
It's not gracefully degrading.

I think we should simply remove the test, ie, if you have more terms
than docs then the terms simply overwrite one another.
"
1,"Cannot version the root nodeAfter making the root node versionable, the checkin method fails with the following exception. 

java.lang.ArrayIndexOutOfBoundsException: 0
    at org.apache.jackrabbit.core.version.persistence.PersistentNode.copyFrom(PersistentNode.java:589)
    at org.apache.jackrabbit.core.version.persistence.InternalFrozenNodeImpl.checkin(InternalFrozenNodeImpl.java:277)
    at org.apache.jackrabbit.core.version.persistence.InternalFrozenNodeImpl.checkin(InternalFrozenNodeImpl.java:307)
    at org.apache.jackrabbit.core.version.persistence.InternalFrozenNodeImpl.checkin(InternalFrozenNodeImpl.java:307)
    at org.apache.jackrabbit.core.version.persistence.InternalVersionHistoryImpl.checkin(InternalVersionHistoryImpl.java:354)
    at org.apache.jackrabbit.core.version.persistence.NativePVM.checkin(NativePVM.java:506)
    at org.apache.jackrabbit.core.version.VersionManagerImpl.checkin(VersionManagerImpl.java:212)
    at org.apache.jackrabbit.core.NodeImpl.checkin(NodeImpl.java:2184)
    at com.gtnet.jcr.VersionedNodeTest.testVersionRootNode(VersionedNodeTest.java:218)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at com.intellij.rt.execution.junit2.JUnitStarter.main(JUnitStarter.java:31)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:78)
"
0,"Make CachingTokenFilter fasterThe LinkedList used by CachingTokenFilter is accessed using the get() method. Direct access on a LinkedList is slow and an Iterator should be used instead. For more than a handful of tokens, the difference in speed grows exponentially."
0,"Change default value of SearchIndex extractorPoolSizeThe current default value for the extractorPoolSize is 0, which means it is disabled by default. I think we should change that default because it is a useful feature and people should not have to dig through documentation to make use of it.

The new default should be computed based on the available processors. I suggest we use: 2 * Runtime.availableProcessors()"
0,"Document the temporary free space requirements of IndexWriter methodsJust opening an issue to track fixes to javadocs around Directory
space usage of optimize(), addIndexes(*), addDocument.

This came out of a recent thread on the users list around unexpectedly
high temporary disk usage during optimize():

  http://www.gossamer-threads.com/lists/lucene/java-user/43475

"
0,"BundleFsPersistenceManager: remove deprecated settingsSome settings of the BundleFsPersistenceManager are not used internally and should be removed:

blobFSInitialCache, blobFSMaximumCache, itemFSBlockSize, itemFSInitialCache, itemFSMaximumCache"
1,"LockMethod.getResponseAsLockDiscovery() fails when status==201 RFC 4918 Section 9.10.6 specifies that 201 is a valid response code for LOCK: ""201 (Created) - The LOCK request was to an unmapped URL, the request succeeded and resulted in the creation of a new resource, and the value of the DAV:lockdiscovery property is included in the response body.""

However, LockMethod.getResponseAsLockDiscovery() would fail in that scenario. 
    org.apache.jackrabbit.webdav.DavException: Created
	 at org.apache.jackrabbit.webdav.client.methods.DavMethodBase.getResponseException(DavMethodBase.java:164)
	 at org.apache.jackrabbit.webdav.client.methods.LockMethod.getResponseAsLockDiscovery(LockMethod.java:119)


The reason is in LockMethod:175 
      return statusCode == DavServletResponse.SC_OK;

Should be: 
      return statusCode == DavServletResponse.SC_OK
             || statusCode ==DavServletResponse.SC_CREATED;"
0,"contrib/benchmark files need eol-style setThe following files in contrib/benchmark don't have eol-style set to native, so when they are checked out, they don't get converted.

./build.xml:                    
./CHANGES.txt:                                                             
./conf/sample.alg:                                                                                
./conf/standard.alg:                                                                           
./conf/sloppy-phrase.alg:                                                                                 
./conf/deletes.alg:                                                                                         
./conf/micro-standard.alg:                                                                   
./conf/compound-penalty.alg:                                                                          
"
0,"Index creates many folders when re-indexingWhen the repository is re-indexed the search index creates a lot of directories, which are finally cleaned up. If the repository contains a lot of content the number of directories that are created can be quite high (thousands of directories).

The re-indexing process should clean up unused index folders right away and not wait until the changes are committed."
0,Upgrade to Tika 1.0Tika 1.0 was released today and has many improvements over the earlier 0.10 release. We should upgrade.
0,"Remove all interning of field names from flex APIIn previous versions of Lucene, interning of fields was important to minimize string comparison cost when iterating TermEnums, to detect changes in field name. As we separated field names from terms in flex, no query compares field names anymore, so the whole performance problematic interning can be removed. I will start with doing this, but we need to carefully review some places e.g. in preflex codec.

Maybe before this issue we should remove the Term class completely. :-) Robert?"
0,"Create jackrabbit-api(.jar) and the respective jackrabbit-rmi extensionscurrently some of the management functions not covered in jcr, like notetype management and workspace creation, are not exposed via any specific api and therfor not accessible via rmi.

create a jackrabbit api and the respective rmi extension."
0,Make contrib analyzers finalThe analyzers in contrib/analyzers should all be marked final. None of the Analyzers should ever be subclassed - users should build their own analyzers if a different combination of filters and Tokenizers is desired.
0,"Supplementary Character Handling in CharTokenizerCharTokenizer is an abstract base class for all Tokenizers operating on a character level. Yet, those tokenizers still use char primitives instead of int codepoints. CharTokenizer should operate on codepoints and preserve bw compatibility. "
0,Parsing mixed inclusive/exclusive range queriesThe current query parser doesn't handle parsing a range query (i.e. ConstantScoreRangeQuery) with mixed inclusive/exclusive bounds.
0,"change sort order to binary orderSince flexible indexing, terms are now represented as byte[], but for backwards compatibility reasons, they are not sorted as byte[], but instead as if they were char[].

I think its time to look at sorting terms as byte[]... this would yield the following improvements:
* terms are more opaque by default, they are byte[] and sort as byte[]. I think this would make lucene friendlier to customizations.
* numerics and collation are then free to use their own encoding (full byte) rather than avoiding the use of certain bits to remain compatible with char[] sort order.
* automaton gets simpler because as in LUCENE-2265, it uses byte[] too, and has special hacks because terms are sorted as char[]
"
0,"Jcr-Server: DavException doesn't allow to specify an exception causeWhile DavException extends Exception it does not allow to specify a exception cause in the constructor.
Adding a separate constructor taking status code plus a Throwable would provide the possibility to specify the original cause."
0,"[PATCH] cleaner API for Field.TextCurrently there are four methods named Field.Text(). As those methods have 
the same name and a very similar method signature, everyone will think these 
are just convenience methods that do the same thing. But they behave 
differently: the one that takes a Reader doesn't store the data, the one that 
takes a String does. I know that this is documented, but it's still not a nice 
API. Methods that behave differently should have diffent names. The attached 
patch deprecates two of the old methods and adds two new ones named 
Field.StoredText(). I think this is much easier to understand from the 
programmer's point-of-view and will help avoid bugs."
1,"HttpConnection isOpen flag concurrency problemThe HttpConnection.java class contains an isOpen boolean used to track the state
of the connection (opened or closed).  The problem is that in the
closeSocketAndStreams(), the flag is only flipped at the end of the
unsynchronized method (after resources have been released) which causes a
concurrency issue in flushRequestOutputStream() where the flag is checked first
and the the outputStream is accessed.

I'm providing a patch for this problem."
0,"Remove/deprecate Tokenizer's default ctorI was working on a new Tokenizer... and I accidentally forgot to call super(input) (and super.reset(input) from my reset method)... which then meant my correctOffset() calls were silently a no-op; this is very trappy.

Fortunately the awesome BaseTokenStreamTestCase caught this (I hit failures because the offsets were not in fact being corrected).

One minimal thing we can do (but it sounds like from Robert there may be reasons why we can't) is add {{assert input != null}} in Tokenizer.correctOffset:

{noformat}
Index: lucene/core/src/java/org/apache/lucene/analysis/Tokenizer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/analysis/Tokenizer.java	(revision 1242316)
+++ lucene/core/src/java/org/apache/lucene/analysis/Tokenizer.java	(working copy)
@@ -82,6 +82,7 @@
    * @see CharStream#correctOffset
    */
   protected final int correctOffset(int currentOff) {
+    assert input != null: ""subclass failed to call super(Reader) or super.reset(Reader)"";
     return (input instanceof CharStream) ? ((CharStream) input).correctOffset(currentOff) : currentOff;
   }
{noformat}

But best would be to remove the default ctor that leaves input null..."
0,"UserImporter should use User.changePasswordthe UserImporter lists a limitation that the password value is expected to be hashed already as it writes the
value as it was retrieved from the xml-import.

Instead it could make use of User#changePassword that (in the implementation present with JR) creates a 
pw-hash if the password is found to be plain text."
0,"Make TokenStream Reuse Mandatory for AnalyzersIn LUCENE-2309 it became clear that we'd benefit a lot from Analyzer having to return reusable TokenStreams.  This is a big chunk of work, but its time to bite the bullet.

I plan to attack this in the following way:

- Collapse the logic of ReusableAnalyzerBase into Analyzer
- Add a ReuseStrategy abstraction to Analyzer which controls whether the TokenStreamComponents are reused globally (as they are today) or per-field.
- Convert all Analyzers over to using TokenStreamComponents.  I've already seen that some of the TokenStreams created in tests need some work to be reusable (even if they aren't reused).
- Remove Analyzer.reusableTokenStream and convert everything over to using .tokenStream (which will now be returning reusable TokenStreams)."
0,"Minor performance improvement to IdleConnectionHandlerThe attached patch does the following changes to IdleConnectionHandler
 - as it iterator over a map of connections, using a LinkedHashMap is a faster
 - rather than using an iterator over the keyset and subsequently getting the values, an iterator over the entry set is used instead for efficiency (at least according to FindBugs)

Note that the patch contains other changes to make variables final where possible. This was done automatically by Eclipse, and can be removed if desired. However I see no harm in them, other than they affect more of the code than intended by the patch."
1,"XML import using MacOS X WebDAV client does not workwhen trying to import a xml file via a webdav mount this does not work.

this is mainly because the client first tries to create a 0-sized file, which fails with the xml importer. after the file is created, it will lock it and put the xml body. a second problem might be the ""dot-underscore"" files mac tries to create. "
1,"New LaxRedirectStrategy class should probably call the super method first.LaxRedirectStrategy extends the defaulRedirect class but does not call the super method as one would expect.

Just adding a patch to make sure it gets called."
0,"Improve InfoStream class in trunk to be more consistent with logging-frameworks like slf4j/log4j/commons-loggingFollowup on a [thread by Shai Erea on java-dev@lao|http://lucene.472066.n3.nabble.com/IndexWriter-infoStream-is-final-td3537485.html]: I already discussed with Robert about that, that there is one thing missing. Currently the IW only checks if the infoStream!=null and then passes the message to the method, and that *may* ignore it. For your requirement it is the case that this is enabled or disabled dynamically. Unfortunately if the construction of the message is heavy, then this wastes resources.

I would like to add another method to this class: abstract boolean isEnabled() that can also be implemented. I would then replace all null checks in IW by this method. The default config in IW would be changed to use a NoOutputInfoStream that returns false here and ignores the message.

A simple logger wrapper for e.g. log4j / slf4j then could look like (ignoring component, could be enabled):

{code:java}
Loger log = YourLoggingFramework.getLogger(IndexWriter.class);

public void message(String component, String message) {
  log.debug(component + "": "" + message);
}

public boolean isEnabled(String component) {
  return log.isDebugEnabled();
}
{code}

Using this you could enable/disable logging live by e.g. the log4j management console of your app server by enabling/disabling IndexWriter.class logging.

The changes are really simple:
- PrintStreamInfoStream returns true, always, mabye make it dynamically enable/disable to allow Shai's request
- infoStream.getDefault() is never null and can never be set to null. Instead the default is a singleton NoOutputInfoStream that returns false of isEnabled(component).
- All null checks on infoStream should be replaced by infoStream.isEanbled(component), this is possible as always != null. There are no slowdowns by this - it's like Collections.emptyList() instead stupid null checks."
1,"If indexwriter hits a non-ioexception from indexExists it leaks a write.lockthe rest of IW's ctor is careful about this.

IndexReader.indexExists catches any IOException and returns false, but the problem
occurs if some other exception (in my test, UnsupportedOperationException, but you
can imagine others are possible), when trying to e.g. read in the segments file.

I think we just need to move the IR.exists stuff inside the try / finally"
1,"WorkspaceImporter throws exceptionsessio.getWorkspace().getImportContentHandler() throws 
java.lang.UnsupportedOperationException: Workspace-Import of protected nodes: Not yet implement.

suggest to issue warning instead of throwing."
0,"refactor HttpClientConnection and HttpProxyConnectionInstead of trying to define a full abstraction for client connections, let's define only a minimal interface in HttpCore with only those methods actually needed in the core. In particular, the core does not need to open connections (since HTTPCORE-11), and it does not care whether a connection is direct or through a proxy. An abstraction for client connections can be defined in HttpConn.

(original description:)
As discussed on the mailing list, separating the responsibility for establishing connections from the connection objects could improve the design and help with proxy support.
"
1,"JNDI Referencable IssuesI'm questioning the use of Referencable in the BindableResource and BindableResourceFactory classes for the JNDI lookup process. Reason for this is because Referencable needs the Addrs to be in the EXACT order in order for it to be considered the same. (see http://java.sun.com/j2se/1.4.2/docs/api/javax/naming/Reference.html#equals(java.lang.Object) )

In order for me to get the JNDI reference to be found correctly I had to change the BindableResource.getReference method to swap the order the StringReferences were added to match up what was being passed in by glassfish. This seems EXTREMELY fragile to me as I don't know what order, say JBoss, would pass the StringRefences in in the Reference object for the Factory method.

Also, another problem is that getReference is binding the class name to BindableRepository class implementation and not javax.jcr.Repository. This again causes them not to match if you follow the example on the wiki on setting up the JNDI reference and use javax.jcr.Repository as the type. This can either be fixed by changing the JNDI reference to use the BindableRepository class or the change the BindableRepository class to set that to the Repository interface. Not sure which would be considered 'better'

I have a patch that fixes the first issue (at least for glassfish), but not the second. Again, this seems like a really 'breakable' setup right now and not sure what would be better to make sure this is avoided."
0,"[PATCH] simplify conversion of strings to primitives by using parseXXX, not valueOf(xxx).xxxValue()Code converts strings to primitives using a two step process, eg

Boolean.valueOf(myString).booleanValue();

can be simplified to 

Boolean.parseBoolean(myString);

true of Float, Double, Int etc. 

In some cases, this avoids allocating temporary boxed objects

patch fixes this."
1,"fix assertions/checks that use File.length() to use getFilePointer()This came up on this thread ""Getting RuntimeException: after flush: fdx size mismatch while Indexing"" 
(http://www.lucidimagination.com/search/document/a8db01a220f0a126)

In trunk, a side effect of the codec refactoring is that these assertions were pushed into codecs as finish() before close().
they check getFilePointer() instead in this computation, which checks that lucene did its part (instead of falsely tripping if directory metadata is stale).

I think we should fix these checks/asserts on 3.x too
"
1,"Database connection leak with DBCP, MySQL, and ObserversWhen using DBCP and MySQL with an observer that modifies the content repository, we are seeing abandoned connections in our connection pool."
0,Improve docs for deployment Models 1 and 2 on Tomcat 5.5.x and provide an example webapp.New users would find a small webapp and associated documentation that walks them through the process of setting up a model 1 or model 2 (or both) deployment scheme.
0,"Avoid string concatenation in AbstractBundlePersistenceManagerThe following line:

        log.debug(""stored bundle "" + bundle.getId());

should be changed to:

        log.debug(""stored bundle {}"", bundle.getId());
"
1,"ShingleFilter skips over trie-shingles if outputUnigram is set to falseSpinoff from http://lucene.markmail.org/message/uq4xdjk26yduvnpa

{quote}
I noticed that if I set outputUnigrams to false it gives me the same output for
maxShingleSize=2 and maxShingleSize=3.

please divide divide this this sentence

when i set maxShingleSize to 4 output is:

please divide please divide this sentence divide this this sentence

I was expecting the output as follows with maxShingleSize=3 and
outputUnigrams=false :

please divide this divide this sentence 
{quote}


"
1,"Redirect to a relative URL failsRequest the url 
http://commerce1.cera.net/discount-pcbooks/catalog/categories.asp?
search_str=0782128092

On a browser the redirect works, while with HttpClient it doesn't."
1,"The deprecated constructor of BooleanClause does not set new stateNick Burch reported this on lucene-user. 
 
Patch will follow. 
 
Regards, 
Paul Elschot"
0,Upgrade all default socket factories to use SO_REUSEADDR parameterSee HTTPCORE-209
0,"Add generics to DocumentsWriterDeleteQueue.NodeDocumentsWriterDeleteQueue.Note should be generic as the subclasses hold different types of items. This generification is a little bit tricks, but the generics policeman can't wait to fix this *g*."
1,"BoostingTermQuery's explanation should be marked as Match even if the payload part negated or zero'ed itSince BTQ multiplies the payload on the score it might return a negative score.
The explanation should be marked as ""Match"" otherwise it is not added to container explanations,
See also in LUCENE-1302."
0,Replace IndexReader.getFieldNames with IndexReader.getFieldInfos
0,"Node Type Management subproject : Default namespace should be emtpyWhen creating node types matching to the class descriptors,  the default namespace should be empty instead of  'ocm'."
0,"queryparser makes all CJK queries phrase queries regardless of analyzerThe queryparser automatically makes *ALL* CJK, Thai, Lao, Myanmar, Tibetan, ... queries into phrase queries, even though you didn't ask for one, and there isn't a way to turn this off.

This completely breaks lucene for these languages, as it treats all queries like 'grep'.

Example: if you query for f:abcd with standardanalyzer, where a,b,c,d are chinese characters, you get a phrasequery of ""a b c d"". if you use cjk analyzer, its no better, its a phrasequery of  ""ab bc cd"", and if you use smartchinese analyzer, you get a phrasequery like ""ab cd"". But the user didn't ask for one, and they cannot turn it off.

The reason is that the code to form phrase queries is not internationally appropriate and assumes whitespace tokenization. If more than one token comes out of whitespace delimited text, its automatically a phrase query no matter what.

The proposed patch fixes the core queryparser (with all backwards compat kept) to only form phrase queries when the double quote operator is used. 

Implementing subclasses can always extend the QP and auto-generate whatever kind of queries they want that might completely break search for languages they don't care about, but core general-purpose QPs should be language independent.
"
0,Cookie docs are outdated.The cookie docs do not reflect the latest code changes.
0,"Cache jcr name to QName mappingsCurrently jcr names are always parsed and resolved into QName instances. Introducing a cache would increase performance and also save memory because well known and often used jcr names would always return the same QName instance from cache.

Testing with common read operations shows a performance improvement of about 25%.
The test involved the following methods on Node interface:

- getProperty()
- getProperties()
- getName()
- getPath()
- isLocked()
- isNodeType()
- getPrimaryNodeType()
- hasNodes()
- getNodes()

Attached proposed implementation of a QNameResolver.

Please comment."
1,"Query index not in sync with workspaceAfter some time the search index is not in sync anymore with the data in the workspace and returns uuids which have no corresponding Node in the workspace. This results in a NodeIterator which throws an ItemNotFoundException on nextNode().

Instructions how to reproduce this error are not yet available.

Possible areas for further investigation are:
- NodeType registry which maps the node types into the workspace with the use of virtual item states
- versioning?
- atomicity of indexing?"
1,"Fix small perf issues with String/TermOrdValComparatorUncovered some silliness when working on LUCENE-2504, eg we are doing unnecessary binarySearch on a single-segment reader."
1,"SpanRegexQuery and SpanNearQuery is not working with MultiSearcherMultiSearcher is using:
queries[i] = searchables[i].rewrite(original);
to rewrite query and then use combine to combine them.

But SpanRegexQuery's rewrite is different from others.
After you call it on the same query, it always return the same rewritten queries.

As a result, only search on the first IndexSearcher work. All others are using the first IndexSearcher's rewrite queries.
So many terms are missing and return unexpected result.

Billow"
0,"Pass potent SR to IRWarmer.warm(), and also call warm() for new segmentsCurrently warm() receives a SegmentReader without terms index and docstores.
It would be arguably more useful for the app to receive a fully loaded reader, so it can actually fire up some caches. If the warmer is undefined on IW, we probably leave things as they are.

It is also arguably more concise and clear to call warm() on all newly created segments, so there is a single point of warming readers in NRT context, and every subreader coming from getReader is guaranteed to be warmed up -> you don't have to introduce even more mess in your code by rechecking it.

"
0,"Move NoDeletionPolicy from benchmark to coreAs the subject says, but I'll also make it a singleton + add some unit tests, as well as some documentation. I'll post a patch hopefully today."
0,"Add LuSql project to ""Apache Lucene - Contributions"" wiki pageAdd [LuSql|http://lab.cisti-icist.nrc-cnrc.gc.ca/cistilabswiki/index.php/LuSql] to the Apache Lucene - Contributions page [http://lucene.apache.org/java/2_9_0/contributions.html]
I am the author of LuSql. I can supply any text needed. 

Perhaps a new heading is needed to capture Database/JDBC oriented Lucene tools (there are others out there)?"
1,"/contrib/orm-persistence/ OJBPersistenceManagerOJBPersistenceManager seems to have the following problems

1. OJBPersistenceBroker inherites from AbstractPersistenceBroker. There's no 
need of using a non transactional implementation as the feature is available in 
jdbc. 

2. A single broker is used and It's not thread-safe. This is not a problem now 
because it inherits from AbstractPersistenceManager, and the store(ChangeLog ) 
method is synchronized.

3. The broker is never closed so it leaves an open connection.

4. There's no pooling with only one broker.

5 Each write method (e.g. store(NodeState state)) starts its own transaction 
but the transaction should start and end in store(ChangeLog log).

6. It never rollbacks, even when an item in the changelog can't be persisted.

7. The mysql example create MyISAM tables which don't support transactions. 
Innodb tables would be more appropriate.

8. jdbc to java type mapping is wrong for 
class: org.apache.jackrabbit.core.state.orm.ORMBlobValue
field: size
Changed from INTEGER to BIGINT

9. When a Blob value is loaded a ArrayStoreException is thrown because in 
load(PropertyId id) BlobFileValues are added to internalValueList instead of 
InternalValue instances.

10. in store(NodeReferences). When storing a NodeReferences which have some (but not all) the references deleted the OJB persistence Manager doesn't delete any one.

Some of this problems are present in the Hibernate implementation."
0,Backport JCR-1197: Node.restore() may throw InvalidItemStateExceptionBackport issue JCR-1197 (Node.restore() may throw InvalidItemStateException) to 1.3 branch for 1.3.4 (separate issue to avoid re-opening JCR-1197 which was already released with 1.4).
0,"Provide names for constants in QueryConstantsFor debugging, logging, and user interaction purposes QueryConstants should include descriptive names for the constants it provides."
0,"Typo in log outputjackrabbit/src/java/org/apache/jackrabbit/core/fs/local/LocalFileSystem.java:
133c133
<         log.info(""LocaaFileSystem initialized on "" + root.getPath());
---
>         log.info(""LocalFileSystem initialized on "" + root.getPath());
"
0,DEFAULT spelled DEFALT in MoreLikeThis.javaDEFAULT is spelled DEFALT in contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis.java
0,"Upgrade to latest SLF4J and LogbackWhile fixing JCR-2836 I ran into LBCLASSIC-183 [1] that's fixed in a recent Logback release. To get this and other fixes we should upgrade Logback and SLF4J in Jackrabbit 2.3.

[1] http://jira.qos.ch/browse/LBCLASSIC-183"
0,"[PATCH] png, apng, mng text extractorText extractor for tEXt chunk for png, apng and mng formats"
1,"Mixin type lossWhen using a bundle persistence manager, the mixin type information may be corrupted in the lucene index, causing queries like '//element(*, my:mixin)' to fail.



The problem is that the 'jcr:mixinTypes' may be stored in the bundle. Here is how this could happen :


First step: Create a node and add a mixin 'A'.

Everything's fine. The query '//element(*, 'A')' works.


Second step : Select the node and add a second mixin 'B'.

When the second mixin is added, the AbstractBundlePersistenceManager#load(PropertyId) is called to get the current mixins for the node. This method will store the PropertyState for 'jcr:mixinTypes' in the bundle (containing only the mixin 'A'). Then the NodeImpl#setMixinTypesProperty() will set the PropertyState for 'jcr:mixinTypes' in the node state (containing the mixins 'A' and 'B').
When the session is saved, the ChangeLog in AbstractBundlePersistenceManager#store() contains a modification for the 'jcr:mixinTypes' but it's being ignored, leaving the bundle with only mixin 'A'. The NodeIndexer looks into the node state to get the mixin types and indexes the node correctly. The queries '//element(*, 'A')' and '//element(*, 'B')' work.


Thrid step : Select the node and update a property. 

When the session is saved, the NodeIndexer asks again for the 'jcr:mixinTypes' property, by calling the AbstractBundlePersistenceManager#load(PropertyId) to load it. The bundle contains this property and returns only mixin 'A' (as it was stored in the second step), causing the index to use only mixin 'A'. The query '//element(*, 'A')' still works but '//element(*, 'B') doesn't work anymore.



A simple solution to this would be to not store the PropertyState for the 'jcr:mixinTypes' (and 'jcr:uuid' and 'jcr:primaryType', as the class description states) in the bundle when the PropertyState is loaded. It would fix the issue but not the contents on existing repositories. One way to allow the repositories to fix themselves is to not read or write these 3 properties in the BundleBinding#readBundle and BundleBinding#writeBundle methods, but I'm not sure wether or not it would have a performance impact.


"
0,"Demo HTML parser gives incorrect summaries when title is repeated as a headingIf you have an html document where the title is repeated as a heading at the top of the document, the HTMLParser will return the title as the summary, ignoring everything else that was added to the summary. Instead, it should keep the rest of the summary and chop off the title part at the beginning (essentially the opposite). I don't see any benefit to repeating the title in the summary for any case.

In HTMLParser.jj's getSummary():

    String sum = summary.toString().trim();
    String tit = getTitle();
    if (sum.startsWith(tit) || sum.equals(""""))
      return tit;
    else
      return sum;

change it to: (* denotes a line that has changed)

    String sum = summary.toString().trim();
    String tit = getTitle();
*    if (sum.startsWith(tit))             // don't repeat title in summary
*      return sum.substring(tit.length()).trim();
    else
      return sum;
"
1,Lucene queries are not properly rewrittenSome of the jackrabbit internal lucene queries are not properly rewritten and may lead to UnsupportedOperationException when terms are extracted from the lucene query.
1,"Deleting docs of all returned Hits during search causes ArrayIndexOutOfBoundsExceptionFor background user discussion:
http://www.nabble.com/document-deletion-problem-to14414351.html

{code}
Hits h = m_indexSearcher.search(q); // Returns 11475 documents 
for(int i = 0; i < h.length(); i++) 
{ 
  int doc = h.id(i); 
  m_indexSearcher.getIndexReader().deleteDocument(doc);  <-- causes ArrayIndexOutOfBoundsException when i = 6400
} 
{code}
"
0,"simple improvements to testsSimon had requested some docs on what all our test options do, so lets clean it up and doc it.

i propose:
# change all vars to be tests.xxx (e.g. tests.threadspercpu, tests.multiplier, ...)
# ensure all 6 build systems (lucene, solr, each solr contrib) respect these.
# add a simple wiki page listing what these do."
1,"SSLSocketFactory.createSSLContext does not process trust storeorg.apache.http.conn.ssl.SSLSocketFactory.createSSLContext() does not process a provided trust store.
Only the default (cacerts) is processed. An additional provided trust store is ignored.
Adding the ""trusted"" certificate to the keystore, the peer is authenticated.

Eventually
        tmfactory.init(keystore);
needs to be
        tmfactory.init(truststore);

"
0,"Improve name resolutionAs discussed in JCR-685, the current CachingNamespaceResolver class contains excessive synchronization causing monitor contention that reduces performance.

In JCR-685 there's a proposed patch that replaces synchronization with a read-write lock that would allow concurrent read access to the name cache."
0,"Add insertWithOverflow to PriorityQueueThis feature proposes to add an insertWithOverflow to PriorityQueue so that callers can reuse the objects that are being dropped off the queue. Also, it changes heap to protected for easier extensibility of PQ"
1,"TestStressNRT failures (reproducible)Build server logs. Reproduces on at least two machines.

{noformat}
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestStressNRT -Dtestmethod=test -Dtests.seed=69468941c1bbf693:19e66d58475da929:69e9d2f81769b6d0 -Dargs=""-Dfile.encoding=UTF-8""
    [junit] NOTE: test params are: codec=Lucene3x, sim=RandomSimilarityProvider(queryNorm=true,coord=false): {}, locale=ro, timezone=Etc/GMT+1
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestStressNRT]
    [junit] NOTE: Linux 3.0.0-16-generic amd64/Sun Microsystems Inc. 1.6.0_27 (64-bit)/cpus=2,threads=1,free=74960064,total=135987200
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: test(org.apache.lucene.index.TestStressNRT):	Caused an ERROR
    [junit] MockDirectoryWrapper: cannot close: there are still open files: {_ng.cfs=8}
    [junit] java.lang.RuntimeException: MockDirectoryWrapper: cannot close: there are still open files: {_ng.cfs=8}
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:555)
    [junit] 	at org.apache.lucene.index.TestStressNRT.test(TestStressNRT.java:385)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$SubclassSetupTeardownRule$1.evaluate(LuceneTestCase.java:743)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$InternalSetupTeardownRule$1.evaluate(LuceneTestCase.java:639)
    [junit] 	at org.apache.lucene.util.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:22)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$TestResultInterceptorRule$1.evaluate(LuceneTestCase.java:538)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$RememberThreadRule$1.evaluate(LuceneTestCase.java:600)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:164)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)
    [junit] 	at org.apache.lucene.util.StoreClassNameRule$1.evaluate(StoreClassNameRule.java:21)
    [junit] 	at org.apache.lucene.util.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:22)
    [junit] Caused by: java.lang.RuntimeException: unclosed IndexInput: _ng.cfs
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.addFileHandle(MockDirectoryWrapper.java:479)
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper$1.openSlice(MockDirectoryWrapper.java:777)
    [junit] 	at org.apache.lucene.store.CompoundFileDirectory.openInput(CompoundFileDirectory.java:221)
    [junit] 	at org.apache.lucene.codecs.lucene3x.TermInfosReader.<init>(TermInfosReader.java:112)
    [junit] 	at org.apache.lucene.codecs.lucene3x.Lucene3xFields.<init>(Lucene3xFields.java:84)
    [junit] 	at org.apache.lucene.codecs.lucene3x.PreFlexRWPostingsFormat$1.<init>(PreFlexRWPostingsFormat.java:51)
    [junit] 	at org.apache.lucene.codecs.lucene3x.PreFlexRWPostingsFormat.fieldsProducer(PreFlexRWPostingsFormat.java:51)
    [junit] 	at org.apache.lucene.index.SegmentCoreReaders.<init>(SegmentCoreReaders.java:108)
    [junit] 	at org.apache.lucene.index.SegmentReader.<init>(SegmentReader.java:51)
    [junit] 	at org.apache.lucene.index.IndexWriter$ReadersAndLiveDocs.getMergeReader(IndexWriter.java:521)
    [junit] 	at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3587)
    [junit] 	at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3257)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:382)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:451)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.index.TestStressNRT FAILED
{noformat}"
1,"ConcurrentModificationException during registration of nodetypesDuring the registration of a set of nodetypes this exception may be encountered:

java.util.ConcurrentModificationException
        at org.apache.commons.collections.map.AbstractReferenceMap$ReferenceEntrySetIterator.checkMod(AbstractReferenceMap.java:761)
        at org.apache.commons.collections.map.AbstractReferenceMap$ReferenceEntrySetIterator.hasNext(AbstractReferenceMap.java:735)
        at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.notifyRegistered(NodeTypeRegistry.java:1750)
        at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.registerNodeTypes(NodeTypeRegistry.java:223)

It seems that the copying of the listeners triggered this exception:

    private void notifyRegistered(QName ntName) {
        // copy listeners to array to avoid ConcurrentModificationException
        NodeTypeRegistryListener[] la =
                new NodeTypeRegistryListener[listeners.size()];
        Iterator iter = listeners.values().iterator();
        int cnt = 0;
1750:   while (iter.hasNext()) {
            la[cnt++] = (NodeTypeRegistryListener) iter.next();
        }
        for (int i = 0; i < la.length; i++) {
            if (la[i] != null) {
                la[i].nodeTypeRegistered(ntName);
            }
        }
    }

The methods ""notifyReRegistered"" and ""notifyUnregistered"" will probably suffer from the same problem.

Reproduction of this exception may be tricky; it only occurred once in our application. It is probably a race condition: another thread might access the listeners during the copy. It may be helpful to use a debugger and set a breakpoint in the middle of the copy giving other threads the opportunity to access the listeners...

We think that a possible solution is the following:

    /**
     * Notify the listeners that a node type <code>ntName</code> has been registered.
     */
    private void notifyRegistered(QName ntName) {
        // copy listeners to array to avoid ConcurrentModificationException
    	NodeTypeRegistryListener[] la;
    	synchronized (listeners) {
            la = (NodeTypeRegistryListener[]) listeners.values().toArray(new NodeTypeRegistryListener[listeners.size()]);
		}

        for (int i = 0; i < la.length; i++) {
            if (la[i] != null) {
                la[i].nodeTypeRegistered(ntName);
            }
        }
    }



"
0,"Allow servlet filters to specify custom session providersIn order to integrate the Jackrabbit davex server functionality with their custom authentication logic, the Sling project currently needs to embed and subclass the davex servlet classes. It would be cleaner if such tight coupling wasn't needed.

One way to achieve something like that would be to allow external components to provide a custom SessionProvider instance as an extra request attribute. This way for example a servlet filter that implements such custom authentication logic could easily make its functionality available to the standard davex servlet in Jackrabbit."
1,"SearchWithSortTask ignores sorting by DocDuring my work in LUCENE-3912, I found the following code:

{code}
if (field.equals(""doc"")) {
    sortField0 = SortField.FIELD_DOC;
} if (field.equals(""score"")) {
    sortField0 = SortField.FIELD_SCORE;
} ...
{code}

This means the setting of SortField.FIELD_DOC is ignored.  While I don't know much about this code, this seems like a valid setting and obviously just a bug."
1,"TestIndexWriter.testBackgroundOptimize fails with too many open filesRecreate with this line:

ant test -Dtestcase=TestIndexWriter -Dtestmethod=testBackgroundOptimize -Dtests.seed=-3981504507637360146:51354004663342240

Might be related to LUCENE-2873 ?"
1,"AbstractJournal doesn't create deep paths for revision filesAbstractJournal throws when trying to create the revision file if the directory the revision file is in doesn't already exist. When initializing a repository during its startup, the create fails is you use a revision param like <param name=""revision"" value=""${rep.home}/repository/revision"" /> because the repository directory hasn't been created yet. Attached is a repository.xml that demonstrates. It uses Oracle for FS and PMs."
1,"DocumentViewExportVisitor class incorrectly handles XML escaping for element namesThe method private static String escapeName(String name) should have the following test:

 if ((i == 0) ? XMLChar.isNCNameStart(ch) : XMLChar.isNCName(ch)) {

changed into

 if ((i == 0) ? !XMLChar.isNCNameStart(ch) :! XMLChar.isNCName(ch)) {

in order to properly escape the text (those two methods in XMLChar return true when the character is valid, not the other way around."
0,"DefaultLoginModule performs anonymous login in case of unsupported Credentials implementationIf Repository.login in called with an unsupported Credentials implementation the DefaultLoginModule#getCredentials returns null
and thus an anonymous login. The expected behavior from my point of view however was, that login with unsupported credentials 
would not be handled by the LoginModule and - if no other module is able to handle it -  login would consequently fails."
0,Remove DocumentWriterDocumentWriter has been replaced by DocumentsWriter (from LUCENE-843) so we need to remove it & fix the unit tests that directly use it...
0,Fixes a handful of misspellings/mistakes in changes.txtThere are a handful of misspellings/mistakes in changes.txt. This patch fixes them. Avoided the one or two British to English conversions <g>
0,"AuthorizableImpl#memberOf and #declaredMemberOf should return RangeIteratorit would be favorable if the iterator returned by Authorizable#memberOf and #declaredMemberOf
would return a RangeIterator in order to all the caller to determine the size without having to
iterate."
1,"SQL parser chokes on prefixes containing a ""-"" characterSQL parser chokes on prefixes containing a ""-"" character, such as in

  SELECT a-b:c FROM nt:resource

"
0,"add an interface for plugable dns clientsCurrently Httpclient implicitly uses InetAddress.getByName() for DNS resolution.
This has some drawbacks. One is that the DNS cache of Java per default caches entries forever.

So I'd like to be able to replace InetAddress.getByName() with another DNS client implementation.

"
1,"NullpointerException in SessionItemStateManagerI got the following exception which is not reproducible and occured during a large batch of write operations. Unfortunately I got no idea how this happened. May be someone has an idea?

[2006-11-27 21:43:53,065, WARN ] {} support.RemoteInvocationTraceInterceptor:80: Processing of RmiServiceExporter remote call resulted in fatal exception: com.subshell.sophora.content.server.IContentManager.importDocument
org.springframework.transaction.TransactionSystemException: Could not commit JCR transaction; nested exception is java.lang.NullPointerException Caused by: 
java.lang.NullPointerException
        at org.apache.jackrabbit.core.state.SessionItemStateManager.nodeModified(SessionItemStateManager.java:878)
        at org.apache.jackrabbit.core.state.StateChangeDispatcher.notifyNodeModified(StateChangeDispatcher.java:143)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.stateModified(LocalItemStateManager.java:426)
        at org.apache.jackrabbit.core.state.StateChangeDispatcher.notifyStateModified(StateChangeDispatcher.java:85)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.stateModified(SharedItemStateManager.java:388)
        at org.apache.jackrabbit.core.state.StateChangeDispatcher.notifyStateModified(StateChangeDispatcher.java:85)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.stateModified(SharedItemStateManager.java:388)
        at org.apache.jackrabbit.core.state.ItemState.notifyStateUpdated(ItemState.java:241)
        at org.apache.jackrabbit.core.state.ChangeLog.persisted(ChangeLog.java:271)
        at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:691)
        at org.apache.jackrabbit.core.state.XAItemStateManager.commit(XAItemStateManager.java:169)
        at org.apache.jackrabbit.core.version.XAVersionManager.commit(XAVersionManager.java:478)
        at org.apache.jackrabbit.core.TransactionContext.commit(TransactionContext.java:172)
        at org.apache.jackrabbit.core.XASessionImpl.commit(XASessionImpl.java:315)
        at org.springmodules.jcr.jackrabbit.support.JackRabbitUserTransaction.commit(JackRabbitUserTransaction.java:104)
        at org.springmodules.jcr.jackrabbit.LocalTransactionManager.doCommit(LocalTransactionManager.java:192)
        at org.springframework.transaction.support.AbstractPlatformTransactionManager.processCommit(AbstractPlatformTransactionManager.java:540)
        at org.springframework.transaction.support.AbstractPlatformTransactionManager.commit(AbstractPlatformTransactionManager.java:510)
        at org.springframework.transaction.interceptor.TransactionAspectSupport.commitTransactionAfterReturning(TransactionAspectSupport.java:310)
        at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:117)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:185)
        at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:89)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:185)
        at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:209)
        at $Proxy14.importDocument(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:318)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:203)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:162)
        at org.springframework.remoting.support.RemoteInvocationTraceInterceptor.invoke(RemoteInvocationTraceInterceptor.java:70)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:185)
        at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:209)
        at $Proxy15.importDocument(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor21.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.springframework.remoting.support.RemoteInvocation.invoke(RemoteInvocation.java:181)
        at org.springframework.remoting.support.DefaultRemoteInvocationExecutor.invoke(DefaultRemoteInvocationExecutor.java:38)
        at org.springframework.remoting.support.RemoteInvocationBasedExporter.invoke(RemoteInvocationBasedExporter.java:76)
        at org.springframework.remoting.rmi.RmiBasedExporter.invoke(RmiBasedExporter.java:72)
        at org.springframework.remoting.rmi.RmiInvocationWrapper.invoke(RmiInvocationWrapper.java:62)
        at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:294)
        at sun.rmi.transport.Transport$1.run(Transport.java:153)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:149)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:460)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:701)
        at java.lang.Thread.run(Thread.java:595)
"
0,"don't require an analyzer, if all fields are NOT_ANALYZEDThis seems wierd, if you analyze only NOT_ANALYZED fields, you must have an analyzer (null will not work)
because documentsinverter wants it for things like offsetGap"
0,"Test failure: org.apache.jackrabbit.test.TestAllSubsequent test runs fail unless doing a mvn clean first.

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.jackrabbit.spi2jcr.spi.TestAll
Tests run: 50, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.965 sec
Running org.apache.jackrabbit.test.TestAll
Tests run: 1038, Failures: 11, Errors: 0, Skipped: 0, Time elapsed: 44.925 sec <<< FAILURE!
Running org.apache.jackrabbit.spi2jcr.jcr2spi.TestAll
Tests run: 394, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.416 sec

Results :

Failed tests:
  testOrderByAscending(org.apache.jackrabbit.test.api.query.SQLOrderByTest)
  testOrderByDescending(org.apache.jackrabbit.test.api.query.SQLOrderByTest)
  testOrderByDefault(org.apache.jackrabbit.test.api.query.SQLOrderByTest)
  testDocOrderIndexedNotation(org.apache.jackrabbit.test.api.query.XPathPosIndexTest)
  testDocOrderPositionFunction(org.apache.jackrabbit.test.api.query.XPathDocOrderTest)
  testDocOrderPositionIndex(org.apache.jackrabbit.test.api.query.XPathDocOrderTest)
  testDocOrderLastFunction(org.apache.jackrabbit.test.api.query.XPathDocOrderTest)
  testDocOrderFirstFunction(org.apache.jackrabbit.test.api.query.XPathDocOrderTest)
  testOrderByAscending(org.apache.jackrabbit.test.api.query.XPathOrderByTest)
  testOrderByDescending(org.apache.jackrabbit.test.api.query.XPathOrderByTest)
  testOrderBy(org.apache.jackrabbit.test.api.query.XPathOrderByTest)

"
1,"Preemptive Authorization parameter initialization incorrect, causes preemptive auth not to workPreemptive authorization is defeated by an incorrect initialization. Patch 
follows:
--- DefaultHttpParamsFactory.java       2005-10-10 19:09:10.000000000 -0700
+++ DefaultHttpParamsFactory.java.fixed 2005-10-17 17:00:10.259174920 -0700
@@ -118,9 +118,9 @@
         if (preemptiveDefault != null) {
             preemptiveDefault = preemptiveDefault.trim().toLowerCase();
             if (preemptiveDefault.equals(""true"")) {
-                params.setParameter
(HttpClientParams.PREEMPTIVE_AUTHENTICATION, ""on"");
+                params.setParameter
(HttpClientParams.PREEMPTIVE_AUTHENTICATION, Boolean.TRUE);
             } else if (preemptiveDefault.equals(""false"")) {
-                params.setParameter
(HttpClientParams.PREEMPTIVE_AUTHENTICATION, ""off"");
+                params.setParameter
(HttpClientParams.PREEMPTIVE_AUTHENTICATION, Boolean.FALSE);
             }
         }"
1,"remove IndexInput.copyBufthis looks really broken/dangerous as an instance variable.

what happens on clone() ?! copyBytes can instead make its own array inside the method.

its protected, so ill list in the 3.x backwards breaks section since its technically a backwards break."
0,change some access-level modifiers to allow for better subclassingSome of the methods in the core parts of jackrabbit are package protected and do not allow easy subclassing. suggest to make some of the methods 'protected'.
1,"Error while restoring OPV=Version childnodes (Restore of root version not allowed)when restoring a version of a node (by name) that has opv=version childnodes, the following error is thrown, if such a version does not exist in the child nodes versionhistory:

Error while restoring nodes: javax.jcr.version.VersionException: Restore of root version not allowed."
0,"Add unit test showing how to do a ""live backup"" of an indexThe question of how to backup an index comes up every so often on the
lists.  Backing up and index is also clearly an important fundamental
admin task that many applications need to do for fault tolerance.

In the past you were forced to stop & block all changes to your index,
perform the backup, and then resume changes.  But many applications
cannot afford a potentially long pause in their indexing.

With the addition of DeletionPolicy (LUCENE-710), it's now possible to
do a ""live backup"", which means backup your index in the background
without pausing ongoing changes to the index.  This
SnapshotDeletionPolicy just has to mark the chosen commit point as not
deletable, until the backup finishes.
"
1,"SSL connections cannot be established using the IP addressHttpClient 4.x introduced a regression in establishing SSL connections to remote peers. The AbstractVerifier class only checks for matches in CN and SubjectAlternative->DNSName. But, when an IP (instead of a hostname) is used, the check should be done on CN and SubjectAlternative->IPAddress."
0,"Sun hotspot compiler bug in 1.6.0_04/05 affects LuceneThis is not a Lucene bug.  It's an as-yet not fully characterized Sun
JRE bug, as best I can tell.  I'm opening this to gather all things we
know, and to work around it in Lucene if possible, and maybe open an
issue with Sun if we can reduce it to a compact test case.

It's hit at least 3 users:

  http://mail-archives.apache.org/mod_mbox/lucene-java-user/200803.mbox/%3c8c4e68610803180438x39737565q9f97b4802ed774a5@mail.gmail.com%3e
  http://mail-archives.apache.org/mod_mbox/lucene-solr-user/200804.mbox/%3c4807654E.7050900@virginia.edu%3e
  http://mail-archives.apache.org/mod_mbox/lucene-java-user/200805.mbox/%3c733777220805060156t7fdb8fectf0bc984fbfe48a22@mail.gmail.com%3e

It's specific to at least JRE 1.6.0_04 and 1.6.0_05, that affects
Lucene.  Whereas 1.6.0_03 works OK and it's unknown whether 1.6.0_06
shows it.

The bug affects bulk merging of stored fields.  When it strikes, the
segment produced by a merge is corrupt because its fdx file (stored
fields index file) is missing one document.  After iterating many
times with the first user that hit this, adding diagnostics &
assertions, its seems that a call to fieldsWriter.addDocument some
either fails to run entirely, or, fails to invoke its call to
indexStream.writeLong.  It's as if when hotspot compiles a method,
there's some sort of race condition in cutting over to the compiled
code whereby a single method call fails to be invoked (speculation).

Unfortunately, this corruption is silent when it occurs and only later
detected when a merge tries to merge the bad segment, or an
IndexReader tries to open it.  Here's a typical merge exception:

{code}
Exception in thread ""Thread-10"" 
org.apache.lucene.index.MergePolicy$MergeException: 
org.apache.lucene.index.CorruptIndexException:
    doc counts differ for segment _3gh: fieldsReader shows 15999 but segmentInfo shows 16000
        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:271)
Caused by: org.apache.lucene.index.CorruptIndexException: doc counts differ for segment _3gh: fieldsReader shows 15999 but segmentInfo shows 16000
        at org.apache.lucene.index.SegmentReader.initialize(SegmentReader.java:313)
        at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:262)
        at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:221)
        at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3099)
        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:2834)
        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:240)
{code}

and here's a typical exception hit when opening a searcher:

{code}
org.apache.lucene.index.CorruptIndexException: doc counts differ for segment _kk: fieldsReader shows 72670 but segmentInfo shows 72671
        at org.apache.lucene.index.SegmentReader.initialize(SegmentReader.java:313)
        at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:262)
        at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:230)
        at org.apache.lucene.index.DirectoryIndexReader$1.doBody(DirectoryIndexReader.java:73)
        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:636)
        at org.apache.lucene.index.DirectoryIndexReader.open(DirectoryIndexReader.java:63)
        at org.apache.lucene.index.IndexReader.open(IndexReader.java:209)
        at org.apache.lucene.index.IndexReader.open(IndexReader.java:173)
        at org.apache.lucene.search.IndexSearcher.<init>(IndexSearcher.java:48)
{code}

Sometimes, adding -Xbatch (forces up front compilation) or -Xint
(disables compilation) to the java command line works around the
issue.

Here are some of the OS's we've seen the failure on:

{code}
SuSE 10.0
Linux phoebe 2.6.13-15-smp #1 SMP Tue Sep 13 14:56:15 UTC 2005 x86_64 
x86_64 x86_64 GNU/Linux 

SuSE 8.2
Linux phobos 2.4.20-64GB-SMP #1 SMP Mon Mar 17 17:56:03 UTC 2003 i686 
unknown unknown GNU/Linux 

Red Hat Enterprise Linux Server release 5.1 (Tikanga)
Linux lab8.betech.virginia.edu 2.6.18-53.1.14.el5 #1 SMP Tue Feb 19 
07:18:21 EST 2008 i686 i686 i386 GNU/Linux
{code}

I've already added assertions to Lucene to detect when this bug
strikes, but since assertions are not usually enabled, I plan to add a
real check to catch when this bug strikes *before* we commit the merge
to the index.  This way we can detect & quarantine the failure and
prevent corruption from entering the index.

"
1,"Destination URI should be normalizedWebdavRequestImpl.getHrefLocator tests if the URI passed as parameter starts with the context path, and passes the next segments to the locator factory.
 
There is a potential hole if the parameter contains "".."", because ""http://example.com/dav/../foo"" starts with the context path ""http://example.com/dav"" but represents to ""http://example.com/foo"". Currently, it is up to the locator factory to detect this situation, meaning that every locator factory should implement this check. Additionally, DavLocatorFactory.createResourceLocator cannot throw exceptions, hence it would not fail cleanly (RuntimeException causing a 500 INTERNAL SERVER ERROR response, when a 403 FORBIDDEN status code would have been apropriate)

Note that the Request-URI should have already been normalized by the servlet container, but in COPY/MOVE operations, the Destination-URI is not normalized.

Conformant clients MUST NOT use dot-segments (""."" or "".."")  [RFC 4918, Section 8.3] in Simple-Ref constructions such as the Destination header [RFC 4918, Section 10.3]), but the server should be able to detect this error.

Proposed change in WebdavRequestImpl:193 (in package org.apache.jackrabbit.webdav from webdav/java)
- ref = uri.getRawPath();
+ ref = uri.normalize().getRawPath();

(This causes /dav/../foo to be rejected because it doesn't start with the context path, and accepts dav/foo/../bar because it starts with the context path)"
0,"Add JCR-RMI documentation to the Jackrabbit web siteUse the org.apache.jackrabbit.rmi.{client,server} package javadocs as a base for a JCR-RMI document page on the Jackrabbit web site."
0,JSR 283: Locking
0,"Update StandardTokenizer and UAX29Tokenizer to Unicode 6.0.0Newly released Unicode 6.0.0 contains some character property changes from the previous release (5.2.0) that affect word segmentation (UAX#29), and JFlex 1.5.0-SNAPSHOT now supports Unicode 6.0.0, so Lucene's UAX#29-based tokenizers should be updated accordingly.

Note that the UAX#29 word break rules themselves did not change between Unicode versions 5.2.0 and 6.0.0."
1,SpanScorer does not respect ConstantScoreRangeQuery settingConstantScoreRangeQuery is actually on and can't be disabled when it should default to off with the option to turn it on.
0,"improve test coverage for Multi*It seems like an easy win that when the test calls newSearcher(), 
it should sometimes wrap the reader with a SlowMultiReaderWrapper.
"
1,"close() throws incorrect IllegalStateEx after IndexWriter hit an OOME when autoCommit is trueSpinoff from http://www.nabble.com/IllegalStateEx-thrown-when-calling-close-to20201825.html

When IndexWriter hits an OOME, it records this and then if close() is
called it calls rollback() instead.  This is a defensive measure, in
case the OOME corrupted the internal buffered state (added/deleted
docs).

But there's a bug: if you opened IndexWriter with autoCommit true,
close() then incorrectly throws an IllegalStatException.

This fix is simple: allow rollback to be called even if autoCommit is
true, internally during close.  (External calls to rollback with
autoCommmit true is still not allowed).
"
0,"AttributeSource holds strong reference to class instances and prevents unloading e.g. in Solr if webapplication reload and custom attributes in separate classloaders are used (e.g. in the Solr plugins classloader)When working on the dynmaic proxy classes using cglib/javaassist i recognized a problem in the caching code inside AttributeSource:
- AttributeSource has a static (!) cache map that holds implementation classes for attributes to be faster on creating new attributes (reflection cost)
- AttributeSource has a static (!) cache map that holds a list of all interfaces implemented by a specific AttributeImpl

Also:
- VirtualMethod in 3.1 hold a map of implementation distances keyed by subclasses of the deprecated API

Both have the problem that this strong reference is inside Lucene's classloader and so persists as long as lucene lives. The classes referenced can never be unloaded therefore, which would be fine if all live in the same classloader. As soon as the Attribute or implementation class or the subclass of the deprecated API are loaded by a different classloder (e.g. Lucene lives in bootclasspath of tomcat, but lucene-consumer with custom attributes lives in a webapp), they can never be unloaded, because a reference exists.

Libs like CGLIB or JavaAssist or JDK's reflect.Proxy have a similar cache for generated class files. They also manage this by a WeakHashMap. The cache will always work perfect and no class will be evicted without reason, as classes are only unloaded when the classloader goes and this will only happen on request (e.g. by Tomcat)."
0,"Use the new Jackrabbit parent POMNow that we have an official release of the new org.apache.jackrabbit:parent:2 POM, we should start using that as the parent of all Jackrabbit builds."
0,"Further improvements to contrib/benchmark for testing NRTSome small changes:

  * Allow specifying a priority for BG threads, after the ""&""
    character; priority increment is + or - int that's added to main
    thread's priority to set child thread's.  For my NRT tests I make
    the reopen thread +2, the indexing threads +1, and leave searching
    threads at their default.

  * Added test case

  * NearRealTimeReopenTask now reports @ the end the full array of
    msec of each reopen latency

  * Added optional breakout of counts by time steps.  If you set
    log.time.step.msec to eg 1000 then reported counts for serial task
    sequence is broken out by 1 second windows.  EG you can use this
    to measure slowdown over time.
"
0,"SnowballAnalyzer has a link to net.sf (a package that is empty and needs to be removed).need to remove net.sf and points to org.tartarus.snowball.ext. Doesn't work as a link though, so I'll also remove the @link to lose the javadoc error and broken link."
1,"Item.isSame() may return true for 2 nodes from different workspaces.the code in ItemImpl.isSame() only compares the item id, but not the source workspace."
1,"OracleFileSystem can't handle empty filesthe following exception is thrown when trying to access a 0-length file
in an OracleFileSystem:
java.sql.SQLException: ORA-22275: invalid LOB locator specified

issue reported on the users list, 
see http://www.nabble.com/problems-with-Oracle-tf2483987.html#a6926522

"
1,"FixedIntBlockIndexInput.Reader does not initialise 'pending' int arrayThe FixedIntBlockIndexInput.Reader.pending int array is not initialised. As a consequence, the FixedIntBlockIndexInput.Reader#next() method returns always 0.

A call to FixedIntBlockIndexInput.Reader#blockReader.readBlock() during the Reader initialisation may solve the issue (to be tested)."
0,"Deprecate StandardBenchmarker and ""old"" benchmarker code in favor of the Task based approachWe should deprecate the StandardBechmarker code that was the start of the benchmark contribution in favor of the much easier to use/extend byTask benchmark code"
1,".war distribution should be configurable, prompting you to setup JNDI with the Repository Home and Config locations.The Embedded Deployment Model documentation (http://jackrabbit.apache.org/doc/deploy/howto-model1.html) on the jackrabbit page describes how to package up a .war file so that you can use JNDI Resource settings to change the location of the repository home and the repository configuration xml file.

Unfortunately, the .war file that is provided as part of the Jackrabbit distribution doesn't behave like this. Instead, it has an inbuilt repository.xml file and settings in web.xml that act as defaults. These defaults are not useful and force a user to act like a developer and modify the files within the .war file.

The current situation is that we have a .war that's not going to be useful to anyone without modification. The repository.xml file that is contained within the .war makes the repository home to be the Tomcat/bin/repository directory. This is not a useful default. It's better to have no default setup and a clear error message that JNDI needs to be setup. It would be even better if the web application could recognise when the JNDI wasn't configured and could prompt the user with an instructional webpage, describing how to setup the required JNDI settings on Tomcat, JBoss etc.

----

The .war distribution for Jackrabbit ignores the JNDI settings that are described in the documentation. I am using this Tomcat config.xml snippet to configure Tomcat 5.5:

{{{
<?xml version='1.0' encoding='utf-8'?>
<Context displayName=""Ark"" docBase=""c:\dev\ark\jackrabbit-server-1.1.1.war"" path=""/ark"" 
         useNaming=""false"" workDir=""work\Catalina\localhost\ark"" unpackWAR=""false"">

<Resource name=""jcr/repository""
          auth=""Container""
          type=""javax.jcr.Repository""
          factory=""org.apache.jackrabbit.core.jndi.BindableRepositoryFactory""
          configFilePath=""c:/dev/ark/src/main/resources/repository.xml""
          repHomeDir=""c:/jackrabbitrepo""/>

</Context>
}}}

Jackrabbit loads fine. However, the logs show:

{{{
02.01.2007 10:33:00 *INFO * RepositoryStartupServlet: RepositoryStartupServlet initializing... (RepositoryStartupServlet.java, line 190)
02.01.2007 10:33:00 *INFO * RepositoryStartupServlet:   repository-home = C:\Program Files\Apache Software Foundation\Tomcat 5.5\bin\jackrabbit\repository (RepositoryStartupServlet.java, line 242)

...
...

02.01.2007 10:33:00 *INFO * LocalFileSystem: LocalFileSystem initialized at path C:\Program Files\Apache Software Foundation\Tomcat 5.5\bin\jackrabbit\repository\repository (LocalFileSystem.java, line 166)
}}}






----

My use case is that I want to use Jackrabbit to host a Maven 2 repository within my company. So, ideally I want to:
   * Download the Jackrabbit .war file and mount it on my Tomcat server as context ""/maven2"".
   * Configure Tomcat to use LDAP authentication and point it at my company's LDAP server. This is a standard J2EE feature, of course.
   * Create my own repository.xml file which points to my AccessManager implementation (which goes to my company's SingleSignOn service for authorization). My AccessManager implementation will be placed on the Tomcat shared classpath.
   * Set the repository home directory, where all the working files will be placed and the location of the repository.xml file. Ideally, this would be done in JNDI.

If I have to put together my own Jackrabbit .war file, I consider that I have my ""developer"" hat on when I only really want to have my ""Jackrabbit user"" hat on.
"
0,"QueryWrapperFilter should not do scoringThe purpose of QueryWrapperFilter is to simply filter to include the docIDs that match the query.

Its implementation is wasteful now because it computes scores for those matching docs even though the score is unused.  We could fix this by getting a Scorer and iterating through the docs without asking for the score:

{code}
Index: src/java/org/apache/lucene/search/QueryWrapperFilter.java
===================================================================
--- src/java/org/apache/lucene/search/QueryWrapperFilter.java	(revision 707060)
+++ src/java/org/apache/lucene/search/QueryWrapperFilter.java	(working copy)
@@ -62,11 +62,9 @@
   public DocIdSet getDocIdSet(IndexReader reader) throws IOException {
     final OpenBitSet bits = new OpenBitSet(reader.maxDoc());
 
-    new IndexSearcher(reader).search(query, new HitCollector() {
-      public final void collect(int doc, float score) {
-        bits.set(doc);  // set bit for hit
-      }
-    });
+    final Scorer scorer = query.weight(new IndexSearcher(reader)).scorer(reader);
+    while(scorer.next())
+      bits.set(scorer.doc());
     return bits;
   }
{code}

Maybe I'm missing something, but this seams like a simple win?
"
0,"Add SearcherManager, to manage IndexSearcher usage across threads and reopensThis is a simple helper class I wrote for Lucene in Action 2nd ed.
I'd like to commit under Lucene (contrib/misc).

It simplifies using & reopening an IndexSearcher across multiple
threads, by using IndexReader's ref counts to know when it's safe
to close the reader.

In the process I also factored out a test base class for tests that
want to make lots of simultaneous indexing and searching threads, and
fixed TestNRTThreads (core), TestNRTManager (contrib/misc) and the new
TestSearcherManager (contrib/misc) to use this base class.
"
0,"Clean up old JIRA issues in component ""Analysis""A list of all JIRA issues in component ""Analysis"" that haven't been updated in 2007:

   *	 LUCENE-760  	 Spellchecker could/should use n-gram tokenizers instead of rolling its own n-gramming   
   *	LUCENE-677 	Italian Analyzer 
   *	LUCENE-571 	StandardTokenizer parses decimal number as <HOST> 
   *	LUCENE-566 	Esperanto Analyzer 
   *	LUCENE-559 	Turkish Analyzer for Lucene 
   *	LUCENE-494 	Analyzer for preventing overload of search service by queries with common terms in large indexes 
   *	LUCENE-424 	[PATCH] Submissiom form simple Romanian Analyzer 
   *	LUCENE-417 	StandardTokenizer has problems with comma-separated values 
   *	LUCENE-400 	NGramFilter -- construct n-grams from a TokenStream 
   *	LUCENE-396 	[PATCH] Add position increment back into StopFilter 
   *	LUCENE-387 	Contrib: Main memory based SynonymMap and SynonymTokenFilter 
   *	LUCENE-321 	[PATCH] Submissiom of my Tswana Analyzer 
   *	LUCENE-233 	[PATCH] analyzer refactoring based on CVS HEAD from 6/21/2004 
   *	LUCENE-210 	[PATCH] Never write an Analyzer again 
   *	LUCENE-205 	[PATCH] Patches for RussianAnalyzer 
   *	LUCENE-185 	[PATCH] Thai Analysis Enhancement 
   *	LUCENE-152 	[PATCH] KStem for Lucene 
   *	LUCENE-82 	[PATCH] HTMLParser: IOException: Pipe closed 

"
0,Allow using FST to hold terms data in DocValues.BYTES_*_SORTED
0,"Analyzer for preventing overload of search service by queries with common terms in large indexesAn analyzer used primarily at query time to wrap another analyzer and provide a layer of protection
which prevents very common words from being passed into queries. For very large indexes the cost
of reading TermDocs for a very common word can be  high. This analyzer was created after experience with
a 38 million doc index which had a term in around 50% of docs and was causing TermQueries for 
this term to take 2 seconds.

Use the various ""addStopWords"" methods in this class to automate the identification and addition of 
stop words found in an already existing index."
0,"remove contrib deprecationsthere aren't too many deprecations in contrib to remove for 3.0, but we should get rid of them."
1,"TCK: Test root path not escaped when used in XPath queriesA repository implementation might use a test root path that contains names that need _xXXXX_ escaping when used in XPath queries. Currently the TCK just uses the test path as-is when constructing queries. Even though this only affects few repositories (I've heard of one legacy connector to run into this problem), it would be good to add the proper escaping."
1,"Multithreading issue with versioningIn a multithreading environment with two or more threads accessing the same version history, inconsistent state may be encountered. Concretely, the first thread is currently checking in the node to which the version history is attached while the second thread walks this same version history by means of a ""self-built"" iterator, which just accesses the successors of each version to get the ""next"" to visit.

At a certain point the second point may encounter an ItemNotFoundException with a stack trace similar to this:

javax.jcr.ItemNotFoundException: c9bd405b-dff4-46ef-845c-d98e073e473a
        at org.apache.jackrabbit.core.ItemManager.createItemInstance(ItemManager.java:354)
        at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:230)
        at org.apache.jackrabbit.core.SessionImpl.getNodeByUUID(SessionImpl.java:494)
        at org.apache.jackrabbit.core.version.VersionImpl.getSuccessors(VersionImpl.java:86)
        ....

It seems that the first thread has already filled the successor of the version, while the node is not yet accessible by the createItemInstance method.

This bug seems to not be enforcible, but it is easily reproducible."
0,"[PATCH] to remove synchronized code from TermVectorsReaderOtis,

here the latest and last patch to get rid of all synchronized code from
TermVectorsReader. It should include at least 3 files, TermVectorsReader.diff,
SegmentReader.diff and the new junit test case TestMultiThreadTermVectors.java.
The patch was generated against the current CVS version of TermVectorsReader and
SegmentReader. All lucene related junit tests pass fine.

best regards
Bernhard"
0,"Some tests assume that an implementation of javax.jcr.Item overrides equals()The following 3 tests (followed by the line number containing the bad assertion):

org.apache.jackrabbit.test.api.ReferencesTest.testReferenceTarget:135
org.apache.jackrabbit.test.api.ReferencesTest.testAlterReference:169
org.apache.jackrabbit.test.api.version.VersionHistoryTest:152

assume that an implementation of javax.jcr.Item overrides equals(), such that 

Assert.assertEquals(n1, n2) or 
java.util.Set.contains(n1) 

works for two ""equal"" nodes n1,n2 or for some node n1 that has been previously put into a set. However, there is no section in the specification that would mandate this. The tests above should therefore replace assertEquals() with one of the other mechanism that officially supported, such as javax.jcr.Node.isSame().

"
1,"NPE w/ AbstractPoolEntry.openjava.lang.NullPointerException
    at org.apache.http.impl.conn.AbstractPoolEntry.open(AbstractPoolEntry.java:171)
    at org.apache.http.impl.conn.AbstractPooledConnAdapter.open(AbstractPooledConnAdapter.java:119)
    at org.apache.http.impl.client.DefaultClientRequestDirector.execute(DefaultClientRequestDirector.java:309)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:501)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:456)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:422)
    at com.limegroup.gnutella.http.DefaultHttpExecutor.performRequest(DefaultHttpExecutor.java:97)
    at com.limegroup.gnutella.http.DefaultHttpExecutor.access$000(DefaultHttpExecutor.java:26)
    at com.limegroup.gnutella.http.DefaultHttpExecutor$MultiRequestor.run(DefaultHttpExecutor.java:135)
    at org.limewire.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1006)
    at org.limewire.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:549)
    at java.lang.Thread.run(Unknown Source)

Seeing a lot of these against Alpha4.  Also seeing still the occassional IllegalStateException of:

java.lang.IllegalStateException: Connection already open.
    at org.apache.http.impl.conn.AbstractPoolEntry.open(AbstractPoolEntry.java:150)
    at org.apache.http.impl.conn.AbstractPooledConnAdapter.open(AbstractPooledConnAdapter.java:119)
    at org.apache.http.impl.client.DefaultClientRequestDirector.execute(DefaultClientRequestDirector.java:309)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:501)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:456)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:422)
    at com.limegroup.gnutella.http.DefaultHttpExecutor.performRequest(DefaultHttpExecutor.java:97)
    at com.limegroup.gnutella.http.DefaultHttpExecutor.access$000(DefaultHttpExecutor.java:26)
    at com.limegroup.gnutella.http.DefaultHttpExecutor$MultiRequestor.run(DefaultHttpExecutor.java:135)
    at org.limewire.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1006)
    at org.limewire.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:549)
    at java.lang.Thread.run(Unknown Source)
"
0,"No way to get the requestBody out of a PostMethod or use if extending classAttempting to extend the PostMethod class I discovered that I had no access to 
the requestBody because the member is declared private and there is no get 
method.

I was trying to override the setRequestContentLength() when I discovered the 
problem.

So my enhancement request specifically is:
1. add a get method to be able to get the requestBody (probably a get method to 
the parameters as well)
2. (optionally) make the requestBody and parameters members protected instead 
of private so extending the class is easier.

In case you were wondering, the reason for extending the class was to add the 
ability to set a timeout value on the httpconnection and also the ability to 
set the character encoding of the request body.  I don't know if these are 
worthy of an enhancement request but I require them for what I'm doing.  Nice 
work by the way.

Thank you."
0,Use java.util.UUIDReplace the use of org.apache.jackrabbit.uuid.UUID with the new java.util.UUID class introduced in Java 5.
0,"Make lazy loading proxy callback SerializableHello (probably Christophe :) ),

It would be nice to have the CGLib callbacks Serializable, because if they're not, the proxies are not either, even if the target is.

I've seen that you've made BeanLazyLoader Serializable. Wouldn't be better to have AbstractLazyLoader Serializable, so CollectionLazyLoader is too?

What I've done is :
*) declaring AbstractLazyLoader as Serializable
*) declaring non-Serializable resources such as Session as volatile
*) throwing an IllegalStateException in BeanLazyLoader.fetch and CollectionLazyLoader.fetch if the Session is null. This case can only happen if the proxy has been serialized without the volatile Session, and it doesn't make sense to lazy load the target then.

BTW, I realized I didn't clean up the beanClassDescriptor in BeanLazyLoader.cleanUp, so I corrected this.

I'll attach the modified classes.

Sincerely,

Stphane Landelle"
0,"Setting SSLSocket parametersIn HttpClient 4.0.3, it was easy to subclass SSLSocketFactory, and set SSLSocket options (e.g. setEnabledCipherSuites() or setSSLParameterse()) before the SSL handshake happened. This way it was possible to e.g. restrict cipher suites on per-HttpClient basis (instead of JVM-wide system properties).

In HttpClient 4.1.1, the design has changed quite a lot, and copy-pasting of several long methods is needed. 

Ideally, SSLSocketFactory should support applying SSLParameters to the socket. However, SSLParameters is Java 1.6, so if we want to keep compatibility with 1.5, that's out.

However, it'd be nice to at least have a method (e.g. ""protected SSLSocket prepareSSLSocket(SSLSocket s)"") that would get called immediately after a socket is retrieved from the socket factory. The default implementation could be just ""return s;"", but subclasses could do something like s.setEnabledCipherSuites() s.setSSLParameters()."
0,"RepositoryLock does not work on NFS sometimesThe RepositoryLock mechanism currently used in Jackrabbit uses FileLock. This doesn't work on some NFS file system. It looks like only NFS version 4 and newer supports locking. Older implementations may throw a IOException ""No locks available"", which means the NFS does not support byte-range locking.

I propose to add a second locking mechanism, and add a configuration option to use it. For example: <FileLocking class=""acme"" />. This second locking mechanism is a cooperative locking protocol that uses a background (watchdog) thread and only uses regular file operations.

"
1,"Merging between workspaces failsI have setup 2 workspaces in Jackrabbit.  I have a preview and a production
workspace.  These workspaces keep a tree of menu nodes that can have content
associated to those menus.  Each node is of type nt:unstructured and has
mixin types of versionable, lockable, and referenceable.

In our system you are only allowed to edit nodes in the preview workspace.
So what I do is when you go to edit a node we check it out, allow for edits,
then check it in.  This creates a new version on the node.  Then we merge
the node up to the production workspace.  All nodes in the production
workspace are always checked in and not locked.

When I go to do a merge I run into problems when I try to merge a node that
has children.  Lets say I have node A with children B and C.  These all have
the same node types as stated above.  I make a change to a property in Node
A in the preview workspace and now want to merge it into the production
workspace (where it exists already).  Here is the code that is run:

Node destNode = destSession.getNodeByUUID(getUUID());
NodeIterator ni = destNode.merge(""preview"", true);

Now this fails in the ItemImpl.internalRemove() method with a
VersionException of cannot remove a child of a checked-in node.  Here is the
trace for the error:
at org.apache.jackrabbit.core.ItemImpl.internalRemove(ItemImpl.java:848)
at org.apache.jackrabbit.core.NodeImpl.internalMerge(NodeImpl.java:3693)
at org.apache.jackrabbit.core.NodeImpl.internalMerge(NodeImpl.java:3587)
at org.apache.jackrabbit.core.NodeImpl.merge(NodeImpl.java:3003)

Now if I understand correctly when doing a merge the node that you are
trying to merge to needs to be older then the source node and the
destination node cannot be checked out (NodeImpl.doMergeTest() is where I
figured that out).  But then when I step through further in the merge in
NodeImpl it gets all the nodes of the src node and retrieves the same
children in the destination workspace and then tries to remove those
destination children but it can't remove those children b/c the parent node
(which is node A in the production workspace) is not checked out, but
according to the mergeTest it can't be checked out or the merge won't even
begin."
0,"Allow Junit4 tests in our environment.Now that we're dropping Java 1.4 compatibility for 3.0, we can incorporate Junit4 in testing. Junit3 and junit4 tests can coexist, so no tests should have to be rewritten. We should start this for the 3.1 release so we can get a clean 3.0 out smoothly.

It's probably worthwhile to convert a small set of tests as an exemplar.


"
0,"Making Term Vectors more accessibleOne of the big issues with term vector usage is that the information is loaded into parallel arrays as it is loaded, which are then often times manipulated again to use in the application (for instance, they are sorted by frequency).

Adding a callback mechanism that allows the vector loading to be handled by the application would make this a lot more efficient.

I propose to add to IndexReader:
abstract public void getTermFreqVector(int docNumber, String field, TermVectorMapper mapper) throws IOException;
and a similar one for the all fields version

Where TermVectorMapper is an interface with a single method:
void map(String term, int frequency, int offset, int position);

The TermVectorReader will be modified to just call the TermVectorMapper.  The existing getTermFreqVectors will be reimplemented to use an implementation of TermVectorMapper that creates the parallel arrays.  Additionally, some simple implementations that automatically sort vectors will also be created.

This is my first draft of this API and is subject to change.  I hope to have a patch soon.

See http://www.gossamer-threads.com/lists/lucene/java-user/48003?search_string=get%20the%20total%20term%20frequency;#48003 for related information."
1,"HttpConnection.isResponseAvailable() calls setSoTimeout() but does not catch IOExceptionHttpConnection.isResponseAvailable() can throw an IOException when setting the
soTimeout but should probably just return false in this case.

<http://marc.theaimsgroup.com/?t=106268485100002&r=1&w=2>"
0,"Maven build failure in textfilter contrib projectI've tried to build the textfilters contrib but get the following error:

Attempting to download jackrabbit-0.16.4.1-dev.jar.
WARNING: Failed to download jackrabbit-0.16.4.1-dev.jar.
The build cannot continue because of the following unsatisfied dependency:
jackrabbit-0.16.4.1-dev.jar

I tried changing the project.xml to look for jackrabbit-1.0-dev.jar instead, but it didn't work, not sure what maven expects here, but should be an easy fix for somebody in the know."
0,"Incorrect transitive snapshot dependenciesUsing ${version} in dependency declarations causes troubles since for snapshot dependencies the version variable apparently gets replaced by the exact timestamp of a deployed snapshot instead of the ""x.y-SNAPSHOT"" string. Typically the timestamps of different artifacts are not the same, causing broken dependencies."
0,"SpanOrQuery.java: simplification and testThe current SpanOrQuery.java has some unnessary attributes. After removing these, I found that there was no existing test for it, so I added some tests to TestSpans.java."
0,"Update SPI locking to match JCR 2.0jcr2spi currently uses the JSR 170 way to determine whether a given Session owns the lock by checking of the lock token is null.
with JSR 283 a new Lock method has been defined for this, while on the other hand the lock token is always null for session-scoped
locks.

In addition 283-locking allows to specify a timeout hint and hint about the owner info that should be displayed
for information purpose.

Proposed changes to SPI:

- extend org.apache.jackrabbit.spi.LockInfo to cover the new functionality added with JSR 283
- add an variant of RepositoryService.lock that allows to specify timeout and owner hint.

Proposed changes to JCR2SPI:
- change jcr2spi to make use of the new functionality and modify the test for session being lock holder.
  this mainly affects
  > LockOperation
  > LockManager impl
  > Lock impl"
1,"Bundle persistence managers node id key store/load is not symertric on MySql causing NoSuchItemState Exceptions It looks like the binary values read back from MySql where the UUID contains 0's is not the same as that generated from the UUID getRawBytes() call. As result, you can store a node with the UUID that has 0's but its never found when read back. This therefore causes corruption in random places when certain UUIDs are generated.

Test Case: 

I've attached 2 files. One causes node corruption when imported, the other does not.
The only difference is that I removed any 0 values from the problem UUID in the file that causes corruption.

As Stefan pointed out, I had manipulated the test case to use standard nt types when in fact I should have provided the following info (sorry Stefan) e.g. the test folder types are referencable hence the jcr:uuid allocation

[acme:Folder] > nt:folder, mix:referenceable

If I import causes_corruption.xml and then attempt to ""ls"" AclObjectIdentities then loadBundle() returns null for the UUID 

a55f3f6b-a909-4e8d-b65a-93002ced0920 which in bytes is [-91, 95, 63, 107, -87, 9, 78, -115, -74, 90, -109, 0, 44, -19, 9, 32]

If I import works.xml then ""ls"" works fine for the same node as I've manually changed the UUID to replace 0s with 1s in the last section.

a55f3f6b-a909-4e8d-b65a-93112ced1921 [-91, 95, 63, 107, -87, 9, 78, -115, -74, 90, -109, 17, 44, -19, 25, 33]


Testing shows this issue highlights a problem with the Bundle persistence manager and MySqls method of handling BINARY columns.
The solution looks to be to replace BINARY(16) with VARBINARY(16). Quoting from http://dev.mysql.com/doc/refman/5.0/en/binary-varbinary.html...
""If the value retrieved must be the same as the value specified for storage with no padding, it might be preferable to use VARBINARY or one of the BLOB data types instead.""
A review of our logs shows that all of the corruption we've seen has related to nodes with UUIDs including 0's.

* Shall I log a JIRA ticket for this?
* Anyone see any issues with this fix?


In the following example you can see I'm showing all bundles in the ""test1"" workspace.

mysql> select hex(node_id) from test1_bundle;
+----------------------------------+
| hex(node_id)                     |
+----------------------------------+
| 28126C3E36A0471D9CDC5AC423BAC9C5 |
| A55F3F6BA9094E8DB65A93002CED0920 |
| CAFEBABECAFEBABECAFEBABECAFEBABE |
| D638EACCDEB641FD8868804C8ECEFFFD |
| DEADBEEFCAFEBABECAFEBABECAFEBABE |
+----------------------------------+
5 rows in set (0.00 sec)

...but a select using the same UUID hex value returns no rows.

mysql>  select node_id from test1_bundle where 
mysql> unhex('A55F3F6BA9094E8DB65A93002CED0920') = node_id;
Empty set (0.00 sec)

I've then created a new ""test3"" workspace which I modified to use varbinary instead of binary with:

alter table test3_bundle modify NODE_ID varbinary(16); alter table test3_refs modify NODE_ID varbinary(16);

My import test case now no longer fails and the following query proves that query operations, after a store, return rows as expected.

mysql>  select node_id from test3_bundle where 
mysql> unhex('A55F3F6BA9094E8DB65A93002CED0920') = node_id;
+------------------+
| node_id          |
+--------Z ,--  |
+------------------+
1 row in set (0.00 sec)

mysql> desc test3_bundle;
ERROR 2006 (HY000): MySQL server has gone away No connection. Trying to reconnect...
Connection id:    7116
Current database: mmptest

+-------------+---------------+------+-----+---------+-------+
| Field       | Type          | Null | Key | Default | Extra |
+-------------+---------------+------+-----+---------+-------+
| NODE_ID     | varbinary(16) | YES  | UNI | NULL    |       |
| BUNDLE_DATA | longblob      | NO   |     |         |       |
+-------------+---------------+------+-----+---------+-------+
2 rows in set (0.00 sec)


mysql>  alter table test3_bundle modify NODE_ID varbinary(16);
Query OK, 2 rows affected (0.00 sec)
Records: 2  Duplicates: 0  Warnings: 0

"
0,"Make the Highlighter use SpanScorer by defaultI've always thought this made sense, but frankly, it took me a year to get the SpanScorer included with Lucene at all, so I was pretty much ready to move on after I it got in, rather than push for it as a default.

I think it makes sense as the default in Solr as well, and I mentioned that back when it was put in, but alas, its an option there as well.

The Highlighter package has no back compat req, but custom has been conservative - one reason I havn't pushed for this change before. Might be best to actually make the switch in 3? I could go either way - as is, I know a bunch of people use it, but I'm betting its the large minority. It has never been listed in a changes entry and its not in LIA 1, so you pretty much have to stumble upon it, and figure out what its for.

I'll point out again that its just as fast as the standard scorer for any clause of a query that is not position sensitive. Position sensitive query clauses will obviously be somewhat slower to highlight, but that is because they will be highlighted correctly rather than ignoring position."
0,"Make it possible to adjust MaxTotalConnections parameter dynamicalyMake it possible to adjust MaxTotalConnections parameter at run time. Document behaviour of MaxTotalConnections and MaxConnectionsPerRoute behaviour (latter cannot be changed for allocated pools)

Oleg"
1,"AttributeSource can have an invalid computed stateIf you work a tokenstream, consume it, then reuse it and add an attribute to it, the computed state is wrong.
thus for example, clearAttributes() will not actually clear the attribute added.

So in some situations, addAttribute is not actually clearing the computed state when it should.
"
0,"BooleanWeight should size the weights Vector correctlyThe weights field on BooleanWeight uses a Vector that will always be sized exactly the same as the outer class' clauses Vector, therefore can be sized correctly in the constructor. This is a trivial memory saving enhancement."
1,"Intermittent failure in TestFieldCacheTermsFilter.testMissingTermsRunning tests in while(1) I hit this:

{noformat}

NOTE: reproduce with: ant test -Dtestcase=TestFieldCacheTermsFilter -Dtestmethod=testMissingTerms -Dtests.seed=-1046382732738729184:5855929314778232889

1) testMissingTerms(org.apache.lucene.search.TestFieldCacheTermsFilter)
java.lang.AssertionError: Must match 1 expected:<1> but was:<0>
	at org.junit.Assert.fail(Assert.java:91)
	at org.junit.Assert.failNotEquals(Assert.java:645)
	at org.junit.Assert.assertEquals(Assert.java:126)
	at org.junit.Assert.assertEquals(Assert.java:470)
	at org.apache.lucene.search.TestFieldCacheTermsFilter.testMissingTerms(TestFieldCacheTermsFilter.java:63)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1214)
	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1146)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:24)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:157)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:136)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at org.junit.runner.JUnitCore.runMain(JUnitCore.java:98)
	at org.junit.runner.JUnitCore.runMainAndExit(JUnitCore.java:53)
	at org.junit.runner.JUnitCore.main(JUnitCore.java:45)
{noformat}

Unfortunately the seed doesn't [consistently] repro for me..."
0,"Improve BufferedIndexInput.readBytes() performanceDuring a profiling session, I discovered that BufferedIndexInput.readBytes(),
the function which reads a bunch of bytes from an index, is very inefficient
in many cases. It is efficient for one or two bytes, and also efficient
for a very large number of bytes (e.g., when the norms are read all at once);
But for anything in between (e.g., 100 bytes), it is a performance disaster.
It can easily be improved, though, and below I include a patch to do that.

The basic problem in the existing code was that if you ask it to read 100
bytes, readBytes() simply calls readByte() 100 times in a loop, which means
we check byte after byte if the buffer has another character, instead of just
checking once how many bytes we have left, and copy them all at once.

My version, attached below, copies these 100 bytes if they are available at
bulk (using System.arraycopy), and if less than 100 are available, whatever
is available gets copied, and then the rest. (as before, when a very large
number of bytes is requested, it is read directly into the final buffer).

In my profiling, this fix caused amazing performance
improvement: previously, BufferedIndexInput.readBytes() took as much as 25%
of the run time, and after the fix, this was down to 1% of the run time! However, my scenario is *not* the typical Lucene code, but rather a version of Lucene with added payloads, and these payloads average at 100 bytes, where the original readBytes() did worst. I expect that my fix will have less of an impact on ""vanilla"" Lucene, but it still can have an impact because it is used for things like reading fields. (I am not aware of a standard Lucene benchmark, so I can't provide benchmarks on a more typical case).

In addition to the change to readBytes(), my attached patch also adds a new
unit test to BufferedIndexInput (which previously did not have a unit test).
This test simulates a ""file"" which contains a predictable series of bytes, and
then tries to read from it with readByte() and readButes() with various
sizes (many thousands of combinations are tried) and see that exactly the
expected bytes are read. This test is independent of my new readBytes()
inplementation, and can be used to check the old implementation as well.

By the way, it's interesting that BufferedIndexOutput.writeBytes was already efficient, and wasn't simply a loop of writeByte(). Only the reading code was inefficient. I wonder why this happened."
0,"PrefixQuery is missing the equals() methodThe PrefixQuery is inheriting the java.lang.Object's object default equals method. This makes it hard to have test working of PrefixFilter or any other task requiring equals to work proerply (insertion in Set, etc.). The equal method should be very similar, not to say identical except for class casting, to the equals() of TermQuery. "
0,"Make the Lucene jar an OSGi bundleIn order to use Lucene in an OSGi environment, some additional headers are needed in the manifest of the jar. As Lucene has no dependency, it is pretty straight forward and it ill be easy to maintain I think."
0,"factor CharTokenizer/CharacterUtils into analyzers moduleCurrently these analysis components are in the lucene core, but should really
be .util in the analyzers module.

Also, with MockTokenizer extending Tokenizer directly, we can add some additional
checks in the future to try to ensure our consumers are being good consumers (e.g. calling reset).

This is mentioned in http://wiki.apache.org/lucene-java/TestIdeas, I didn't implement it here yet,
this is just the factoring. I think we should try to do this before LUCENE-3040.
"
0,"Store all metadata in human-readable segments fileVarious index-reading components in Lucene need metadata in addition to data.
This metadata is presently stored in arbitrary binary headers and spread out
over several files.  We should move to concentrate it in a single file, and 
this file should be encoded using a human-readable, extensible, standardized 
data serialization language -- either XML or YAML.

* Making metadata human-readable makes debugging easier.  Centralizing it
  makes debugging easier still.  Developers benefit from being able to scan
  and locate relevant information quickly and with less debug printing.  Users
  get a new window through which to peer into the index structure.
* Since metadata is written to a separate file, there would no longer be a 
  need to seek back to the beginning of any data file to finish a header, 
  solving issue LUCENE-532.
* Special-case parsing code needed for extracting metadata supplied by 
  different index formats can be pared down.  If a value is no longer 
  necessary, it can just be ignored/discarded.
* Removing headers from the data files simplifies them and makes the file
  format easier to implement. 
* With headers removed, all or nearly all data structures can take the
  form of records stacked end to end, so that once a decoder has been
  selected, an iterator can read the file from top to tail.  To an extent,
  this allows us to separate our data-processing algorithms from our
  serialization algorithms, decoupling Lucene's code base from its file
  format.  For instance, instead of further subclassing TermDocs to deal with
  ""flexible indexing"" formats, we might replace it with a PostingList which
  returns a subclass of Posting.  The deserialization code would be wholly
  contained within the Posting subclass rather than spread out over several
  subclasses of TermDocs.
* YAML and XML are equally well suited for the task of storing metadata, 
  but in either case a complete parser would not be needed -- a small subset 
  of the language will do.  KinoSearch 0.20's custom-coded YAML parser 
  occupies about 600 lines of C -- not too bad, considering how miserable C's 
  string handling capabilities are. "
0,"IndexReader subclasses must implement flex APIsTo be fixed only on trunk...

I made IndexReader's base flex APIs abstract, fixed all core/contrib/solr places that subclassed IR and didn't already implement flex (including contrib/memory, contrib/instantiated), and remove all the classes for the back-compat layer that emulated flex APIs on top of pre-flex APIs."
1,"BooleanQuery.hashCode and equals ignore isCoordDisabledBooleanQuery.isCoordDisabled() is not considered by BooleanQuery's hashCode() or equals() methods ... this can cause serious badness to happen when caching BooleanQueries.

bug traces back to at least 1.9"
0,"Excessive Arrays.fill(0) in DocumentsWriter drastically slows down small docs (3.9X slowdown!)I've been doing some ""final"" performance testing of 2.3RC1 and
uncovered a fairly serious bug that adds a large fixed CPU cost when
documents have any term vector enabled fields.

The bug does not affect correctness, just performance.

Basically, for every document, we were calling Arrays.fill(0) on a
large (32 KB) byte array when in fact we only needed to zero a small
part of it.  This only happens if term vectors are turned on, and is
especially devastating for small documents."
1,"some valid email address characters not correctly recognizedthe EMAIL expression in StandardTokenizerImpl.jflex misses some unusual but valid characters in the left-hand-side of the email address. This causes an address to be broken into several tokens, for example:

somename+site@gmail.com gets broken into ""somename"" and ""site@gmail.com""
husband&wife@talktalk.net gets broken into ""husband"" and ""wife@talktalk.net""

These seem to be occurring more often. The first seems to be because of an anti-spam trick you can use with google (see: http://labnol.blogspot.com/2007/08/gmail-plus-smart-trick-to-find-block.html). I see the second in several domains but a disproportionate amount are from talktalk.net, so I expect it's a signup suggestion from the service.

Perhaps a fix would be to change line 102 of StandardTokenizerImpl.jflex from:
EMAIL      =  {ALPHANUM} (("".""|""-""|""_"") {ALPHANUM})* ""@"" {ALPHANUM} (("".""|""-"") {ALPHANUM})+

to 

EMAIL      =  {ALPHANUM} (("".""|""-""|""_""|""+""|""&"") {ALPHANUM})* ""@"" {ALPHANUM} (("".""|""-"") {ALPHANUM})+

I'm aware that the StandardTokenizer is meant to be more of a basic implementation rather than an implementation the full standard, but it is quite useful in places and hopefully this would improve it slightly."
0,"Contrib: ThaiAnalyzer to enable Thai full-text search in LuceneThai text don't have space between words. Usually, a dictionary-based algorithm is used to break string into words. For Lucene to be usable for Thai, an Analyzer that know how to break Thai words is needed.

I've implemented such Analyzer, ThaiAnalyzer, using ICU4j DictionaryBasedBreakIterator for word breaking. I'll upload the code later.

I'm normally a C++ programmer and very new to Java. Please review the code for any problem. One possible problem is that it requires ICU4j. I don't know whether this is OK."
1,SPI: Description of Path.isDescendantOf(Path) Description of Path.isDescendantOf lists different reasons for IllegalArgumentException and RepositoryException than isAncestorOf... this is obviously a mistake.
0,"Provide feedback mechanism to CredentialsProviderIf the remote server is using BASIC or NT authentication and you pass in 
invalid credentials you get stuck in an infinite for loop, repeatedly sending 
the same authentication request again and again to the server.  The for loop is 
in the executeMethod method of the HttpMethodDirector class.

Sample code:
=================================================================


import org.apache.commons.httpclient.Credentials;
import org.apache.commons.httpclient.NTCredentials;
import org.apache.commons.httpclient.UsernamePasswordCredentials;
import org.apache.commons.httpclient.HttpClient;
import org.apache.commons.httpclient.methods.GetMethod;
import org.apache.commons.httpclient.auth.*;

import java.io.IOException;
import java.io.BufferedInputStream;
import java.io.ByteArrayOutputStream;

/**
 * Created by IntelliJ IDEA.
 * User: dmartineau
 * Date: Nov 8, 2005
 * Time: 1:43:21 PM
 */
public class ShowProblem
{

    private String location;
    private String user;
    private String pass;
    private String domain;

    public ShowProblem(String location, String user, String pass, String domain)
    {
        this.location = location;
        this.user=user;
        this.pass=pass;
        this.domain=domain;

    }

    public int getFile()
    {
        int status = 500;
        HttpClient client = new HttpClient();
        client.getParams().setParameter(
            CredentialsProvider.PROVIDER, new CProvider(user,pass,domain));
        GetMethod httpget = new GetMethod(location);
        httpget.setDoAuthentication(true);

        try
        {
            // execute the GET
            status = client.executeMethod(httpget);
            if (status==200)
            {
                BufferedInputStream bin = new BufferedInputStream
(httpget.getResponseBodyAsStream());

                ByteArrayOutputStream bos = new ByteArrayOutputStream();
                int bytesRead = 0;
                byte[] buff = new byte[16384];

                while ( (bytesRead = bin.read(buff)) != -1) {
                    bos.write(buff, 0, bytesRead);
                }

                // display the results.
                System.out.println(new String(bos.toByteArray()));
            }
        }
        catch (Throwable t)
        {
            t.printStackTrace();
        }
        finally
        {
            // release any connection resources used by the method
            httpget.releaseConnection();
        }
        return status;

    }

    public static void main(String[] args)
    {
        ShowProblem showProblem = new ShowProblem(args[0],args[1],args[2],args
[3]);
        int response = showProblem.getFile();
        
    }



    class CProvider implements CredentialsProvider
    {
        private String user;
        private String password;
        private String domain;

        public CProvider(String user, String password, String domain)
        {
            super();
            this.user = user;
            this.password = password;
            this.domain = domain;
        }

        public Credentials getCredentials(final AuthScheme authscheme,final 
String host,int port,boolean proxy)
        throws CredentialsNotAvailableException
        {
            if (authscheme == null)
            {
                return null;
            }
            try
            {
                if (authscheme instanceof NTLMScheme)
                {
                    return new NTCredentials(user, password, host, domain);
                }
                else if (authscheme instanceof RFC2617Scheme)
                {
                    return new UsernamePasswordCredentials(user, password);
                }
                else
                {
                    throw new CredentialsNotAvailableException(""Unsupported 
authentication scheme: "" +
                        authscheme.getSchemeName());
                }
            }
            catch (IOException e)
            {
                throw new CredentialsNotAvailableException(e.getMessage(), e);
            }
        }

    }
}"
1,"NullPointerException on DelegatingObservationDispatcher cause by parameter null on call : createEventStateCollection(null)There is a NullPointerException when jackrabbit try to synchronise its indexes :
22 janv. 2009 09:53:56 INFO  [ClusterNode] - Processing revision: 4485
22 janv. 2009 09:53:56 ERROR [ClusterNode] - Unexpected error while syncing of journal: null
java.lang.NullPointerException
        at org.apache.jackrabbit.core.observation.DelegatingObservationDispatcher.createEventStateCollection(DelegatingObservationDispatcher.java:80)
        at org.apache.jackrabbit.core.version.VersionManagerImpl$DynamicESCFactory.createEventStateCollection(VersionManagerImpl.java:556)
        at org.apache.jackrabbit.core.version.VersionManagerImpl.externalUpdate(VersionManagerImpl.java:500)
        at org.apache.jackrabbit.core.cluster.ClusterNode.process(ClusterNode.java:853)
        at org.apache.jackrabbit.core.cluster.ChangeLogRecord.process(ChangeLogRecord.java:457)
        at org.apache.jackrabbit.core.cluster.ClusterNode.consume(ClusterNode.java:799)
        at org.apache.jackrabbit.core.journal.AbstractJournal.doSync(AbstractJournal.java:213)
        at org.apache.jackrabbit.core.journal.AbstractJournal.sync(AbstractJournal.java:188)
        at org.apache.jackrabbit.core.cluster.ClusterNode.sync(ClusterNode.java:315)
        at org.apache.jackrabbit.core.cluster.ClusterNode.run(ClusterNode.java:286)
        at java.lang.Thread.run(Thread.java:595)

In fact the method createEventStateCollection() of DelegatingObservationDispatcher is called by the VersionManagerImpl with session parameter as null...

DelegatingObservationDispatcher:

 public EventStateCollection createEventStateCollection(SessionImpl session,
                                                           Path pathPrefix) {
        String userData = null;
        try {
            userData = ((ObservationManagerImpl) session.getWorkspace().getObservationManager()).getUserData();
        } catch (RepositoryException e) {
            // should never happen because this
            // implementation supports observation
        }
        return new EventStateCollection(this, session, pathPrefix, userData);
    }

VersionManagerImpl$DynamicESCFactory :
 public EventStateCollection createEventStateCollection(SessionImpl source) {
            return obsMgr.createEventStateCollection(source, VERSION_STORAGE_PATH);
        }

VersionManagerImpl :
public void externalUpdate(ChangeLog changes, List events,
                               long timestamp, String userData)
            throws RepositoryException {
        EventStateCollection esc = getEscFactory().createEventStateCollection(null);
        esc.addAll(events);
        esc.setTimestamp(timestamp);
        esc.setUserData(userData);

        sharedStateMgr.externalUpdate(changes, esc);
    }"
1,"method.getURI()  returns escaped URIs but it shouldn'tHi guys,

Please, consider the following imaginary and simplified code:


URI u = new URI(""http://some.host.com/%41.html"", true);
HttpClient httpClient = new HttpClient();
GetMethod method = new GetMethod();
method.setURI(u);
URI u2 = method.getURI();

System.out.println(""1. "" + u);
System.out.println(""2. "" + new String(u.getRawURI()));
System.out.println(""3. "" + u.getURI());
System.out.println(""4. "" + u2);
System.out.println(""5. "" + new String(u2.getRawURI()));
System.out.println(""6. "" + u2.getURI());


The result that you'll get is:

1. http://some.host.com/%41.html
2. http://some.host.com/%41.html
3. http://some.host.com/A.html
4. http://some.host.com/%2541.html
5. http://some.host.com/%2541.html
6. http://some.host.com/%41.html


You can see that for lines 4, 5, and 6, the URI suddenly gets escaped (the 
percent sign gets converted to %25).

Why is that? Am I doing something wrong? Is this the desired behaviour? I would 
have expected to get the SAME URI back, without any escaping.

Besides, I have another question:

After executing a method -- httpClient.executeMethod(method) -- what will 
method.getURI() return? The URI *after* all redirections or the original URI? 
It seems I get the URI *after* the redirections, which is fine, but the 
documentation doesn't say that. It only explicitly says that the getPath() 
method has that behaviour.

Best regards and thanks,
Bisser"
1,"RepositoryImpl.acquireRepositoryLock() fails to detect that the file lock is already held by the current processwith java 1.4 and 1.5 on a *nix-based platform it is possible to (concurrently) instantiate 
more than one repository instance in the same jvm based on same/identical configurations.

this is a critical issue since it might lead to data corruption.

the issue only exists with java versions prior to 1.6 and *nix-based platforms (only verified
on mac os-x 10.4).

note that the issue does not exist when the file lock is held by another jvm.

 code snippet to reproduce the issue:

            Repository rep1 = new TransientRepository();
            Session s1 = rep1.login(new SimpleCredentials(""johndoe"", """".toCharArray()));
            Repository rep2 = new TransientRepository();
            Session s2 = rep2.login(new SimpleCredentials(""johndoe"", """".toCharArray()));


the root problem is the incorrect behavior of java.nio.channels.FileChannel#tryLock()
which is demonstrated by the following code snippet:

            try {
                FileLock fl1 = new FileOutputStream(""foo"").getChannel().tryLock();
                System.out.println(""1st lock: "" + fl1);
                FileLock fl2 = new FileOutputStream(""foo"").getChannel().tryLock();
                System.out.println(""2nd lock: "" + fl2);
            } catch (Throwable t) {
                t.printStackTrace();
            }

"
1,"HttpClient loops endlessly while trying to retrieve status lineWhen fed with the wrong URL, for example http://localhost:19/ (chargen port),
HttpClient will loop endlessly while attempting to read the status line.

This is caused by a bug in HttpMethodBase.readStatusLine(HttpState, HttpConnection)

(while loop without any exceptional abort condition).

wire log excerpt:

2003/11/10 12:33:04:085 CET [DEBUG] HttpMethodDirector - -Execute loop try 1
2003/11/10 12:33:04:312 CET [DEBUG] wire - ->> ""GET / HTTP/1.1[\r][\n]""
2003/11/10 12:33:04:351 CET [DEBUG] HttpMethodBase - -Adding Host request header
2003/11/10 12:33:04:532 CET [DEBUG] wire - ->> ""User-Agent: Jakarta
Commons-HttpClient[\r][\n]""
2003/11/10 12:33:04:554 CET [DEBUG] wire - ->> ""Host: localhost:19[\r][\n]""
2003/11/10 12:33:04:559 CET [DEBUG] wire - ->> ""[\r][\n]""
2003/11/10 12:33:04:639 CET [DEBUG] wire - -<<
""!""#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefgh[\r][\n]""
2003/11/10 12:33:04:669 CET [DEBUG] wire - -<<
""""#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghi[\r][\n]""
2003/11/10 12:33:04:673 CET [DEBUG] wire - -<<
""#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghij[\r][\n]""
2003/11/10 12:33:04:692 CET [DEBUG] wire - -<<
""$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijk[\r][\n]""
2003/11/10 12:33:04:698 CET [DEBUG] wire - -<<
""%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijkl[\r][\n]""
2003/11/10 12:33:04:703 CET [DEBUG] wire - -<<
""&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklm[\r][\n]""
<snip>"
0,"contrib/remote tests fail randomlyThe contrib/remote tests will fail randomly.

This is because they use this _TestUtil.getRandomSocketPort() which
simply generates a random number, but if this is already in use, it will fail.

Additionally there is duplicate RMI logic across all 3 test classes."
0,"Small performance enhancement for StandardAnalyzerThe class StandardAnalyzer has an inner class, SavedStreams, which is used internally for maintaining some state. This class doesn't use the implicit reference to the enclosing class, so it can be made static and reduce some memory requirements. A patch will be attached shortly."
0,"Observation: avoid running out of memoryJackrabbit uses an unbounded observation queue for event listeners (for asynchronous listeners, which are the default). If an observation listener is very slow, the observation queue gets larger and larger, and the JVM will eventually run out of memory.

I suggest to use a maximum queue size of 100'000 by default. Adding new events to the queue will block until the observation listeners removed an item. I'm not sure if we need a way to configure this option; probably a system property is enough as a start (we can still add a better way to configure this setting if it turns out somebody actually needs a different value).

A special case is observation listeners that themselves write to the repository and therefore cause new events. In this case, it doesn't make sense to block adding an event, because that would block the whole system. However a warning should be written to the log file."
0,"tests-local fails on 3 tests1)
testMultiSendCookieGet(org.apache.commons.httpclient.TestWebappCookie)junit.framework.AssertionFailedError:
<html>
<head><title>ReadCookieServlet: GET</title></head>
<body>
<p>This is a response to an HTTP GET request.</p>
<p><tt>Cookie:
$Version=1;simplecookie=value;$Path=/httpclienttest/cookie;$Domain=localhost</tt></p>
<tt>$Version=1</tt><br>
<tt>simplecookie=value</tt><br>
<tt>$Path=/httpclienttest/cookie</tt><br>
<tt>$Domain=localhost</tt><br>
</body>
</html>
    at
org.apache.commons.httpclient.TestWebappCookie.testMultiSendCookieGet(TestWebappCookie.java:348)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
2)
testDeleteCookieGet(org.apache.commons.httpclient.TestWebappCookie)junit.framework.AssertionFailedError:
<html>
<head><title>ReadCookieServlet: GET</title></head>
<body>
<p>This is a response to an HTTP GET request.</p>
<p><tt>Cookie:
$Version=1;simplecookie=value;$Path=/httpclienttest/cookie;$Domain=localhost</tt></p>
<tt>$Version=1</tt><br>
<tt>simplecookie=value</tt><br>
<tt>$Path=/httpclienttest/cookie</tt><br>
<tt>$Domain=localhost</tt><br>
</body>
</html>
    at
org.apache.commons.httpclient.TestWebappCookie.testDeleteCookieGet(TestWebappCookie.java:389)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
3)
testDeleteCookiePut(org.apache.commons.httpclient.TestWebappCookie)junit.framework.AssertionFailedError:
<html>
<head><title>ReadCookieServlet: PUT</title></head>
<body>
<p>This is a response to an HTTP PUT request.</p>
<p><tt>Cookie:
$Version=1;simplecookie=value;$Path=/httpclienttest/cookie;$Domain=localhost</tt></p>
<tt>$Version=1</tt><br>
<tt>simplecookie=value</tt><br>
<tt>$Path=/httpclienttest/cookie</tt><br>
<tt>$Domain=localhost</tt><br>
</body>
</html>
    at
org.apache.commons.httpclient.TestWebappCookie.testDeleteCookiePut(TestWebappCookie.java:464)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
FAILURES!!!
Tests run: 108,  Failures: 3,  Errors: 0
httpclient/build.xml [271] Java returned: -1
BUILD FAILED
Total time: 9 seconds"
0,IndexWriter has incomplete JavadocsA couple of getter methods in IndexWriter have no javadocs.
1,"Host request header does not contain portThe Host request header is always added with just the hostname used for the 
connection.  If the port is different than 80 it needs to be included as well, 
with a colon separating it from the hostname.  This problem is especially 
apparent when you use the httpclient to connect to tomcat 4 and then use 
HttpUtils to create a full URL representing the request.  HttpUtils pulls the 
host and port from the Host header.  When commons-httpclient is used HttpUtils 
never includes the port since it was never in the Host header."
0,"Cleanup MMapDirectory to use only one MMapIndexInput impl with mapping sized of powers of 2Robert and me discussed a little bit after Mike's investigations, that using SingleMMapIndexinput together with MultiMMapIndexInput leads to hotspot slowdowns sometimes.

We had the following ideas:
- MultiMMapIndexInput is almost as fast as SingleMMapIndexInput, as the switching between buffer boundaries is done in exception catch blocks. So normal code path is always the same like for Single*
- Only the seek method uses strange calculations (the modulo is totally bogus, it could be simply: int bufOffset = (int) (pos % maxBufSize); - very strange way of calculating modulo in the original code)
- Because of speed we suggest to no longer use arbitrary buffer sizes. We should pass only the power of 2 to the indexinput as size. All calculations in seek and anywhere else would be simple bit shifts and AND operations (the and masks for the modulo can be calculated in the ctor like NumericUtils does when calculating precisionSteps).
- the maximum buffer size will now be 2^30, not 2^31-1. But thats not an issue at all. In my opinion, a buffer size of 2^31-1 is stupid in all cases, as it will no longer fit page boundaries and mmapping gets harder for the O/S.

We will provide a patch with those cleanups."
1,"RMI published Repository using the jcr-rmi library gets lost over timeThe jcr-server/webapp project contains a servlet - RepositoryStartupServlet - which may be used in a web app to start a repository and optionally register the repository with JNDI and RMI. To register the repository with JNDI, the jcr-rmi library is used to create a Remote repository instance, which is registered with the RMI registry. Inside the RMI implementation mechanisms based on stub classes created by the RMI compiler are created to make the remote repository available remotely. This includes creating a object table to map remote references to local objects. This table stores references to the local object as weak references to support distributed garbage collection.

Over time, it may now be that this remote repository instance is actually collected and the object table cannot access it anymore thus preventing the repository from being accessed remotely. To prevent this from happening, the RepositoryStartupServlet must keep a strong reference to the remote repository and drop this reference when the servlet is destroyed and the repository unregistered.

*NOTE:* This is an issue to all long running applications which publish repository instances over RMI using the jcr-rmi library."
0,"When A URL is redirected, there is no easy way to encode the new url before HC tries to execute it/When you implement your custom RedirectHandler, there is no easy way to encode the new URL being redirected to.

A public method to access the location string prior to the URI generation would be useful."
0,"Change security configuration from 'simple' to the some reasonable defaultI'd like to change the security configuration from 'simple' to a default that enables ac-management and user management (and consequently doesn't skip the corresponding tests).

there is currently an issue with WEAKREFERENCEs (see issue JCR-2135) that prevents me from changing the config.
unless someone objects i would like to change the default config as soon as JCR-2135 is solved.

"
0,"Make MultiThreadedHttpConnectionManager defaults public statics.Could the defaults for MultiThreadedHttpConnectionManager be made public
constants? I would do it my self since I have karma as a contributer to [lang]
and [codec] but I do not want to step on anyones toes. ;-)

Patch attached."
0,"Add top-down version of BlockJoinQueryToday, BlockJoinQuery can join from child docIDs up to parent docIDs.
EG this works well for product (parent) + many SKUs (child) search.

But the reverse, which BJQ cannot do, is also useful in some cases.
EG say you index songs (child) within albums (parent), but you want to
search and present by song not album while involving some fields from
the album in the query.  In this case you want to wrap a parent query
(against album), joining down to the child document space.
"
0,Versioning operations should be done on the workspacecurrently all versioning operations modify the transient states of the items where the operation is executed although all operations are workspace operations.
0,"contrib/xml-query-parser, BoostingTermQuery supportI'm not 100% on this patch. 

BooleanTermQuery is a part of the spans family, but I generally use that class as a replacement for TermQuery.  Thus in the DTD I have stated that it can be a part of the root queries as well as a part of a span. 

However, SpanFooQueries xml elements are named <SpanFoo/> rather than <SpanFooQuery/>, I have however chosen to call it <BoostingTermQuery/>. It would be possible to set it up so it would be parsed as <SpanBoostingTerm/> when inside of a <SpanSomething>, but I just find that confusing.
"
0,"SPI: improve description of locking methods on RepositoryServicein detail:

1) getLockInfo

- intended behavior if no lock is present?
- intended behavior if locking is not supported?

2) lock

- currently InvalidItemStateException is listed. i don't think this make too much sense.

3) refreshLock

- intended behavior if locking is not supported?

4) unlock

- currently InvalidItemStateException is listed. i don't think this make too much sense."
1,"jcr:baseVersion is not updated when the base version is removed from the version history
        Session s1 = repo.login(new SimpleCredentials(""user1"", ""pwd1"".toCharArray()));
        Node root1 = s1.getRootNode() ;
        Node test1 = root1.addNode(""test"") ;
        test1.addMixin(""mix:versionable"");
        s1.save() ;
        System.out.println(test1.getProperty(""jcr:baseVersion"").getValue().getString()) ;
        test1.checkin() ;
        System.out.println(test1.getProperty(""jcr:baseVersion"").getValue().getString()) ;
        test1.getVersionHistory().removeVersion(""1.0"") ;
        // the base version wasn't updated :(
        System.out.println(test1.getProperty(""jcr:baseVersion"").getValue().getString()) ;
        // the next line throws ItemNotFoundException :(
        test1.getBaseVersion() ;

javax.jcr.ItemNotFoundException: c33bf049-c7e1-4b34-968a-63ff1b1113b0
	at org.apache.jackrabbit.core.ItemManager.createItemInstance(ItemManager.java:498)
	at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:349)
	at org.apache.jackrabbit.core.PropertyImpl.getNode(PropertyImpl.java:642)
	at org.apache.jackrabbit.core.NodeImpl.getBaseVersion(NodeImpl.java:2960)
	at org.apache.jackrabbit.core.RemoveVersionTest.main(RemoveVersionTest.java:56)


"
1,"cache does not honor must-revalidate or proxy-revalidate Cache-Control directiveshttp://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.4

There are a couple of missed requirements here regarding must-revalidate and proxy-revalidate (which applies only to shared caches).
1. When a cache entry with this directive is revalidated, it must be an end-to-end revalidation (meaning it must include 'max-age=0' on the request).
2. If the revalidation with the origin fails, the cache MUST NOT return a stale entry and MUST return a 504 response.
"
0,"Adding same IndexDocValuesField twice trips assertDoc values fields are single-valued by design, ie a given field name can only occur once in the document.

But if you accidentally add it more than once, you get an assert error, which is spooky because if you run w/o asserts maybe something eviler happens.

I think we should explicitly check for this and throw clear exc since user could easily do this by accident."
1,"NPE in RequestProxyAuthentication on AndroidGot a NPE backtrace in RequestProxyAuthentication.process(). 

HttpRoute route = conn.getRoute();
        if (route.isTunnelled()) {      <= line 88, NPE here
            return;
        }

There's no null check on the returned route although getRoute() can return null.
I guess it's not supposed to happen.

In the httpclient code, there's a few more calls to getRoute() without a null check on the returned route.


java.lang.NullPointerException
at com.bubblesoft.org.apache.http.client.protocol.RequestProxyAuthentication.process(SourceFile:88)
at com.bubblesoft.org.apache.http.protocol.ImmutableHttpProcessor.process(SourceFile:108)
at com.bubblesoft.org.apache.http.protocol.HttpRequestExecutor.preProcess(SourceFile:174)
at com.bubblesoft.org.apache.http.impl.client.DefaultRequestDirector.execute(SourceFile:457)
at com.bubblesoft.org.apache.http.impl.client.AbstractHttpClient.createHttpProcessor(SourceFile:821)
                                                                 execute
at com.bubblesoft.org.apache.http.impl.client.AbstractHttpClient.createHttpProcessor(SourceFile:755)
                                                                 execute
at com.bubblesoft.org.apache.http.impl.client.AbstractHttpClient.createHttpProcessor(SourceFile:733)
                                                                 execute
"
0,Promote the classloader component from contribThis is a dummy issue for a change I already made (revisions 529068 and 529137) to promote the classloader component from contrib. I'm including this here in the issue tracker to complete the release notes for Jackrabbit 1.3.
1,"VarDerefBytesImpl doc values prefix length may fall across two pagesThe VarDerefBytesImpl doc values encodes the unique byte[] with prefix (1 or 2 bytes) first, followed by bytes, so that it can use PagedBytes.fillSliceWithPrefix.

It does this itself rather than using PagedBytes.copyUsingLengthPrefix...

The problem is, it can write an invalid 2 byte prefix spanning two blocks (ie, last byte of block N and first byte of block N+1), which fillSliceWithPrefix won't decode correctly.

"
0,"Checksum Wrong for HttpComponent project pom v4.1 on centralAs evidenced on the log here: http://vmgump.apache.org/gump/public/httpcomponents/httpcomponents-core/gump_work/build_httpcomponents_httpcomponents-core.html

The checksum in central for httpcomponents-project-4.1.pom is incorrect in maven central.

---------------------------8<--------------------------------

Downloading: http://localhost:8192/maven2/org/apache/httpcomponents/project/4.1/project-4.1.pom

[WARNING] *** CHECKSUM FAILED - Checksum failed on download: local = 'b63ff67e6ffc1940041319e0e06d7c6b1d671fd2'; remote = '8edff11652ca51b9d110ebfb321daac24f031c07' - RETRYING
Downloading: http://localhost:8192/maven2/org/apache/httpcomponents/project/4.1/project-4.1.pom

[WARNING] *** CHECKSUM FAILED - Checksum failed on download: local = 'b63ff67e6ffc1940041319e0e06d7c6b1d671fd2'; remote = '8edff11652ca51b9d110ebfb321daac24f031c07' - IGNORING
This pom appears to be a dependency for httpcomponents 4.0.3

---------------------------8<--------------------------------

This checksum failure causes configurations that reject such artifacts (such as many maven proxy configurations) to result in build failures due to unsatisfied dependencies. 

"
1,"Clustering is broken due to duplicated CachingPersistenceManager interfaceThere are now two interfaces CachingPersistenceManager in the packages org.apache.jackrabbit.core.persistence.bundle and org.apache.jackrabbit.core.persistence.pool. A persistence manager that implements the ..bundle... interface doesn't receive the onExternalUpdate events that are required for clustering to work.

I will move this interface to the package org.apache.jackrabbit.core.persistence and remove the second implementation.

This change has no affect to backward compatibility, because anyway there were many breaking changes in the past (NodeId / UUID for example).
"
0,[API Doc] HttpClient tutorial updateBring the tutorial up to date with the latest best practices
0,"WriteLineDocTask improvementsMake WriteLineDocTask and LineDocSource more flexible/extendable:
* allow to emit lines also for empty docs (keep current behavior as default)
* allow more/less/other fields"
0,Reduce number of compiler warning by adding @Override and generics where appropriate Add @Override and generics where possible to reduce the number of warnings issued by the compiler.
1,"sysview export/import of multivalue properties seems not to workthe sysview export of multivalue properties does not differ from the export of singlevalue properties.

if a mv-property contains only 1 value, how can the import find the correct property-def?

currently, it throws: 
  javax.jcr.nodetype.ConstraintViolationException: no matching property definition found for <propertyname>"
0,"in advance(), don't try to skip if there is evidence it will failThere are TODO's about this in the code everywhere, and this was part of
Mike speeding up ExactPhraseScorer.

I think the codec should do this."
0,AbstractLockTest.testLockExpiration fails intermittently This seems to be a timing issue. I propose to wait a bit longer for the lock to expire. 
0,"Fine grained locking in SharedItemStateManagerThe SharedItemStateManager (SISM) currently uses a simple read-write lock to ensure data consistency. Store operations to the PersistenceManager (PM) are effectively serialized.

We should think about more sophisticated locking to allow concurrent writes on the PM.

One possible approach:

If a transaction is currently storing data in a PM a second transaction may check if the set of changes does not intersect with the first transaction. If that is the case it can safely store its data in the PM.

This fine grained locking must also be respected when reading from the SISM. A read request for an item that is currently being stored must be blocked until the store is finished."
1,"SO_TIMEOUT is not set on a request levelThe scenario is as follows: I'm doing two consecutive requests to the same host, using a multi-threaded (or thread safe) connection pool manager. The first invocation has a timeout of 10s and the second has a timeout of 30s. 

In version 3.1 of HttpClient all works well, but in 4.0 I get a timeout exception in the second request, after ~10 seconds, which means the first timeout is used.

Looking at the code, I see that in version 3.1, the HttpMethodDirector.executeWithRetry() method invokes a method named applyConnectionParams() that took care of setting the timeout taken from the request on the socket. 

But in version 4.0, the only place I see the timeout is set on the socket is when DefaultRequestDirector.execute(HttpHost, HttpRequest, HttpContext) opens a connection using the managedConn.open() method. Since the connection is reused between the requests, the second request uses a socket with a timeout of the first request.
"
0,Add a Lucene3x private SegmentInfosFormat implemenationwe still don't have a Lucene3x & preflex version of segment infos format. we need this before we release 4.0
1,"StaleItemStateException with distributed transactionsThere seams to be a serious bug in jackrabbit when used in distributed transactions. It does not occur with local transactions! And it seams to be related to JCR-566.

There are 2 scenarios where a StaleItemStateException occurs reproducible that causes transactions to fail. All my operations (implemented in a custom ServiceBean) such as setProperty() or deleteNode() run in separate transactions. The transactions are configured through Spring Annotations (@Transactional).

Scenario A (setProperty):
(1) multiple setProperty() with same property name on the same node (newly created or already existent)
=> With the 3. setProperty() (and sometimes also the 5.), a StaleItemStateException for the property state is raised when the transaction is commited. Following setProperty invocations will not fail!

Scenario B (deleteNode):
(1) iterate 10 times:
(1.1) create new node n and a subnode for n
(1.2) delete node n
=> Deletion of node n raises a StaleItemStateException for node n in iteration 1, 3 and (6 or 7), when the related transaction is commited. Following deletions of node n will also fail with a predictable pattern.

The Exception trace for scenario A (it's the same for scenario B, with one difference: StaleItemStateException is raised for the node and not for the property):

org.springframework.transaction.UnexpectedRollbackException: JTA transaction unexpectedly rolled back (maybe due to a timeout); nested exception is javax.transaction.RollbackException: Error during one-phase commit
	at org.springframework.transaction.jta.JtaTransactionManager.doCommit(JtaTransactionManager.java:1031)
	at org.springframework.transaction.support.AbstractPlatformTransactionManager.processCommit(AbstractPlatformTransactionManager.java:709)
	at org.springframework.transaction.support.AbstractPlatformTransactionManager.commit(AbstractPlatformTransactionManager.java:678)
	at org.springframework.transaction.interceptor.TransactionAspectSupport.commitTransactionAfterReturning(TransactionAspectSupport.java:321)
	at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:116)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:204)
	at $Proxy9.setNodeProperty(Unknown Source)
	at de.zeb.control.prototype.jrTxBug.test.TestJackrabbitTxBug.testTransactionBug001(TestJackrabbitTxBug.java:97)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at org.testng.internal.MethodHelper.invokeMethod(MethodHelper.java:580)
	at org.testng.internal.Invoker.invokeMethod(Invoker.java:478)
	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:607)
	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:874)
	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:125)
	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109)
	at org.testng.TestRunner.runWorkers(TestRunner.java:689)
	at org.testng.TestRunner.privateRun(TestRunner.java:566)
	at org.testng.TestRunner.run(TestRunner.java:466)
	at org.testng.SuiteRunner.runTest(SuiteRunner.java:301)
	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:296)
	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:276)
	at org.testng.SuiteRunner.run(SuiteRunner.java:191)
	at org.testng.TestNG.createAndRunSuiteRunners(TestNG.java:808)
	at org.testng.TestNG.runSuitesLocally(TestNG.java:776)
	at org.testng.TestNG.run(TestNG.java:701)
	at org.testng.remote.RemoteTestNG.run(RemoteTestNG.java:73)
	at org.testng.remote.RemoteTestNG.main(RemoteTestNG.java:124)
Caused by: javax.transaction.RollbackException: Error during one-phase commit
	at org.apache.geronimo.transaction.manager.TransactionImpl.commit(TransactionImpl.java:281)
	at org.apache.geronimo.transaction.manager.TransactionManagerImpl.commit(TransactionManagerImpl.java:143)
	at org.apache.geronimo.transaction.context.InheritableTransactionContext.complete(InheritableTransactionContext.java:196)
	at org.apache.geronimo.transaction.context.InheritableTransactionContext.commit(InheritableTransactionContext.java:146)
	at org.apache.geronimo.transaction.context.OnlineUserTransaction.commit(OnlineUserTransaction.java:80)
	at org.jencks.factory.UserTransactionFactoryBean$GeronimoUserTransaction.commit(UserTransactionFactoryBean.java:118)
	at org.springframework.transaction.jta.JtaTransactionManager.doCommit(JtaTransactionManager.java:1028)
	... 30 more
Caused by: javax.transaction.xa.XAException
	at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:155)
	at org.apache.jackrabbit.core.XASessionImpl.commit(XASessionImpl.java:337)
	at org.apache.jackrabbit.jca.TransactionBoundXAResource.commit(TransactionBoundXAResource.java:39)
	at org.apache.geronimo.transaction.manager.WrapperNamedXAResource.commit(WrapperNamedXAResource.java:47)
	at org.apache.geronimo.transaction.manager.TransactionImpl.commit(TransactionImpl.java:272)
	... 36 more
Caused by: org.apache.jackrabbit.core.TransactionException: Unable to prepare transaction.
	at org.apache.jackrabbit.core.state.XAItemStateManager.prepare(XAItemStateManager.java:150)
	at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:138)
	... 40 more
Caused by: org.apache.jackrabbit.core.state.StaleItemStateException: bef3c056-bc91-4195-a35c-aa184182b5ad/{}TEST_PROPERTY has been modified externally
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:620)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:843)
	at org.apache.jackrabbit.core.state.XAItemStateManager.prepare(XAItemStateManager.java:144)
	... 41 more


When debugging into jackrabbit you will see, that the cause of the StaleItemStateException is, that the local state und the overlayed state differ in the value of the 'modCount' attribute: modCount of local state is lower than modCount of overlayed state. Perhaps its a state caching problem...
	
I'm attaching a simple java application configured with maven and ready to run standalone. The JCA container of JBoss is therefore replaced with jencks in order to support distributed transactions. The configured repository uses the InMemPersistenceManager. Both scenarios are implemented in a TestNG - test, that catches the occuring TransactionExceptions and prints out the stacktrace. Therefore you will see the exceptions, but the tests will not fail."
0,"Update commons-io dependency from versiom 1.4 to 2.0.1 Jackrabbit may be used as JackRabbit-JCA. In many application servers we need to change the classloading policy to ""global"" or ""flat"". These classloading policies result in many conflicts between versions of own server implementation or version of applications. It would be diminished if Jackrabbit dependences are keep ""up-to-date"".

<dependency>
	<groupId>commons-io</groupId>
	<artifactId>commons-io</artifactId>
	<version>2.0.1</version>
</dependency>"
0,"Enable FileSystem unit testsThe FileSystem tests are implemented, but not actually run, because the TestAll class is missing.
Also, there is a bug in the tests that causes the tests to fail."
1,BitVector never skips fully populated bytes when writing ClearedDgapsWhen writing cleared DGaps in BitVector we compare a byte against 0xFF (255) yet the byte is casted into an int (-1) and the comparison will never succeed. We should mask the byte with 0xFF before comparing or compare against -1
1,"invalid groupid for tm-extractors in textfilters projectgroupid for tm-extractors should be ""org.textmining"" and not ""textmining"".
The dependency with the correct groupid is available on ibiblio"
0,"improve how MTQs interact with the terms dict cacheSome small improvements:

  * Adds a TermsEnum.cacheCurrentTerm ""hint"" (codec can make this a no-op)

  * Removes the FTE.useTermsCache

  * Changes MTQ's TermCollector API to accept the TermsEnum so collectors can eg call .docFreq directly

  * Adds expert ctor to TermQuery allowing you to pass in the docFreq"
0,"RAMDirectory.close() should have a comment about not releasing any resourcesI wrongly assumed that calling RAMDirectory.close() would free up the memory occupied by the RAMDirectory.
It might be helpful to add a javadoc comment that warns users that RAMDirectory.close() is a no-op, since it might be a common assumption that close() would release resources."
0,"Bad transitive dependencies in commons-httpclientAs reported in HTTPCLIENT-605, the commons-httpclient 3.0 dependency introduces junit as a transitive ""compile"" scope dependency. The library also uses commons-logging, which sidesteps Jackrabbit's use of slf4j for logging.

To avoid these issues we should locally override the junit dependency in commons-httpclient and replace the commons-logging dependency with jcl104-over-slf4j."
0,"Tests failing when run with tests.iter > 1TestMultiLevelSkipList and TestsFieldReader are falling if run with -Dtests.iter > 1 - not all values are reset though
I will attach a patch in a second."
0,"Command line access to remote repositoriesA few years ago Edgar Poce implemented a nice command line JCR access tool called jcr-commands. We haven't really been using it much and the code is currently parked in sandbox/inactive.

I'd like to resurrect this codebase and integrate it to jackrabbit-standalone to implement command line access to remote repositories. The idea would be to have an easy-to-use tool for simple testing and administration tasks.
"
1,"DocViewSaxEventGenerator may generate non-NS-wellformed XMLThe XML serialization code relies on the fact that all required prefix-to-uri mappings are known beforehand (actually, when serializing the root node). So there's an assumption that the permanent namespace registry will never change during serialization, which may be incorrect when another client adds namespace registrations while the XML export is in progress.

To fix this, ""addNamespacePrefixes"" should ensure that namespace declarations have been written for all prefixes used on the current node (node name + properties), potentially going back to the namespace resolver when needed.

(Should there be consensus for that change I'm happy to give it a try)"
1,"Proxy NTLM Authentication  Redirecting to different address fails saying Proxy Auth Required.The issue has been discussed in,
http://www.nabble.com/redirect-fails-when-NTLM-authentication-is-used-for-proxy-tt23867531.html

This was found in http client 3.1 release,  where NTLM proxy authentication is must and the server ask the redirect to a new url, in this case, when redirecting, the earlier proxy auth status is not cleared, so, it does not do proxy authentication for the new URL and hence fails.

Target Host Authenticaiton NTLM authentication - redirect also had problem and fixed as said,
http://issues.apache.org/jira/browse/HTTPCLIENT-211
Proxy Authentication - redirect has to be fixed, 

The wire logs for the release https://repository.apache.org/content/repositories/snapshots/org/apache/httpcomponents/httpclient/4.0-beta3-SNAPSHOT/
is given below,

[DEBUG] wire - >> ""GET http://verisign.com HTTP/1.1[EOL]""
[DEBUG] wire - >> ""Host: verisign.com[EOL]""
[DEBUG] wire - >> ""Proxy-Connection: Keep-Alive[EOL]""
[DEBUG] wire - >> ""User-Agent: Apache-HttpClient/UNAVAILABLE (java 1.5)[EOL]""
[DEBUG] wire - >> ""[EOL]""
[DEBUG] wire - << ""HTTP/1.1 407 Proxy Authentication Required ( The ISA Server requires authorization to fulfill the request. Access to the Web Proxy filter is denied.  )[EOL]""
[DEBUG] wire - << ""Via: 1.1 lab1[EOL]""
[DEBUG] wire - << ""Proxy-Authenticate: Negotiate[EOL]""
[DEBUG] wire - << ""Proxy-Authenticate: Kerberos[EOL]""
[DEBUG] wire - << ""Proxy-Authenticate: NTLM[EOL]""
[DEBUG] wire - << ""Proxy-Authenticate: Basic realm=""lab1.""[EOL]""
[DEBUG] wire - << ""Connection: Keep-Alive[EOL]""
[DEBUG] wire - << ""Proxy-Connection: Keep-Alive[EOL]""
[DEBUG] wire - << ""Pragma: no-cache[EOL]""
[DEBUG] wire - << ""Cache-Control: no-cache[EOL]""
[DEBUG] wire - << ""Content-Type: text/html[EOL]""
[DEBUG] wire - << ""Content-Length: 4107  [EOL]""
[DEBUG] wire - << ""[EOL]""
[DEBUG] wire - << ""<!DOCTYPE HTML PUBLIC ""-//W3C//DTD HTML 4.0 Transitional//EN"">[\r][\n]""
[DEBUG] wire - << ""<HTML><HEAD><TITLE>Error Message</TITLE>[\r][\n]""
[DEBUG] wire - << ""<META http-equiv=Content-Type content=""text/html; charset=UTF-8"">[\r][\n]""
[DEBUG] wire - << ""<STYLE id=L_default_1>A {[\r][\n]""
[DEBUG] wire - << ""[0x9]FONT-WEIGHT: bold; FONT-SIZE: 10pt; COLOR: #005a80; FONT-FAMILY: tahoma[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << ""A:hover {[\r][\n]""
[DEBUG] wire - << ""[0x9]FONT-WEIGHT: bold; FONT-SIZE: 10pt; COLOR: #0d3372; FONT-FAMILY: tahoma[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << ""TD {[\r][\n]""
[DEBUG] wire - << ""[0x9]FONT-SIZE: 8pt; FONT-FAMILY: tahoma[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << ""TD.titleBorder {[\r][\n]""
[DEBUG] wire - << ""[0x9]BORDER-RIGHT: #955319 1px solid; BORDER-TOP: #955319 1px solid; PADDING-LEFT: 8px; FONT-WEIGHT: bold; FONT-SIZE: 12pt; VERTICAL-ALIGN: middle; BORDER-LEFT: #955319 0px solid; COLOR: #955319; BORDER-BOTTOM: #955319 1px solid; FONT-FAMILY: tahoma; HEIGHT: 35px; BACKGROUND-COLOR: #d2b87a; TEXT-ALIGN: left[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << ""TD.titleBorder_x {[\r][\n]""
[DEBUG] wire - << ""[0x9]BORDER-RIGHT: #955319 0px solid; BORDER-TOP: #955319 1px solid; PADDING-LEFT: 8px; FONT-WEIGHT: bold; FONT-SIZE: 12pt; VERTICAL-ALIGN: middle; BORDER-LEFT: #955319 1px solid; COLOR: #978c79; BORDER-BOTTOM: #955319 1px solid; FONT-FAMILY: tahoma; HEIGHT: 35px; BACKGROUND-COLOR: #d2b87a; TEXT-ALIGN: left[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << "".TitleDescription {[\r][\n]""
[DEBUG] wire - << ""[0x9]FONT-WEIGHT: bold; FONT-SIZE: 12pt; COLOR: black; FONT-FAMILY: tahoma[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << ""SPAN.explain {[\r][\n]""
[DEBUG] wire - << ""[0x9]FONT-WEIGHT: normal; FONT-SIZE: 10pt; COLOR: #934225[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << ""SPAN.TryThings {[\r][\n]""
[DEBUG] wire - << ""[0x9]FONT-WEIGHT: normal; FONT-SIZE: 10pt; COLOR: #934225[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << "".TryList {[\r][\n]""
[DEBUG] wire - << ""[0x9]MARGIN-TOP: 5px; FONT-WEIGHT: normal; FONT-SIZE: 8pt; COLOR: black; FONT-FAMILY: tahoma[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << "".X {[\r][\n]""
[DEBUG] wire - << ""[0x9]BORDER-RIGHT: #955319 1px solid; BORDER-TOP: #955319 1px solid; FONT-WEIGHT: normal; FONT-SIZE: 12pt; BORDER-LEFT: #955319 1px solid; COLOR: #7b3807; BORDER-BOTTOM: #955319 1px solid; FONT-FAMILY: verdana; BACKGROUND-COLOR: #d1c2b4[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << "".adminList {[\r][\n]""
[DEBUG] wire - << ""[0x9]MARGIN-TOP: 2px[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << ""</STYLE>[\r][\n]""
[DEBUG] wire - << ""<META content=""MSHTML 6.00.2800.1170"" name=GENERATOR></HEAD>[\r][\n]""
[DEBUG] wire - << ""<BODY bgColor=#f3f3ed>[\r][\n]""
[DEBUG] wire - << ""<TABLE cellSpacing=0 cellPadding=0 width=""100%"">[\r][\n]""
[DEBUG] wire - << ""  <TBODY>[\r][\n]""
[DEBUG] wire - << ""  <TR>[\r][\n]""
[DEBUG] wire - << ""    <TD class=titleborder_x width=30>[\r][\n]""
[DEBUG] wire - << ""      <TABLE height=25 cellSpacing=2 cellPadding=0 width=25 bgColor=black>[\r][\n]""
[DEBUG] wire - << ""        <TBODY>[\r][\n]""
[DEBUG] wire - << ""        <TR>[\r][\n]""
[DEBUG] wire - << ""          <TD class=x vAlign=center alig""
[DEBUG] wire - << ""n=middle>X</TD>[\r][\n]""
[DEBUG] wire - << ""        </TR>[\r][\n]""
[DEBUG] wire - << ""        </TBODY>[\r][\n]""
[DEBUG] wire - << ""      </TABLE>[\r][\n]""
[DEBUG] wire - << ""    </TD>[\r][\n]""
[DEBUG] wire - << ""    <TD class=titleBorder id=L_default_2>Network Access Message:<SPAN class=TitleDescription> The page cannot be displayed</SPAN> </TD>[\r][\n]""
[DEBUG] wire - << ""  </TR>[\r][\n]""
[DEBUG] wire - << ""  </TBODY>[\r][\n]""
[DEBUG] wire - << ""</TABLE>[\r][\n]""
[DEBUG] wire - << ""[\r][\n]""
[DEBUG] wire - << ""<TABLE id=spacer>[\r][\n]""
[DEBUG] wire - << ""  <TBODY>[\r][\n]""
[DEBUG] wire - << ""  <TR>[\r][\n]""
[DEBUG] wire - << ""    <TD height=10></TD></TR></TBODY></TABLE>[\r][\n]""
[DEBUG] wire - << ""<TABLE width=400>[\r][\n]""
[DEBUG] wire - << ""  <TBODY>[\r][\n]""
[DEBUG] wire - << ""  <TR>[\r][\n]""
[DEBUG] wire - << ""    <TD noWrap width=25></TD>[\r][\n]""
[DEBUG] wire - << ""    <TD width=400><SPAN class=explain><ID id=L_default_3><B>Explanation:</B></ID></SPAN><ID id=L_default_4> There is a problem with the page you are trying to reach and it cannot be displayed. </ID><BR><BR>[\r][\n]""
[DEBUG] wire - << ""    <B><SPAN class=tryThings><ID id=L_default_5><B>Try the following:</B></ID></SPAN></B> [\r][\n]""
[DEBUG] wire - << ""      <UL class=TryList>[\r][\n]""
[DEBUG] wire - << ""        <LI id=L_default_6><B>Refresh page:</B> Search for the page again by clicking the Refresh button. The timeout may have occurred due to Internet congestion.[\r][\n]""
[DEBUG] wire - << ""<LI id=L_default_7><B>Check spelling:</B> Check that you typed the Web page address correctly. The address may have been mistyped.[\r][\n]""
[DEBUG] wire - << ""<LI id=L_default_8><B>Access from a link:</B> If there is a link to the page you are looking for, try accessing the page from that link.[\r][\n]""
[DEBUG] wire - << ""[\r][\n]""
[DEBUG] wire - << ""      </UL>[\r][\n]""
[DEBUG] wire - << ""<ID id=L_default_9>If you are still not able to view the requested page, try contacting your administrator or Helpdesk.</ID> <BR><BR>[\r][\n]""
[DEBUG] wire - << ""    </TD>[\r][\n]""
[DEBUG] wire - << ""  </TR>[\r][\n]""
[DEBUG] wire - << ""  </TBODY>[\r][\n]""
[DEBUG] wire - << ""</TABLE>[\r][\n]""
[DEBUG] wire - << ""[\r][\n]""
[DEBUG] wire - << ""<TABLE id=spacer><TBODY><TR><TD height=15></TD></TR></TBODY></TABLE>[\r][\n]""
[DEBUG] wire - << ""[\r][\n]""
[DEBUG] wire - << ""<TABLE width=400>[\r][\n]""
[DEBUG] wire - << ""  <TBODY>[\r][\n]""
[DEBUG] wire - << ""  <TR>[\r][\n]""
[DEBUG] wire - << ""    <TD noWrap width=25></TD>[\r][\n]""
[DEBUG] wire - << ""    <TD width=400 id=L_default_10><B>Technical Information (for support personnel)</B> [\r][\n]""
[DEBUG] wire - << ""      <UL class=adminList>[\r][\n]""
[DEBUG] wire - << ""        <LI id=L_default_11>Error Code: 407 Proxy Authentication Required. The ISA Server requires authorization to fulfill the request. Access to the Web Proxy filter is denied. (12209)[\r][\n]""
[DEBUG] wire - << ""<LI id=L_default_12>IP Address: x.x.x.x[\r][\n]""
[DEBUG] wire - << ""<LI id=L_default_13>Date: 6/29/2009 11:15:15 AM [GMT][\r][\n]""
[DEBUG] wire - << ""<LI id=L_default_14>Server: lab1[\r][\n]""
[DEBUG] wire - << ""<LI id=L_default_15>Source: proxy[\r][\n]""
[DEBUG] wire - << ""[\r][\n]""
[DEBUG] wire - << ""      </UL>[\r][\n]""
[DEBUG] wire - << ""    </TD>[\r][\n]""
[DEBUG] wire - << ""  </TR>[\r][\n]""
[DEBUG] wire - << ""  </TBODY>[\r][\n]""
[DEBUG] wire - << ""</TABLE>[\r][\n]""
[DEBUG] wire - << ""[\r][\n]""
[DEBUG] wire - << ""</BODY>[\r][\n]""
[DEBUG] wire - << ""</HTML>[\r][\n]""
[DEBUG] wire - << ""[\r][\n]""
[DEBUG] wire - >> ""GET http://verisign.com HTTP/1.1[EOL]""
[DEBUG] wire - >> ""Host: verisign.com[EOL]""
[DEBUG] wire - >> ""Proxy-Connection: Keep-Alive[EOL]""
[DEBUG] wire - >> ""User-Agent: Apache-HttpClient/UNAVAILABLE (java 1.5)[EOL]""
[DEBUG] wire - >> ""Proxy-Authorization: NTLM TlRMTVNTUAABAAAAATIAAAgACAAgAAAADgAOACgAAABNWURPTUFJTkpDSUZTMjMwXzg2Xzkx[EOL]""
[DEBUG] wire - >> ""[EOL]""
[DEBUG] wire - << ""HTTP/1.1 407 Proxy Authentication Required ( Access is denied.  )[EOL]""
[DEBUG] wire - << ""Via: 1.1 lab1[EOL]""
[DEBUG] wire - << ""Proxy-Authenticate: NTLM TlRMTVNTUAACAAAAAAAAADgAAAABAgACqbXrIWnZ3i4AAAAAAAAAAAAAAAA4AAAABQLODgAAAA8=[EOL]""
[DEBUG] wire - << ""Connection: Keep-Alive[EOL]""
[DEBUG] wire - << ""Proxy-Connection: Keep-Alive[EOL]""
[DEBUG] wire - << ""Pragma: no-cache[EOL]""
[DEBUG] wire - << ""Cache-Control: no-cache[EOL]""
[DEBUG] wire - << ""Content-Type: text/html[EOL]""
[DEBUG] wire - << ""Content-Length: 0     [EOL]""
[DEBUG] wire - << ""[EOL]""
[DEBUG] wire - >> ""GET http://verisign.com HTTP/1.1[EOL]""
[DEBUG] wire - >> ""Host: verisign.com[EOL]""
[DEBUG] wire - >> ""Proxy-Connection: Keep-Alive[EOL]""
[DEBUG] wire - >> ""User-Agent: Apache-HttpClient/UNAVAILABLE (java 1.5)[EOL]""
[DEBUG] wire - >> ""Proxy-Authorization: NTLM TlRMTVNTUAADAAAAGAAYAEAAAAAwADAAWAAAABAAEACIAAAAGgAaAJgAAAAcABwAsgAAAAAAAAAAAAAAAQIAAAXLpW40q7jqh7E6FgFnJqy9529ANaSLqfTiwjyF2BrUP9F8ObYOyYsBAQAAAAAAACDgxRg9+skBRt4mUOFFCs0AAAAAAAAAAE0AWQBEAE8ATQBBAEkATgBBAGQAbQBpAG4AaQBzAHQAcgBhAHQAbwByAEoAQwBJAEYAUwAyADMAMABfADgANgBfADkAMQA=[EOL]""
[DEBUG] wire - >> ""[EOL]""
[DEBUG] wire - << ""HTTP/1.1 301 Unknown reason[EOL]""
[DEBUG] wire - << ""Via: 1.1 lab1[EOL]""
[DEBUG] wire - << ""Connection: Keep-Alive[EOL]""
[DEBUG] wire - << ""Proxy-Connection: Keep-Alive[EOL]""
[DEBUG] wire - << ""Content-length: 0[EOL]""
[DEBUG] wire - << ""Date: Mon, 29 Jun 2009 11:16:50 GMT[EOL]""
[DEBUG] wire - << ""Location: http://www.verisign.com/[EOL]""
[DEBUG] wire - << ""Content-type: text/html[EOL]""
[DEBUG] wire - << ""Server: Netscape-Enterprise/4.1[EOL]""
[DEBUG] wire - << ""[EOL]""
[ERROR] RequestProxyAuthentication - Proxy authentication error: Unexpected state: MSG_TYPE3_GENERATED
[DEBUG] wire - >> ""GET http://www.verisign.com/ HTTP/1.1[EOL]""
[DEBUG] wire - >> ""Host: www.verisign.com[EOL]""
[DEBUG] wire - >> ""Proxy-Connection: Keep-Alive[EOL]""
[DEBUG] wire - >> ""User-Agent: Apache-HttpClient/UNAVAILABLE (java 1.5)[EOL]""
[DEBUG] wire - >> ""[EOL]""
[DEBUG] wire - << ""HTTP/1.1 407 Proxy Authentication Required ( The ISA Server requires authorization to fulfill the request. Access to the Web Proxy filter is denied.  )[EOL]""
[DEBUG] wire - << ""Via: 1.1 lab1[EOL]""
[DEBUG] wire - << ""Proxy-Authenticate: Negotiate[EOL]""
[DEBUG] wire - << ""Proxy-Authenticate: Kerberos[EOL]""
[DEBUG] wire - << ""Proxy-Authenticate: NTLM[EOL]""
[DEBUG] wire - << ""Proxy-Authenticate: Basic realm=""lab1.""[EOL]""
[DEBUG] wire - << ""Connection: Keep-Alive[EOL]""
[DEBUG] wire - << ""Proxy-Connection: Keep-Alive[EOL]""
[DEBUG] wire - << ""Pragma: no-cache[EOL]""
[DEBUG] wire - << ""Cache-Control: no-cache[EOL]""
[DEBUG] wire - << ""Content-Type: text/html[EOL]""
[DEBUG] wire - << ""Content-Length: 4107  [EOL]""
[DEBUG] wire - << ""[EOL]""
----------------------------------------
HTTP/1.1 407 Proxy Authentication Required ( The ISA Server requires authorization to fulfill the request. Access to the Web Proxy filter is denied.  )

Thanks,
Raj





"
1,"Impossible comparison in NodeTypeImplorg.apache.jackrabbit.jcr2spi.nodetype.NodeTypeImpl does

    public boolean isNodeType(Name nodeTypeName) {
        return getName().equals(nodeTypeName) ||  ent.includesNodeType(nodeTypeName);
    }


as getName() is a string and nodeTypeName is a Name this will always be false. Perhaps you meant

    public boolean isNodeType(Name nodeTypeName) {
        return getName().equals(nodeTypeName.getLocalName()) ||  ent.includesNodeType(nodeTypeName);
    }

"
0,Introduce QNodeTypeDefinition cache per userIdOnce read from the server a QNodeTypeDefinition can be cached and shared across SessionImpl with the same userId.
1,"FNFE hit when creating an empty index and infoStream is onShai just reported this on the dev list.  Simple test:
{code}
Directory dir = new RAMDirectory();
IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), MaxFieldLength.UNLIMITED);
writer.setInfoStream(System.out);
writer.addDocument(new Document());
writer.commit();
writer.close();
{code}

hits this:

{code}
Exception in thread ""main"" java.io.FileNotFoundException: _0.prx
    at org.apache.lucene.store.RAMDirectory.fileLength(RAMDirectory.java:149)
    at org.apache.lucene.index.DocumentsWriter.segmentSize(DocumentsWriter.java:1150)
    at org.apache.lucene.index.DocumentsWriter.flush(DocumentsWriter.java:587)
    at org.apache.lucene.index.IndexWriter.doFlushInternal(IndexWriter.java:3572)
    at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:3483)
    at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3474)
    at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1940)
    at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1894)
{code}

Turns out it's just silly -- this is actually an issue I've already fixed on the flex (LUCENE-1458) branch.  DocumentsWriter has its own method to enumerate the flushed files and compute their size, but really it shouldn't do that -- it should use SegmentInfo's method, instead."
1,Aggregate include ignored if no primaryType setIf the include element of an aggregate definition does not have a primaryType attribute then the include is never matched.
1,"Proxy authentication error: Unexpected state: MSG_TYPE3_GENERATED when using NTLM authenticationTrying to connect to a website that requires basic authentication through a proxy that requires NTLM authentication.

Proxy authentication fails with ""Proxy authentication error: Unexpected state: MSG_TYPE3_GENERATED"".

Full wire log attached.  Code to replicate problem follows:

    private void execute() throws HttpException, IOException {
    	
    	URL targetUrl = new URL(TARGET_URL);
    	
        DefaultHttpClient httpclient = new DefaultHttpClient();

        HttpHost targetHost = new HttpHost(targetUrl.getHost()); 
        HttpHost proxyHost = new HttpHost(PROXY_HOST, PROXY_PORT); 
        
        httpclient.getParams().setParameter(ConnRoutePNames.DEFAULT_PROXY, 
        		proxyHost);

        CredentialsProvider credProvider = httpclient.getCredentialsProvider();
        
        Credentials proxyCredentials = new NTCredentials(PROXY_USER, 
        		PROXY_PASSWORD, PROXY_MACHINE, PROXY_DOMAIN);
        AuthScope proxyAuthScope = new AuthScope(proxyHost.getHostName(),
        		proxyHost.getPort());
        
        credProvider.setCredentials(proxyAuthScope, proxyCredentials);
        
        Credentials targetCredentials = new UsernamePasswordCredentials(
        		TARGET_USER, TARGET_PASSWORD);
        AuthScope targetAuthScope = new AuthScope(targetHost.getHostName(),
        		targetHost.getPort());
        
        credProvider.setCredentials(targetAuthScope, targetCredentials);
      
        HttpGet httpget = new HttpGet(targetUrl.getPath());

        HttpResponse response = httpclient.execute(targetHost, httpget);
        
        System.out.println(""response = "" + response);
        
       
    }
"
0,"RepositoryConfig instance can not be reused once it has been passed to RepositoryImpl constructorRepositoryConfig and other *Config classes maintain state apart from parsed configuration information;
specifically they instantiate FileSystem implementations based on their configurations. this makes it
for the config consumers very hard to control the lifecycle of such FileSystem instances as they need
to close the file systems on repository shutdown.

the following code illustrates the issue:

RepositoryConfig repConf = RepositoryConfig.create(configFile, repHomeDir);
RepositoryImpl rep = RepositoryImpl.create(repConf);
// ...
rep.shutdown();

rep = RepositoryImpl.create(repConf);   
// ==> repConfig (et al) contains references to FileSystem objects 
// that have been closed by previous rep.shutdown() call

"
0,NodeImpl.checkin() calls save() three timesThe version related properties on a versionable node that is checked in are saved individually. There is no need to save them individually because such a node must not have pending changes and save() can be called safely on the node itself.
0,"Add method to remove mappings from NamespaceMappingo.a.j.spi.commons.namespace.NamespaceMapping has currently no means to remove a mapping. I suggest to add a method
public String removeMapping(String uri) "
0,"Node.addNode() does not scale with increasing contentWith increasing repository content (and versions), the time to create new nodes increases. For example with around 6500 nodes and 33500 properties, it takes around 3 seconds (!) to just add one single node !

When attaching to the application with a Debugger and delibaretly suspending the VM, this stack trace is displayed all the times :

   [ changing internals of access List iterator ]
   PersistentNodeState(NodeState).getChildNodeEntries(String) line: 362
   PersistentNode.getName() line: 84
   PersistentVersionManager.getVersion(String) line: 278
   VersionManager.getVersion(String) line: 304
   VersionItemStateProvider.getNodeState(NodeId) line: 124
   VersionItemStateProvider.hasPropertyState(PropertyId) line: 154
   VersionItemStateProvider.hasItemState(ItemId) line: 174
   SessionItemStateManager.getItemState(ItemId) line: 246
   ItemManager.createItemInstance(ItemId) line: 563
   ItemManager.getItem(ItemId) line: 332
   NodeImpl.getProperty(QName) line: 876
   NodeImpl.hasProperty(QName) line: 893
   NodeImpl.safeIsCheckedOut() line: 2515
   NodeImpl.internalAddChildNode(QName, NodeTypeImpl, String) line: 527
   NodeImpl.internalAddNode(String, NodeTypeImpl, String) line: 475
   NodeImpl.internalAddNode(String, NodeTypeImpl) line: 436
   NodeImpl.addNode(String, String) line: 1145
   ...

It seems, that virtual item state providers are asked for whatever property is looked for and this in return calls into the version handler, which loops over some child entries (currently around 1100 entries) to find one single entry with a given UUID.

Besides the latter not being optimal and certainly not scaling, the former has its problems in its own right."
0,"NodeTypeRegistry could auto-subtype from nt:basewhen tying to register a (primary) nodetype that does not extend from nt:base the following error is
thrown:

""all primary node types except nt:base itself must be (directly or indirectly) derived from nt:base""

since the registry is able to detect this error, it would be easy to auto-subtype all nodetypes from nt:base. imo it's pointless to explzitely add the nt:base to every supperclass set. as an analogy, you don't need to 'extend from java.lang.Object' explicitely - the compiler does that automatically for your."
0,"extend security config -> repository-1.5.dtdalong with issue #JCR-1171 i'd like to extend the configuration. 

this requires a new version (repository-1.5.dtd) of the repository dtd to be present in jackrabbit-site/src/site/resources/dtd in order to have the new versions of repository.xml work properly.

see attached diff that shows the difference to the most recent repository-1.4.dtd

if nobody objects i would put the proposed repository-1.5.dtd to the site.
once this is done i can properly adjust the repository.xml files (uncommenting the DOCTYPE tag) and start committing the new security functionality.

angela"
0,"Replace commons-logging dependency with SLF4JThe poi dependency in jackrabbit-text-extractors brings in a transitive dependency to commons-logging. Since we use SLF4J for all logging, we should exclude the commons-logging dependency and replace it with jcl104-over-slf4j."
0,"Log creation impairs performanceRunning JProfiler on a program that uses HttpClient with a ThreadSafeClientConnManager, revealed that 5% of the time was spent constructing Log instances in class ClientParamsStack.

Oleg did some further investigation and found that DefaultRequestDirector also has the same problem.

A simple solution would be to make the Log a static member variable, and do this on all classes for consistency.  However this might not be the best solution for interoperating with some frameworks (see http://wiki.apache.org/jakarta-commons/Logging/StaticLog)

Another solution would be to simply remove the Log from the affected classes, although they are presumably there for a reason...
"
0,"Callback for intercepting merging segments in IndexWriterFor things like merging field caches or bitsets, it's useful to
know which segments were merged to create a new segment.

"
0,"Change AttributeSource API to use genericsThe AttributeSource API will be easier to use with JDK 1.5 generics.

Uwe, if you started working on a patch for this already feel free to assign this to you."
1,"spi2dav: EventFilters not respectedi have the impression that the event filter passed to the event subscription in spi2dav is not (or not properly) respected.

marcel, is there a specific reason that you always pass the static SubscriptionInfo constant (no node type filter, noLocal false) to the SubscribeMethod
in spi2dav/RepositoryServiceImpl#createSubscription ?

i guess this is the reason for the failure of
  testNodeType(org.apache.jackrabbit.test.api.observation.AddEventListenerTest)
  testNoLocalTrue(org.apache.jackrabbit.test.api.observation.AddEventListenerTest)
"
0,"ItemInfoBuilder should not include PropertyInfos in ChildInfosWhen building a NodeInfo instance using the ItemInfoBuilder, getChildInfos() returns entries for the PopertyInfos. This is wrong: getChildInfos should only include entries for the child nodes. "
1,"FileDataStore garbage collection can throw a NullPointerException if there is I/O problemThe FileDataStore can throw a NPE when doing garbage collection, if there is file I/O problem (for example an access rights problem). The reason is that it doesn't check if File.list / listFiles returns null. Stack trace:

Exception in thread ""Thread-461"" java.lang.NullPointerException
	at org.apache.jackrabbit.core.data.FileDataStore.deleteOlderRecursive(FileDataStore.java:334)
	at org.apache.jackrabbit.core.data.FileDataStore.deleteOlderRecursive(FileDataStore.java:328)
	at org.apache.jackrabbit.core.data.FileDataStore.deleteOlderRecursive(FileDataStore.java:328)
	at org.apache.jackrabbit.core.data.FileDataStore.deleteOlderRecursive(FileDataStore.java:328)

"
0,"MultiReader.norm() takes up too much memory: norms byte[] should be made into an ObjectMultiReader.norms() is very inefficient: it has to construct a byte array that's as long as all the documents in every segment.  This doubles the memory requirement for scoring MultiReaders vs. Segment Readers.  Although this is cached, it's still a baseline of memory that is unnecessary.

The problem is that the Normalization Factors are passed around as a byte[].  If it were instead replaced with an Object, you could perform a whole host of optimizations
a.  When reading, you wouldn't have to construct a ""fakeNorms"" array of all 1.0fs.  You could instead return a singleton object that would just return 1.0f.
b.  MultiReader could use an object that could delegate to NormFactors of the subreaders
c.  You could write an implementation that could use mmap to access the norm factors.  Or if the index isn't long lived, you could use an implementation that reads directly from the disk.

The patch provided here replaces the use of byte[] with a new abstract class called NormFactors.  
NormFactors has two methods on it
    public abstract byte getByte(int doc) throws IOException;  // Returns the byte[doc]
    public float getFactor(int doc) throws IOException;            // Calls Similarity.decodeNorm(getByte(doc))

There are four implementations of this abstract class
1.  NormFactors.EmptyNormFactors - This replaces the fakeNorms with a singleton that only returns 1.0
2.  NormFactors.ByteNormFactors - Converts a byte[] to a NormFactors for backwards compatibility in constructors.
3.  MultiNormFactors - Multiplexes the NormFactors in MultiReader to prevent the need to construct the gigantic norms array.
4.  SegmentReader.Norm - Same class, but now extends NormFactors to provide the same access.

In addition, Many of the Query and Scorer classes were changes to pass around NormFactors instead of byte[], and to call getFactor() instead of using the byte[].  I have kept around IndexReader.norms(String) for backwards compatibiltiy, but marked it as deprecated.  I believe that the use of ByteNormFactors in IndexReader.getNormFactors() will keep backward compatibility with other IndexReader implementations, but I don't know how to test that.
"
0,"Flush volatile index when size limit is reachedCurrently the volatile index is committed when minMergeDocs is reached. This is inconvenient because it does not take the size of nodes into account account. When lots of small nodes are added the volatile index should be committed less frequently. Similarly when nodes with lots of properties are indexed the volatile index should be committed more frequently.

Instead the size of the volatile index in bytes should trigger a disk write."
0,"Define clear semantics for Directory.fileLengthOn this thread: http://mail-archives.apache.org/mod_mbox/lucene-java-dev/201003.mbox/%3C126142c1003121525v24499625u1589bbef4c0792e7@mail.gmail.com%3E it was mentioned that Directory's fileLength behavior is not consistent between Directory implementations if the given file name does not exist. FSDirectory returns a 0 length while RAMDirectory throws FNFE.

The problem is that the semantics of fileLength() are not defined. As proposed in the thread, we'll define the following semantics:

* Returns the length of the file denoted by <code>name</code> if the file exists. The return value may be anything between 0 and Long.MAX_VALUE.
* Throws FileNotFoundException if the file does not exist. Note that you can call dir.fileExists(name) if you are not sure whether the file exists or not.

For backwards we'll create a new method w/ clear semantics. Something like:

{code}
/**
 * @deprecated the method will become abstract when #fileLength(name) has been removed.
 */
public long getFileLength(String name) throws IOException {
  long len = fileLength(name);
  if (len == 0 && !fileExists(name)) {
    throw new FileNotFoundException(name);
  }
  return len;
}
{code}

The first line just calls the current impl. If it throws exception for a non-existing file, we're ok. The second line verifies whether a 0 length is for an existing file or not and throws an exception appropriately."
0,Add memory based bundle store
0,"Change all FilteredTermsEnum impls into TermsEnum decoratorsCurrently, FilteredTermsEnum has two ctors:
* FilteredTermsEnum(IndexReader reader, String field)
* FilteredTermsEnum(TermsEnum tenum)

But most of our concrete implementations (e.g. TermsRangeEnum) use the IndexReader+field ctor

In my opinion we should remove this ctor, and switch over all FilteredTermsEnum implementations to just take a TermsEnum.

Advantages:
* This simplifies FilteredTermsEnum and its subclasses, where they are more decorator-like (perhaps in the future we could compose them)
* Removes silly checks such as if (tenum == null) in every next()
* Allows for consumers to pass in enumerators however they want: e.g. its their responsibility if they want to use MultiFields or not, it shouldnt be buried in FilteredTermsEnum.

I created a quick patch (all core+contrib+solr tests pass), but i think this opens up more possibilities for refactoring improvements that haven't yet been done in the patch: we should explore these too.
"
0,"Line-separator differences cause PredefinedNodeTypeTest to fail on different operating systems.In testPredefinedNodeType(), the test reads in a test file from the file system and then performs a string comparison, which may fail due to line-separator differences:

    private void testPredefinedNodeType(String name)
            throws NotExecutableException {
        try {
            StringBuffer spec = new StringBuffer();
            String resource =
                ""org/apache/jackrabbit/test/api/nodetype/spec/""
                + name.replace(':', '-') + "".txt"";
            Reader reader = new InputStreamReader(
                    getClass().getClassLoader().getResourceAsStream(resource));
            for (int ch = reader.read(); ch != -1; ch = reader.read()) {
                spec.append((char) ch);
            }

            NodeType type = manager.getNodeType(name);

            assertEquals(
                    ""Predefined node type "" + name,
                    spec.toString(),
                    getNodeTypeSpec(type));
...

The above works when the file being read in has line-separators that match the operating system the test is being run on.  However, if there is a mismatch, the string comparison will fail.

The fix is to replace line-separators in both strings being compared:

Helper method to replace line separators

    /** Standardize line separators around ""\n"". */
    public String replaceLineSeparators(String stringValue) {
        // Replace ""\r\n"" (Windows format) with ""\n"" (Unix format) 
        stringValue = stringValue.replaceAll(""\r\n"", ""\n"");
        // Replace ""\r"" (Mac format) with ""\n"" (Unix format)
        stringValue = stringValue.replaceAll(""\r"", ""\n"");
        
        return stringValue;
    }
    
Updated test method:

    private void testPredefinedNodeType(String name)
            throws NotExecutableException {
        try {
            StringBuffer spec = new StringBuffer();
            String resource =
                ""org/apache/jackrabbit/test/api/nodetype/spec/""
                + name.replace(':', '-') + "".txt"";
            Reader reader = new InputStreamReader(
                    getClass().getClassLoader().getResourceAsStream(resource));
            for (int ch = reader.read(); ch != -1; ch = reader.read()) {
                spec.append((char) ch);
            }

            NodeType type = manager.getNodeType(name);
            
            String nodeTypeSpecValue = replaceLineSeparators(getNodeTypeSpec(type));
            String specValue = replaceLineSeparators(spec.toString());
            
            assertEquals(
                    ""Predefined node type "" + name,
                    specValue,
                    nodeTypeSpecValue);
..."
0,"unicode escapes in files generated by JJTreeMaven build fails on windows machines if sources are located in a directory starting with a 'u'. This is because files created by JJTree (javacc) put filename and path in a comment of the generated files, like this:

/*@bgen(jjtree) Generated By:JJTree: Do not edit this line. D:\usr\projects\workspace\jackrabbit\target\generated-src\main\org\apache\jackrabbit\core\query\sql\JCRSQL.jj */

The \u in interpreted as an escape characted and so you get a 
""BUILD FAILED ... Invalid escape character""
"
0,"Improper deprecation of Locked classThe Locked class in the jcr-commons package has been deprecated with 1.4 and moved to the spi-commons.
However as this is a common class which does not depend on the spi, it should rather stay in jcr-commons.
The dependencies to spi can simply be removed again."
0,"Position based TermVectorMapperAs part of the new TermVectorMapper approach to TermVectors, the ensuing patch loads term vectors and stores the term info by position.  This should let people directly index into a term vector given a position.  Actually, it does it through Maps, b/c the array based bookkeeping is a pain given the way positions are stored.  

The map looks like:
Map<String,   Map<Integer, TVPositionInfo>>

where the String is the field name, the integer is the position, and TVPositionInfo is a storage mechanism for the terms and offsets that occur at a position.  It _should_ handle multiple terms per position (which is always my downfall! )

I have not tested performance of this approach.
"
1,"jcr2spi: Item.isSame may return wrong result if any ancestor is invalidatedjulian detected an issue with jcr2spi that was previously shadowed due to heavy reloading of items upon save.
with the most recent changes however reloading of items is postponed until the next access. this will cause the following test to fail:

        Node n = testRootNode.addNode(""aFile"", ""nt:file"");
        n = n.addNode(""jcr:content"", ""nt:resource"");
        n.setProperty(""jcr:lastModified"", Calendar.getInstance());
        n.setProperty(""jcr:mimeType"", ""text/plain"");
        Property jcrData = n.setProperty(""jcr:data"", ""abc"", PropertyType.BINARY);
        testRootNode.save();

        // access same property through different session
        Session otherSession = helper.getReadOnlySession();
        try {
            Property otherProperty = (Property) otherSession.getItem(jcrData.getPath());
            assertTrue(jcrData.isSame(otherProperty));
        } finally {
            otherSession.logout();
        }

while 
     
       assertTrue(n.isSame(otherSession.getItem(n.getPath()));

would be successful.

the reason: the jcrData property is not reloaded and it's parent is still _invalidated_. consequently the property isn't aware of it's id having changed due to the fact that nt:resource is a node type extending from mix:referenceable.

possible fixes:

1) mark all items _invalid_ after save 
    instead of setting status non-protected/autocreated properties to EXISTING.
    -> forcing jcrData to be reloaded before isSame can be called.
    -> drawback: much more round trip(s) to the server just to make sure the id is up to date.

2) change Item#isSame to make sure the workspaceId is up to date (walking up the
     hierarchy and force reloading of the first invalidated ancestor).
     -> drawback: if referenceable nodes are rare or missing at all, this causes some
          extra round trips.
 
3) change Item.isSame to compare the 'workspacePath' instead of the 'workspaceId'.
     -> drawback: upon persisted move of a referenceable node Item#isSame will return false


after taking a closer look at the code and at some additional tests i would opt for 2).
"
0,"DefaultItemStateProvider contains grow-only cacheThe DefaultItemStateProvider class contains a private HashMap ""items"" which contains references to ItemState objects. The bad thing about this cache is, that it only grows, but is not being managed to forget about ""unused"" items.

Example: A repository which is filled with 9350 nodes and 52813 properties grows this items map to 1'667'557 (!) entries. In this concrete case, the VM all13ates 213MB to the heap of which 57MB is referenced by the DefaultItemStateProvider.items map."
0,"nuke/clean up AtomicReader.hasNormsimplementations already have to return fieldInfos() [which can tell you this], and normValues() [which can also tell you this].

So if we want to keep it, I think it should just have a final implementation and not be required for FilterReaders, etc.

Or we can just nuke it... do we really need 3 ways to do the same thing?"
0,cutover oal.index.* tests to use a random IWC to tease out bugs
1,"TestNRTThreads hangs in nightly 3.x buildsMaybe we have a problem, maybe its a bug in the test.

But its strange that lately the 3.x nightlies have been hanging here."
1,"PerFieldCodecWrapper.loadTermsIndex concurrency problemSelckin's while(1) testing on RT branch hit another error:
{noformat}
    [junit] Testsuite: org.apache.lucene.TestExternalCodecs
    [junit] Testcase: testPerFieldCodec(org.apache.lucene.TestExternalCodecs):	Caused an ERROR
    [junit] (null)
    [junit] java.lang.NullPointerException
    [junit] 	at org.apache.lucene.index.PerFieldCodecWrapper$FieldsReader.loadTermsIndex(PerFieldCodecWrapper.java:202)
    [junit] 	at org.apache.lucene.index.SegmentReader.loadTermsIndex(SegmentReader.java:1005)
    [junit] 	at org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:652)
    [junit] 	at org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:609)
    [junit] 	at org.apache.lucene.index.BufferedDeletesStream.applyDeletes(BufferedDeletesStream.java:276)
    [junit] 	at org.apache.lucene.index.IndexWriter.applyAllDeletes(IndexWriter.java:2660)
    [junit] 	at org.apache.lucene.index.IndexWriter.maybeApplyDeletes(IndexWriter.java:2651)
    [junit] 	at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:381)
    [junit] 	at org.apache.lucene.index.IndexReader.open(IndexReader.java:316)
    [junit] 	at org.apache.lucene.TestExternalCodecs.testPerFieldCodec(TestExternalCodecs.java:541)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1246)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1175)
    [junit] 
    [junit] 
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.909 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestExternalCodecs -Dtestmethod=testPerFieldCodec -Dtests.seed=-7296204858082494534:5010909751437000758
    [junit] WARNING: test method: 'testPerFieldCodec' left thread running: merge thread: _i(4.0):Cv130 _m(4.0):Cv30 _n(4.0):cv10 into _o
    [junit] RESOURCE LEAK: test method: 'testPerFieldCodec' left 1 thread(s) running
    [junit] NOTE: test params are: codec=PreFlex, locale=zh_TW, timezone=America/Santo_Domingo
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestDemo, TestExternalCodecs]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=2,free=104153512,total=125632512
    [junit] ------------- ---------------- ---------------
    [junit] TEST org.apache.lucene.TestExternalCodecs FAILED
    [junit] Exception in thread ""Lucene Merge Thread #5"" org.apache.lucene.util.ThreadInterruptedException: java.lang.InterruptedException: sleep interrupted
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:505)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:472)
    [junit] Caused by: java.lang.InterruptedException: sleep interrupted
    [junit] 	at java.lang.Thread.sleep(Native Method)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:503)
    [junit] 	... 1 more
{noformat}

I suspect this is also a trunk issue, but I can't reproduce it yet.

I think this is happening because the codecs HashMap is changing (via another thread), while .loadTermsIndex is called."
0,"Hardening of NativeFSLockNativeFSLock create a test lock file which its name might collide w/ another JVM that is running. Very unlikely, but still it happened a couple of times already, since the tests were parallelized. This may result in a false exception thrown from release(), when the lock file's delete() is called and returns false, because the file does not exist (deleted by another JVM already). In addition, release() should give a second attempt to delete() if it fails, since the file may be held temporarily by another process (like AntiVirus) before it fails. The proposed changes are:

1) Use ManagementFactory.getRuntimeMXBean().getName() as part of the test lock name (should include the process Id)
2) In release(), if delete() fails, check if the file indeed exists. If it is, let's attempt a re-delete() few ms later.
3) If (3) still fails, throw an exception. Alternatively, we can attempt a deleteOnExit.

I'll post a patch later today."
0,"CompoundFileReader's openInput produces streams that may do an extra buffer copySpinoff of LUCENE-888.

The class for reading from a compound file (CompoundFileReader) has a
primary stream which is a BufferedIndexInput when that stream is from
an FSDirectory (which is the norm).  That is one layer of buffering.

Then, when its openInput is called, a CSIndexInput is created which
also subclasses from BufferedIndexInput.  That's a second layer of
buffering.

When a consumer actually uses that CSIndexInput to read, and a call to
readByte or readBytes runs out of what's in the first buffer, it will
go to refill its buffer.  But that refill calls the first
BufferedIndexInput which in turn may refill its buffer (a double
copy) by reading the underlying stream.

Not sure how to fix it yet but we should change things to not do the
extra buffer copy.
"
1,"DbDataStore connection does not always reconnectIf a DbDataStore connection is closed due to an error all subsequent addRecord calls will fail with 'connection has been closed and autoReconnect == false'
 after getRecord is called and the connection is reconnected addRecord will succeed.

the connection should be validated before setting autoReconnect = false or on retrieval from the pool."
1,"intermittant exceptions in TestConcurrentMergeScheduler
The TestConcurrentMergeScheduler throws intermittant exceptions that
do not result in a test failure.

The exception happens in the ""testNoWaitClose()"" test, which repeated
tests closing an IndexWriter with ""false"", meaning abort any
still-running merges.  When a merge is aborted it can hit various
exceptions because the files it is reading and/or writing have been
deleted, so we ignore these exceptions.

The bug was just that we were failing to properly check whether the
running merge was actually aborted because of a scoping issue of the
""merge"" variable in ConcurrentMergeScheduler.  So the exceptions are
actually ""harmless"".  Thanks to Ning for spotting it!

"
1,"SpellChecker min score is increased by timeThe minimum score, an instance variable, is modified in a search. That is wrong, since it makes it 1. thread unsafe and 2. not working. 

Lucky enought it is only used from the one and same method call, so I simply compied the instance variable to a local method variable.

        float min = this.min; 
"
0,"Error in FSDirectory if java.io.tmpdir incorrectly specifiedA user of the JAMWiki project (http://jamwiki.org/) reported an error with the following stack trace:

SEVERE: Unable to create search instance /usr/share/tomcat5/webapps/jamwiki-0.3.4-beta7/test/base/search/indexen
java.io.IOException: Cannot create directory: /temp
        at org.apache.lucene.store.FSDirectory.init(FSDirectory.java:171)
        at org.apache.lucene.store.FSDirectory.getDirectory(FSDirectory.java:141)
        at org.apache.lucene.store.FSDirectory.getDirectory(FSDirectory.java:117)
        at org.jamwiki.search.LuceneSearchEngine.getSearchIndexPath(LuceneSearchEngine.java:318)

The culprit is that the java.io.tmpdir property was incorrectly specified on the user's system.  Lucene could easily handle this issue by modifying the FSDirectory.init() method.  Currently the code uses the index directory if java.io.tmpdir and org.apache.lucene.lockDir are unspecified, but it could use that directory if those values are unspecified OR if they are invalid.  Doing so would make Lucene a bit more robust without breaking any existing installations.
"
1,"Problem with redirect on HEAD when (bad, naughty) server returns body contentI've been testing/using HttpClient 2.0a3 with Resin 2.1.9. I've found that when
using a HEAD request on a JSP, Resin returns the body content along with the
headers.

In this case, something in the HttpClient breaks. Looking at the httpclient
logs, it looks like:

1) HttpClient does a HEAD against the original URL
2) Resin returns valid status line and headers
3) HttpClient parses the headers and recognizes the redirect header
4) HttpClient does a HEAD against the new URL (from the Location header)
5) HttpMethodBase calls readStatusLine, which (eventually) calles readRawLine in
HttpConnection (which reads from the internal inputStream)
6) readRawLine returns the first line in the body from the original HEAD request
in (1).

It looks like the original body content (in response to the first HEAD) is being
buffered somewhere, but I can't figure out where.

I know that this is invalid behavior on the server's part, but I would like to
be able to recover from it.



---- redir_test.jsp ----
<?xml version=""1.0""?>
<% 
  response.setStatus(response.SC_MOVED_TEMPORARILY);
  response.setHeader(""Location"", ""redirect_pass.xml"");
%>
<some>
  <dummy>
    <data attr=""yea, well""/>
  </dummy>
</some>"
1,"NTLM Authentication FailsNTLM Authentication requires multiple request/responses for the authentication
to succeed.  Since HttpMethodBase is now using just the host, port and realm to
identify whether or not authentication has been attempted the second pass for
NTLM authentication is never performed."
1,Deleted documents are visible across reopened MSRs
1,"NoSuchItemStateException on checkin after removeVersion in XA EnvironmentAfter removing a version, a checkin on the same node in a different transaction (with a different session) fails.
The NoSuchItemStateException refer to the uuid of the previously removed version. 
I'll attach a test demonstrating the problem. "
0,"Sort and SortField does not have equals() and hashCode()During developing for my project panFMP I had the following issue:
I have a cache for queries (like Solr has, too)  for query results. This cache also uses the Sort/SortField as key into the cache. The problem is, because Sort/SortField does not implement equals() and hashCode(), you cannot store them as cache keys. To workaround, currently I use Sort.toString() as cache key, but this is not so nice.

In corelation with issue LUCENE-1478, I could fix this there in one patch together with the other improvements."
1,"BindableRepositoryFactory doesn't handle repository shutdownThe BindableRepositoryFactory class keeps a cached reference to a repository even after the repository has been shut down.

This causes the following code snippet to fail with an IllegalStateException:

        Hashtable environment = new Hashtable();
        environment.put(
                Context.INITIAL_CONTEXT_FACTORY,
                DummyInitialContextFactory.class.getName());
        environment.put(Context.PROVIDER_URL, ""http://jackrabbit.apache.org/"");
        Context context = new InitialContext(environment);

        JackrabbitRepository repository;
        String xml = ""src/test/repository/repository.xml"";
        String dir = ""target/repository"";
        String key = ""repository"";

        // Create first repository
        RegistryHelper.registerRepository(context, key, xml, dir, true);
        repository = (JackrabbitRepository) context.lookup(key);
        repository.login().logout();
        repository.shutdown();

        // Create second repository with the same configuration
        RegistryHelper.registerRepository(context, key, xml, dir, true);
        repository = (JackrabbitRepository) context.lookup(key);
        repository.login().logout(); // throws an IllegalStateException!
        repository.shutdown();
"
0,"test case (TCK) maintenance for JCR 2.0Umbrella issue for changes/additions to JUnit test cases, setup and config."
1,"Internal error in WorkspaceItemStateFactory#createDeepNodeState When WorkspaceItemStateFactory#createDeepNodeState receives the current entry as argument for anyParent, it throws RepositoryException with the message ""Internal error while getting deep itemState"". This is incorrect (probably a leftover from JCR-1797) since any entry is valid as argument for anyParent. "
0,"Add User#changePassword(String newPw, String oldPw)... where the oldPw must match in order to have the password of the user successfully changed.

while this could be done by applications with quite some effort, the implementation can easily achieve this
as the functionality required is already present."
0,"Missing jackrabbit-rmi-service.xml from jackrabbit-jcr-rmi-1.2.1.jarThe file jackrabbit-rmi-service.xml is missing from the jackrabbit-jcr-rmi-1.2.1.jar.

The cause of the issue appears that the directory structure of the jackrabbit-jcr-rmi sub-project doesn't match the Maven 2 standard.  

To fix: src/resources should be moved to src/main/resources."
0,"Text extractor classes are obsolete in webText extractor classes are obsolete in http://jackrabbit.apache.org/doc/components/index-filters.html

""org.apache.jackrabbit.core.query"" are actually ""org.apache.jackrabbit.extractor""

Plain text extractor continue being ""org.apache.jackrabbit.core.query.lucene.TextPlainTextFilter""? "
0,"Improve reusability of AbstractRepositoryService and AbstractReadableRepositoryServiceMuch of the functionality in AbstractReadableRepositoryService is not specific to reading but rather applies to any implementation (node types, name spaces, descriptors). I suggest to pull this functionality up from AbstractReadableRepositoryService to AbstractRepositoryService"
0,"Make Token.DEFAULT_TYPE publicMake Token.DEFAULT_TYPE public so that TokenFilters using the reusable Token model have a way of setting the type back to the default.

No patch necessary.  I will commit soon."
1,"SQL Parser fails with SQL 92 timestamp formatThe SQL query parser fails with an exception if the SQL 92 timestamp format is used.

E.g:
... WHERE my:date > TIMESTAMP '1976-01-01 00:00:00.000+01:00'

does not work, but the following will succeed using ISO8601:

... WHERE my:date > TIMESTAMP '1976-01-01T00:00:00.000+01:00'"
1,"Automatic type conversion no longer worksString values are no longer converted to binary when required. Example:

Node n = testRootNode.addNode(""testConvert"", ""nt:file"");
Node content = n.addNode(""jcr:content"", ""nt:resource"");
content.setProperty(""jcr:lastModified"", Calendar.getInstance());
content.setProperty(""jcr:mimeType"", ""text/html"");
content.setProperty(""jcr:data"", ""Hello"");
n.getSession().save();

This used to work in a previous 2.0 build, but now throws:

javax.jcr.nodetype.ConstraintViolationException: no matching property definition found for {http://www.jcp.org/jcr/1.0}data
at org.apache.jackrabbit.core.nodetype.EffectiveNodeType.getApplicablePropertyDef(EffectiveNodeType.java:782)
at org.apache.jackrabbit.core.NodeImpl.getApplicablePropertyDefinition(NodeImpl.java:747)
at org.apache.jackrabbit.core.ItemManager.getDefinition(ItemManager.java:241)
at org.apache.jackrabbit.core.ItemData.getDefinition(ItemData.java:101)
at org.apache.jackrabbit.core.ItemManager.canRead(ItemManager.java:409)
at org.apache.jackrabbit.core.ItemManager.getItemData(ItemManager.java:383)
at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:316)
at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:595)
at org.apache.jackrabbit.core.NodeImpl.removeChildProperty(NodeImpl.java:554)
at org.apache.jackrabbit.core.NodeImpl.removeChildProperty(NodeImpl.java:534)
at org.apache.jackrabbit.core.NodeImpl.setProperty(NodeImpl.java:2303)
at org.apache.jackrabbit.core.nodetype.ConvertDataTypeTest.testStringToBinary(ConvertDataTypeTest.java:36)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)"
0,"Move ocm documentation to jackrabbit-siteThe OCM documentation from jackrabbit-ocm/xdocs should be moved to jackrabbit-site.

Also all old references to Graffito should be replaced with Jackrabbit."
1,"IndexWriter.addIndexes(IndexReader[] readers) doesn't correctly handle exception success flag.After this bit of code in addIndexes(IndexReader[] readers)

 try {
        flush(true, false, true);
        optimize();					  // start with zero or 1 seg
        success = true;
      } finally {
        // Take care to release the write lock if we hit an
        // exception before starting the transaction
        if (!success)
          releaseWrite();
      }

The success flag should be reset to ""false"" because it's used again in another try/catch/finally block.  

TestIndexWriter.testAddIndexOnDiskFull() sometimes will hit this bug; but it's infrequent.


"
1,"importXML prepending line feeds to tag valuesImporting using Session.importXML(...) results in new line characters being inserted at the beginning of tag
values:

<?xml version=""1.0"" encoding=""UTF-8""?>
<Policy xmlns=""urn:oasis:names:tc:xacml:1.0:policy""  xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" PolicyId=""test:policy-one"" RuleCombiningAlgId=""urn:oasis:names:tc:xacml:1.0:rule-combining-algorithm:ordered-permit-overrides"">
  <Description>policy-description</Description>
  <Target>
...

Becomes

/test/policies/Policy/jcr:primaryType=nt:unstructured
/test/policies/Policy/PolicyId=test:policy-one
/test/policies/Policy/RuleCombiningAlgId=urn:oasis:names:tc:xacml:1.0:rule-combining-algorithm:ordered-permit-overrides
/test/policies/Policy/Description/jcr:primaryType=nt:unstructured
/test/policies/Policy/Description/jcr:xmltext/jcr:primaryType=nt:unstructured
/test/policies/Policy/Description/jcr:xmltext/jcr:xmlcharacters=
policy-description
/test/policies/Policy/Target/jcr:primaryType=nt:unstructured

(in other cases, many LFs are inserted)

FULL EXAMPLE XML FILE:

<?xml version=""1.0"" encoding=""UTF-8""?>
<Policy xmlns=""urn:oasis:names:tc:xacml:1.0:policy"" 
  xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" 
  PolicyId=""test:policy-one"" 
  RuleCombiningAlgId=""urn:oasis:names:tc:xacml:1.0:rule-combining-algorithm:ordered-permit-overrides"">
  <Description>policy-description</Description>
  <Target>
    <Resources>
      <Resource>
        <ResourceMatch 
          MatchId=""urn:oasis:names:tc:xacml:1.0:function:string-equal"">
          <AttributeValue DataType=""http://www.w3.org/2001/XMLSchema#string"">test/12345-resource-67890</AttributeValue>
          <ResourceAttributeDesignator 
            DataType=""http://www.w3.org/2001/XMLSchema#string"" 
            AttributeId=""urn:oasis:names:tc:xacml:1.0:resource:resource-id""/>
        </ResourceMatch>
      </Resource>
    </Resources>
    <Actions>
      <AnyAction/>
    </Actions>
  </Target>
  <Rule RuleId=""PermitRule"" Effect=""Permit"">
    <Target>
      <Subjects>
        <Subject>
          <SubjectMatch 
            MatchId=""urn:oasis:names:tc:xacml:1.0:function:string-equal"">
            <AttributeValue DataType=""http://www.w3.org/2001/XMLSchema#string"">alice</AttributeValue>
            <SubjectAttributeDesignator 
              DataType=""http://www.w3.org/2001/XMLSchema#string"" 
              AttributeId=""urn:oasis:names:tc:xacml:1.0:subject:subject-id""/>
          </SubjectMatch>
        </Subject>
      </Subjects>
      <Actions>
        <Action>
          <ActionMatch 
            MatchId=""urn:oasis:names:tc:xacml:1.0:function:string-equal"">
            <AttributeValue DataType=""http://www.w3.org/2001/XMLSchema#string"">read</AttributeValue>
            <ActionAttributeDesignator 
              DataType=""http://www.w3.org/2001/XMLSchema#string"" 
              AttributeId=""urn:oasis:names:tc:xacml:1.0:action:action-id""/>
          </ActionMatch>
        </Action>
        <Action>
          <ActionMatch 
            MatchId=""urn:oasis:names:tc:xacml:1.0:function:string-equal"">
            <AttributeValue DataType=""http://www.w3.org/2001/XMLSchema#string"">write</AttributeValue>
            <ActionAttributeDesignator 
              DataType=""http://www.w3.org/2001/XMLSchema#string"" 
              AttributeId=""urn:oasis:names:tc:xacml:1.0:action:action-id""/>
          </ActionMatch>
        </Action>
        <Action>
          <ActionMatch 
            MatchId=""urn:oasis:names:tc:xacml:1.0:function:string-equal"">
            <AttributeValue DataType=""http://www.w3.org/2001/XMLSchema#string"">delete</AttributeValue>
            <ActionAttributeDesignator 
              DataType=""http://www.w3.org/2001/XMLSchema#string"" AttributeId=""urn:oasis:names:tc:xacml:1.0:action:action-id""/>
          </ActionMatch>
        </Action>
      </Actions>
    </Target>
  </Rule>
</Policy>"
0,Child axis support for XPath predicatesIt seems that Jackrabbit currently only supports the attribute axis in XPath predicates. Support for the child axis would be a nice addition.
0,"Add sort-by-term with DocValuesThere are two sorted byte[] types with DocValues (BYTES_VAR_SORTED,
BYTES_FIXED_SORTED), so you can index this type, but you can't yet
sort by it.

So I added a FieldComparator just like TermOrdValComparator, except it
pulls from the doc values instead.

There are some small diffs, eg with doc values there are never null
values (see LUCENE-3504).
"
1,"org.apache.lucene.ant.HtmlDocument creates a FileInputStream in its constructor that it doesn't closeA look through the jtidy source code doesn't show a close that i can find in parse (seems to be standard that you close your own streams anyway), so this looks like a small descriptor leak to me."
1,"jcr:mixinTypes property inconsitent, if addMixin() throws exceptionIf Node.addMixin() throws an exception, that was not due to validation checks but rather internal, the jcr:mixinTypes still contains the newly added nodetype. a subsequent call to Item.save() will store that property. The effective nodetype is not affected though."
0,Move ReusableAnalyzerBase into coreIn LUCENE-2309 it was suggested that we should make Analyzer reusability compulsory.  ReusableAnalyzerBase is a fantastic way to drive reusability so lets move it into core (so that we can then change all impls over to using it).
0,"Improve Spatial Utility like classes- DistanceUnits can be improved by giving functionality to the enum, such as being able to convert between different units, and adding tests.  

- GeoHashUtils can be improved through some code tidying, documentation, and tests.

- SpatialConstants allows us to move all constants, such as the radii and circumferences of Earth, to a single consistent location that we can then use throughout the contrib.  This also allows us to improve the transparency of calculations done in the contrib, as users of the contrib can easily see the values being used.  Currently this issues does not migrate classes to use these constants, that will happen in issues related to the appropriate classes."
0,"Avoid path resolution in case of non-wildcard ACEs (follow-up to JCR-2573)adding the ability to specify wildcard-ac-entries in the default resource based access control management lead to
always resolving the id passed to AccessControlProvider#canRead in order to be able to properly evaluate
any wildcard-aces present.

this could be improved with minor refactoring that postpones the path resolution and omitting it if there are no
wildcard-aces to compare with."
0,"Optimize bundle serializationThere are a number of ways we could use to make bundle serialization more optimized. Thomas has already done some work on this in the Jackrabbit 3 sandbox, and I'd like to apply some of the optimizations also to the trunk."
1,"TokenSources.getTokenStream() does not assign positionIncrementTokenSources.StoredTokenStream does not assign positionIncrement information. This means that all tokens in the stream are considered adjacent. This has implications for the phrase highlighting in QueryScorer when using non-contiguous tokens.

For example:
Consider  a token stream that creates tokens for both the stemmed and unstemmed version of each word - the fox (jump|jumped)
When retrieved from the index using TokenSources.getTokenStream(tpv,false), the token stream will be - the fox jump jumped

Now try a search and highlight for the phrase query ""fox jumped"". The search will correctly find the document; the highlighter will fail to highlight the phrase because it thinks that there is an additional word between ""fox"" and ""jumped"". If we use the original (from the analyzer) token stream then the highlighter works.

Also, consider the converse - the fox did not jump
""not"" is a stop word and there is an option to increment the position to account for stop words - (the,0) (fox,1) (did,2) (jump,4)
When retrieved from the index using TokenSources.getTokenStream(tpv,false), the token stream will be - (the,0) (fox,1) (did,2) (jump,3).

So the phrase query ""did jump"" will cause the ""did"" and ""jump"" terms in the text ""did not jump"" to be highlighted. If we use the original (from the analyzer) token stream then the highlighter works correctly."
1,"SSL + proxy + Host auth + Keep Alive off causes an infinite loop in HttpMethodDirectorThe combination of SSL tunnelling, host authentication, and disabled persistent
connection support (HTTPD KeepAlive off) causes an infinite loop in
HttpMethodDirector. 

The problem has been reported on the httpclient-dev list by Rindress MacDonald
<RMacDona at enterasys.com>"
1,"o.a.j.core.state.ChildNodeEntries does not override equals(Object) and hashCode() methodso.a.j.c.state.NodeStateMerger calls ChildNodeEntries.equals(ChildNodeEntries) to compare two child node entries collections.
ChildNodeEntries however doesn't override the equals(Object) method."
0,"UUIDDocId cache does not work properly because of weakReferences in combination with new instance for combined indexreader Queries that use ChildAxisQuery or DescendantSelfAxisQuery make use of getParent() functions to know wether the parents are correct and if the result is allowed. The getParent() is called recursively for every hit, and can become very expensive. Hence, in DocId.UUIDDocId, the parents are cached. 

Currently,  docId.UUIDDocId's are cached by having a WeakRefence to the CombinedIndexReader, but, this CombinedIndexReader is recreated all the time, implying that a gc() is allowed to remove the 'expensive' cache.

A much better solution is to not have a weakReference to the CombinedIndexReader, but to a reference of each indexreader segment. This means, that in getParent(int n) in SearchIndex the return 

return id.getDocumentNumber(this) needs to be replaced by return id.getDocumentNumber(subReaders[i]); and something similar in CachingMultiReader. 

That is all. Obviously, when a node/property is added/removed/changed, some parts of the cached DocId.UUIDDocId will be invalid, but mainly small indexes are updated frequently, which obviously are less expensive to recompute."
0,"Invalid redirects are not correctedIf a get is made to a page with a query argument containing a space, many web
servers, notably including Tomcat 5 can generate a redirect in which the space
in the query argument is not escaped correctly.  Most browsers including IE and
Firefox compensate for this by quoting any included spaces in the redirect
location.  Http client does not.  When this broken URL is presented to a
subsequent server, the GET command is interprted incorrectly resulting (usually)
in a 505.

The fix is to replace spaces in redirect locations with +'s.  This doesn't
entirely fix the problem but that is the job of the web server developers."
1,"Docview import fails, if attribute and childelem have same namedocimport fails, if element has same name as one of the attributes of its parent element.

example:

<?xml version=""1.0"" encoding=""UTF-8""?>
<feature plugin=""foobar"">
    <plugin>test</plugin>
</feature>

importing this results in a ItemExistsException: 'plugin'"
0,"add @Deprecated annotationsas discussed on LUCENE-2084, I think we should be consistent about use of @Deprecated annotations if we are to use it.

This patch adds the missing annotations... unfortunately i cannot commit this for some time, because my internet connection does not support heavy committing (it is difficult to even upload a large patch).

So if someone wants to take it, have fun, otherwise in a week or so I will commit it if nobody objects.
"
1,"Missing skip()-Method in ContentLengthInputStreamContentLengthInputStream is missing the skip()-Method.

This causes the internal pos variable to get out of sync with the content 
length. 
We oberseved that closing the stream caused a wait time of about 15 sec in 
routines which use the skip()-method of InputStream.

Here's a possible implementation which should solve the problem:

    public long skip(long len) throws IOException {
        long count = super.skip(len);
        pos += count;
        return count;
    }"
1,"PostgreSQL: Failed to guess validation queryWhen using PostgreSQL, the following warning appears in the log file:

> *WARN  [org.apache.jackrabbit.core.util.db.ConnectionFactory] (main)
> Failed to guess validation query for URL
> jdbc:postgresql:..."
0,Properly close resourcesJava has exceptions so resources must always be closed on a finally clause
0,"Default configuration not suitable for demo web applicationThe default configuration is not suitable for the demo application. There are no text extractors configured, which makes the populate and search demos useless.

Proposed solution: create a new repository.xml in jackrabbit-webapp with text extractors configured.

I know we should actually try to reduce the number of repository.xml files, but having one dedicated to jackrabbit-webapp seems reasonable, while we should try to achieve the same for the jackrabbit-core module."
1,"Support updateDocument() with DWPTsWith separate DocumentsWriterPerThreads (DWPT) it can currently happen that the delete part of an updateDocument() is flushed and committed separately from the corresponding new document.

We need to make sure that updateDocument() is always an atomic operation from a IW.commit() and IW.getReader() perspective.  See LUCENE-2324 for more details."
0,"Broken javadocs->site docs linksSee the java-dev mailing list discussion: [http://www.nabble.com/Broken-javadocs-%3Esite-docs-links-to20369092.html].

When the Lucene Java website transitioned to versioning some of the documentation, links from some javadocs were not modified to follow the resources.  I found broken links to gettingstarted.html and queryparsersyntax.html.  Here is one example, to gettingstarted.html (the link text is ""demo""): 

[http://lucene.apache.org/java/2_4_0/api/org/apache/lucene/document/package-summary.html]

The attached patch converts absolute URLs from javadocs to versioned docs to be relative, and modifies the ""javadocs-all"" target in build.xml to add a path element named ""all"", so that both versions of the javadocs (all: core+contrib; and separated: core, contribs) can use the same relative URLs.  Adding a path element to the ""javadocs-all"" target is necessary because currently the ""all"" javadocs have one fewer path element than the separated javadocs.

I left as-is one absolute URL, in the o.a.l.index.SegmentInfos javadocs, to fileformats.html, because SegmentInfos is a package-private class, and the javadocs targets in build.xml only generate javadocs for public classes.
"
1,"QueryParser doesn't accept empty stringfoo:"""" currently throws a parse exception
foo: bar is also parsed as foo:bar (not serious since it's arguably illegal syntax)"
1,"UsernamePasswordCredentials.equals(null) throws NPESteps to reproduce:
1. new UsernamePasswordCredentials().equals(null);

Observed:
NullPointerException is thrown

Expected:
equals() returns false"
1,"NullPointerException when deleting a property of type REFERENCEIn method org.apache.jackrabbit.rmi.value.SerialValueFactory#createValue a NPE is thrown when parameter value is null.

Solution:

Change:

   public final Value createValue(Node value) throws RepositoryException {
        return createValue(value.getUUID(), PropertyType.REFERENCE);
    }

to

   public final Value createValue(Node value) throws RepositoryException {
        if (value == null) {
           return null;
        }
        
        return createValue(value.getUUID(), PropertyType.REFERENCE);
    }"
0,"Support unicode escapes in QueryParserAs suggested by Yonik in http://issues.apache.org/jira/browse/LUCENE-573 the QueryParser should be able to handle unicode escapes, i. e. \uXXXX.

I have already working and tested code. It is based on the patch i submitted for LUCENE-573, so once this is (hopefully ;-)) committed, I will submit another patch here."
0," MalformedCookieException: distinguish cookie syntax errors from cross-domain errorsMalformedCookieException is used for both cookies with syntax errors,
and for cookies which are invalid for the particular context - e.g.
cross-domain cookies.

I think it would be helpful to be able to distinguish these without
needing to examine the message text."
0,"HuperDuperSynonymsFilterThe current synonymsfilter uses a lot of ram and cpu, especially at build time.

I think yesterday I heard about ""huge synonyms files"" three times.

So, I think we should use an FST-based structure, sharing the inputs and outputs.
And we should be more efficient with the tokenStream api, e.g. using save/restoreState instead of cloneAttributes()
"
1,"Setting a property which has been transiently removed fails with a PathNotFoundExceptionThe following tests currently all fail with a PathNotFoundException

org.apache.jackrabbit.jcr2spi.AddPropertyTest#testReplacingProperty
org.apache.jackrabbit.jcr2spi.AddPropertyTest#testReplacingProperty2
org.apache.jackrabbit.jcr2spi.AddPropertyTest#testAddingProperty
org.apache.jackrabbit.jcr2spi.AddPropertyTest#testAddingProperty2

"
0,"Unit tests TestBackwardsCompatibility and TestIndexFileDeleter might fail depending on JVMIn the two units tests TestBackwardsCompatibility and TestIndexFileDeleter several index file names are hardcoded. For example, in TestBackwardsCompatibility.testExactFileNames() it is tested if the index directory contains exactly the expected files after several operations like addDocument(), deleteDocument() and setNorm() have been performed. Apparently the unit tests pass on the nightly build machine, but in my environment (Windows XP, IBM JVM 1.5) they fail for the following reason:

When IndexReader.setNorm() is called a new norm file for the specified field is created with the file  ending .sx, where x is the number of the field. The problem is that the SegmentMerger can not guarantee to keep the order of the fields, in other words after a merge took place a field can have a different field number. This specific testcase fails, because it expects the file ending .s0, but the file has the ending .s1.

The reason why the field numbers can be different on different JVMs is the use of HashSet in SegmentReader.getFieldNames(). Depending on the HashSet implementation an iterator might not iterate over the entries in insertion order. When I change HashSet to LinkedHashSet, the two testcases pass.

However, even with a LinkedHashSet the order of the field numbers might change during a merge, because the order in which the SegmentMerger merges the FieldInfos depends on the field options like TERMVECTOR, INDEXED... (see SegmentMerger.mergeFields() for details). 

So I think we should not use LinkedHashSet but rather change the problematic testcases. Furthermore I'm not sure if we should have hardcoded filenames in the tests anyway, because if we change the index format or file names in the future these test cases would fail without modification."
0,Improve multihome supportMultihomePlainSocketFactory is basically broken and should be deprecated. Multihome logic needs to be moved to the DefaultClientConnectionOperator
0,"use the internal CND file for builtin nodetypesthe jackrabbit node type registry is reading the built in node types from a XML file.
since the CND (compact node type definition notation) is now specified by jsr283,
i would like to drop the builtin .xml file and read the builtin node typesonly from the .cnd file.
this certainly helps the developers. furthermore, all the node types in jsr283 are now speced
in CND, and converting them to XML is a pain and error prone."
0,"GCJ build fails with JDK 1.5The build.xml doesn't specify a target VM version. Using JDK 1.5, this means the compiled .class files 
are automatically made for 1.5, with java.lang.StringBuilder insidiously used for string concatenation. 
GCJ doesn't seem to include this class yet, so when it gets to the gcj build it dies trying to read the class 
files.

Steps to reproduce:
1. Install Sun JDK 1.5 for a Java compiler
2. Check out Lucene from svn
3. 'ant gcj'

Expected behavior:
Should build Lucene to .class files and .jar with the JDK compiler and then compile an .a with GCJ.

Actual behavior:
The GCJ build fails, complaining of being unable to find java.lang.StringBuilder.

Suggested fix:
Adding source=""1.3"" target=""1.3"" to the <javac> tasks seems to take care of this. Patch to be attached.

Additional notes:
Using Lucene from SVN and GCJ pulled from GCC CVS circa 2005-04-19. Ant 1.6.2."
0,"lucenetestcase ease of use improvementsI started working on this in LUCENE-2658, here is the finished patch.

There are some problems with LuceneTestCase:
* a tests beforeClass, or the test itself (its @befores and its method), might have some
  random behavior, but only the latter can be reproduced with -Dtests.seed
* if you want to do things in beforeClass, you have to use a different API: newDirectory(random)
  instead of newDirectory, etc.
* for a new user, the current output can be verbose, confusing and overwhelming.

So, I refactored this class to address these problems. 
A class still needs 2 seeds internally, as the beforeClass will only run once, 
but the methods or setUp() might run many times, especially when increasing iterations.

but lucenetestcase deals with this, and the ""seed"" is 128-bit (UUID): 
the MSB is initialized in beforeClass, the LSB varied for each method run.
if you provide a seed with a -D, they are both fixed to the UUID you provided.

I fixed the API to be consistent, so you should be able to migrate a test from 
setUp() to beforeClass() [junit3 to junit4] without changing parameters.

The codec, locale, timezone is only printed once at the end if any tests fail, 
as its per-class anyway (setup in beforeClass)

finally, when a test fails, you get a single ""reproduce with"" command line you can copy and paste to reproduce.
this way you dont have to spend time trying to figure out what the command line should be.

{noformat}
    [junit] Tests run: 2, Failures: 2, Errors: 0, Time elapsed: 0.197 sec
    [junit]
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestExample -Dtestmethod=testMethodA 
              -Dtests.seed=a51e707b-6550-7800-9f8c-72622d14bf5f
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestExample -Dtestmethod=testMethodB 
              -Dtests.seed=a51e707b-6550-7800-f7eb-2efca3820738
    [junit] NOTE: test params are: codec=PreFlex, locale=ar_LY, timezone=Etc/UCT
    [junit] ------------- ---------------- ---------------
    [junit] Test org.apache.lucene.util.TestExample FAILED
{noformat}
"
1,"Document View Import: ISO 9075-encoded element/attribute names may lead to illegal node/property names reported by sridhar raman on the users-list:

importing the following xml document leads to a node of name ""abc [1]"" which is illegal:

<?xml version=""1.0""?>
<abc_x0020__x005B_1_x005D_ foo=""bar""/>
"
1,"New versions added after a restore have bad version nameI add several versions to a node (1.0, 1.1, 1.2, 1.3, 1.4). Perform a restore to version 1.2 and add more versions. After that VersionHistory is like this:

- 1.0
- 1.1
- 1.2
- 1.3
- 1.4
- 1.3.1
- 1.3.2
- 1.3.3
- 1.3.4
- 1.3.5

New versions should be 1.2.x no 1.3.x, isn't it?"
1,"VolatileIndex not closed properlyThe MultiIndex.resetVolatileIndex() method doesn't properly close the existing VolatileIndex instance before creating a new one. This can confuse the DynamicPooledExecutor reference count added in JCR-2836, leading to a background thread leak."
0,SerializationTest leaks sessionsThe class TreeComparator extends from AbstractJCRTest and opens a session in its constructor because it calls the setUp() method. The tearDown() method is never called.
0,Add supported for Wikipedia English as a corpus in the benchmarker stuffAdd support for using Wikipedia for benchmarking.
1,"SQLException with OracleBundle PM in name indexThe oracle bundle pm shows errors like:

java.lang.IllegalStateException: Unable to insert index: java.sql.SQLException:  
  ORA-01400: cannot insert NULL into  (""MARTIJNH"".""WM9_VERSIONING_PM_NAMES"".""ID"")

this is due to the fact that oracle treats empty strings as NULL values which does the schema not allow."
1,"SortField.AUTO doesn't work with longThis is actually the same as LUCENE-463 but I cannot find a way to re-open that issue. I'm attaching a test case by dragon-fly999 at hotmail com that shows the problem and a patch that seems to fix it.

The problem is that a long (as used for dates) cannot be parsed as an integer, and the next step is then to parse it as a float, which works but which is not correct. With the patch the following parsers are used in this order: int, long, float.
"
1,"Repository is corrupt after concurrent changes with the same sessionAfter concurrent write operations using the same session, the repository can get corrupt, meaning a ItemNotFoundException is thrown when trying to remove a node.

Concurrent write operations are not supported, however I believe the persistent state of the repository should not be get corrupt.

One way to solve this problem is to synchronize on the session internally."
0,"No documentation on how to use CookieSpecNone of http://hc.apache.org/httpcomponents-client/httpclient/apidocs/org/apache/http/cookie/CookieSpec.html, http://hc.apache.org/httpcomponents-client/httpclient/apidocs/org/apache/http/cookie/CookieSpecFactory.html, http://hc.apache.org/httpcomponents-client/httpclient/apidocs/org/apache/http/cookie/CookieSpecRegistry.html, or http://hc.apache.org/httpcomponents-client/httpclient/apidocs/org/apache/http/impl/client/DefaultHttpClient.html explain how to set the CookieSpec that the HttpClient actually uses. It looks like CookieSpecRegistry might be it, but it doesn't document what the ""names"" mean, so I don't know what to pick to make a factory actually get used."
0,"need the ability to also sort SpellCheck results by freq, instead of just by Edit Distance+freqThis issue was first noticed and reported in this Solr thread; http://lucene.472066.n3.nabble.com/spellcheck-issues-td489776.html#a489788

Basically, there are situations where it would be useful to sort by freq first, instead of the current ""sort by edit distance, and then subsort by freq if edit distance is equal""

The author of the thread suggested ""What I think would work even better than allowing a custom compareTo function would be to incorporate the frequency directly into the distance function.  This would allow for greater control over the trade-off between frequency and edit distance""

However, custom compareTo functions are not always be possible (ie if a certain version of Lucene must be used, because it was release with Solr) and incorporating freq directly into the distance function may be overkill (ie depending on the implementation)

it is suggested that we have a simple modification of the existing compareTo function in Lucene to allow users to specify if they want the existing sort method or if they want to sort by freq.

"
0,"Allow setting the IndexWriter docstore to be a different directoryAdd an IndexWriter.setDocStoreDirectory method that allows doc
stores to be placed in a different directory than the IW default
dir."
1,"Index recovery may fail with IllegalArgumentExceptionWhen repeatedly killed and started up again, jackrabbit may throw an IllegalArgumentException on index recovery:

Caused by: java.lang.IllegalArgumentException: already contains: _c
   at org.apache.jackrabbit.core.query.lucene.IndexInfos.addName(IndexInfos.java:170)
   at org.apache.jackrabbit.core.query.lucene.MultiIndex.deleteIndex(MultiIndex.java:716)
   at org.apache.jackrabbit.core.query.lucene.MultiIndex$DeleteIndex.execute(MultiIndex.java:1553)
   at org.apache.jackrabbit.core.query.lucene.MultiIndex.executeAndLog(MultiIndex.java:809)
   at org.apache.jackrabbit.core.query.lucene.MultiIndex.flush(MultiIndex.java:740)
   at org.apache.jackrabbit.core.query.lucene.Recovery.run(Recovery.java:160)
"
1,http client cache: SizeLimitedResponseReader is not setting content type for InputStreamEntity in constructResponse()the newly created InputStreamEntity should be populated with content-encoding and content-type.
0,"FastVectorHighlighter: add a method to set an arbitrary char that is used when concatenating multiValued dataIf the following multiValued names are in authors field:

* Michael McCandless
* Erik Hatcher
* Otis Gospodneti

Since FragmentsBuilder concatenates multiValued data with a space in BaseFragmentsBuilder.getFragmentSource():

{code}
while( buffer.length() < endOffset && index[0] < values.length ){
  if( index[0] > 0 && values[index[0]].isTokenized() && values[index[0]].stringValue().length() > 0 )
    buffer.append( ' ' );
  buffer.append( values[index[0]++].stringValue() );
}
{code}

an entire field snippet (using LUCENE-2464) will be ""Michael McCandless Erik Hatcher Otis Gospodneti"". There is a requirement an arbitrary char (e.g. '/') can be set so that client can separate the snippet easily. i.e. ""Michael McCandless/Erik Hatcher/Otis Gospodneti"""
1,"Xpath query parser accepts ""/a | /b"" and treats it as ""/a/b""The XPath query parser accepts the query

  /a | /b

and parses it into a query tree corresponging the Xpath query

  /a/b

It should be rejected instead."
1,"MMapDirectory can't create new index on WindowsWhen I set the system property to request the use of the mmap directory, and start building a large index, the process dies with an IOException trying to delete a file. Apparently, Lucene isn't closing down the memory map before deleting the file.
"
0,"More details for beginnersHi everyone,

I'm one of these beginners trying to make their way in Jackrabbit and content repositories universe.

Besides the fact that there exist but very few examples on Jackrabbit use, all of them, including the official web site of Jackrabbit miss few details, simple though essential, for beginners.

My first point is the following: once Jackrabbit source is checked out and built, time to test it using simple examples. But, where to put the example directory from the beginning. How to run it (maven java:compile....) ?

A second point is the fact that there is no help forum on the Jackrabbit web site.

 A third point, taking my case as an example, the example would just not create a new workspace configuration.
And many other troubleshoots - I repeat - basic but essential, that could avoid dozens of wasted hours and discouragment, if just mentioned on the website.

Here it is my wish :-) for the best of Jackrabbit ;-) I hope!
Regards,
Celina"
0,Add RAR META-INF/ra.xml descriptor to be used with JCA1.5I added ra.xml that lets jackrabbit jca to be used with JCA1.5 like in JBoss
0,"Similarity.Stats class for term & collection statisticsIn order to support ranking methods besides TF-IDF, we need to make the statistics they need available. These statistics could be computed in computeWeight (soon to become computeStats) and stored in a separate object for easy access. Since this object will be used solely by subclasses of Similarity, it should be implented as a static inner class, i.e. Similarity.Stats.

There are two ways this could be implemented:
- as a single Similarity.Stats class, reused by all ranking algorithms. In this case, this class would have a member field for all statistics;
- as a hierarchy of Stats classes, one for each ranking algorithm. Each subclass would define only the statistics needed for the ranking algorithm.

In the second case, the Stats class in DefaultSimilarity would have a single field, idf, while the one in e.g. BM25Similarity would have idf and average field/document length."
0,"Create EMPTY_ARGS constsant in SnowballProgram instead of allocating new Object[0]Instead of allocating new Object[0] create a proper constant in SnowballProgram. The same (for new Class[0]) is created in Among, although it's less critical because Among is called from static initializers ... Patch will follow shortly."
0,"QueryHandler should use lucene Input-/OutputStream implementationsCurrently the QueryHandler uses a jackrabbit specific implementation of the lucene Directory interface to make use of the jackrabbit FileSystem abstraction. Lucene operations on the file system however requires quite often random access on the index files. With the current FileSystem interface / abstraction random access is not possible on a FileSystemResource, therefore it is simulated by re-aquiring the InputStream and then seeking to the desired position. This it not efficient at all.

With respect to performance any other use than file based index storage does not make sense with lucene. Hence, the current abstraction using FileSystem should be dropped in favour of direct file access."
1,"Null Pointer Exception while looking for a DavProperty that hasn't been setNull pointer exception.
Exception occurs because the DavPropertySet.map does not contain an expected entry: ItemResourceConstants.JCR_NAME

Suggested fix: add the constant to the nameSet in RepositoryServiceImpl.java:760
 nameSet.add(ItemResourceConstants.JCR_NAME);

I tried that and it works. See stack trace at below.

Exception in thread ""main"" java.lang.NullPointerException
  at org.apache.jackrabbit.spi2dav.URIResolverImpl.buildPropertyId(URIResolverImpl.java:201)
  at org.apache.jackrabbit.spi2dav.RepositoryServiceImpl.getNodeInfo(RepositoryServiceImpl.java:808)
  at org.apache.jackrabbit.spi2dav.RepositoryServiceImpl.getItemInfos(RepositoryServiceImpl.java:834)
  at org.apache.jackrabbit.jcr2spi.state.WorkspaceItemStateFactory.createNodeState(WorkspaceItemStateFactory.java:88)
  at org.apache.jackrabbit.jcr2spi.state.TransientISFactory.createNodeState(TransientISFactory.java:99)
  at org.apache.jackrabbit.jcr2spi.hierarchy.NodeEntryImpl.doResolve(NodeEntryImpl.java:959)
  at org.apache.jackrabbit.jcr2spi.hierarchy.HierarchyEntryImpl.resolve(HierarchyEntryImpl.java:95)
  at org.apache.jackrabbit.jcr2spi.hierarchy.HierarchyEntryImpl.getItemState(HierarchyEntryImpl.java:212)
  at org.apache.jackrabbit.jcr2spi.ItemManagerImpl.getItem(ItemManagerImpl.java:157)
  at org.apache.jackrabbit.jcr2spi.SessionImpl.getRootNode(SessionImpl.java:225)
"
1,"JackrabbitIndexReader prevents use of DocNumberCacheThe JackrabbitIndexReader was introduced in 1.5 by JCR-1363. Unfortunately it does not overwrite the method termDocs(Term), which means the default implementation in IndexReader is used. This bypasses the DocNumberCache built into CachingIndexReader, which is used for UUID terms that look up individual documents."
1,"TestIndexWriter.testCommitThreadSafety fails on realtime_search branchHudson failed on RT with this error - I wasn't able to reproduce yet....

{noformat}
NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testCommitThreadSafety -Dtests.seed=410261592077577885:-4099127561715488589 -Dtests.multiplier=3
NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testCommitThreadSafety -Dtests.seed=410261592077577885:-4099127561715488589 -Dtests.multiplier=3
The following exceptions were thrown by threads:
*** Thread: Thread-331 ***
java.lang.RuntimeException: java.lang.AssertionError: term=f:2_0; r=DirectoryReader(segments_6 _8(4.0):Cv7) expected:<1> but was:<0>
	at org.apache.lucene.index.TestIndexWriter$5.run(TestIndexWriter.java:2416)
Caused by: java.lang.AssertionError: term=f:2_0; r=DirectoryReader(segments_6 _8(4.0):Cv7) expected:<1> but was:<0>
	at org.junit.Assert.fail(Assert.java:91)
	at org.junit.Assert.failNotEquals(Assert.java:645)
	at org.junit.Assert.assertEquals(Assert.java:126)
	at org.junit.Assert.assertEquals(Assert.java:470)
	at org.apache.lucene.index.TestIndexWriter$5.run(TestIndexWriter.java:2410)
NOTE: test params are: codec=RandomCodecProvider: {=SimpleText, f6=MockVariableIntBlock(baseBlockSize=91), f7=MockFixedIntBlock(blockSize=1289), f8=Standard, f9=MockRandom, f1=MockSep, f0=Pulsing(freqCutoff=15), f3=Pulsing(freqCutoff=15), f2=MockFixedIntBlock(blockSize=1289), f5=MockVariableIntBlock(baseBlockSize=91), f4=MockRandom, f=MockSep, c=MockVariableIntBlock(baseBlockSize=91), termVector=SimpleText, d9=SimpleText, d8=MockSep, d5=MockVariableIntBlock(baseBlockSize=91), d4=MockRandom, d7=Standard, d6=SimpleText, d25=Standard, d0=MockVariableIntBlock(baseBlockSize=91), c29=Standard, d24=SimpleText, d1=MockFixedIntBlock(blockSize=1289), c28=MockFixedIntBlock(blockSize=1289), d23=MockVariableIntBlock(baseBlockSize=91), d2=Standard, c27=MockVariableIntBlock(baseBlockSize=91), d22=MockRandom, d3=MockRandom, d21=MockFixedIntBlock(blockSize=1289), d20=MockVariableIntBlock(baseBlockSize=91), c22=MockVariableIntBlock(baseBlockSize=91), c21=MockRandom, c20=Pulsing(freqCutoff=15), d29=MockVariableIntBlock(baseBlockSize=91), c26=SimpleText, d28=MockRandom, c25=MockSep, d27=Pulsing(freqCutoff=15), c24=MockRandom, d26=MockFixedIntBlock(blockSize=1289), c23=Standard, e9=MockRandom, e8=MockFixedIntBlock(blockSize=1289), e7=MockVariableIntBlock(baseBlockSize=91), e6=MockSep, e5=Pulsing(freqCutoff=15), c17=Standard, e3=MockFixedIntBlock(blockSize=1289), d12=SimpleText, c16=SimpleText, e4=Pulsing(freqCutoff=15), d11=MockSep, c19=MockSep, e1=MockSep, d14=Pulsing(freqCutoff=15), c18=Pulsing(freqCutoff=15), e2=SimpleText, d13=MockFixedIntBlock(blockSize=1289), e0=Standard, d10=Standard, d19=Pulsing(freqCutoff=15), c11=SimpleText, c10=MockSep, d16=MockRandom, c13=MockSep, c12=Pulsing(freqCutoff=15), d15=Standard, d18=SimpleText, c15=MockFixedIntBlock(blockSize=1289), d17=MockSep, c14=MockVariableIntBlock(baseBlockSize=91), b3=MockRandom, b2=Standard, b5=SimpleText, b4=MockSep, b7=MockSep, b6=Pulsing(freqCutoff=15), d50=MockVariableIntBlock(baseBlockSize=91), b9=MockFixedIntBlock(blockSize=1289), b8=MockVariableIntBlock(baseBlockSize=91), d43=Pulsing(freqCutoff=15), d42=MockFixedIntBlock(blockSize=1289), d41=SimpleText, d40=MockSep, d47=MockRandom, d46=Standard, b0=SimpleText, d45=MockFixedIntBlock(blockSize=1289), b1=Standard, d44=MockVariableIntBlock(baseBlockSize=91), d49=MockSep, d48=Pulsing(freqCutoff=15), c6=MockVariableIntBlock(baseBlockSize=91), c5=MockRandom, c4=Pulsing(freqCutoff=15), c3=MockFixedIntBlock(blockSize=1289), c9=MockSep, c8=MockRandom, c7=Standard, d30=MockFixedIntBlock(blockSize=1289), d32=MockRandom, d31=Standard, c1=MockVariableIntBlock(baseBlockSize=91), d34=Standard, c2=MockFixedIntBlock(blockSize=1289), d33=SimpleText, d36=MockSep, c0=MockSep, d35=Pulsing(freqCutoff=15), d38=MockVariableIntBlock(baseBlockSize=91), d37=MockRandom, d39=SimpleText, e92=MockFixedIntBlock(blockSize=1289), e93=Pulsing(freqCutoff=15), e90=MockSep, e91=SimpleText, e89=MockVariableIntBlock(baseBlockSize=91), e88=MockSep, e87=Pulsing(freqCutoff=15), e86=SimpleText, e85=MockSep, e84=MockRandom, e83=Standard, e80=MockFixedIntBlock(blockSize=1289), e81=Standard, e82=MockRandom, e77=MockVariableIntBlock(baseBlockSize=91), e76=MockRandom, e79=Standard, e78=SimpleText, e73=MockSep, e72=Pulsing(freqCutoff=15), e75=MockFixedIntBlock(blockSize=1289), e74=MockVariableIntBlock(baseBlockSize=91), binary=MockVariableIntBlock(baseBlockSize=91), f98=Pulsing(freqCutoff=15), f97=MockFixedIntBlock(blockSize=1289), f99=MockRandom, f94=Standard, f93=SimpleText, f96=MockSep, f95=Pulsing(freqCutoff=15), e95=SimpleText, e94=MockSep, e97=Pulsing(freqCutoff=15), e96=MockFixedIntBlock(blockSize=1289), e99=MockFixedIntBlock(blockSize=1289), e98=MockVariableIntBlock(baseBlockSize=91), id=MockRandom, f34=Standard, f33=SimpleText, f32=MockVariableIntBlock(baseBlockSize=91), f31=MockRandom, f30=MockFixedIntBlock(blockSize=1289), f39=Standard, f38=MockVariableIntBlock(baseBlockSize=91), f37=MockRandom, f36=Pulsing(freqCutoff=15), f35=MockFixedIntBlock(blockSize=1289), f43=MockSep, f42=Pulsing(freqCutoff=15), f45=MockFixedIntBlock(blockSize=1289), f44=MockVariableIntBlock(baseBlockSize=91), f41=SimpleText, f40=MockSep, f47=Standard, f46=SimpleText, f49=MockSep, f48=Pulsing(freqCutoff=15), content=MockSep, e19=SimpleText, e18=MockSep, e17=Standard, f12=SimpleText, e16=SimpleText, f11=MockSep, f10=MockRandom, e15=MockVariableIntBlock(baseBlockSize=91), e14=MockRandom, f16=MockRandom, e13=MockSep, f15=Standard, e12=Pulsing(freqCutoff=15), e11=Standard, f14=MockFixedIntBlock(blockSize=1289), e10=SimpleText, f13=MockVariableIntBlock(baseBlockSize=91), f19=Pulsing(freqCutoff=15), f18=Standard, f17=SimpleText, e29=MockRandom, e26=MockSep, f21=Pulsing(freqCutoff=15), e25=Pulsing(freqCutoff=15), f20=MockFixedIntBlock(blockSize=1289), e28=MockFixedIntBlock(blockSize=1289), f23=MockVariableIntBlock(baseBlockSize=91), e27=MockVariableIntBlock(baseBlockSize=91), f22=MockRandom, f25=SimpleText, e22=MockFixedIntBlock(blockSize=1289), f24=MockSep, e21=MockVariableIntBlock(baseBlockSize=91), f27=Pulsing(freqCutoff=15), e24=MockRandom, f26=MockFixedIntBlock(blockSize=1289), e23=Standard, f29=MockFixedIntBlock(blockSize=1289), f28=MockVariableIntBlock(baseBlockSize=91), e20=Pulsing(freqCutoff=15), field=MockRandom, string=Standard, e30=MockRandom, e31=MockVariableIntBlock(baseBlockSize=91), a98=Standard, e34=MockSep, a99=MockRandom, e35=SimpleText, f79=MockSep, e32=Standard, e33=MockRandom, b97=MockRandom, f77=MockRandom, e38=Standard, b98=MockVariableIntBlock(baseBlockSize=91), f78=MockVariableIntBlock(baseBlockSize=91), e39=MockRandom, b99=SimpleText, f75=MockFixedIntBlock(blockSize=1289), e36=MockVariableIntBlock(baseBlockSize=91), f76=Pulsing(freqCutoff=15), e37=MockFixedIntBlock(blockSize=1289), f73=Pulsing(freqCutoff=15), f74=MockSep, f71=SimpleText, f72=Standard, f81=MockFixedIntBlock(blockSize=1289), f80=MockVariableIntBlock(baseBlockSize=91), e40=Standard, e41=Pulsing(freqCutoff=15), e42=MockSep, e43=MockFixedIntBlock(blockSize=1289), e44=Pulsing(freqCutoff=15), e45=MockRandom, e46=MockVariableIntBlock(baseBlockSize=91), f86=SimpleText, e47=MockSep, f87=Standard, e48=SimpleText, f88=Pulsing(freqCutoff=15), e49=MockFixedIntBlock(blockSize=1289), f89=MockSep, f82=MockVariableIntBlock(baseBlockSize=91), f83=MockFixedIntBlock(blockSize=1289), f84=Standard, f85=MockRandom, f90=MockRandom, f92=SimpleText, f91=MockSep, str=MockSep, a76=SimpleText, e56=SimpleText, f59=MockVariableIntBlock(baseBlockSize=91), a77=Standard, e57=Standard, a78=Pulsing(freqCutoff=15), e54=MockRandom, f57=Pulsing(freqCutoff=15), a79=MockSep, e55=MockVariableIntBlock(baseBlockSize=91), f58=MockSep, e52=MockVariableIntBlock(baseBlockSize=91), e53=MockFixedIntBlock(blockSize=1289), e50=Pulsing(freqCutoff=15), e51=MockSep, f51=MockFixedIntBlock(blockSize=1289), f52=Pulsing(freqCutoff=15), f50=SimpleText, f55=Standard, f56=MockRandom, f53=MockVariableIntBlock(baseBlockSize=91), e58=MockFixedIntBlock(blockSize=1289), f54=MockFixedIntBlock(blockSize=1289), e59=Pulsing(freqCutoff=15), a80=MockRandom, e60=MockRandom, a82=SimpleText, a81=MockSep, a84=MockSep, a83=Pulsing(freqCutoff=15), a86=MockFixedIntBlock(blockSize=1289), a85=MockVariableIntBlock(baseBlockSize=91), a89=Standard, f68=Standard, e65=Pulsing(freqCutoff=15), f69=MockRandom, e66=MockSep, a87=MockVariableIntBlock(baseBlockSize=91), e67=MockVariableIntBlock(baseBlockSize=91), a88=MockFixedIntBlock(blockSize=1289), e68=MockFixedIntBlock(blockSize=1289), e61=Standard, e62=MockRandom, e63=MockSep, e64=SimpleText, f60=MockRandom, f61=MockVariableIntBlock(baseBlockSize=91), f62=SimpleText, f63=Standard, e69=SimpleText, f64=MockSep, f65=SimpleText, f66=MockFixedIntBlock(blockSize=1289), f67=Pulsing(freqCutoff=15), f70=MockSep, a93=MockVariableIntBlock(baseBlockSize=91), a92=MockRandom, a91=Pulsing(freqCutoff=15), e71=Pulsing(freqCutoff=15), a90=MockFixedIntBlock(blockSize=1289), e70=MockFixedIntBlock(blockSize=1289), a97=SimpleText, a96=MockSep, a95=MockRandom, a94=Standard, c58=MockFixedIntBlock(blockSize=1289), a63=MockVariableIntBlock(baseBlockSize=91), a64=MockFixedIntBlock(blockSize=1289), c59=Pulsing(freqCutoff=15), c56=MockSep, d59=MockRandom, a61=Pulsing(freqCutoff=15), c57=SimpleText, a62=MockSep, c54=SimpleText, c55=Standard, a60=SimpleText, c52=MockRandom, c53=MockVariableIntBlock(baseBlockSize=91), d53=Standard, d54=MockRandom, d51=MockVariableIntBlock(baseBlockSize=91), d52=MockFixedIntBlock(blockSize=1289), d57=Pulsing(freqCutoff=15), b62=MockFixedIntBlock(blockSize=1289), d58=MockSep, b63=Pulsing(freqCutoff=15), d55=SimpleText, b60=MockSep, d56=Standard, b61=SimpleText, b56=SimpleText, b55=MockSep, b54=MockRandom, b53=Standard, d61=SimpleText, b59=MockVariableIntBlock(baseBlockSize=91), d60=MockSep, b58=MockSep, b57=Pulsing(freqCutoff=15), c62=MockSep, c61=Pulsing(freqCutoff=15), a59=Pulsing(freqCutoff=15), c60=Standard, a58=MockFixedIntBlock(blockSize=1289), a57=MockSep, a56=Pulsing(freqCutoff=15), a55=Standard, a54=SimpleText, a72=Standard, c67=MockRandom, a73=MockRandom, c68=MockVariableIntBlock(baseBlockSize=91), a74=MockSep, c69=SimpleText, a75=SimpleText, c63=Pulsing(freqCutoff=15), c64=MockSep, a70=MockRandom, c65=MockVariableIntBlock(baseBlockSize=91), a71=MockVariableIntBlock(baseBlockSize=91), c66=MockFixedIntBlock(blockSize=1289), d62=MockSep, d63=SimpleText, d64=MockFixedIntBlock(blockSize=1289), b70=MockFixedIntBlock(blockSize=1289), d65=Pulsing(freqCutoff=15), b71=MockRandom, d66=MockVariableIntBlock(baseBlockSize=91), b72=MockVariableIntBlock(baseBlockSize=91), d67=MockFixedIntBlock(blockSize=1289), b73=SimpleText, d68=Standard, b74=Standard, d69=MockRandom, b65=Pulsing(freqCutoff=15), b64=MockFixedIntBlock(blockSize=1289), b67=MockVariableIntBlock(baseBlockSize=91), b66=MockRandom, d70=Pulsing(freqCutoff=15), b69=MockRandom, b68=Standard, d72=MockVariableIntBlock(baseBlockSize=91), d71=MockRandom, c71=MockFixedIntBlock(blockSize=1289), c70=MockVariableIntBlock(baseBlockSize=91), a69=SimpleText, c73=MockRandom, c72=Standard, a66=MockFixedIntBlock(blockSize=1289), a65=MockVariableIntBlock(baseBlockSize=91), a68=MockRandom, a67=Standard, c32=MockSep, c33=SimpleText, c30=Standard, c31=MockRandom, c36=MockVariableIntBlock(baseBlockSize=91), a41=MockRandom, c37=MockFixedIntBlock(blockSize=1289), a42=MockVariableIntBlock(baseBlockSize=91), a0=SimpleText, c34=Pulsing(freqCutoff=15), c35=MockSep, a40=Pulsing(freqCutoff=15), b84=Pulsing(freqCutoff=15), d79=MockSep, b85=MockSep, b82=SimpleText, d77=Standard, c38=SimpleText, b83=Standard, d78=MockRandom, c39=Standard, b80=Standard, d75=MockRandom, b81=MockRandom, d76=MockVariableIntBlock(baseBlockSize=91), d73=MockFixedIntBlock(blockSize=1289), d74=Pulsing(freqCutoff=15), d83=Standard, a9=MockSep, d82=SimpleText, d81=MockVariableIntBlock(baseBlockSize=91), d80=MockRandom, b79=MockSep, b78=Standard, b77=SimpleText, b76=MockVariableIntBlock(baseBlockSize=91), b75=MockRandom, a1=SimpleText, a35=Pulsing(freqCutoff=15), a2=Standard, a34=MockFixedIntBlock(blockSize=1289), a3=Pulsing(freqCutoff=15), a33=SimpleText, a4=MockSep, a32=MockSep, a5=MockFixedIntBlock(blockSize=1289), a39=MockRandom, c40=Pulsing(freqCutoff=15), a6=Pulsing(freqCutoff=15), a38=Standard, a7=MockRandom, a37=MockFixedIntBlock(blockSize=1289), a8=MockVariableIntBlock(baseBlockSize=91), a36=MockVariableIntBlock(baseBlockSize=91), c41=MockFixedIntBlock(blockSize=1289), c42=Pulsing(freqCutoff=15), c43=MockRandom, c44=MockVariableIntBlock(baseBlockSize=91), c45=Standard, a50=SimpleText, c46=MockRandom, a51=Standard, c47=MockSep, a52=Pulsing(freqCutoff=15), c48=SimpleText, a53=MockSep, b93=MockVariableIntBlock(baseBlockSize=91), d88=MockFixedIntBlock(blockSize=1289), c49=MockVariableIntBlock(baseBlockSize=91), b94=MockFixedIntBlock(blockSize=1289), d89=Pulsing(freqCutoff=15), b95=Standard, b96=MockRandom, d84=SimpleText, b90=SimpleText, d85=Standard, b91=MockFixedIntBlock(blockSize=1289), d86=Pulsing(freqCutoff=15), b92=Pulsing(freqCutoff=15), d87=MockSep, d92=MockSep, d91=Pulsing(freqCutoff=15), d94=MockFixedIntBlock(blockSize=1289), d93=MockVariableIntBlock(baseBlockSize=91), b87=MockSep, b86=Pulsing(freqCutoff=15), d90=SimpleText, b89=MockFixedIntBlock(blockSize=1289), b88=MockVariableIntBlock(baseBlockSize=91), a44=MockVariableIntBlock(baseBlockSize=91), a43=MockRandom, a46=Standard, a45=SimpleText, a48=SimpleText, a47=MockSep, c51=Standard, a49=MockFixedIntBlock(blockSize=1289), c50=SimpleText, d98=MockFixedIntBlock(blockSize=1289), d97=MockVariableIntBlock(baseBlockSize=91), d96=MockSep, d95=Pulsing(freqCutoff=15), d99=MockRandom, a20=MockRandom, c99=MockVariableIntBlock(baseBlockSize=91), c98=MockRandom, c97=Pulsing(freqCutoff=15), c96=MockFixedIntBlock(blockSize=1289), b19=MockVariableIntBlock(baseBlockSize=91), a16=SimpleText, a17=Standard, b17=Pulsing(freqCutoff=15), a14=MockRandom, b18=MockSep, a15=MockVariableIntBlock(baseBlockSize=91), a12=MockVariableIntBlock(baseBlockSize=91), a13=MockFixedIntBlock(blockSize=1289), a10=Pulsing(freqCutoff=15), a11=MockSep, b11=MockFixedIntBlock(blockSize=1289), b12=Pulsing(freqCutoff=15), b10=SimpleText, b15=Standard, b16=MockRandom, a18=MockFixedIntBlock(blockSize=1289), b13=MockVariableIntBlock(baseBlockSize=91), a19=Pulsing(freqCutoff=15), b14=MockFixedIntBlock(blockSize=1289), b30=MockSep, a31=Pulsing(freqCutoff=15), a30=MockFixedIntBlock(blockSize=1289), b28=Standard, a25=Pulsing(freqCutoff=15), b29=MockRandom, a26=MockSep, a27=MockVariableIntBlock(baseBlockSize=91), a28=MockFixedIntBlock(blockSize=1289), a21=Standard, a22=MockRandom, a23=MockSep, a24=SimpleText, b20=MockRandom, b21=MockVariableIntBlock(baseBlockSize=91), b22=SimpleText, b23=Standard, a29=SimpleText, b24=MockSep, b25=SimpleText, b26=MockFixedIntBlock(blockSize=1289), b27=Pulsing(freqCutoff=15), b41=MockFixedIntBlock(blockSize=1289), b40=MockVariableIntBlock(baseBlockSize=91), c77=MockRandom, c76=Standard, c75=MockFixedIntBlock(blockSize=1289), c74=MockVariableIntBlock(baseBlockSize=91), c79=Standard, c78=SimpleText, c80=MockVariableIntBlock(baseBlockSize=91), c83=MockSep, c84=SimpleText, c81=Standard, b39=MockSep, c82=MockRandom, b37=MockRandom, b38=MockVariableIntBlock(baseBlockSize=91), b35=MockFixedIntBlock(blockSize=1289), b36=Pulsing(freqCutoff=15), b33=Pulsing(freqCutoff=15), b34=MockSep, b31=SimpleText, b32=Standard, str2=Pulsing(freqCutoff=15), b50=MockRandom, b52=SimpleText, str3=MockRandom, b51=MockSep, c86=SimpleText, tvtest=MockSep, c85=MockSep, c88=Pulsing(freqCutoff=15), c87=MockFixedIntBlock(blockSize=1289), c89=MockVariableIntBlock(baseBlockSize=91), c90=Pulsing(freqCutoff=15), c91=MockSep, c92=MockFixedIntBlock(blockSize=1289), c93=Pulsing(freqCutoff=15), c94=MockRandom, c95=MockVariableIntBlock(baseBlockSize=91), content1=MockSep, b46=SimpleText, b47=Standard, content3=Standard, b48=Pulsing(freqCutoff=15), content4=SimpleText, b49=MockSep, content5=MockRandom, b42=MockVariableIntBlock(baseBlockSize=91), b43=MockFixedIntBlock(blockSize=1289), b44=Standard, b45=MockRandom}, locale=lv_LV, timezone=Australia/Lindeman
NOTE: all tests run in this JVM:
[TestNumericTokenStream, TestIndexFileDeleter, TestIndexInput, TestIndexReaderCloneNorms, TestIndexReaderReopen, TestIndexWriter]
NOTE: FreeBSD 8.2-RELEASE amd64/Sun Microsystems Inc. 1.6.0 (64-bit)/cpus=16,threads=1,free=44228576,total=213778432
{noformat}"
0,"Wrong schemaObjectPrefix parameter in default repository.xmlThe object schema prefix is hard-coded in the default configuration file (I think this taken from the jackrabbit-core.jar):

        <PersistenceManager class=""org.apache.jackrabbit.core.persistence.bundle.DerbyPersistenceManager"">
          <param name=""url"" value=""jdbc:derby:${wsp.home}/db;create=true""/>
          <param name=""schemaObjectPrefix"" value=""Jackrabbit Core_""/>
        </PersistenceManager>

This is probably caused by JCR-945, though I've no idea why ${wsp.name} is replaced with the name of the module...

I have marked this issue as minor because it still works with the DerbyPersistenceManager. There are separate database instances for each workspace, but it will become a problem if a data base persistence manager on a dedicated server is used."
1,"SloppyPhraseScorer returns non-deterministic results for queries with many repeatsProximity queries with many repeats (four or more, based on my testing) return non-deterministic results. I run the same query multiple times with the same data set and get different results.

So far I've reproduced this with Solr 1.4.1, 3.1, 3.2, 3.3, and latest 4.0 trunk.

Steps to reproduce (using the Solr example):
1) In solrconfig.xml, set queryResultCache size to 0.
2) Add some documents with text ""dog dog dog"" and ""dog dog dog dog"". http://localhost:8983/solr/update?stream.body=%3Cadd%3E%3Cdoc%3E%3Cfield%20name=%22id%22%3E1%3C/field%3E%3Cfield%20name=%22text%22%3Edog%20dog%20dog%3C/field%3E%3C/doc%3E%3Cdoc%3E%3Cfield%20name=%22id%22%3E2%3C/field%3E%3Cfield%20name=%22text%22%3Edog%20dog%20dog%20dog%3C/field%3E%3C/doc%3E%3C/add%3E&commit=true
3) Do a ""dog dog dog dog""~1 query. http://localhost:8983/solr/select?q=%22dog%20dog%20dog%20dog%22~1
4) Repeat step 3 many times.

Expected results: The document with id 2 should be returned.

Actual results: The document with id 2 is always returned. The document with id 1 is sometimes returned.

Different proximity values show the same bug - ""dog dog dog dog""~5, ""dog dog dog dog""~100, etc show the same behavior.

So far I've traced it down to the ""repeats"" array in SloppyPhraseScorer.initPhrasePositions() - depending on the order of the elements in this array, the document may or may not match. I think the HashSet may be to blame, but I'm not sure - that at least seems to be where the non-determinism is coming from."
0,"move DocumentStoredFieldsVisitor to o.a.l.documentwhen examining the changes to the field/document API, i noticed this class was in o.a.l.index

I think it should be in o.a.l.document, its more intuitive packaging"
0,"Add AttributeSource.copyTo(AttributeSource)One problem with AttributeSource at the moment is the missing ""insight"" into AttributeSource.State. If you want to create TokenStreams that inspect cpatured states, you have no chance. Making the contents of State public is a bad idea, as it does not help for inspecting (its a linked list, so you have to iterate).

AttributeSource currently contains a cloneAttributes() call, which returns a new AttrubuteSource with all current attributes cloned. This is the (more expensive) captureState. The problem is that you cannot copy back the cloned AS (which is the restoreState). To use this behaviour (by the way, ShingleMatrix can use it), one can alternatively use cloneAttributes and copyTo. You can easily change the cloned attributes and store them in lists and copy them back. The only problem is lower performance of these calls (as State is a very optimized class).

One use case could be:
{code}
AttributeSource state = cloneAttributes();
// .... do something ...
state.getAttribute(TermAttribute.class).setTermBuffer(foobar);
// ... more work
state.copyTo(this);
{code}"
1,"ObjectIterator may return null, which is not readily expected from an IteratorThe ObjectIterator class implements an Iterator of objects mapped from an underlying NodeIterator. This ObjectIterator may return null from next() if no mapping for a node in the iterator exists. Rather than returning null, the iterator should probably just ignore the unmappable node and return an object from the next node in the underlying iterator which is mappable."
0,"new merge policyNew merge policy developed in the course of 
http://issues.apache.org/jira/browse/LUCENE-565
http://issues.apache.org/jira/secure/attachment/12340475/newMergePolicy.Sept08.patch"
1,Jcr-Server: ItemDefinitionImpl.toXml throws NPE for the root node.ItemDefinitionImpl.toXml throws NPE for the root node due to a missing assertion regarding the declaring nodetype.
0,"Node.setPrimaryNodeType should only redefine child-definitions that are not covered by the new effective ntNodeImpl.setPrimaryNodeType changes the primary node type of an node and resets the definition of child items if required. Currently all child items that are not part of the effective node type of the new primary type get their definition reset or are removed in case not matching definition is found.
From my point of view this doesn't properly cope with mixin types present on the node: child items defined by any of the mixin node types present should probably not be touched (or removed).

I run into this while testing the latest 283 security changes and will try to provide a fix along with those changes."
0,"Always use bulk-copy when merging stored fields and term vectorsLucene has nice optimizations in place during merging of stored fields
(LUCENE-1043) and term vectors (LUCENE-1120) whereby the bytes are
bulk copied to the new segmetn.  This is much faster than decoding &
rewriting one document at a time.

However the optimization is rather brittle: it relies on the mapping
of field name to number to be the same (""congruent"") for the segment
being merged.

Unfortunately, the field mapping will be congruent only if the app
adds the same fields in precisely the same order to each document.

I think we should fix IndexWriter to assign the same field number for
a given field that has been assigned in the past.  Ie, when writing a
new segment, we pre-seed the field numbers based on past segments.
All other aspects of FieldInfo would remain fully dynamic."
1,"DataStore: garbage collection can fail when using workspace maxIdleTimeThe GarbageCollectorTest fails because some workspaces have an idle timeout. The data store garbage collector should prevent workspace close-on-idle.

Proposed solution: instead of using the 'regular' system sessions in the garbage collector, use special 'registered system sessions'. The sessions get garbage collected when no longer used, that means this patch requires that JCR-1216 ""Unreferenced sessions should get garbage collected"" is applied. So for each workspace, the code is:

// this will initialize the workspace if required
wspInfo.getSystemSession();

SessionImpl session = SystemSession.create(rep, wspInfo.getConfig());
// mark this session as 'active' for so the workspace does
// not get disposed by workspace-janitor until the garbage collector is done
rep.onSessionCreated(session);            
"
0,SPNEGO authentication schemeConsider integrating the SPNEGO auth scheme from Commons HttpClient contrib package into HttpClient 4.0
1,"RepositoryCopier does not copy open-scoped LocksIf you use the RepositoryCopier to make a backup of your repository and you have open-scoped (not session scoped) locks, these locks will not be copied. If you try to restore your copy of the repository all locks are gone."
1,"problem with edgengramtokenfilter and highlighteri ran into a problem while using the edgengramtokenfilter, it seems to report incorrect offsets when generating tokens, more specifically all the tokens have offset 0 and term length as start and end, this leads to goofy highlighting behavior when creating edge grams for tokens beyond the first one, i created a small patch that takes into account the start of the original token and adds that to the reported start/end offsets.

"
1,"SSL connections cannot be established using resolvable IP addressHttpClient 4.1 introduced a regression in establishing SSL connections to remote peers (it seems this is a common regression for major httpclient updates, see HTTPCLIENT-803).
The new SSLSocketFactory.connectSocket method calls the X509HostnameVerifier with InetSocketAddress.getHostName() parameter. When the selected IP address has a reverse lookup name, the verifier is called with the resolved name, and so the IP check fails.
4.0 release checked for original ip/hostname, but this cannot be done with the new connectSocket() method. 
The TestHostnameVerifier.java only checks 127.0.0.1/.2 and so masked the issue, because the matching certificate has both ""localhost"" and ""127.0.0.1"", but actually only ""localhost"" is matched. A test case with 8.8.8.8 would be better."
0,Add pattern matching for pathsI suggest to add utility classes to spi-commons which can be used to do pattern matching on paths similar to regular expressions. 
1,Extra </div> in populate.jspThe populate.jsp page in jackrabbit-webapp has an extra </div> that causes minor breakage to the page layout.
0,"Contributed ClassLoader project still uses commons-logging for logging.As of JCR-215 Jackrabbit core code has been migrated from Log4J to SLF4J. The ClassLoader contribution always used commons-logging. It is about time, to also migrate that project to proper SLF4J."
0,"Filter to process output of ICUTokenizer and create overlapping bigrams for CJK The ICUTokenizer produces unigrams for CJK. We would like to use the ICUTokenizer but have overlapping bigrams created for CJK as in the CJK Analyzer.  This filter would take the output of the ICUtokenizer, read the ScriptAttribute and for selected scripts (Han, Kana), would produce overlapping bigrams."
1,"Request with DIGEST authentication fails when redirectedRequest with DIGEST authentication fails when redirected due to invalid URI
parameter.

-- Client side log ----------------------------------------------------------

[DEBUG] HttpClient - -Java version: 1.2.2
[DEBUG] HttpClient - -Java vendor: Sun Microsystems Inc.
[DEBUG] HttpClient - -Operating system name: Linux
[DEBUG] HttpClient - -Operating system architecture: i386
[DEBUG] HttpClient - -Operating system version: 2.4.20-13.9-ok
[DEBUG] HttpClient - -SUN 1.2: SUN (DSA key/parameter generation; DSA signing;
SHA-1, MD5 digests; SecureRandom; X.509 certificates; JKS keystore)
[DEBUG] HttpClient - -SunJSSE 1.0301: Sun JSSE provider(implements RSA
Signatures, PKCS12, SunX509 key/trust factories, SSLv3, TLSv1)
[DEBUG] HttpConnection - -Creating connection for localhost using protocol http:80
[DEBUG] HttpConnection - -HttpConnection.setSoTimeout(0)
[DEBUG] HttpMethod - -Execute loop try 1
[DEBUG] wire - ->> ""GET /transfer HTTP/1.1[\r][\n]""
[DEBUG] HttpMethod - -Adding Host request header
[DEBUG] wire - ->> ""User-Agent: Jakarta Commons-HttpClient/2.0beta1[\r][\n]""
[DEBUG] wire - ->> ""Host: localhost[\r][\n]""
[DEBUG] wire - ->> ""[\r][\n]""
[DEBUG] wire - -<< ""HTTP/1.1 401 Authorization Required[\r][\n]""
[DEBUG] wire - -<< ""Date: Fri, 20 Jun 2003 08:30:06 GMT[\r][\n]""
[DEBUG] wire - -<< ""Server: Apache/2.0.40 (Red Hat Linux)[\r][\n]""
[DEBUG] wire - -<< ""WWW-Authenticate: Digest realm=""guest realm"",
nonce=""ei+T7oPAAwA=53c8e6d609ff81a8dcbc370b51f8aadec565009a"", algorithm=MD5,
domain=""/transfer"", qop=""auth""[\r][\n]""
[DEBUG] wire - -<< ""Vary: accept-language[\r][\n]""
[DEBUG] wire - -<< ""Accept-Ranges: bytes[\r][\n]""
[DEBUG] wire - -<< ""Content-Length: 1285[\r][\n]""
[DEBUG] wire - -<< ""Content-Type: text/html; charset=ISO-8859-1[\r][\n]""
[DEBUG] HttpMethod - -Authorization required
[DEBUG] HttpAuthenticator - -Using 'guest realm' authentication realm
[DEBUG] HttpMethod - -HttpMethodBase.execute(): Server demanded authentication
credentials, will try again.
...
[DEBUG] HttpMethod - -Resorting to protocol version default close connection policy
[DEBUG] HttpMethod - -Should NOT close connection, using HTTP/1.1.
[DEBUG] HttpMethod - -Execute loop try 2
[DEBUG] wire - ->> ""GET /transfer HTTP/1.1[\r][\n]""
[DEBUG] HttpMethod - -Request to add Host header ignored: header already added
[DEBUG] wire - ->> ""User-Agent: Jakarta Commons-HttpClient/2.0beta1[\r][\n]""
[DEBUG] wire - ->> ""Host: localhost[\r][\n]""
[DEBUG] wire - ->> ""Authorization: Digest username=""guest"", realm=""guest realm"",
nonce=""ei+T7oPAAwA=53c8e6d609ff81a8dcbc370b51f8aadec565009a"", uri=""/transfer"",
qop=""auth"", algorithm=""MD5"", nc=00000001,
cnonce=""81d4b905a4e9def944beaed8daf79283"",
response=""71394edcddf4bcee6237ea4bb50cfaa5""[\r][\n]""
[DEBUG] wire - ->> ""[\r][\n]""
[DEBUG] wire - -<< ""HTTP/1.1 301 Moved Permanently[\r][\n]""
[DEBUG] wire - -<< ""Date: Fri, 20 Jun 2003 08:30:06 GMT[\r][\n]""
[DEBUG] wire - -<< ""Server: Apache/2.0.40 (Red Hat Linux)[\r][\n]""
[DEBUG] wire - -<< ""Location: http://localhost/transfer/[\r][\n]""
[DEBUG] wire - -<< ""Content-Length: 302[\r][\n]""
[DEBUG] wire - -<< ""Content-Type: text/html; charset=iso-8859-1[\r][\n]""
[DEBUG] HttpMethod - -Redirect required
[DEBUG] HttpMethod - -Redirect requested to location 'http://localhost/transfer/'
[DEBUG] HttpMethod - -Redirecting from 'http://localhost:80/transfer' to
'http://localhost/transfer/
...
[DEBUG] HttpMethod - -Resorting to protocol version default close connection policy
[DEBUG] HttpMethod - -Should NOT close connection, using HTTP/1.1.
[DEBUG] HttpMethod - -Execute loop try 3
[DEBUG] wire - ->> ""GET /transfer/ HTTP/1.1[\r][\n]""
[DEBUG] HttpMethod - -Request to add Host header ignored: header already added
[DEBUG] wire - ->> ""User-Agent: Jakarta Commons-HttpClient/2.0beta1[\r][\n]""
[DEBUG] wire - ->> ""Host: localhost[\r][\n]""
[DEBUG] wire - ->> ""Authorization: Digest username=""guest"", realm=""guest realm"",
nonce=""ei+T7oPAAwA=53c8e6d609ff81a8dcbc370b51f8aadec565009a"", uri=""/transfer"",
qop=""auth"", algorithm=""MD5"", nc=00000001,
cnonce=""81d4b905a4e9def944beaed8daf79283"",
response=""71394edcddf4bcee6237ea4bb50cfaa5""[\r][\n]""
[DEBUG] wire - ->> ""[\r][\n]""
[DEBUG] wire - -<< ""HTTP/1.1 400 Bad Request[\r][\n]""
[DEBUG] wire - -<< ""Date: Fri, 20 Jun 2003 08:30:06 GMT[\r][\n]""
[DEBUG] wire - -<< ""Server: Apache/2.0.40 (Red Hat Linux)[\r][\n]""
[DEBUG] wire - -<< ""Vary: accept-language[\r][\n]""
[DEBUG] wire - -<< ""Accept-Ranges: bytes[\r][\n]""
[DEBUG] wire - -<< ""Content-Length: 973[\r][\n]""
[DEBUG] wire - -<< ""Connection: close[\r][\n]""
[DEBUG] wire - -<< ""Content-Type: text/html; charset=ISO-8859-1[\r][\n]""

-- End of client side log -----------------------------------------------------


-- Server side log ------------------------------------------------------------

[Fri Jun 20 10:30:06 2003] [error] [client 127.0.0.1] Digest: uri mismatch -
</transfer> does not match request-uri </transfer/>

-- End of server side log -----------------------------------------------------"
0,"Lucene Search not scallingI've noticed that when doing thousands of searches in a single thread the average time is quite low i.e. a few milliseconds. When adding more concurrent searches doing exactly the same search the average time increases drastically. 
I've profiled the search classes and found that the whole of lucene blocks on 

org.apache.lucene.index.SegmentCoreReaders.getTermsReader
org.apache.lucene.util.VirtualMethod
  public synchronized int getImplementationDistance 
org.apache.lucene.util.AttributeSourcew.getAttributeInterfaces

These cause search times to increase from a few milliseconds to up to 2 seconds when doing 500 concurrent searches on the same in memory index. Note: That the index is not being updates at all, so not refresh methods are called at any stage.


Some questions:
  Why do we need synchronization here?
  There must be a non-lockable solution for these, they basically cause lucene to be ok for single thread applications but disastrous for any concurrent implementation.

I'll do some experiments by removing the synchronization from the methods of these classes."
0,Add getTotalSize() to QueryResultsAs discussed in http://www.nabble.com/Total-size-of-a-query-result-and-setLimit%28%29-tf4280909.html#a12185543 a getTotalSize() method should be added to QueryResults.
0,"Update license termsCopyright 1999-2003 The Apache Software Foundation.

   Licensed under the Apache License, Version 2.0 (the ""License"");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an ""AS IS"" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License."
0,"Split PrivilegeRegistry in a per-session manager instance and a repository level registryin order to resolve the privilegeregistry related TODOs within jackrabbit-core, i would like to split off those 
methods from PrivilegeRegistry  that are used on a per-session level (including jcr-names) and add them
to a manager class that was present with each session context. consequently the responsibility of the
registry was then limited to read/build the privilege definitions and would be present on the repositorycontext
deprecating those methods that would be covered by the manager).
in addition the naming was then consistent with what we use to have for nodetypes and namespaces."
0,"allow different strategies when checking CN of x509 certWe're now doing a decent job for checking the CN of the x509 cert with https:

http://issues.apache.org/jira/browse/HTTPCLIENT-613

I think the patch for HTTPCLIENT-613 should cover 99.9% of the users out there.  But there are some more esoteric possibilities, so I think Oleg is right.  We need to let the user change the strategy, or provide their own strategy if they want to. 

Some additional things to think about:

- http://wiki.cacert.org/wiki/VhostTaskForce !!!   CN is depreciated?!?!   (I am not able to find a popular website on HTTPS that isn't using CN!)

- [*.example.com] matches subdomains [a.b.example.com] on Firefox, but not IE6.  The patch for HTTPCLIENT-613 allows subdomains.

- Should we support multiple CN's in the subject?

- Should we support ""subjectAltName=DNS:www.example.com"" ?  Should we support lots of them in a single cert?

- Should we support a mix of CN and subjectAltName?


If we do create some alternate strategies for people to try, I'd probably lean towards something like this:

X509NameCheckingStrategy.SUN_JAVA_6  (default)
X509NameCheckingStrategy.FIREFOX2
X509NameCheckingStrategy.IE7
X509NameCheckingStrategy.FIRST_CN_AND_NO_WILDCARDS   (aka ""STRICT"")

"
0,"New Preferences ArchitectureAn architectural solution is needed to configure various aspects of HttpClient,
Methods and Connections. 

Features:
- can configure certain properties per request / per connection
- all configuration is done in a consistant way 
- do not use system properties
- configuration is completely optional: default values should be used if no
configuration is made

This is a refactoring request / reminder. File configuration issues as
dependencies of this bug."
0,Reorganize test suitesI'd like to better organize the test setup in jackrabbit-core. The current test repository is located in applications/test and managed with custom ant tasks and explicit do_init/do_test surefire configuration. It would be better to have the test repository located in target/repository (with template content coming from src/test/repository) and managed using the Maven 2 integration test lifecycle phases.
1,"MultiThreadedHttpConnectionManager does not properly respond to thread interruptsMultiThreadedHttpConnectionManager uses interrupts to notify waiting threads when a connection is ready for them. Issues arise if the threads are interrupted by someone else while they are still waiting on a thread, because doGetConnection does not remove the threads from the queue of waiting threads when they are interrupted:

                        connectionPool.wait(timeToWait);

                        // we have not been interrupted so we need to remove ourselves from the 
                        // wait queue
                        hostPool.waitingThreads.remove(waitingThread);                        connectionPool.waitingThreads.remove(waitingThread);
                    } catch (InterruptedException e) {
                        // do nothing                    } finally {
                        if (useTimeout) {
                            endWait = System.currentTimeMillis();
                            timeToWait -= (endWait - startWait);                        }                    }

Under ordinary circumstances, the queue maintenance is done by the notifyWaitingThread method. However, if the thread is interrupted by any other part of the system, it will (1) not actually be released, since the loop in doGetConnection will force it back to the wait, and (2) will be added the waiting thread to the queue repeatedly, which basically means that the thread will eventually receive the interrupt from notifyWaitingThread at some later point, when it is no longer actually waiting for a connection.

This code could probably be re-architected to make it less error-prone, but the fundamental issue seems to be the use of interrupts to signal waiting threads, as opposed to something like a notify. "
1,"Value#getBinary() and #getStream() return internal representation for type PATH and NAMEjust found a path-related spi2dav test failing that passed some time before jackrabbit 2.0 (BatchTest#testSetPathValue).

i had a quick look at it and it seems to me that the reasons is the internal (Path, Name) value representation 
being exposed when calling Value#getBinary(), Value#getStream() and the corresponding shortcuts on Property.

from my understanding of the specification these methods should always return the standard JCR path (or name) representation as it
is exposed by Value#getString() and Property#getString() as it used to be in previous versions.



"
0,"JCRTest.java (First Steps example code): to few parameters in session.importXMLIn the code on the First Steps page:

if (!rn.hasNode(""importxml"")) {
        System.out.println(""importing xml"");
        Node n=rn.addNode(""importxml"", ""nt:unstructured"");
        session.importXML(""/importxml"", new FileInputStream(""repotest/test.xml""));
        session.save();
      }

The importXML needs a third parameter, compare to: 

http://www.day.com/maven/jsr170/javadocs/jcr-1.0/javax/jcr/Session.html

This prevents the code from the First Steps page from compiling."
0,DisjunctionMaxQuery -  Iterator code to  for ( A  a : container ) constructFor better readability  - converting the Iterable<T> to  for ( A  a : container ) constructs that is more intuitive to read. 
0,"BooleanScorer should not limit number of prohibited clausesToday it's limited to 32, because it uses a separate bit in the mask
for each clause.

But I don't understand why it does this; I think all prohibited
clauses can share a single boolean/bit?  Any match on a prohibited
clause sets this bit and the doc is not collected; we don't need each
prohibited clause to have a dedicated bit?

We also use the mask for required clauses, but this code is now
commented out (we always use BS2 if there are any required clauses);
if we re-enable this code (and I think we should, at least in certain
cases: I suspect it'd be faster than BS2 in many cases), I think we
can cutover to an int count instead of bit masks, and then have no
limit on the required clauses sent to BooleanScorer also.

Separately I cleaned a few things up about BooleanScorer: all of the
embedded scorer methods (nextDoc, docID, advance, score) now throw
UOE; pre-allocate the buckets instead of doing it lazily
per-sub-collect.
"
1,"[PATCH] Javadoc improvements and minor fixesJavadoc improvements for Scorer.java and Weight.java. 
This also fixes some recent changes introduced minor warnings when building 
the javadocs and adds a small comment in Similarity.java. 
The individual patches will be attached."
0,"Make open scoped locks recoverableThe lock tokens for open scoped locks are currently tied to the session which created the lock. If the session dies (for whatever reason) there is no way to recover the lock and unlock the node.
There is a theoretical way of adding the lock token to another session, but in most cases the lock token is not available.

Fortunately, the spec allows to relax this behaviour and I think it would make sense to allow all sessions from the same user to unlock the node - this is still in compliance with the spec but would make unlocked locked nodes possible in a programmatic way."
0,Add Compact Namespace and Node Type Definition support to spi-commonsAdd support for reading and writing of Compact Namespace and Node Type Definitions (cnd-files) to spi-commons. 
0,"DocValuesField should not overload setInt/setFloat etcSee my description on LUCENE-3687. In general we should avoid this for primitive types and give them each unique names.

So I think instead of setInt(byte), setInt(short), setInt(int), setInt(long), setFloat(float) and setFloat(double),
we should have setByte(byte), setShort(short), setInt(int), setLong(long), setFloat(float) and setDouble(double)."
0,"[PATCH] unnecessary synchronized collections used only in thread safe wayNodeTypeReader uses Vector in only a local variable thread safe way. Thus the synchronized value of Vector is not needed, and just slowing the code down for nothing. this patch switches the collections to ArrayLists."
1,"Handling of multiple residual prop defs in EffectiveNodeTypeImplorg.apache.jackrabbit.jcr2spi.nodetype.EffectiveNodeTypeImpl currently rejects multiple residual property definitions, if they do not differ in getMultiple(). In fact, it should accept all combinations, so differing values for getOnParentVersionAction and other aspects should be accepted as well.

See JSR 170, 6.7.8:

""For purposes of the above, the notion of two definitions having the same name does not apply to two residual definitions. Two (or more) residual property or child node definitions with differing subattributes must be permitted to co-exist in the same effective node type. They are interpreted as disjunctive (ORed) options."""
0,"Enable access to the freq information in a Query's sub-scorersThe ability to gather more details than just the score, of how a given
doc matches the current query, has come up a number of times on the
user's lists.  (most recently in the thread ""Query Match Count"" by
Ryan McV on java-user).

EG if you have a simple TermQuery ""foo"", on each hit you'd like to
know how many times ""foo"" occurred in that doc; or a BooleanQuery +foo
+bar, being able to separately see the freq of foo and bar for the
current hit.

Lucene doesn't make this possible today, which is a shame because
Lucene in fact does compute exactly this information; it's just not
accessible from the Collector.
"
0,"Code cleanup from all sorts of (trivial) warningsI would like to do some code cleanup and remove all sorts of trivial warnings, like unnecessary casts, problems w/ javadocs, unused variables, redundant null checks, unnecessary semicolon etc. These are all very trivial and should not pose any problem.

I'll create another issue for getting rid of deprecated code usage, like LuceneTestCase and all sorts of deprecated constructors. That's also trivial because it only affects Lucene code, but it's a different type of change.

Another issue I'd like to create is about introducing more generics in the code, where it's missing today - not changing existing API. There are many places in the code like that.

So, with you permission, I'll start with the trivial ones first, and then move on to the others."
0,Use FieldSelector in Sorted/LuceneQueryHits when reading UUIDLuceneQueryHits currently reads the complete lucene document. This also prevents usage of an underlying UUID cache.
1,"Session.save() and Session.refresh(boolean) rely on accessibility of the root nodefollow-up issue to JCR-2418:

an editing session that is only allowed to write in a subtree but isn't allowed to access the root node will not be
able to save or revert changes made in the transient space within that subtree.

the reason for this is, that both SessionImpl.save() and SessionImpl.refresh(boolean) access the root node
in order to execute the call. since it's the regular call READ permissions are checked, although the user
made no attempt to *look* at the root.

A workaround would be to call Item.save() on the modified tree itself that obviously was visible for the 
user... unfortunately that method is deprecated as of JCR 2.0. Therefore, I have the impression that we
should fix the methods mentioned above.

"
1,"incorrect HTML excerpt generation for queries on japanese text content The generated excerpt highlights single characters instead of full words. Test case (to be added to FullTextQueryTest):

     public void testJapaneseAndHighlight() throws RepositoryException {
        // http://translate.google.com/#auto|en|%E3%82%B3%E3%83%B3%E3%83%86%E3%83%B3%E3%83%88
        String jContent = ""\u30b3\u30fe\u30c6\u30f3\u30c8"";
        // http://translate.google.com/#auto|en|%E3%83%86%E3%82%B9%E3%83%88
        String jTest = ""\u30c6\u30b9\u30c8"";
        
        String content = ""some text with japanese: "" + jContent
                + "" ('content')"" + "" and "" + jTest + "" ('test')."";

        // expected excerpt; note this may change if excerpt providers change
        String expectedExcerpt = ""<div><span>some text with japanese: "" + jContent
                + "" ('content') and <strong>"" + jTest
                + ""</strong> ('test').</span></div>"";
        
        Node n = testRootNode.addNode(""node1"");
        n.setProperty(""title"", content);
        testRootNode.getSession().save();
        
        String xpath = ""/jcr:root"" + testRoot + ""/element(*, nt:unstructured)""
                + ""[jcr:contains(., '"" + jTest + ""')]/rep:excerpt(.)"";
        Query q = superuser.getWorkspace().getQueryManager()
                .createQuery(xpath, Query.XPATH);
        
        QueryResult qr = q.execute();
        RowIterator it = qr.getRows();
        int cnt = 0;
        while (it.hasNext()) {
            cnt++;
            Row found = it.nextRow();
            assertEquals(n.getPath(), found.getPath());
            String excerpt = found.getValue(""rep:excerpt(.)"").getString();
            assertEquals(expectedExcerpt, excerpt);
        }
        
        assertEquals(1, cnt);
    }
"
0,Remove the unneeded cqfs dependenciesThere's still unneeded dependencies to the cqfs libraries in jcr-server/webapp and jca.
1,"(Parallel-)MultiSearcher: using Sort object changes the scoresExample: 
Hits hits=multiSearcher.search(query);
returns different scores for some documents than
Hits hits=multiSearcher.search(query, Sort.RELEVANCE);
(both for MultiSearcher and ParallelMultiSearcher)

The documents returned will be the same and in the same order, but the scores in the second case will seem out of order.

Inspecting the Explanation objects shows that the scores themselves are ok, but there's a bug in the normalization of the scores.

The document with the highest score should have score 1.0, so all document scores are divided by the highest score.  (Assuming the highest score was>1.0)

However, for MultiSearcher and ParallelMultiSearcher, this normalization factor is applied *per index*, before merging the results together (the merge itself is ok though).

An example: if you use
Hits hits=multiSearcher.search(query, Sort.RELEVANCE);
for a MultiSearcher with two subsearchers, the first document will have score 1.0.
The next documents from the same subsearcher will have decreasing scores.
The first document from the other subsearcher will however have score 1.0 again !

The same applies for other Sort objects, but it is less visible.

I will post a TestCase demonstrating the problem and suggested patches to solve it in a moment..."
0,"AbstractRecord does inefficient List.indexOf()AbstractRecord keeps a list of already used UUIDs and references
them by index when used again in a record. Using a List does not
scale well, when the record grows larger. e.g. a transaction of
10k nodes takes more than a minute on my machine when the journal
is enabled. Most of the time is spent doing List.indexOf() in
AbstractRecord.getOrCreateIndex()."
1,"RMIRemoteBindingServlet fails to initialize if the RMI registry is not availableIf the RMI registry is not available, the RMIRemoteBindingServlet in jcr-rmi will throw an exception in the init() method and prevent the servlet from being loaded.

The same servlet can however also be mapped to the normal HTTP URL space as an alternative mechanism of making the RMI endpoint available to clients. Thus it would be better if the init() method just logged a warning instead of failing completely."
0,Support grouping by IndexDocValuesAlthough IDV is not yet finalized (More particular the SortedSource). I think we already can discuss / investigate implementing grouping by IDV.
0,"CoordConstrainedBooleanQuery + QueryParser supportAttached 2 new classes:

1) CoordConstrainedBooleanQuery
A boolean query that only matches if a specified number of the contained clauses
match. An example use might be a query that returns a list of books where ANY 2
people from a list of people were co-authors, eg:
""Lucene In Action"" would match (""Erik Hatcher"" ""Otis Gospodneti&#263;"" ""Mark Harwood""
""Doug Cutting"") with a minRequiredOverlap of 2 because Otis and Erik wrote that.
The book ""Java Development with Ant"" would not match because only 1 element in
the list (Erik) was selected.

2) CustomQueryParserExample
A customised QueryParser that allows definition of
CoordConstrainedBooleanQueries. The solution (mis)uses fieldnames to pass
parameters to the custom query."
0,"Use the Jackrabbit RMI extensions by default in jackrabbit-webappUsing the Jackrabbit RMI extensions by default in jackrabbit-webapp

Ref :  http://www.nabble.com/Custom-node-types-with-RMI-tf3728625.html

"
1,"VirtualItemStates of node types definitions not accessible with uuidThe VirtualNodeTypeStateProvider that maps node type definitions into the workspace under /jcr:system/jcr:nodeTypes does not implement the methods:

- internalGetNodeState(NodeId id)
- internalHasNodeState(NodeId id)

This has the effect that ItemStates that reflect node type definitions are not accessible directly with their uuid."
0,"Make ItemIds more stableThe ItemIds returned by spi2dav are currently not stable in the sense that they are sometimes uuid based and sometimes not: If a node is referenceable some of its properties will receive fully path based ids while others will receive ids based on the uuid of its parent node. 

The efficiency of caching introduced with JCR-2498 depends on stable ids. I therefore suggest to improve spi2dav such that property ids are always uuid based if the parent's node has a uuid. "
1,"Deadlock during checkinUnder a load of 3 threads performing checkin and restore operations it's possible for all to become deadlocked in AbstractVersionManager.checkin(). This method attempts to upgrade a read lock to a write lock with the following code

    aquireReadLock();
    ....

    try {
        aquireWriteLock();
        releaseReadLock();
        ...

If 2 or more threads acquire the read lock then neither can acquire the write lock resulting in the deadlock, and after that any other thread that calls this method will block waiting for the write lock. The release of the read lock needs to be done before acquiring the write lock, this is documented Concurrent library javadoc.

There is another area where there is an attempt to upgrade a read lock to write lock, RepositoryImpl.WorkspaceInfo.disposeIfIdle() acquires a read lock and calls dispose() which then acquires a write lock, this maybe ok, as I assume there is only 1 thread that will attempt to dispose of idle workspaces.
"
1,"When BG merge hits an exception, optimize sometimes throws an IOException missing the root cause
When IndexWriter.optimize() is called, ConcurrentMergeScheduler will
run the requested merges with background threads and optimize() will
wait for these merges to complete.

If a merge hits an exception, it records the root cause exception such
that optimize can then retrieve this root cause and throw its own
exception, with the root cause.

But there is a bug: sometimes, the fact that an exception occurred on
a merge is recorded, but the root cause is missing.  In this cause,
optimize() still throws an exception (correctly indicating that the
optimize() has not finished successfully), but it's not helpful
because it's missing the root cause.  You must then go find the root
cause in the JRE's stderr logs.

This has hit a few users on this lists, most recently:

  http://www.nabble.com/Background-merge-hit-exception-td19540409.html#a19540409

I found the isssue, and finally got a unit test to intermittently show
it.  It's a simple thread safety issue: in a finally clause in
IndexWriter.merge we record the fact that the merge hit an exception
before actually setting the root cause, and then only in
ConcurrentMergeScheduler's exception handler do we set the root
cause.  If the optimize thread is scheduled in between these two, it
can throw an exception missing its root cause.

The fix is straightforward.  I plan to commit to 2.4 & 2.9.
"
0,"Expose explicit 2-phase commit in IndexWriterCurrently when IndexWriter commits, it does so with a two-phase
commit, internally: first it prepares all the new index files, syncs
them; then it writes a new segments_N file and syncs that, and only if
that is successful does it remove any now un-referenced index files.

However, these two phases are done privately, internal to the commit()
method.

But when Lucene is involved in a transaction with external resources
(eg a database), it's very useful to explicitly break out the prepare
phase from the commit phase.

Spinoff from this thread:

  http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200804.mbox/%3C16627610.post@talk.nabble.com%3E

"
0,"Add <a name=""""> anchors to documentation sectionsIn all docs, sections are missing <a name> anchors. I see that the xdocs stylesheet in the repository is supposed to generate them, yet the site is missing them at this moment.

See https://svn.apache.org/repos/asf/jakarta/site/xdocs/stylesheets/site.xsl 
template match=""section"""
0,"SorterTemplate.quickSort stack overflows on broken comparators that produce only few disticnt values in large arraysLooking at Otis's sort problem on the mailing list, he said:
{noformat}
* looked for other places where this call is made - found it in
MultiPhraseQuery$MultiPhraseWeight and changed that call from
ArrayUtil.quickSort to ArrayUtil.mergeSort
* now we no longer see SorterTemplate.quickSort in deep recursion when we do a
thread dump
{noformat}

I thought this was interesting because PostingsAndFreq's comparator
looks like it needs a tiebreaker.

I think in our sorts we should add some asserts to try to catch some of these broken comparators."
0,"Improve org.apache.lucene.search.Filter Documentation and Tests to reflect per segment readersFilter Javadoc does not mention that the Reader passed to getDocIDSet(Reader) could be on a per-segment basis.
This caused confusion on the users-list -- see http://lucene.markmail.org/message/6knz2mkqbpxjz5po?q=date:200912+list:org.apache.lucene.java-user&page=1
We should improve the javadoc and also add a testcase that reflects filtering on a per-segment basis."
1,"Saving a node deletion that has been modified externally throws a ConstraintViolationExceptionDeleting a node ""a"" and saving its parent might result in a ConstraintViolationException if node ""a"" has been modified externally, where an InvalidItemStateException with message ""item x has been modified externally"" would be more intuitive.
"
1,"Transient states should be persisted in depth-first traversal orderInside Node.save(), when filling the list of transient (modified) items, the node itself is added first (if transient) and all transient descendant nodes in depth-first order. This can lead to the following problem with shareable nodes and path-based access management: 

1) assume a node N has a shared child S, which is shared with at least one other node N'
2) S.removeShare is invoked: this removes S from the list of child nodes in N
3) N.save is invoked

N is persisted first, then S. If a path-based access manager tries to build the path of S after N has been persisted, S will no longer be returned in the list of removed child node entries, and an exception will be thrown. This can be circumvented by adding N last."
1,"SQL2 query - supplying column selector fails with NPE on getColumnName()I am preparing and executing an SQL2 query (JCR 2.0) as follows:

QueryManager qm = jcrSession.getWorkspace().getQueryManager();
String queryString = ""select order.[customerAccountUUID] as cust from [atl:order] as order"";
Query query = qm.createQuery(queryString, Query.JCR_SQL2);
QueryResult queryResult = query.execute();

The following query fails:

select order.[customerAccountUUID] from [atl:order] as order

java.lang.NullPointerException
        org.apache.jackrabbit.commons.query.sql2.QOMFormatter.isSimpleName(QOMFormatter.java:577)
        org.apache.jackrabbit.commons.query.sql2.QOMFormatter.formatName(QOMFormatter.java:567)
        org.apache.jackrabbit.commons.query.sql2.QOMFormatter.format(QOMFormatter.java:452)
        org.apache.jackrabbit.commons.query.sql2.QOMFormatter.format(QOMFormatter.java:123)
        org.apache.jackrabbit.commons.query.sql2.QOMFormatter.format(QOMFormatter.java:117)

line 452: c.getColumnName() returns null.

The following query is fine:

select order.[customerAccountUUID] as cust from [atl:order] as order

I have been using the test case (here: http://svn.apache.org/viewvc/jackrabbit/trunk/jackrabbit-spi-commons/src/test/resources/org/apache/jackrabbit/spi/commons/query/sql2/test.sql2.txt?view=markup) as a guideline.

Cheers,

James "
1,"Empty response body is not properly handled when chunked encoding is usedIIS 5.0 server, when returning no content in response to an HTTP/1.1 request,
still includes ""Transfer-Encoding: chunked"" response header. As HttpClient
always expects chunk-encoded stream to be properly terminated, an
HttpRecoverableException exception results, when no content is sent back

=====================================================================

POST /someurl.aspx HTTP/1.1
Content-Length: 1132
Host: xxx.xxx.xxx.xxx
User-Agent: Jakarta Commons-HttpClient/2.0alpha2
Content-Type: multipart/form-data; boundary=----------------314159265358979323846

------------------314159265358979323846
Content-Disposition: form-data; name=""nmFile""; filename=""xxxxxxxxx.xml""
Content-Type: application/octet-stream

<... content removed ...>

------------------314159265358979323846--

HTTP/1.1 200 OK
Server: Microsoft-IIS/5.0
Date: Sat, 08 Feb 2003 15:22:26 GMT
Transfer-Encoding: chunked
Cache-Control: private
Content-Type: text/html

=====================================================================

Bug reported by Jim Crossley"
0,"NearSpansOrdered does not lazy load payloads as the PayloadSpans javadoc impliesBest would be to lazy load, but I don't see how with the current algorithm. Short that, we should add an option to ignore payloads - otherwise, if you are doing non payload searching, but the payloads are present, they will be needlessly loaded.

Already added this to LUCENE-1748, but spinning from that issue to this - patch to follow when LUCENE-1748 is committed."
1,"coord should still apply to missing terms/clausesMissing terms in a boolean query ""disappear"" (i.e. they don't even affect the coord factor)."
0,"FVH: uncontrollable color tagsThe multi-colored tags is a feature of FVH. But it is uncontrollable (or more precisely, unexpected by users) that which color is used for each terms."
1,"Phrase query with term repeated 3 times requires more slop than expectedConsider a document with the text ""A A A"".
The phrase query ""A A A"" (exact match) succeeds.
The query ""A A A""~1 (same document and query, just increasing the slop value by one) fails.
""A A A""~2 succeeds again.

If the exact match succeeds, I wouldn't expect the same query but with more slop to fail.  The fault seems to require some term to be repeated at least three times in the query, but the three occurrences do not need to be adjacent.  I will attach a file that contains a set of JUnit tests that demonstrate what I mean."
0,"Add more unit tests on BeanConvertersSome BeanConverters are not yet stable. We have to add more unit tests.  It seems that null values for bean attributes are not well supported. We have to test that for all BeanConverters. 

Here is a good scenario to test : 

Model : Class A contains an attribute ""b"" based on class B. 

Create an instance of A with ""b"" = null
insert A / save
Get instance of A
set the attribute of B
update A /save




"
1,"Lucene is not fsync'ing files on commitThanks to hurricane Irene, when Mark's electricity became unreliable, he discovered that on power loss Lucene could easily corrumpt the index, which of course should never happen...

I was able to easily repro, by pulling the plug on an Ubuntu box during indexing.  On digging, I discovered, to my horror, that Lucene is failing to fsync any files, ever!

This bug was unfortunately created when we committed LUCENE-2328... that issue added tracking, in FSDir, of which files have been closed but not sync'd, so that when sync is called during IW.commit we only sync those files that haven't already been sync'd.

That tracking is done via the FSDir.onIndexOutputClosed callback, called when an FSIndexOutput is closed.  The bug is that we only call it on exception during close:

{noformat}

    @Override
    public void close() throws IOException {
      // only close the file if it has not been closed yet
      if (isOpen) {
        boolean success = false;
        try {
          super.close();
          success = true;
        } finally {
          isOpen = false;
          if (!success) {
            try {
              file.close();
              parent.onIndexOutputClosed(this);
            } catch (Throwable t) {
              // Suppress so we don't mask original exception
            }
          } else
            file.close();
        }
      }
    }
{noformat}

And so FSDir thinks no files need syncing when its sync method is called....

I think instead we should call it up-front; better to over-sync than under-sync.

The fix is trivial (move the callback up-front), but I'd love to somehow have a test that can catch such a bad regression in the future.... still I think we can do that test separately and commit this fix first.

Note that even though LUCENE-2328 was backported to 2.9.x and 3.0.x, this bug wasn't, ie the backport was a much simpler fix (to just address the original memory leak); it's 3.1, 3.2, 3.3 and trunk when this bug is present."
0,"Cleanup DR.getCurrentVersion/DR.getUserData/DR.getIndexCommit().getUserData()Spinoff from Ryan's dev thread ""DR.getCommitUserData() vs DR.getIndexCommit().getUserData()""... these methods are confusing/dups right now."
0,promote TestExternalCodecs.PerFieldCodecWrapper to corePerFieldCodecWrapper lets you set the Codec for each field; I'll promote to core & mark experimental.
1,"Bug in UtilDateTypeConverterImplIn this converter following line is used:
return this.getValueFactory().createValue(((java.util.Date) propValue).getTime());
but propValue must be converted to java.util.Calendar, not into long! ValueFactory than converts to LongValue not DateValue as expected.

Following code works OK:
final long timeInMilis = ((java.util.Date) propValue).getTime();
final Calendar calendar = Calendar.getInstance();
calendar.setTimeInMillis(timeInMilis);
return this.getValueFactory().createValue( calendar );

but I dont know better Date-> Calendar conversion.
"
0,"Binary value may leave temp file behindThe following call leaves a temp file behind that is never deleted:

InputStream in = ...
ValueFactory vf = ....
vf.createBinary(in).dispose();

Only happens when the datastore is disabled."
0,"remove deprecated classes from spatialspatial has not been released, so we can remove the deprecated classes"
1,"PathFactoryImpl creates illegal Path objectsit is currently possible to create illegal/inconsistent paths using the default path factory.
Path objects are expected to represent syntactically correct paths.

some examples:

            PathFactory pf = PathFactoryImpl.getInstance();
            Path.Element re = pf.getRootElement();
            Path illegalPath = pf.create(new Path.Element[]{re, re});
            
            Path.Element pe = pf.getParentElement();
            Path nonNormalizedPath = pf.create(new Path.Element[]{pe, pe});    // ""../..""
            assertFalse(nonNormalizedPath.isNormalized());

"
1,"FSDirectory.getDirectory always creates index pathThis was reported to me as a Luke bug, but going deeper it proved to be a non-intuitive (broken?) behavior of FSDirectory.

If you use FSDirectory.getDirectory(File nonexistent) on a nonexistent path, but one that is located under some existing parent path, then FSDirectory:174 uses file.mkdirs() to create this directory. One would expect a variant of the method with a boolean flag to decide whether or not to create the output path. However, the API with ""create"" flag is now deprecated, with a comment that points to IndexWriter's ""create"" flag. This comment is misleading, because the indicated path is created anyway in the file system just by calling FSDirectory.getDirectory().

I propose to do one of the following:

* reinstate the variant of the method with ""create"" flag. In case if this flag is false, and the index directory is missing, either return null or throw an IOException,

* keep the API as it is now, but either return null or throw IOException if the index dir is missing. This breaks the backwards compatibility, because now users are required to do file.mkdirs() themselves prior to calling FSDirectory.getDirectory()."
1,"TCK: Incorrect check of namespace mappings in System View XML exportorg.apache.jackrabbit.test.api.SysViewContentHandler. In endDocument(), two issues:

1. line 351: tries to go through a table of prefixes but uses a fixed index inside the loop;
2. The mapping for the 'xml' prefix should be skipped (it must be registered in the Session but must not be registered during export since this is a built-in XML mapping."
0,"Implement anonymous login with credentialsJackrabbit currently implements anonymous login by detecting a null credentials argument on login. This is actually not compliant to the specification.

<spec>
If credentials is null, it is assumed that authentication is handled by a mechanism external to the repository itself (for example, through the JAAS framework) and that the repository implementation exists within a context (for example, an application server) that allows it to handle authorization of the request for access to the specified workspace.
</spec>

Jackrabbit should rather support anonymous login with a defined credential, either some subclass of SimpleCredentials or a predefined / known userId that has read-only access."
0,Refactor RewriteMethods out of MultiTermQueryPoliceman work :-) - as usual
1,"ParameterParser parse method for authentication headers does not appear to deal with empty value stringsHi, I have found an issue with HTTPClient due to the way it parses parameter 
strings.

In particular, consider the following WWW-Authenticate header:

WWW-Authenticate: Digest realm="""", algorithm=MD5, qop=""auth"", 
domain=""/content"", nonce=""0e11dcf146563c3a89e5327f0c5f5bad""
 
The realm is definitely specified, but is equal to the empty string.  It is not 
a null value.

However, the extractParams method of AuthChallengeParser which in turn calls 
ParameterParser will actually parse the value as Null  instead of an empty 
string.

This is due to parseQuotedToken getToken(true) call which essentially returns a 
null String result  as the condition i2>i1 fails :-

        String result = null;
        if (i2 > i1) {
            result = new String(chars, i1, i2 - i1);
        }
        return result;

As the processChallenge method of DigestScheme throws an exception when 
getParameter(""realm"") == null, HTTPClient is not able to process the digest 
request when an empty string realm value is present."
0,"Change remaining contrib streams/filters to use new TokenStream APIAll other contrib streams/filters have already been converted with LUCENE-1460.

The two shingle filters are the last ones we need to convert."
0,"another highlighterI've written this highlighter for my project to support bi-gram token stream (general token stream (e.g. WhitespaceTokenizer) also supported. see test code in patch). The idea was inherited from my previous project with my colleague and LUCENE-644. This approach needs highlight fields to be TermVector.WITH_POSITIONS_OFFSETS, but is fast and can support N-grams. This depends on LUCENE-1448 to get refined term offsets.

usage:
{code:java}
TopDocs docs = searcher.search( query, 10 );
Highlighter h = new Highlighter();
FieldQuery fq = h.getFieldQuery( query );
for( ScoreDoc scoreDoc : docs.scoreDocs ){
  // fieldName=""content"", fragCharSize=100, numFragments=3
  String[] fragments = h.getBestFragments( fq, reader, scoreDoc.doc, ""content"", 100, 3 );
  if( fragments != null ){
    for( String fragment : fragments )
      System.out.println( fragment );
  }
}
{code}

features:
- fast for large docs
- supports not only whitespace-based token stream, but also ""fixed size"" N-gram (e.g. (2,2), not (1,3)) (can solve LUCENE-1489)
- supports PhraseQuery, phrase-unit highlighting with slops
{noformat}
q=""w1 w2""
<b>w1 w2</b>
---------------
q=""w1 w2""~1
<b>w1</b> w3 <b>w2</b> w3 <b>w1 w2</b>
{noformat}
- highlight fields need to be TermVector.WITH_POSITIONS_OFFSETS
- easy to apply patch due to independent package (contrib/highlighter2)
- uses Java 1.5
- looks query boost to score fragments (currently doesn't see idf, but it should be possible)
- pluggable FragListBuilder
- pluggable FragmentsBuilder

to do:
- term positions can be unnecessary when phraseHighlight==false
- collects performance numbers
"
0,"httpclient doesn't read and parse response from certain types of proxy servers when POST method is usedIt was determined that when sending post data to server via Squid proxy server
of version 2.4.STABLE2 and Squid responds 
with ""407 proxy authentication required"" response, httpclient doesn't read this
response in order to parse, but rather
fails with soket exception ""java.net.SocketException: Software caused connection
abort: recv failed"".

This behaviour is reproduced with the latest nigtly build of httpclient version
3.0. (from 9 of February 2005) as
well as 3.0. RC1, 2.0.2 and 2.0.

This is the piece of code that sends post data using httpclient:

try
{
	HttpClientParams httpClientParams = new HttpClientParams();
	HttpClient client = new HttpClient(httpClientParams);

	HostConfiguration hostconfig = new HostConfiguration();
	hostconfig.setProxy(""db00-devl.eps.agfa.be"", 3128); // SQUID proxy server
version 2.4.STABLE2
	client.setHostConfiguration(hostconfig);
	PostMethod postMethod = new
PostMethod(""http://brugge.eps.agfa.be/portal03/servlet/selectFiles"");

	postMethod.addParameter(""data"", ""some data"");
	int status = client.executeMethod(postMethod);
	System.out.println(""status = "" + status);
	if (status == HttpStatus.SC_OK)
		System.out.println(""Ok"");
	else if (status == HttpStatus.SC_PROXY_AUTHENTICATION_REQUIRED)
		System.out.println(""Proxy authentication required."");
}
catch (Exception e)
{
	System.out.println(""Socket exception."");
	e.printStackTrace();
}

Look at ""debug log of the problem"" attachment to see all output from httpclient
and mentioned piece of code.
In ""problem_request_response_interaction"" attacment it is possible to see
interaction beetween httpclient and Squid proxy server: httpclient sends initial
request and headers, then squid responds with ""proxy authentication required""
response and afterwards httpclient tries to send post data(without reading the
response) but fails because connection is already closed.

For more details look at ""ethereal_problem"" attachment for all network traffic
during running of mentioned piece of code:
Ethereal protocol analyzer can be used to open this file(http://www.ethereal.com/).

Most likely this particular version of Squid closes connection after it sends
proxy athentication response back,
which causes httpclient to fail while sending post data.

Let's have a look at what writeRequest(...) method of HttpMethodBase class does:

1) sends request line and headers to server,
2) handles 'Expect: 100-continue' handshake if needed,
3) sends post data to server.

My question is should HTTPClient send initial request and headers before data
even if it is not going to read 
a response from the server(proxy server), or this should be done only in case of
'Expect: 100-continue' handshake 
(this seems the only case when HTTPClient is going to listen to server
in-between of steps 1 and 3)?

My understanding is that the command

        // make sure the status line and headers have been sent
        conn.flushRequestOutputStream();
        
which actually splits sending of data in two parts are needed only for 'Expect:
100-continue' handshake case.
Just by moving ""flush"" command to appropriate place inside 'Expect:
100-continue' handshake case:
		.....
                try {
	            conn.flushRequestOutputStream(); // moved
                    conn.setSocketTimeout(RESPONSE_WAIT_TIME_MS);
		.....
it is posible to solve described problem.

I created PostMethodEx that overrides writeRequest(...) method of
HttpMethodBase(look at ""PostMethodEx"" attachment) 
and for all cases but the 'Expect: 100-continue' handshake it sends request
line, headers and post data to server 
at once.

When mentioned piece of code(with PostMethod changed to PostMethodEx) is
executed everyting works fine:
look at ""debug log of the fix"", ""fix_request_response_interaction"" and
""ethereal_fix""(all network trafic) 
attachments.

According to mentioned logs httpclient sends all post data at once and then
reads and parses ""proxy authentication required"" 
response from squid and sets status code to 407. Correct."
0,[PATCH] Comment corrections in MMapDirectory.javaThese comments ended up on the wrong lines after the last changes
0,"Cleanup highlighter test classcleanup highlighter test class - did some of this in another issue, but there is a bit more to do"
0,"Move 'good' contrib/queries classes to Queries moduleWith the Queries module now filled with the FunctionQuery stuff, we should look at closing down contrib/queries.  While not a huge contrib, it contains a number of pretty useful classes and some that should go elsewhere.

Heres my proposed plan:

- similar.* -> suggest module
- regex.* -> queries module
- BooleanFilter -> queries module under .filters package
- BoostingQuery -> queries module
- ChainedFilter -> queries module under .filters package
- DuplicateFilter -> queries module under .filters package
- FieldCacheRewriteMethod -> This doesn't belong in this contrib or the queries module.  I think we should push it to contrib/misc for the time being.  It seems to have quite a few constraints on when its useful.  If indeed CONSTANT_SCORE_AUTO rewrite is better, then I dont see a purpose for it.
- FilterClause -> class inside BooleanFilter
- FuzzyLikeThisQuery -> suggest module. This class seems a mess with its Similarity hardcoded.  With all that said, it does seem to do what it claims and with some cleanup, it could be good.
- TermsFilter -> queries module under .filters package
- SlowCollated* -> They can stay in the module till we have a better place to nuke them.

One of the implications of the above moves, is that the xml-query-parser, which supports many of the queries, will need to have a dependency on the queries module.  But that seems unavoidable at this stage.



"
0,Add init method to CloseableThreadLocalJava ThreadLocal has an init method that allows subclasses to easily instantiate an initial value.  
1,"HttpMethod#getResponseBody throws NPEHttpMethod#getResponseBody throws an NPE if the response from the server was 
204.  Shouldn't getResponseBody return null by contract instead of throwing 
NPE?"
1,"Incorrect node position after importI have found a behavior that does not seem to be consistent with the
spec:

After replacing a node with importXML using IMPORT_UUID_COLLISION_REPLACE_EXISTING the new node is not at the position of the replaced node (talking about the position among the siblings).

The origininal node is removed, but the new node is created as the last child of the parent node, and not spec-compliant at the position of the replaced node.

Here how I use it:

// assume Session s, Node n, String text (holding XML data)

s.importXML(
	n.getPath(), 
	new ByteArrayInputStream (text.getBytes(""UTF-8"")),
	ImportUUIDBehavior.IMPORT_UUID_COLLISION_REPLACE_EXISTING
);
s.save();
 

And here a quote from the spec section 7.3.6

ImportUUIDBehavior.IMPORT_UUID_COLLISION_REPLACE_EXISTING: 
If an incoming referenceable node has the same UUID as a node already existing in the workspace then the already existing node is replaced by the incoming node in the same position as the existing node.
[note ""same position""]
"
1,Query may throw ArrayIndexOutOfBoundsExceptionThere's a bug in DescendantSelfAxisQuery.DescendantSelfAxisScorer.skipTo() that causes the exception.
0,"Create a Size Estimator model for Lucene and SolrIt is often handy to be able to estimate the amount of memory and disk space that both Lucene and Solr use, given certain assumptions.  I intend to check in an Excel spreadsheet that allows people to estimate memory and disk usage for trunk.  I propose to put it under dev-tools, as I don't think it should be official documentation just yet and like the IDE stuff, we'll see how well it gets maintained."
1,NearSpansOrdered.getPayload does not return the payload from the minimum match span
0,"Need stopwords and stoptags lists for default Japanese configurationStopwords and stoptags lists for Japanese needs to be developed, tested and integrated into Lucene."
1,"CharFilters not being invoked in Solr
On Solr trunk, *all* CharFilters have been non-functional since LUCENE-3396 was committed in r1175297 on 25 Sept 2011, until Yonik's fix today in r1235810; Solr 3.x was not affected - CharFilters have been working there all along."
1,"SpanMultiTermQueryWrapper with Prefix Query issueIf we try to do a search with SpanQuery and a PrefixQuery this message is returned:

""You can only use SpanMultiTermQueryWrapper with a suitable SpanRewriteMethod.""

The problem is in the WildcardQuery rewrite function.

If the wildcard query is a prefix, a new prefix query is created, the rewrite method is set with the SpanRewriteMethod and the prefix query is returned.

But, that's the rewritten prefix query which should be returned:

-      return rewritten;
+      return rewritten.rewrite(reader);

I will attach a patch with a unit test included.



"
0,"JSR 283: JCR Pathwith jsr 283 the jcr path is defined to consist of a combination of the following segments

	a name segment, (J, I), where J is a JCR name and I is an integer index (I  1).
	an identifier segment, U, where U is a JCR identifier.
	the root segment.
	the self segment.
	the parent segment.

-> the name segment can be in extended or qualified form -> see issue JCR-1712
-> the identifier segment is new for jsr283 and always identifies a node (-> see new method Node.getIdentifier())

Non-standard parts always need to be standardized. Any of the following makes a path non-standard:
- expanded name segments
- trailing /
- index [1]

Identifier-segments
- get resolved upon being passed to any API calls that take path to an existing Node
- don't get resolved when being used to create a PATH value object.

Except for PATH values, all jcr paths returned by the API are normalized and standard, thus never identifier-based.

PATH values in contrast:
- must be converted to standard form
- must NOT be normalized. i.e. redundant segments and identifiers must be preserved.
"
0,"Missing possibility to supply custom FieldParser when sorting search resultsWhen implementing the new TrieRangeQuery for contrib (LUCENE-1470), I was confronted by the problem that the special trie-encoded values (which are longs in a special encoding) cannot be sorted by Searcher.search() and SortField. The problem is: If you use SortField.LONG, you get NumberFormatExceptions. The trie encoded values may be sorted using SortField.String (as the encoding is in such a way, that they are sortable as Strings), but this is very memory ineffective.

ExtendedFieldCache gives the possibility to specify a custom LongParser when retrieving the cached values. But you cannot use this during searching, because there is no possibility to supply this custom LongParser to the SortField.

I propose a change in the sort classes:
Include a pointer to the parser instance to be used in SortField (if not given use the default). My idea is to create a SortField using a new constructor
{code}SortField(String field, int type, Object parser, boolean reverse){code}

The parser is ""object"" because all current parsers have no super-interface. The ideal solution would be to have:

{code}SortField(String field, int type, FieldCache.Parser parser, boolean reverse){code}

and FieldCache.Parser is a super-interface (just empty, more like a marker-interface) of all other parsers (like LongParser...). The sort implementation then must be changed to respect the given parser (if not NULL), else use the default FieldCache.getXXXX without parser."
1,"DateTools needs to use UTC for correct collation,If your local timezone is Europe/London then the times Sun, 30 Oct 2005 00:00:00 +0000 and exactly one hour later are both converted to 200530010000 by DateTools.dateToString() with minute resolution.   The Linux date command is useful in seeing why:

    $ date --date ""Sun, 30 Oct 2005 00:00:00 +0000""
    Sun Oct 30 01:00:00 BST 2005

    $ date --date ""Sun, 30 Oct 2005 01:00:00 +0000""
    Sun Oct 30 01:00:00 GMT 2005

Both times are 1am in the morning, but one is when DST is in force, the other isn't.   Of course, these are actually different times!

Of course, if dates are stored in the index with implicit timezone information then not only do we get problems when the clocks go back at the end of summer, but we also have problems crossing timezones.   If a database is created in California and used in Paris then the times are going to be badly skewed (there's a nine hour time difference most of the year).
"
0,Discovery of privileges of any set of Principalsjsr 283 defines means to discover the privileges for the editing session. however there is no way to determine the privileges for other principals.
0,"Source packaging fails if ${dist.dir} does not existpackage-tgz-src and package-zip-src fail if ${dist.dir} does not exist, since these two targets do not call the package target, which is responsible for making the dir.

I have a fix and will commit shortly."
0,Remove QueryResultImpl and rename LazyQueryResultImpl to QueryResultImplQueryResultImpl isn't used in Jackrabbit anymore. Instead LazyQueryResultImpl is now used. See the discussion in JCR-1073.
0,"optimize automatonqueryMike found a few cases in flex where we have some bad behavior with automatonquery.
The problem is similar to a database query planner, where sometimes simply doing a full table scan is faster than using an index.

We can optimize automatonquery a little bit, and get better performance for fuzzy,wildcard,regex queries.

Here is a list of ideas:
* create commonSuffixRef for infinite automata, not just really-bad linear scan cases
* do a null check rather than populating an empty commonSuffixRef
* localize the 'linear' case to not seek, but instead scan, when ping-ponging against loops in the state machine
* add a mechanism to enable/disable the terms dict cache, e.g. we can disable it for infinite cases, and maybe fuzzy N>1 also.
* change the use of BitSet to OpenBitSet or long[] gen for path-tracking
* optimize the backtracking code where it says /* String is good to go as-is */, this need not be a full run(), I think...
"
1,"ChunkedInputStream incorrectly handles chunksize without semicolonChunkedInputStream does not correctly read the chunk size when a semicolon does
not appear in the first line of the chunk.  If whitespace exists between the
chunk size value and the end of line and no semicolon is present, the whitespace
is not removed before parseInt is called resulting in an IOException ""Bad chunk
size""

I can not tell from RFC2616 if whitespace is legal here, but I have received it
from at least one web server.  The relevant section is 3.6.1.

A small patch repairs the problem.  I will attach it immediately."
0,"Cloned SegmentReaders fail to share FieldCache entriesI just hit this on LUCENE-1516, which returns a cloned readOnly
readers from IndexWriter.

The problem is, when cloning, we create a new [thin] cloned
SegmentReader for each segment.  FieldCache keys directly off this
object, so if you clone the reader and do a search that requires the
FieldCache (eg, sorting) then that first search is always very slow
because every single segment is reloading the FieldCache.

This is of course a complete showstopper for LUCENE-1516.

With LUCENE-831 we'll switch to a new FieldCache API; we should ensure
this bug is not present there.  We should also fix the bug in the
current FieldCache API since for 2.9, users may hit this.
"
0,"Deprecate all non-bundle persistence managersBundle persistence has been the recommended default since Jackrabbit 1.3, and there is little reason for anyone to be using non-bundle persistence anymore. Thus I'd like to deprecate all non-bundle PMs in Jackrabbit 2.2 and target for their removal in Jackrabbit 3.0."
1,"""overwriting cached entry"" warningswhen using multiple concurrent sessions you'll find *lots* of log entries like:

    03.11.2010 21:17:03 *WARN * ItemStateReferenceCache: overwriting cached entry ad79ca57-5eb1-4b7d-a439-a9fd73cc8c5a (ItemStateReferenceCache.java, line 176)

those are actually legitimate warnings since there's a siginificant risk of data loss/inconsistency involved.

this is apparently a regression of changes introduced by JCR-2699, specifically svn r1004223"
1,"Moved node disappearsMoving a node and then refreshing it can make it disappear.

deleteDirectory(new File(""repository""));
Repository rep = new TransientRepository();
Session session = rep.login(new SimpleCredentials("""", new char[0]));
Node root = session.getRootNode();
Node a = root.addNode(""a"");
Node b = a.addNode(""b"");
session.save();
session.move(""/a/b"", ""/b"");
b.refresh(false);
// session.save(); // no effect
for (NodeIterator it = root.getNodes(); it.hasNext();) {
    Node n = it.nextNode();
    System.out.println(n.getName());
    for (NodeIterator it2 = n.getNodes(); it2.hasNext();) {
        System.out.println(""  "" + it2.nextNode().getName());
    }
}

In the trunk, the node 'b' is not listed after the refresh (not under the root page, and not under a). The output is:
jcr:system
  jcr:versionStorage
  jcr:nodeTypes
a


Jackrabbit 1.4.x throws an exception:

jcr:system
  jcr:versionStorage
  jcr:nodeTypes
a
Exception in thread ""main"" javax.jcr.RepositoryException: failed to resolve name of acee31c4-c33b-4ed4-b1b5-39db6f17fb09
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getName(HierarchyManagerImpl.java:451)
	at org.apache.jackrabbit.core.CachingHierarchyManager.getName(CachingHierarchyManager.java:287)
	at org.apache.jackrabbit.core.NodeImpl.getName(NodeImpl.java:1931)
	at org.apache.jackrabbit.core.fuzz.TestMoveRemoveRefresh.test(TestMoveRemoveRefresh.java:33)
	at org.apache.jackrabbit.core.fuzz.TestMoveRemoveRefresh.main(TestMoveRemoveRefresh.java:15)


void deleteDirectory(File file) {
    if (file.isDirectory()) {
        File[] list = file.listFiles();
        for(int i=0; i<list.length; i++) {
            deleteDirectory(list[i]);
        }
    }
    file.delete();
}
"
1,"JCR2SPI; setProperty(name, date-string) fails when property is added and property type is PropertyType.DATE.Example code:

        Node l_parent = (Node)session.getItem(this.m_path);
        
        Node l_test = l_parent.addNode(""createcontenttest"", ""nt:file"");
        Node l_content = l_test.addNode(""jcr:content"", ""nt:resource"");
        
        l_content.setProperty(""jcr:encoding"", ""UTF-8"");
        l_content.setProperty(""jcr:mimeType"", ""text/plain"");
        l_content.setProperty(""jcr:data"", new ByteArrayInputStream(""foobar"".getBytes()));
        l_content.setProperty(""jcr:lastModified"", ""2007-07-25T17:04:00.000Z""); // TODO: this should work as well, bug in JCR2SPI?
        session.save();

This will fail when the property is defined as DATE, what should happen is that a value comparison is attempted (note that it works when the property already exists and just is overwritten).

The exception is:

javax.jcr.nodetype.ConstraintViolationException: no matching property definition found for {http://www.jcp.org/jcr/1.0}lastModified
        at org.apache.jackrabbit.jcr2spi.nodetype.ItemDefinitionProviderImpl.getQPropertyDefinition(ItemDefinitionProviderImpl.java:269)
        at org.apache.jackrabbit.jcr2spi.nodetype.ItemDefinitionProviderImpl.getQPropertyDefinition(ItemDefinitionProviderImpl.java:159)
        at org.apache.jackrabbit.jcr2spi.NodeImpl.getApplicablePropertyDefinition(NodeImpl.java:1672)
        at org.apache.jackrabbit.jcr2spi.NodeImpl.createProperty(NodeImpl.java:1369)
        at org.apache.jackrabbit.jcr2spi.NodeImpl.setProperty(NodeImpl.java:264)
        at org.apache.jackrabbit.jcr2spi.NodeImpl.setProperty(NodeImpl.java:345)
        at org.apache.jackrabbit.jcr2spi.NodeImpl.setProperty(NodeImpl.java:336)
"
0,"Similarity.java javadocs and simplifications for 4.0As part of adding additional scoring systems to lucene, we made a lower-level Similarity
and the existing stuff became e.g. TFIDFSimilarity which extends it.

However, I always feel bad about the complexity introduced here (though I do feel there
are some ""excuses"", that its a difficult challenge).

In order to try to mitigate this, we also exposed an easier API (SimilarityBase) on top of 
it that makes some assumptions (and trades off some performance) to try to provide something 
consumable for e.g. experiments.

Still, we can cleanup a few things with the low-level api: fix outdated documentation and
shoot for better/clearer naming etc.
"
0,"Remove deprecated Directory stuff and IR/IW open/ctor hellThis patch removes primarily the deprecated Directory stuff. This also removes parts of the ctor/open hell in IR and IW. IndexModifier is completely removed as deprecated, too."
0,"Add sort missing first/last ability to SortField and ValueComparatorWhen SortField and ValueComparator use EntryCreators (from LUCENE-2649) they use a special sort value when the field is missing.

This enables lucene to implement 'sort missing last' or 'sort missing first' for numeric values from the FieldCache.
"
0,"ReorderReferenceableSNSTest failureI have checked out the Jackrabbit 1.4 branch to a new directory, and called:

mvn clean install

The error is:

Building Jackrabbit JCR to SPI
  task-segment: [clean, install]
---------------------------------
...
testRevertReorder(org.apache.jackrabbit.jcr2spi.ReorderReferenceableSNSTest)
junit.framework.AssertionFailedError: Reorder added a child node.
       at junit.framework.Assert.fail(Assert.java:47)
       at org.apache.jackrabbit.jcr2spi.ReorderTest.testOrder(ReorderTest.java:90)
       at org.apache.jackrabbit.jcr2spi.ReorderTest.testRevertReorder(ReorderTest.java:122)
"
0,"Stop using BaseExceptionThe o.a.j.BaseException class is deprecated (since JCR-1169) and not caught anywhere, so there's no need to keep using it."
0,"Incorrect decodedAttributeValue in AbstractImportXmlTestThe string literal is not correctly escaped.

Later on the decoded attribute value should be used to check the imported value. There is currently an odd test that checks the encoded attribute value twice."
0,"lucene benchmark has some unnecessary fileslucene/contrib/benchmark/.rsync-filter is only in the source pack (and in SVN), I was not aware of this file, though it was added long ago in https://issues.apache.org/jira/browse/LUCENE-848?focusedCommentId=12491404&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-12491404
Not a blocker for this RC, just interesting to note.

maybe this is related to LUCENE-3155 too, in that we could consider this one for automatic exclusion (like DS_Store), but we should fix it if its committed in SVN too.
"
0,"Factor out ByteSliceWriter from DocumentsWriterFieldDataDocumentsWriter uses byte slices into shared byte[]'s to hold the
growing postings data for many different terms in memory.  This is
probably the trickiest (most confusing) part of DocumentsWriter.

Right now it's not cleanly factored out and not easy to separately
test.  In working on this issue:

  http://mail-archives.apache.org/mod_mbox/lucene-java-user/200805.mbox/%3c126142c0805061426n1168421ya5594ef854fae5e4@mail.gmail.com%3e

which eventually turned out to be a bug in Oracle JRE's JIT compiler,
I factored out ByteSliceWriter and created a unit test to stress test
the writing & reading of byte slices.  The test just randomly writes N
streams interleaved into shared byte[]'s, then reads them back
verifying the results are correct.

I created the stress test to try to find any bugs in that code.  The
test ran fine (no bugs were found) but I think the refactoring is
still very much worthwhile.

I expected the changes to reduce indexing throughput, so I ran a test
indexing first 200K Wikipedia docs using this alg:

{code}
analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
doc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker

docs.file=/Volumes/External/lucene/wiki.txt
doc.stored = true
doc.term.vector = true
doc.add.log.step=2000

directory=FSDirectory
autocommit=false
compound=true

ram.flush.mb=256

{ ""Rounds""
  ResetSystemErase
  { ""BuildIndex""
    - CreateIndex
     { ""AddDocs"" AddDoc > : 200000
    - CloseIndex
  }
  NewRound
} : 4

RepSumByPrefRound BuildIndex

{code}

Ok trunk it produces these results:
{code}
Operation   round   runCnt   recsPerRun        rec/s  elapsedSec    avgUsedMem    avgTotalMem
BuildIndex      0        1       200000        791.7      252.63   338,552,096  1,061,814,272
BuildIndex -  - 1 -  -   1 -  -  200000 -  -   793.1 -  - 252.18 - 605,262,080  1,061,814,272
BuildIndex      2        1       200000        794.8      251.63   601,966,528  1,061,814,272
BuildIndex -  - 3 -  -   1 -  -  200000 -  -   782.5 -  - 255.58 - 608,699,712  1,061,814,272
{code}

and with the patch:

{code}
Operation   round   runCnt   recsPerRun        rec/s  elapsedSec    avgUsedMem    avgTotalMem
BuildIndex      0        1       200000        745.0      268.47   338,318,784  1,061,814,272
BuildIndex -  - 1 -  -   1 -  -  200000 -  -   792.7 -  - 252.30 - 605,331,776  1,061,814,272
BuildIndex      2        1       200000        786.7      254.24   602,915,712  1,061,814,272
BuildIndex -  - 3 -  -   1 -  -  200000 -  -   795.3 -  - 251.48 - 602,378,624  1,061,814,272
{code}

So it looks like the performance cost of this change is negligible (in
the noise).

"
1,"UserManager throws javax.jcr.query.InvalidQueryException on createUserThe UserManager method createUser(String userID, String password) throws an exception (javax.jcr.query.InvalidQueryException) if the user name contains a '@' character.

Stack trace:
Exception in thread ""main"" javax.jcr.query.InvalidQueryException: Lexical error at line 1, column 76.  Encountered: ""@"" (64), after : """" for statement: for $v in /jcr:root/rep:security/rep:authorizables/rep:groups//element(test@example.com,rep:Group) return $v: Lexical error at line 1, column 76.  Encountered: ""@"" (64), after : """": Lexical error at line 1, column 76.  Encountered: ""@"" (64), after : """"
        at org.apache.jackrabbit.spi.commons.query.xpath.XPathQueryBuilder.<init>(XPathQueryBuilder.java:302)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPathQueryBuilder.createQuery(XPathQueryBuilder.java:331)
        at org.apache.jackrabbit.spi.commons.query.xpath.QueryBuilder.createQueryTree(QueryBuilder.java:39)
        at org.apache.jackrabbit.spi.commons.query.QueryParser.parse(QueryParser.java:57)
        at org.apache.jackrabbit.core.query.lucene.QueryImpl.<init>(QueryImpl.java:91)
        at org.apache.jackrabbit.core.query.lucene.SearchIndex.createExecutableQuery(SearchIndex.java:615)
        at org.apache.jackrabbit.core.query.QueryImpl.init(QueryImpl.java:128)
        at org.apache.jackrabbit.core.SearchManager.createQuery(SearchManager.java:282)
        at org.apache.jackrabbit.core.query.QueryManagerImpl.createQuery(QueryManagerImpl.java:102)
        at org.apache.jackrabbit.core.security.user.IndexNodeResolver.buildQuery(IndexNodeResolver.java:105)
        at org.apache.jackrabbit.core.security.user.IndexNodeResolver.findNode(IndexNodeResolver.java:50)
        at org.apache.jackrabbit.core.security.user.UserManagerImpl.getAuthorizable(UserManagerImpl.java:93)
        at org.apache.jackrabbit.core.security.user.UserManagerImpl.createUser(UserManagerImpl.java:177)
        at org.apache.jackrabbit.core.security.user.UserManagerImpl.createUser(UserManagerImpl.java:158)
        at FirstHop.main(FirstHop.java:20)
Caused by: org.apache.jackrabbit.spi.commons.query.xpath.TokenMgrError: Lexical error at line 1, column 76.  Encountered: ""@"" (64), after : """"
        at org.apache.jackrabbit.spi.commons.query.xpath.XPathTokenManager.getNextToken(XPathTokenManager.java:13263)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.jj_ntk(XPath.java:9187)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ElementTest(XPath.java:8745)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.KindTest(XPath.java:8120)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.NodeTest(XPath.java:5041)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.AbbrevForwardStep(XPath.java:4891)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ForwardStep(XPath.java:4747)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.AxisStep(XPath.java:4692)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.StepExpr(XPath.java:4597)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.RelativePathExpr(XPath.java:4547)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.PathExpr(XPath.java:4396)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ValueExpr(XPath.java:4125)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.UnaryExpr(XPath.java:4032)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.CastExpr(XPath.java:3935)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.CastableExpr(XPath.java:3898)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.TreatExpr(XPath.java:3861)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.InstanceofExpr(XPath.java:3824)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.IntersectExceptExpr(XPath.java:3748)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.UnionExpr(XPath.java:3672)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.MultiplicativeExpr(XPath.java:3586)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.AdditiveExpr(XPath.java:3510)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.RangeExpr(XPath.java:3451)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ComparisonExpr(XPath.java:3353)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.AndExpr(XPath.java:3290)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.OrExpr(XPath.java:3227)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ExprSingle(XPath.java:2214)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ForClause(XPath.java:2337)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.FLWORExpr(XPath.java:2233)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ExprSingle(XPath.java:2133)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.Expr(XPath.java:2094)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.QueryBody(XPath.java:2066)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.MainModule(XPath.java:512)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.Module(XPath.java:387)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.QueryList(XPath.java:151)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.XPath2(XPath.java:118)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPathQueryBuilder.<init>(XPathQueryBuilder.java:295)
        ... 14 more
org.apache.jackrabbit.spi.commons.query.xpath.TokenMgrError: Lexical error at line 1, column 76.  Encountered: ""@"" (64), after : """"
        at org.apache.jackrabbit.spi.commons.query.xpath.XPathTokenManager.getNextToken(XPathTokenManager.java:13263)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.jj_ntk(XPath.java:9187)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ElementTest(XPath.java:8745)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.KindTest(XPath.java:8120)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.NodeTest(XPath.java:5041)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.AbbrevForwardStep(XPath.java:4891)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ForwardStep(XPath.java:4747)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.AxisStep(XPath.java:4692)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.StepExpr(XPath.java:4597)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.RelativePathExpr(XPath.java:4547)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.PathExpr(XPath.java:4396)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ValueExpr(XPath.java:4125)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.UnaryExpr(XPath.java:4032)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.CastExpr(XPath.java:3935)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.CastableExpr(XPath.java:3898)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.TreatExpr(XPath.java:3861)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.InstanceofExpr(XPath.java:3824)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.IntersectExceptExpr(XPath.java:3748)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.UnionExpr(XPath.java:3672)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.MultiplicativeExpr(XPath.java:3586)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.AdditiveExpr(XPath.java:3510)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.RangeExpr(XPath.java:3451)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ComparisonExpr(XPath.java:3353)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.AndExpr(XPath.java:3290)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.OrExpr(XPath.java:3227)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ExprSingle(XPath.java:2214)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ForClause(XPath.java:2337)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.FLWORExpr(XPath.java:2233)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ExprSingle(XPath.java:2133)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.Expr(XPath.java:2094)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.QueryBody(XPath.java:2066)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.MainModule(XPath.java:512)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.Module(XPath.java:387)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.QueryList(XPath.java:151)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.XPath2(XPath.java:118)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPathQueryBuilder.<init>(XPathQueryBuilder.java:295)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPathQueryBuilder.createQuery(XPathQueryBuilder.java:331)
        at org.apache.jackrabbit.spi.commons.query.xpath.QueryBuilder.createQueryTree(QueryBuilder.java:39)
        at org.apache.jackrabbit.spi.commons.query.QueryParser.parse(QueryParser.java:57)
        at org.apache.jackrabbit.core.query.lucene.QueryImpl.<init>(QueryImpl.java:91)
        at org.apache.jackrabbit.core.query.lucene.SearchIndex.createExecutableQuery(SearchIndex.java:615)
        at org.apache.jackrabbit.core.query.QueryImpl.init(QueryImpl.java:128)
        at org.apache.jackrabbit.core.SearchManager.createQuery(SearchManager.java:282)
        at org.apache.jackrabbit.core.query.QueryManagerImpl.createQuery(QueryManagerImpl.java:102)
        at org.apache.jackrabbit.core.security.user.IndexNodeResolver.buildQuery(IndexNodeResolver.java:105)
        at org.apache.jackrabbit.core.security.user.IndexNodeResolver.findNode(IndexNodeResolver.java:50)
        at org.apache.jackrabbit.core.security.user.UserManagerImpl.getAuthorizable(UserManagerImpl.java:93)
        at org.apache.jackrabbit.core.security.user.UserManagerImpl.createUser(UserManagerImpl.java:177)
        at org.apache.jackrabbit.core.security.user.UserManagerImpl.createUser(UserManagerImpl.java:158)
        at FirstHop.main(FirstHop.java:20)

"
0,"HttpMethodBase does not compile on JDK prior to 1.3reason is the use of URL.getPath() and URL.getQuery() within method
processRedirectResponse.

should use URIUtil.getPath and URIUtil.getQuery instead.

so, HttpMethodBase around line 952:

//update the current location with the redirect location
setPath(URIUtil.getPath(redirectUrl.toString()));
setQueryString(URIUtil.getQuery(redirectUrl.toString()));

thanks,

marius"
0,"GData - Server wrong commit does not buildThe last GData - Server commit  does not build due to a wrong commit.
Yonik did not commit all the files in the diff file. There are several sources and packages missing.
  
The diff - file with the date of 26.06.06 should be applied.
--> http://issues.apache.org/jira/browse/LUCENE-598
26.06.06.diff (644 kb)

could any of the lucene committers apply this patch. Yonik is on the way to Dublin.

Thanks Simon
"
1,"Revision 949509 (LUCENE-2480) causes IOE ""read past EOF"" when processing older format SegmentInfo data when JVM assertion processing is disabled.At revision 949509 in org.apache.lucene.index.SegmentInfo at line 155, there is the following code:
{noformat} 
    if (format > SegmentInfos.FORMAT_4_0) {
      // pre-4.0 indexes write a byte if there is a single norms file
      assert 1 == input.readByte();
    }
{noformat} 
Note that the assert statement invokes input.readByte().
If asserts are disabled for the JVM, input.readByte() will not be invoked, causing the following readInt() to return a bogus value, and then causing an IOE during the (mistakenly entered) loop at line 165.
This can occur when processing old format (format ""-9"") index data under Tomcat (whose startup scripts by default do not turn on asserts).

Full stacktrace:
{noformat} 
SEVERE: java.lang.RuntimeException: java.io.IOException: read past EOF
	at org.apache.solr.core.SolrCore.getSearcher(SolrCore.java:1066)
	at org.apache.solr.core.SolrCore.<init>(SolrCore.java:581)
	at org.apache.solr.core.CoreContainer.create(CoreContainer.java:431)
	at org.apache.solr.core.CoreContainer.load(CoreContainer.java:286)
	at org.apache.solr.core.CoreContainer$Initializer.initialize(CoreContainer.java:125)
	at org.apache.solr.servlet.SolrDispatchFilter.init(SolrDispatchFilter.java:86)
	at org.apache.catalina.core.ApplicationFilterConfig.getFilter(ApplicationFilterConfig.java:275)
	at org.apache.catalina.core.ApplicationFilterConfig.setFilterDef(ApplicationFilterConfig.java:397)
	at org.apache.catalina.core.ApplicationFilterConfig.<init>(ApplicationFilterConfig.java:108)
	at org.apache.catalina.core.StandardContext.filterStart(StandardContext.java:3800)
	at org.apache.catalina.core.StandardContext.start(StandardContext.java:4450)
	at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:791)
	at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:771)
	at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:526)
	at org.apache.catalina.startup.HostConfig.deployWAR(HostConfig.java:850)
	at org.apache.catalina.startup.HostConfig.deployWARs(HostConfig.java:724)
	at org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:493)
	at org.apache.catalina.startup.HostConfig.start(HostConfig.java:1206)
	at org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:314)
	at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:119)
	at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1053)
	at org.apache.catalina.core.StandardHost.start(StandardHost.java:722)
	at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1045)
	at org.apache.catalina.core.StandardEngine.start(StandardEngine.java:443)
	at org.apache.catalina.core.StandardService.start(StandardService.java:516)
	at org.apache.catalina.core.StandardServer.start(StandardServer.java:710)
	at org.apache.catalina.startup.Catalina.start(Catalina.java:583)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:288)
	at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:413)
Caused by: java.io.IOException: read past EOF
	at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:154)
	at org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:39)
	at org.apache.lucene.store.ChecksumIndexInput.readByte(ChecksumIndexInput.java:40)
	at org.apache.lucene.store.DataInput.readInt(DataInput.java:76)
	at org.apache.lucene.store.DataInput.readLong(DataInput.java:99)
	at org.apache.lucene.index.SegmentInfo.<init>(SegmentInfo.java:165)
	at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:230)
	at org.apache.lucene.index.DirectoryReader$1.doBody(DirectoryReader.java:91)
	at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:649)
	at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:87)
	at org.apache.lucene.index.IndexReader.open(IndexReader.java:415)
	at org.apache.lucene.index.IndexReader.open(IndexReader.java:294)
	at org.apache.solr.core.StandardIndexReaderFactory.newReader(StandardIndexReaderFactory.java:38)
	at org.apache.solr.core.SolrCore.getSearcher(SolrCore.java:1055)
	... 32 more
{noformat} "
1,"intermittent failure in TestIndexWriter. testExceptionDuringSync {code}
common.test:

    [mkdir] Created dir: C:\Projects\lucene\trunk-full1\build\test

    [junit] Testsuite: org.apache.lucene.index.TestIndexWriter

    [junit] Tests run: 102, Failures: 0, Errors: 1, Time elapsed: 100,297sec

    [junit]

    [junit] Testcase: testExceptionDuringSync(org.apache.lucene.index.TestIndexWriter): Caused an ERROR

    [junit] _a.fnm

    [junit] java.io.FileNotFoundException: _a.fnm

    [junit]     at org.apache.lucene.store.MockRAMDirectory.openInput(MockRAMDirectory.java:226)

    [junit]     at org.apache.lucene.index.FieldInfos.<init>(FieldInfos.java:68)

    [junit]     at org.apache.lucene.index.SegmentReader$CoreReaders.<init>(SegmentReader.java:116)

    [junit]     at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:620)

    [junit]     at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:590)

    [junit]     at org.apache.lucene.index.DirectoryReader.<init>(DirectoryReader.java:104)

    [junit]     at org.apache.lucene.index.ReadOnlyDirectoryReader.<init>(ReadOnlyDirectoryReader.java:27)

    [junit]     at org.apache.lucene.index.DirectoryReader$1.doBody(DirectoryReader.java:74)

    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:704)

    [junit]     at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:69)

    [junit]     at org.apache.lucene.index.IndexReader.open(IndexReader.java:307)

    [junit]     at org.apache.lucene.index.IndexReader.open(IndexReader.java:193)

    [junit]     at org.apache.lucene.index.TestIndexWriter.testExceptionDuringSync(TestIndexWriter.java:2723)

    [junit]     at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:206)

    [junit]

    [junit]

    [junit] Test org.apache.lucene.index.TestIndexWriter FAILED
{code}"
0,"Optimize TermInfosWriter.addI found one more optimization, in how terms are written in
TermInfosWriter.  Previously, each term required a new Term() and a
new String().  Looking at the cpu time (using YourKit), I could see
this was adding a non-trivial cost to flush() when indexing Wikipedia.

I changed TermInfosWriter.add to accept char[] directly, instead.

I ran a quick test building first 200K docs of Wikipedia.  With this
fix it took 231.31 sec (best of 3) and without the fix it took 236.05
sec (best of 3) = ~2% speedup.
"
0,"Improved Payloads APIWe want to make some optimizations to the Payloads API.

See following thread for related discussions:
http://www.gossamer-threads.com/lists/lucene/java-dev/54708"
0,"HttpClient drops connection to the proxy when an invalid 'connection: close' directive is encountered in 'connection established' responseOne of our customer is using our application to connect to our servlet using 
https.  We are using httpClient for http protocol handling.  The customer has a 
IBM proxy (see log file).  The connect failed with a null pointer exception.

The log seem to indicate that the proxy server is returning 200 for ""CONNECT"", 
but the proxy also sends a ""Connection:close"" header.  The httpClient closed 
the connection and then tried to create the SSL socket.  If the proxy server is 
incorrect in sending 200 with ""Connection:close"", then httpClient should throw 
exception for invalid state (IllegalStateException ?).

I will attach the log file."

label,summmarydescription
0,"cache should use both Last-Modified and ETag for validations when availableThis is a protocol recommendation:

""[HTTP/1.1 clients], if both an entity tag and a Last-Modified value have been provided by the origin server, SHOULD use both validators in cache-conditional requests. This allows both HTTP/1.0 and HTTP/1.1 caches to respond appropriately.""

http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.3.4

The current implementation only uses the ETag when conditionally validating an entry, so HTTP/1.0 caches can't currently reply to us with a 304 (Not Modified), even if that would be appropriate.
"
1,"EchoHandler:104 possible NPELine 104 of EchoHandler is

    bae.setContentType(entity.getContentType());

a few lines previously, entity is checked for null, so it appears that entity can be null."
0,"need a way to set time out when using HttpClient and HttpMultiClientWhen using class HttpClient or HttpMultiClient, there is no way to set the time 
out value. Because the setTimeout method is in HttpConnection and HttpClient or 
HttpMultiClient doesn't expose the HttpConnection object. One option is to add 
a method setTimeout in HttpClient and HttpMultiClient. Another option is to add 
such a method in HttpMethod."
0,"make SchemeRegistry friendlier for DI frameworksScheme's in SchemeRegistry are registered via 'register' method, but there is no way to pass it a set of schemes so those can be registered in one step. This way it can be externally configured and 'spring/guice friendly'... something like this is sufficient...

public void setSchemes (final Set <Scheme> schemes) {
    for (final Scheme scheme : schemes) 
        register(scheme);    
}
"
0,"not all applicable URIs are invalidated on PUT/POST/DELETEs that pass through client cache""Some HTTP methods MUST cause a cache to invalidate an entity. This is either the entity referred to by the Request-URI, or by the Location or Content-Location headers (if present). These methods are: PUT, DELETE, POST.""

http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.10

The current caching implementation only invalidates the Request URI, and not those present in the Location or Content-Location headers on the request.

I have a patch that fixes this which I will upload momentarily.
"
0,"User interaction for authenticationSome actions require user input.  Like forwarding to another host or retrieveing
authentication credentials.  Should have some way for clients to setup listeners
for such events so that they can be handled on the fly.  No gui, programatic
only."
0,"Provide BestMatch cookie policyPresently HttpClient uses a single cookie policy for all target hosts, which is suboptimal for two reasons:
(1) the user needs to know beforehand what kind of HTTP cookie support the target host provides
(2) does not work well with multiple sites with different level of HTTP cookie support 

Introduce new cookie policy that dynamically picks up a CookieSpec (browser compatibility | Netscape draft | RFC2109 | RFC2965) based on properties of the response received from the target host"
0,"ServerTestBase, LocalTestServer should be redistributedServerTestBase and LocalTestServer should be redistributed, so that we can all benefit from these wonderful classes without having to copy / paste them.

For instance, I am currently creating a REST client for some web service, and I would totally benefit from your classes to check borderline cases (404, 500, etc..)

the simplest possible way to achieve this would be to add a maven-jar-plugin entry to the POM that executes the test-jar goal.

regards,
Sami Dalouche

"
0,"Quick Start guide for HttpClient 4.0cannot determine the required jar files to compile samples/ own files

example org.apache.http.examples.client.ClientFormLogin

used

set CLASSPATH=P:\j\samples\httpClient\examples;j:\j\j5\h;P:\j\e\mysql.jar;P:\j\e\commons-logging-api-1.1.1.jar;P:\j\e\commons-logging-1.1.1.jar;P:\j\e\4\httpmime-4.0-beta2.jar;P:\j\e\4\httpclient-4.0-beta2.jar;P:\j\e\4\h\e\4\httpcore-4.0-beta3.jar;P:\j\e\4\httpcore-nio-4.0-beta3.jar

but still needs org.apache.http.*; ... where are these jars? a quick start guide / link to all jars for one version would help

- only known workaround - go back to old version 3.1"
0,"Change the error message in the exception at URIUtils#rewriteURI The message in URIUtils#rewriteURI is misspelled - ""URI may nor be null"" should be ""URI should not be null"""
1,"Resource Leakage when loading keystore in AuthSSLProtocolSocketFactoryOpened stream not closed after keystore is loaded, resulting in resource leakage:

private static KeyStore createKeyStore(final URL url, final String password) 
        throws KeyStoreException, NoSuchAlgorithmException, CertificateException, IOException
    {
        if (url == null) {
            throw new IllegalArgumentException(""Keystore url may not be null"");
        }
        LOG.debug(""Initializing key store"");
        KeyStore keystore  = KeyStore.getInstance(""jks"");
        keystore.load(url.openStream(), password != null ? password.toCharArray(): null);
        return keystore;
    }

Should be changed to something like:

private static KeyStore createKeyStore(final URL url, final String password) 
        throws KeyStoreException, NoSuchAlgorithmException, CertificateException, IOException
    {
        if (url == null) {
            throw new IllegalArgumentException(""Keystore url may not be null"");
        }
        LOG.debug(""Initializing key store"");
        KeyStore keystore  = KeyStore.getInstance(""jks"");
        InputStream is = ulr.openStream();
        try {
          keystore.load(is, password != null ? password.toCharArray(): null);
        } finally {
           is.close();
        }
        return keystore;
    }"
0,"jackrabbit-jca rar archive misses required classesSince the Maven 2 upgrade, the jackrabbit-jca rar archive doesn't contain the rar-specific classes in a jackrabbit-jca jar file that was previously also included in the archive."
1,"spi2dav : Query offset not respectedthe TCK query test SetOffsetTest fails in the setup jcr2spi - spi2dav(ex) - jcr-server.
not sure whether it is due to missing implementation on client or server part of something completely different...."
0,"[PATCH] retain exception stack tracesCode catches one exception, and throws another, losing the stack trace information of the causal exception. This makes it more difficult to understand what happened when an exception occurs. This patch retains all stack traces that occur that causes a thrown exception."
1,Tika configuration may use wrong class loaderThe configurable Tika parser construction mechanism added in JCR-2864 constructs the parser instance lazily when the first indexing request is made. This may confuse things as the context class loader used by Tika to load all the available parser classes may not always be the class loader used to create the repository. To avoid this problem the Tika parser should be constructed already during normal repository initialization.
1,"Web client/WebDAV fails to unescape workspace namesIt seems that when you try to access to a workspace through the web client/webdav, and the workspace has special characteres like spaces on its name like (""my workspace""), then the web cliente/webdav is not able to load it.

Probablly the only thing to do is to unscape the workspace name."
1,"Error instantiating lucene search index in Turkish Regional SettingThere is an issue when changing regional setting to Turkish. 
It fails when starting a repository, instantiating the lucene search index due to the following issue :

org.apache.jackrabbit.core.config.ConfigurationException: Configured class org.apache.jackrabbit.core.query.lucene.SearchIndex does not contain a property named indexingConfiguration
	at
org.apache.jackrabbit.core.config.BeanConfig.newInstance(BeanConfig.java
:205)
	at
org.apache.jackrabbit.core.config.RepositoryConfigurationParser$1.getQue
ryHandler(RepositoryConfigurationParser.java:631)
	at
org.apache.jackrabbit.core.config.RepositoryConfig.getQueryHandler(Repos
itoryConfig.java:1013)

This issue is known in java world, due to lower case conversion of 'I' character (in Turkish locale). JackRabbit source code try to instantiate the indexing configuration during the repository starting and is accessing indexingConfiguration property. It instantiates a setter for this property with a bad 'i' character.
"
1,"Crash when adding node to cluster with big journal on PSQL DBWhen adding a new node to a cluster with big journal on PSQL database application runs out of memory on the new node and crashes (no exception, application exits with code 137).
It's because with PSQL, when no fetchSize is specified, all the results of query are loaded into memory before being passed to application. Furthermore, specification of fetchSize only works in transactional mode and have no effect if autoCommit is true. (these are configured in ConnectionHelper)"
1,"Test dependency on Jackrabbit core (from o.a.j.c.security.user.XPathQueryEvaluator)o.a.j.c.security.user.XPathQueryEvaluator wrongly imports 

    import org.apache.jackrabbit.test.api.util.Text

instead of

    import org.apache.jackrabbit.util.Text



"
1,"Property.getLength() returns -1 for BOOLEAN, REFERENCE and DATE valuesIt seems those three are simply missing in PropertyImpl.getLength()."
0,"jcr mapping layer (OCM) should expose lock ownerjcr mapping layer 's  persistencemanager.java  does not expose an API for returning lockowner. Ideally   , the following method 

public String lock(final String absPath, final boolean isDeep, final boolean isSessionScoped) 

should return a hashmap/String array containing locktoken as well as lockowner. 

I tried having lockowner as a field in my java object and mapping it to jcr:lockOwner , so that I can just use getLockOwner() . But the problem is this property gets introduced in the node only if  the node is locked. So, when I try to insert a node , before I can even lock it , the insertion fails since there is no property like jcr:lockOwner   till then . 

So, I feel there is need for the above API. It is ok to have it exposed via separate call in order to maintain backward compatability
"
0,Improve behaviour when 'too many open files' occursThe MultiIndex may leave unused directories that were created in an attempt to create an index segment. The directory is not removed again when an error occurs (e.g. 'too many open files').
0,Typos in jcr-server io package javadocs
0,read IOHandlers from the config.xmlI would like to be able to change the order of the IOHandlers and add some.
0,"Create org.apache.jackrabbit.api.jsr283 (in jackrabbit-api) and move future jsr283 interfaces and classes there(as discussed on mailing list)

as the implementation of the new features for jsr283 have started, i
suggest to put the new jsr283 interfaces to jackrabbit-api instead to
core (where possible). this way, we can already figure out
inter-module dependency issues and people can start using experimental
features through an API than rather through casting objects to core
interfaces.

suggestion: use 'org.apache.jackrabbit.api.jsr283' as base package for
the new jsr283 interfaces and classes.

for example, use
org.apache.jackrabbit.api.jsr283.nodetype.NodeDefinitionTemplate
for the future javax.jcr.nodetype.NodeDefinitionTemplate

once jcr2.0 is released, we will drop the interim interfaces.

"
1,"ArrayIndexOutOfBounds thrown on re-index of repositoryI encountered a problem with the Lucene NodeIndexer when forcing the repository to re-index itself.

Using the default repository.xml file provided with the examples contribution, I loaded a number of PDF files using the sample application FSImport.  In this utility, the ""encoding"" property is set to the empty string """" for all the files.  The system appeared to index everything properly.  I then stopped the repository, deleted the index files and then restarted the repositoyr.  Re-indexing was initiated and a ""ArrayIndexOutOfBoundsException"" was thrown from the org.apache.jackrabbit.core.query.lucene.NodeIndexer.java

The code in question:

                // jcr:encoding is not mandatory
                String encoding = null;
                if (node.hasPropertyName(JCR_ENCODING)) {
                    PropertyState encodingProp =
                            (PropertyState) stateProvider.getItemState(new PropertyId(node.getUUID(), JCR_ENCODING));
                    encodingProp.getValues()[0].internalValue().toString();


                }

Expects the encodingProperty to be set if the property exists.  However, the node has the property, but the XMLPersistenceManager did not create any entries in the property array.  Either there is a problem in the XMLPersistenceManager (zero length string issues), or the NodeIndexer needs to be altered to verify that there is actually a value for a particular property.

Since the jcr:encoding property is not considered a multi-value property, the requirement to check for an initialized array is probably not the correct route.

Looking at the code for the XMLPersistenceManager readState(DOMWalker walker, PropertyState state) method (line 294), it indicates that if the content length for a property is zero, the property will not have a value added.  However, our encoding property is configured as the empty string and should be created.  Therefore, a suggested alteration is to check if the property is a string, and, even if zero length, add the property value.

        ArrayList values = new ArrayList();
        if (walker.enterElement(VALUES_ELEMENT)) {
            while (walker.iterateElements(VALUE_ELEMENT)) {
                // read serialized value
                String content = walker.getContent();
                if ((content.length() > 0) || (PropertyType.STRING == type)) {   // <==== suggested update
                    if (type == PropertyType.BINARY) {
                        // special handling required for binary value:
                        // the value stores the path to the actual binary file in the blob store
                        try {
                            values.add(InternalValue.create(new FileSystemResource(blobStore, content)));
                        } catch (IOException ioe) {
                            String msg = ""error while reading serialized binary value"";
                            log.debug(msg);
                            throw new ItemStateException(msg, ioe);
                        }
                    } else {
                        values.add(InternalValue.valueOf(content, type));
                    }
                }
            }
            walker.leaveElement();
        }
"
1,"JCR2SPI: VersionHistoryImpl.getQLabels() needs to skip jcr:mixinTypes as wellgetQLabels() iterates through the properties on a version labels node to compute the set of labels. Currently it only ignores jcr:primaryType, but it needs to skip jcr:mixinTypes as well.
"
1,"Non-versionable children of a versionable node should not be updated when a merge failsThe JCR specification (JSR-170) includes a merge algorithm that is inconsistent with the functionality described elsewhere in the JCR specification. Specifically from JSR-170 section 8.2.10 Merge:

""In either case, (regardless of whether bestEffort is true or false) for each non-versionable node (including both referenceable and non-referenceable), if the merge result of its nearest versionable ancestor is update, or if it has no versionable ancestor, then it is updated to reflect the state of its corresponding node. Otherwise, it is left unchanged.""

The algorithm presented in 8.2.10.1 of the specification goes against the above statement as it does not take into consideration the merge result of the nearest versionable ancestor.

One solution would be to have the doLeave(n) call that dofail(n, v') calls altered to only perform a merge on the versionable children rather than all of the children. The merging of all children (versionable and non-versionable) should only be done if the nearest parent is not in a failed merge state regardless of whether the failure occurred from the current merge operation or a previous merge operation.

I will attach a patch file that makes what I think is the required change.
"
1,"Compatibility issue if admin impersonates admin sessionin revision 1076596 in made some improvements in ImpersonationImpl removing the shortcut for ""AdminPrincipal"" which from my point of view is problematic.

however, this introduced the following compatibility issue (detected by tom):
while - according to my tests - a user is allowed to impersonate itself (jcr isn't totally clear about this but states that Session.impersonate is used to ""[...] impersonate"" another [...]"" this was possible for the admin-user due to the shortcut mentioned above.

in order not to break existing code relying on that special case, i would suggest to change the code accordingly.


"
1,"InputContextImpl: cannot upload file larger than 2GBIf an entity is larger than 2GB, the Content-Length cannot be obtained by using getIntHeader because of integer overflow. One needs to parse the value of the header from string to long. This issue affects InputContextImpl.getContentLength() in org.apache.jackrabbit.webdav.io from webdav/java (the current behavior is that the header is converted from string to int by the servlet API, then from int to long by Jackrabbit)

Testcase: largefile from Litmus. (test 3 - large_put fails when the PUT request is received)"
0,"incompatible with newers versions of xml-apisApparently the newer version of the xml-api does not play nice with file objects passed in via a streamresult. So to get around this, I have modified the jackrabbit code RepositoryConfig.java:311 to read:
 
               transformer.transform(
                    //new DOMSource(template), new StreamResult(xml)); COMMENTED OUT! 
                    new DOMSource(template), new StreamResult(directory+""\\""+WORKSPACE_XML));

A similar issue can be found here: http://forum.java.sun.com/thread.jspa?forumID=34&threadID=563077 and a somewhat similar issue can be found logged as a bug at sun: http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=5077403"
1,"NullPointerException in ItemStateThe following happens quite regularly when multiple threads are adding, retrieving and removing nodes simultaneously. Looking at the code of the pull method, this seems due to under-synchronization somewhere as overlayedState was tested at line 153 for null.

java.lang.NullPointerException
        at org.apache.jackrabbit.core.state.ItemState.pull(ItemState.java:156)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.stateModified(LocalItemStateManager.java:421)
        at org.apache.jackrabbit.core.state.StateChangeDispatcher.notifyStateModified(StateChangeDispatcher.java:85)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.stateModified(SharedItemStateManager.java:434)
        at org.apache.jackrabbit.core.state.ItemState.notifyStateUpdated(ItemState.java:241)
        at org.apache.jackrabbit.core.state.ChangeLog.persisted(ChangeLog.java:271)
        at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:741)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:937)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:327)
        at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:313)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:303)
        at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:307)
        at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1374)"
1,"Set_property permission not checked when saving a new nodeWhen a new node is saved, the add_node permission is checked, but not the set_property permission on it's properties in ItemImpl.validateTransientItems(). This is already fixed in trunk where the implementation is slightly different."
0,JSR 283 Property Types
0,"Test cases not fully initialized at first runIt seems that some test cases in o.a.j.test.api.query and o.a.j.test.api do not properly initialized the test repository before running tests against it. The repository gets initialized by other test cases, and later runs of the troublesome tests report no errors.

Thus the problem appears right after a fresh checkout and a tree cleanup. The command sequence below illustrates this problem. The error messages reported by the initial test runs are ""Workspace does not contain test data at: /testdata/query"" and ""Workspace does not contain test data at: /testdata"".

I tried tracing the cause of this problem, but couldn't find it easily as I'm not yet too familiar with the test setup.

$ svn co
$ maven test

    [junit] Running org.apache.jackrabbit.test.api.query.TestAll
    [junit] Tests run: 77, Failures: 30, Errors: 0, Time elapsed: 5,333 sec
    [junit] [ERROR] TEST org.apache.jackrabbit.test.api.query.TestAll FAILED
    [junit] Running org.apache.jackrabbit.test.api.TestAll
    [junit] Tests run: 534, Failures: 181, Errors: 0, Time elapsed: 16,105 sec
    [junit] [ERROR] TEST org.apache.jackrabbit.test.api.TestAll FAILED

$ maven test

    [junit] Running org.apache.jackrabbit.test.api.query.TestAll
    [junit] Tests run: 77, Failures: 0, Errors: 0, Time elapsed: 5,887 sec
    [junit] Running org.apache.jackrabbit.test.api.TestAll
    [junit] Tests run: 534, Failures: 0, Errors: 0, Time elapsed: 18,427 sec

$ maven clean
$ maven test

    [junit] Running org.apache.jackrabbit.test.api.query.TestAll
    [junit] Tests run: 77, Failures: 30, Errors: 0, Time elapsed: 13,185 sec
    [junit] [ERROR] TEST org.apache.jackrabbit.test.api.query.TestAll FAILED
    [junit] Running org.apache.jackrabbit.test.api.TestAll
    [junit] Tests run: 534, Failures: 181, Errors: 0, Time elapsed: 40,42 sec
    [junit] [ERROR] TEST org.apache.jackrabbit.test.api.TestAll FAILED

$ maven test

    [junit] Running org.apache.jackrabbit.test.api.query.TestAll
    [junit] Tests run: 77, Failures: 0, Errors: 0, Time elapsed: 5,942 sec
    [junit] Running org.apache.jackrabbit.test.api.TestAll
    [junit] Tests run: 534, Failures: 0, Errors: 0, Time elapsed: 17,797 sec
"
0,"TCK: PredicatesTest does not respect testroot configuration propertyTest does not respect testroot configuration property.

Proposal: use testroot configuration property in constructing the test queries.

--- PredicatesTest.java (revision 422074)
+++ PredicatesTest.java (working copy)
@@ -78,7 +78,7 @@
      * @throws RepositoryException
      */
     public void testEquality() throws RepositoryException {
-        String stmt = ""/"" + jcrRoot + ""/*[@"" + jcrPrimaryType + ""='"" + nodeTypeName + ""']"";
+        String stmt = ""/"" + jcrRoot + ""/"" + testPath + ""/*[@"" + jcrPrimaryType + ""='"" + nodeTypeName + ""']"";  
         try {
             qm.createQuery(stmt, Query.XPATH);
@@ -93,7 +93,7 @@
      * @throws RepositoryException
      */
     public void testCombinedOr() throws RepositoryException {
-        String stmt = ""/"" + jcrRoot + ""/*[@"" + jcrPrimaryType + ""='"" + nodeTypeName + ""' or @"" + jcrPrimaryType + ""='"" + ntBase + ""']"";
+        String stmt = ""/"" + jcrRoot  + ""/"" + testPath + ""/*[@"" + jcrPrimaryType + ""='"" + nodeTypeName + ""' or @"" + jcrPrimaryType + ""='"" + ntBase + ""']"";
  
         try {
             qm.createQuery(stmt, Query.XPATH);
@@ -108,7 +108,7 @@
      * @throws RepositoryException
      */
     public void testOr() throws RepositoryException {
-        String stmt = ""/"" + jcrRoot + ""/*[@"" + jcrPrimaryType + "" or @"" + jcrMixinTypes + ""]"";
+        String stmt = ""/"" + jcrRoot  + ""/"" + testPath + ""/*[@"" + jcrPrimaryType + "" or @"" + jcrMixinTypes + ""]"";
  
         try {
             qm.createQuery(stmt, Query.XPATH);
@@ -123,7 +123,7 @@
      * @throws RepositoryException
      */
     public void testAnd() throws RepositoryException {
-        String stmt = ""/"" + jcrRoot + ""/*[@"" + jcrPrimaryType + "" and @"" + jcrMixinTypes + ""]"";
+        String stmt = ""/"" + jcrRoot  + ""/"" + testPath + ""/*[@"" + jcrPrimaryType + "" and @"" + jcrMixinTypes + ""]"";
  
         try {
             qm.createQuery(stmt, Query.XPATH);
@@ -138,7 +138,7 @@
      * @throws RepositoryException
      */
     public void testCombinedAnd() throws RepositoryException {
-        String stmt = ""/"" + jcrRoot + ""/*[@"" + jcrPrimaryType + ""='"" + nodeTypeName + ""' and @"" + jcrPrimaryType + ""='"" + ntBase + ""']"";
+        String stmt = ""/"" + jcrRoot  + ""/"" + testPath + ""/*[@"" + jcrPrimaryType + ""='"" + nodeTypeName + ""' and @"" + jcrPrimaryType + ""='"" + ntBase + ""']"";
  
         try {
             qm.createQuery(stmt, Query.XPATH);

"
0,Keep jackrabbit jar/pom's updated at ibiblioPlease keep jackrabbit updated on ibiblio or some other maven repository.  jackrabbit-1.1 needs to be added.
1,"org.apache.jackrabbit.server.remoting.davex.JsonWriter: wrong value type for ::NodeIteratorSize attributethe ::NodeIteratorSize attribute is serialized as string value whereas the client expects a long value.
this causes unnecessery server-roundtrips since the client doesn't detect this hint."
1,"XML export (stream) doesn't initialize TransformerHandler properlyFor instance, in SessionImpl.java:

    public void exportSystemView(String absPath, OutputStream out,
                                 boolean skipBinary, boolean noRecurse)
            throws IOException, PathNotFoundException, RepositoryException {

        SAXTransformerFactory stf = (SAXTransformerFactory) SAXTransformerFactory.newInstance();
        try {
            TransformerHandler th = stf.newTransformerHandler();
            th.setResult(new StreamResult(out));
            th.getTransformer().setParameter(OutputKeys.METHOD, ""xml"");
            th.getTransformer().setParameter(OutputKeys.ENCODING, ""UTF-8"");
            th.getTransformer().setParameter(OutputKeys.INDENT, ""no"");

            exportSystemView(absPath, th, skipBinary, noRecurse);
        } catch (TransformerException te) {
            throw new RepositoryException(te);
        } catch (SAXException se) {
            throw new RepositoryException(se);
        }
    }

(1) It should be ""setOutputProperty()"", not ""setParameter()"",

(2) My tests show that setting the parameters only has an effect when done before calling setResult()

That being said, the effect is minor, as the default settings for the TransformerHandler seem to be correct anway.

"
0,"OSGi bundle symbolic names changed due to changes in maven-bundle-pluginSee FELIX-1886 - I have noticed this with jackrabbit-jcr-commons 2.0-beta3 but I assume all modules which use maven-bundle-plugin are affected.

Having bundle symbolic name changes is problematic as OSGi frameworks then consider the old a new bundles to be different components, instead of different versions of the same component.

The simplest way to go back to the previous symbolic names is probably to use the maven-bundle-plugin config workaround described in FELIX-1886, in the jackrabbit parent pom (or whever that plugin is configured).

I'll try that and supply a patch."
0,"Item retrieval inefficient after refreshWhen RepositoryService#getItemInfos() returns a sufficiently large batch for a path, then the second invocation of getItem() below is significantly slower than the first. 

String path = ...
Item i = session.getItem(path);
i.refresh(false); // same for refresh(true)
session.getItem(path);

In my test setup RepositoryService#getItemInfos() returns 3946 elements. The first invocation takes approx. 800ms, the second 3000ms."
0,jcr:path in QueryResult is only tested with SQLThe TCK should also include a test case for XPath query syntax.
0,"[PATCH] Exception not thrown where it appears it should have beenCode creates an expection in an apparent error state, but doesn't throw it when it looks like it should

class: org.apache.jackrabbit.webdav.jcr.search.SearchResultProperty
ctor: public SearchResultProperty(DavProperty property, ValueFactory valueFactory) throws RepositoryException
around line 96

        } else {
            new IllegalArgumentException(""SearchResultProperty requires a list of 'dcr:column' xml elements."");
        }

Patch fixes this"
1,Path.equals does not work for other Path implementationsPathImpl.equals does not take other path implementations into account (likely a typo).
1,"Memory is not freed up when jackrabbit-server war is redeployed in tomcatThis bug was introduced with the new CacheManager feature. See JCR-619.

The CacheManager starts a new background thread which optimizes memory distribution every second accross the various caches. When a jackrabbit repository is shutdown, this background thread is still running and prevents the GC from collecting the classloader when jackrabbit is deployed in a web application.

Steps to reproduce:
1) build jackrabbit and jcr-server from trunk and deploy into a tomcat
2) touch the web.xml file of the jcr-server web app (this will force a redeployment)

After step 2 two things may happen. Either:
- The memory consumption increases because the CacheManager thread is not shutdown
or
- The CacheManager thread dies unexpectedly with a NullPointerException:

Exception in thread ""org.apache.jackrabbit.core.state.CacheManager"" java.lang.NullPointerException
        at org.apache.jackrabbit.core.state.CacheManager.run(CacheManager.java:90)
        at java.lang.Thread.run(Unknown Source)"
0,"Allow workspace creation over clusterWhen workspace is created on cluster node A and then node added to that workspace, the proper event is sent to the journal, but other cluster nodes are not able to process it because they don't have the workspace.

It would be nice to have a configuration option for the cluster to create such workspace automatically (instead of just logging an error)"
0,"Exception messages in AuthorizableImpl include full tracebacks at warn level.In a number of places in AuthourizableImpl there are log.warn with tracebacks.
This would be fine, however in production with Sling its quite easy to attempt to set a property on the Authorizable and generate the traceback in the logs, which soon fill up. 

We could block all logging from the user manager, but that feels wrong.
Would it be possible to have the log message at warn level put out a single log line and at debug level put out the full traceback ?"
1,"Binary throws NullPointerException Precondition: repository with datastore disabled!

Steps to reproduce:

1) create binary from stream
2) set binary on property
3) dispose binary
4) get binary from property and dispose it immediately
5) go to 4)

Binary.dispose() will throw a NullPointerException when 4) is executed the second time.

The exception is not thrown if the property is saved after 2).

See also attached test."
1,"ocm fails with NPE when a ClassDescriptor isn't foundObjectConverterImpl#getObject(Session session, Class clazz, String path) should validate whether there's a suitable mapping in order to throw a more descriptive exception as ObjectConverterImpl#getObject(Session session, String path) does.

"
1,"Wrong argument check in BTreeManager constructorIn the constructor of BTreeManager the argument check on maxChildren and minChildren is wrong. Instead of

        if (2 * minChildren < maxChildren) {
            throw new IllegalArgumentException(""maxChildren must be at least twice minChildren"");
        }

it should be

        if (2 * minChildren > maxChildren) {
            throw new IllegalArgumentException(""maxChildren must be at least twice minChildren"");
        }
"
1,"Observation events are not triggered for intermediate nodes in version storageWhen a new version history is created no observation events are triggered for the intermediate nodes.

E.g. for the VersionHistory d94d4b41-f68e-4586-9e88-96e6790981d8 the following events are triggered (with a node filter applied, property events are not visible):

Node added: /jcr:system/jcr:versionStorage/d9/4d/4b/d94d4b41-f68e-4586-9e88-96e6790981d8
Node added: /jcr:system/jcr:versionStorage/d9/4d/4b/d94d4b41-f68e-4586-9e88-96e6790981d8/jcr:versionLabels
Node added: /jcr:system/jcr:versionStorage/d9/4d/4b/d94d4b41-f68e-4586-9e88-96e6790981d8/jcr:rootVersion
Node added: /jcr:system/jcr:versionStorage/d9/4d/4b/d94d4b41-f68e-4586-9e88-96e6790981d8/jcr:rootVersion/jcr:frozenNode

Observation should also trigger node added events for:
/jcr:system/jcr:versionStorage/d9
/jcr:system/jcr:versionStorage/d9/4d
/jcr:system/jcr:versionStorage/d9/4d/4b"
1,"Session.getAttributes( ) call always returns an empty arrayRepository repository = new RMIRemoteRepository(""//localhost:1099/jackrabbit.repository"");
SimpleCredentials c = new SimpleCredentials(""alex"",""ok"".toCharArray());
c.setAttribute(""anAttribute"", ""aValue"");
Session s = repository.login(c, ""aWorkspace"");
String[]attr=s.getAttributeNames();

array attr is empty.
according to docs it should contains attributes from the SimpleCredentials object."
1,"RMI-DateValue does not support full ISO8601 formatas mentioned in the javadoc:

 * To convert <code>Calendar</code> instances to and from strings, this class
 * uses a <code>SimpleDateFormat</code> instance with the pattern
 * <code>yyyy-MM-dd'T'HH:mm:ss'Z'</code>. The issue with this pattern is that
 * the era specification as defined in the JCR specification (+/- prefix) as
 * well as full time zone naming are not supported.
"
1,"autoCreate attribute of PropDef instances not serializedWhen a property of custom node type is defined to be autoCreate=true, this fact is not serialized to the custom_nodetypes.xml file. Digging into the source revealse the NodeTypeDefStore.writeDef method to not write the autoCreate attribute:

   // autoCreate
   String autoCreate = elem.getAttributeValue(AUTOCREATE_ATTRIBUTE);
   if (autoCreate != null && autoCreate.length() > 0) {
     pd.setAutoCreate(Boolean.valueOf(autoCreate).booleanValue());
   }

This seems to be a remains of a copy-paste procedure :-) with the correct code most probably being something like

   // autoCreate
   elem.setAttribute(AUTOCREATE_ATTRIBUTE, Boolean.toString(pd.isAutoCreate()));
"
0,Implement QueryResult.getSelectorNames()
0,"Session#importXML can't handle properly uuid collision if user has insufficient permissionWhen importing referenceable nodes, if there are nodes with the same uuid in the workspace but the session has no sufficient permission to read them then the import will fail no matter what ImportUUIDBehavior is chosen. 
But the same xml will be imported successfully in another repository or if the user have read access.

SessionImpl.java :
 public NodeImpl getNodeById(NodeId id) ...{
...
 try {
            return (NodeImpl) getItemManager().getItem(id);
        } catch (AccessDeniedException ade) {
            throw new ItemNotFoundException(id.toString());
        }
}

SessionImporter.java :

 public void startNode(NodeInfo nodeInfo, List propInfos)...{
...
  if (node == null) {
            // create node
            if (id == null) {
            ...
            } else {
                // potential uuid conflict
                NodeImpl conflicting;
                try {
                    conflicting = session.getNodeById(id);
                } catch (ItemNotFoundException infe) {
                    conflicting = null;
                }
                if (conflicting != null) {
                    // resolve uuid conflict
                 ...
               }
...
}

In the JCR 1.0 spec says ""lack of read access to an item blocks access to both information about the content of that item and information about the existence of the item"" but this should probably not be true, internally, when doing an import. 
Otherwise it means that read access to an entire workspace must be granted to a user so that it could successfully use the IMPORT_UUID_CREATE_NEW behaviour.

"
1,"PROPPATCH doesn't respect document orderPROPPATCH is currently implemented in terms of DavResource.alterProperties(...), which takes a set of properties to be set and a set of properties to be removed. This is not sufficient to model WebDAV's method semantics, as the order in which set/remove instructions appear is supposed to be relevant.

I have submitted a patch to the Litmus mailing list checking this (see <http://mailman.webdav.org/pipermail/litmus/2006-April/000196.html>).

In jcr-server, alterProperties probably should be changed to take an (ordered) list of set/remove instructions instead. The simplest approach for that would probably be to use a List containing either DavProperty (set) or DavPropertyName (remove) objects.
"
0,Improve javadoc of User#getCredentialsUser#getCredentials is not meant to return the credentials used for a repository login but the javadoc in the interface doesn't make this clear.
1,"REMOVE access is not checked when moving a nodeWhen a node cannot be removed because AccessManager does not allow this, it still can be moved (using Session.move())."
1,"Row.getValue(""rep:excerpt(.)"") may throw ArrayIndexOutOfBoundsExceptionHappens when the matching node only has properties that are configured not to be used in excerpts."
1,"Node.getReferences(String) and Node.getWeakReferences(String) issuesNode.getReferences(String) always returns empty iterator.

Node.getWeakReferences() & getWeakReferences(String)  cannot handle multi-valaued reference properties"
1,"Local AuthContext authenticates if LoginModule should be ignoredThe LoginModule indicates that it doesn't handle thecurrent login-attemp by returning false:
on login or commit invokation. It should thus be ignored.
The LoginContext calculates the overall login success.
In case of the local context, the login should faile if the only LoginModule is ignored.
Currently this is a success, but the "
1,"NodeTypeRegistry.validateNodeTypeDef causes NullPointerExceptionNodeTypeRegistry.registerNodeType(NodeTypeDef) checks the given node type definition for circular inheritance (amongst other things, of course). If the the node type definition does not contain a list of super types, the validateNodeTypeDef() (line 442) causes a NullPointerException being thrown in checkForCircularInheritance() because the ""supertypes"" variable is null and is not being checked.

Interestingly the other accesses to the same supertypes object in validNodeTypeDef() are all guarded against null and length==0. Might be an ommission."
0,"fix jackrabbit groupIdActually the groupid used in maven project.xml is simply ""jackrabbit"".
However, the new naming policy adopted by maven requires the groupid to mirror the main package name, so it should be changed to ""org.apache.jackrabbit"".
Probably this could be a good moment to fix it, after the recent merge and modification of artifact ids.

A note about the new policy for groupid can be found in http://maven.apache.org/reference/repository-upload.html"
1,"Jcr-Remoting: PathNotFoundException if item name ends with .jsonthe jcr-remoting-servlet contains the following commented issue:

    * TODO: TOBEFIXED will not behave properly if resource path (i.e. item name)
    * TODO  ends with .json extension and/or contains a depth-selector pattern."
0,"SPI: Add RepositoryService.getQNodeTypeDefinitionFinding of the F2F (2/3 July)

similar to recent modifications to the retrieval of name spaces the node type management should be changed in order to only retrieve the complete set of node types on demand. Otherwise single node type definitions should be retrieved as required.
To achieve this we agreed to add RepositoryService.getQNodeTypeDefinition"
0,"Use GrowingLRUMap in CachingEntryCollectorin order to allow for more flexibility of the cache size in caching entry collection, i would like to use the GrowingLRUMap."
0,"Add RepositoryException to JackrabbitAccessControlList#getRestrictionNames and #getRestrictionTypeThroughout the JCR and Jackrabbit API methods that include name processing are define to throw RepositoryException in case of 
an error related to name processing (due to lack of a more specific exception in the javax.jcr package space).

Unfortunately, I forgot those in JackrabbitAccessControlList and JackrabbitAccessControlEntry. While the latter has already been addressed for the 2.2 release,
I would like to fix the JackrabbitAccessControlList interface as well (and subsequently also fix the implementations that currently do not properly cope with 
names).
"
1,"spi2jcr: RepositoryServiceImpl.getRootId returns bad NodeId I think org.apache.jackrabbit.spi2jcr.RepositoryServiceImpl.getRootId() should not return getIdFactory().createNodeId((String) null, Path.ROOT). Rather should it do a round trip to the wrapped repository and return a UUID based NodeId if the root node of the wrapped repository is mix:referenceable.

The javadoc reads: ""If the root node can be identified with a unique ID the returned NodeId simply has a uniqueID part and the path part is null. If the root node cannot be identified with a unique ID the uniqueID part is null and the path part will be set to ""/""."""
1,"repository lock file not removed without a clean shutdownactually when a repository is loaded a "".lock"" file is created. This file is removed only after a clean shutdown but, if a jackrabbit instance has been killed, you have to manually delete the file in order to load the repository again, also if there was no live instance of jackrabbit that was using it.

The problem comes from the fact that the simple presence of the .lock file is used to indicate a live instance.
I suggest replacing this behavior using this method (used for example by eclipse when opening workspaces):
- when an instance is loaded create a "".lock"" file and open it with exclusive access
- when a new instance is started try to delete an eventual .lock file. Only if the file can't be deleted because in use assume that another jackrabbit instance is running.
"
0,"PersistentVersionManager contains grow-only cacheThe PersistentVersionManager class contains a private HashMap ""histories"" which contains references to InternalVersionHistory objects. The bad thing about this cache is, that it only grows, but is not being managed to forget about ""unused"" histories. This is even badder, as the class has support for on-demand loading of version histories.

Example: A repository which is filled with 9350 nodes and 52813 properties grows this histories map to 1'222'030 (!) entries. In this concrete case, the VM allocates 213MB to the heap of which 41MB is referenced by the PersistentVersionManager.histories map."
0,"Unnecessary parsing of Name valueInternalValue.toJCRValue(NamePathResolver) formats the Name value and constructs a NameValue using the formatted String. The implementation of NameValue.valueOf(String) again checks the format, which is quite expensive and unnecessary in this case."
0,"missing blob.remove in Berkeley DB persistance managerorg/apache/jackrabbit/core/state/bdb/BerkeleyDBPersistenceManager.destroy(PropertyState state) does not remove binary file from
the BLOBStore (filesystem impl)



"
1,Deadlock on concurrent commit/checkin operationsRunning concurrently jackrabbit transactions including checkin operations leads to deadlock.
0,"repository is locked by WorkspaceJanitor when another workspace is reindexingwhen the searchindex is corrupt or missing, it is rebuilt lazily after initialization of a workspace. usually by the first login on that workspace. during this time, the workspaceinfo is locked. unfortunately, the workspace janitor locks the repository and checks all workspace infos, if they can be disposed. in this case, no other access to the repository can be performed until the searchindex is initialized (which can take some time).

T1 -> WS1.login -> WS initializing
Janitor -> lock repo -> scan -> try lock WS1
T2 -> WS2.login -> must wait for T1
"
0,"JCR2SPI: Add specific deep loading of Nodes and Propertieswith jcr2spi an item is 'deep' loaded whenever the hierarchy is not complete. while for Session#itemExists or Session#getItem it is ok to try loading a Node first and if not found a Property, this is inconvenient (and generates unnecessary round trips to the SPI) for those cases, where the original JCR call indicates whether a Node or Property is expected.

This is the case for Node.getNode(String relPath) and Node.getProperty(String relPath) ((and maybe others))

Therefore i suggest to add specific methods

HierarchyManager#getNodeEntry
HierarchyManager#getPropertyEntry
NodeEntry#getDeepNodeEntry
NodeEntry#getDeepPropertyEntry

(or something similar)"
1,"LockOperation - In a clustered environment, a lock created on one server is not sent to the other servers.The cluster operation for a lock (LockOperation) always has the isLock variable set to false.
The operation to represent a lock should set the isLock variable to true.

    /**
     * Flag indicating whether this is a lock. 
     */
    private boolean isLock;
"
1,"Background threads should use jackrabbit classloader as thread context classloaderThe RepositoryImpl uses a thread executor with a default thread factory for some background threads. These threads should use the jackrabbit class loader (the classloader used for loading jackrabbit)
as thread context classloader. Currently the classloader is used which causes a new thread to be greated.
For example in combination with Sling the following can happen: a jsp in sling initiates a save to jackrabbit, this causes the indexing to start which is done in a background thread. A new thread is taken from the pool and the thread context class loader is set to the thread context classloader of the jsp/sling. If now Sling is undeployed, jackrabbit still holds this class loader. This creates a hugh memory leak.
"
1,"Query for string literal broken when literal can be coerced into other typeWhen a string literal can be coerced into another type (e.g. integer) a property of type string that matches the literal is not found.

E.g. the following query will match prop if its value is the string '1234'.
//*[@prop = '1234']

The query should match properties with string value '1234' and integer value 1234."
0,"Provide means to display the effective policies for a given set of principalsJSR 283 currently defines AccessControlManager#getEffectivePolicies(String nodePath) that would allow any Permission related UI to
display what policies contribute to a particular set of privileges. In addition the API defines AccessControlManager#getPrivileges(String nodePath) which
returns the privileges the editing session has at the specified path.

In order to have additional flexibility we started to add custom extensions (-> JackrabbitAccessControlManager) that allows e.g. to retrieve the privileges any set
of principals has hat a specified path. I would like to extend this and in addition provide a method that allows to retrieve the effective policies for a set of principals.
Currently this can only be achieved by relying on a specific access control model and making assumptions about it's implementation, which obviously isn't
the desired effect.
I"
0,"TCK: NodeTest#testMixinTypesProtected incorrectly failsThe test calls addMixin to add mix:referenceable to a node.  This step is not required to test that jcr:mixinTypes is protected, yet may fail if the node is already mix:referenceable or cannot be mix:referenceable.

Proposal: remove the call to addMixin.

The test attempts to set jcr:mixinTypes to a scalar value, but reports a failure if the implementation throws ValueFormatException instead of ConstraintViolationException.

Proposal: set jcr:mixinTypes to an array of length 1.

--- NodeTest.java       (revision 422074)
+++ NodeTest.java       (working copy)
@@ -1130,10 +1142,9 @@
         Node defaultRootNode = (Node) superuser.getItem(testRootNode.getPath());
  
         Node testNode = defaultRootNode.addNode(nodeName1, testNodeType);
-        testNode.addMixin(mixReferenceable);
  
         try {
-            testNode.setProperty(jcrMixinTypes,mixLockable);
+            testNode.setProperty(jcrMixinTypes, new String[] { mixLockable });
             fail(""Manually setting jcr:mixinTypes should throw a ConstraintViolationException"");
         }
         catch (ConstraintViolationException success) {
"
0,Improve performance for queries with large result setsThe current implementation of QueryResult requires that access rights are checked on all NodeIds before they are passed ot the QueryResult. This handling should be improved to a more lazy approach where result nodes are checked in configurable batches. Usually a client is only interested in the top scoring results.
0,"maven2 repositoryCould somebody care to upload jackrabbit to maven2 repo's at ibiblio? (ideally libraries, javadocs, source, pom's - but at least the lib's)"
0,"Redundant calls to RepositoryService.getChildInfosIn some cases jcr2spi issues calls to RepositoryService.getChildInfos for items which haven been returned by the last call to RepositoryService.getItemInfos. 

This happens because WorkspaceItemStateFactory.createDeepPropertyState is asked to create the node states for all items returned by RepositoryService.getChildInfos in the order they are returned by the Iterator. When trying to create an item state for an item which is deeper down the hierarchy than another item which comes later in the iterator, a call to RepositoryService.getChildInfos is issued for the latter. "
0,add calendar mime types to jcr-server's mime type registryattached is a patch that adds mime types for .ics and .ifb files to jcr-server's mime type registry.
1,"Namespace comparison in Namespace.java doesn't work, if a node return null, but it expects an empty stringIf a node returns null for Node.getNamespaceUri(), but Namespace.EMPTY_NAMESPACE expects an empty string, the comparison fails. But to my knowledge a null and """" namespaceUri should be treated the same.

"
0,"Poor performance of ISDESCENDANTNODE on SQL 2 queriesUsing the latest source code, I have noticed very bad performance on SQL-2 queries that use the ISDESCENDANTNODE constraint on a large sub-tree. For example, the query : 

select * from [jnt:news] as news where ISDESCENDANTNODE(news,'/root/site') order by news.[date] desc 

executes in 600ms 

select * from [jnt:news] as news order by news.[date] desc

executes in 4ms

From looking at the problem in the Yourkit profiler, it seems that the culprit is the constraint building, that uses recursive Lucene searches to build the list of descendant node IDs : 

    private Query getDescendantNodeQuery(
            DescendantNode dn, JackrabbitIndexSearcher searcher)
            throws RepositoryException, IOException {
        BooleanQuery query = new BooleanQuery();

        try {
            LinkedList<NodeId> ids = new LinkedList<NodeId>();
            NodeImpl ancestor = (NodeImpl) session.getNode(dn.getAncestorPath());
            ids.add(ancestor.getNodeId());
            while (!ids.isEmpty()) {
                String id = ids.removeFirst().toString();
                Query q = new JackrabbitTermQuery(new Term(FieldNames.PARENT, id));
                QueryHits hits = searcher.evaluate(q);
                ScoreNode sn = hits.nextScoreNode();
                if (sn != null) {
                    query.add(q, SHOULD);
                    do {
                        ids.add(sn.getNodeId());
                        sn = hits.nextScoreNode();
                    } while (sn != null);
                }
            }
        } catch (PathNotFoundException e) {
            query.add(new JackrabbitTermQuery(new Term(
                    FieldNames.UUID, ""invalid-node-id"")), // never matches
                    SHOULD);
        }

        return query;
    }

In the above example this generates over 2800 Lucene queries, which is the culprit. I wonder if it wouldn't be faster to retrieve the IDs by using the JCR to retrieve the list of child IDs ?

This was probably also missed because I didn't seem to find any performance tests on this constraint."
0,"Fix remaining localization test failures in lucenesee also LUCENE-1836 and LUCENE-1846

all tests should pass under different locales.
the fix is to run 'ant test' under different locales, look and fix problems, and use the LocalizedTestCase from LUCENE-1836 to keep them from coming back.

the same approach as LUCENE-1836 fixes the core queryparser, but I am running ant test under a few locales to look for more problems.
"
0,"explore morfologik integrationDawid Weiss mentioned on LUCENE-2298 that there is another Polish stemmer available:
http://sourceforge.net/projects/morfologik/

This works differently than LUCENE-2298, and ideally would be another option for users.
"
1,"TestMinimize.testAgainstBrzozowski reproducible seed OOM{code}
    [junit] Testsuite: org.apache.lucene.util.automaton.TestMinimize
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 3.792 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestMinimize -Dtestmethod=testAgainstBrzozowski -Dtests.seed=-7429820995201119781:1013305000165135537
    [junit] NOTE: test params are: codec=PreFlex, locale=ru, timezone=America/Pangnirtung
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestMinimize]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=294745976,total=310378496
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testAgainstBrzozowski(org.apache.lucene.util.automaton.TestMinimize):     Caused an ERROR
    [junit] Java heap space
    [junit] java.lang.OutOfMemoryError: Java heap space
    [junit]     at java.util.BitSet.initWords(BitSet.java:144)
    [junit]     at java.util.BitSet.<init>(BitSet.java:139)
    [junit]     at org.apache.lucene.util.automaton.MinimizationOperations.minimizeHopcroft(MinimizationOperations.java:85)
    [junit]     at org.apache.lucene.util.automaton.MinimizationOperations.minimize(MinimizationOperations.java:52)
    [junit]     at org.apache.lucene.util.automaton.RegExp.toAutomaton(RegExp.java:502)
    [junit]     at org.apache.lucene.util.automaton.RegExp.toAutomatonAllowMutate(RegExp.java:478)
    [junit]     at org.apache.lucene.util.automaton.RegExp.toAutomaton(RegExp.java:428)
    [junit]     at org.apache.lucene.util.automaton.AutomatonTestUtil.randomAutomaton(AutomatonTestUtil.java:256)
    [junit]     at org.apache.lucene.util.automaton.TestMinimize.testAgainstBrzozowski(TestMinimize.java:43)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1282)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1211)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.util.automaton.TestMinimize FAILED


{code}"
0,"Remove Lucene core's FunctionQuery implsAs part of the consolidation of FunctionQuerys, we want to remove Lucene core's impls.  Included in this work, we will make sure that all the functionality provided by the core impls is also provided by the new module.  Any tests will be ported across too, to increase the test coverage."
0,"LogByteSizeMergePolicy over-merges with autoCommit=false and documents with term vectors and/or stored fieldsMark Miller noticed this slowdown (see details in LUCENE-994) in his
app.

This happens because in SegmentInfo.sizeInBytes(), we just run through
all files associated with that segment, summing up their byte sizes.

But in the case of shared doc stores (which happens when
autoCommit=false), this is not quite correct because those files are
shared across multiple segments.

I plan to fix sizeInBytes() to not include the size of the doc stores
when they are shared.
"
0,"Add doBeforeFlush to IndexWriterIndexWriter has doAfterFlush which can be overridden by extensions in order to perform operations after flush has been called. Since flush is final, one can only override doAfterFlush. This issue will handle two things:
# Make doAfterFlush protected, instead of package-private, to allow for easier extendability of IW.
# Add doBeforeFlush which will be called by flush before it starts, to allow extensions to perform any operations before flush begings.

Will post a patch shortly.

BTW, any chance to get it out in 3.0.1?"
0,"QueryScorer and SpanRegexQuery are incompatible.Since the resolution of #LUCENE-1685, users are not supposed to rewrite their queries before submitting them to QueryScorer:

bq.------------------------------------------------------------------------
bq.r800796 | markrmiller | 2009-08-04 06:56:11 -0700 (Tue, 04 Aug 2009) | 1 line
bq.
bq.LUCENE-1685: The position aware SpanScorer has become the default scorer for Highlighting. The SpanScorer implementation has replaced QueryScorer and the old term highlighting QueryScorer has been renamed to QueryTermScorer. Multi-term queries are also now expanded by default. If you were previously rewritting the query for multi-term query highlighting, you should no longer do that (unless you switch to using QueryTermScorer). The SpanScorer API (now QueryScorer) has also been improved to more closely match the API of the previous QueryScorer implementation.
bq.------------------------------------------------------------------------

This is a great convenience for the most part, but it's causing me difficulties with SpanRegexQuerys, as the WeightedSpanTermExtractor uses Query.extractTerms() to collect the fields used in the query, but SpanRegexQuery does not implement this method, so highlighting any query with a SpanRegexQuery throws an UnsupportedOpertationException.  If this issue is circumvented, there is still the issue of SpanRegexQuery throwing an exception when someone calls its getSpans() method.

I can provide the patch that I am currently using, but I'm not sure that my solution is optimal.  It adds two methods to SpanQuery: extractFields(Set<String> fields) which is equivalent to fields.add(getField()) except when MaskedFieldQuerys get involved, and mustBeRewrittenToGetSpans() which returns true for SpanQuery, false for SpanTermQuery, and is overridden in each composite SpanQuery to return a value depending on its components.  In this way SpanRegexQuery (and any other custom SpanQuerys) do not need to be adjusted.

Currently the collection of fields and non-weighted terms are done in a single step.  In the proposed patch the WeightedSpanTerm extraction from a SpanQuery proceeds in two steps.  First, if the QueryScorer's field is null, then the fields are collected from the SpanQuery using the extractFields() method.  Second the terms are collected using extractTerms(), rewriting the query for each field if mustBeRewrittenToGetSpans() returns true."
0,"Improve how IndexWriter flushes deletes against existing segmentsIndexWriter buffers up all deletes (by Term and Query) and only
applies them if 1) commit or NRT getReader() is called, or 2) a merge
is about to kickoff.

We do this because, for a large index, it's very costly to open a
SegmentReader for every segment in the index.  So we defer as long as
we can.  We do it just before merge so that the merge can eliminate
the deleted docs.

But, most merges are small, yet in a big index we apply deletes to all
of the segments, which is really very wasteful.

Instead, we should only apply the buffered deletes to the segments
that are about to be merged, and keep the buffer around for the
remaining segments.

I think it's not so hard to do; we'd have to have generations of
pending deletions, because the newly merged segment doesn't need the
same buffered deletions applied again.  So every time a merge kicks
off, we pinch off the current set of buffered deletions, open a new
set (the next generation), and record which segment was created as of
which generation.

This should be a very sizable gain for large indices that mix
deletes, though, less so in flex since opening the terms index is much
faster.
"
0,"Lucli: Command to change the AnalyzerCurrently, Lucli is hardcoded to use StandardAnalyzer. The provided patch introduces a command ""analyzer"" to specify a different Analyzer class. 
If something fails, StandardAnalyzer is the fall-back."
1,"Index changes are lost if you call prepareCommit() then close()You are supposed to call commit() after calling prepareCommit(), but... if you forget, and call close() after prepareCommit() without calling commit(), then any changes done after the prepareCommit() are silently lost (including adding/deleting docs, but also any completed merges).

Spinoff from java-user thread ""lots of .cfs (compound files) in the index directory"" from Tim Bogaert.

I think to fix this, IW.close should throw an IllegalStateException if prepareCommit() was called with no matching call to commit()."
0,"haversine() is broken / misdocumentedDistanceUtils.haversine() is coded in a way that is erroneous based on the documented order of the parameters.  The parameters are defined as (x1,y1,x2,y2,radius)  -- i.e. lon,lat order.  The code implementing the algorithm, however, is as if the meaning of x and y are transposed, which means that if you supply the arguments in lat,lon (y,x) order, you will get the correct behavior.  It turns out that all callers of this method do this!

FYI I found out about this bug since it is inherited code in LSP (lucene-spatial-playground) and I have been supplying parameters according to its documented order.  Apparently I shouldn't do that ;-)"
1,"IndexWriter.rollback can hang if a previous call hit an exceptionIW.rollback has logic to make sure only one thread actually gets to do
the rollback whenever multiple threads are calling it at the same
time, by setting the ""boolean closing"" to true in the thread that got
there first.

Other threads wait for that variable to become false again before
returning from abort.

But, we are not restoring closing to false in a try/finally in
rollback(), which means on hitting an exception in rollback, a
subsequent call to rollback() will hang forever.

close() has the same logic, but there is already a try/finally there
to restore closing to false on exception.

The fix is straightforward.
"
0,"IndexWriter.setInfoStream should percolate down to mergePolicy & mergeSchedulerRight now *MergePolicy and *MergeScheduler have their own ad-hoc means
of being verbose about their actions.  We should unify these with
IndexWriter's infoStream.  Thanks to Hoss for suggesting this."
0,"SmartChineseAnalyzer javadoc improvementChinese -> English, and corrections to match reality (removes several javadoc warnings)"
1,"Possible thread hazard in IndexWriter.close(false)Spinoff from this thread:

  http://www.gossamer-threads.com/lists/lucene/java-dev/55391

On reviewing the code I found one case where an aborted merge (from
calling close(false)) could write to files that a newly opened
IndexWriter would also try to write to.

I strengthened an existing test case in TestConcurrentMergeScheduler
to tickle this case, and also modified MockRAMDirectory to throw an
IOException if ever a file besides segments.gen is overwritten.

However, strangely, I can't get an unhandled exception to occur during
the test and I'm not sure why.  Still I think this is a good defensive
check so we should commit it.
"
1,"Fuzzy query scoring issuesQueries which automatically produce multiple terms (wildcard, range, prefix, 
fuzzy etc)currently suffer from two problems:

1) Scores for matching documents are significantly smaller than term queries 
because of the volume of terms introduced (A match on query Foo~ is 0.1 
whereas a match on query Foo is 1).
2) The rarer forms of expanded terms are favoured over those of more common 
forms because of the IDF. When using Fuzzy queries for example, rare mis-
spellings typically appear in results before the more common correct spellings.


I will attach a patch that corrects the issues identified above by 
1) Overriding Similarity.coord to counteract the downplaying of scores 
introduced by expanding terms.
2) Taking the IDF factor of the most common form of expanded terms as the 
basis of scoring all other expanded terms."
0,"highlight-vs-vector-highlight.alg is unfairhighlight-vs-vector-highlight.alg uses EnwikiQueryMaker which makes SpanQueries, but FastVectorHighlighter simply ignores SpanQueries."
0,"MemoryIndex memory estimation in toString inconsistent with getMemorySize()After LUCENE-3867 was committed, there are some more minor problems with MemoryIndex's estimates. This patch will fix those and also add verbose test output of RAM needed for MemoryIndex vs. RAMDirectory.

Interestingly, the RAMDirectory always takes (according to estimates, so even with buffer overheads) only 2/3 of the MemoryIndex (excluding IndexReaders)."
0,"Create a grouping convenience classCurrently the grouping module has many collector classes with a lot of different options per class. I think it would be a good idea to have a GroupUtil (Or another name?) convenience class. I think this could be a builder, because of the many options (sort,sortWithinGroup,groupOffset,groupCount and more) and implementations (term/dv/function) grouping has."
0,"IndexWriter does not do the right thing when a Thread is interrupt()'dSpinoff from here:

    http://www.nabble.com/Deadlock-with-concurrent-merges-and-IndexWriter--Lucene-2.4--to22714290.html

When a Thread is interrupt()'d while inside Lucene, there is a risk currently that it will cause a spinloop and starve BG merges from completing.

Instead, when possible, we should allow interruption.  But unfortunately for back-compat, we will need to wrap the exception in an unchecked version.  In 3.0 we can change that to simply throw InterruptedException."
0,"CheckIndex API changed without backwards compaitibilityThe API of CheckIndex changed. The Check function returns a CheckIndexStatus and not boolean. And JavaDocs notes the boolean return value.

I am not sure if it works, but it would be good to have the check method that returns boolean available @Deprecated, i.e.
@Deprecated public static CheckIndexStatus check(Directory dir, boolean doFix) throws IOException {
 final CheckIndexStatus stat=this.check(dir,doFix);
 return stat.clean;
}

I am not sure, if it can be done with the same method name, but it prevents drop-in-replacements of Lucene to work."
0,"Some improvements to BenchmarkI've noticed that WriteLineDocTask declares it does not support multi-threading, but taking a closer look I think this is really for no good reason. Most of the work is done by reading from the ContentSource and constructing the document. If those two are mult-threaded (and I think all ContentSources are), then we can synchronize only around writing the actual document to the line file.

While investigating that, I've noticed some 1.5 TODOs and some other minor improvements that can be made. If you've wanted to make some minor improvements to benchmark, let me know :). I intend to include only minor and trivial ones."
0,"Remove deprecated TermAttribute from tokenattributes and legacy support in indexerThe title says it:
- Remove interface TermAttribute
- Remove empty fake implementation TermAttributeImpl extends CharTermAttributeImpl
- Remove methods from CharTermAttributeImpl (and indirect from Token)
- Remove sophisticated® backwards™ Layer in TermsHash*
- Remove IAE from NumericTokenStream, if TA is available in AS
- Fix rest of core tests (TestToken)"
1,"setFlushPending fails if we concurrently hit a aborting exceptionIf we select a DWPT for flushing but that DWPT is currently in flight and hits an exception after we selected them for flushing the num of docs is reset to 0 and we trip that exception. So we rather check if it is > 0 than assert on it here.
{noformat}
[junit] Testsuite: org.apache.lucene.index.TestIndexWriterExceptions
    [junit] Testcase: testRandomExceptionsThreads(org.apache.lucene.index.TestIndexWriterExceptions):	FAILED
    [junit] thread Indexer 3: hit unexpected failure
    [junit] junit.framework.AssertionFailedError: thread Indexer 3: hit unexpected failure
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions.testRandomExceptionsThreads(TestIndexWriterExceptions.java:227)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1226)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1154)
    [junit] 
    [junit] 
    [junit] Tests run: 18, Failures: 1, Errors: 0, Time elapsed: 30.287 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] Indexer 3: unexpected exception2
    [junit] java.lang.AssertionError
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushControl.setFlushPending(DocumentsWriterFlushControl.java:170)
    [junit] 	at org.apache.lucene.index.FlushPolicy.markLargestWriterPending(FlushPolicy.java:108)
    [junit] 	at org.apache.lucene.index.FlushByRamOrCountsPolicy.onInsert(FlushByRamOrCountsPolicy.java:61)
    [junit] 	at org.apache.lucene.index.FlushPolicy.onUpdate(FlushPolicy.java:77)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushControl.doAfterDocument(DocumentsWriterFlushControl.java:115)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:341)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1367)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1339)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread.run(TestIndexWriterExceptions.java:92)
    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterExceptions -Dtestmethod=testRandomExceptionsThreads -Dtests.seed=3493970007652348212:2010109588873167237
    [junit] WARNING: test method: 'testRandomExceptionsThreads' left thread running: merge thread: _1v(4.0):Cv2 _27(4.0):cv1 into _2h
    [junit] WARNING: test method: 'testRandomExceptionsThreads' left thread running: merge thread: _2c(4.0):cv1 into _2m
    [junit] RESOURCE LEAK: test method: 'testRandomExceptionsThreads' left 2 thread(s) running
    [junit] NOTE: test params are: codec=RandomCodecProvider: {content=MockFixedIntBlock(blockSize=421), field=MockSep, id=SimpleText, other=MockSep, contents=MockRandom, content1=Pulsing(freqCutoff=11), content2=MockSep, content4=SimpleText, content5=SimpleText, content6=MockRandom, crash=MockRandom, content7=MockVariableIntBlock(baseBlockSize=109)}, locale=mk_MK, timezone=Europe/Malta
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestToken, TestDateTools, Test2BTerms, TestAddIndexes, TestFilterIndexReader, TestIndexWriterExceptions]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=8,threads=1,free=78897400,total=195821568
{noformat}"
1,"FileSwitchDirectory should uniqueify the String file names returned by listAllRight now we blindly concatenate what's returned from primary & secondary.

But a legit use of FSD is pointing to the same underlying FSDir but w/ different impls for opening the inputs/outputs.

I have simple patch that just uniqueifies using Set<String>."
0,"Remove remaining deprecations in document packageRemove different deprecated APIs:
- Field.Index.NO_NORMS, etc.
- Field.binaryValue()
- getOmitTf()/setOmitTf()
"
0,"[PATCH] public static members in class TermVectorsWriterhi all,

looking at the implementation of TermVectorsWriter, you'll find a bunch of
public static final members where the visibility could be reduced to be
protected. I don't see a reason for having them public if the class itself is
protected and all members are final values. May be somebody could check and
either commit or enlighten me ;-)

thx
Bernhard"
0,"Simplify configuration API of contrib Query ParserThe current configuration API is very complicated and inherit the concept used by Attribute API to store token information in token streams. However, the requirements for both (QP config and token stream) are not the same, so they shouldn't be using the same thing.

I propose to simplify QP config and make it less scary for people intending to use contrib QP. The task is not difficult, it will just require a lot of code change and figure out the best way to do it. That's why it's a good candidate for a GSoC project.

I would like to hear good proposals about how to make the API more friendly and less scaring :)"
1,"[PATCH] IndexSearcher.search(query,filter,nDocs) accepts zero nDocsThis caused an npe from the ht.top().score lateron. 
The root cause was a bug in a test case, which took 
more time to track down than would have been necessary 
with the attached patch. 
The patch throws an IllegalArgumentException for non positive nDocs. 
All current tests pass with the patch applied. 
 
Regards, 
Paul"
0,"Add ""direct"" PackedInts.Reader impl, that reads directly from disk on each getSpinoff from LUCENE-3518.

If we had a direct PackedInts.Reader impl we could use that instead of
the RandomAccessReaderIterator.
"
1,"NumericRangeQuery.NumericRangeTermsEnum sometimes seeks backwardsSubclasses of FilteredTermsEnum are ""supposed to"" seek forwards only (this gives better performance, typically).

However, we don't check for this, so I added an assert to do that (while digging into testing the SimpleText codec) and NumericRangeQuery trips the assert!

Other MTQs seem not to trip it.

I think I know what's happening -- say NRQ has term ranges a-c, e-f to seek to, but then while it's .next()'ing through the first range, the first term after c is f.  At this point NRQ sees the range a-c is done, and then tries to seek to term e which is before f.  Maybe NRQ's accept method should detect this case (where you've accidentally .next()'d into or possibly beyond the next one or more seek ranges)?"
0,"Simplify StandardTokenizer JFlex grammarSummary of thread entitled ""Fullwidth alphanumeric characters, plus a question on Korean ranges"" begun by Daniel Noll on java-user, and carried over to java-dev:

On 01/07/2008 at 5:06 PM, Daniel Noll wrote:
> I wish the tokeniser could just use Character.isLetter and
> Character.isDigit instead of having to know all the ranges itself, since
> the JRE already has all this information.  Character.isLetter does
> return true for CJK characters though, so the ranges would still come in
> handy for determining what kind of letter they are.  I don't support
> JFlex has a way to do this...

The DIGIT macro could be replaced by JFlex's predefined character class [:digit:], which has the same semantics as java.lang.Character.isDigit().

Although JFlex's predefined character class [:letter:] (same semantics as java.lang.Character.isLetter()) includes CJK characters, there is a way to handle this using JFlex's regex negation syntax {{!}}.  From [the JFlex documentation|http://jflex.de/manual.html]:

bq. [T]he expression that matches everything of {{a}} not matched by {{b}} is !(!{{a}}|{{b}}) 

So to exclude CJ characters from the LETTER macro:

{code}
    LETTER = ! ( ! [:letter:] | {CJ} )
{code}
 
Since [:letter:] includes all of the Korean ranges, there's no reason (AFAICT) to treat them separately; unlike Chinese and Japanese characters, which are individually tokenized, the Korean characters should participate in the same token boundary rules as all of the other letters.

I looked at some of the differences between Unicode 3.0.0, which Java 1.4.2 supports, and Unicode 5.0, the latest version, and there are lots of new and modified letter and digit ranges.  This stuff gets tweaked all the time, and I don't think Lucene should be in the business of trying to track it, or take a position on which Unicode version users' data should conform to.  

Switching to using JFlex's [:letter:] and [:digit:] predefined character classes ties (most of) these decisions to the user's choice of JVM version, and this seems much more reasonable to me than the current status quo.

I will attach a patch shortly.
"
0,"Integrate MockBM25Similarity and MockLMSimilarity into the frameworkSteps:
1. Decide if {{MockLMSimilarity}} is needed at all (we have {{LMDirichletSimilarity}})
2. Move the classes to the similarities package
3. Move the similarities package to src/
4. Move all sims (inc. Similarity) to similarities
5. Make MockBM25Similarity a subclass of EasySimilarity?"
1,Instantiating SimpleFSLockFactory by its String param constructor throws an IllegalStateException
1,"Backwards problems with CharStream and Tokenizers with custom reset(Reader) methodWhen reviewing the new CharStream code added to Tokenizers, I found a
serious problem with backwards compatibility and other Tokenizers, that do
not override reset(CharStream).

The problem is, that e.g. CharTokenizer only overrides reset(Reader):

{code}
  public void reset(Reader input) throws IOException {
    super.reset(input);
    bufferIndex = 0;
    offset = 0;
    dataLen = 0;
  }
{code}

If you reset such a Tokenizer with another CharStream (not a Reader), this
method will never be called and breaking the whole Tokenizer.

As CharStream extends Reader, I propose to remove this reset(CharStream
method) and simply do an instanceof check to detect if the supplied Reader
is no CharStream and wrap it. We could also remove the extra ctor (because
most Tokenizers have no support for passing CharStreams). If the ctor also
checks with instanceof and warps as needed the code is backwards compatible
and we do not need to add additional ctors in subclasses.

As this instanceof check is always done in CharReader.get() why not remove
ctor(CharStream) and reset(CharStream) completely?

Any thoughts?

I would like to fix this somehow before RC4, I'm, sorry :(
"
0,IndexReader.open should take CodecsNeed to make this public... it's private now.
1,"sometimes if a BG merge hits an exception, optimize() will fail to forward the exceptionI was seeing an intermittant failure, only on a Windows instance running inside VMWare, of TestIndexWriter.testAddIndexOnDiskFull.

It is happening because the while loop that checks for merge exceptions that had occurred during optimize fails to catch the case where all the BG optimize merges completed (or hit exceptions) before the while loop begins.  IE, all BG threads finished before the FG thread advanced to the while loop.  In that case the code fails to check if there were any exceptions.

The fix is straightforward: change the while loop so that it always checks, at least once, whether there were exceptions."
0,"speedup recycling of per-doc RAMRobert found one source of slowness when indexing tiny docs, where we use List.toArray to recycle the byte[] buffers used by per-doc doc store state (stored field, term vectors).  This was added in LUCENE-2283, so not yet released."
0,"switch MultiTermQuery to ""constant score auto"" rewrite by defaultRight now it defaults to scoring BooleanQuery, and that's inconsistent w/ QueryParser which does constant score auto.

The new multi-term queries already set this default, so the only core queries this will impact are PrefixQuery and WildcardQuery.  FuzzyQuery, which has its own rewrite to BooleanQuery, will keep doing so."
0,"DisjunctionSumScorer small tweakMove ScorerDocQueue initialization from next() and skipTo() methods to the Constructor. Makes DisjunctionSumScorer a bit faster (less than 1% on my tests). 

Downside (if this is one, I cannot judge) would be throwing IOException from DisjunctionSumScorer constructors as we touch HardDisk there. I see no problem as this IOException does not propagate too far (the only modification I made is in BooleanScorer2)

if (scorerDocQueue == null) {
      initScorerDocQueue();
}
 

Attached test is just quick & dirty rip of  TestScorerPerf from standard Lucene test package. Not included as patch as I do not like it.


All test pass, patch made on trunk revision 613923
"
0,"add -Dtests.codecproviderCurrently to test a codec (or set of codecs) you have to add them to lucene's core and edit a couple of arrays here and there...

It would be nice if when using the test-framework you could instead specify a codecprovider by classname (possibly containing your own set of huper-duper codecs).

For example I made the following little codecprovider in contrib:
{noformat}
public class AppendingCodecProvider extends CodecProvider {
  public AppendingCodecProvider() {
    register(new AppendingCodec());
    register(new SimpleTextCodec());
  }
}
{noformat}

Then, I'm able to run tests with 'ant -lib build/contrib/misc/lucene-misc-4.0-SNAPSHOT.jar test-core -Dtests.codecprovider=org.apache.lucene.index.codecs.appending.AppendingCodecProvider', and it always picks from my set of  codecs (in this case Appending and SimpleText), and I can set -Dtests.codec=Appending if i want to set just one.

"
0,"[PATCH] disable coord for generated BooleanQueriesHere's a patch that disables Similiarty.coord() in the scoring of most
automatically generated boolean queries.  The coord() score factor is
appropriate when clauses are independently specified by a user, but is usually
not appropriate when clauses are generated automatically, e.g., by a fuzzy,
wildcard or range query.  Matches on such automatically generated queries are
currently penalized for not matching all terms."
0,"[patch] better support gcj compilationIn order to workaround http://gcc.gnu.org/bugzilla/show_bug.cgi?id=15411 the
attached patch is necessary."
0,"Rename RangeQuery -> TermRangeQuerySince we now have NumericRangeQuery (LUCENE-1701) we should rename RangeQuery to TextRangeQuery to make it clear that TextRangeQuery (TermRangeQuery?  StringRangeQuery) is based entirely on text comparison.

And, existing users on upgrading to 2.9 and using RangeQuery for [slow] numeric searching would realize they now have a good option for numeric range searching."
0,possible SynonymFilter bug: hudson failSee https://builds.apache.org/job/Lucene-trunk/1867/consoleText (no seed)
0,"Some contrib packages are missing a package.htmlDunno if we will get to this one this release, but a few contribs don't have a package.html (or a good overview that would work as a replacement) - I don't think this is hugely important, but I think it is important - you should be able to easily and quickly read a quick overview for each contrib I think.

So far I have identified collation and spatial."
1,FastVectorHighlighter: latter terms cannot be highlighted if two or more terms are concatenatedMy customer found a bug in FastVectorHighlighter. I'm working for the fix. I'll post it as soon as possible. We hope the fix in 2.9.
0,"Allow other string distance measures in spellcheckerUpdated spelling code to allow for other string distance measures to be used.

Created StringDistance interface.
Modified existing Levenshtein distance measure to implement interface (and renamed class).
Verified that change to Levenshtein distance didn't impact runtime performance.
Implemented Jaro/Winkler distance metric
Modified SpellChecker to take distacne measure as in constructor or in set method and to use interface when calling.
"
0,"Make Explanation include information about match/non-matchAs discussed, I'm looking into the possibility of improving the Explanation class to include some basic info about the ""match"" status of the Explanation -- independent of the value...

http://www.nabble.com/BooleanWeight.normalize%28float%29-doesn%27t-normalize-prohibited-clauses--t1596471.html#a4347644

This is neccesary to deal with things like LUCENE-451"
1,"DateTools UTC/GMT mismatchPost from Antony Bowesman on java-user:

-----

I just noticed that although the Javadocs for Lucene 2.2 state that the dates 
for DateTools use UTC as a timezone, they are actually using GMT.

Should either the Javadocs be corrected or the code corrected to use UTC instead.

-----

I'm attaching a patch that changes the javadoc and will commit it, unless someone knows a reason the javadoc is correct and the code should be changed to UTC. To my understanding, there's no significant difference between UTC and GMT.
"
1,"docvalues FNFEI created a test for LUCENE-3335, and it found an unrelated bug in docvalues."
0,BytesHashThis issue will have the BytesHash separated out from LUCENE-2186
0,Systemrequirements should say 1.5 instead of 1.4The website still says Java 1.4 but it should say 1.5
0,"cutover FunctionQuery tests to use RandomIndexWriter, for better testing"
0,"TestFSDirectory fails on Windows""ant test"" generates the following error consistently when run on a Windows machine even when run as user with Administrator privileges

    [junit] Testcase: testTmpDirIsPlainFile(org.apache.lucene.index.store.TestFSDirectory):     Caused an ERROR
    [junit] Access is denied
    [junit] java.io.IOException: Access is denied
    [junit]     at java.io.WinNTFileSystem.createFileExclusively(Native Method)
    [junit]     at java.io.File.createNewFile(File.java:828)
    [junit]     at org.apache.lucene.index.store.TestFSDirectory.testTmpDirIsPlainFile(TestFSDirectory.java:66)"
0,Remove synchronization in SegmentReader.isDeletedRemoves SegmentReader.isDeleted synchronization by using a volatile deletedDocs variable on Java 1.5 platforms.  On Java 1.4 platforms synchronization is limited to obtaining the deletedDocs reference.
1,"Document not guaranteed to be found after write and commitafter same email on developer list:
""I developed a stress test to assert that a new document containing a
specific term ""X"" is always found after a commit on the IndexWriter.
This works most of the time, but it fails under load in rare occasions.

I'm testing with 40 Threads, both with a SerialMergeScheduler and a
ConcurrentMergeScheduler, all sharing a common IndexWriter.
Attached testcase is using a RAMDirectory only, but I verified a
FSDirectory behaves in the same way so I don't believe it's the
Directory implementation or the MergeScheduler.
This test is slow, so I don't consider it a functional or unit test.
It might give false positives: it doesn't always fail, sorry I
couldn't find out how to make it more likely to happen, besides
scheduling it to run for a longer time.""

I tested this to affect versions 2.4.1 and 2.9.1;
"
0,"Calculate MD5 checksums in target <dist-all>Trivial patch that extends the ant target <dist-all> to calculate
the MD5 checksums for the dist files."
0,"javadocs cleanupbasic cleanup in core/contrib: typos, apache license header as javadoc, missing periods that screw up package summary, etc.
"
0,"Throw exception for ""Multi-SortedSource"" instead of returning nullSpinoff of LUCENE-3623: currently if you addIndexes(FIR) or similar, you get a NPE deep within codecs during merge.

I think the NPE is confusing, it looks like a bug but a clearer exception would be an improvement."
1,"IndexWriter.commit()  does not update the index versionIndexWriter.commit() can update the index *version* and *generation* but the update of *version* is lost.
As result added documents are not seen by IndexReader.reopen().
(There might be other side effects that I am not aware of).
The fix is 1 line - update also the version in SegmentsInfo.updateGeneration().
(Finding this line involved more lines though... :-) )
"
0,"Add missing getter methods to NumericField, NumericTokenStream, NumericRangeQuery, NumericRangeFilterThese classes are missing bean-style getter methods for some basic properties. This is inconsistent and should be fixed."
0,"Update the WikiThe wiki needs updating.  For starters, the URL is still Jakarta.  I think infrastructure needs to be contacted to do this move.  If someone is so inclined, it might be useful to go through and cleanup/organize what is there."
1,"StandardTokenizer disposes of Hiragana combining mark dakuten instead of attaching it to the character it belongs toLucene 3.3 (possibly 3.1 onwards) exhibits less than great behaviour for tokenising hiragana, if combining marks are in use.

Here's a unit test:

{code}
    @Test
    public void testHiraganaWithCombiningMarkDakuten() throws Exception
    {
        // Hiragana 'S' following by the combining mark dakuten
        TokenStream stream = new StandardTokenizer(Version.LUCENE_33, new StringReader(""\u3055\u3099""));

        // Should be kept together.
        List<String> expectedTokens = Arrays.asList(""\u3055\u3099"");
        List<String> actualTokens = new LinkedList<String>();
        CharTermAttribute term = stream.addAttribute(CharTermAttribute.class);
        while (stream.incrementToken())
        {
            actualTokens.add(term.toString());
        }

        assertEquals(""Wrong tokens"", expectedTokens, actualTokens);

    }
{code}

This code fails with:
{noformat}
java.lang.AssertionError: Wrong tokens expected:<[ざ]> but was:<[さ]>
{noformat}

It seems as if the tokeniser is throwing away the combining mark entirely.

3.0's behaviour was also undesirable:
{noformat}
java.lang.AssertionError: Wrong tokens expected:<[ざ]> but was:<[さ, ゙]>
{noformat}

But at least the token was there, so it was possible to write a filter to work around the issue.

Katakana seems to be avoiding this particular problem, because all katakana and combining marks found in a single run seem to be lumped into a single token (this is a problem in its own right, but I'm not sure if it's really a bug.)
"
1,"QueryWrapperFilter gets null DocIdSetIterator when wrapping TermQueryIf you try to get the iterator for the DocIdSet returned by a QueryWrapperFilter which wraps a TermQuery you get null instead of an iterator that returns the same documents as the search on the TermQuery.

Code demonstrating the issue:

{code:java}
import java.io.IOException;
import org.apache.lucene.analysis.WhitespaceAnalyzer;
import org.apache.lucene.document.Document;
import org.apache.lucene.document.Field;
import org.apache.lucene.document.Field.Index;
import org.apache.lucene.document.Field.Store;
import org.apache.lucene.index.IndexReader;
import org.apache.lucene.index.IndexWriter;
import org.apache.lucene.index.IndexWriterConfig;
import org.apache.lucene.index.Term;
import org.apache.lucene.store.RAMDirectory;
import org.apache.lucene.util.Version;
import org.apache.lucene.search.DocIdSet;
import org.apache.lucene.search.DocIdSetIterator;
import org.apache.lucene.search.Filter;
import org.apache.lucene.search.IndexSearcher;
import org.apache.lucene.search.QueryWrapperFilter;
import org.apache.lucene.search.TermQuery;
import org.apache.lucene.search.TopDocs;

public class TestQueryWrapperFilterIterator {
   public static void main(String[] args) {
		try {
			IndexWriterConfig iwconfig = new IndexWriterConfig(Version.LUCENE_34, new WhitespaceAnalyzer(Version.LUCENE_34));
			iwconfig.setOpenMode(IndexWriterConfig.OpenMode.CREATE);
			RAMDirectory dir = new RAMDirectory();
		
			IndexWriter writer = new IndexWriter(dir, iwconfig);
			Document d = new Document();
			d.add(new Field(""id"", ""1001"", Store.YES, Index.NOT_ANALYZED));
			d.add(new Field(""text"", ""headline one group one"", Store.YES, Index.ANALYZED));
			d.add(new Field(""group"", ""grp1"", Store.YES, Index.NOT_ANALYZED));
		    writer.addDocument(d);
			writer.commit();
			writer.close();
			
			IndexReader rdr = IndexReader.open(dir);
			IndexSearcher searcher = new IndexSearcher(rdr);
			
			TermQuery tq = new TermQuery(new Term(""text"", ""headline""));
			
			TopDocs results = searcher.search(tq, 5);
			System.out.println(""Number of search results: "" + results.totalHits);
			
			Filter f = new QueryWrapperFilter(tq);
			
			DocIdSet dis = f.getDocIdSet(rdr);
			
			DocIdSetIterator it = dis.iterator();
			if (it != null) {
				int docId = it.nextDoc();
				while (docId != DocIdSetIterator.NO_MORE_DOCS) {
					Document doc = rdr.document(docId);
					System.out.println(""Iterator doc: "" + doc.get(""id""));
					docId = it.nextDoc();
				}
			} else {
				System.out.println(""Iterator was null: "");
			}
			
			searcher.close();
			rdr.close();
		} catch (IOException ioe) {
			ioe.printStackTrace();
		}

	}
}
{code}"
1,"We should never open an IndexInput when an IndexOutput is still openI modified MockDirWrapper to assert this (except for
segments_N/segments.gen, where it's expected), and, it uncovered a
couple of places involving NRT readers where we open a shared doc
store file that's still open for writing.

First, if you install a merged segment warmer, we were failing to
force the merge of the doc stores in this case, thus potentially
opening the same doc stores that are also still open for writing.

Second, if you're actively adding docs in other threads when you call
IW.getReader(), the other threads could sneak in and flush new
segments sharing the doc stores.  The returned reader then opens the
doc store files that are still open for writing.
"
0,"move preflexrw to lucene3x packageCurrently there are a lot of things made public in lucene3x codec, but all marked internal/experimental/deprecated.

A lot of this is just so our test codec (preflexrw) can subclass it. I think we should just move it to the same
package, then it call all be package-private."
1,"MockRandomCodec loads termsIndex even if termsIndexDivisor is set to -1When working on LUCENE-2891 (on trunk), I found out that if MockRandomCodec is used, then setting IWC.readerTermsIndexDivisor to -1 allows seeking e.g., termDocs, when it shouldn't. Other Codecs fail to seek, as expected by the test. We need to find out why MockRandomCodec does not fail as expected.

To verify that, run ""ant test-core -Dtestcase=TestIndexWriterReader -Dtestmethod=testNoTermsIndex -Dtests.codec=MockRandom"", but comment out the line which adds MockRandom to the list of illegal codecs in the test."
1,"TestScoredDocIDsUtils.testWithDeletions test failureant test -Dtestcase=TestScoredDocIDsUtils -Dtestmethod=testWithDeletions -Dtests.seed=-2216133137948616963:2693740419732273624 -Dtests.multiplier=5

In general, on both 3.x and trunk, if you run this test with -Dtests.iter=100 it tends to fail 2% of the time.

"
0,"Remove GCJ IndexReader specializationsThese specializations are outdated, unsupported, most probably pointless due to the speed of modern JVMs and, I bet, nobody uses them (Mike, you said you are going to ask people on java-user, anybody replied that they need it?). While giving nothing, they make SegmentReader instantiation code look real ugly.

If nobody objects, I'm going to post a patch that removes these from Lucene."
0,"IndexWriter should let you optionally enable reader poolingFor apps using a large index and frequently need to commit and resolve deletes, the cost of opening the SegmentReaders on demand for every commit can be prohibitive.

We an already pool readers (NRT does so), but, we only turn it on if NRT readers are in use.

We should allow separate control.

We should do this after LUCENE-2294."
0,Optimize FixedStraightBytes for bytes size == 1Currently we read all the bytes in a PagedBytes instance wich is unneeded for single byte values like norms. For fast access this should simply be a straight array.
1,"DirectSpellChecker throws NPE if field doesn't existDirectSpellchecker doesn't check that the resulting Terms is null,
it should return an empty list here."
1,TestCompoundFile fails on windowsant test-core -Dtestcase=TestCompoundFile -Dtestmethod=testReadNestedCFP -Dtests.seed=-61cb66ec0d71d1ac:-46685c36ec38fd32:568c63299214892c
0,"Create a new sub-class of SpanQuery to enable use of a RangeQuery within a SpanQueryOur users express queries using a syntax which enables them to embed various query types within SpanQuery instances.  One feature they've been asking for is the ability to embed a numeric range query so they could, for example, find documents matching ""[2.0 2.75]MHz"".  The attached patch adds the capability and I hope others will find it useful.
"
0,"Block tree terms dict & indexOur default terms index today breaks terms into blocks of fixed size
(ie, every 32 terms is a new block), and then we build an index on top
of that (holding the start term for each block).

But, it should be better to instead break terms according to how they
share prefixes.  This results in variable sized blocks, but means
within each block we maximize the shared prefix and minimize the
resulting terms index.  It should also be a speedup for terms dict
intensive queries because the terms index becomes a ""true"" prefix
trie, and can be used to fast-fail on term lookup (ie returning
NOT_FOUND without having to seek/scan a terms block).

Having a true prefix trie should also enable much faster intersection
with automaton (but this will be a new issue).

I've made an initial impl for this (called
BlockTreeTermsWriter/Reader).  It's still a work in progress... lots
of nocommits, and hairy code, but tests pass (at least once!).

I made two new codecs, temporarily called StandardTree, PulsingTree,
that are just like their counterparts but use this new terms dict.

I added a new ""exactOnly"" boolean to TermsEnum.seek.  If that's true
and the term is NOT_FOUND, we will (quickly) return NOT_FOUND and the
enum is unpositioned (ie you should not call next(), docs(), etc.).

In this approach the index and dict are tightly connected, so it does
not support a pluggable index impl like BlockTermsWriter/Reader.
Blocks are stored on certain nodes of the prefix trie, and can contain
both terms and pointers to sub-blocks (ie, if the block is not a leaf
block).  So there are two trees, tied to one another -- the index
trie, and the blocks.  Only certain nodes in the trie map to a block
in the block tree.

I think this algorithm is similar to burst tries
(http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.18.3499),
except it allows terms to be stored on inner blocks (not just leaf
blocks).  This is important for Lucene because an [accidental]
""adversary"" could produce a terms dict with way too many blocks (way
too much RAM used by the terms index).  Still, with my current patch,
an adversary can produce too-big blocks... which we may need to fix,
by letting the terms index not be a true prefix trie on it's leaf
edges.

Exactly how the blocks are picked can be factored out as its own
policy (but I haven't done that yet).  Then, burst trie is one policy,
my current approach is another, etc.  The policy can be tuned to
the terms' expected distribution, eg if it's a primary key field and
you only use base 10 for each character then you want block sizes of
size 10.  This can make a sizable difference on lookup cost.

I modified the FST Builder to allow for a ""plugin"" that freezes the
""tail"" (changed suffix) of each added term, because I use this to find
the blocks.
"
1,"StandardTokenizer splits host names with hyphens into multiple tokens
StandardTokenizer does not recognize host names with hyphens as a single HOST token. Specifically ""www.m-w.com"" is tokenized as ""www.m"" and ""w.com"", both of ""<HOST>"" type.

StandardTokenizer should instead output a single HOST token for ""www.m-w.com"", since hyphens are a legitimate character in DNS host names.

We've a local fix to the grammar file which also required us to significantly simplify the NUM type to get the behavior we needed for host names.

here's a junit test for the desired behavior;

	public void testWithHyphens() throws Exception {
		final String host = ""www.m-w.com"";
		final StandardTokenizer tokenizer = new StandardTokenizer(
				new StringReader(host));
		final Token token = new Token();
		tokenizer.next(token);
		assertEquals(""<HOST>"", token.type());
		assertEquals(""www.m-w.com"", token.term());
	}

"
0,"Memory codecThis codec stores all terms/postings in RAM.  It uses an
FST<BytesRef>.  This is useful on a primary key field to ensure
lookups don't need to hit disk, to keep NRT reopen time fast even
under IO contention.
"
1,"ReadTask ignores traversalSizeThe ReadTask doLogic() method ignores the value of the traversalSize and loops over hits.length() instead, thus falsely reporting the desired number of iterations through the hit list.

The fix is relatively trivial since we already calculate 
{code}
int traversalSize = Math.min(hits.length(), traversalSize());
{code}
so we just need to use this value in the loop condition."
0,"include junit JAR in source distWe recently added the junit JAR under ""lib"" so that we can checkout & run tests, but we fail to include it in the source dist."
1,"Using ConstantScoreQuery on a RemoteSearchable throws java.io.NotSerializableExceptionUsing a ConstantScoreQuery through a MultiSearcher on a Searchable obtained through RMI (RemoteSearchable) will throw a java.io.NotSerializableException

The problem seems to be the fact that the ConstantScoreQuery.ConstantWeight has a Searcher member variable which is not serializable. Keeping a reference to the Searcher does not seem to be required: the fix seems trivial.

I've created the TestCase to reproduce the issue and the patch to fix it."
0,Adding FilteredDocIdSet and FilteredDocIdSetIteratorAdding 2 convenience classes: FilteredDocIdSet and FilteredDocIDSetIterator.
0,"Expose IndexFileNames as public, and make use of its methods in the codeIndexFileNames is useful for applications that extend Lucene, an in particular those who extend Directory or IndexWriter. It provides useful constants and methods to query whether a certain file is a core Lucene file or not. In addition, IndexFileNames should be used by Lucene's code to generate segment file names, or query whether a certain file matches a certain extension.

I'll post the patch shortly."
1,"Possible index corruption if crashing while replacing segments fileLucene's indexing is expected to be reasonably tolerant to computer crashes or the indexing process being killed. By reasonably tolerant, I mean that it is ok to lose a few documents (those currently buffered in memory), or have to repeat some work (e.g., a long merge that was in progress) - but it is not ok for the entire index, or large chunks of it, to become irreversebly corrupt.

The fact that Lucene works by repeated merging of several small segments into a new larger segments, solves most of the crash problems, because until the new segment is fully created, the old segments are still there and fully functional. However, one possibility for corruption remains in the segment replacement code:

After a new segment is created, a new segments file is written as a new file ""segments.new"", and then this file is renamed to ""segments"". The problem is that this renaming is done using Directory.renameFile(), and FSDirectory.renameFile is *NOT* atomic: it first deletes the old file, and then renames the new file. A crash between these stages (or perhaps during Java's rename which also isn't guaranteed to be atomic) will potentially leave us without a working ""segments"" file.

I will post here a patch for this bug shortly.

The patch will also include a change to Directory.renameFile()'s Javadoc. It currently claims ""This replacement should be atomic."", which is false in FSDirectory. Instead it should make a weaker claim, for example
   ""This replacement does not have to be atomic, but must at least obey a weaker guarantee: at any time during the replacement, either the ""from"" file is still available, or the ""to"" file is available with either the new or old content.""
(or, we can just drop the guaranteee altogether, like Java's File.renameTo() provides no atomic-ness guarantees)."
0,"deprecate Scorer.explainSpinoff from LUCENE-1749.

We already have QueryWeight.explain, which is directly invoked by IndexSearcher.explain.  Some queries in turn will defer to their Scorer impl's explain, but many do not (and their Scorer.explain simply throw UOE).  So we should deprecate & remove Scorer.explain, leaving it up to individual queries to define that method if they need it."
1,"CharArraySet.contains(char[] text, int off, int len) does not workI try to use the CharArraySet for a filter I am writing. I heavily use char-arrays in my code to speed up things. I stumbled upon a bug in CharArraySet while doing that.

The method _public boolean contains(char[] text, int off, int len)_ seems not to work.

When I do 

{code}
if (set.contains(buffer,offset,length) {
  ...
}
{code}

my code fails.

But when I do

{code}
if (set.contains(new String(buffer,offset,length)) {
   ...
}
{code}

everything works as expected.

Both variants should behave the same. I attach a small piece of code to show the problem."
0,"Hitting disk full during DocumentWriter.ThreadState.init(...) can cause hangMore testing of RC2 ...

I found one case, if you hit disk full during init() in
DocumentsWriter.ThreadState, when we first create the term vectors &
fields writer, such that subsequent calls to
IndexWriter.add/updateDocument will then hang forever.

What's happening in this case is we are incrementing nextDocID even
though we never call finishDocument (because we ""thought"" init did not
succeed).  Then, when we finish the next document, it will never
actually write because missing finishDocument call never happens.
"
0,"Remove @lucene.experimental from Numeric*NumericRangeQuery and NumericField are now there since 2.9. It is still marked as experimental. The API stabilized and there are no changes in the public parts (even in Lucene trunk no changes). Also lot's of people ask, if ""experimental"" means ""unstable"" in general, but it means only ""unstable API"".

I will remove the @lucene.experimental from Numeric* classes. NumericUtils* stays with @lucene.internal, as it is not intended for public use. Some people use it to make ""TermQuery"" on a numeric field, but this should be done using a NRQ with upper==lower and included=true, which does not affect scoring (applies also to Solr)."
0,"Further updates to the site scoring pageupdate the site scoring page - see Appendix:
{quote}
Class Diagrams
Karl Wettin's UML on the Wiki
{quote}
Karl's diagrams are outdated - I think this link should be pulled for 2.9

{quote}
Sequence Diagrams
FILL IN HERE. Volunteers?
{quote}
I think this should be pulled - I say put something like this as a task in JIRA - not the published site docs."
0,"DefaultSkipListReader should not be publicThere's no need for org.apache.lucene.index.DefaultSkipListReader to be public.
This class hasn't been released yet, so we should fix this before 2.2."
0,"LuceneTestCase should check for modifications on System properties- fail the test if changes have been detected.
- revert the state of system properties before the suite.
- cleanup after the suite."
0,"upgrade icu to 4.6version 4.6 supports unicode 6, new collators (search collators) etc."
1,"Bug in SegmentTermPositions if used for first term in the dictionaryWhen a SegmentTermPositions object is reset via seek() it does not move
the proxStream to the correct position in case the term is the first one
in the dictionary.

The reason for this behavior is that the skipStream is only moved if
lazySkipPointer is != 0. But 0 is a valid value for the posting list of
the very first term. The fix is easy: We simply have to set lazySkipPointer
to -1 in case no lazy skip has to be performed and then we only move the
skipStream if lazySkipPointer!=-1."
0,"remove custom encoding support in Greek/Russian AnalyzersThe Greek and Russian analyzers support custom encodings such as KOI-8, they define things like Lowercase and tokenization for these.

I think that analyzers should support unicode and that conversion/handling of other charsets belongs somewhere else. 

I would like to deprecate/remove the support for these other encodings.
"
0,Remove MultiTermQuery.getTerm()Removes the field and methods in MTQ that return the pattern term.
0,"FieldCacheTermsFilterThis is a companion to FieldCacheRangeFilter except it operates on a set of terms rather than a range. It works best when the set is comparatively large or the terms are comparatively common.

"
0,Rename FilterIndexReader to FilterAtomicReaderThis class has to be renamed to be consistent with the new naming.
0,"Adding a factory to QueryParser to instantiate query instancesWith the new efforts with Payload and scoring functions, it would be nice to plugin custom query implementations while using the same QueryParser.
Included is a patch with some refactoring the QueryParser to take a factory that produces query instances."
0,"Enable native per-field codec support Currently the codec name is stored for every segment and PerFieldCodecWrapper is used to enable codecs per fields which has recently brought up some issues (LUCENE-2740 and LUCENE-2741). When a codec name is stored lucene does not respect the actual codec used to encode a fields postings but rather the ""top-level"" Codec in such a case the name of the top-level codec is  ""PerField"" instead of ""Pulsing"" or ""Standard"" etc. The way this composite pattern works make the indexing part of codecs simpler but also limits its capabilities. By recoding the top-level codec in the segments file we rely on the user to ""configure"" the PerFieldCodecWrapper correctly to open a SegmentReader. If a fields codec has changed in the meanwhile we won't be able to open the segment.

The issues LUCENE-2741 and LUCENE-2740 are actually closely related to the way PFCW is implemented right now. PFCW blindly creates codecs per field on request and at the same time doesn't have any control over the file naming nor if a two codec instances are created for two distinct fields even if the codec instance is the same. If so FieldsConsumer will throw an exception since the files it relies on are already created.

Having PerFieldCodecWrapper AND a CodecProvider overcomplicates things IMO. In order to use per field codec a user should on the one hand register its custom codecs AND needs to build a PFCW which needs to be maintained in the ""user-land"" an must not change incompatible once a new IW of IR is created. What I would expect from Lucene is to enable me to register a codec in a provider and then tell the Field which codec it should use for indexing. For reading lucene should determ the codec automatically once a segment is opened. if the codec is not available in the provider that is a different story. Once we instantiate the composite codec in SegmentsReader we only have the codecs which are really used in this segment for free which in turn solves LUCENE-2740. 

Yet, instead of relying on the user to configure PFCW I suggest to move composite codec functionality inside the core an record the distinct codecs per segment in the segments info. We only really need the distinct codecs used in that segment since the codec instance should be reused to prevent additional files to be created. Lets say we have the follwing codec mapping :
{noformat}
field_a:Pulsing
field_b:Standard
field_c:Pulsing
{noformat}

then we create the following mapping:
{noformat}
SegmentInfo:
[Pulsing, Standard]

PerField:
[field_a:0, field_b:1, field_c:0]
{noformat}

that way we can easily determ which codec is used for which field an build the composite - codec internally on opening SegmentsReader. This ordering has another advantage, if like in that case pulsing and standard use really the same type of files we need a way to distinguish the used files per codec within a segment. We can in turn pass the codec's ord (implicit in the SegmentInfo) to the FieldConsumer on creation to create files with segmentname_ord.ext (or something similar). This solvel LUCENE-2741). 
"
0,"upgrade icu libraries to 4.4.2modules/analysis uses 4.4
solr/contrib/extraction uses 4.2.1

I think we should keep them the same version, for consistency, and go to 4.4.2 since it has bugfixes."
0,"DocValues type should be recored in FNX file to early fail if user specifies incompatible typeCurrently segment merger fails if the docvalues type is not compatible across segments. We already catch this problem if somebody changes the values type for a field within one segment but not across segments. in order to do that we should record the type in the fnx fiel alone with the field numbers.

I marked this 4.0 since it should not block the landing on trunk"
0,"Add setters to Field to allow re-use of Field instances during indexingIf we add setters to Field it makes it possible to re-use Field
instances during indexing which is a sizable performance gain for
small documents.  See here for some discussion:

    http://www.gossamer-threads.com/lists/lucene/java-dev/51041
"
1,"CharArraySet behaves inconsistently in add(Object) and contains(Object)CharArraySet's add(Object) method looks like this:
    if (o instanceof char[]) {
      return add((char[])o);
    } else if (o instanceof String) {
      return add((String)o);
    } else if (o instanceof CharSequence) {
      return add((CharSequence)o);
    } else {
      return add(o.toString());
    }
You'll notice that in the case of an Object (for example, Integer), the o.toString() is added. However, its contains(Object) method looks like this:
    if (o instanceof char[]) {
      char[] text = (char[])o;
      return contains(text, 0, text.length);
    } else if (o instanceof CharSequence) {
      return contains((CharSequence)o);
    }
    return false;
In case of contains(Integer), it always returns false. I've added a simple test to TestCharArraySet, which reproduces the problem:
  public void testObjectContains() {
    CharArraySet set = new CharArraySet(10, true);
    Integer val = new Integer(1);
    set.add(val);
    assertTrue(set.contains(val));
    assertTrue(set.contains(new Integer(1)));
  }
Changing contains(Object) to this, solves the problem:
    if (o instanceof char[]) {
      char[] text = (char[])o;
      return contains(text, 0, text.length);
    } 
    return contains(o.toString());

The patch also includes few minor improvements (which were discussed on the mailing list) such as the removal of the following dead code from getHashCode(CharSequence):
      if (false && text instanceof String) {
        code = text.hashCode();
and simplifying add(Object):
    if (o instanceof char[]) {
      return add((char[])o);
    }
    return add(o.toString());
(which also aligns with the equivalent contains() method).

One thing that's still left open is whether we can avoid the calls to Character.toLowerCase calls in all the char[] array methods, by first converting the char[] to lowercase, and then passing it through the equals() and getHashCode() methods. It works for add(), but fails for contains(char[]) since it modifies the input array."
0,"[PATCH] Highlighter: Delegate output escaping to FormatterPatch for jakarta-lucene-sandbox/contributions/highlighter
CVS version 3rd February 2005

This patch allows the highlighter Formatter to control escaping of the non
highlighted text as well as the highlighting of the matching text.

The example formatters highlight the matching text using XML/HTML tags. This
works fine if the plain text does not contain any characters that need to be
escaped for HTML output (i.e. <, &, and ""), however this cannot be guaranteed.
As the formatter controls the method of highlighting (in the examples this is
HTML, but it could be any other form of markup) it should also be responsible
for escaping the rest of the output.

This patch adds a method, encodeText(String), to the Formatter interface. This
is a breaking change. This method is called from the Highlighter with the text
that is not passed to the formatter's highlightTerm method. 
The SimpleHTMLFormatter has a public static method for performing simple HTML
escaping called htmlEncode. 
The SimpleHTMLFormatter, GradientFormatter, and SpanGradientFormatter have been
updated to implement the encodeText method and call the htmlEncode method to
escape the output.

For existing formatter to maintain exactly the same behaviour as before applying
this patch they would need to implement the encodeText method to return the
argument value without modification, e.g.:

public String encodeText(String originalText)
{
  return originalText;
}"
0,Nightly BuildsNightly builds for Lucene are HUGE due to the inclusion of the contrib/benchmark temp and work directories.  These directories should be excluded.
1,"minor/nitpick TermInfoReader bug ?Some code flagged by a bytecode static analyzer - I guess a nitpick, but we should just drop the null check in the if? If its null it will fall to the below code and then throw a NullPointer exception anyway. Keeping the nullpointer check implies we expect its possible - but then we don't handle it correctly.

{code}
  /** Returns the nth term in the set. */
  final Term get(int position) throws IOException {
    if (size == 0) return null;

    SegmentTermEnum enumerator = getThreadResources().termEnum;
    if (enumerator != null && enumerator.term() != null &&
        position >= enumerator.position &&
	position < (enumerator.position + totalIndexInterval))
      return scanEnum(enumerator, position);      // can avoid seek

    seekEnum(enumerator, position/totalIndexInterval); // must seek
    return scanEnum(enumerator, position);
  }

{code}"
1,"AIOOB thrown when length of termText is longer than 16384 characters (ArrayIndexOutOfBoundsException)DocumentsWriter has a max term length of 16384; if you cross that you
get an unfriendly ArrayIndexOutOfBoundsException.  We should fix to raise a clearer exception."
0,(Char)TermAttribute cloning memory consumptionThe memory consumption problem with cloning a (Char)TermAttributeImpl object was raised on thread http://markmail.org/thread/bybuerugbk5w2u6z
0,Remove Deprecated Benchmarking Utilities from contrib/benchmarkThe old Benchmark utilities in contrib/benchmark have been deprecated and should be removed in 2.9 of Lucene.
0,"remove relative paths assumptions from benchmark codeAlso see Eric comments in:
   http://www.nabble.com/forum/ViewPost.jtp?post=14347924&framed=y

Benchmark's config.xml relies on relative paths, more or less like this;
- base-dir
   -- conf-dir
   -- work-dir
       --- docs-dir
       --- indexes-dir

These assumptions are also in the Java code, and so it is inconvenient for
using absolute paths, e.g. for specifying a docs dir that is not under work-dir.

Relax this by modifying in build.xml to replace ""value"" and ""line"" props by 
""location"" and ""file"" and by requiring absolute paths in the Java code."
1,"commongrams filter calls incrementToken() after it returns falseIn LUCENE-3064, we beefed up MockTokenizer with assertions, and I started cutting over some analysis tests to use MockTokenizer for better coverage.

The commongrams tests fail, because they call incrementToken() after it already returns false. 

In general its my understanding consumers should not do this (and i know of a few tokenizers that will actually throw exceptions if you do this, just like java iterators and such)."
1,"Buffered deletes are not flushed by RAM or countWhen a segment is flushed, we will generally NOT flush the deletes, ie we simply buffer up the pending delete terms/queries, and the only apply them if 1) a segment is going to be merged (so we can remove the del docs in that segment), or 2) the buffered deletes' RAM exceeds 1/2 of IW's RAM limit when we are flushing a segment, or 3) the buffered deletes count exceeds IWC's maxBufferedDeleteTerms.

But the latter 2 triggers are currently broken on trunk; I suspect (but I'm not sure) when we landed DWPT we introduced this bug."
0,"Remove BitSet caching from QueryFilterSince caching is built into the public BitSet bits(IndexReader reader)  method, I don't see a way to deprecate that, which means I'll just cut it out and document it in CHANGES.txt.  Anyone who wants QueryFilter caching will be able to get the caching back by wrapping the QueryFilter in the CachingWrapperFilter."
1,"[PATCH] GermanAnalyzer fails silently + doesn't close filesAs mentioned on the developer list, the German analyzer will assume an empty list of 
stopwords if the stopword file isn't found. I'll attach a patch that makes it throw an 
IOException instead. Also the patch makes sure the file readers are closed."
0,SimpleText is too slowIf you are unlucky enough to get SimpleText codec on the TestBasics (span query) test then it runs very slowly...
0,"TokenStream API javadoc improvements- Change or remove experimental warnings of new TokenStream API
- Improve javadocs for deprecated Token constructors
- javadocs for TeeSinkTokenStream.SinkFilter"
0,"enable DefaultSimilarity.setDiscountOverlaps by defaultI think we should enable setDiscountOverlaps in DefaultSimilarity by default.

If you are using synonyms or commongrams or a number of other 0-posInc-term-injecting methods, these currently screw up your length normalization.
These terms have a position increment of zero, so they shouldnt count towards the length of the document.

I've done relevance tests with persian showing the difference is significant, and i think its a big trap to anyone using synonyms, etc: your relevance can actually get worse if you don't flip this boolean flag."
0,"Move Solr's FunctionQuery impls to Queries ModuleNow that we have the main interfaces in the Queries module, we can move the actual impls over.

Impls that won't be moved are:

function/distance/* (to be moved to a spatial module)
function/FileFloatSource.java (depends on Solr's Schema, data directories and exposes a RequestHandler)"
1,"TermVectors index files can become corrupt when autoCommit=falseSpinoff from this thread:

  http://www.gossamer-threads.com/lists/lucene/java-dev/55951

There are actually 2 separate cases here, both only happening when
autoCommit=false:

  * First issue was caused by LUCENE-843 (sigh): if you add a bunch of
    docs with no term vectors, such that 1 or more flushes happen;
    then you add docs that do have term vectors, the tvx file will not
    have enough entries (= corruption).

  * Second issue was caused by bulk merging of term vectors
    (LUCENE-1120 -- only in trunk) and bulk merging of stored fields
    (LUCENE-1043, in 2.3), and only shows when autoCommit=false, and,
    the bulk merging optimization runs.  In this case, the code that
    reads the rawDocs tries to read too far in the tvx/fdx files (it's
    not really index corruption but rather a bug in the rawDocs
    reading).

"
0,"pull CoreReaders out of SegmentReaderSimilar to LUCENE-3117, I think we should pull the CoreReaders class out of SR,
to make it easier to navigate the code."
0,"Make ShingleAnalyzerWrapper and PerFieldAnalyzerWrapper immutableBoth ShingleAnalyzerWrapper and PerFieldAnalyzerWrapper have setters which change some state which impacts their analysis stack.  If these are going to become reusable, then the state must be immutable as changing it will have no effect.

Process will be similar to QueryAutoStopWordAnalyzer, I will remove in trunk and deprecate in 3x."
1,"File leak when IOException occurs during index optimization.I am not sure if this issue requires a fix due to the nature of its occurrence, or if it exists in other versions of Lucene.

I am using Lucene Java 3.0.3 on a SUSE Linux machine with Java 6 and have noticed there are a number of file handles that are not being released from my java application. There are IOExceptions in my log regarding disk full, which causes a merge and the optimization to fail. The index is not currupt upon encountering the IOException. I am using CFS for my index format, so 3X my largest index size during optimization certainly consumes all of my available disk. 

I realize that I need to add more disk space to my machine, but I investigated how to clean up the leaking file handles. After failing to find a misuse of Lucene's IndexWriter in the code I have wrapping Lucene, I did a quick search for close() being invoked in the Lucene Jave source code. I found a number of source files that attempt to close more than one object within the same close() method. I think a try/catch should be put around each of these close() attempts to avoid skipping a subsequent closes. The catch may be able to ignore a caught exception to avoid masking the original exception like done in SimpleFSDirectory.close().

Locations in Lucene Java source where I suggest a try/catch should be used:
- org.apache.lucene.index.FormatPostingFieldsWriter.finish()
- org.apache.lucene.index.TermInfosWriter.close()
- org.apache.lucene.index.SegmentTermPositions.close()
- org.apache.lucene.index.SegmentMergeInfo.close()
- org.apache.lucene.index.SegmentMerger.mergeTerms() (The finally block)
- org.apache.lucene.index.DirectoryReader.close()
- org.apache.lucene.index.FieldsReader.close()
- org.apache.lucene.index.MultiLevelSkipListReader.close()
- org.apache.lucene.index.MultipleTermPositions.close()
- org.apache.lucene.index.SegmentMergeQueue.close()
- org.apache.lucene.index.SegmentMergeDocs.close()
- org.apache.lucene.index.TermInfosReader.close()"
0,JCR2SPI: Use namespace decl. present in imported xml to resolve Name/Path values
0,"AbstractResource: Use jcr:createdBy to expose DAV:creator-displaynameas of jcr 2.0 the DeltaV property DAV:creator-displayname could be extracted from the jcr:createdBy property.
the comment in abstract resource: ""// creator-displayname, comment: not value available from jcr"" therefore is outdated. and so is the subsequent line."
0,"Consistently refer to ""Apache Jackrabbit"" in docs and siteWe need to consistently refer to our project (and product) as Apache Jackrabbit
on our site and docs, except where the context is obvious.
"
1,"NullPointerException in LuceneQueryBuilderafter setting up the following query:

//mycoreclass[@ID= 'ArchNachl_class_003']//label[@* = 'A']

I get a NullPointerException:


java.lang.NullPointerException
        at org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder.visit(LuceneQueryBuilder.java:553)
        at org.apache.jackrabbit.core.query.RelationQueryNode.accept(RelationQueryNode.java:157)
        at org.apache.jackrabbit.core.query.NAryQueryNode.acceptOperands(NAryQueryNode.java:131)
        at org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder.visit(LuceneQueryBuilder.java:421)
        at org.apache.jackrabbit.core.query.LocationStepQueryNode.accept(LocationStepQueryNode.java:156)
        at org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder.visit(LuceneQueryBuilder.java:400)
        at org.apache.jackrabbit.core.query.PathQueryNode.accept(PathQueryNode.java:47)
        at org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder.visit(LuceneQueryBuilder.java:200)
        at org.apache.jackrabbit.core.query.QueryRootNode.accept(QueryRootNode.java:112)
        at org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder.createLuceneQuery(LuceneQueryBuilder.java:190)
        at org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder.createQuery(LuceneQueryBuilder.java:172)
        at org.apache.jackrabbit.core.query.lucene.QueryImpl.execute(QueryImpl.java:152)
        at org.apache.jackrabbit.core.query.QueryImpl.execute(QueryImpl.java:132)

I don't really know if it's a valid query. I should search for every label that has a property value (what ever name the property may have) that is  somewhere under a specific ""mycoreclass"" node. Either way Jackrabbit should of cause not exit with a NullPointerExeption here.

I'm using the very current svn snapshot of jackrabbit. For the records line 550-557 of LuceneQueryBuilder.java look like this now:

        String field = """";
        try {
            field = node.getProperty().toJCRName(nsMappings);
        } catch (NoPrefixDeclaredException e) {
            // should never happen
            exceptions.add(e);
        }

"
0,"Use a pre-generated version of XPath.jjtThe workaround described in JCR-46 is still causing extra steps for Java 5 users. I'd like to solve this issue by including a pre-generated version of the XPath.jjt file as a normal source file. This will avoid the need for XSL transformations during normal builds and thus remove the need for the extra steps.

I'll create a patch for this and unless anyone objects, I'm planning to include it in the 1.0 branch as well as the svn trunk."
0,Improvement in comment of QValue.getLength() The comment of QValue.getLength() should document -1 return values.
1,"System-view export/import of multi-value property does not respect JCR 2.0JCR 2.0 has a defined specification about system-view export of multi-value properties when these property have only one value.
The attribute sv:multiple attribute of sv:property tag is not written in output stream result.

Anyway if I add that one manually and try to import the modified system-view, multi-value properties with an only value are not recognized, and these properties are simply stored like a single-value ones.
"
0,extensibility changes to SimpleWebdavServletthe new SimpleWebdavServlet class lost some of the extensibility of the old WebdavServlet that allowed subclasses to ignore the dependency on RepositoryAccessServlet and provide their own support objects. attached is a patch that adds back these qualities.
1,"FileDataStore ignores return code from setLastModifiedGarbage collection depends on the file modification date being successfully updated when records are ""touched"" during the mark phase. The result of a silent failure is the catastrophic loss of the file in the sweep phase.

FileDataStore.getRecordIfStored does not, however, check the return code from setLastModified.

I believe I was bitten by this when my dev deployment ran out of disk space. A substantial portion of my datastore was deleted, and the best explanation I can come up with is that the setLastModified calls started (silently) failing, leading to massive overkill in the sweep.

There is also a call to setLastModified in FileDataStore.addRecord which is not strictly correct in the face of GC (i.e. it needs the resolution offset, and also must succeed if the file is writable or risk incorrect collection).

Patch to follow."
0,"Move generic locking tests from jcr2spi to jackrabbit-jcr-testsonce we touch the jackrabbit-jcr-tests for the JSR 283 implementation, we should also move the generic tests from jackrabbit-jcr2spi to the overall test suite.

opening this issue as a reminder and as a marker issue.
angela"
0,"Stats for the PersistenceManagerStatistics for the PersistenceManager impl that cover: 
 - bundle cache access count, 
 - bundle cache miss count, 
 - bundle cache miss avg duration (this avg includes the penalty of having to load from the underlying storage / can be interpreted as avg read latency as there is no cache involved) 
 - bundle writes per second

What it doesn't cover is :
 - number of bundles
 - size of workspace
as these are values that are expensive to compute on demand, and caching them would imply being able to store the values (which is not possible currently)."
0,"Maintan a stable ordering of properties in xml exportWhen exporting to xml (system view, not tested with document view) the order of properties is not consistent.
This is not an issue with the jcr specification, since the order of properties is undefined, but keeping the same (whatever) order in xml export could be useful.

At this moment if you try running a few import->export->import->export roundtrips you will notice that the exported xml often changes. This is an example of the differences you can see:

  <sv:property sv:name=""jcr:uuid"" sv:type=""String"">
    <sv:value>59357999-b4fb-45cd-8111-59277caf14b7</sv:value>
  </sv:property>
+  <sv:property sv:name=""title"" sv:type=""String"">
+    <sv:value>test</sv:value>
+  </sv:property>
  <sv:property sv:name=""visible"" sv:type=""String"">
    <sv:value>true</sv:value>
  </sv:property>
-  <sv:property sv:name=""title"" sv:type=""String"">
-    <sv:value>test</sv:value>
-  </sv:property>

If you may need to diff between two exported files that could be pretty annoying, you have no clear way to understand if something has really changed or not.
I would propose to keep ordering consistent between export: an easy way could be sorting properties alphabetically during export.

This behavior has been tested on a recent jackrabbit build from trunk (1.4-SNAPSHOT)



"
0,SPI sandbox: use tests-jars introduced with JCR-1629 and JCR-1683... and remove the duplicated tests.
0,JSR 283 Evaluate Capabilities
1,"PROPPATCH does not send multistatus after revision 397835After changes to use alterProperties for PROPPATCH in revision 397835, it returns a status code 200 and doesn't return a multistatus body. Patch below...

Index: C:/jprojects/eclipse/jackrabbit/jcr-server/server/src/java/org/apache/jackrabbit/server/AbstractWebdavServlet.java
===================================================================
--- C:/jprojects/eclipse/jackrabbit/jcr-server/server/src/java/org/apache/jackrabbit/server/AbstractWebdavServlet.java	(revision 398580)
+++ C:/jprojects/eclipse/jackrabbit/jcr-server/server/src/java/org/apache/jackrabbit/server/AbstractWebdavServlet.java	(working copy)
@@ -448,6 +448,7 @@
         MultiStatus ms = new MultiStatus();
         MultiStatusResponse msr = resource.alterProperties(changeList);
         ms.addResponse(msr);
+        response.sendMultiStatus(ms);
     }
 
     /**
"
0,"Stats for Queries continuedThis is to track the last missing item on the Query Stats list: Top Queries.
Also some needed refactoring."
0,"CacheManager resizeAll is slowCacheManager.resizeAll calls log.debug with a complex constructed log message.
The message is immediately discarded except when using the debug log level.

To improve performance, log.isDebugEnabled() should be used."
0,"Enhance test dataRunning the test cases currently results in a number of test cases that cannot be run. Mostly because not enough test data is in the repository to execute the test.

You can find out the affected test cases by setting the log level for 'org.apache.jackrabbit.test' to DEBUG and grep for 'executable' in the jcr.log file.

This returns the following list:

testGetDeclaringNodeType(org.apache.jackrabbit.test.api.nodetype.PropertyDefTest) not executable
testIsMandatory(org.apache.jackrabbit.test.api.nodetype.NodeDefTest) not executable
testValueConstraintNotSatisfied(org.apache.jackrabbit.test.api.nodetype.CanSetPropertyBinaryTest) not executable
testValueConstraintNotSatisfied(org.apache.jackrabbit.test.api.nodetype.CanSetPropertyBooleanTest) not executable
testValueConstraintNotSatisfied(org.apache.jackrabbit.test.api.nodetype.CanSetPropertyDoubleTest) not executable
testGetSize(org.apache.jackrabbit.test.api.observation.EventIteratorTest) not executable
testGetAttribute(org.apache.jackrabbit.test.api.SessionReadMethodsTest) not executable
testReferenceableRootNode(org.apache.jackrabbit.test.api.ReferenceableRootNodesTest) not executable
testBinaryProperty(org.apache.jackrabbit.test.api.SetValueConstraintViolationExceptionTest) not executable
testBooleanProperty(org.apache.jackrabbit.test.api.SetValueConstraintViolationExceptionTest) not executable
testDoubleProperty(org.apache.jackrabbit.test.api.SetValueConstraintViolationExceptionTest) not executable
testReferenceProperty(org.apache.jackrabbit.test.api.SetValueConstraintViolationExceptionTest) not executable
testMultipleReferenceProperty(org.apache.jackrabbit.test.api.SetValueConstraintViolationExceptionTest) not executable
testBinaryProperty(org.apache.jackrabbit.test.api.SetPropertyConstraintViolationExceptionTest) not executable
testBooleanProperty(org.apache.jackrabbit.test.api.SetPropertyConstraintViolationExceptionTest) not executable
testDoubleProperty(org.apache.jackrabbit.test.api.SetPropertyConstraintViolationExceptionTest) not executable
testReferenceProperty(org.apache.jackrabbit.test.api.SetPropertyConstraintViolationExceptionTest) not executable

Test data in Jackrabbit should be enhanced in order to run those test cases successfully."
0,"Move NamespaceMappings/Index from lucene to namespace registry.The NamespaceMappings class in the indexer is used for generating small prefixes for namespace uris that are stored in the index. This mechanism of stable prefixes could be used in other places as well, for example in the persistence managers.

Suggest to introduce general methods in the namespace registry:

int getURIIndex(String uri)
String getURI(int index)
"
0,"Refactoring config handlingAs discussed on the mailing list:

   * Move all XML handling to a separate ConfigurationParser class
   * Clean up and document the *Config classes
   * Get rid of the messy AbstractConfig class"
0,"Occasional IndexingQueueTest failureUsually the following assertion fails:

junit.framework.AssertionFailedError
	at junit.framework.Assert.fail(Assert.java:47)
	at junit.framework.Assert.assertTrue(Assert.java:20)
	at junit.framework.Assert.assertTrue(Assert.java:27)
	at org.apache.jackrabbit.core.query.lucene.IndexingQueueTest.testQueue(IndexingQueueTest.java:77)"
0,"Jcr-Server: DavResource#getDavSession() missingInstead of having DavResource#getDavSession() this method is defined individually by most of the
derived interfaces."
0,"Parallelize testsAs mentioned on the mailing list I'd like to parallelize test execution.

There will be a pool of RepositoryHelper instances, each represents a distinct repository. This ensures that test cases that run in parallel do not interfere with each other. I suggest we start with a pool of two repositories, but we can later extend this setup."
0,Separate initial index creation from MultiIndex constructionIf there is no index present the MultiIndex constructor will create an initial index by traversing the workspace item states. This makes it difficult for an outside class to detect the situation where no index is present.
0,"TCK does not clean 2nd workspace during AbstractJCRTest.setUp()the XATest.testXAVersionsThoroughly fails if run 2 times, since the 2nd workspace is not cleaned on startup. will provide provisonairy fix."
0,"Jcr2Spi: Warning upon reloading property valuestobi reported the following log-warning being written upon reloading property values:

[WARN ] Property data has been discarded

It seems to me that this has been introduced with JCR-1963. Taking a closer look at it, i get the impression that
the 'discarded' flag should only be set if any values are notified accordingly. In addition it seems to me that the MergeResult should have a dispose() method (or something similar) in order to have the replaced (old) property values properly released..."
1,"bad assumptions in VersionHistoryTest.testInitallyGetAllVersionsContainsTheRootVersion()There are two incorrect assumptions in testInitallyGetAllVersionsContainsTheRootVersion:

- getAllVersions() returns versions in a particular order (test assumes root version comes first), and

- Node.equals() is suitable for node comparison

And finally, there's a typo in the test case name.
"
0,"Release references to JCR items in tearDownOn my 64-Bit environment OS/JVM I tried a ""mvn clean install"" and got an OutOfMemory Exception.
On my 32-Bit environment Mac OSX 10.5 Java 1.5 the tests were all  fine and the IndexMerger was significant faster.

Running org.apache.jackrabbit.test.TestAll
21.11.2007 10:29:51 *INFO * [IndexMerger] IndexMerger: merged 549 documents in 289 ms into _a. (IndexMerger.java, line 304)
21.11.2007 10:29:55 *ERROR* [main] ImportHandler: fatal error encountered at line: 1, column: 10 while parsing XML stream: org.xml.sax.SAXParseException: Attribute name ""is"" associated with an element type ""this"" must be followed by the ' = ' character. (ImportHandler.java, line 116)
21.11.2007 10:29:55 *ERROR* [main] ImportHandler: fatal error encountered at line: 1, column: 10 while parsing XML stream: org.xml.sax.SAXParseException: Attribute name ""is"" associated with an element type ""this"" must be followed by the ' = ' character. (ImportHandler.java, line 104)
21.11.2007 10:29:59 *ERROR* [main] ImportHandler: fatal error encountered at line: -1, column: -1 while parsing XML stream: org.xml.sax.SAXParseException: Premature end of file. (ImportHandler.java, line 104)
21.11.2007 10:29:59 *ERROR* [main] ImportHandler: fatal error encountered at line: -1, column: -1 while parsing XML stream: org.xml.sax.SAXParseException: Premature end of file. (ImportHandler.java, line 116)
21.11.2007 10:30:45 *INFO * [IndexMerger] IndexMerger: merged 555 documents in 2015 ms into _l. (IndexMerger.java, line 304)
21.11.2007 10:33:13 *INFO * [IndexMerger] IndexMerger: merged 412 documents in 25587 ms into _w. (IndexMerger.java, line 304)
Exception in thread ""Timer-1"" java.lang.OutOfMemoryError: Java heap space
        at org.apache.lucene.store.BufferedIndexOutput.<init>(BufferedIndexOutput.java:26)
        at org.apache.lucene.store.FSDirectory$FSIndexOutput.<init>(FSDirectory.java:592)
        at org.apache.lucene.store.FSDirectory.createOutput(FSDirectory.java:435)
        at org.apache.lucene.util.BitVector.write(BitVector.java:122)
        at org.apache.lucene.index.SegmentReader.doCommit(SegmentReader.java:236)
        at org.apache.lucene.index.IndexReader.commit(IndexReader.java:794)
        at org.apache.lucene.index.FilterIndexReader.doCommit(FilterIndexReader.java:190)
        at org.apache.lucene.index.IndexReader.commit(IndexReader.java:825)
        at org.apache.lucene.index.IndexReader.close(IndexReader.java:841)
        at org.apache.jackrabbit.core.query.lucene.AbstractIndex.close(AbstractIndex.java:327)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex$DeleteIndex.execute(MultiIndex.java:1715)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.executeAndLog(MultiIndex.java:936)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.flush(MultiIndex.java:880)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.checkFlush(MultiIndex.java:1110)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.access$100(MultiIndex.java:75)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex$1.run(MultiIndex.java:324)
        at java.util.TimerThread.mainLoop(Timer.java:512)
        at java.util.TimerThread.run(Timer.java:462)
21.11.2007 10:34:37 *ERROR* [main] DatabasePersistenceManager: failed to write node state: cfbffd6d-114d-4738-9383-48da2b5dbc1d (DatabasePersistenceManager.java, line 441)
java.lang.OutOfMemoryError: Java heap space
        at java.util.Properties$LineReader.<init>(Properties.java:346)
        at java.util.Properties.load(Properties.java:284)
"
0,"NodeTypeRegistry.unregisterNodeTypes(Collection) missingwhen having nodetypes depending on each other, or even have cyclic dependencies, they must be unregistered in the correct sequence. 

i suggest a NodeTypeRegistry.unregisterNodeTypes(Collection) for that case."
1,"Data store garbage collection: inUse not correctly synchronizedAccess to the fields DbDataStore.inUse and FileDataStore.inUse is not synchronized.
This is a problem when concurrently calling GarbageCollector.deleteUnused() 
and accessing the data store (ConcurrentModificationException is thrown)."
1,NamespaceRegistryImpl.getNameResolver/getPathResolver always return nullThis seems to be a left over from restructuring commons classes: JCR-1169. Those methods should be removed.
1,"Text Extractor: Image parser throws exception (jpeg)the below exception is thrown over an over while uploading jpeg images:
16.11.2009 17:20:42 *WARN * LazyTextExtractorField: Failed to extract text from a binary property (LazyTextExtractorField.java, line 165)
org.apache.tika.exception.TikaException: TIKA-198: Illegal IOException from org.apache.tika.parser.image.ImageParser@c7bc3
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:125)
	at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:105)
	at org.apache.jackrabbit.core.query.lucene.LazyTextExtractorField$ParsingTask.run(LazyTextExtractorField.java:160)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:417)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:269)
	at java.util.concurrent.FutureTask.run(FutureTask.java:123)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:65)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:168)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:650)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:675)
	at java.lang.Thread.run(Thread.java:613)
Caused by: javax.imageio.IIOException: Not a JPEG file: starts with 0x00 0x05
	at com.sun.imageio.plugins.jpeg.JPEGImageReader.readImageHeader(Native Method)
	at com.sun.imageio.plugins.jpeg.JPEGImageReader.readNativeHeader(JPEGImageReader.java:554)
	at com.sun.imageio.plugins.jpeg.JPEGImageReader.checkTablesOnly(JPEGImageReader.java:309)
	at com.sun.imageio.plugins.jpeg.JPEGImageReader.gotoImage(JPEGImageReader.java:431)
	at com.sun.imageio.plugins.jpeg.JPEGImageReader.readHeader(JPEGImageReader.java:547)
	at com.sun.imageio.plugins.jpeg.JPEGImageReader.getHeight(JPEGImageReader.java:609)
	at org.apache.tika.parser.image.ImageParser.parse(ImageParser.java:47)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:119)
	... 10 more"
1,"ConcurrentModificationException in CacheManager.Using the test code below, I was able to produce this stack:

java.util.ConcurrentModificationException
	at java.util.WeakHashMap$HashIterator.nextEntry(WeakHashMap.java:762)
	at java.util.WeakHashMap$KeyIterator.next(WeakHashMap.java:795)
	at org.apache.jackrabbit.core.cache.CacheManager.logCacheStats(CacheManager.java:164)
	at org.apache.jackrabbit.core.cache.CacheManager.cacheAccessed(CacheManager.java:137)
	at org.apache.jackrabbit.core.cache.AbstractCache.recordCacheAccess(AbstractCache.java:122)
	at org.apache.jackrabbit.core.cache.ConcurrentCache.get(ConcurrentCache.java:122)
	at org.apache.jackrabbit.core.state.MLRUItemStateCache.retrieve(MLRUItemStateCache.java:71)
	at org.apache.jackrabbit.core.state.ItemStateReferenceCache.retrieve(ItemStateReferenceCache.java:139)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.getNonVirtualItemState(SharedItemStateManager.java:1716)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.getItemState(SharedItemStateManager.java:268)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.getNodeState(LocalItemStateManager.java:110)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.getItemState(LocalItemStateManager.java:175)
	at org.apache.jackrabbit.core.state.XAItemStateManager.getItemState(XAItemStateManager.java:260)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:161)
	at org.apache.jackrabbit.core.ItemManager.getItemData(ItemManager.java:382)
	at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:328)
	at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:622)
	at org.apache.jackrabbit.core.ItemManager.getRootNode(ItemManager.java:531)
	at org.apache.jackrabbit.core.SessionImpl.getRootNode(SessionImpl.java:760)
	at test.JackrabbitTest$1.run(JackrabbitTest.java:37)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)

-------------------------

package test;

import java.io.File;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicInteger;

import javax.jcr.Repository;
import javax.jcr.RepositoryException;
import javax.jcr.Session;
import javax.jcr.SimpleCredentials;

import org.apache.jackrabbit.core.TransientRepository;

public class JackrabbitTest {

 public static void main(final String[] args) throws Exception {
   File dir = File.createTempFile(""jackrabbit-test"", """");
   dir.delete();
   dir.mkdir();
   System.out.println(""created temporary directory: "" +
       dir.getAbsolutePath());
   dir.deleteOnExit();

   final Repository jcrRepo = new TransientRepository(dir);
   final AtomicBoolean passed = new AtomicBoolean(true);
   final AtomicInteger counter = new AtomicInteger(0);
   ExecutorService executor = Executors.newFixedThreadPool(50);
   Runnable runnable = new Runnable() {

     @Override
     public void run() {
       try {
         Session session = jcrRepo.login(
             new SimpleCredentials(""admin"",
                 ""admin"".toCharArray()));
         session.getRootNode().addNode(""n"" +
                 counter.getAndIncrement()); //unique name
         session.save();
         session.logout();
       } catch (RepositoryException e) {
         e.printStackTrace();
         passed.set(false);
       }
     }

   };
   System.out.println(""Running threads"");
   for (int i = 0; i<  500; i++) {
     executor.execute(runnable);
   }
   executor.shutdown(); //Disable new tasks from being submitted
   if (!executor.awaitTermination(120, TimeUnit.SECONDS)) {
     System.err.println(""timeout"");
     System.exit(1);
   }
   if (!passed.get()) {
     System.err.println(""one or more threads got an exception"");
     System.exit(1);
   } else {
     System.out.println(""all threads ran with no exceptions"");
     System.exit(0);
   }

 }

}
"
0,"Add PlainTextExtractor to default configuration of TransientRepositoryThe current default configuration of TransientRepository does not include the PlainTextExtractor, which means resources of type text/plain are not fulltext indexed. The PlainTextExtractor should be added to the configuration."
0,"TCK: Node types selected by SetValueValueFormatExceptionTest may lead to incorrect test failureDuring setup, the test selects two node types, one with a BOOLEAN property and one with a DATE property, and creates nodes of these types as children of the target node.  Having the TCK select these node types creates two problems:  First, JSR-170 does not require an implementation to provide deterministic, stable ordering of node types returned by NodeTypeManager.  Consequently, the node types selected during test setup may vary from run to run, making test configuration difficult or impossible.  Second, if a repository imposes implementation-specific type constraints not discoverable through JSR-170, the TCK may select a node type inappropriate as a child of the test node.

Proposal: introduce configuration properties which, if set, override the property definition and node type selected by NodeTypeUtil.
"
0,"Cache also failed principal lookupsThe principal cache in Jackrabbit normally does a good job in ensuring good performance in critical areas like ACL evaulation. However, the cache only includes successful principal lookups, so an ACE that references a missing (or mistyped) principal can cause notable performance issues as a new principal lookup is needed whenever the node covered by such an ACL is accessed.

To solve that problem I propose that we extend the principal cache to also cover negative principal lookups."
1,"NullPointerException in DatabasePersistenceManager and DatabaseFileSystem after a failed reconnection attemptAs reported on the dev mailing-list, this is what happens:

The reconnection/retry mechanism in DatabasePersistenceManager/DatabaseFileSystem seems to behave fine when the connection times out or is killed for some reason, and the DB server is in fact still running.
However there is a problem if the connection cannot be re-established directly, for example if a transient network outage lasts longer than the few reconnection attempts.
Inside DatabasePersistenceManager.reestablishConnection(), initConnection() will fail, and the preparedStatements map will stay empty.
This in turn will trigger a nasty NullPointerException (never caught) next time executeStmt() is called, because the map is still empty, and there is no check for that.


The following proposed fix from Stefan Guggisberg has been tested to work when applied on 1.2-rc2:

> the simplest fix would be to remove line 783 in
> DatabasePersistenceManager.java and line 1010 in
> DatabaseFileSystem.java,
> i.e. the following stmt:
> 
 >      preparedStatements.clear();
"
0,"TCK: LockTest.testGetLock compares Nodes with equalsi think comparison by 'isSame' would be better.

-> line 545

-        assertTrue(""lock holding node must be parent"", lock.getNode().equals(n1));
+        assertTrue(""lock holding node must be parent"", lock.getNode().isSame(n1));"
0,"Add isDeclaredMember() method to GroupI suggest to add a method for checking whether an authorizable is a declared member of a group. Currently there is only a method for checking membership (which includes indirect memberships). 

"
0,"H2PersistenceManager: no need to call shutdown; javadoc bugsThe H2PersistenceManager implementation calls ""shutdown"" to force closing the database when using file based databases. There is no need to do that when using the H2 database engine: the database is closed automatically when the last connection is closed.

Also, the javadocs of the H2PersistenceManager need to be fixed.
"
0,"Remove dependency on  EDU.oswego.cs.dl.util.concurrentEDU.oswego.cs.dl.util.concurrent is in maintenance mode, and http://g.oswego.edu/dl/classes/EDU/oswego/cs/dl/util/concurrent/package-summary.html advises to migrate to the JDK5 java.util.concurrent package.
"
1,"InternalVersionManagerBase.calculateCheckinVersionName will fail with NPE upon empty predecessors property(Note: this can only happen on inconsistent version storage)

We should add a check here, and throw a more descriptive exception."
1,"Restart of RMI-component fails (because it's not released while shutdown)I just moved setup model for the Jackrabbit repository from a Tomcat-global JNDI-datasouce to a autonomous server connected via RMI to get rid off the problem of a total restart of the tomcat, if e.g. something is changed in the jackrabbit setup.

But the restart of the RMI component of the jackrabbit server package will fail, because on shutdown the rmi binding isn't released. From that, at restart, the socket is still in use and the (just) RMI component fails to start. In the other hand, it isn't possible to connect to the server through the remaining rmi component; you'll get a EOF-exception in RMI communication. Of course, a complete restart of the Tomcat will help, but isn't appropriate. 

It looks to me like just some release on shutdown is missing. May somebody provide a patch?

(log exception at restart)
20080306-093849.086 INFO  [ajp-8009-2] [] [RepositoryStartupServlet] Cannot create Registry
java.rmi.server.ExportException: Port already in use: 1099; nested exception is: 
        java.net.BindException: Address already in use
        at sun.rmi.transport.tcp.TCPTransport.listen(TCPTransport.java:249)
        at sun.rmi.transport.tcp.TCPTransport.exportObject(TCPTransport.java:184)
        at sun.rmi.transport.tcp.TCPEndpoint.exportObject(TCPEndpoint.java:382)
        at sun.rmi.transport.LiveRef.exportObject(LiveRef.java:116)
        at sun.rmi.server.UnicastServerRef.exportObject(UnicastServerRef.java:180)
        at sun.rmi.registry.RegistryImpl.setup(RegistryImpl.java:92)
        at sun.rmi.registry.RegistryImpl.<init>(RegistryImpl.java:68)
        at java.rmi.registry.LocateRegistry.createRegistry(LocateRegistry.java:222)
        at org.apache.jackrabbit.j2ee.RepositoryStartupServlet.registerRMI(RepositoryStartupServlet.
        at org.apache.jackrabbit.j2ee.RepositoryStartupServlet.startup(RepositoryStartupServlet.java
        at org.apache.jackrabbit.j2ee.RepositoryStartupServlet.init(RepositoryStartupServlet.java:21
        at javax.servlet.GenericServlet.init(GenericServlet.java:212)
"
1,"default value with autocreated fields like nullI have a custom nodetype which has two properties, both autocreated (but not mandatory).
If I create the node and want to read the first property (not filling in any value), I get a:

javax.jcr.RepositoryException: Unable to get value of /pages/mjo:page/jcr:content/mjo:title:java.lang.ArrayIndexOutOfBoundsException: 0
 at org.apache.jackrabbit.core.PropertyImpl.getValue(PropertyImpl.java:378)
 at de.freaquac.test.JCRTest.propIterator(JCRTest.java:207)

After setting the property the exception is no longer thrown. Not sure if this really an error but is a little bit anyoing. Are there any default values (null) in jsr170?"
0,Build instructions for contributionsAdd READMEs in order to instruct how the contribution projects can ne built
1,"DefaultProtectedPropertyImporter masks several fields from parent, causing potential derived classes to not perform correctlyThe fields session, resolver, and referenceTracker are duplicated in DefaultProtectedPropertyImporter from DefaultProtectedItemImporter, and thus future derived classes will not function correctly if they attempt to use those fields, as they will be null.

I Plan to remove them from DefaultProtectedPropertyImporter"
0,"Default blob size for mysql ddl too smallthe default datatype for:
NODE.NODE_DATA
PROP.PROP_DATA
REFS.REFS_DATA 
in the mysql ddl is ""BLOB"" which is pretty small to the default size in other dbs.

When playing with a (not very large) jackrabbit repo using mysql for persistence I easily got data truncation errors on both NODE.NODE_DATA and PROP.PROP_DATA columns. The same issue has been reported in the past by other users.
Although anyone could easily create a custom ddl with larger fields it should be nice to increase the blob size in the mysql ddl embedded in jackrabbit, in order to avoid this kind of problems for new users (you usually learn this the hard way, when the number of nodes in your repository starts to grow and jackrabbit start throwing errors :/).
Changing BLOB to MEDIUMBLOB will make the default size for mysql more similar to the one in other dbs, without critically increasing the used space...

"
0,[PATCH] Add Column and line numbers to repository.xml parse exception messagesCode caught SAXExceptions when the an xml parsing exception occurred parsing the repository.xml. But catching SAXParseException (a subclass) allows access to column and line numbers of the problem. So also catch this exception and add this to the exception message to make it easier to fix errors.
0,"Handling of binary properties (streams) in QValue interfaceThe current SPI requires QValue to return new streams upon each call to getStream(). As far as I can tell, this essentially requires a QValue implementation to preserve the whole content of a stream, be it in memory or on disk.

In particular (and unless I'm missing something), when importing large content into a repository, this causes the whole data stream to be written twice. We really should try to avoid that.
"
1,"Unable to save session after saving a renamed node		TransientRepository repo = new TransientRepository(
				""applications/test/repository.xml"", ""applications/test"");
		Session s = repo.login(new SimpleCredentials(""test"", """".toCharArray()));

		if (s.getRootNode().hasNode(""parent"")) {
			s.getRootNode().getNode(""parent"").remove();
			s.save();
		}

		// create parent node
		Node parent = s.getRootNode().addNode(""parent"");
		
		// create node to rename
		parent.addNode(""newnode"");
		s.save();

		// rename node
		s.move(""/parent/newnode"", ""/parent/renamedNewNode"");

		// save renamed node
		s.getRootNode().getNode(""parent/renamedNewNode"").save();

		// try to save session --> FAILS
		s.save();

		s.logout();"
0,"allow subclassing of NodeTypeReaderin working towards an offline tool to import custom namespaces and node types, i found that i needed to make some small changes to NodeTypeReader and DOMWalker so that i could subclass NodeTypeReader and access the namespaces specified in the node type definition file. see the attached patch.
"
0,"Allow extendability of RepositoryImpl.WorkspaceInfothe workspace info has some package private and some protected methods. in order to be able to extend the workspace info, we need to have all methods protected."
0,"maven 2 poms contain variables in dependency versions that are never resolved, breaking transitive dependency resolutionThere are problems with the dependencies declared in jackrabbit-server-1.0 and other poms using variables for dependency versions. These variables are never resolved and break the transitive resolution of these dependencies"
0,"Set omit term freq positions flag on parent field in the indexThe flag to omit term frequencies is set to true by default and it is not changed by any of the constructors on the Field class.
We don't use this info in the index anyway, so it is safe to remove it.

The index size gain for 130K nodes (3 leves, ~50 nodes per level) is around 150kb for a 30mb index."
1,"Inconsistencies in BitSetKey comparisonHi,

I encountered a problem with the BitsetENTCacheImpl and the BitsetKey comparisons. I have 3 bitsets A, B and C , defined as :

A : bits 0,4,17,38,60,63 
B : bits 4,17,38,52,59,60
C : bits 0,17,38,60,61,63

If call BitsetKey.compareTo  method on each pair , i get : 

A < B
B < C
C < A

which is not correct and leads to inconsistencies in the TreeSet.

All 2 bitsets are contained in one single word (max bit is 63). So, the method is comparing first the 32 MSB - which are enough in that case to compare the bits. But the problem is, that the difference between the 32 MSB of B and C is too big to fit in an integer : for B, we have 403701824 - for C , 2952790080 . The difference between both is 2549088256 (positive) , which is bigger than Integer.MAX_VALUE , and makes a  -1745879040 (negative) after casting to an int .

In order to fix that, the shift should either be bigger in order to fit a signed integer ( 33 instead of 32 ), or a simple -1 / 0 / +1 could be returned
"
1,"Repository.login throws IllegalStateExceptionCalling any login method on Repository instance, which has been shut down throws an IllegalStateException, which is caused by the RepositoryImpl.sanityCheck method.

This exception is unexpected by callers of the login method, which is specified to throw one of LoginException, NoSuchWorkspaceException and RepositoryException. In particular the spec says, that a RepositoryException is thrown ""if another error occurs"".

So I suggest to modify the RepositoryImpl.login(Credentials, String) as follows (patch against trunk):

Index: /usr/src/jackrabbit/trunk/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/RepositoryImpl.java
===================================================================
--- /usr/src/jackrabbit/trunk/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/RepositoryImpl.java	(revision 706543)
+++ /usr/src/jackrabbit/trunk/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/RepositoryImpl.java	(working copy)
@@ -1358,6 +1358,8 @@
         } catch (AccessDeniedException ade) {
             // authenticated subject is not authorized for the specified workspace
             throw new LoginException(""Workspace access denied"", ade);
+        } catch (RuntimeException re) {
+            throw new RepositoryException(re.getMessage(), re);
         } finally {
             shutdownLock.readLock().release();
         }
"
1,"Indexing configuration ignored when indexing lengthThe NodeIndexer does not respect the indexing configuration when it adds a field for the property length. NodeIndexer.addValue(Document doc, InternalValue value, Name name) should check it the property actually needs to be indexed."
0,"JMX Bindings for JackrabbitThere has been a slight interest in the past for adding JMX support.
This would be the first stab at it. It is a Top 15 slow query log. (the 15 part is configurable)

It is not enabled by default, so just being there should not affect the overall performance too much. You can enable it and play a little, tell me what you think.
I've also added a test case that does a duration comparison with and without the logger.

The most important part of this issue is that it should open the way for some proper monitoring tools support around queries, caches, anything/everything Jackrabbit.

As usual, please let me know what you guys think"
1,Versioning might no be thread safecheck versioning for thread safeness
0,"Access control for repository level API operationsit is a open issue (i guess since jackrabbit 1.0) that the repository level write operations lack any kind of permission check.
this issues has been raised during specification of jsr 283 [1] but didn't made it into the specification (left to implementation).

in jackrabbit 2.0 this affects the following parts of the API

- namespace registration
- node type registration
- workspace creation/removal

based on a issue reported by david (""currently an anonymous user can write the namespace registry which is probably
undesirable [...]""), we could at least add some minimal restrictions. In addition i would like to take up this discussion
for jsr 333.

[1] https://jsr-283.dev.java.net/issues/show_bug.cgi?id=486"
1,"DatabaseJournal: java.lang.IllegalStateException: already in batch modeUsing the database journal (any database) fails with the following stack trace:

java.lang.IllegalStateException: already in batch mode
	at org.apache.jackrabbit.core.util.db.ConnectionHelper.startBatch(ConnectionHelper.java:212)
	at org.apache.jackrabbit.core.journal.DatabaseJournal.doSync(DatabaseJournal.java:449)
	at org.apache.jackrabbit.core.journal.AbstractJournal.lockAndSync(AbstractJournal.java:254)
	at org.apache.jackrabbit.core.journal.DefaultRecordProducer.append(DefaultRecordProducer.java:51)
	at org.apache.jackrabbit.core.cluster.ClusterNode$WorkspaceUpdateChannel.updateCreated(ClusterNode.java:539)
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:559)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:1457)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:1487)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:351)
	at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:354)"
1,"UUIDDocId.getDocumentNumbers() may return illegal valueHappens when the node with the given UUID is not present in the index. The method then returns -1, which is illegal. Document numbers must be >= 0. The method must returns an empty array when the id is invalid, as documented in DocId.getDocumentNumbers()."
0,serious performance degradation of node operations when node has a large number of child nodes (e.g. > 10k child node entries)
1,"IllegalStateException thrown when consuming eventsi assume, when session is closed, or beeing closed, the observation still tries to deliver some events:

[java] 2005-11-24 22:58:41,764 [ObservationManager] WARN  org.apache.jackrabbit.core.observation.ObservationManagerFactory - EventConsumer threw exception: java.lang.IllegalStateException: not in
itialized
[java] 2005-11-24 22:58:41,764 [ObservationManager] DEBUG org.apache.jackrabbit.core.observation.ObservationManagerFactory - Stacktrace:
[java] java.lang.IllegalStateException: not initialized
[java]     at org.apache.jackrabbit.core.security.SimpleAccessManager.isGranted(SimpleAccessManager.java:119)
[java]     at org.apache.jackrabbit.core.observation.EventConsumer.consumeEvents(EventConsumer.java:231)
[java]     at org.apache.jackrabbit.core.observation.ObservationManagerFactory.run(ObservationManagerFactory.java:161)
[java]     at java.lang.Thread.run(Thread.java:595)

"
0,Move tests that require indexing configurationTests that require an indexing configuration should be moved to the indexing-test workspace. This allows us to remove the indexing configuration from the default workspace and speed up the JCR API tests.
0,"Allow users to disable validationDigesterMapperImpl leaves validating set to default true when creating a DigesterDescriptorReader.

But as the dtd is not available anywhere (published or in the source), it is usually not declared in mapping files, and DigesterDescriptorReader complains about it.

Could it be possible to leave the user a way to configure the validation? The simpliest way would be to add this constructor to DigesterMapperImpl :

    public DigesterMapperImpl(InputStream[] streams, boolean validate) {
        descriptorReader = new DigesterDescriptorReader(streams);
        DigesterDescriptorReader.class.cast(descriptorReader).setValidating(validate);
        this.buildMapper();
    }

Best regard,

Stephane Landelle"
0,add framework for performance testsAdd a test suite for running various kinds of performance tests.
0,"Weird locking behaviour in CachingHierarchyManagerin some of our itegration tests the repository sometime locks-up with a stacktrace like this:

""Thread-38"" daemon prio=5 tid=0x08cb3908 nid=0xdd8 runnable [9fef000..9fefd90]
        at org.apache.jackrabbit.core.CachingHierarchyManager.removeLRU(CachingHierarchyManager.java:540)
        - waiting to lock <0x16a9b0e0> (a java.lang.Object)
        at org.apache.jackrabbit.core.CachingHierarchyManager.cache(CachingHierarchyManager.java:510)
        - locked <0x16a9b0e0> (a java.lang.Object)
        at org.apache.jackrabbit.core.CachingHierarchyManager.buildPath(CachingHierarchyManager.java:163)
        at org.apache.jackrabbit.HierarchyManagerImpl.buildPath(HierarchyManagerImpl.java:296)
[...]

although i think that this sacktrace is valid (a thread holding a monitor waiting to lock it again) this scenario shows up a lot during stress-testing. i assume it's the JIT that messesup synchornization. 

the fix is to remove the double monitor entry in this case."
1,"Single quote in contains function is not parsed correctlyIf there is a single quote in the contains statement the parser will throw an exception.

Example:
//element(*, nt:resource)[jcr:contains(., 'it''s fun')]

The LuceneQueryBuilder replaces the single quote with a double quote and hence the lucene fulltext query parser fails because there is a missing closing double quote. Not sure why this is done in the code, maybe this is a left over from an early JSR 170 draft."
0,"SetValueBinaryTest: some repositories have constraints on where binary properties can be setFor repositories that do only support binary properties in the form of jcr:content/jcr:data, the current configurability is not sufficient. To avoid more config params, I'd suggest to check for propertyName1 == ""jcr:data"" && node.hasNode(""jcr:content""), and in that case to automatically navigate down to the ""jcr:content"" child node.

"
1,NPE in ConsolidatingChangeLogThe hasSNS(NodeId) method in ConsolidatingChangeLog throws an NPE when nodeId is null. It should rather return false. 
0,"war missing jcr jar dropping the latest war (from latest svn) presents with this error when i point my browser to http://localhost:8080/jackrabbit-webapp-1.4-SNAPSHOT/. Simple solution is to make sure the jcr-1.0.jar is added to the generated war.

org.apache.jasper.JasperException: Unable to compile class for JSP: 

An error occurred at line: 1 in the generated java file
The type javax.jcr.Repository cannot be resolved. It is indirectly referenced from required .class files

An error occurred at line: 9 in the generated java file
The import javax.jcr.Repository cannot be resolved

An error occurred at line: 27 in the jsp file: /index.jsp
Repository cannot be resolved to a type
24: </head>
25: <body style=""font-family:monospace"">
26: <%
27:     Repository rep;
28:     try {
29:         rep = RepositoryAccessServlet.getRepository(pageContext.getServletContext());
30:     } catch (Throwable e) {


An error occurred at line: 84 in the jsp file: /index.jsp
Repository.REP_VENDOR_URL_DESC cannot be resolved to a type
81:     </li>
82: </ol>
83: <p/>
84: <hr size=""1""><em>Powered by <a href=""<%= rep.getDescriptor(Repository.REP_VENDOR_URL_DESC) %>""><%= rep.getDescriptor(Repository.REP_NAME_DESC)%></a> version <%= rep.getDescriptor(Repository.REP_VERSION_DESC) %>.</em>
85: </body>
86: </html>


An error occurred at line: 84 in the jsp file: /index.jsp
Repository.REP_NAME_DESC cannot be resolved to a type
81:     </li>
82: </ol>
83: <p/>
84: <hr size=""1""><em>Powered by <a href=""<%= rep.getDescriptor(Repository.REP_VENDOR_URL_DESC) %>""><%= rep.getDescriptor(Repository.REP_NAME_DESC)%></a> version <%= rep.getDescriptor(Repository.REP_VERSION_DESC) %>.</em>
85: </body>
86: </html>


An error occurred at line: 84 in the jsp file: /index.jsp
Repository.REP_VERSION_DESC cannot be resolved to a type
81:     </li>
82: </ol>
83: <p/>
84: <hr size=""1""><em>Powered by <a href=""<%= rep.getDescriptor(Repository.REP_VENDOR_URL_DESC) %>""><%= rep.getDescriptor(Repository.REP_NAME_DESC)%></a> version <%= rep.getDescriptor(Repository.REP_VERSION_DESC) %>.</em>
85: </body>
86: </html>


Stacktrace:
	org.apache.jasper.compiler.DefaultErrorHandler.javacError(DefaultErrorHandler.java:92)
	org.apache.jasper.compiler.ErrorDispatcher.javacError(ErrorDispatcher.java:330)
	org.apache.jasper.compiler.JDTCompiler.generateClass(JDTCompiler.java:423)
	org.apache.jasper.compiler.Compiler.compile(Compiler.java:308)
	org.apache.jasper.compiler.Compiler.compile(Compiler.java:286)
	org.apache.jasper.compiler.Compiler.compile(Compiler.java:273)
	org.apache.jasper.JspCompilationContext.compile(JspCompilationContext.java:566)
	org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:317)
	org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:320)
	org.apache.jasper.servlet.JspServlet.service(JspServlet.java:266)
	javax.servlet.http.HttpServlet.service(HttpServlet.java:803)

note The full stack trace of the root cause is available in the Apache Tomcat/6.0.14 logs."
0,"WebDAV: add support for DAV:lockrootsee http://www.webdav.org/specs/rfc4918.html#ELEMENT_lockroot

this element has been added with RFC 4918.
add constant to the DAVConstants and extend ActiveLock interface accordingly."
0,"Request for other RMI binding options in RepositoryStartupServletThe current deployment options for RepositoryStartupServlet bind a local repository to JNDI and/or register a remote ServerRepository via RMI using an RMIServerSocketFactory and LocateRegistry. 

The LocateRegistry mechanism does not appear to work by default in a Weblogic environment.

I would like to request the option of binding a ServerRepository in registerRMI() to JNDI using the servlets current context instead of attempting LocateRegistry.createRegistry() and then LocateRegistry.getRegistry(). This JNDI binding option could use a different name so as to not interfere with the JNDI binding attempted in registerJNDI().

For example, if requested in the web.xml config, registerRMI() could bind the following using it's reference to a ServerRepository:

      Context ctx = new InitialContext();
      ctx.bind(repositoryName + ""Remote"", remote);

This allows for easy remote access using weblogic's native T3 protocol using the following on the client with:

      Context ctx = new InitialContext(); // with say -Djava.naming.provider.url and -Djava.naming.factory.initial set 
      Object ref = ctx.lookup(repositoryName + ""Remote"");
      LocalAdapterFactory laf = new ClientAdapterFactory();
      Repository remote = laf.getRepository((RemoteRepository) ref);

From the initial tests I have done this appears to work well inside the Weblogic container. 

- Paul."
1,"Data Store: Oracle fails to create the tableWhen using an Oracle database, the following exception occurs when trying to create the table: ORA.00902: invalid datatype
The problem is that Oracle doesn't support the data type BIGINT. Instead, LONG should be used."
0,Add new JSR283 features to CND reader/writerthe current CND parser(s) and writers do not support the new options specified by JSR283
1,"Data Store: garbage collection continues when the session is closedCurrently, the data store garbage collection continues even if the session is closed.

This can cause problems."
0,"FileDataStore performance improvementsAs seen in JCR-2695, the FileDataStore is slow on some file system. 

Some file operations such as File.exists() or File.isDirectory() can be replaced with try / catch, or by inspecting the return value of a previous method (File.renameTo)."
0,Update Jackrabbit API JavaDoc on http://jackrabbit.apache.orgUpdate the JavaDoc on http://jackrabbit.apache.org to the latest release version of jackrabbit.
1,Remove double quote as illegal XPathSearchChar from helper method in TextSince Lucene 2.4 the double quote character at the end of a search term is no more illegal and must not be escaped
1,"jcr2spi: transient removal of mandatory item throws ConstraintViolationExceptionreported by tobi:

the transient removal of a mandatory (non-protected) item immediately fails. 
instead the check should be postponed until the save() call, since it would be perfectly legal to remove the mandatory item and then re-add it.

suggested fix:
ItemStateValidator#checkRemoveConstraints should only check for protection and ignore mandatory definitions."
0,"Node.removeMixin() should not remove valid itemsassume you define a mixin like:

[test] mix
- aprop (string)
+ anode (nt:base)

and you add this mixin to a nt:unstructured and add 'anode' and set 'aprop'.
then a subsequent node.removeMixin(""test"") will also remove 'anode' and 'aprop' although they are valid by the definition of nt:unstructured.

imo, the items should only be removed if they become invalid by the definition of the resulting effective node type.
"
0,"Reduce number of different repository.xml present with jackrabbit-corewhile taking a look at the repository configuration and the related test-cases, i saw that there are quite some repository.xml files around... which i think is a bit confusion and probably hard to maintain once we make
changes to the config.

i would to suggest to consolidate that and - if possible - get rid of some of them.
if we can't i would suggest to put some comment in every of the different configuration files indicating
what they are used for.

from what i've seen so far (still missing complete overview)

1) http://svn.apache.org/repos/asf/jackrabbit/trunk/jackrabbit-core/src/main/config/repository.xml

    Current comment: <!-- Example Repository Configuration File -->
    Usage: ??

2) http://svn.apache.org/repos/asf/jackrabbit/trunk/jackrabbit-core/src/main/resources/org/apache/jackrabbit/core/test-repository.xml

   current comment: -  
   Used as repository configuration in org.apache.jackrabbit.core.TestRepository.java
   

3) http://svn.apache.org/repos/asf/jackrabbit/trunk/jackrabbit-core/src/main/resources/org/apache/jackrabbit/core/repository.xml
  
   current comment: <!-- Example Repository Configuration File -->

   Used by org.apache.jackrabbit.core.config.RepositoryConfigTest.java in order to create another repository.xml 
   under target/test-repository.xml. a bit confusing given the fact, that a test-repository.xml exists as well. I would
   suggest to rename the REPOSITORY_XML constant in RepositoryConfigTest.


4) http://svn.apache.org/repos/asf/jackrabbit/trunk/jackrabbit-core/src/test/repository/repository.xml

   current comment: <!-- Example Repository Configuration File -->
   Usage: i assume, that is the one referenced in test/resources/repositoryStubImpl.properties

5) http://svn.apache.org/repos/asf/jackrabbit/trunk/jackrabbit-core/src/test/resources/org/apache/jackrabbit/core/config/repository.xml

   current comment: <!-- Example Repository Configuration File -->
   Usage: ?? 


"
1,"Node.hasProperty() with relative path can throw ClassCastExceptionCalling Node.hasProperty() with a relative path that traverses higher than the root node will throw a ClassCastException because the ItemId returned by HierarchyManagerImpl.resolvePath() will be the root node id.  The blind cast in the HierarchyManagerImpl.resolvePropertyPath() will then throw the ClassCastException.  This issue is not just with hasProperty/resolvePropertyPath, but any call to resolvePath that goes higher than the root node, will wrongfully get the root node id returned as result.
"
0,"move core module to a subdirectoryActually the jackrabbit svn holds the code for the main module in the top level dir
http://svn.apache.org/repos/asf/incubator/jackrabbit/trunk/
and all the subprojects in subdirectories
http://svn.apache.org/repos/asf/incubator/jackrabbit/trunk/contrib/

given this layout is not possible to checkout from svn only the main module (if you get trunk, you get all), and you can't work to different modules using any IDE which doesn't support nested projects (namely Eclipse).

I would like to request moving the main module (that means moving all the files and directories in trunk except ""contrib"") from 
http://svn.apache.org/repos/asf/incubator/jackrabbit/trunk/
to 
http://svn.apache.org/repos/asf/incubator/jackrabbit/trunk/jackrabbit
following the usual organization of maven-based projects and solving these problems...


"
1,can't add lock token to session after 3 login/logoutI login and lock a file and logout. Perfoms a new login and add the previous lock token to the current session because I want to unlock this file. This works fine. But if I do a new logout/login I can't unlock the file (the file is locked). It is best understanded looking at the test case.
1,"NullPointerException on removing a node acquired from search resultwith a code snipped like the following, i get a NullPointerException in ItemState:

Session s = repo.login(sc,workspace);
QueryManager qm = s.getWorkspace().getQueryManager();
Query q = qm.createQuery(""SELECT * FROM [nt:unstructured]"", Query.JCR_SQL2);
QueryResult r = q.execute();
NodeIterator i = r.getNodes();

Node n = i.nextNode();
n.remove(); // breaks here with NullPointerException

Exception in thread ""main"" java.lang.NullPointerException                                                                                                                                                                                   
        at org.apache.jackrabbit.jcr2spi.state.ItemState.getParent(ItemState.java:210)                                                                                                                                                      
        at org.apache.jackrabbit.jcr2spi.operation.Remove.create(Remove.java:98)                                                                                                                                                            
        at org.apache.jackrabbit.jcr2spi.ItemImpl.remove(ItemImpl.java:306) "
0,"Contrib JCR-Server: improve handing of strong etagscopied from dev-mail:

[..]

so... could we
- delegate the calculation of the etag to the
  commands, letting them decide on whether they are able
  to provide any and whether it would be a strong or a
  weak one?
- remove that additional method in NodeResource that is
  not used? "
0,"SISM blocks the item state cache when loading a new itemThe SharedItemStateManager.getNonVirtualItemState() method contains a loadItemState() call within a ""synchronized (cache)"" block. This prevents all item state cache access while a new item is being loaded from the persistence manager. I have at least one case where this has caused a serious performance drop, essentially synchronizing repository access for all readers."
0,RepositoryUtil moved outside of main source treeIt appears that the RepositoryUtil class was moved from src/main to src/test. This class is used by the ocm-spring project.
1,"NullPointerException in constructor of JcrDavExceptionwithin DavSessionProviderImpl.attachSession() a org.apache.jackrabbit.rmi.client.RemoteRepositoryException is thrown, catched as RepositoryException and passed into the 
ctor of JcrDavException

the lookup for the class in the static HashMap codeMap for the class fails and a NPE is thrown 

suggested fix:
The static lookup by the javax.jcr.*Exception classed must be fixed to include derived exception classes.

"
1,"Indexing rules inheritance doesn't workIndexing rules are supposed to be inherited by children node types.
In org.apache.jackrabbit.core.query.lucene.IndexingConfigurationImpl.init, rules are registered for the declared node type and all its children. However, as the rule's node type is still the original one, the rule gets rejected in org.apache.jackrabbit.core.query.lucene.IndexingConfigurationImpl$IndexingRule.appliesTo.

One simple solution would be to register not the original rule, but a copy where the original node type has been replaced by the child one.

Please find corrected class attached.

Sincerely,

Stéphane"
0,"Tablespace (Filegroup) support for MS SQL ServerTablespace support was added for Oracle database servers in this issue

https://issues.apache.org/jira/browse/JCR-968

We would like tablespace (or filegroup) support for MS SQL Server as well. To address this, we have created a patch using the trunk of JackRabbit that closely follows the patch in the above named issue."
1,"OutOfMemoryError when re-indexing the repository[ERROR] 20060825 17:06:40
(org.apache.jackrabbit.core.observation.ObservationManagerFactory) -
Synchronous EventConsumer threw exception. java.lang.OutOfMemoryError

when we try to re-index a repository, the repository is quite big (more then 4 Gb of disk usage) and sometimes it stores 40Mb size documents.

As attach I put all the last logs we registered, with the full stack traces.

Related to this whe have also errors with Lucene:

[DEBUG] 20060803 08:24:01 (org.apache.jackrabbit.core.query.LazyReader)
- Dump: 
java.io.IOException: Invalid header signature; read 8656037701166316554,
expected -2226271756974174256
        at org.apache.jackrabbit.core.query.MsWordTextFilter

and then this ones:

[DEBUG] 20060803 08:37:17 (org.apache.jackrabbit.core.ItemManager) -
removing item 8637bf5f-4689-4e75-888f-b7b89bef40c8 from cache
[ WARN] 20060803 08:40:13 (org.apache.jackrabbit.core.RepositoryImpl) -
Existing lock file at C:\Wave\Repository\.lock deteteced. Repository was
not shut down properly.
[ERROR] 20060803 09:33:14
(org.apache.jackrabbit.core.observation.ObservationManagerFactory) -
Synchronous EventConsumer threw exception.
java.lang.NullPointerException: null values not allowed

this is our repository.xml configuration for indexing

<SearchIndex
class=""org.apache.jackrabbit.core.query.lucene.SearchIndex"">
        <param name=""path"" value=""${wsp.home}/index""/>
        <param name=""textFilterClasses""
value=""org.apache.jackrabbit.core.query.lucene.TextPlainTextFilter,
org.apache.jackrabbit.core.query.MsExcelTextFilter,
org.apache.jackrabbit.core.query.MsPowerPointTextFilter, 
org.apache.jackrabbit.core.query.MsWordTextFilter,
org.apache.jackrabbit.core.query.PdfTextFilter,
org.apache.jackrabbit.core.query.HTMLTextFilter,
org.apache.jackrabbit.core.query.XMLTextFilter,
org.apache.jackrabbit.core.query.RTFTextFilter,
                        org.apache.jackrabbit.core.query.OpenOfficeTextFilter""/>
        <param name=""useCompoundFile"" value=""true""/>
        <param name=""minMergeDocs"" value=""100""/>
        <param name=""volatileIdleTime"" value=""3""/>
        <param name=""maxMergeDocs"" value=""100000""/>
        <param name=""mergeFactor"" value=""10""/>
        <param name=""bufferSize"" value=""10""/>
        <param name=""cacheSize"" value=""1000""/>
        <param name=""forceConsistencyCheck"" value=""false""/>
        <param name=""autoRepair"" value=""true""/>
                <param name=""respectDocumentOrder"" value=""false""/>
        <param name=""analyzer""
value=""org.apache.lucene.analysis.standard.StandardAnalyzer""/>
</SearchIndex>"
0,"SetValueFormatExceptiontest.testNode() relies on addMixin(), does not allow specification of node typeSetValueFormatExceptiontest.testNode() has two flaws:

- it doesn't cope with cases where the created node already is mix:referenceable.

- it doesn't allow specification of the node typer to be created.
"
1,Fails to remove a previously assigned mixinJackrabbit fails to remove a previously assigned mixin. Works fine on version 1.6.1
1,"SISM.checkAddedChildNodes() prevents merging of concurrent changesThis is a regression caused by JCR-2456. The check method reports false positives and prevents merges of concurrently removed child nodes.

The check is done before the local item states are connected to their shared states, which means getAddedChildNodes() will always return the complete list of local child nodes. In addition the merge attempt is also done after the check, which means it is impossible to handle concurrently removed child nodes."
0,"Make version recovery extensibleCurrently JCR-2551 (Recovery from a lost version history) is implemented within the protected method RepositoryImpl$WorkspaceInfo.doInitialize(). However, it is implemented in a way that can't be re-used in a subclass (RepositoryChecker is not a public class).

I will create a new method doVersionRecovery() that contains the code for JCR-2551."
0,Reduce memory usage of DocNumberCacheInstead of using the uuid String the a UUID instance should be used as the key in the cache.
1,Incomplete JCR-1664 fix in BindableRepositoryThe changes in JCR-1664 were not fully merged from trunk to the 1.4 branch due to a typo in the commit message of revision 683268. As a result a BindableRepository subclass becomes a bit cumbersome to implement. I hope to fix this in 1.4.7 so that subclasses written for both the partial and fully merged JCR-1664 fix should work.
1,"Using transactions still leads to memory leakThis is a result of the way that JCR-395 was fixed. If you look at the code, you'll see that txGlobal.remove(xid) is called as the last statement in both XASessionImpl.commit() and XASessionImpl.rollback(). However, in both methods an exception could be thrown either as a result of calling tx.commit() (or tx.prepare()) and tx.rollback(). 

As a result, the transaction will not be removed from txGlobals whenever the commit or the rollback has failed for any reason. My suggestion would be to move the txGlobal.remove(xid) into a finally block."
0,"PROPPATCH error marshalling when the resource can't be modified in generalLitmus test case ""notowner_modify"" (see <http://mail-archives.apache.org/mod_mbox/jackrabbit-dev/200604.mbox/%3c4432A7CF.30008@gmx.de%3e>) complains about a 423 (Locked) status code being sent back inside a 207 Multistatus:

  9. notowner_modify....... WARNING: PROPPATCH failed with 0 not 423
     ...................... pass (with 1 warning)

I think that warning is correct, as this is an error condition that doesn't need to be marshalled inside multistatus (1: it affects the resource at the Request URI and only that, 2: the operation failed completely). Let me also note that none of the other servers I tested with do return a 207 here (MS IIS, Apache/moddav, Xythos, SAP Netweaver KM),

RFC2518bis will hopefully clarify error marshalling for PROPPATCH. 

From the source code, the current server behaviour is fully intentional (by specifically catching the DavException and using it in MultiStatus). Removing that code seems to fix the issue.
"
0,"Wiki reference a missing CND resourceWiki page ""http://wiki.apache.org/jackrabbit/nt%3aexample"" references a missing resource:  ""http://jackrabbit.apache.org/doc/nodetype/cnd.html"""
0,Upgrade nekohtml dependencyThe latest CyberNeko HTML parser versions are ALv2-licensed and have better Maven dependency metadata.
0,"Session.importXML and Workspace.importXML throw wrong exceptionAccording to the JCR specification (section 7.3.6 and 7.3.7), if uuidBehaviour is set to IMPORT_UUID_COLLISION_REMOVE_EXISTING, a ConstraintViolationException should be thrown, when an incoming node has the same UUID as the node at parentAbsPath or one of its ancestors.

Currently, a RepositoryException is thrown instead.
"
1,"Restore to base version throws NullPointerExceptionThis only happens when the operations are enclosed in an XA transaction.
See test: org.apache.jackrabbit.core.version.RestoreTest"
0,.NET build scripts for JackrabbitHugo Burm has created build scripts that make it possible to run Jackrabbit under .NET. We should add the scripts and excample client code as a contrib project or integrate them with the Maven build system.
0,"Workspace.xml can't be loaded if it has a BOMI wondered if there was a specific reason why workspace.xml files are loaded using a FileReader instead of an InputStream in RepositoryConfig?

If the workspace.xml file has a BOM (which could happen if someone edited the file manually with some misbehaving editor), then it can't be loaded (""Content not allowed in prolog"") - here's a little patch that fixes this.

I left the output part untouched (i.e still using a Writer) - which makes it a little inconsistent - maybe someone with a better knowledge of the JR FileSystem api could fix this.

"
0,JSR 283: QOM and SQL2Adjusted title. This issue now covers all query changes and enhancements in JSR 283.
0,"Disable norms for untokenized fields to reduce memory consumptionFor repositories with many indexed fields, the norms cause memory problems both during indexing and querying (see LUCENE-448). Since the fields in question were never boosted they could as well be indexed without norms."
0,"Poor performance in range queries using datesI am evaluating migrating from 1.5 to 1.6. I created several test cases that prove the query performance of 1.6 is the same or better than 1.5. That is until I add a date property into my query. The repository has 400,000 nodes. Each node as several string based properties (@property, @property2, ...) and a date based property (@datestart). Every node has a relatively unique datestart and the total date range spans 6 years.

In my tests, my base query is:
//element(*,my:namespace)[@property='value'] order by @datestart descending

The time to run this query in 1.5 and 1.6 is:
1.5 = 1.5 seconds
1.6 = 1.5 seconds

If I add a date property:
//element(*,my:namespace)[@property='value' and @datestart<=xs:dateTime('2009-09-24T11:53:23.293-05:00')] order by @datestart descending

the results are:
1.5 = 1.5 seconds
1.6 = 3.5 seconds 

I have isolated the slow down to the implementation of SortedLuceneQueryHits. SortedLuceneQueryHits is not present in 1.5. I have run versions of the test where the query is run 20 times simultaneously and a different time where the query is run 20 times sequentially. In both tests I do see evidence that caching is taking place, but it provides only very minor performance gains. Also, running the 1.6 query multiple times does not decrease the query time dramatically.

http://www.nabble.com/Date-Property-Performance-in-1.6-td25704607.html"
0,"[Patch] Adding history, tab completion, status info command and masked password input for jcr-commandsI have created this patch which improves the usability of the interactive jcr command line client. It uses jline (http://jline.sourceforge.net) for the input, which gives history, tab completion and masked password input. Tab completion completes on available commands and on the jcr children of the current node for command arguments.

The login command now asks for the password if none is given, this uses the advanced password masking feature of jline to avoid the echoing of the password while typing (which is not possible using standard java System.in).

I also changed the Maven 2 pom to version 1.3-SNAPSHOT to compile with the current jackrabbit trunk and to include a manifest file with the correct starter class."
1,"a dead lock in DefaultISMLockingThe jackrabbit 2.2 's org.apache.jackrabbit.core.state.DefaultISMLocking has a defect which will cause a dead lock in concurrent use cases.
The use case is as follows:
1.	Thread A apply a read lock, now there is an active reader hold the read lock.

2.	Thread B apply a write lock, and then thread B will wait for thread A's reading end. You could see below code snippet from the Jackrabbit source. readerCount is the current active reader.
writersWaiting++;
while (writerId != null? !isSameThreadId(writerId, currentId) : readerCount > 0) {
                                wait();
}

3.	Thread A apply another read lock, then it will wait too, since there is a writer is waiting.  Then a dead lock happens.
while (writerId != null? (writerCount > 0 && !isSameThreadId(writerId, currentId)): writersWaiting > 0) {
                                wait();
}

Since the lock in DefaultISMLocking is global lock, so I think if a thread has already hold a reader lock, it could get the reader lock again. I create a fix with this idea.
"
0,"improved internal representation of DATE valuesDATE values are currently internally represented as java.util.Calendar objects.

Calendar objects have a huge memory footprint (approx 200bytes per instance) 
and are mutable.

i suggest to replace the internal DATE representation with a ISO8601 format string
(immutable and approx. 85-90% smaller footprint)."
0,"jcr2spi: avoid unnecessary roundtrips with NodeEntry.getPropertyEntrySince NodeInfo.getPropertyIds always returns the complete set of property names, there is no need for an extra round trip to the SPI upon NodeEntry.getPropertyEntry. The corresponding code could be simplified."
0,[PATCH] No need to call toString on a Stringcode calls toString on a String
1,"AccessControlManager#getEffectivePolicies(String) may expose AC content without proper permissionsThe implementation of AccessControlManager#getEffectivePolicies(String) in the DefaultAccessManager only checks if the session is allowed
to read AC content at the specified path. However the result may also include policies effective at absPath that should not be visible to the editing
session (read_AC permissions denied e.g. at an ancestor node) and could not be read by the editing session be means of #getPolicies().
"
1,"Under heavy load, database journal may contain empty update records.In a clustering scenario with a database journal, empty records may be produced. A passive node will then throw the following exception during its synchronization: 

28.02.2007 08:34:11 *ERROR* ClusterNode: Unexpected error while syncing of journal: null (ClusterNode.java, line 260)
java.lang.NullPointerException
	at java.io.FilterInputStream.close(Unknown Source)
	at org.apache.jackrabbit.core.journal.ReadRecord.close(ReadRecord.java:197)
	at org.apache.jackrabbit.core.journal.DatabaseRecordIterator.close(DatabaseRecordIterator.java:148)
	at org.apache.jackrabbit.core.journal.DatabaseRecordIterator.close(DatabaseRecordIterator.java:114)
	at org.apache.jackrabbit.core.journal.AbstractJournal.doSync(AbstractJournal.java:196)
	at org.apache.jackrabbit.core.journal.AbstractJournal.sync(AbstractJournal.java:165)
	at org.apache.jackrabbit.core.journal.ClusterNode.sync(ClusterNode.java:283)
	at org.apache.jackrabbit.core.journal.ClusterNode.run(ClusterNode.java:254)
	at java.lang.Thread.run(Unknown Source)"
0,"CLONE -Handling of multiple residual prop defs in EffectiveNodeTypeImplorg.apache.jackrabbit.jcr2spi.nodetype.EffectiveNodeTypeImpl currently rejects multiple residual property definitions, if they do not differ in getMultiple(). In fact, it should accept all combinations, so differing values for getOnParentVersionAction and other aspects should be accepted as well.

See JSR 170, 6.7.8:

""For purposes of the above, the notion of two definitions having the same name does not apply to two residual definitions. Two (or more) residual property or child node definitions with differing subattributes must be permitted to co-exist in the same effective node type. They are interpreted as disjunctive (ORed) options."""
0,Run TCK on Jackrabbit 1.0-rc3Run TCK on Jackrabbit 1.0-rc3
1,"move method of the MemoryFileSystem may accept invalid destination path resulting in invalid entries in FSIt seems there can be a problem with the move method of the MemoryFileSystem class: when looking at its code it looks that it can accept destinations, specifying folders that do not exist in the file system.
For example, if there is a ""somefolder/somefile"" file and I call the method passing, say ""somefolder/someotherfolder/somefile"" as the destination. The destination will be accepted by the code even if ""somefolder/someotherfolder"" is not an existing folder and the function execution will result in a file having the ""somefolder/someotherfolder/somefile"" path within a file system having no ""somefolder/someotherfolder"" folder - the code should probably check whether the destination path is really a valid one. Such  validation could be performed , for example, in the following way: take all path elements from the destination path except the last one and insure that the resulting path points to an existing folder, throw an exception otherwise.
Currently I have no JackRabbit build/test environment set up and could not verify practically whether the issue described can really take place, the supposition is made after looking at the move method implementation."
1,spi2dav : move/reorder not properly handled by observationall TCK tests including move or reorder fail in the setup jcr2spi - spi2dav(ex) - jcr-server.
0,"SPI: RepositoryService.obtain should allow to pass null workspaceName indicating the default workspaceimprovement suggested by tobi

the contract of

public SessionInfo obtain(Credentials credentials, String workspaceName)
public SessionInfo obtain(Credentials credentials, String workspaceName)

should be changed to allow for null workspaceName.

* @param workspaceName the name of the workspace the <code>SessionInfo</code>
* should be built for. <code>null</code> indicates that the info should be created for the
* default workspace.

consequently we could either deprecate 

RepositoryConfig.getDefaultWorkspaceName()

or allow it to return null as well or remove it altogether.




"
0,"Initialize the cause of a login exception in the repository===================================================================
--- RepositoryImpl.java8(revision 379871)
+++ RepositoryImpl.java (working copy)
@@ -1056,7 +1056,9 @@
             }
             authCtx.login();
         } catch (javax.security.auth.login.LoginException le) {
-            throw new LoginException(le.getMessage());
+           LoginException nle = new LoginException(le.getMessage());
+           nle.initCause(le);
+           throw nle;
         }

         // create session"
1,MatchAllQuery does not implement extractTerms()This is required for rep:excerpt() functionality.
1,"NPE if RepositoryService#getItemInfos throws ItemNotFoundExceptionWhen RepositoryService#getItemInfos throws an ItemNotFoundException, HierarchyEntryImpl#internalRemove in some cases throws an NPE. This is caused by a missing null check of the parent node."
0,Group.addMemeber() might add a REFERENCE instead of a WEAKREFERENCE...and this causes that the member can't be removed afterwards.
1,"Session scoped lock not always removed on Session.logout()Consider the following use case:

      Session s = repo.login(...);
      Node root = s.getRootNode();
      root.lock(true, true); // session-scoped, deep lock
      // modifiy items
      // root.isModified() still is true
      s.logout();

To my understanding, the session scoped locks should be removed (unlocked) and unsaved should be dropped on logout of a session. Unfortunately currently this is not the case, as the lock implementation gets notified by the SessionImpl on the logout situation and just calls Node.unlock() on the lock's node for session scoped locks. This method fails as there are unsaved changes. Hence after logout, the lock on the session is still there and will only be gone when the repository is stopped.
      "
0,"Improve performance of DescendantSelfAxisQueryInstead of calculating the full result of the sub query, the DescendantSelfAxisQuery should make use of the skipTo() method on the sub scorer."
0,"contrib/orm-persistence Node ordering not supportedDue to a limitation in the implementation, node ordering isn't supported (it is optional in the specification but Jackrabbit provides support for it) in the ORM persistence manager. This is due to the fact that in the database, although same-name sibling ordering is supported, no guarantee is given for the ordering of nodes in the child list.

Jackrabbit has some tests that use node ordering, such as the DerefQueryLevel1Test, where we look for the first property of type reference by using the following code :

        Property refProp = PropertyUtil.searchProp(session, testRootNode, PropertyType.REFERENCE);

The searchProp method traverses the tree and stops at the first property of the type specified. If node ordering is not correct, we return a property that is not the expected one (in the case of the DerefTest we were returning a multi-values reference property), which can could test failures."
1,"NPE in spi2dav when server does not send all headersThe ValueLoader may throw a NPE if the desired headers are not present in the response:

org.apache.jackrabbit.spi2davex.ValueLoader:

    public Map<String, String> loadHeaders(String uri, String[] headerNames) throws IOException, RepositoryException {
    ....
                for (String name : headerNames) {
--->                headers.put(name, method.getResponseHeader(name).getValue());
                }
    .....
    }

In my case, the server does not return the ETag response header, but the 'loadHeaders' is indirectly called by the QValueFactoryImpl:

                        this.preInitialize(new String[] {HEADER_ETAG, HEADER_LAST_MODIFIED});
"
1,"Offset not working correctly in user/group query when restricting to group membersFor user/group queries having a scope *and* a  limit clause offsetting does not work correctly.

    builder.setScope(""contributors"", false);
    builder.setLimit(100, 50);

In the above case, the result is often not offset at 100 but instead at some place >100.
"
0,"Add support for the Ingres RDBMSHi Folks,
    I've put together all the stuff I can figure out that is required to add support for using the Ingres RBMS. I'll upload a svn diff for what I've done. It is against the 1.5.2 version from the tags repository.

    I was looking around but couldn't see if there was a way of running a test suite using Ingres as the DBMS provider. Is that possible with the current environment?

Cheers"
0,"Upgrade to Tika 0.7Apache Tika 0.7 is now available. It's probably too late for the 2.1 release, but I'll upgrade the dependency in the trunk for Jackrabbit 2.2."
1,"Version.getReferences() does not work correctlysince the /jcr:system/jcr:versionStorage is shared among all workspaces, referes of versions and version histories need to be workspace sensitive. for example can a workspace W1 contain a versionable node N1. Its respective version history VH is visible in the jcr:versionStorage. calling VH.getReferences() should return the jcr:versionHistory property of that node N1. If accessing the repository using another workspace, W2, which does not have the node N2 (that corresponds to N1), calling VH.getReferences() should return an empty set. The same is true for version nodes referenced by jcr:baseVersion and jcr:predecessors properties.


see also spec chapter 8.2.2.1 (jcr:versionStorage):
The full set of version histories in the version storage, though stored in a single location in the repository, must be reflected in each workspace as a subtree below the node /jcr:system/jcr:versionStorage. 
[...]"
0,"TCK: ExportDocViewTest.exportValues fails on empty multivalued propertyIn ExportDocViewTest.exportValues, line 988 fails if the property being exported is empty (array of size 0, which is allowed by the spec) since there is no space to remove. This code should be skipped if the number of values is zero."
1,SearchIndex parameter cacheSize is ignoredThe cacheSize is always set to 1024 no matter what is specified in the configuration.
0,"JsonWriter: missing handling of new JCR 2.0 property typesthe json writer sends extra type information in case it cannot be determined unambiguously from the json string.
this needs to be adjusted for the new property types added for JCR 2.0"
0,"Clustering configuration documentation for syncDelay doesn't matchThere is a bit of mismatch in the current documentation that is available on configuring a Cluster node for a repository.  If you look at the DTD for repository.xml[1] it states that the syncDelay attribute of the Cluster element is in seconds.  However if you read the Javadoc for the ClusterConfig[2] object it states the syncDelay is in milliseconds.  I'm guessing that the value is actually in milliseconds but at the very least the two documents should be telling the same story.


[1] -http://jackrabbit.apache.org/dtd/repository-1.4.dtd
[2] - http://jackrabbit.apache.org/api/1.4/org/apache/jackrabbit/core/config/ClusterConfig.html"
0,Prefer PathFactory.createElement() over createPath().getNameElement()The latter creates an unnecessary Path instance.
0,"WebDAV: drop dependency on commons-collectionsthe webdav library brings in a dependency on commons-collections solely for one reference to LinkedMap. Since none of the additional features of this class are used, and as I understand it Jackrabbit requires JDK 1.4+, this can be replaced with LinkedHashMap.

jcr-commons still brings in the commons-collections dependency, but I believe it can be safely excluded by users that don't need to use the predicates (Which is true of the webdav client)"
1,"MsPowerPointTextExtractor does not extract from PPTs with € signThe MsPowerPointTextExtractor class has a problem when reading PPTs when an € sign is contained. All text following that sign is ignored. Perhaps the POI PowerPointExtractor should be used instead of parsing the data by hand. As a side effect, this would simply the code. Extracting could be done as follows:

	public Reader extractText(InputStream stream, String type, String encoding) throws IOException {
		try {
			PowerPointExtractor extractor = new PowerPointExtractor(stream);
			return new StringReader(extractor.getText(true,true));
		} catch (RuntimeException e) {
			logger.warn(""Failed to extract PowerPoint text content"", e);
			return new StringReader("""");
		} finally {
			try { stream.close(); } catch (IOException ignored) {}
		}
	}
"
1,"PathParser accepts illegal paths containing curly bracketso.a.jackrabbit.spi.commons.conversion.PathParser accepts the following path:

""/public/.{.}/private""

the normalized resulting Path object represents ""/private"" 

that's a potential security risk."
1,"UUID field not populated when saving a new nodeIn the following 'Article' class, there is are fields set to true for 'path' and 'uuid' jcr properties. 
The mixins for referencing (hence, support for UUID) are declared at the @Node level of the class. 

After saving the node with the ObjectContentManager, the uuid field is not populated as it could be expected

@Node(jcrMixinTypes=""mix:referenceable,mix:lockable,mix:versionable"")
public class Article {

        @Field(uuid=true)
        private String id = null;
       
        @Field(path=true)
        private String path = null;
       
        @Field
        private String content = null;

        ... constructor, getters and setters
} 

The full discussion is here : http://www.nabble.com/OCM-issues-with-path-and-id-fields-%28annotations%29-tt15460625.html#a15460625 "
1,"Two consecutive score() calls return different scores for Boolean Queries.Two consecutive calls to score() return different scores (no next() or skipTo() calls in between). 
Background in LUCENE-912 .
"
0,add IndexCommit.getTimestamp methodConvenience method for getDirectory().fileModified(getSegmentsFileName()).
1,"Corrupted segment file not detected and wipes index contentsLucene will happily wipe an existing index if presented with a latest generation segments_n file of all zeros. File format documentation says segments_N files should start with a format of -9 but SegmentInfos.read accepts >=0 as valid for backward compatibility reasons.

"
1,"BooleanQuery can not find all matches in special conditionquery: (name:tang*)
doc=5137 score=1.0  doc:Document<stored,indexed<name:tangfulin>>
doc=11377 score=1.0  doc:Document<stored,indexed<name:tangfulin>>
query: name:tang* name:notexistnames
doc=5137 score=0.048133932  doc:Document<stored,indexed<name:tangfulin>>

It is two queries on the same index, one is just a prefix query in a
boolean query, and the other is a prefix query plus a term query in a
boolean query, all with Occur.SHOULD .

what I wonder is why the later query can not find the doc=11377 doc ?

the problem can be repreduced by the code in the attachment ."
0,Demo HTMLParser compares StringBuffer to an empty String with .equals
0,"synchronize grammar/token types across StandardTokenizer, UAX29EmailURLTokenizer, ICUTokenizer, add CJK types.I'd like to do LUCENE-2906 (better cjk support for these tokenizers) for a future target such as 3.2

But, in 3.1 I would like to do a little cleanup first, and synchronize all these token types, etc.
"
0,Improve DocValues mergingSome DocValues impl. still load all values from merged segments into memory during merge. For efficiency we should merge them on the fly without buffering in memory
1,"NPE in PhraseQuery.toString(String f)the section

public String toString(String f) {
    StringBuffer buffer = new StringBuffer();
    if (!field.equals(f)) {
      buffer.append(field);
      buffer.append("":"");
    }
    <snip>


should be

public String toString(String f) {
    StringBuffer buffer = new StringBuffer();
    if (field != null && !field.equals(f)) {
      buffer.append(field);
      buffer.append("":"");
    }
    <snip>


The issue arises if a phrase query is created, no terms are added, then the phrase query is added to a boolean query. Calling toString on the boolean query will result in a NPE insdie of the PhraseQuery.
"
0,"FST Builder methods need fixing,documentation,or improved type safetyIts confusing the way an FST Builder has 4 add() methods, and you get assertion errors (what happens if assertions are disabled?) if you use the wrong one:

For reference we have 3 FST input types:
* BYTE1 (byte)
* BYTE2 (char)
* BYTE4 (int)

For the builder add() method signatures we have:
* add(BytesRef)
* add(char[], int offset, int len)
* add(CharSequence)
* add(IntsRef)

But certain methods only work with certain FST input types, and these mappings are not the ones you think. 

For example, you would think that if you have a char-based FST you should use add(char[]) or add(CharSequence), but this is not the case: those add methods actually only work with int-based FST (they use codePointAt() to extract codepoints). Instead, you have to use add(IntsRef) for the char-based one.

The worst is if you use the wrong one, you get an assertion error, but i'm not sure what happens if assertions are disabled.

Maybe the ultimate solution is to parameterize FST's generics on input too (FST<input,output>) and just require BytesRef/CharsRef/IntsRef as the parameter? Then you could just have add(), and this might clean up FSTEnum too (it would no longer need that InputOutput class but maybe could use Map.Entry<input,output> or something?
 
I think the documentation is improving but i still notice add(BytesRef) has no javadoc at all, and it only works with BYTE1, so I think we still have some work to do even if we want to just pursue a documentation fix.
"
0,"[PATCH] fix various small issues with the ""getting started"" demo pages
This patch contains numerous small fixes for the ""getting started""
pages on the Lucene Java web site.  Here are the rough fixes:

  * To results.jsp:

    - changed StopAnalyzer -> StandardAnalyzer

    - changed references of ""url"" to ""path"" (field ""url"" is never set
      and was therefore always null)

    - remove prefix of ""../webapps"" from path so clicking through works

  * Fixed typos, grammar and other cosmetic things.

  * Modernized some things that have changed with time (names of JAR
    files, which languages have analyzers, etc.)

  * Added outbound links to Javadocs, Wiki, Lucene static web site,
    external sites, when appropriate.

  * Removed exact version of Tomcat for the demo web app (I think all
    recent versions of Tomcat will work as described)

  * Other small changes...

Net/net I think this is an improved version of what's available on the
site today."
1,"Access to VirtualNodeTypeStateManager.virtualProvider should be guardedThe virtualProvider field of the VirtualNodeTypeStateManager class is dynamically created by the getter method. Two methods of the class access that field directly though risking NullPointerException.

Access should be guarded against the field being not assigned yet."
1,"Deadlock on concurrent save/checkin operations possibleSave and checkin operations are trying to acquire 2 locks in different order, what leads to deadlock.

->save
1.SharedItemStateManager.acquireWriteLock
2.AbstractVersionManager.acquireWriteLock	->	locked

->checkin
1.AbstractVersionManager.acquireWriteLock
2.SharedItemStateManager.acquireReadLock	->	locked

""Thread-4"" prio=6 tid=0x0312d840 nid=0x824 in Object.wait() [0x03cef000..0x03cefa68]
	at java.lang.Object.wait(Native Method)
	- waiting on <0x23210968> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock)
	at java.lang.Object.wait(Unknown Source)
	at EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock.acquire(Unknown Source)
	- locked <0x23210968> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock)
	at org.apache.jackrabbit.core.version.AbstractVersionManager.acquireWriteLock(AbstractVersionManager.java:124)
	at org.apache.jackrabbit.core.version.VersionManagerImpl.setNodeReferences(VersionManagerImpl.java:413)
	at org.apache.jackrabbit.core.version.VersionItemStateProvider.setNodeReferences(VersionItemStateProvider.java:125)
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:699)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:810)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:326)
	at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:313)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:302)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:295)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1204)
	- locked <0x2332eaa0> (a org.apache.jackrabbit.core.XASessionImpl)
	at JrTestDeadlock.run(JrTestDeadlock.java:87)

""Thread-3"" prio=6 tid=0x0312db18 nid=0xa04 in Object.wait() [0x03caf000..0x03cafae8]
	at java.lang.Object.wait(Native Method)
	- waiting on <0x232d1360> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$ReaderLock)
	at java.lang.Object.wait(Unknown Source)
	at EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$ReaderLock.acquire(Unknown Source)
	- locked <0x232d1360> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$ReaderLock)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.acquireReadLock(SharedItemStateManager.java:1361)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.hasItemState(SharedItemStateManager.java:270)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.hasItemState(LocalItemStateManager.java:180)
	at org.apache.jackrabbit.core.state.XAItemStateManager.hasItemState(XAItemStateManager.java:252)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.hasItemState(SessionItemStateManager.java:188)
	at org.apache.jackrabbit.core.ItemManager.itemExists(ItemManager.java:256)
	at org.apache.jackrabbit.core.NodeImpl.hasProperty(NodeImpl.java:1509)
	at org.apache.jackrabbit.core.version.InternalFrozenNodeImpl.checkin(InternalFrozenNodeImpl.java:276)
	at org.apache.jackrabbit.core.version.InternalFrozenNodeImpl.checkin(InternalFrozenNodeImpl.java:248)
	at org.apache.jackrabbit.core.version.InternalVersionHistoryImpl.checkin(InternalVersionHistoryImpl.java:440)
	at org.apache.jackrabbit.core.version.AbstractVersionManager.checkin(AbstractVersionManager.java:397)
	at org.apache.jackrabbit.core.version.VersionManagerImpl$2.run(VersionManagerImpl.java:289)
	at org.apache.jackrabbit.core.version.VersionManagerImpl$DynamicESCFactory.doSourced(VersionManagerImpl.java:611)
	- locked <0x2320c5d8> (a org.apache.jackrabbit.core.version.VersionManagerImpl$DynamicESCFactory)
	at org.apache.jackrabbit.core.version.VersionManagerImpl.checkin(VersionManagerImpl.java:285)
	at org.apache.jackrabbit.core.version.XAVersionManager.checkin(XAVersionManager.java:161)
	at org.apache.jackrabbit.core.NodeImpl.checkin(NodeImpl.java:2944)
	at JrTestDeadlock.run(JrTestDeadlock.java:103)
"
0,"Only search the index for the ""jcr:system"" tree if neededRight now every time a query is executed the index of the current workspace as well as the index of the ""jcr:system"" tree is searched. A lot of queries are not searching in the ""jcr:system"" tree at all therefore it should be checked if the query contains paths that include ""jcr:system"". There are two relevant nodes in the query tree to find that out:

- what's the first location step and does it include the jcr:system tree? I think that's an easy one.
- does the query contain a jcr:deref node? If there is an intermediate result of a query may dereference into the jcr:system tree. 

This should notably speed up query execution if you are working extensively with versioning."
0,Remove SessionImpl dependency from QueryObjectModelFactoryImplThe QueryObjectModelFactoryImpl should be independent of the jackrabbit-core.
0,"Update to Lucene 2.4.1Lucene 2.4 contains a couple of performance improvements.

See: http://lucene.apache.org/java/2_4_0/changes/Changes.html"
0,"Move common node type functionality to jackrabbit-spi-commonsnow, that jackrabbit-core has a dependency to jackrabbit-spi-commons it would be possible to have the common functionality shared by core and jcr2spi in the spi-commons project.

the node type package offers quite some potential for that.
"
0,spi2davex: some value factory tests from the SPI test suite are failing
0,"jackrabbit-jcr-client should put all test data under ./targetCurrently the jackrabbit-jcr-client component drops a ""repository"" directory and the ""repository.xml"" and ""derby.log"" files into the project root when running the test suite. These files should go inside ""target""."
0,"CooperativeFileLock improvementsThe CooperativeFileLock doesn't have any test cases. Also, there are a few ways to improve it:

- Avoid the exception ""lock file modified in the future""
- Stop the thread immediately after releasing the lock
- Detect locked repositories a bit earlier

"
0,Show referencing nodes in debug log when trying to delete a node with referencesThis can be very useful when trying to analyze errors from non-interactive applications in a production environment.
0,"Wrong exeption returned from Repository.login(Credentials, String)According to specification, calling Repository.login(Credentials, String) with a non-existent wokspaceName should return NoSuchWorkspaceException.

In fact it returns RepositoryException."
1,NullPointerException when using ancestor axis in indexing configurationHappens when there is an index-rule that matches the root node type and has a condition that uses the ancestor axis.
1,"Node#addNode fails with AccessDeniedException if session lacks read-permission to an ancestorConsider a Session that has following permissions:
/home  -> no permission
/home/myself -> read|remove|set_property|add_node

if this session tries to add a Node to /home/myself.
An AccessDeniedException is thrown indicateing that it can not read /home.

The Exception is caused by the Node's check, if it is checked-out.
This check asumes that the session has read-access to all its ancestors.
Which breaks in this case:

see NodeImpl internalIsCheckedOut()   (ln 3875)
"
0,"SPI: Describe equality requirements of ItemIdsMichael Duerig asked for clarification regarding the equality of  ItemIds.

While discussing this we came to the following conclusion:

Two ItemIds should be considered equal if both the unique part and the path part are equal AND if they denote the same type of id (see #denotesNode).

If nobody objects i would adjust the javadoc of ItemId accordingly."
0,"AbstractLoginModule logs a warning on anonymous loginsCurrently a ""No credentials available -> try default (anonymous) authentication"" warning is logged by AbstractLoginModule whenever an anonymous login is made. Since this is a normal situation, the log message should be at debug or at most info level."
0,RowIteratorImpl should make use of QueryResultRow.getValues()Values are currently retrieved from regular nodes. Using QueryResultRow.getValues() uses less calls to the server.
0,Create HTML excerpt providerJackrabbit should have a built in HTML excerpt because web applications usually prefer HTML rather than XML.
1,Node.removeMixin fails if the mixin defines a protected child nodeNode.removeMixin fails if the mixin to removed defines a protected child node.... the problem is caused by line 253 of RemoveMixinOperation.
0,BTreeManager needs more flexible mechanism for ignoring (internal) propertiesThe current BTreeManager implementation has some hard coded logic to ignore jcr:primaryType properties. There should be a mechanism to parametrize BTreeManager with a set of properties to ignore.
0,Provide change log consolidatorspi-commons should provide base implementations for consolidating change logs (Batch). 
0,"Decouple packages in core.queryThe packages o.a.j.c.query has cyclic dependencies to sub packages lucene, sql, xpath.

Decoupling the packages will allow to better extend the query implementation with additional query languages."
0,"Script for checking releasesScript to automate checking of releases, similar to [0] 
(heavily inspired by the sling script)

The command line call will look like:
    ./check-release.sh jukka 1.6.5 4fa1e032f9b641fbc5c9ff8d6ba76fdb58b539ba

This command will also be embedded in the emails announcements that come with each release, so anybody can easily run it.

[0] http://svn.apache.org/repos/asf/sling/trunk/check_staged_release.sh
"
1,"Invalid node type definitions with test_nodetypes.xmlan attempt to register the node types defined with 
core/src/test/resources/org/apache/jackrabbit/core/nodetype/xml/test_nodetypes.xml
will fail:

- invalid reference constraint
- autocreated prop-def without default values
- invalid required type with autocreated prop-def (undefined is not allowed)
- invalid required primary type (non existing)
- invalid default primary type (non existing)

"
0,"ACL with glob restrictions does not work on '/'i tried to define a ACL on '/' that would allow 'read' on '/' itself, but not for the nodes underneath. i tried ""*"", ""/*"", ""./*"" but none of them seem to do the desired effect.

eg:
everyone,allow,jcr:read, '/'
everyone,deny,jcr:read, '/', glob=""/*""

the same works for a non-root node.

"
1,"[jcr-rmi] xpath queries don't work when the underlying QueryResult doesn't return the rows/nodes sizeo.a.j.rmi.server.ServerQueryResult assumes the underlying NodeIterator always return the number of elements, but it migth return -1 in some cases [1], AFAIK depending on the jcr impl. When -1 is returned jcr-rmi fails to return the QueryResult. 

e.g. it fails with the following xpath query //*

[1] http://www.day.com/maven/jsr170/javadocs/jcr-1.0/javax/jcr/RangeIterator.html#getSize()"
0,"TextFilter implementations in a new project under contrib AFAIK the TextFilters sent by Ján Hala?a were not added yet. If that's the case, I think we can create a project under contrib with all the implementations in order to avoid adding new dependencies to the core distribution."
1,"JCR-RMI UnmarshalException when calling getProperty()I've been trying to get the JCR-RMI adaptors going to talk to my noddy test repository. It seems I can successfully login and traverse the nodes (getNode etc..) but whenever I try to get something from a nt:resource (actually I've subclassed nt:resource to add our own properties) I get the following exception:

org.apache.jackrabbit.rmi.client.RemoteRepositoryException: error unmarshalling return; nested exception is: 
	java.io.WriteAbortedException: writing aborted; java.io.NotSerializableException: org.apache.jackrabbit.value.BinaryValue: error unmarshalling return; nested exception is: 
	java.io.WriteAbortedException: writing aborted; java.io.NotSerializableException: org.apache.jackrabbit.value.BinaryValue
	at org.apache.jackrabbit.rmi.client.ClientProperty.getValue(ClientProperty.java:139)
	at org.apache.jackrabbit.rmi.client.ClientProperty.getString(ClientProperty.java:131)
	at ClientTest.main(ClientTest.java:20)
Caused by: java.rmi.UnmarshalException: error unmarshalling return; nested exception is: 
	java.io.WriteAbortedException: writing aborted; java.io.NotSerializableException: org.apache.jackrabbit.value.BinaryValue
	at sun.rmi.server.UnicastRef.invoke(UnicastRef.java:164)
	at org.apache.jackrabbit.rmi.server.ServerProperty_Stub.getValue(Unknown Source)
	at org.apache.jackrabbit.rmi.client.ClientProperty.getValue(ClientProperty.java:137)
	... 2 more

My svn is up-to-date as of this morning 2005/7/5 although the maven builds don't inspire much confidence as it falls over with Jelly exceptions at various points and there's a couple of dozen unit tests that fail. Don't know if this is the norm, the maven reports on the jackrabbit main site reports suggest not? But calling the same code with an in-process repository works fine.

"
0,"Reduce temporary memory usage of hierarchy cache initializationInitializing the hierarchy cache temporarily uses memory, which is linear to the size of the index segment. This process should be split into multiple phases to limit the memory usage to a fixed amount.

The temporary memory usage for an index segment current is about 170 bytes per node."
0,"Global data store for binariesThere are three main problems with the way Jackrabbit currently handles large binary values:

1) Persisting a large binary value blocks access to the persistence layer for extended amounts of time (see JCR-314)
2) At least two copies of binary streams are made when saving them through the JCR API: one in the transient space, and one when persisting the value
3) Versioining and copy operations on nodes or subtrees that contain large binary values can quickly end up consuming excessive amounts of storage space.

To solve these issues (and to get other nice benefits), I propose that we implement a global ""data store"" concept in the repository. A data store is an append-only set of binary values that uses short identifiers to identify and access the stored binary values. The data store would trivially fit the requirements of transient space and transaction handling due to the append-only nature. An explicit mark-and-sweep garbage collection process could be added to avoid concerns about storing garbage values.

See the recent NGP value record discussion, especially [1], for more background on this idea.

[1] http://mail-archives.apache.org/mod_mbox/jackrabbit-dev/200705.mbox/%3c510143ac0705120919k37d48dc1jc7474b23c9f02cbd@mail.gmail.com%3e
"
1,"Memory leak in UUIDDocIdThe hierarchy cache of the search index uses DocId's to reference the parent of a Node. One implementation of a DocId is the UUIDDocId which may hold a reference to the IndexReader that was used to calculate the id. UUIDDocId's are invalidated on a lazy basis. That is the referenced IndexReader may still be present even though the IndexReader instance has long been closed.

-> UUIDDocId should only hold a weak reference to the IndexReader."
0,"Remove deprecated classes in jcr-commonsThere are many classes in jackrabbit-jcr-commons that we've replaced with better alternatives in spi-commons.

I'd like to clean things up by removing the deprecated versions from jcr-commons now that we're upgrading to Jackrabbit 2.0"
1,"Security of token base authenticationToken based authentication as implemented with JCR-2851 seems to exhibit a security issue: the token returned by the server consists of the identifier of a (newly created) node in the repository. An attacker who is able to guess (or acquire by other means i.e. via log files) that identifier will be granted access to the repository. Worse yet, JCR-2857 introduces sequential node ids. Guessing is a piece of cake in such a setup.

I think we should decouple authentication secrets from node ids. A simple solution would be to store the secret in a token attribute and delegate generation of the secret to a dedicated handler. Such a handler can then use a secure random generator, private/public key encryption or whatever other method that is deemed appropriate to generate the authentication secret. 

Initial discussion see: http://markmail.org/thread/aspetgvmj2qud25a"
0,"Persistence: support property databaseTypeIn persistence managers and cluster journal the term 'schema' is used to mean 'database type'. 

Using the term 'schema' for that is actually quite confusing (in my view):

The definition of schema http://en.wikipedia.org/wiki/Database_schema is ""the schema defines the tables, the fields in each table, and the relationships between fields and tables.""

Additionally in most databases a schema is is a name space within a database: http://java.sun.com/j2se/1.4.2/docs/api/java/sql/DatabaseMetaData.html#getSchemas() .

I suggest to support the property 'databaseType' in addition to 'schema' for the persistence managers and the cluster journal.

"
0,"JSR 283: new methods on NodeTypeJSR 283 adds the methods

  NodeType.getSubtypes

and

  NodeType.getDeclaredSubtypes"
0,"TCK: PropertyDefTest and NodeDefTest do not respect the value of the testroot configuration propertyIn PropertyDefTest and NodeDefTest, the test setup doesn't respect the value of the testroot configuration property.

Proposal: use the testroot configuration property.

Patch for PropertyDefTest:
--- nodetype/PropertyDefTest.java       (revision 422074)
+++ nodetype/PropertyDefTest.java       (working copy)
@@ -94,7 +94,7 @@
  
         session = helper.getReadOnlySession();
         manager = session.getWorkspace().getNodeTypeManager();
-        rootNode = session.getRootNode();
+        rootNode = testRootNode;
     }

Patch for NodeDefTest:
--- nodetype/NodeDefTest.java   (revision 422074)
+++ nodetype/NodeDefTest.java   (working copy)
@@ -68,7 +68,7 @@
  
         session = helper.getReadOnlySession();
         manager = session.getWorkspace().getNodeTypeManager();
-        rootNode = session.getRootNode();
+        rootNode = testRootNode;
     }
"
0,"PropertyReadMethodsTest.testIsSame leaks sessionThe test case obtains a second session but doesn't logout().
"
1,"Node removal fails with AccessDeniedExceptionI have a hierarchy of nodes which are all access controllable. The following hierarchy illustrates the setup for my problem.
root  -  read permissions to everyone
  | - subFolder  -  all permissions to user A
        | - subsubFolder  -  all permissions to user A

The user A has all rights from the node ""subFolder"" downwards.

I tried to remove the node ""subsubFolder"" with the user A. Clearly A has enough permissions to remove the node. But as soon as I call Session.save() an AccessDeniedException is thrown.

I did a lot of debugging and found a possible cause for this fault. It led me to the function ACLProvider.AclPermissions.buildResult(). All line references are based on the source code in the subversion repository found here: http://svn.apache.org/viewvc/jackrabbit/tags/1.6.0/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/security/authorization/acl/ACLProvider.java?view=markup.
On line 458 Jackrabbit collects all access control entries of the node, that I want to remove, and all its parents and puts it in the variable ""entries"". In my example this variable contains three entries:
1. all permissions to user A
2. all permissions to user A
3. read permissions to everyone
On lines 460 - 466 it collects all access control entries of the node, that I want to remove, and puts it in ""localACEs"". This variable contains one entry: all permissions to user A.
If I want to be able to remove ""subsubFolder"", user A needs the permission from the parent node. The permissions of the parent nodes of ""subsubFolder"" are: all permissions to user A and read permissions to everyone. But that's where the access check fails. In line 488 Jackrabbit checks if a permission from ""entries"" is local or not by looking it up in ""localACEs"". If it is in there, the permission is local, else not. Unfortunately it recognizes the permission of the node ""subFolder"" as local. Thus the permissions of the parent nodes of ""subsubFolder"" are: read permissions to everyone. So I cannot remove the node.
The source of the error is the equals check of the access control entries. The permissions of node ""subFolder"" are considered equal to the one of ""subsubFolder"". If I explicitly assign the permission ""remove node permission to user A"" to the node ""subFolder"", it works fine, because it is recognized as parent permission."
1,"Unsynchronized access to MultiIndex#indexesThis may result in a concurrent modification exception:

java.util.ConcurrentModificationException
	at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
	at java.util.AbstractList$Itr.next(AbstractList.java:343)
	at org.apache.jackrabbit.core.query.lucene.MultiIndex.getIndexReader(MultiIndex.java:744)
	at org.apache.jackrabbit.core.query.lucene.MultiIndex.getIndexReader(MultiIndex.java:712)
	at org.apache.jackrabbit.core.query.lucene.SearchIndex.getIndexReader(SearchIndex.java:1024)
	at org.apache.jackrabbit.core.query.lucene.SearchIndex.executeQuery(SearchIndex.java:820)
	at org.apache.jackrabbit.core.query.lucene.SingleColumnQueryResult.executeQuery(SingleColumnQueryResult.java:78)
	at org.apache.jackrabbit.core.query.lucene.QueryResultImpl.getResults(QueryResultImpl.java:293)
	at org.apache.jackrabbit.core.query.lucene.SingleColumnQueryResult.<init>(SingleColumnQueryResult.java:70)
	at org.apache.jackrabbit.core.query.lucene.QueryImpl.execute(QueryImpl.java:132)
	at org.apache.jackrabbit.core.query.QueryImpl.execute(QueryImpl.java:127)

This is usually very unlikely but with the recent changes to our tests, index flushes are very frequent and may cause the above exception."
0,"RepositoryService.checkin should return information about newly created versionWe have a mismatch between Node.checkin(), which returns a new Version object, and RepositoryService.checkin(), which returns void.

Client of SPI, such as JCR2SPI, thus will have to make an additional request for the base version property, with the obvious drawbacks (another call, and a potential cause for a race condition).

Proposal: change the return code to NodeId.
 "
1,"NamespaceAdder.addNamespace throws ClassCastExceptionHere's the method

public void addNamespaces(NamespaceMapping nsm)
            throws NamespaceException, 
UnsupportedRepositoryOperationException, RepositoryException {
        Map m = nsm.getPrefixToURIMapping();
        for (Iterator i = m.values().iterator(); i.hasNext();) {
            Map.Entry e = (Map.Entry) i.next();
            String prefix = (String) e.getKey();
            String uri = (String) e.getKey();
            registry.registerNamespace(prefix, uri);
        }
    }


should be the entrySet iterator, and uri should come from the value, patch fixes this.
Occurs in both spi-commons and jcr-commons (duplicate code)"
0,"Typo in repository.xmlThe extractorPoolSize parameter name has a trailing white space, which makes jackrabbit ignore it."
0,"[PATCH] Trivial Javadoc fix for RepositoryConfigYes, this is really trivial, but i keep coming to this class, trying to figure out it works, and the javadoc parms are scrambled.

patch 'fixes' this."
1,"Node.getLock() on a lock non-lockable node throws javax.jcr.UnsupportedRepositoryOperationExceptionConsider Node ""/test"" with mixin type mix:lockable and Node ""/test/child"" without mixin type mix:lockable.

       Node root = Session.getRootNode();
       Node test = root.getNode(""test"");
       Node child = test.getNode(""child"");

       // create lock on /test
       test.lock();

       // succeeds
       Lock lock = test.getLock();

       // sets flag true
       boolean locked = child.isLocked();

       // throws UnsupportedRepositoryOperationException
       lock = child.getLock();

According to the spec, the getLock() method must return the applicable lock if the node is locked regardless of whether the node has mixin type ""mix:lockable"" or not."
1,"Several test cases fail when declaring nt:base / nt:hierarchy node types as 'abstract' JSR283 introduces a new node type attribute 'abstract' and defines nt:base and nt:hierarchyNode as such.
when changing those nodetypes, the following test cases fail:

Failed tests: 
  testDefinedAndLegalType(org.apache.jackrabbit.test.api.nodetype.CanAddChildNodeCallWithNodeTypeTest)
  testResidualAndLegalType(org.apache.jackrabbit.test.api.nodetype.CanAddChildNodeCallWithNodeTypeTest)

Tests in error: 
  testAddNodeConstraintViolationExceptionUndefinedNodeType(org.apache.jackrabbit.test.api.NodeTest)
  testRemoveMandatoryNode(org.apache.jackrabbit.test.api.NodeTest)
  testCloneNodesConstraintViolationException(org.apache.jackrabbit.test.api.WorkspaceCloneTest)
  testCopyNodesConstraintViolationException(org.apache.jackrabbit.test.api.WorkspaceCopyBetweenWorkspacesTest)
  testCopyNodesConstraintViolationException(org.apache.jackrabbit.test.api.WorkspaceCopyTest)
  testMoveNodesConstraintViolationException(org.apache.jackrabbit.test.api.WorkspaceMoveTest)
  testNodeTypeConstraintViolationWorkspaceWithHandler(org.apache.jackrabbit.test.api.SerializationTest)
  testNodeTypeConstraintViolationSessionWithHandler(org.apache.jackrabbit.test.api.SerializationTest)
  testNodeTypeConstraintViolationWorkspace(org.apache.jackrabbit.test.api.SerializationTest)
  testNodeTypeConstraintViolationSession(org.apache.jackrabbit.test.api.SerializationTest)
  testJoinFilterPrimaryType(org.apache.jackrabbit.test.api.query.SQLJoinTest)
  testElementTest(org.apache.jackrabbit.test.api.query.ElementTest)
  testElementTestAnyNode(org.apache.jackrabbit.test.api.query.ElementTest)
  testElementTestAnyNodeNtBase(org.apache.jackrabbit.test.api.query.ElementTest)
  testElementTestAnyNodeSomeNT(org.apache.jackrabbit.test.api.query.ElementTest)
  testElementTestNameTest(org.apache.jackrabbit.test.api.query.ElementTest)
  testElementTestNameTestNtBase(org.apache.jackrabbit.test.api.query.ElementTest)
  testElementTestNameTestSomeNT(org.apache.jackrabbit.test.api.query.ElementTest)
  testElementTestNameTestSomeNTWithSNS(org.apache.jackrabbit.test.api.query.ElementTest)
  testNodeType(org.apache.jackrabbit.test.api.observation.AddEventListenerTest)


here's a stacktrace of a failing test:

javax.jcr.nodetype.ConstraintViolationException: nt:hierarchyNode: is an abstract node type.
        at org.apache.jackrabbit.core.NodeImpl.internalAddChildNode(NodeImpl.java:768)
        at org.apache.jackrabbit.core.NodeImpl.internalAddNode(NodeImpl.java:737)
        at org.apache.jackrabbit.core.NodeImpl.internalAddNode(NodeImpl.java:691)
        at org.apache.jackrabbit.core.NodeImpl.addNode(NodeImpl.java:2147)
        at org.apache.jackrabbit.test.api.SessionTest.testMoveItemExistsException(SessionTest.java:69)


the failing tests are actually a backwards compatibility issue. nt:base and nt:hierarchyNode were
non-abstract in JCR 1.0, i.e. 

     node.addNode(""foo"", ""nt:base"");

was perfectly legal.

however, as of JCR 2.0, above statement fails. all above mentioned tests fail because they 
create nodes of type nt:base or nt:hierarchyNode."
0,"BeanLazyLoader is not SerializableClass org.apache.jackrabbit.ocm.manager.objectconverter.impl.BeanLazyLoader is not serializable.
In ocm module we can mark some property to be lazy loaded. For example @Bean(..., proxy=true)
In such scenario instead of object we will have here proxy BeanLazyLoader which is not serializable.

It is problematic while using another technologies. 
For example Spring WebFlow requires objects (model) stored in scope to be Serializable.
So when we use proxied model with Spring WebFlow we received exception ""org.springframework.webflow.execution.repository.snapshot.SnapshotCreationException: Could not serialize flow execution; make sure all objects stored in flow or flash scope are serializable.... Caused by: java.io.NotSerializableException: org.apache.jackrabbit.ocm.manager.objectconverter.impl.BeanLazyLoader
...""

Please make BeanLazyLoader Serializable.
"
1,"Error when registering node types on virgin repositoryWhen a node type is registered on a repository that has never been started before an error is written to the log:

26.10.2006 10:36:02 *ERROR* [main] VirtualNodeTypeStateManager: Unable to index new nodetype: javax.jcr.ItemNotFoundException (VirtualNodeTypeStateManager.java, line 159)

Steps to reproduce:

> maven test:clean
> cp applications/test/repository/namespaces/ns_re.properties.install applications/test/repository/namespaces/ns_re.properties
> cp applications/test/repository/nodetypes/custom_nodetypes.xml.install applications/test/repository/nodetypes/custom_nodetypes.xml

Run test case o.a.j.init.NodeTypeData

It seems that the workspace initialization creates node states for jcr:system and a child node jcr:nodeTypes. The latter however is overlayed by a virtual node state provided by the VirtualNodeTypeStateProvider.

In case of an initial startup, the jcr:nodeTypes node is cached on creation and is not overlayed by the virtual twin from the VirtualNodeTypeStateProvider.

IMO the jcr:nodeTypes node state shouldn't have been created in the first place because it is overlayed anyway. The workspace initialization routine should only create a child node entry in the jcr:system node state but no actual child node state for that entry.

Opinions?"
0,"Allow multiple producers to feed/consume journalSome clustered application based on jackrabbit might want to append custom records to the central journal in order to synchronize all nodes. Therefore, journal should provide support for multiple consumers/producers."
0,"InternalValue should implement QValue.discard() for BINARY typescurrently jackrabbit-core always extracts the BLOBFileValue in order to free resources. Since InternalValue now implements QValue this could be achieved on the InternalValue directly.
However, currently the base implementation is inherited instead of dealing with the BLOBFileValues.

"
0,"Test cases for all FileSystem implementationsCurrently we only have test cases for MemoryFileSystem, but those tests could easily be generalized to cover also the other FileSystem implementations."
1,"AbstractLoginModule#logout() : credentials will not be clared as Subject.getPublicCredentials(Class) isn't backed by the subject internal setAbstractLoginModule#logout()  clears the credentials list retrieved by calling Subject.getPublicCredentials(Class).
this doesn't have the desired effect as the returned set isn't backed by the subjects internal credentials set. (see API documentation of javax.security.auth.Subject)"
1,"NPE in ItemManager when calling Session.save() with nothing to saveI'm getting an NPE on the id.denoteNodes() call below, in ItemManager:

    private ItemData retrieveItem(ItemId id) {
        synchronized (itemCache) {
            ItemData data = itemCache.get(id);
            if (data == null && id.denotesNode()) {
...

because the id is null after taking the second branch of this if in SessionSaveOperation.perform:

        if (context.getSessionImpl().hasPermission(""/"", Session.ACTION_READ)) {
            id = context.getRootNodeId();
        } else {
            id = context.getItemStateManager().getIdOfRootTransientNodeState();
        }

context.toString() says:

session-author-3623:
ItemManager (org.apache.jackrabbit.core.ItemManager@1e911ccc)
Items in cache:

SessionItemStateManager (org.apache.jackrabbit.core.state.SessionItemStateManager@15472b43)
[transient]
{}[attic]
{}

which I assume means there's nothing to save.

The correct behavior is probably to do nothing in perform to avoid the NPE."
0,"changed behavior of javax.jcr.Value get* methodsstream handling semantics of javax.jcr.Value has been simplified in JCR 2.0:

Section '5.10.5.1 Deprecated Binary Behavior'

[...]
Unlike in JCR 1.0, calling a get method other than getStream before 
calling getStream on the same Value object will never cause an 
IllegalStateException. 


see https://jsr-283.dev.java.net/issues/show_bug.cgi?id=658"
1,"String literal must not interpret entity referencesThe ampersand character in a string literal is interpreted as a start character for an entity reference. This is because Jackrabbit uses an XQuery parser where a string literal is slightly more constraint than in XPath.

Example:

//element(*, nt:base)[jcr:contains(., 'max&moritz')]

throws a parse exception. Instead the parser should simply recognize the ampersand as regular character."
1,Broken handling of outer join results over davexThe davex join support added in JCR-3089 only works correctly when the join returns at least one row and none of the returned rows contain null values for any of the selectors. This should be reasonably straightforward to fix.
0,"Usage of ""qualified name"" in JavaDoc and CommentsWithin jackrabbit the term ""qualified name"" is used, when the internal representation of a JCR name is referred according to the usage of ""fully-qualified name"" within the JSR 170 specification.

Based on input by julian (Issue #449) the phrasing has been adjusted in JSR 283 to match the terminology used in XMLNS.

As of JSR 283

- Qualified Name -> refers to a JCR Name in the form prefix:localname
- Expanded Name -> is used for what was formerly called ""fully-qualified name"".

In order to avoid confusion i would suggest to fix the javadoc throughout the project and eventually add explanations in cases where method or class names are misleading."
0,"Add hit miss statistics and logging to cachesThe current caches (ConcurrentCache) doesn't maintain hit and miss statistics. This makes it very hard to know if you need to increase the caches in a deployment. This functionality does exist in the 1.5 and 1.6 branches, but is missing from the 2.x branches. The patch adds these statistics and adds logging on info level. The frequency of the logging is by default configured to maximal once a minute but can be configured with the system property ""org.apache.jackrabbit.cacheLogStatsInterval"" (in ms). 

The log lines look like:

07.10.2011 09:00:39 INFO  [org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.logCacheStats():737] name=defaultBundleCache[ConcurrentCache@54fb02fd] num=21074 mem=34504k max=65536k hits=93352 miss=21074 puts=21135
07.10.2011 09:00:40 INFO  [org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.logCacheStats():737] name=versionBundleCache[ConcurrentCache@47b1de1a] num=10637 mem=250k max=8192k hits=36352 miss=10637 puts=10637

This patch will also make possible to later on expose these statistics over JMX when the initial JMX work has been settled.  "
0,"[PATCH] refactor access to SAXParser and log which parser class is usedThis parser collects code that was duplicated in the SessionImpl and WorkspaceImpl class to initialize the SAXParser.

Also, the actual SAXParser class being used is logged (once only) to make it easier to debug problems like JCR-984."
0,spi2dav : list of known issues in the pom.xml
1,"Executing query throws UnsupportedRepositoryOperationException(LEVEL_2_SUPPORTED) for a level 1 only implementation Executing a query  throws UnsupportedRepositoryOperationException(LEVEL_2_SUPPORTED) it the spi implementation is not level 2. This is because org.apache.jackrabbit.jcr2spi.query.execute() calls session.getValueFactory() which - by contract - throws if level 2 is not supported. A quick fix would be to call getJcrValueFactory() (available from the ManagerProvider interface implemented by SessionImpl) instead of getValueFactory(). However, I think a better fix might be to pass the ManagerProvider to the QueryImpl constructor instead of the session, all the managers and providers separately."
0,"InternalValue refactoringNow that we have use the value factory from spi-commons, the InternalValue code should be simplified."
1,"Failing Node.unlock() might leave inconsistent transient stateSimilar to issue JCR-538 but for Node.unlock():

If updating the lock related properties (jcr:lockIsDeep or jcr:lockOwner) fails e.g. due to missing permission, there might be inconsistent transient modifications pending."
1,"IllegalStateException on session#save()The following code throws an IllegalStateException:

Node node = ...
Session session = node.getSession();
node.setProperty( ""tags"", ""test1"");
node.setProperty( ""tags"", ""test2"");
node.remove();
session.save();


"
0,Promote ChildNodeEntry and ChildNodeEntries to top level classes.The current NodeState class is quite heavy weight (source code wise) and the inner class ChildNodeEntry is used in a lot of places outside of NodeState. I think it is useful to have them promoted to top level classes.
1,Bundle consistency check does not workThe consistencyCheck feature in the database bundle persistence manager does not work because the correct table prefix is not used when loading all bundles.
1,jcr-server: NPE in SearchResourceImpl if PathValue path is nullline 368 in SearchResourceImpl doesn't deal with null path thus causing NPE in the valuefactory. 
0,"sysview import cannot handle auto-created nodeswhen importing a serialized system-view via the Session.importXML() method, an ItemExistsException is thrown, when a nodes has an auto-create child nodes.

when the parent node is created during the import, the repository automatically creates the auto-create child nodes. when then import handler tries to create the respective child node, the exception is thrown.


"
0,"Improvements to user managementContainer issue for various improvements needed for the user management implementation in jr-core.

Known improvements are:

- extensibility
- current structuring of users/groups in the JCR content doesn't allow for easy finding user/group by ID.
- groupID should be unescaped before being returned by getID"
0,AbstractReadableRepositoryService should pass credentials to createSessionInfoCurrently AbstractReadableRepositoryService.obtain() calls checkCredentials() followed by createSessionInfo() where it only passes the userId rather than the credentials. In many cases an implementation will need the credentials to create the SessionInfo. 
0,"Improved logging for session operationsI'd like to add the following logging features to SessionOperations:

* Use MDC [1] to make it possible to filter and redirect logs based on which session is being used
* Add simple debug-level timing information for executed SessionOperations to help pinpoint performance issues
* Classify SessionOperations as read or write operations, and log warnings (instead of the current debug messages) for concurrent writes on a session

[1] http://logback.qos.ch/manual/mdc.html"
1,"jackrabbit-jcr-rmi: Supplied javax.transaction.xa.Xid is assumed serializable, but is not on some environments.Websphere provides a non-serializable javax.transaction.xa.Xid implementation, causing ClientXAResource to fail with NotSerializableException when passing Xid over RMI.
I have worked around this by converting the supplied Xid to a local serializable Xid implementation that takes the supplied Xid parameters, and implements hashCode() and equals() correctly:

    private static class SerializableXID implements javax.transaction.xa.Xid, Serializable {
    	/**
		 * Serial version ID
		 */
		private static final long serialVersionUID = -1390620315181450507L;
		
		private final byte[] branchQualifier;
    	private final byte[] globalTransactionId;
    	private final int formatId;
    	private final int hashCode;
   	
    	public SerializableXID(Xid xid) {
    		branchQualifier = xid.getBranchQualifier();
    		globalTransactionId = xid.getGlobalTransactionId();
    		formatId = xid.getFormatId();
    		hashCode = xid.hashCode();
    	}

		public byte[] getBranchQualifier() {
			return branchQualifier;
		}

		public int getFormatId() {
			return formatId;
		}

		public byte[] getGlobalTransactionId() {
			return globalTransactionId;
		}

        public final int hashCode() {
        	return hashCode;
        }
        
        public final boolean equals(Object obj) {
	        if(obj == this) {
	            return true;
	        }
	        
        	if(!(obj instanceof Xid)) {
        		return false;        		
        	}
        	
        	Xid xidimpl = (Xid)obj;
        	if(formatId != xidimpl.getFormatId()) {
        		return false;
        	}
	        else {
	            return Arrays.equals(branchQualifier, xidimpl.getBranchQualifier())
	            	&& Arrays.equals(globalTransactionId, xidimpl.getGlobalTransactionId());
	        }
    	}
    }
"
1,"InputStream not being explicitly closedAfter deploying a j2ee artifact that uses jackrabbit and org.apache.jackrabbit.core.persistence.pool.MySqlPersistenceManager, Glassfish starts complaining there are input streams without being explicitly closed.
The specific inputStream mey be found at org.apache.jackrabbit.core.persistence.pool.BundleDbPersistenceManager.createCheckSchemaOperation(BundleDbPersistenceManager.java:584).

I've checked the code and in BundleDbPersistenceManager:530 the run method is invoked to the object CheckSchemaOperation.
In run method CheckSchemaOperation:78, the finally block that calls IOUtils.closeQuietly(ddl); to close the stream is inside the condition if (!conHelper.tableExists(table)) (CheckSchemaOperation:79).
So, if this condition is false, the inputStream will not be explicitly closed.

In my opinion, there are two fix alternatives:
The most robust should be:
1 - insert a finalize() method:
    @Override
    protected void finalize() throws Throwable {
	    if (ddl!=null){
            IOUtils.closeQuietly(ddl);
	    }
		super.finalize();
	}

Another alternative:
2 - Put the condition if (!conHelper.tableExists(table)) inside try-finally block.


StackTrace:
[#|2011-05-05T11:43:28.087-0300|WARNING|glassfish3.1|javax.enterprise.system.core.classloading.com.sun.enterprise.loader|_ThreadID=1233;_ThreadName=Thread-1;|Input stream has been finalized or forced closed without being explicitly closed; stream instantiation reported in following stack trace
java.lang.Throwable
        at com.sun.enterprise.loader.ASURLClassLoader$SentinelInputStream.<init>(ASURLClassLoader.java:1230)
        at com.sun.enterprise.loader.ASURLClassLoader$InternalJarURLConnection.getInputStream(ASURLClassLoader.java:1338)
        at java.net.URL.openStream(URL.java:1010)
        at java.lang.ClassLoader.getResourceAsStream(ClassLoader.java:1195)
        at com.sun.enterprise.loader.ASURLClassLoader.getResourceAsStream(ASURLClassLoader.java:872)
        at java.lang.Class.getResourceAsStream(Class.java:2030)
        at org.apache.jackrabbit.core.persistence.pool.BundleDbPersistenceManager.createCheckSchemaOperation(BundleDbPersistenceManager.java:584)
        at org.apache.jackrabbit.core.persistence.pool.BundleDbPersistenceManager.init(BundleDbPersistenceManager.java:530)
        at org.apache.jackrabbit.core.persistence.pool.MySqlPersistenceManager.init(MySqlPersistenceManager.java:51)
        at org.apache.jackrabbit.core.RepositoryImpl.createPersistenceManager(RepositoryImpl.java:1353)"
0,"Improve logging in NodeTypeRegistry.persistCustomNodeTypeDefsWhen the closing of out in the finally block of persistCustomNodeTypeDefs throws an IOException this is ignored. At least some logging should take place, because an IOException here might still mean that the custom nodetype definitions were not stored correctly. This is the case, for instance, when a DatabaseFileSystem is used: the call to out.close triggers the SQL statement execution which causes an IOException if it fails. "
0,"Create a jackrabbit-site subprojectThe Jackrabbit web site sources should be moved to a new ""jackrabbit-site"" subproject so that the top-level Jackrabbit POM would only be used for the multimodule build setup and generic project metadata. This way the web site could be built more easily without invoking all the subprojects, and Eclipse would also be happier since the site project wouldn't ""contain"" the other Jackrabbit modules."
1,"WebDAV Library: VersionControlledResource constant lists wrong methodVersionControlledResource .methods_checkedIn constant lists UNCHECKOUT which is obviously
wrong.

RFC3253 lists the UNCHECKOUT method as mandatory method for checked-out vc-resource
for the 'checkout-in-place'  feature."
0,jcr2spi: use Soft refs for hierarchystefan suggested to use soft refs for the hierarchy.
1,"WorkspaceItemStateFactory#createItemStates throws ClassCastExceptionWhen the first item in the ItemInfo iterator returned by the RepositoryService is a PropertyInfo instead a NodeInfo, a ClassCastException is thrown. This should rather be a ItemNotFoundException.
"
0,JSR 283: Event user dataImplement JSR 283 Event user data.
1,"Prevent data inconsistencies due to incorrect or missed merges in the ItemStateManagersThere are two places where transient state changes are merged with persisted state: (i) in the SessionISM.stateModified call back method (typically called by a saving thread after it's changes have been saved) and (ii) in the SharedISM.Update.begin method. Sometimes the merge strategy fails in the sense that corrupt data is written to database. 

How to reproduce:
The attached test program contains steps to reproduce the issue. The testInconsistency1 method concurrently adds a node D to a node A and moves child node B of A to another place. This sometimes results in a situation in which node A still has a child reference to node B whereas B has another parent. In this situation, Jackrabbit cannot be started anymore if the search index is missing: 

Jun 8, 2009 2:16:23 PM org.apache.jackrabbit.core.query.OnWorkspaceInconsistency$1 handleMissingChildNode
SEVERE: Node /A (a7a29e8c-8d13-4fbd-b0ca-4f93f9c0ef42) has missing child 'B' (80aa13c5-1db6-4f62-b576-5e7f626d90c1)
Jun 8, 2009 2:16:23 PM org.apache.jackrabbit.core.RepositoryImpl initStartupWorkspaces
SEVERE: Failed to initialize workspace 'wm9'

The testInconsistency2 method concurrently adds a reference property to a node B (the threads do exactly the same). This sometimes results in a situation in which the referenced node can never be removed anymore because there is a ""ghost"" reference to it which cannot be removed. (It gives a ReferentialIntegrityException).

Jun 8, 2009 2:19:51 PM org.apache.jackrabbit.core.state.MLRUItemStateCache cache
WARNING: overwriting cached entry bc28ff87-216d-4ccd-bd73-03e7499ab54e/{}ref to B
Exception in thread ""main"" javax.jcr.ReferentialIntegrityException: 9f025634-d3e1-448e-904c-1c285f6b1bf6: the node cannot be removed because it is still being referenced.

It seems the first problem (the parent-child relation) is caused by an incorrect merge in the NodeStateMerger class. The second problem might also be caused by an incorrect or missed merge, but I am not sure whether that's the real problem.

"
0,"tm-extractors.jar blocks usage of newer poi versionsThe used tm-extractors-0.4.jar (http://repo1.maven.org/maven2/org/textmining/tm-extractors/) includes various classes from poi as well as poi-2.5.1 is referenced as dependency in the pom.xml.
It's seems not possible to use a newer version of poi (e.g. poi-3.0.1-FINAL) together with tm-extractors (and so jackrabbit).

A solution could be using functions of newer poi versions (I'm not sure if they are only in scratchpad yet) or use a fixed version of tm-extractors.jar which doesn't inlude the poi classes."
1,"Versioning does not make use of txfor example:

- begin transaction
- create node
- add mix:versionable
- save --> will create version
- checkin
- cancel transaction

should not leave any traces in versioning"
1,"Invalid Journal Record appearing when read during sync operationERROR: Error while processing revision 3161: Unknown entry type: 
(a) (2007-05-13 19:57:02,258 main_org.apache.jackrabbit.core.cluster.ClusterNode)
ERROR: Unable to start clustered node, forcing shutdown... (2007-05-13 19:57:02,259 main_org.apache.jackrabbit.core.RepositoryImpl)
org.apache.jackrabbit.core.cluster.ClusterException: Unable to read record with revision: 3161
        at org.apache.jackrabbit.core.cluster.ClusterNode.sync(ClusterNode.java:285)
        at org.apache.jackrabbit.core.cluster.ClusterNode.start(ClusterNode.java:229)
        at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:308)
        at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:584)
        at org.sakaiproject.jcr.jackrabbit.RepositoryBuilder.init(RepositoryBuilder.java:213)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)


This is a 2 node cluster, with persistance managers in the DB and journal on the shared filesystem.

Start the first node in the cluster up from a completely clean and empty repo.

Let it add some note types, and create a since workspace (called sakai) and then connect via webdav (using OSX Finder) which creates some Journal Records (due to the finder putting some .xxxx files in)

Dont add any files.

Then start the second node in the cluster up,

It runs through the first 15 or so journal entries and then hits a one where the entry is unknown (stack trace above)

Some analysis to follow
"
1,"XPath parser ignores parent axisThe query handler in Jackrabbit does not support the parent axis and should throw an invalid query exception in that case.

Example query:

//foo/.."
0,"JSR 283: VersionManager and new versioning methodsJSR 283 introduces a new interface ""VersionManager"" that exposes all versioning functionality that was formerly present on Node and Workspace. 

In addition the following new methods:

- checkpoint(String absPath) Version 
- merge(String absPath, String srcWorkspace, boolean bestEffort, boolean isShallow) NodeIterator 
- merge(Node activityNode) NodeIterator 

See Issue #JCR-1592 for Activity and Configuration feature."
1,"Sleep in possibly endless loop in ObservationDispatcherThe rate-limitation code we added in JCR-2402 to prevent the observation queue from growing too large was a good idea, but the current implementation is a bit troublesome since it blocks the thread while it still holds the journal lock, the SISM reader lock, and the SessionState lock. This can cause a deadlock under heavy workloads if any of the observation listeners attempts to reuse the session (not recommended/supported, but can still happen) or write to the repository (quite likely).

To solve this problem we should move the rate-limiter code to outside the scope of any internal locks."
0,"Provide overridables for lock checkingCurrently, checking whether a session is allowed to write to some locked node or whether it is allowed to unlock it is quite spread throughout the code. This should be collected to allow a custom lock manager overriding just a few methods to alter the default behavior."
1,"Overlapping index aggregates not updatedWhen there are overlapping index aggregates in an indexing configuration and an item is updated that belongs to multiple aggregates, then only the first aggregate root is re-indexed."
0,Deprecated classes point to wrong replacements (due to various package renamings)classes that have been replaced by equivalents in jackrabbit-spi-commons point to wrong replacements in the @deprecated javadoc tag.
1,"Workspace.move() and Session.move()  allow moves to an invalid pathWhen calling Workspace.move() with an invalid destination path, which is a child of the source path, e.g. move(""/path"", ""/path/path2""), results in the source path nodes being removed. When calling Session.move() the failure is a StackOverflowError when save is called.

"
0,"Authentication Mechanism Based on Login Tokenimplementing an authentication mechanism that apart from simple credentials allows for credentials being built on a login token
could rely on the fact that jackrabbit stores the user data in the repository: adding additional information (generated tokens, expiration time, 
additional security parameters) could be stored in additional subnodes to the user and used for matching during login as alternative
ways to authenticate against the system."
0,"please have spi2dav create a test-jarfor my davex-on-sling integration tests, I need the test classes from spi2dav."
0,"Prevent excessive Path.Element instancesEven when a CachingHierarchyManager is used jackrabbit creates a lot of Path.Element instances. The internally used PathMap (spi-commons) creates new Path.Element instances whenever a path is constructed, even when the path is constructed from cached PathMap.Elements.

Running a test with 10k nodes results in 250k Path.Element instances being created and held in memory (mostly for events)."
1,"Repository is not unlocked if version manager init failed and assertions are enabledThe following test case will work as expected, except when assertions are enabled (java -ea ...):

Connection conn = DriverManager.getConnection(
        ""jdbc:derby:repository/version/db;create=true"");
Statement stat = conn.createStatement();
stat.execute(""create table version_bundle(id int)"");
TransientRepository rep = new TransientRepository();
try {
    rep.login(new SimpleCredentials("""", new char[0]));
} catch (Exception e) {
    // ignore
}
rep.shutdown();
stat.execute(""drop table version_bundle"");
new TransientRepository().login(new SimpleCredentials("""", new char[0]));

The reason is the assertion in RepositoryContext.getInternalVersionManager. Because of this assertion, the repository lock is not released during the repository shutdown.
"
0,"Make behaviour configurable when re-indexing detects workspace inconsistencyCurrently the query handler throws an exception and the repository will not start.

There are multiple options how to deal with this situation:

1) fail and repository startup will fail
2) ignore and proceed
3) fix the inconsistency and proceed

1) and 2) can be implemented quite easily. with the recent enhancement JCR-1428, 3) can also be implemented with reasonable effort.

In any case an error message should be written to the log."
0,"Path parser does not allow trailing slashesThe current implementation of the Path class violates the spec in that the trailing slashes are not allowed while the path syntax clearly allows trailing slashes.

(Of course the Path class violates the spec in another way, too, in that it allows for the root path ""/"" to be valid while the path syntax in the spec does not allow that, which is clearly an error of the spec of course)."
1,"PersistentNode.store() ignores status when storingWhile looking for a performance bottle neck I came across this issue: When a PersistentNodeState is asked to store itself in the PersistentNodeState.store() method, it calls its PersistenceManager to store it.

This is not a problem in itself. The problem is, that if the PersistentNodeState has not been modified, the object does not need to be stored. Doiing it anyway just consumes cycles ! In the case of a deep, unmodified hierarchy, this just results in nodes being written to persistence for nothing.

Comes to it, that this method sends an event, which in the case of an unmodified node state will be notifyStateUpdated(), which is complete nonsense, because nothing has actually been updated.

I suggest to modify the PersistentNodeState.store() method to only do work if modified.

Note: I encountered this issue, whily tracking down performance problems when creating versionable nodes, which turned out to be located somewhere within the PersistentVersionManager.createVersionHistory(NodeImpl) method. And there, predominantly the store() methods consume time."
0,"Deprecate RepositoryService.getPropertyInfo methodI would like to deprecate and ultimately remove the RepositoryService.getPropertyInfo method and extend RepositoryService.getItemInfos to take over that functionality. getItemInfos would thus change to

    /**
     * Method used to 'batch-read' from the persistent storage. It returns the
     * <code>ItemInfo</code> for the given <code>ItemId</code> as the first
     * element in the <code>Iterator</code>. In addition the iterator may contain
     * arbitrary <code>ItemInfo</code>s.
     */
    public Iterator<? extends ItemInfo> getItemInfos(SessionInfo sessionInfo, ItemId itemId) throws ItemNotFoundException, RepositoryException;

"
0,"Deprecate XASessionThe XASession interface in jackrabbit-api extends Session with a single getXAResource() method. The idea is that a transactional client (or a transaction manager) will test whether a session implements XASession and can then get the XAResource instance that can be used to bind the session to a distributed transaction. The essential code is:

    if (session instanceof XASession) {
        return ((XASession) session).getXAResource();
    }

This works fine except for the extra dependency to jackrabbit-api that it introduces in code that otherwise would only need the JCR API. Since the link between a transaction-enabled session and the related XAResource instance is always one-to-one, we could avoid this dependency by making the session directly implement XAResource, leading to code like this:

    if (session instanceof XAResource) {
        return (XAResource) session;
    }

This is essentially what jackrabbit-jcr-rmi did in 2.0.0 to avoid the jackrabbit-api dependency while maintaining XA transaction support, and I'd like to extend this solution also to other parts of Jackrabbit."
1,"support stores where binary properties are mandatory (such as in nt:resource)SetValueBinaryTest tries to remove binary properties by setting them to null. However, some stores only support binary properties in the case of jcr:content/jcr:data, in which case the property can not be removed directly.

Suggestion: check for mandatory/protected, and throw NotExecutableException in that case.
"
1,"JCARepositoryManager.createNonTransientRepository throws NPE with no JCAManagedConnectionFactory.CONFIGFILE_KEYJCARepositoryManager.createNonTransientRepository fails if

String configFile = parameters.get(JCAManagedConnectionFactory.CONFIGFILE_KEY);

is null, because

config = RepositoryConfig.create(configFile, homeDir);

will always throw an NPE, perhaps the call should just be

config = RepositoryConfig.create(homeDir);

"
0,"Add SessionImpl#isAdminOrSystemwe have several places in jackrabbit-core where we need to find out if the executing session is either a SystemSession or corresponds to the admin user.
currently the same code (analysing the sessions subject) is copied throughout jackrabbit-core. i would like to replace that by a helper method on SessionImpl."
0,"Better error message for non-trivial nodetype changesCurrently Jackrabbit only throws a RepositoryException with the message ""Not yet implemented"". This says almost nothing to newcomers and is very time consuming to debug even if you know what is means. It would be better to use the information already availble through NodeTypeDefDiff to provide a more descriptive error message."
0,"UserManagement: Add Membership Cachedue to weakreference nature of the group members, retrieving the groups a given authorizable is member is expensive as the corresponding
API call (Node#getWeakReferences) executes a query [fallback if search is disabled: traversing the complete group tree, which isn't for free either].

i would therefore suggest to add a cache (authorizable nodeId -> group nodeids) that is gets cleared upon any modification to group membership or
group removal and doesn't need any extra observation. this cache may potentially obsolete the principal-based cache in DefaultPrincipalProvider... "
1,"ClassCastException in GroupImpl.isCyclicMembershipGiven three groups and one user with the following membership relation

group1 > group2 > group3
group2 > user

where x > y means x contains y.

group3.addMember(group1) throws a ClassCastException.

The reason is that the search type (i.e. UserManager.SEARCH_TYPE_GROUP) is not honored correctly when constructing the transitive membership relation. "
0,"Ensure queries are not blocked during large updatesThe index currently guarantees that long running queries do not block updates. In addition a query *may* run during an update, but there is not guarantee because it depends on the availability of an index reader being available when the update starts. The index reader is invalidated at the end of the update, which will force the creation of a new index reader when the next query is executed.

Consider the following scenario:

1) update index -> transaction id T1
2) potential index reader is invalidated
3) execute query -> creates index reader R1, which includes changes up to T1
4) update index -> transaction id T2
5) index reader R1 is invalidated
6) update index (large transaction) -> transaction id T3
7) while previous update is running execute query -> thread is blocked because no reader is available


The improvement should detect the large transaction and prepare an index reader for potential queries during the update. That is, 6) should be split into:

6a) detect large transaction and prepare index reader R2, which includes changes up to T2
6b) update index -> transaction id T3

While the update is running a query will use index reader R2."
1,Like expression does not match line terminator in StringIf a string property contains a line terminator a like pattern with a % or _ does not match the line terminator. This is because the implementation uses the java.util.regexp.Pattern class without the DOTALL option.
1,"select query fails on 'like xxx322' statementIn class org.apache.jackrabbit.core.search.lucene.WildcardTermEnum, there's a condition around line 77 like

...
if (Character.isLetter(likePattern.charAt(i))) {
...

which fails on query like
SELECT * FROM my:type LOCATION /news// WHERE my:text LIKE ""asd2""

it should be changed to 

if (Character.isLetter(likePattern.charAt(i)) || Character.isDigit(likePattern.charAt(i))) {

"
1,"Lock.refresh(): throws if lock is aliveThe spec says:

Lock.refresh():
If this lock's time-to-live is governed by a timer, this method resets that timer so that the lock does not timeout and expire. If this lock's time-to-live is not governed by a timer, then this method has no effect.A LockException is thrown if this Session does not hold the correct lock token for this lock.A RepositoryException is thrown if another error occurs.


The jackrabbit impl does:

/**
     * {@inheritDoc}
     */
    public void refresh() throws LockException, RepositoryException {
        if (isLive()) {
            throw new LockException(""Lock still alive."");
        }
        [...]
    }


Isn't this a leftover from a very old version of the spec?
There was ones a misunderstanding about the refresh (bringing locks back to live) that has been discussed (mail by david to g. clemm, tobi, peeter and myself, 25.4.2005). as far is i know everybody agreed that this does not make sense and the spec has been adjusted accordingly.

The usage of the refresh is to prevent the lock from being timeouted. That was the original meaning of the refresh, when i suggested it for the JCR locking. If the lock is still alive and there is no timeout to reset, then the method should simply not do anything.

am i missing something? 

regards
angela

"
0,"Rename package core.search to core.queryThe package org.apache.jackrabbit.core.search should be renamed to org.apache.jackrabbit.core.query for consistency with the JCR api. All other packages in Jackrabbit already use the package names as suggested by the JCR api (javax.jcr.<subpackage>).

If there are other implementations than the Jackrabbit one of the org.apache.jackrabbit.core.search.QueryHandler interface they need to be adapted to use the new package name!"
0,"DbDataStore: tablePrefix not accomodated during init test for existing DATASTORE tablewe are providing a test db deployment with prepopulated data, including jackrabbit DataStore. I tried specifying the tablePrefix when creating the initial repository, and tables are created properly.

However, when a clean installation is run with a fresh database that has an existing DataStore (stored at JACKRABBIT_DS_DATASTORE table), the startup fails, because during init, the meta data only checks for tables name matching the tableSQL property:
ResultSet rs = meta.getTables(null, null, tableSQL, null);

but the tableSQL property is never modified based on the tablePrefix property (other uses of tableSQL modify queries based on the prefix).

I think the init method should modify the tested table name based on the tablePrefix.

Note: I assume different JDBC drivers may handle this differently (we are using SQL Server 2007 and jTDS driver), since the DatabaseMetaData is API is unclear on the parameter to getTables being a ""tableNamePattern"" - should wildcards work? Or should a specific table be specified? 

My DataStore config is below:

    <DataStore class=""org.apache.jackrabbit.core.data.db.DbDataStore"">
      <param name=""className"" value=""org.apache.jackrabbit.core.data.db.DbDataStore""/>
      <param name=""url"" value=""jdbc:jtds:SQLServer://localhost:1433/nga_admin;prepareSQL=2;responseBuffering=adaptive""/>
      <param name=""user"" value=""sa""/>
      <param name=""password"" value=""""/>
      <param name=""databaseType"" value=""sqlserver""/>
      <param name=""driver"" value=""net.sourceforge.jtds.jdbc.Driver""/>
      <!-- a bug in jackrabbit makes tablePrefix not work -->
      <param name=""tablePrefix"" value=""JACKRABBIT_DS_"">
      <param name=""minRecordLength"" value=""1""/>
      <param name=""maxConnections"" value=""2""/>
      <param name=""copyWhenReading"" value=""true""/>         
    </DataStore>
"
0,"spi2davex: unspecific BadRequest error instead of error code matching the RepositoryExceptionthe JsonDiffHandler#NodeHandler in the server part of the jcr remoting may only throw IOException
if an error occurs. this results in unspecific BadRequest responses even if the problem source was
something very specific such as e.g. a locked node.

after having a first glance at this i think that making DiffException a subclass of IOException would
allow to generate much more specific responses codes that even include the original exception
details.

i will attach a patch as i didn't had time to carefully test it.  [the conformance tests passed]."
0,"Broke lock tests in jcr2spi after the JCR 2.0 upgradeMy changes during the JCR 2.0 upgrade have broken the following tests in spi2jcr:

    org.apache.jackrabbit.test.api.lock.LockTest
    org.apache.jackrabbit.jcr2spi.lock.SessionScopedLockTest

I'll mark them as known issues for now to get the Hudson build back up again, but will continue looking at what I did to cause this breakage."
0,ValueFactory is not extensibleThe Jackrabbit ValueFactory implementation should have a generic base class in jackrabbit-jcr-commons. This base class could be reused in SPI.
0,"FileDataStore: only open a stream when really necessaryCurrently, PropertyImpl.getValue() opens a FileInputStream if the FileDataStore is used.
If the application doesn't use the value, this stream is never closed.

PropertyImpl.getValue():
  return internalGetValue().toJCRValue(session);
InternalValue.toJCRValue(..):
  case PropertyType.BINARY:
    return new BinaryValue(((BLOBFileValue) val).getStream());
BLOBInDataStore.getStream():
  return getDataRecord().getStream();
FileDataRecord.getStream():
  return new FileInputStream(file);

One solution is to return a 'lazy' file input stream that only opens the file when reading from the stream (and closing the file when the last byte was read). Maybe there is already a class (in Apache Commons maybe?) that can do that.
"
1,"VersionManager lock not released in some circumstancesIn some circumstances it is possible that lock is not released in VersionManager.checkin method.

There is following block of code in checkin nethod :

        aquireReadLock();
        try {
              .....
        } catch (IllegalStateException e) {
            releaseReadLock();
            throw new RepositoryException(""Unable to start edit operation."");
        }

Lock release should be in finally block to make sure that lock is released when unexpected exception is thrown.
In our environment we are getting NPE in mentioned block of code, it results in persisten lock. 
No versioning operation is possible and our application server is running ot of threads (all threads are locked).
"
0,"Slow performance due to JCR-2138 (Prevent persistence of faulty back-references)In revision 782898, the following code was introduced:

updateReferences() {
    for (Iterator i = local.addedStates(); i.hasNext();) {
        ...
        if (hasItemState(state.getId())) {
            ...
        }
    }
}

This will try to fetch inexistent nodes from the persistence manager.
Depending on the persistence manager implementation, this is very slow.
I hope there is a way to avoid this call, or if not, speed it up.
"
0,Remove calls to System.out in testsTwo tests write to System.out: QueryTest and HierarchyNodeTest
0,"Remove recently added getJCRPath()/getQPath() from NamespaceResolverissue JCR-473 added 2 new methods to NamespaceResolver: 

    public Path getQPath(String jcrPath) throws MalformedPathException;
    public String getJCRPath(Path qPath) throws NoPrefixDeclaredException;

which do not belong here, since the NamespaceResolver has nothing to do with paths. suggest to remove them.

further is the naming of the QName related methods a bit vague. suggest to rename them to:

   QName parseName(String jcrName)
   String formatName(QName qName)

(although they do not belong here either, but helps to leverage evt. caching namespace resolvers).

"
0,JSR 283: Activities
0,"UserManagement: make membership a weakreferencesthe 2.0 user management code still contains a TODO asking for changing group-membership from reference to weakreference. 

now that weakreferences are implemented in jackrabbit-core i'd like to make that change both in the node type definitions and the code.

i'm opening this issue in order to give us the chance to keep track of node type change that may cause troubles...."
1,"Restoring a node which has OPV=Version children fails if they are not versionablewhen a node has a OPV=Version childnode which is not versionable itself, restoring of the node fails,
when it tries to read the versionhistory of that childnode."
1,"Deadlock in SharedItemStateManager on session.move and node.saveI have multiple threads in a test case for performing action on different workflow instances stored inside a Jackrabbit repository.
Most of the time the test case hangs because of the following deadlock:

""ActionTrigger38"" daemon prio=10 tid=0x9060ec00 nid=0x2f7e in Object.wait() [0x8f505000..0x8f505f20]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0xb3ef0208> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$ReaderLock)
        at java.lang.Object.wait(Object.java:485)
        at EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$ReaderLock.acquire(Unknown Source)
        - locked <0xb3ef0208> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$ReaderLock)
        at org.apache.jackrabbit.core.state.DefaultISMLocking$ReadLockImpl.<init>(DefaultISMLocking.java:84)
        at org.apache.jackrabbit.core.state.DefaultISMLocking$ReadLockImpl.<init>(DefaultISMLocking.java:78)
        at org.apache.jackrabbit.core.state.DefaultISMLocking.acquireReadLock(DefaultISMLocking.java:44)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.acquireReadLock(SharedItemStateManager.java:1409)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.hasItemState(SharedItemStateManager.java:286)
        at org.apache.jackrabbit.core.state.XAItemStateManager.hasItemState(XAItemStateManager.java:295)
        at org.apache.jackrabbit.core.state.SessionItemStateManager.hasItemState(SessionItemStateManager.java:206)
        at org.apache.jackrabbit.core.HierarchyManagerImpl.hasItemState(HierarchyManagerImpl.java:164)
        at org.apache.jackrabbit.core.CachingHierarchyManager.nodeAdded(CachingHierarchyManager.java:674)
        at org.apache.jackrabbit.core.CachingHierarchyManager.nodeAdded(CachingHierarchyManager.java:366)
        - locked <0xb4020630> (a java.lang.Object)
        at org.apache.jackrabbit.core.state.StateChangeDispatcher.notifyNodeAdded(StateChangeDispatcher.java:159)
        at org.apache.jackrabbit.core.state.SessionItemStateManager.nodeAdded(SessionItemStateManager.java:905)
        at org.apache.jackrabbit.core.state.NodeState.notifyNodeAdded(NodeState.java:852)
        at org.apache.jackrabbit.core.state.NodeState.renameChildNodeEntry(NodeState.java:370)
        - locked <0xb26df780> (a org.apache.jackrabbit.core.state.NodeState)
        at org.apache.jackrabbit.core.NodeImpl.renameChildNode(NodeImpl.java:559)
        at org.apache.jackrabbit.core.SessionImpl.move(SessionImpl.java:1034)
        at org.ametys.plugins.repository.DefaultSessionFactory$SessionWrapper.move(DefaultSessionFactory.java:398)
        at org.ametys.plugins.workflow.store.JackrabbitWorkflowStore.moveToHistory(JackrabbitWorkflowStore.java:797)
        at com.opensymphony.workflow.AbstractWorkflow.createNewCurrentStep(AbstractWorkflow.java:1474)
        at com.opensymphony.workflow.AbstractWorkflow.transitionWorkflow(AbstractWorkflow.java:1256)
        at com.opensymphony.workflow.AbstractWorkflow.doAction(AbstractWorkflow.java:567)
        at org.ametys.plugins.workflow.Workflow.doAction(Workflow.java:227)
        at org.ametys.plugins.workflow.WorkflowTestCase$ActionTrigger.run(WorkflowTestCase.java:195)

""ActionTrigger12"" daemon prio=10 tid=0x904dd000 nid=0x2f64 waiting for monitor entry [0x8fd3f000..0x8fd40020]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at org.apache.jackrabbit.core.CachingHierarchyManager.nodeModified(CachingHierarchyManager.java:306)
        - waiting to lock <0xb4020630> (a java.lang.Object)
        at org.apache.jackrabbit.core.state.StateChangeDispatcher.notifyNodeModified(StateChangeDispatcher.java:189)
        at org.apache.jackrabbit.core.state.SessionItemStateManager.nodeModified(SessionItemStateManager.java:929)
        at org.apache.jackrabbit.core.state.StateChangeDispatcher.notifyNodeModified(StateChangeDispatcher.java:189)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.stateModified(LocalItemStateManager.java:446)
        at org.apache.jackrabbit.core.state.XAItemStateManager.stateModified(XAItemStateManager.java:595)
        at org.apache.jackrabbit.core.state.StateChangeDispatcher.notifyStateModified(StateChangeDispatcher.java:111)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.stateModified(SharedItemStateManager.java:400)
        at org.apache.jackrabbit.core.state.ItemState.notifyStateUpdated(ItemState.java:244)
        at org.apache.jackrabbit.core.state.ChangeLog.persisted(ChangeLog.java:285)
        at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:735)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:1092)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:337)
        at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:347)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:312)
        at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:313)
        at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1103)
        - locked <0xb4004650> (a org.apache.jackrabbit.core.XASessionImpl)
        at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:858)
        at org.ametys.plugins.repository.DefaultSessionFactory$SessionWrapper.save(DefaultSessionFactory.java:414)
        at org.ametys.plugins.workflow.store.JackrabbitWorkflowStore.markFinished(JackrabbitWorkflowStore.java:757)
        at com.opensymphony.workflow.AbstractWorkflow.createNewCurrentStep(AbstractWorkflow.java:1473)
        at com.opensymphony.workflow.AbstractWorkflow.transitionWorkflow(AbstractWorkflow.java:1256)
        at com.opensymphony.workflow.AbstractWorkflow.doAction(AbstractWorkflow.java:567)
        at org.ametys.plugins.workflow.Workflow.doAction(Workflow.java:227)
        at org.ametys.plugins.workflow.WorkflowTestCase$ActionTrigger.run(WorkflowTestCase.java:203)

Different session instances (not XA) are used in both threads.
The nodes modified are different:
- ActionTrigger38 is moving a node and the session has not been saved yet.
- ActionTrigger12 has updated another node and is saving it.

I try to reproduce this behavior in a Jackrabbit test case without success.
If you need more information or a test case, let me know i will give it a second try."
0,"Simplify the usage of OCM annotationsIf we are using more reflections during the OCM init phase (class descriptor loading), some OCM annotation settings are not necessary : 

@Node(isAbtract=true) : used to specify an abstract classes
@Node(extend=....) : used to specify the ancestor class
@Node(isInterface= ...) : used to specify the entity as an interface
@implement  : used to specify the associated interfaces

If this refactoring is done, we can set them as deprecated.

The performances will not suffer because this is done only once during the application startup (when the ObjectContentManager is initialized). "
0,"Document inheritance of node type attributes such as orderableDocumentation task.

see http://mail-archives.apache.org/mod_mbox/jackrabbit-users/200801.mbox/browser"
0,"Put resource files in java/{main,test}/resourcesThe Maven standard directory layout (see http://maven.apache.org/guides/introduction/introduction-to-the-standard-directory-layout.html) suggests that all resource files should be placed in src/main/resources or src/test/resources instead of putting them inside the java subdirectory. Following that guideline would simplify the pom files as we wouldn't need to explicitly configure the resource directories. I'll move the resource files unless anyone argues otherwise."
0,"Improve handling of concurrent node modificationsconsider the following scenario:
- session1 modifies node /a by adding a child node b
- session2 modifies node /a by adding a child node c
- session2 saves its changes 
- session1 tries to save its changes which results in a InvalidItemStateException

this behavior is in accordance with the spec. the spec however doesn't prevent a 
more lenient handling of this scenario.

if the concurrent modifications are non-conflicting and trivial the changes could
be silently merged in session1's transient state of node /a.

examples for trivial non-conflicting changes:
- s1 adds child node a, s2 removes child node b
- s1 adds child node a, s2 adds child node b
- s1 adds child node a, s2 adds mixin type

examples for non-trivial and/or conflicting changes:
- s1 removes child node a, s2 removes child node a
- s1 adds child node a, s2 adds child node a
- s1 adds sns child node a (-> a[3]), s2 adds sns child node a (-> a[3])
- s1 adds sns child node a (-> a[3]), s2 removes sns child node a[1]
- s1 removes sns child node a[2], s2 reorders child nodes affecting sns nodes a

"
0,Backport JCR-940: add db connection autoConnect for BundleDbPersistenceManagerBackport issue JCR-940 ( add db connection autoConnect for BundleDbPersistenceManager) to 1.3 branch for 1.3.4 (separate issue to avoid re-opening JCR-940 which was already released with 1.4). 
0,Use inheritance rather than delegation for SPI ValueFactoryImplThe ValueFactoryImpl now has a protected constructor and the SPI variant of the value factory can use it.
1,"When trying to reuse version label in transaction, exception is thrownFollowing sequence causes failure:
1. first transaction:
  1.1 create versionable node
  1.2 create version 1 of this node
2. second transaction:
  2.1 create version 2 of this node
  2.2 assign a label to version 2
3. third transaction:
  3.1 restore a node to version 1
  3.2 remove version 2
  3.3 make version 3 of  same node
  3.4 assign same label (which was assigned to version 2) to version 3 -> fails

Same sequence which does not use transactions at all works fine.
Going to attach a test case."
0,"Add support for encrpted db password in repository.xmlBasically this is same to the issue https://issues.apache.org/jira/browse/JCR-2673. I can not reopen JCR-2673, so I filed a new one instead. 

The reason for this jira is because for a lot of companies it is not allowed to store password in a clear text. 

Sorry, I dont know how this can be implemented yet. But I hope at least the requirement is clear. 

Thanks."
0,"JCR-Server Code depends on Log4J directlyThe code is written against the Log4J APIs, which forces all users of Jackarabbit to pick up log4J dependency and to juggle with JDK logging and Log4J configuration if other components of the project uses JDK 1.4 logging.
If the code is move to depend on Apache commons-logging this issue will be resolved. Also this should be a minor fix. "
1,"renaming a node with Session.move() or Workspace.move() is not visible from other concurrent sessionsthis is a CachingHierarchyManager issue.

to reproduce:

Node n1 = session1.getRootNode().getNode(""foo"");
String path = n1.getPath();   // returns ""/foo""

Node n2 = session2.getRootNode().getNode(""foo"");
path = n2.getPath();   // returns ""/foo""

session1.move(""/foo"", ""/bar"");
session1.save();

path = n1.getPath();   // returns ""/bar""
path = n2.getPath();   // should return ""/bar"" but still returns ""/foo""
"
1,"persistent locks persist even after removing the locked nodeI read my post to the mailing list and I realized it wasn't clear at all :( . So I decided to upload the code to reproduce the error.

        RepositoryConfig conf = RepositoryConfig.create(""/temp/repository/repository.xml"", ""/temp/repository/repository"");
        Repository repo = RepositoryImpl.create(conf);
        
        // Session 1 creates a locked node
        Session s1 = repo.login(new SimpleCredentials(""user1"", ""pwd1"".toCharArray()));
        Node root1 = s1.getRootNode() ;
        Node test1 = root1.addNode(""test"") ;
        test1.addMixin(""mix:lockable"");
        s1.save() ;
        test1.lock(false, false);
        s1.logout() ;
        
        // Session 2 deletes the node and create a new one
        // with the same path
        Session s2 = repo.login(new SimpleCredentials(""user2"", ""pwd2"".toCharArray()));
        s2.getRootNode().getNode(""test"").remove();
        Node test2 = s2.getRootNode().addNode(""test"");
        s2.save() ;
        // the next line returns true :(
        System.out.println(test2.isLocked()) ;
        // the next line will throw an exception 
        test2.addMixin(""mix:lockable"") ;

javax.jcr.lock.LockException: Node locked.
	at org.apache.jackrabbit.core.lock.LockManagerImpl.checkLock(LockManagerImpl.java:449)
	at org.apache.jackrabbit.core.lock.LockManagerImpl.checkLock(LockManagerImpl.java:435)
	at org.apache.jackrabbit.core.NodeImpl.checkLock(NodeImpl.java:3847)
	at org.apache.jackrabbit.core.NodeImpl.addMixin(NodeImpl.java:964)
	at org.apache.jackrabbit.core.NodeImpl.addMixin(NodeImpl.java:2420)
	at org.apache.jackrabbit.core.LockTest.main(LockTest.java:55)"
0,Set svn:eol-style on ddl filesMost of the ddl files in jackrabbit-core/src/main/resources don't have the svn:eol-style property set. It should be set to native.
0,"Introduce Timer idle timeCurrently the Timer stops the internal thread as soon as there are no more tasks scheduled. With a usage pattern that repeatedly schedules tasks and immediately cancels them, one thread per scheduled task is created. There should be some idle time before the background thread is stopped."
1,AbstractExcerpt uses wrong loggerIt uses DefaultXMLExcerpt instead of AbstractExcerpt.
1,"occasional index out of bounds exception while running UserManagerImplTest.testFindAuthorizableByRelativePathStack trace:

java.lang.ArrayIndexOutOfBoundsException: 8
	at org.apache.jackrabbit.core.query.lucene.MultiScorer.score(MultiScorer.java:89)
	at org.apache.lucene.search.ConjunctionScorer.score(ConjunctionScorer.java:133)
	at org.apache.lucene.search.BooleanScorer2$2.score(BooleanScorer2.java:182)
	at org.apache.lucene.search.BooleanScorer2.score(BooleanScorer2.java:303)
	at org.apache.jackrabbit.core.query.lucene.LuceneQueryHits.nextScoreNode(LuceneQueryHits.java:68)
	at org.apache.jackrabbit.core.query.lucene.QueryHitsAdapter.nextScoreNodes(QueryHitsAdapter.java:54)
	at org.apache.jackrabbit.core.query.lucene.FilterMultiColumnQueryHits.nextScoreNodes(FilterMultiColumnQueryHits.java:63)
	at org.apache.jackrabbit.core.query.lucene.QueryResultImpl.collectScoreNodes(QueryResultImpl.java:328)
	at org.apache.jackrabbit.core.query.lucene.QueryResultImpl.getResults(QueryResultImpl.java:291)
	at org.apache.jackrabbit.core.query.lucene.SingleColumnQueryResult.<init>(SingleColumnQueryResult.java:66)
	at org.apache.jackrabbit.core.query.lucene.QueryImpl.execute(QueryImpl.java:134)
	at org.apache.jackrabbit.core.query.QueryImpl$1.perform(QueryImpl.java:130)
	at org.apache.jackrabbit.core.query.QueryImpl$1.perform(QueryImpl.java:1)
	at org.apache.jackrabbit.core.session.SessionState.perform(SessionState.java:200)
	at org.apache.jackrabbit.core.query.QueryImpl.execute(QueryImpl.java:126)
	at org.apache.jackrabbit.core.security.user.IndexNodeResolver.findNodes(IndexNodeResolver.java:109)
	at org.apache.jackrabbit.core.security.user.UserManagerImpl.findAuthorizables(UserManagerImpl.java:498)
	at org.apache.jackrabbit.core.security.user.UserManagerImpl.findAuthorizables(UserManagerImpl.java:462)
	at org.apache.jackrabbit.core.security.user.UserManagerImplTest.testFindAuthorizableByRelativePath(UserManagerImplTest.java:560)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at org.apache.jackrabbit.test.AbstractJCRTest.run(AbstractJCRTest.java:456)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:130)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)

"
1,"Access control evaluation does not properly cope with XA transactionsthe following test fails with ItemNotFoundException at the indicated position due to the fact that
the parent n2 is EXISTING but still not visible to the system session responsible for the ac
evaluation.

public void testTransaction() throws Exception {

        // make sure testUser has all privileges
        Privilege[] privileges = privilegesFromName(Privilege.JCR_ALL);
        givePrivileges(path, privileges, getRestrictions(superuser, path));

        // create new node and lock it
        Session s = getTestSession();
        UserTransaction utx = new UserTransactionImpl(s);
        utx.begin();

        // add node and save it
        Node n = s.getNode(childNPath);
        if (n.hasNode(nodeName1)) {
            Node c = n.getNode(nodeName1);
            c.remove();
            s.save();
        }

        Node n2 = n.addNode(nodeName1);
        s.save();
            
        Node n3 = n2.addNode(nodeName2);
        s.save(); // exception

        // commit
        utx.commit();
    }

A possible workaround would be to make sure that ItemSaveOperation.persistTransientItems 
retrieves the parent without having the checkPermission enabled since we can assume that
the new item could not be added if the parent was not readable in the first place.... but careful
evaluation would be required.

NOTE: this is just one example of the AC-evaluation not properly dealing with XA transactions.
I am convinced that other examples could be find....
"
0,Improve equals for binary valuesThe current implementation of spi2dav's equals method for binary values is not symmetric. Moreover it forces the values to be loaded instead of trying to determine equality by examining etag and last-modified headers. 
1,"PROPPATCH in simple webdav server failing with 403 Forbidden error
i've configured the import-collection chain (called via MKCOL) to add nodes with the node type ""dav:collection"", which is defined as such:

NodeTypeName
  dav:collection
SuperTypes
  nt:folder
IsMixin
  false
HasOrderableChildNodes
  false
PrimaryItemName
  null
PropertyDef
  Name *
  RequiredType UNDEFINED
  DefaultValues null
  AutoCreate false
  Mandatory false
  OnParentVersion COPY
  Protected false
  Multiple false
PropertyDef
  Name *
  RequiredType UNDEFINED
  DefaultValues null
  AutoCreate false
  Mandatory false
  OnParentVersion COPY
  Protected false
  Multiple true

the idea is that i should be able to set arbitrary webdav properties (single- or multi-value) on a webdav collection.

when i use cadaver to mkcol a collection (creating a jcr node of type dav:collection) and then try to propset an arbitrary property on that collection (which as i understand it would set a jcr property of the same name on the dav:collection node), i get a 403 Forbidden error:

dav:/home/> mkcol hi
Creating `hi': succeeded.

dav:/home/> propset hi hi hi
Setting property on `hi': failed:
403 Forbidden

it's not clear to me if i'm using cadaver incorrectly, misunderstanding the PROPPATCH implementation, or both :) "
0,AccessControlEntries should be orderablethe entries of an AccessControlList should be orderable if the order of the ac-entries matters for the access control evaluation.
0,"Utility code for filtering and packaging treesThe attached zip contains new utility code for filtering and packaging trees in the repository.

A tree can be traversed by the provided tree walker. During the traversal configurable filters can be applied. The filters have influence on the traversal, like skipping nodes or properties.
Included filters test the node name, node type etc. Custom filters are possible as well.
A tree walker notifies a tree walker listener (interface) whenever it traverses an item.

The second utility code is able to package a whole tree (through a description) and export this in some way - the exporter is an interface and could e.g. be an exporter serializing the tree into a zip archiv etc."
0,"Improve handling of inherited mixinsIf an abstract class is annotated with a mixin type, the annotation must be repeated in concrete classes.

E.g.
@Node(jcrMixinTypes=""mix:referenceable"", isAbstract=true)
public abstract class Content {
...
...}

/**
* This class will not be referenceable
**/
@Node(extend=Content.class)
public class Page extends Content {
...
...
}

/**
* But this one will
**/
@Node(extend=Content.class, jcrMixinTypes=""mix:referenceable"")
public class Folder extends Content {
...
...
}

It would be nice if the annotation was inherited by default."
0,"JCR2SPI: add configurable cache for Item instances (ItemManager)Currently the ItemManager implementation uses a simple map with weak keys (ItemState) and weak values (Item) as cache.
Marcel recently suggested to replace this with a more sophisticated cache mechanism that can be configured."
0,Login performance drop when using DefaultAccessManagerJCR-2700 caused a drop in login performance when using DefaultAccessManager. The drop is caused by the initialization of the LRU map used in CompiledPermissionsImpl. 
1,"Session.save() potentially causes endless loop when READ permission is denied on root nodeif the current session doesn't have read permission on the root node, calling Session.save() triggers a call to SessionItemStateManager.getIdOfRootTransientNodeState()
in order to find the root of the minimal subtree including all transient states. this might cause an endless loop, depending on the transient changes."
0,"UserManagerImpl: typo in ""compatibleJR16"" config option constant"
0,Package names for spring project do not match update ocm packagesThe spring package and tests reference the old graffitto package naming scheme.
0,"Improve and promote spi-loggerThe spi-logger is very useful for debugging an SPI implementation. However it only supports the RepositoryService interface. Other SPI interfaces are not supported. Also writing log information as string seems a bit restrictive to me. Finally it is not included in the jackrabbit-spi package which introduces an additional dependency for debugging. 

I therefore suggest:
- Add support for the other major SPI interfaces 
- Replace the current way of logging a String to a Writer instance by a more versatile mechanism (i.e. clients have to provide an interface which consumes more structured log data). 
- Provide some default implementation for the above mechanism (i.e. writing to a file, writing to a slf4j based logger, writing to the console...)
- Promote this project to jackrabbit-spi
"
0,"JCR unit tests use invalid queriesAccording to Section 8.5.2.11 of the JCR 1.0 specification:

    It is optional to support properties in the SELECT, WHERE and ORDER BY clauses that are not explicitly
    defined in the node types listed in the FROM clause but which are defined in subtypes of those node types.

    It is optional to support the specifying of properties in the SELECT,WHERE and ORDERBY clauses that 
    are not explicitly defined in the node types listed in the FROM clause but which are defined in mixin 
    node types that may be assigned to node instances of the types that are mentioned in the SELECT clause.

However, two of the test methods in the org.apache.jackrabbit.test.api.query.SQLJoinTest class are producing and executing queries that use in the WHERE clause different properties and node types than those listed in the FROM clause.  The testJoinNtBase() method is producing a query using the following code:

        StringBuffer query = new StringBuffer(""SELECT * FROM "");
        query.append(ntBase).append("", "").append(testMixin);
        query.append("" WHERE "");
        query.append(testNodeType).append(""."").append(jcrPath);
        query.append("" = "");
        query.append(mixReferenceable).append(""."").append(jcrPath);
        query.append("" AND "").append(jcrPath).append("" LIKE "");
        query.append(""'"").append(testRoot).append(""/%'"");

This code will produce a valid query only when ""testNodeType"" is set to ""nt:base"" and ""testMixin"" is set to ""mix:referenceable"":

    SELECT * FROM nt:base, mix:referenceable 
    WHERE nt:base.jcr:path = mix:referenceable.jcr:path AND ...

However, when any other values for ""testNodeType"" and ""testMixin"" are used, this produces an invalid query in which the WHERE criteria references tuple sources that do not exist in the FROM clause.  For example, when ""testNodeType"" is ""my:type"" and ""testMixin"" is ""my:mixin"", the query becomes:

    SELECT * FROM nt:base, my:mixin 
    WHERE my:type.jcr:path = mix:referenceable.jcr:path AND ...

This code can be corrected by simply using the ""testNodeType"" in the FROM clause.

A similar bug is in the testJoinFilterPrimaryType() method, which uses this code:

        StringBuffer query = new StringBuffer(""SELECT * FROM "");
        query.append(testNodeType).append("", "").append(ntBase);
        query.append("" WHERE "");
        query.append(testNodeType).append(""."").append(jcrPath);
        query.append("" = "");
        query.append(mixReferenceable).append(""."").append(jcrPath);
        query.append("" AND "").append(jcrPath).append("" LIKE "");
        query.append(""'"").append(testRoot).append(""/%'"");

This code will really never produce a valid query, since the FROM clause uses the ""testNodeType"" and ""nt:base"" node types, whereas the WHERE clause will use the ""testNodeType"" and ""mix:referenceable"" types.  For example, if ""testNodeType"" has a value of ""my:type"", the query becomes:

    SELECT * FROM my:type, nt:base 
    WHERE my:type.jcr:path = mix:referenceable.jcr:path AND ...
"
0,"[SUBMISSION] Amazon S3 Persistence Manager ProjectAs I noted previously on the dev-list (http://markmail.org/search/?q=amazon+list%3Aorg.apache.jackrabbit.dev#query:amazon%20list%3Aorg.apache.jackrabbit.dev+page:1+mid:qw27gopsn4lnbde5+state:results) I have written an Amazon S3 bundle persistence manager for Jackrabbit. I want to submit the code for the sandbox, the full source is included in the zip file. Licensed under the ASF.

The project also aims to implement a normal persistence manager (which I abandoned in favor of the more efficient bundle pm, which is implemented, but does not work 100%), a file system impl for S3 (only rough structure present) and an SPI impl that connects to S3 (dreaming ;-)). For more infos, I will include the README.txt of the project here:

=================================================================
Welcome to Jackrabbit persistence for Amazon Webservices (ie. S3)
=================================================================

This module contains various persistence options for using
Amazon Webservices as backend for Jackrabbit / JCR. Amazon has
two persistence services: S3 (public) and SimpleDB (still beta).
The following options are available/ideas:

- (1) persistence managers that connects to S3
      (normal + bundle, in work, probably not very efficient)
      
- (2) persistence manager that connects to SimpleDB
      (NOT feasible)
      
- (3) SPI implementation that connects to S3
      (not implemented, very complicated, probably more efficient)
      
See details below and also TODO.txt


Installing / Testing
====================

This needs a patched Jackrabbit 1.3.x version. The patches can
be found in the directory ""patches-for-1.3"". One patch will modify
the pom of jackrabbit-core to generated the jackrabbit test jar
for reuse in this project. To build that customized version, you need
to do the following steps:

1) svn co http://svn.apache.org/repos/asf/jackrabbit/branches/1.3 jackrabbit-1.3
2) cd jackrabbit-1.3
3) apply all patches from the ""patches-for-1.3"" directory:
   patch -p0 < %JR-AMAZON-PATH%/patches-for-1.3/%PATCH%.patch
4) mvn install
5) cd %JR-AMAZON-PATH%
6) change jackrabbit version number in pom.xml to the one you just built
   (eg. project/parent/version = 1.3.4)
7) cp aws.properties.template aws.properties
8) enter your credentials in aws.properties
9) mvn test

For debugging, you can change the logging in applications/test/log4j.properties
and set up proxying (for monitoring the traffic with eg. tcp mon) in
applications/test/jets3t.properties.


Details about Implementations
=============================

(1) org.apache.jackrabbit.persistence.amazon.AmazonS3PersistenceManager

http://www.amazon.com/s3

Stores JCR Nodes and Properties inside S3 Objects. Uses UUID for Nodes and
UUID/Name for Properties as Object names. Node references are stored
via references/UUID.

Configuration parameters:

accessKey
    Amazon AWS access key (aka account user id) [required]

secretKey
    Amazon AWS secret key (aka account password) [required]
    
bucket
    Name of the S3 bucket to use [optional, default uses accessKey]
    Note that bucket names are global, so using your accessKey is
    recommended to prevent conflicts with other AWS users. 
    
objectPrefix
    Prefix used for all object names [optional, default is """"]
    Should include the workspace name (""${wsp.name}"" or ""version"" for
    the versioning PM) to put multiple workspaces into one bucket.

Example XML Config:

<PersistenceManager class=""org.apache.jackrabbit.persistence.amazon.AmazonS3PersistenceManager"">
    <param name=""accessKey""    value=""abcde01234""/>
    <param name=""secretKey""    value=""topsecret""/>
    <param name=""bucket""       value=""abcde01234.jcrstore""/>
    <param name=""objectPrefix"" value=""${wsp.name}/""/>
</PersistenceManager>

-----

(2) AmazonSimpleDBPersistenceManager

This is *not* feasible because of the restrictions that are applied
to SimpleDB. An item can only have up to 256 attributes, each attribute
can only contain a string value and that one can only have 1024 chars.
See this link for more information:

http://docs.amazonwebservices.com/AmazonSimpleDB/2007-11-07/DeveloperGuide/SDB_API_PutAttributes.html

-----

(3) org.apache.jackrabbit.spi2s3

TODO

lots of work...


About
=====

It was originally written by Alexander Klimetschek
(alexander.klimetschek at googlemail dot com) in 2008.

See the Apache Jackrabbit web site (http://jackrabbit.apache.org/)
for documentation and other information. You are welcome to join the
Jackrabbit mailing lists (http://jackrabbit.apache.org/mail-lists.html)
to discuss this component and to use the Jackrabbit issue tracker
(http://issues.apache.org/jira/browse/JCR) to report issues or request
new features.

Apache Jackrabbit is a project of the Apache Software Foundation
(http://www.apache.org).
"
0,"Add JcrUtils.getPropertyTypeNames for property related ui its common to populate a list with all property type names.
instead of hardcoding at various places that list could be provided by JcrUtils."
1,"Deadlock when executing Version operationsThis only happens when a XA transaction is committed without changes. In XAVersionManager there is a check in the InternalXAResource returned by getXAResourceBegin(), which only acquires the write lock on the version manager if there are version related changes in the transaction. This kind of check is missing in the methods XAVersionManager.prepare/commit/rollback()."
0,"Webdav Simple: Delegate PROPPATCH to (extended) IOHandlerscomplete description:
http://article.gmane.org/gmane.comp.apache.jackrabbit.devel/6582

Proposed solution
------------------------------------------------------------------------------------------------------------------------------------

The situation described before leads me to the following conclusion:

- IOHandler should not only read/write resource data and properties during GET, PUT, PROPFIND but should
   also take care of setting/removing properties upon PROPPATCH.

- Since the previous suggestion would still limit the properties to (jcr:encoding, jcr:mimeType and
   jcr:lastModified), we may think about changing the default nodetype for the jcr:content node to
   nt:unstructured.

I guess this would meet the requirements for those expecting a webDAV server that is (as a first step)
not limited regarding PROPPATCH. Second it would allow to have a handling of property modifications
which is specific for individual resource types instead of trying to set all properties to the uppermost node."
1,"Lucene Query Exception: 'attempt to access a deleted document'Hi,

I am getting an exception when trying to execute a query through the (Spring) JcrTemplate class....using the following code:
QueryManager qMgr = session.getWorkspace().getQueryManager();
QueryResult result = qMgr.createQuery(xpathQuery, Query.XPATH ).execute();

The exception is thrown at the second line and is as follows:

[DEBUG] << ""[0x9]at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:113)[\n]""
[DEBUG] << ""[0x9]at org.apache.lucene.search.Hits.getMoreDocs(Hits.java:74)[\n]""
[DEBUG] << ""[0x9]at org.apache.lucene.search.Hits.&lt;init>(Hits.java:53)[\n]""
[DEBUG] << ""[0x9]at org.apache.lucene.search.Searcher.search(Searcher.java:46)[\n]""
[DEBUG] << ""[0x9]at org.apache.lucene.search.Searcher.search(Searcher.java:38)[\n]""
[DEBUG] << ""[0x9]at org.apache.jackrabbit.core.query.lucene.SearchIndex.executeQuery(SearchIndex.java:660)[\n]""
[DEBUG] << ""[0x9]at org.apache.jackrabbit.core.query.lucene.QueryResultImpl.executeQuery(QueryResultImpl.java:242)[\n]""
[DEBUG] << ""[0x9]at org.apache.jackrabbit.core.query.lucene.QueryResultImpl.getResults(QueryResultImpl.java:290)[\n]""
[DEBUG] << ""[0x9]at org.apache.jackrabbit.core.query.lucene.QueryResultImpl.&lt;init>(QueryResultImpl.java:192)[\n]""
[DEBUG] << ""[0x9]at org.apache.jackrabbit.core.query.lucene.QueryImpl.execute(QueryImpl.java:138)[\n]""
[DEBUG] << ""[0x9]at org.apache.jackrabbit.core.query.QueryImpl.execute(QueryImpl.java:176)[\n]""
[DEBUG] << ""[0x9]at com.intel.cds.cr.jcr.JcrManager$5.doInJcr(JcrManager.java:363)[\n]""
[DEBUG] << ""[0x9]at org.springmodules.jcr.JcrTemplate.execute(JcrTemplate.java:76)[\n]""
[DEBUG] << ""[0x9]at org.springmodules.jcr.JcrTemplate.execute(JcrTemplate.java:108)[\n]""
[DEBUG] << ""[0x9]... 19 more[\n]""
[DEBUG] << ""</Exception></detail></soapenv:Fault></soapenv:Body></soapenv:Envelope>""
org.apache.axis2.AxisFault: attempt to access a deleted document
	at org.apache.axis2.util.Utils.getInboundFaultFromMessageContext(Utils.java:486)
	at org.apache.axis2.description.OutInAxisOperationClient.handleResponse(OutInAxisOperation.java:343)
	at org.apache.axis2.description.OutInAxisOperationClient.send(OutInAxisOperation.java:389)
	at org.apache.axis2.description.OutInAxisOperationClient.executeImpl(OutInAxisOperation.java:211)
	at org.apache.axis2.client.OperationClient.execute(OperationClient.java:163)


My Jackrabbit/Lucene configuration is as follows:

<SearchIndex class=""org.apache.jackrabbit.core.query.lucene.SearchIndex"">
        <param name=""path"" value=""${rep.home}/repository/index""/>
        
        <param name=""useCompoundFile"" value=""false""/>
        <param name=""mergeFactor"" value=""5""/>
        <param name=""cacheSize"" value=""10000""/>
        <param name=""respectDocumentOrder"" value=""false""/>  
  </SearchIndex>

Is this a configuration issue or a bug?

Thanks,
David."
0,"SessionImpl#getSubject() should return an unmodifiable subjectfor security reasons the subject exposed by SessionImpl#getSubject() should be unmodifiable or at least changes made
to it should not be modify the subject hold by the session.

currently i see the following options to get there:
a: set readonly flag on the subject associated with the session
b: getSubject() returns a new instance of Subject having the same characteristics as the subject associated with the session
c: getSubject() returns a new but readonly Subject instance

my preferred solution was c as
- it doesn't change the characteristics of the subject
- the unmodifiable status is transparent to the caller since modifying the subject fails without forcing the api consumer
  to read the javadoc to know why changing the subject is not reflected on the session itself (that would be a drawback of b)."
0,"ClusteringImplement basic clustering, i.e. make two or more repositories available at the same time,  allowing them to stay in sync with changes applied to only one of them."
0,"ReferencesPropertyTest can't deal with multivalued reference propertiesThe setUp() method uses prop.getNode(), thus assuming that the reference property is not multivalued.
"
0,"Inline nested jars in OSGi bundlesEclipse doesn't support bundles with nested jars (https://bugs.eclipse.org/bugs/show_bug.cgi?id=111238). The workaround is to inline the contents of the nested jars. This is a simple fix that shouldn't impact non-Eclipse users:

pom.xml
===================================================================
- <Embed-Dependency>*;scope=compile|runtime;inline=false</Embed-Dependency>
+ <Embed-Dependency>*;scope=compile|runtime;inline=true</Embed-Dependency>
"
0,"Default charsetAs defined in RFC2616 the default character set is ISO-8859-1 an not US-ASCII 
as defined in HttpMethodBase. See ""3.7.1 Canonicalization and Text Defaults"" at
RFC 2616"
0,"cache does not allow client to override origin-specified freshness using max-staleAccording to the RFC, the default freshness lifetime is supposed to be the LEAST restrictive of that specified by the origin, the client, and the cache. Right now, a client can't use 'max-stale' to relax the freshness constraints to get a cache hit without validation occuring first.
"
1,"Preemptive authentication causes NTLM auth scheme to failThe NTLM authentication scheme does not work when the preemptive authentication
is enabled.

Reported by Dave Seidel <dave at mindreef.com>"
1,"HttpMethodDirector fails when redirecting to a encoded URL locationWhen HttpMethodDirector handles the case of redirecting the incoming connection to the location specified in the header of the http caller method, if this location has any ""special"" charset encoding (extended charsets like ISO 8859-1,etc.) the redirection fails this way:

dd-MMM-YYYY hh:mm:ss org.apache.commons.httpclient.HttpMethodDirector processRedirectResponse
WARNING: Redirected location 'http://www.anyCharsetEncodedUrl.ko' is malformed


You can test it using this class:


public class SimpleHttpTestNotWorking {

	public static int urlStatus(String pUrl) throws org.apache.commons.httpclient.HttpException,java.io.IOException{
		org.apache.commons.httpclient.HttpClient client = new org.apache.commons.httpclient.HttpClient();
		org.apache.commons.httpclient.HttpMethod method = new org.apache.commons.httpclient.methods.GetMethod(pUrl);
		return client.executeMethod(method);
	}
		
	public static void main(String[] args) {
		try{
			String url = ""http://www.dipualba.es/municipios/F%E9rez""; //known problematic URL
			System.out.println(""Return code for [""+url+""]: ""+SimpleHttpTestWorking.urlStatus(url));
		}catch(Exception e){
			e.printStackTrace();
		}
	}
}


What I've done to solve it for my particular case has been:


1) In the requester side, I've modified the calling:


public class SimpleHttpTestWorking {

	public static int urlStatus(String pUrl) throws org.apache.commons.httpclient.HttpException,java.io.IOException{
		org.apache.commons.httpclient.HttpClient client = new org.apache.commons.httpclient.HttpClient();
		org.apache.commons.httpclient.HttpMethod method;
	    String encoding = (String)client.getParams().getParameter(""http.protocol.content-charset"");
	    client.getParams().setParameter(""http.protocol.element-charset"", encoding);
	    try{
	    	method = new org.apache.commons.httpclient.methods.GetMethod(pUrl);
	    }catch(IllegalArgumentException iae){
		    try{
		    	org.apache.commons.httpclient.URI uri = new org.apache.commons.httpclient.URI(pUrl,true);
		    	method = new org.apache.commons.httpclient.methods.GetMethod(uri.getURI());
		    }catch(org.apache.commons.httpclient.URIException ue){
		    	org.apache.commons.httpclient.URI uri = new org.apache.commons.httpclient.URI(pUrl,false,encoding);
			    method = new org.apache.commons.httpclient.methods.GetMethod(uri.getEscapedURI());
		    }		    	
	    }		
		return client.executeMethod(method);
	}
			
	public static void main(String[] args) {
		try{
			String url = ""http://www.dipualba.es/municipios/Férez""; //the same problematic URL
			System.out.println(""Return code for [""+url+""]: ""+SimpleHttpTestWorking.urlStatus(url));
		}catch(Exception e){
			e.printStackTrace();
		}
	}
}


2) In org.apache.commons.httpclient.HttpMethodDirector.processRedirectResponse(HttpMethod method) , I've replaced


...
redirectUri = new URI(location, true);
...


for the following code:


...
/*
 * [2006-11-14] 
 * Handles redirections to encoded URI locations 
 * (only if URI and Connection encoding charset has been properly setted)
 * */ 
try{
	redirectUri = new URI(location, true);
}catch(URIException ue){
	Object encoding = this.conn.getParams().getParameter(""http.protocol.element-charset"");
	if(encoding != null){
		redirectUri = new URI(location, false, (String)encoding);
	}else{
		throw ue;
	}
}
...



Hope it helps!"
0,"Inadequate HTTP proxy server support in HttpClient.1. The HttpClient class does not save the StatusLine from the hidden
ConnectMethod object used to connect via an HTTP proxy server, thus any proxy
failures are only picked up as 'anonymous exceptions', this is useless for
gracefull recovery and rapid debugging.

2. The current class structure is too fragile to neatly support HTTP Proxy (and
authenication) chains so it would be a good idea to look at this at the same
time, preferable with support for a Proxy chain redirect when an non/dead HTTP
Proxy server is found."
0,"[GSoC 2011] Fluent API to HttpClientDevelop fluent API / facade to HttpClient based on code currently maintained by Apache Stanbol and Apache Sling projects. 

For details see 

http://markmail.org/message/mmyljtgjp3za6kyz

or contact Apache HttpComponents committers at dev@hc.apache.org"
1,"infinite loop on 302 redirect with different host in Location: headerUsing the CVS version - 2.1 rc I think.

Trying to get the url contents for a url with followRedirects specified and 
StrictMode off and I get a 302 redirect to a different url with a different 
host in the Location header causes it to go into an infinite loop (mercifully 
aborting at 100 redirects).  An example offending url: 

http://www.snowcrest.net/mice/mice.htm

The problem lies in HttpMethodBase.java version 1.177 at line 1225.  It does 
the following:

        if (getRequestHeader(""host"") != null) {
            LOG.debug(
                ""Request to add Host header ignored: header already added"");
            return;
        }

and there is already the Host header from the previous url request so it 
endlessly loops until it aborts at the max redirects (default is 100 I guess).  
I commented this out and it worked fine."
0,"Incorrect copyright statementsMost of the copyright statements in http-core are for 1999-2004.  These should
be updated with the correct years."
0,"URI reference resolution fails examples in RFC 3986org.apache.http.client.utils.URIUtils.resolve(final URI baseURI, URI reference) fails to resolve some examples from RFC 3986 section 5.3 correctly. See TestCase."
1,"NullPointerException in HttpMethodBase.getResponseBodyAsStringThe following code in a cocoon component, causes the NPE.
A delay seems to help sometimes.

-------------
      int htcode = httpClient.executeMethod( method );
       
      // @todo: fix-me
      // This sleep() is a temporary workaround 
      // to avoid NullPointerException in the next line.
      Thread.currentThread().sleep( 100 ); 

      String ret = method.getResponseBodyAsString();
---------------------

java.lang.NullPointerException 
at java.lang.String.<init>(String.java:399) 
at org.apache.commons.httpclient.HttpMethodBase.getResponseBodyAsString
(HttpMethodBase.java:579) 
at org.apache.cocoon.generation.WebServiceProxyGenerator.fetch
(WebServiceProxyGenerator.java:264)"
0,"typo in RFC reference in web siteQuoting from <http://hc.apache.org/httpcomponents-client/index.html>:

""Standards Compliance

HttpClient strives to conform to the following specifications endorsed by the Internet Engineering Task Force (IETF) and the internet at large:

    * RFC 1945 - Hypertext Transfer Protocol -- HTTP/1.0
    * RFC 2116 - Hypertext Transfer Protocol -- HTTP/1.1
    * RFC2617 HTTP Authentication: Basic and Digest Access Authentication
    * RFC2109 HTTP State Management Mechanism (Cookies)
    * RFC2965 HTTP State Management Mechanism (Cookies v2)""

Note the typo in the reference to HTTP/1.1.
"
0,"o.a.h.conn.scheme.PlainSocketFactory is not a true SingletonThe class ""org.apache.http.conn.scheme.PlainSocketFactory"" has a factory method, getSocketFactory(), and clearly indicates in the Javadocs that it expects to be a Singleton; however, the presence of public constructors makes it quite possible that this is not the case.

To protect the Singleton status of the class, the constructors should be private, or, at the very least, default (package) access. This will force access to the single instance through the factory method."
0,"Use TRACE logging instead of DEBUG for the absolute nitty-gritties[This is basically a copy of the Spring improvement request SPR-2873: http://opensource.atlassian.com/projects/spring/browse/SPR-2873 )

Given a developer situation: Much of the DEBUG information in the log of HttpClient is very un-interesting as long as it works. Some of these lines are however of much bigger importance than others (thus turning off DEBUG globally for HttpClient isn't good either).

TRACE and DEBUG are the two developer-centric logging levels of log4j and commons logging (the rest are ""production levels""). Since log4j-1.2.12, TRACE have existed. Clogging have always had trace, but before release 1.1 mapped Log.trace to log4j's DEBUG, but 1.1 (released May 9. 2006) now maps to log4j's TRACE.

I think that HttpClient's logging would benefit a lot by using TRACE level extensively, in that developers could turn all of httpclient's logging down to DEBUG, but still see ""major developer events"" like connections being opened, the request being sent, and e.g. the response's status line, size of headers and body, keep-alive vs. closing of connection.

Candidates for TRACE level include:
  * httpclient.wire.*
  * org.apache.commons.httpclient.params.DefaultHttpParams
  * org.apache.commons.httpclient.HttpMethodBase
  * .. and probably a bunch of others that doesn't bring the developer in the standard ""good flow mode"" any highly interesting information. 

Please note that I do NOT view these lines as worthless. It is however in _normal_ developer circumstances not valuable information, and it would ease development if it was possible to turn these ultra-verbose loglines off easily. When things just aren't working out, and your exciting REST-based query doesn't work out, or your charset encodings just doesn't give what you're expecting, you'd turn on TRACE to really get down to the hard core. You'd find the problem, fix it, and set it to DEBUG again.

In addition, the lines that were left on the DEBUG level should obviously be as informative as possible, and thus maybe somewhat more verbose than now, trying to ""aggregate"" some pieces of information that now are output over several DEBUG lines..

I do realize that I could achive a lot of this with a rather extensive log configuration, that also had to include raw text filters, but I do believe that this affects more developers than me!

PS: it wouldn't hurt either if all of httpclient's log-lines came from a common root, e.g. ""HttpClient"", or ""org.apache.commons.httpclient"", instead of having several roots. This would however be a somewhat ""backward incompatible"" change, since it now has (at least?) two roots."
0,"add support to caching module for RFC 5861 (stale-on-error and stale-while-revalidate)These are Cache-Control extensions that allow an origin server to specify some additional behavior for stale cache entries. Stale-on-error configurations allow a cache to continue serving stale content for a certain period of time if a revalidation fails, and stale-while-revalidate similarly allows revalidation to occur asynchronously. Some reverse proxies such as Squid can be configured to understand these headers, which means that some origin servers are probably sending them, and that we can likewise take advantage of them.
"
0,"API surface of caching module can be reducedWhile the caching module can currently be considered functional and useful for folks as-is, there are several near-term enhancements planned that could change the exposed binary API of the caching module (although it is not yet clear whether they would or not). In an effort to allow the 4.1 GA release to go forward while hedging bets against future development, we should consider drastically reducing the exposed binary API of the caching module, and not exposing extension points until someone explicitly asks for them.
"
0,"AbstractConnPool constructor calls thread.Start()AbstractConnPool constructor calls thread.Start()

Findbugs says:

Constructor invokes Thread.start()

The constructor starts a thread. This is likely to be wrong if the class is ever extended/subclassed, since the thread will be started before the subclass constructor is started.

The class is not final (and the constructor is protected) which suggests that the class is intended to be extended..."
1,"String.toLowerCase() / toUpperCase() should specify Locale.ENGLISHThere are quite a few instances of String.toLowerCase() - and some of toUpperCase() - method calls which don't specify the Locale.

These should probably mostly/all use Locale.ENGLISH, otherwise there may be problems in some Locales.
e.g. Turkey, where ""i"".toUpperCase() is not equal to ""I"" - and vice-versa.

The isSpecialDomain() method in NetscapeDomainHandler is one instance where the code won't always work in Turkey."
0,"Incorrect debug message in HttpMethodBaseHttpMethodBase.addContentLengthRequestHeader has the wrong debug message.  See
attached patch."
0,"String constants should be finalRFC2109Spec - SET_COOKIE_KEY

RFC2965Spec - SET_COOKIE2_KEY

both should be final."
0,"Allow polymorphic use of addParameterI have some common code (in a reverse proxy server) that uses addParameter on 
instances of both PostMethod and MultipartPostMethod

It would be great if either addParameter were made an abstract method on 
ExpectContinueMethod or both were made to implement a common base class. Here's 
my workaround:

    private void addPostParameter(ExpectContinueMethod method, String name, 
String value) {
        if (method instanceof PostMethod) {
            ((PostMethod)method).addParameter(name, value);
        } else if (method instanceof MultipartPostMethod) {
            ((MultipartPostMethod)method).addParameter(name, value);
        } else {
            throw new IllegalArgumentException(""addPostParameter is only 
defined for PostMethod and MultipartPostMethod"");
        }
        
    }
    // whoa - smells pretty bad"
1,"Missing Content-Length header makes cached entry invalidA cached entry whose original response didn't carry a Content-Length header, should not be rejected for considered invalid because the length of its cached content is different from the non-existing Content-Length header value. The attached patch only verifies the lengths if the header was originally present."
0,[API DOC] Authentication guide update: alternate authenticationAdd a section on alternate authentication.
1,"FileRequestEntity in SVN does not close input fileFileRequestEntity.java in SVN does not close input file - however the version on the web page:

http://jakarta.apache.org/commons/httpclient/performance.html

has a finally clause that closes the file ;-) - perhaps the source code should too...!"
0,"Redesign of HTTP authentication frameworkThe existing HTTP authentication framework has got a few glaring deficiencies:
- Authentication headers management evolved (or degraded) into a some sort of
black art and proved very error-prone.
- Existing logic intended to deal with authentication failures and
authentication failure recovery is flawed. The resolution of the HTTPCLIENT-213 did
appear possible without a better approach than the one based on AuthScheme#getID.

On top of that authentication logic got quite messy with the series of attempts
to fix breakages in complex authentication schemes (the latest being NTLM proxy
+ basic host fix) 

The patch I am about to attach is an attempt to address all the shortcomings
mentioned above. It builds upon my previous patch that enabled authentication
schemes to maintain authentication state and presents a complete redesign of the
existing HTTP authentication framework.

Basically there's no authentication code left untouched, so please do take a
closer look. Critique, comments, suggestions welcome.

Oleg"
0,"[HttpClient] Better proxy support in HttpMultiClientIf proxy requires authentication, it sends status 407 (Proxy Authentication 
Required) and the response header ""Proxy-Authenticate"" (see RFC2616; e.g. Squid 
can be configured to do so).
HttpClient doesn't yet process this response.
Behavior should be similar to processing of status 401 (Unauthorized)."
1,"Cookies with null value are not formatted correctlyI have a server that sets a bunch of empty cookies:

2003/03/06 16:28:52:055 PST [DEBUG] wire - -<< ""Set-Cookie: list%2ESince=; 
path=/[\r][\n]""
2003/03/06 16:28:52:055 PST [DEBUG] wire - -<< ""Set-Cookie: search%2EPhoneSDA=; 
path=/[\r][\n]""


  On subsequent requests, httpclient attaches these cookies thusly:

2003/03/06 16:28:55:480 PST [DEBUG] wire - ->> ""Cookie: $Version=0; list%
2ESince=null; $Path=/[\r][\n]""
2003/03/06 16:28:55:480 PST [DEBUG] wire - ->> ""Cookie: $Version=0; search%
2EPhoneSDA=null; $Path=/[\r][\n]""


  I'm not sure how to read this portion of wirelog, but seems that actual
values containing the string ""null"" are being sent as part of the request.
In the response to my request, the server now echos cookies with ""null""
values back to me.


2003/03/06 16:28:55:660 PST [DEBUG] wire - -<< ""Set-Cookie: search%
2EPhoneSDA=null; path=/[\r][\n]""
2003/03/06 16:28:55:660 PST [DEBUG] wire - -<< ""Set-Cookie: list%2ESince=null; 
path=/[\r][\n]""


  This isn't good.  Basically, the list.Since= cookie is being
converted to list.Since=null.  This causes the server's script to
crash:

<p>Microsoft VBScript runtime </font> <font face=""Arial"" 
size=2>error '800a000d'</font>
<p>
<font face=""Arial"" size=2>Type mismatch: 'CINT'</font>
<p>
<font face=""Arial"" size=2>/listCust.asp</font><font face=""Arial"" size=2>, line 
283</font> 


  I guess the script tries to assign the string ""null"" to an integer, and
dies.

Reported by Tom Samplonius <tom@sdf.com>"
1,"AutoCloseInputStream.available() throws IOException when auto-closedACIS auto-close itself as soon as EOF is detected. That leads to IOExceptions being throw in response to calls that are valid for a stream that has reached EOF.
ACIS should instead close the _underlying_ stream and switch itself into an EOF mode that does not throw exceptions until it is closed explicitly.

reported by Tom Lipkis on the developer mailing list
http://mail-archives.apache.org/mod_mbox/jakarta-httpcomponents-dev/200702.mbox/%3c200702101905.l1AJ5MeK027997@fw3.pss.com%3e"
0,"Feature Request: include contributed code for Plugin Proxy DetectionAttached is a zip file containing two classes - PluginProxyUtil and
ProxyDetectionException.  I've tested
PluginProxyUtil.detectProxy(URL) on Windows XP(JRE's 1.3/1.4/1.5 with IE) and
Solaris (JRE 1.4 with Netscape)
and it correctly detects browser plugin settings.  I don't have access to MacOS
X  to try it, but I doubt that
it works there anyway based on Dmitri's comments here:

http://forum.java.sun.com/thread.jspa?threadID=364342&tstart=120

Please change the header and package as necessary to include it in the contrib
section.  I plan to contribute
an example Applet that uses this code at some point - our app is way too
complicated to use as an example.
If you want to wait until that is done to include it, that's fine too.  Just
wanted to offer this up now in case
anyone else is looking for it."
1,"Header Connection Close - Closes the ConnectionIf the connectionHeader equals Close the conection imediateley closes, without 
waiting for the responce back from the server. 

If the client is pulling data from a CGI script which has not sent the Content 
Length - most servers will send a Connection Close header. For example 

-----------------------------------------
HTTP/1.1 200 OK
Date: Fri, 21 Jun 2002 17:08:46 GMT
Server: Apache/1.3.14 (Unix)
Connection: close
Content-Type: text/html
 
<html>
      <head>
            <title>thegumtree.com - London's online community for Aussies, Kiwis
 and South Africans</title>
                           <meta http-equiv=""Content-Type"" content=""text/html; c
harset=iso-8859-1"">
                   </head>

--------------------------------

I do not yet have a work arround apart from commenting out the the following 
code in 

Header connectionHeader = getResponseHeader(""connection""); etc

in HttpMethodBase"
1,"Occasional ""Host connection pool not found""I'm using HttpClient with MultiThreadedHttpConnectionManager in a crawler
application. The application issues requests to many hosts, in 10-20 parallel
request threads. Each thread creates a new GetMethod, but all threads use the
same instance of HttpClient, created once with a multi-threaded manager.

The code in each thread looks like this:

GetMethod get = new GetMethod(url);
try {
  int code = getSharedHttpClient().executeMethod(get);
  // ... read response, do stuff
} finally {
  get.releaseConnection();
}


From time to time I get an error like this:

Host connection pool not found, hostConfig=HostConfiguration[host=http://a.b.c]

where the url is a random url from my fetch list. I looked into the source code
of the nightly release (MultiThreadedHttpConnectionManager.java:979), but the
comment there is not enlightening... ;-) Any help or suggestions for further
debugging would be appreciated."
1,"Minor RFC 2109 / 2965 violationHi all,

we received this bug report for the debian commons-httpclient
package:

<debian_bugreport>
The following bug is present in upstream, 2.0.2 and 3.0RC3, at least as far
as I can tell by testing.

The specification grammar for the Cookie and Cookie2 HTTP headers
(specified by RFC 2109 section 4.3.4, and RFC 2965 section 3.3.4,
respectively) require that the ordering of pairs is ""Version, NAME, path,
domain"" (and, in RFC 2965, ""port"" after ""domain""). However, HTTPClient
produces a cookie string with the domain pair appearing before, rather
than after, the path pair. The RFCs specifically *do not* use either the
grammar or the clarifying text (""can occur in any order"") that occurs in
the sections that define the Set-Cookie and Set-Cookie2 headers (4.2.2 and
3.2.2, respectively).

Since the sections in question do not, in fact, discuss the issue of pair
ordering in Set-Cookie/Set-Cookie2 at all (other than in using a grammar
that clearly expresses the requirement), and since the complimentary
header explicitly permits them to occur in any order, it seems likely
that HTTPClient is not the only client with this issue, and that most
servers will accomodate this situation (in fact, for it to have gone
unnoticed for this long, it seems likely that either I'm badly misreading
the specification, or no major server has a problem coping with this).
</debian_bugreport>

For your reference the debian bug number:
http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=329245

Regards,

Wolfgang"
1,"[PATCH] FilePart fails to send data on second call to sendWhen using a FilePart with the MultipartPostMethod and a server that requires 
authentication, the first call to FilePart.send() sends the data correctly and 
HttpClient receives an unauthorized response from the server.  If HttpClient is 
set to automatically handle authentication attempts it then attempts to send 
the FilePart again at which time the InputStream FilePart reads from is empty 
so it doesn't send any data.

Due to this, the data actually sent by HttpClient doesn't match the content 
length specified so the server continues to wait for the data and doesn't 
respond, leaving HttpClient to timeout while waiting for a response.

This occurs with the latest source from CVS as of 16 October 2002."
1,Stale connections are never detected when wireLog.isDebugEnabled() == trueWhen wireLog.isDebugEnabled() == true SessionInputBuffer is wrapped with LoggingSessionInputBuffer which doesn't implement EofSensor that AbstractHttpClientConnection.isStale() is relying on. This causes stale connections to be never detected and attempted to use.
0,"move class Scheme and friends to a separate packageWe currently have a recursive dependency between packages o.a.h.conn and o.a.h.conn.routing, because routing depends on Scheme/SchemeRegistry. While these classes are used throughout the connection management code, they are not really part of the connection management itself and would therefore fit nicely into a separate package.

preliminary list of classes and interfaces to move:
- Scheme
- SchemeRegistry
- SocketFactory
- LayeredSocketFactory
- PlainSocketFactory

suggested package name: o.a.h.conn.scheme

Package o.a.h.conn.ssl can stay where it is, it only changes its dependency from conn to the new package.

cheers,
  Roland
"
1,"CachingHttpClient leaks connections with stale-if-errorIf you are using the ""stale-if-error"" Cache-control header and CachingHttpClient decides to use a stale cached response it does not clean up the existing backend response.

This bug causes connections to leak from the connection pool each time the stale-if-error flow is executed.
"
1,"AbstractClientConnAdapter prone to concurrency issuesAbstractClientConnAdapter is currently prone to all sorts of concurrency issues. (1) Access to internal state is not properry synchronized making the class prone  to race conditions. Presently none of the instance variables is even declared volatile. (2) AbstractClientConnAdapter treats aborted connection as one in an illegal state, which is not quite right.

Oleg"
1,"Parsing expiresSeeing this very often:

 Invalid cookie header: ""Set-Cookie: _asid=011e7014f5e7718e02d893335aa5a16e; path=/; expires=Wed, 16 May 2018 17:13:32 GMT"". Unable to parse expires attribute: Wed, 16 May 2018 17:13:32 GMT"
1,"DateUtil.formatDate() uses default timezone instead of GMTDateUtil.formatDate() uses default timezone instead of GMT.  In section 3.3.1,
RFC 2616 states:  ""All HTTP date/time stamps MUST be represented in Greenwich
Mean Time (GMT), without exception.""

To reproduce, run the following snippet:

   public static void main(String[] args) {
      TimeZone tz = TimeZone.getTimeZone(""GMT"");
      GregorianCalendar gc = new GregorianCalendar(tz);
      gc.set(1900 + 104, GregorianCalendar.JANUARY, 1, 0, 0, 0);
      System.out.println(DateUtil.formatDate(gc.getTime()));
      
   }

Expected result:
Thu, 01 Jan 2004 00:00:00 GMT

Actual result (if your default timezone is PST):
Wed, 31 Dec 2003 16:00:00 PST"
0,"Move to the new URIUtil classDepricate httpclient.URIUtil class and methods
Move all URIUtil calls to httpclient.util.URIUtil"
0,"HttpClientUtils  - Helper methods to release resources of HttpClient / HttpResponse after use Found myself writing this boiler plate code in various httpclient related projects , to release resources , and wanted to provide a simpler way to release resources similar to IOUtils.closeQuietly as opposed to maintaining the same in my code. 

New class: 

o.a.http.client.utils.HttpClientUtils added: 
<pre>
  public static void closeQuietly(final HttpResponse response); 
  public static void closeQuietly(final HttpClient httpClient); 
</pre>

with 2 methods ( as above ) in the same, to help release resources: 
"
0,"migrate to commons-codec Base64Commons Codec is now the authoritative source for Base64 functionality.  The
Base64 in HttpClient is now deprecated and should be removed in 2.1.  This will
also add a new dependancy for HttpClient on the commons-codec package."
0,"Set-Cookie2 and Set-CookieAcording to RFC2965 9.1:
                                                 User agents that
   receive in the same response both a Set-Cookie and Set-Cookie2
   response header for the same cookie MUST discard the Set-Cookie
   information and use only the Set-Cookie2 information.

this is read that the header for a cetain cookie, but not all cookie.
So, Server can send only Set-Cookie header for some cookies and,
for cookies send Set-Cookie2,Set-cookie both.

But httpclient implementation handles this that if find any set-cookie2 header,
then ignores all Set-cookie header.
I know some sites use set-cookie2 only for cookies which needs
 more flexible exiration handling, and for other cookies use only
Set-Cookie. One of exmaples of such sites I know is 
TDNet Database service provided by Tokyo Stock Exchange.

So, the preferred implementation is that if set-cookie2 header
 found for a certain cookie then cookie value is set from set-cookie2 header, if
not, then from Set-Cookie header."
1,"Digest auth uses incorrect URIThis bug seems to be strongly related to #36918. But in this case, I don't have 
proxy and it is GET method.

The problem is that when a GET request with query parameters is sent to the 
server, uri does not count in the parameters.  The server (Apache 1.3) then 
responds with HTTP/1.1 400 Bad Request. In the error log, the server writes:  
Digest: uri mismatch - </query.cgi> does not match request-uri 
</query.cgi?format=advanced&js=1&rememberjs=1>

As far as I can tell, the problem is with the following line of code:
------------ cut DigestScheme.java
public String authenticate(Credentials credentials, HttpMethod method)
....
getParameters().put(""uri"", method.getPath());
....
------------ cut

I am inclined to quick-hack this and add method.getQueryString() to the uri; 
but to make it right it probably needs some refactoring, esp. considering issue 
#36918."
0,"Should USE_EXPECT_CONTINUE be false by default?It seems the point of USE_EXPECT_CONTINUE is to improve performance when posting large data. 
http://www.w3.org/Protocols/rfc2616/rfc2616-sec8.html says:

<< The purpose of the 100 (Continue) status (see section 10.1.1) is to allow a client that is sending a request message with a request body to determine if the origin server is willing to accept the request (based on the request headers) before the client sends the request body. In some cases, it might either be inappropriate or highly inefficient for the client to send the body if the server will reject the message without looking at the body. >>

There's nothing wrong with HttpClient performing well by default, however, every other HTTP client library I've used does not behave like this (PHP curl, Perl LWP). The default is always to do one request, including the body. Maybe dumb, but simple.

It seems to me HttpClient's default behavior should the simplest, most compatible with all HTTP-speaking services out there. ""100 Continue"" is somewhat advanced, and may not be correctly implemented by all services. (That's of course how I found out about it -- my server doesn't implement it.)

If USE_EXPECT_CONTINUE is used only for performance reasons, it seems like it would be simpler (and therefore maybe more ""correct"") to have it ""off"" by default. And only enable it when needed, when there is a good reason to.

Just my thoughts. And a wish. Thanks! 


"
1,"DefaultClientConnectionOperator doesn't update socket after call to connectSocket(...)In the DefaultClientConnectionOperator function openConnection(...) it calls SocketFactory.connectSocket(...). The documentation for connectSocket(...) says that it returns:
   ""the connected socket. The returned object may be different from
the sock argument if this factory supports a layered protocol. ""

A quick peek at the source showed:
In org.apache.http.impl.conn.DefaultClientConnectionOperator:

117         final SocketFactory sf = schm.getSocketFactory();
118
119         Socket sock = sf.createSocket();
120         conn.opening(sock, target);
121
122         try {
123             sock = sf.connectSocket(sock, target.getHostName(),
124                     schm.resolvePort(target.getPort()),
125                     local, 0, params);
126         } catch (ConnectException ex) {
127             throw new HttpHostConnectException(target, ex);
128         }
129         prepareSocket(sock, context, params);
130         conn.openCompleted(sf.isSecure(sock), params);

So DefaultClientConnectionOperator never updates conn with the new version of sock that may have been returned from connectSocket(...).

adding:
        130         conn.openCompleted(sf.isSecure(sock), params);
+++ 131         conn.update(sock, target, sf.isSecure(sock), params);
appears to fix the issue.
"
1,"HttpMethodBase(String) incorrectly encoding URIThe HttpMethodBase(String) constructor is handling URIs incorrectly. The
Javadocs indicate that the given URI should already be escaped but the
constructor uses the URI constructor for unescaped URIs."
0,Cookie.java: 'bookean' typo 
0,"entities and connection handling incomplete1. entities can not be explicitly disconnected from the underlying stream
2. entities do not tell whether they have an underlying stream

patch follows

cheers,
  Roland"
1,"Bad request vulnerability The HttpParser.readRawLine() method below has no guard code against a post without a end-of-line.  A large post of data without ""\n"" will be read into the ByteArray.  If this post is large enough, it will deplete the system of free memory.  A DOS attack could easily be played out by submitting several of these post at once.   readRawLine should decide that its not reading character data (basically because character data should never show up over something like a megabyte a line) and report an error.  

   /**
     * Return byte array from an (unchunked) input stream.
     * Stop reading when <tt>""\n""</tt> terminator encountered 
     * If the stream ends before the line terminator is found,
     * the last part of the string will still be returned. 
     * If no input data available, <code>null</code> is returned.
     *
     * @param inputStream the stream to read from
     *
     * @throws IOException if an I/O problem occurs
     * @return a byte array from the stream
     */
    public static byte[] readRawLine(InputStream inputStream) throws IOException {
        LOG.trace(""enter HttpParser.readRawLine()"");

        ByteArrayOutputStream buf = new ByteArrayOutputStream();
        int ch;
        while ((ch = inputStream.read()) >= 0) {
            buf.write(ch);
            if (ch == '\n') { // be tolerant (RFC-2616 Section 19.3)
                break;
            }
        }
        if (buf.size() == 0) {
            return null;
        }
        return buf.toByteArray();
    }"
0,"User/Developer Documentation- quotes from user's on why they used it.
- better project docs, including checkstyle and clover reports, and changelog
- examples showing how to configure logging and why you may want to
- Links on HttpClient to 'sister' projects such as Slide, Cactus and Latka to
show where how it's being used."
1,"Failed CONNECT leaves connection in an inconsistent stateOpening a HTTPS Connection over an authenticating Proxy (Basic auth. scheme) 
fails, if proxy credentials are not provided at the first try. 

The following example code will fail:

HttpClient client = new HttpClient(new MultiThreadedHttpConnectionManager());
URL url = new URL(""https://examplehttpsurl"");
  
//first try 
GetMethod get = new GetMethod(url.toExternalForm());
HostConfiguration hc = new HostConfiguration();
hc.setHost(url.getHost(), 443, ""https"");
hc.setProxy(""proxyhost"", 4711);

try {
  client.executeMethod(hc, get);
} catch (Exception e){
  LOG.error("""",e);
} finally {
  get.releaseConnection();
}

//returns 407 (expected)
LOG.debug(""Answer: "" + get.getStatusLine().toString()); 

//retry with credentials (normally requested from the user)
client.getState().setProxyCredentials(new AuthScope(""proxyhost"",4711),
      new NTCredentials(""USER"", ""PASS"", """", """"));

get = new GetMethod(url.toExternalForm());

try {
  client.executeMethod(hc, get);
} catch (Exception e) {
  e.printStackTrace();
} finally {
  get.releaseConnection();
}
//should be 200 but is 407
LOG.debug(""Answer: "" + get.getStatusLine().toString());



----------


From what I see from HttpMethodDirector.executeWithRetry(final
HttpMethod method), the cause is, that the connection is kept open, and
thus the connect is never retried:


if (!this.conn.isOpen()) {
  // this connection must be opened before it can be used
  // This has nothing to do with opening a secure tunnel
  this.conn.open();
  if (this.conn.isProxied() && this.conn.isSecure() 
      && !(method instanceof ConnectMethod)) {
    // we need to create a secure tunnel before we can execute the real method
    if (!executeConnect()) {
      // abort, the connect method failed
      return;
    }
  }
}


If I add a conn.close() before returning on !executeConnect(), the
above code will work, the CONNECT is reattempted."
0,"Disallow the use of SecureProtocolSocketFactory with ProxyClientProxyClient cannot work correctly if SecureProtocolSocketFactory socket factory
is being used to establish connection with the target server"
1,"Error releasing chunked connections with no response body.HttpMethodBase.releaseConnection() does not successfully release the connection
if closing the response stream throws an exception."
1,"Charset omitted from UrlEncodedFormEntity Content-Type headerUrlEncodedFormEntity sets the Content-Type header to:
   ""application/x-www-form-urlencoded""

It should set the header to:
   ""application/x-www-form-urlencoded; charset="" + charset

As a result, content can be misinterpreted by the recipient (e.g. if the entity content includes multibyte Unicode characters encoded with the ""UTF-8"" charset).

For a correct example of specifying the charset in the Content-Type header, see StringEntity.java.

Here's the fix:

    public UrlEncodedFormEntity (
        final List <? extends NameValuePair> parameters, 
        final String encoding) throws UnsupportedEncodingException {
        super(URLEncodedUtils.format(parameters, encoding),  encoding);
-        setContentType(URLEncodedUtils.CONTENT_TYPE);
+        setContentType(URLEncodedUtils.CONTENT_TYPE + HTTP.CHARSET_PARAM +
+            (encoding != null ? encoding : HTTP.DEFAULT_CONTENT_CHARSET));
    }

    public UrlEncodedFormEntity (
        final List <? extends NameValuePair> parameters) throws UnsupportedEncodingException {
-        super(URLEncodedUtils.format(parameters, HTTP.DEFAULT_CONTENT_CHARSET), 
-            HTTP.DEFAULT_CONTENT_CHARSET);
-        setContentType(URLEncodedUtils.CONTENT_TYPE);
+        this(parameters, HTTP.DEFAULT_CONTENT_CHARSET);
    }
"
1,"Authentication fails when connecting to server with username and password in non ascii charactersTried connecting to an exchange server using NTLM authentication
Username : íñëäíñëä
password : íñëäíñëä

I am getting 401 response.
Auth failed
Body: 
Error: Access is Denied."
0,"Allow redirects between hosts and portsRedirects to different hosts, ports and protocols are currently prevented. 
Historicly, HttpMethodBase.checkValidRedirects() is used to prevent these types
of redirects due how state information was being managed in the connection.  

Much has changed since then.  We should relax the check and allow for redirects
 between hosts and ports. 

Redirects across protocols should not be considered at this time as there are
other issues related to security that is best left up to the user of HttpClient."
1,"NPE in SimpleHttpConnectionManager.shutdown()SimpleHttpConnectionManager.shutdown() causes NPE if no connection has been created, whereas MultiThreadedHttpConnectionManager.shutdown() does not.

Simple test case:

	MultiThreadedHttpConnectionManager cm = new MultiThreadedHttpConnectionManager();
	cm.shutdown(); // OK
		
	SimpleHttpConnectionManager sm = new SimpleHttpConnectionManager();
	sm.shutdown(); // NPE


I came across this in JMeter - a sample was using Post with AutoRedirect, which (correctly) caused an IllegalArgumentException, and so the connection was not created. 

The JMeter code could try to keep track of this, but it would be tedious, and it seems to me that SimpleHttpConnectionManager should ignore the shutdown() if the connection is null.

The problem does not arise when using closeIdleConnections(timeout) - unless one uses the special value:

      closeIdleConnections(System.currentTimeMillis() - Long.MAX_VALUE)

but it would probably be sensible to protect against this as well."
0,"Do not consume the remaining response content if the connection is to be closedI am working on a HttpClient-based application to send and receive potentially large files (up to Gigabytes). When receiving large files the application allows the user to cancel the download, at which time it closes the response input stream behind the scenes.

The input stream currently provided by HttpMethodBase.getResponseBody() for un-chunked responses with a known content length is a ContentLengthInputStream, which automatically reads the remainder of the wrapped response instead of closing it straight away. This behaviour does not work well with very large files as the data is downloaded unnecessarily and the connection is held open for long very periods.

Per the HTTP 1.1 spec section 14.10 it seems to me that either a server or a client in an HTTP 1.1 connection can use the Connection:close directive to signal that a connection will be non-persistent, and will therefore not require that all data be read before the connection can be released (the cleaning up ContentLengthInputStream performs for persistent connections).

http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.10

Could HttpMethodBase be modified to check for this directive, from the server or client, and avoid wrapping the response input stream in ContentLengthInputStream when it is present? It seems straight-forward, though there may be side-effects I am not aware of. 
"
0,"RandomAccessFile mode ""w"" is not validAccording to the Java docs for RandomAccessFile, mode must be ""r"" ""rw"" ""rws"" or ""rwd"" - anything else results in an IllegalArgumentException. It seems that Sun/Oracle/OpenJDK's don't document it, but supports ""w"" mode that is equivalent to ""rw"" Android does as the Javadocs say, and throws an IllegalArgumentException when mode ""w"" is passed as HttpClientCache does IOUtils.copyFile() (line 70-71).

This means that HttpClient Cache does not work on Android."
0,"Disentangle commons-httpclient from commons in GumpCurrently, the Gump project for HttpClient 3.x is defined as one of the commons projects.
It should be moved to a separate definition, either all by itself or as a new group of HttpComponents Gump projects.
"
0,"Allow heuristic freshness cachingI noticed that the CachingHttpClient behaves strangely when it receives responses with only the public cache-control directive, e.g.:

HTTP/1.0 200 OK
Server: My test server
Cache-control: public
Content-Length: 1

1


Using a debugger, I could see that the response is cached. But when the response is queried from the cache, it is not considered as ""fresh"".
According to the HTTP RFC, such responses ""may"" be cached (I understand it as a ""should"" in our case)... but there's no reason to put responses in the cache if we don't use them later one.

The ""freshness of the response is analysed after the response is queried from the cache, thanks to:
CachedResponseSuitabilityChecker#canCachedResponseBeUsed()
... calling CacheEntry#isResponseFresh()
... returning true if the response date (getCurrentAgeSecs()) is lower than its use-by date (getFreshnessLifetimeSecs())

The issue is that getFreshnessLifetimeSecs() returns 0 when there is no max-age directive.

This could be fixed by replacing the code of CacheEntry#isResponseFresh() by:
    public boolean isResponseFresh() {
        final long freshnessLifetime = getFreshnessLifetimeSecs();
        if (freshnessLifetime == 0) {
            return true;
        }
        return (getCurrentAgeSecs() < getFreshnessLifetimeSecs());
    }

But i'm not 100% confident about not producing some bad side-effects..."
1,"Unencoded redirect URI causes exception when following redirectsWhen HttpClient is set to follow redirects, the DefaultRedirectHandler gets the redirect location from the appropriate request header and attempts to create a new java.net.URI from it. If the location contains an invalid URI character, creating the URI fails. For example, if the redirect location were ""/foo?bar=<baz/>"", it would fail because the '<' and '>' are not legal in a URI.

I'm not sure if this should actually be considered a bug in HttpClient, since the website in question should probably be responsible for encoding the URI appropriately; however, browsers handle the situation gracefully, and it would be nice if this excellent library would do so as well."
0,"RequestEntity, EntityEnclosingMethod have inconsistent Javadocs, use deprecated variablesRobert Manning <Robert.Manning at collabraspace.com> reported a problem on the
httpclient-user list regarding inconsistencies in javadoc of RequestEntity
interface and EntityEnclosingMethod class. This prompted me to review the said
classes. I have discovered several issues that must be dealt with before 3.0
goes final. 

(1) There's virtually no test coverage for the EntityEnclosingMethod class
(2) The code in EntityEnclosingMethod class extensively uses deprecated methods
and variables beyond what is required to maintain backward compatibility with
2.0.x API
(3) Existing code cannot gracefully handle faulty RequestEntity implementations
if the getContentLength method returns a negative value < -2

I have committed additional test cases to cover the most fundamental
functionality of EntityEnclosingMethod:
http://svn.apache.org/repos/asf/jakarta/commons/proper/httpclient/trunk/src/test/org/apache/commons/httpclient/TestEntityEnclosingMethod.java

I will submit a patch addressing issues (2) and (3) shortly

Oleg"
1,"RequestWrapper does not use the headers of the request it wraps.The RequestWrapper does not use the headers of the request it wraps. Therefore the wrapper appears as having no header, while the wrapped request may have some.

To work-around that behavior, I have to call resetHeaders() on the wrapper just after having created it.
This method does the following:
    public void resetHeaders()
    {
        headergroup.clear();
        setHeaders(original.getAllHeaders());
    }

I suggest calling setHeaders directly in the constructor. Or at least highlight in the Javadoc that we should call resetHeaders()."
0,"Replace HttpState with CredentialsProvier and CookieStore interfacesReplace HttpState, which is a concrete class, with CredentialsProvier and CookieStore interfaces. Provide default impls of those interfaces."
0,"Redesign virtual host APIHttpClient is ignoring an explicity set host.  e.g. if you set the host like
client.getHostConfiguration().setHost(""127.0.0.1"") then execute a method looking
up say http://google.com then the program will connect to google.com rather than
the localhost.

The fix that works for me:
diff -Naur
../../t2/commons-httpclient/src/java/org/apache/commons/httpclient/HttpClient.java
src/java/org/apache/commons/httpclient/HttpClient.java
---
../../t2/commons-httpclient/src/java/org/apache/commons/httpclient/HttpClient.java
2005-12-22 01:06:54.000000000 +1300
+++ src/java/org/apache/commons/httpclient/HttpClient.java	2005-12-22
19:13:30.000000000 +1300
@@ -383,7 +383,9 @@
         if (hostconfig == defaulthostconfig || uri.isAbsoluteURI()) {
             // make a deep copy of the host defaults
             hostconfig = new HostConfiguration(hostconfig);
-            if (uri.isAbsoluteURI()) {
+	    // if the host is explicity set already (e.g. to the IP of the virtual host
+	    // on which we are executing a method), just leave it
+            if (uri.isAbsoluteURI()  && hostconfig.getHost()==null) {
                 hostconfig.setHost(uri);
             }

Note: Why do we care that the host is specified?  Why not just use the uri
authority?  In my case I have a virtual host running on several servers/IPs and
I need to make sure the request goes through to a specific IP and the response
that comes back is for the virtual host I am testing."
0,"Built-in way to do auto-retry for certain status codesThe HttpRequestRetryHandler mechanism is great.  It allows API users to plug in their own logic to control whether or not a retry should automatically be done, how many times it should be retried, etc.  That works perfectly in scenarios where an exception is caught while issuing the request.  It falls short, however, in this scenario...

If I'm hitting a service that returns a 503, I want to be able to retry that request automatically as well.  As of right now, I need to write my own logic to accomplish that, and it's clunky trying to integrate it with the httpClient.execute() call, since it's my ResponseHandler impl that ends up getting the 503.  I can see use cases for auto-retrying upon getting other HTTP statuses as well, not just 503.

My request here is...I would love to be able to configure, either on the HttpClient itself or on a wrapping class or something, that a request should automatically be retried if the HTTP status code is among of a set of statuses that I configure.  It would be nice if you could set the max # of retries, an optional sleep time in between retries (perhaps optional incremental backoff if you want to get fancy).  I'm not sure if this is possible, but it would be nice if -- when this type of status-based retry is enabled -- my ResponseHandler wouldn't even get invoked until retry was successful.

Here's an alternative suggestion, possibly simpler to build, but definitely not as elegant:

In my ResponseHandler, you could throw a RetryRequestException or something like that, and the calling code would catch that and do as expected.  That might simplify the mechanism so to speak.

Anyway, I would love not to have to roll my own retry code, since I suspect this is something that hundreds (thousands?) of HttpClient users have had to code.  Seems like providing a standardized, well-written way to do it would go a long way to helping many coders out there.

Thanks!"
1,"HttpMethodDirector.executeWithRetry method fails to close the underlying connection if a RuntimeException is thrownThe following code snippet is from the end of the HttpMethodDirector.executeWithRetry method:

        } catch (IOException e) {
            if (this.conn.isOpen()) {
                LOG.debug(""Closing the connection."");
                this.conn.close();
            }
            releaseConnection = true;
            throw e;
        } catch (RuntimeException e) {
            if (this.conn.isOpen) {                                             <<<===========================   BAD!  :-)
                LOG.debug(""Closing the connection."");
                this.conn.close();
            }
            releaseConnection = true;
            throw e;
        }

When an IOException is caught, you can see that the ""open"" status of the connection is accurately checked by calling the ""isOpen()"" method.

When a RuntimeException is caught, however, the code mistakenly checks only the ""isOpen"" member field.  In the case where ""conn"" is, for example, a MultiThreadedHttpConnectionManager, the ""isOpen()"" method is overridden to check for a wrapped connection and returns the ""isOpen"" status of that connection.  In cases like that checking the ""isOpen"" member field is obviously wrong and we end up not calling ""close()"" and the connection is not cleaned up.  This causes issues with later calls.

A very difficult bug to diagnose and <steps up on soapbox> one that could have been easily avoided by making member variables private! <steps down>  thank you.  :-)


"
0,"javadoc often has <code> without </code>Just to mention it:
there are a lot of javadoc comments that read like
 <tt>false</ff> otherwise.
which prints the rest of the document in <tt>
can't give a complete list, it happens very often.
To check, just go to the bottom of a page and see if it appears in <tt> or 
<code>"
0,need getResponseContentLength in HttpMethodWe need a way to find out the response content length in HttpMethod
0,Add the ability to disable the content-type and transfer encoding headers for Parts 
1,CachingHttpClient.execute() does not catch the IOException thrown by HttpCache.getCacheEntry()The IOException caused by the HttpCache is not caught and thus the whole http request fails. I would expect the response to be retrieved from the backend when the cache fails for some reason.
0,"commons codec not documented as a dependencyExcept for some entries on the jdepend-report, commons codec is not documented
as a dependency for using HttpClient 3.0 RC1.  The only dependency documented is
commons logging.

java.lang.NoClassDefFoundError: org/apache/commons/codec/DecoderException
	at org.apache.commons.httpclient.HttpMethodBase.<init>(HttpMethodBase.java:217)
	at
org.apache.commons.httpclient.methods.ExpectContinueMethod.<init>(ExpectContinueMethod.java:92)
	at
org.apache.commons.httpclient.methods.EntityEnclosingMethod.<init>(EntityEnclosingMethod.java:114)
	at org.apache.commons.httpclient.methods.PostMethod.<init>(PostMethod.java:105)"
0,"BasicAuthenticatonExample.java in CVSFrom the example in CVS Revision 1.1.2.1 the code below would lead you to 
belive that setCredentials uses (""HOST"", ""REALM"", credientials). It actually 
should be (""REALM"", ""HOST"", credientials). It looks like it was correct in the 
Revision 1.1 and was changed with Revision 1.1.2.1

// pass our credentials to HttpClient, they will only be used for
// authenticating to servers with realm ""realm"", to authenticate agains
// an arbitrary realm change this to null.
client.getState().setCredentials(
            ""www.verisign.com"",
            ""realm"",
            new UsernamePasswordCredentials(""username"", ""password"")
);"
1,"Socket streams are closed in the incorrect order.HttpConnection should close the streams/socket in the following order:

OutputStream
InputStream
Socket

<http://java.sun.com/docs/books/tutorial/networking/sockets/readingWriting.html>"
0,"Cookie Documentation clarificationsThe JavaDoc for CookieSpec mentions that the default policy is RFC2109 - it
would be nice if this was mentioned on
http://jakarta.apache.org/commons/httpclient/cookies.html as well
(likewise for 2.0)

The Javadoc for getDefaultPolicy() says to use getCookieSpec(String); it would
be better to say to use getCookieSpec(DEFAULT), and it would be helpful to
mention getDefaultSpec().

CookiePolicy Javadoc does not mention the IGNORE_COOKIES policy in the header
documentation.

The cookies.html page mentions automatic and manual handling of cookies, but
does not provide any links as to how to control these. For example, how does one
turn off automatic cookie handling?"
1,"HttpGet request not being created when parameter ""url"" is present
/*
* @formatter:off
* 
* The following redirect Location results in a Bad Request (404) being made.
* 
* http://www.qpassport.co.uk/passport/register.php?do=signup&who=adult&url=http%3A%2F%2Fwww.qpassport.co.uk%2Fpassport%2F&month=2&year=1947&day=26
* 
* The GET request is made with these headers (Notice the ""Host"" value):
* 
* DEBUG org.apache.http.wire  - >> ""GET http://www.qpassport.co.uk/passport/register.php?do=signup&who=adult&url=http%3A%2F%2Fwww.qpassport.co.uk%2Fpassport%2F&month=2&year=1947&day=26 HTTP/1.1[\r][\n]""
* DEBUG org.apache.http.wire  - >> ""Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*//*;q=0.8[\r][\n]""
* DEBUG org.apache.http.wire  - >> ""Accept-Language: en-us,en;q=0.5[\r][\n]""
* DEBUG org.apache.http.wire  - >> ""Accept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7[\r][\n]""
* DEBUG org.apache.http.wire  - >> ""Proxy-Connection: Keep-Alive[\r][\n]""
* DEBUG org.apache.http.wire  - >> ""Referer: http://www.qpassport.co.uk/passport/register.php?s=b9761dfa820bb55722e3feb6438fa11f&[\r][\n]""
* DEBUG org.apache.http.wire  - >> ""Host: www.qpassport.co.uk/passport/register.php?do=signup&who=adult&url=http[\r][\n]""
* DEBUG org.apache.http.wire  - >> ""User-Agent: Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)[\r][\n]""
*  
* They should be: (Notice the ""Host"" value)
* DEBUG org.apache.http.wire  - >> ""GET http://www.qpassport.co.uk/passport/register.php?do=signup&who=adult&month=2&year=1947&day=26 HTTP/1.1[\r][\n]""
* DEBUG org.apache.http.headers  - >> Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*//*;q=0.8
* DEBUG org.apache.http.headers  - >> Accept-Language: en-us,en;q=0.5
* DEBUG org.apache.http.headers  - >> Accept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7
* DEBUG org.apache.http.headers  - >> Proxy-Connection: Keep-Alive
* DEBUG org.apache.http.headers  - >> Referer: http://www.qpassport.co.uk/passport/register.php?s=005ef80064a0fb2f5aa6f4677a194928&
* DEBUG org.apache.http.headers  - >> Content-Length: 134
* DEBUG org.apache.http.headers  - >> Content-Type: application/x-www-form-urlencoded; charset=UTF-8
* DEBUG org.apache.http.headers  - >> Host: www.qpassport.co.uk
* DEBUG org.apache.http.headers  - >> User-Agent: Opera/9.20 (Windows NT 6.0; U; en)
* 
* The problem appears to be related to the URL parameter in the request
* when it is removed, the request succeeds.
* 
* @formatter:on
*/
"
0,"Move to commons-loggingCommons-logging was derived from httpclient.log, still using the old logging
which should be removed.  Some complaints on mailing list about setting up
commons-logging: should be simple and well documented."
1,"Problem getting the HTTPClient to use HTTP 1.0 with a proxy serverI am using HTTPClient 3.0-rc1.
I am connecting to an HTTPS site through a proxy.

I used HTTPLook to see the HTTP messages between the Proxy and the HTTPClient.
I noticed that it always used HTTP/1.1 and setVersion() on either the 
httpclient and the method do not help. I could not find how to get 
the HTTPClient to use HTTP/1.0 with the proxy.

Looking at the ConnectMethod class, the HTTP1.1 was indeed hardcoded.

Thanks
riad"
0,"web site for download of 3.1 failsDown loading version 3 is not possible.
Documentation for 4. does not match 3."
0,"client cache currently allows incomplete responses to be passed on to the clientPer the HTTP/1.1 spec:

""A cache that receives an incomplete response (for example, with fewer bytes of data than specified in a Content-Length header) MAY store the response. However, the cache MUST treat this as a partial response. Partial responses MAY be combined as described in section 13.5.4; the result might be a full response or might still be partial. A cache MUST NOT return a partial response to a client without explicitly marking it as such, using the 206 (Partial Content) status code. A cache MUST NOT return a partial response using a status code of 200 (OK).""

(http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.8)

For example, if a 200 response shows up with 128 bytes in the body but a Content-Length header of 256, the cache MUST NOT pass this through unchanged.
"
1,"[PATCH]multiple wildcards ? at the end of search pattern return incorrect hitsThe problem is if you search on ""ca??"", the hit includes 'cat', 'CA', 
etc, while the user only wants 4 letter words start with CA, such as 
'card', 'cash', to be returned. This happens only when multiple '?' at 
the end of search pattern. The solution is to check if the word that is 
matching against search pattern ends while there is still '?' left. If 
this is the case, match should return false.

Attached is the patch code I generated use 'diff'
********************************************************************

--- WildcardTermEnum.org	2004-05-11 11:42:10.000000000 -0400
+++ WildcardTermEnum.java	2004-11-08 14:35:14.823610500 -0500
@@ -132,6 +132,10 @@
             }
             else
             {
+	      //to prevent ""cat"" matches ""ca??""
+	      if(wildchar == WILDCARD_CHAR){
+		return false;
+	      }	      
               // Look at the next character
               wildcardSearchPos++;
             }
**********************************************************************"
1,"if you open an NRT reader while addIndexes* is running it may miss segmentsEarwin spotted this in pending ongoing refactoring of Dir/MultiReader, but I wanted to open this separately just to make sure we fix it for 3.1...

This is the fix:
{code}
Index: src/java/org/apache/lucene/index/DirectoryReader.java
===================================================================
--- src/java/org/apache/lucene/index/DirectoryReader.java	(revision 919119)
+++ src/java/org/apache/lucene/index/DirectoryReader.java	(working copy)
@@ -145,7 +145,7 @@
     for (int i=0;i<numSegments;i++) {
       boolean success = false;
       try {
-        final SegmentInfo info = infos.info(upto);
+        final SegmentInfo info = infos.info(i);
         if (info.dir == dir) {
           readers[upto++] = writer.readerPool.getReadOnlyClone(info, true, termInfosIndexDivisor);
         }
{code}

"
0,"Add an ""termInfosIndexDivisor"" to IndexReaderThe termIndexInterval, set during indexing time, let's you tradeoff
how much RAM is used by a reader to load the indexed terms vs cost of
seeking to the specific term you want to load.

But the downside is you must set it at indexing time.

This issue adds an indexDivisor to TermInfosReader so that on opening
a reader you could further sub-sample the the termIndexInterval to use
less RAM.  EG a setting of 2 means every 2 * termIndexInterval is
loaded into RAM.

This is particularly useful if your index has a great many terms (eg
you accidentally indexed binary terms).

Spinoff from this thread:

  http://www.gossamer-threads.com/lists/lucene/java-dev/54371

"
0,"Detect the test thread by reference, not by name.Get rid of this:
{code}
      if (doFail && (Thread.currentThread().getName().equals(""main"") 
          || Thread.currentThread().getName().equals(""Main Thread""))) {
{code}"
1,"Highlighter throws StringIndexOutOfBoundsExceptionUsing the canonical Solr example (ant run-example) I added this document (using exampledocs/post.sh):

<add><doc>
  <field name=""id"">Test for Highlighting StringIndexOutOfBoundsExcdption</field>
  <field name=""name"">Some Name</field>
  <field name=""manu"">Acme, Inc.</field>
  <field name=""features"">Description of the features, mentioning various things</field>
  <field name=""features"">Features also is multivalued</field>
  <field name=""popularity"">6</field>
  <field name=""inStock"">true</field>
</doc></add>

and then the URL http://localhost:8983/solr/select/?q=features&hl=true&hl.fl=features caused the exception.

I have a patch.  I don't know if it is completely correct, but it avoids this exception.
"
0,"Improve payload error handling/reportingIf you try to load a payload more than once you get the exception:  IOException(""Payload cannot be loaded more than once for the same term position."");

You also get this exception if their is no payload to load, and its a bit confusing, as the message doesn't relate to the actual problem."
0,"Analysis Package Level JavadocsAnalysis package level javadocs need improving.  An overview of what an Analyzer does, and maybe some sample code showing how to write you own Analyzer, Tokenizer and TokenFilter would be really helpful.  Bonus would be some discussion on best practices for achieving performance during analysis. "
0,"NativeFSLockFactory throws an exception on Android 2.2 platform as java.lang.management package is not available on android.NativeFSLockFactory throws an exception on Android 2.2 platform as java.lang.management package is not available on android.
   Looks like FSDirectory defaults to NativeFSLockFactory, and this class refers to java.lang.management package to generate a unique lock. java.lang.management is not available in Android 2.2 and hence a runtime exception is raised. The workaround is to use another custom LockFactory or SimpleFSLockFactory, but Fixing NativeFSLockFactroy will help.

Thanks,
Surinder"
0,"Lucene Search has poor cpu utilization on a 4-CPU machineI noticed that the class org.apache.lucene.index.FieldInfos uses private class
members Vector byNumber and Hashtable byName, both of which are synchronized
objects, thus resulting in unessesary locks. 

By changing the Vector byNumber to ArrayList byNumber, and Hashtable byName to
HashMap byName, both are not synchronized objects, I was able to get 110%
improvement in performance (number of searches per second).


Here is a sample of blocked thread
""Thread-32"" daemon prio=1 tid=0x082334c0 nid=0xa66 waiting for monitor entry
[4f385000..4f38687c]
        at java.util.Vector.elementAt(Vector.java:430)
        - waiting to lock <0x452b93a8> (a java.util.Vector)
        at org.apache.lucene.index.FieldInfos.fieldInfo(FieldInfos.java:155)
        at org.apache.lucene.index.FieldInfos.fieldName(FieldInfos.java:151)
        at
org.apache.lucene.index.SegmentTermEnum.readTerm(SegmentTermEnum.java:149)
        at org.apache.lucene.index.SegmentTermEnum.next(SegmentTermEnum.java:115)
        at
org.apache.lucene.index.TermInfosReader.scanEnum(TermInfosReader.java:143)
        at org.apache.lucene.index.TermInfosReader.get(TermInfosReader.java:137)
        at org.apache.lucene.index.SegmentTermDocs.seek(SegmentTermDocs.java:51)
        at org.apache.lucene.index.IndexReader.termDocs(IndexReader.java:364)
        at org.apache.lucene.search.TermQuery$TermWeight.scorer(TermQuery.java:59)
        at
org.apache.lucene.search.BooleanQuery$BooleanWeight.scorer(BooleanQuery.java:165)
        at
org.apache.lucene.search.BooleanQuery$BooleanWeight.scorer(BooleanQuery.java:165)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:154)
        at
gov.gsa.search.SearcherByPageAndSortedField.search(SearcherByPageAndSortedField.java:317)
        at
gov.gsa.search.SearcherByPageAndSortedField.search(SearcherByPageAndSortedField.java:203)
        at
gov.gsa.search.grants.SearchGrants.searchByPageAndSortedField(SearchGrants.java:308)
        at
gov.gsa.search.grants.SearchServlet.searchByIndex(SearchServlet.java:1541)
        at gov.gsa.search.grants.SearchServlet.getResults(SearchServlet.java:1325)
        at gov.gsa.search.grants.SearchServlet.doGet(SearchServlet.java:500)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:740)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:853)
        at
org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:247)
        at
org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:193)
        at
org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:256)
        at
org.apache.catalina.core.StandardPipeline$StandardPipelineValveContext.invokeNext(StandardPipeline.java:643)
        at
org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:480)
        at org.apache.catalina.core.ContainerBase.invoke(ContainerBase.java:995)
        at
org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)
        at
org.apache.catalina.core.StandardPipeline$StandardPipelineValveContext.invokeNext(StandardPipeline.java:643)
        at
org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:480)
        at org.apache.catalina.core.ContainerBase.invoke(ContainerBase.java:995)
        at
org.apache.catalina.core.StandardContext.invoke(StandardContext.java:2415)
        at
org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:180)
        at
org.apache.catalina.core.StandardPipeline$StandardPipelineValveContext.invokeNext(StandardPipeline.java:643)
        at
org.apache.catalina.valves.ErrorDispatcherValve.invoke(ErrorDispatcherValve.java:171)
        at
org.apache.catalina.core.StandardPipeline$StandardPipelineValveContext.invokeNext(StandardPipeline.java:641)
        at
org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:172)
        at
org.apache.catalina.core.StandardPipeline$StandardPipelineValveContext.invokeNext(StandardPipeline.java:641)
        at
org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:480)
        at org.apache.catalina.core.ContainerBase.invoke(ContainerBase.java:995)
        at
org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:174)
        at
org.apache.catalina.core.StandardPipeline$StandardPipelineValveContext.invokeNext(StandardPipeline.java:643)
        at
org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:480)
        at org.apache.catalina.core.ContainerBase.invoke(ContainerBase.java:995)
        at org.apache.coyote.tomcat4.CoyoteAdapter.service(CoyoteAdapter.java:223)
        at org.apache.jk.server.JkCoyoteHandler.invoke(JkCoyoteHandler.java:261)
        at org.apache.jk.common.HandlerRequest.invoke(HandlerRequest.java:360)
        at org.apache.jk.common.ChannelSocket.invoke(ChannelSocket.java:604)
        at
org.apache.jk.common.ChannelSocket.processConnection(ChannelSocket.java:562)
        at org.apache.jk.common.SocketConnection.runIt(ChannelSocket.java:679)
        at
org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadPool.java:619)
        at java.lang.Thread.run(Thread.java:534)"
0,"Caching does not work when using RMIFilters and caching uses transient maps so that caching does not work if you are using RMI and a remote searcher 

I want to add a new RemoteCachededFilter that will make sure that the caching is done on the remote searcher side 
 "
0,"Fix Document.getFieldables and others to never return nullDocument.getFieldables (and other similar methods) returns null if there are no fields matching the name.  We can avoid NPE in consumers of this API if instead we return an empty array.

Spinoff from http://markmail.org/message/g2nzstmce4cnf3zj"
0,"optimize lev automata constructionin our lev automata algorithm, we compute an upperbound of the maximum possible states (not the true number), and
create some ""useless"" unconnected states ""floating around"".

this isn't harmful, in the original impl we did the Automaton is simply a pointer to the initial state, and all algorithms
traverse this list, so effectively the useless states were dropped immediately. But recently we changed automaton to
cache its numberedStates, and we set them here, so these useless states are being kept around.

it has no impact on performance, but can be really confusing if you are debugging (e.g. toString). Thanks to Dawid Weiss
for noticing this. 

at the same time, forcing an extra traversal is a bit scary, so i did some benchmarking with really long strings and found
that actually its helpful to reduce() the number of transitions (typically cuts them in half) for these long strings, as it
speeds up some later algorithms. 

won't see any speedup for short terms, but I think its easier to work with these simpler automata anyway, and it eliminates
the confusion of seeing the redundant states without slowing anything down.
"
0,"Can't quickly create StopFilterDue to the use of CharArraySet by StopFilter, one can no longer efficiently pre-create a Set for use by future StopFilter instances."
0,"Packed ints: move .getArray into Reader APIThis is a simple code cleanup... it's messy that a consumer of
PackedInts.Reader must check whether the impl is Direct8/16/32/64 in
order to get an array; it's better to move up the .getArray into the
Reader interface and then make the DirectN impls package private.
"
1,"NearSpans skipTo bugNearSpans appears to have a bug in skipTo that causes it to skip over some matching documents completely.  I discovered this bug while investigating problems with SpanWeight.explain, but as far as I can tell the Bug is not specific to Explanations ... it seems like it could potentially result in incorrect matching in some situations where a SpanNearQuery is nested in another query such thatskipTo will be used ... I tried to create a high level test case to exploit the bug when searching, but i could not.  TestCase exploiting the class using NearSpan and SpanScorer will follow..."
1,"remove NativeFSLockFactory's attempt to acquire a test lockNativeFSLockFactory tries to acquire a test lock the first time a lock is created.  It's the only LF to do this, and, it's caused us hassle (LUCENE-2421,  LUCENE-2688).

I think we should just remove it.  The caller of .makeLock will presumably immediately thereafter acquire the lock and at the point hit any exception that acquireTestLock would've hit."
0,FSDirectory.open should return MMap on 64-bit SolarisMMap is ~ 30% faster than NIOFS on this platform.
1,EdgeNgrams creates invalid offsetsA user reported this because it was causing his highlighting to throw an error.
0,"ComparatorKey in Locale based sortingThis is a reply/follow-up on Chris Hostetter's message on Lucene developers list (aug 2006):
http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200608.mbox/%3cPine.LNX.4.58.0608211050330.5081@hal.rescomp.berkeley.edu%3e

> perhaps it would be worthwhile for comparatorStringLocale to convert the String[] it gets back from FieldCache.DEFAULT.getStrings to a new CollationKey[]? or maybe even for FieldCache.DEFAULT.getStrings to be deprecated, and replaced with a FieldCache.DEFAULT.getCollationKeys(reader,field,Collator)?

I think the best is to keep the default behavior as it is today. There is a cost of building caches for sort fields which I think not everyone wants. However for some international production environments there are indeed possible performance gains in comparing precalculated keys instead of comparing strings with rulebased collators.

Since Lucene's Sort architecture is pluggable it is easy to create a custom locale-based comparator, which utilizes the built-in caching/warming mechanism of FieldCache, and may be used in SortField constructor.

I'm not sure whether there should be classes for this in Lucene core or not, but it could be nice to have the option of performance vs. memory consumption in localized sorting without having to use additional jars.

"
1,"XMLparser drops user boostingThe lucene XML parser seems to convert user defined boosting back to default 1.0 and thus boosting value is dropped from the query...

e.g.

{code:xml}
<BooleanQuery>
	<Clause occurs=""must"">
		<BooleanQuery>
			<Clause occurs=""should"">
				<UserQuery fieldName=""Vehicle.Colour"">red^66 blue~^8</UserQuery>
			</Clause>
		</BooleanQuery>
	</Clause>
	<Clause occurs=""should"">
		<BooleanQuery>
			<Clause occurs=""should"">
				<UserQuery fieldName=""Vehicle.Colour"">black^0.01</UserQuery>
			</Clause>
		</BooleanQuery>
	</Clause>
</BooleanQuery>
{code}

produces a lucene query: +( ( Vehicle.Colour:red^66 Vehicle.Colour:blue~0.5^8 ) ) ( Vehicle.Colour:black )

The expected query : +( ( Vehicle.Colour:red^66 Vehicle.Colour:blue~0.5^8 ) ) ( Vehicle.Colour:black^0.01 )

I have developed a work around by modifying line 77 of UserInputQueryBuilder.java 

from:

{code:java}
q.setBoost(DOMUtils.getAttribute(e,""boost"",1.0f));
{code}

to:

{code:java}
q.setBoost( DOMUtils.getAttribute( e, ""boost"", q.getBoost() ) );
{code}

"
0,"exception consistency in o.a.l.storejust some minor improvements:
* always use EOFException when its eof
* always include the inputstream too so we know filename etc
* use FileNotFoundException consistently in CFS when a sub-file is not found
"
0,"Correct spatial and trie documentation links in JavaDocs and siteAfter updating myself in the site docs, I have some changes to the site and javadocs of Lucene 2.9:
- Add spatial contrib to javadocs
- Add trie package to the contrib/queries package
Both changes prevent these pacakges from a apearing in core's pacakge list on the javadocs/all homepage. I also adjusted the documentation page to reflect the changes.

I will commit the attached patch, if nobody objects."
0,"ArabicAnalyzer: Lowercase before StopfilterArabicAnalyzer lowercases text in case you have some non-Arabic text around.
It also allows you to set a custom stopword list (you might augment the Arabic list with some English ones, for example).

In this case its helpful for these non-Arabic stopwords, to lowercase before stopfilter.
"
1,"wrong stats/scoring from MemoryCodecI hit some random failures in the flexscoring branch: wierd because its not a random test.

I noticed the test always failed with memorycodec, and wrote a specific test for it.

I haven't traced thru it yet, but I think its likely the issue that memorycodec is somehow returning wrong stats here?"
1,"TestAddIndexes fails (norms file not found)ant test-core -Dtestcase=TestAddIndexes -Dtestmethod=testAddIndexesWithRollback -Dtests.seed=2f55291b308dc34b:-4d72bfad34f3f357:4bc5ec894269c041 -Dargs=""-Dfile.encoding=UTF-8"" -Dtests.iter=100

fails about 4 or 5 times out of 100."
0,"IndexWriter should call MP.useCompoundFile and not LogMP.getUseCompoundFileSpin off from here: http://www.gossamer-threads.com/lists/lucene/java-dev/112311.

I will attach a patch shortly that addresses the issue on trunk."
0,"IndexWriter should throw IndexFormatTooOldExc on open, not later during optimize/getReader/closeSpinoff of LUCENE-2618 and also related to the original issue LUCENE-2523...

If you open IW on a too-old index, you don't find out until much later that the index is too old.

This is because IW does not go and open segment readers on all segments.  It only does so when it's time to apply deletes, do merges, open an NRT reader, etc.

This is a serious bug because you can in fact succeed in committing with the new major version of Lucene against your too-old index, which is catastrophic because suddenly the old Lucene version will no longer open the index, and so your index becomes unusable."
1,"TestPerFieldCodecSupport intermittent fail{noformat}

    [junit] Testsuite: org.apache.lucene.index.TestPerFieldCodecSupport
    [junit] Testcase: testChangeCodecAndMerge(org.apache.lucene.index.TestPerFieldCodecSupport):	FAILED
    [junit] expected:<4> but was:<3>
    [junit] junit.framework.AssertionFailedError: expected:<4> but was:<3>
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:881)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:847)
    [junit] 	at org.apache.lucene.index.TestPerFieldCodecSupport.assertHybridCodecPerField(TestPerFieldCodecSupport.java:189)
    [junit] 	at org.apache.lucene.index.TestPerFieldCodecSupport.testChangeCodecAndMerge(TestPerFieldCodecSupport.java:145)
    [junit] 
    [junit] 
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.416 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestPerFieldCodecSupport -Dtestmethod=testChangeCodecAndMerge -Dtests.seed=1508266713336297966:-102145263724760840
    [junit] NOTE: test params are: codec=SimpleText, locale=ms, timezone=America/Winnipeg
    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestPerFieldCodecSupport]
    [junit] ------------- ---------------- ---------------
    [junit] Test org.apache.lucene.index.TestPerFieldCodecSupport FAILED
{noformat}

I haven't tried to figure it out yet..."
0,"Remove Scorer.getSimilarity()Originally this was part of the patch for per-field Similarity (LUCENE-2236), but I pulled it 
out here as its own issue as its really mostly unrelated. I also like it as a separate issue 
to apply the deprecation to branch_3x to just make less surprises/migration hassles for 4.0 users.

Currently Scorer takes a confusing number of ctors, either a Similarity, or a Weight + Similarity.
Also, lots of scorers don't use the Similarity at all, and its not really needed in Scorer itself.
Additionally, the Weight argument is often null. The Weight makes sense to be here in Scorer, 
its the parent that created the scorer, and used by Scorer itself to support LUCENE-2590's features.
But I dont think all queries work with this feature correctly right now, because they pass null.

Finally the situation gets confusing if you start to consider delegators like ScoreCachingWrapperScorer,
which arent really delegating correctly so I'm unsure features like LUCENE-2590 aren't working with this.

So I think we should remove the getSimilarity, if your scorer uses a Similarity its already coming
to you via your ctor from your Weight and you can manage this yourself.

Also, all scorers should pass the Weight (parent) that created them, and this should be Scorer's only ctor.
I fixed all core/contrib/solr Scorers (even the internal ones) to pass their parent Weight, just for consistency
of this visitor interface. The only one that passes null is Solr's ValueSourceScorer.

I set fix-for 3.1, not because i want to backport anything, only to mark the getSimilarity deprecated there.
"
1,"Few issues with CachingCollectorCachingCollector (introduced in LUCENE-1421) has few issues:
# Since the wrapped Collector may support out-of-order collection, the document IDs cached may be out-of-order (depends on the Query) and thus replay(Collector) will forward document IDs out-of-order to a Collector that may not support it.
# It does not clear cachedScores + cachedSegs upon exceeding RAM limits
# I think that instead of comparing curScores to null, in order to determine if scores are requested, we should have a specific boolean - for clarity
# This check ""if (base + nextLength > maxDocsToCache)"" (line 168) can be relaxed? E.g., what if nextLength is, say, 512K, and I cannot satisfy the maxDocsToCache constraint, but if it was 10K I would? Wouldn't we still want to try and cache them?

Also:
* The TODO in line 64 (having Collector specify needsScores()) -- why do we need that if CachingCollector ctor already takes a boolean ""cacheScores""? I think it's better defined explicitly than implicitly?

* Let's introduce a factory method for creating a specialized version if scoring is requested / not (i.e., impl the TODO in line 189)

* I think it's a useful collector, which stands on its own and not specific to grouping. Can we move it to core?

* How about using OpenBitSet instead of int[] for doc IDs?
** If the number of hits is big, we'd gain some RAM back, and be able to cache more entries
** NOTE: OpenBitSet can only be used for in-order collection only. So we can use that if the wrapped Collector does not support out-of-order

* Do you think we can modify this Collector to not necessarily wrap another Collector? We have such Collector which stores (in-memory) all matching doc IDs + scores (if required). Those are later fed into several processes that operate on them (e.g. fetch more info from the index etc.). I am thinking, we can make CachingCollector *optionally* wrap another Collector and then someone can reuse it by setting RAM limit to unlimited (we should have a constant for that) in order to simply collect all matching docs + scores.

* I think a set of dedicated unit tests for this class alone would be good.

That's it so far. Perhaps, if we do all of the above, more things will pop up."
0,"Document Maven nightly builds, artifact generation, and using Maven to build Lucene/SolrThere should be documentation we can point people to when they ask how to use Maven with Lucene and Solr."
0,"Refactor TestIndexWriterTestIndexWriter is getting a bit unwieldy:
* 5,315 lines of code
* 116 test methods
* runtimes frequently between 1 and 2 minutes.

It starts to be pretty scary to merge changes because its so massive.

A lot of the tests arguably belong somewhere else (e.g. the addIndex* tests belong in TestAddIndexes)

But here is a start:
# Pulls out the *OnDiskFull tests into TestIndexWriterOnDiskFull
# Pulls out the multithreaded tests into TestIndexWriterWithThreads
"
1,Can't create NIOFSDirectory w/o setting a system propertyNIOFSDirectory.getDirectory() returns a FSDirectory object
0,"Remove segments with all documents deleted in commit/flush/close of IndexWriter instead of waiting until a merge occurs.I do not know if this is a bug in 2.9.0, but it seems that segments with all documents deleted are not automatically removed:

{noformat}
4 of 14: name=_dlo docCount=5
  compound=true
  hasProx=true
  numFiles=2
  size (MB)=0.059
  diagnostics = {java.version=1.5.0_21, lucene.version=2.9.0 817268P - 2009-09-21 10:25:09, os=SunOS,
     os.arch=amd64, java.vendor=Sun Microsystems Inc., os.version=5.10, source=flush}
  has deletions [delFileName=_dlo_1.del]
  test: open reader.........OK [5 deleted docs]
  test: fields..............OK [136 fields]
  test: field norms.........OK [136 fields]
  test: terms, freq, prox...OK [1698 terms; 4236 terms/docs pairs; 0 tokens]
  test: stored fields.......OK [0 total field count; avg ? fields per doc]
  test: term vectors........OK [0 total vector count; avg ? term/freq vector fields per doc]
{noformat}

Shouldn't such segments not be removed automatically during the next commit/close of IndexWriter?

*Mike McCandless:*
Lucene doesn't actually short-circuit this case, ie, if every single doc in a given segment has been deleted, it will still merge it [away] like normal, rather than simply dropping it immediately from the index, which I agree would be a simple optimization. Can you open a new issue? I would think IW can drop such a segment immediately (ie not wait for a merge or optimize) on flushing new deletes.
"
1,"thaiwordfilter uses attributesource.copyTo incorrectlyThe bug can be seen by https://builds.apache.org/hudson/job/Lucene-Solr-tests-only-3.x/7367/

It looks like the issue is this lazy initialization of the cloned token: if the tokenstream is reused
and the consumer is interested in a different set of attributes, it could be a problem.

one probably-probably-not-totally-correct fix would be to add 'clonedToken = null;' to reset(), at 
least it would solve this case?"
1,"IW#nrtIsCurrent retruns true if changes are in del queue but not in bufferedDeleteStream yetspinnoff from SOLR-2861 - since the delete queue is not necessarily applied entirely on each request there is a chance that there are changes in the delete queue but not yet in buffered deletes. this can prevent NRT readers from reopen when they should... this shows the problematic code:

{code}
Index: java/org/apache/lucene/index/IndexWriter.java
===================================================================
--- java/org/apache/lucene/index/IndexWriter.java	(revision 1195214)
+++ java/org/apache/lucene/index/IndexWriter.java	(working copy)
@@ -4074,7 +4074,7 @@
   synchronized boolean nrtIsCurrent(SegmentInfos infos) {
     //System.out.println(""IW.nrtIsCurrent "" + (infos.version == segmentInfos.version && !docWriter.anyChanges() && !bufferedDeletesStream.any()));
     ensureOpen();
-    return infos.version == segmentInfos.version && !docWriter.anyChanges() && !bufferedDeletesStream.any();
+    return infos.version == segmentInfos.version && !docWriter.anyChanges() && !docWriter.deleteQueue.anyChanges();
   }
{code}"
0,"Remove DocumentBuilder interface from facet moduleThe facet module contains an interface called DocumentBuilder, which contains a single method, build(Document) (it's a builder API). We use it in my company to standardize how different modules populate a Document object. I've included it with the facet contribution so that things will compile with as few code changes as possible.

Now it's time to do some cleanup and I'd like to start with this interface. If people think that this interface is useful to reside in 'core', then I don't mind moving it there. But otherwise, let's remove it from the code. It has only one impl in the facet module: CategoryDocumentBuilder, and we can certainly do without the interface.

More so, it's under o.a.l package which is inappropriate IMO. If it's moved to 'core', it should be under o.a.l.document.

If people see any problem with that, please speak up. I will do the changes and post a patch here shortly."
0,"Generalize directory copy operationThe copy operation in RAMDirectory(Directory) constructor can be used more generally to copy one directory to another. Why bound it only to RAMDirectory?. For example, I build index in RAMDirectory but I need it to persist in FSDirectory. I created a patch to solve it."
1,"TestIndexWriter failure: AIOOBEtrunk: r1133486 
{code}
    [junit] Testsuite: org.apache.lucene.index.TestIndexWriter
    [junit] Testcase: testEmptyFieldName(org.apache.lucene.index.TestIndexWriter):      Caused an ERROR
    [junit] CheckIndex failed
    [junit] java.lang.RuntimeException: CheckIndex failed
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:158)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:144)
    [junit]     at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:477)
    [junit]     at org.apache.lucene.index.TestIndexWriter.testEmptyFieldName(TestIndexWriter.java:857)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1362)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1280)
    [junit] 
    [junit] 
    [junit] Tests run: 39, Failures: 0, Errors: 1, Time elapsed: 17.634 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] CheckIndex failed
    [junit] Segments file=segments_1 numSegments=1 version=FORMAT_4_0 [Lucene 4.0]
    [junit]   1 of 1: name=_0 docCount=1
    [junit]     codec=SegmentCodecs [codecs=[PreFlex], provider=org.apache.lucene.index.codecs.CoreCodecProvider@3f78807]
    [junit]     compound=false
    [junit]     hasProx=true
    [junit]     numFiles=8
    [junit]     size (MB)=0
    [junit]     diagnostics = {os.version=2.6.39-gentoo, os=Linux, lucene.version=4.0-SNAPSHOT, source=flush, os.arch=amd64, java.version=1.6.0_25, java.vendor=Sun Microsystems Inc.}
    [junit]     no deletions
    [junit]     test: open reader.........OK
    [junit]     test: fields..............OK [1 fields]
    [junit]     test: field norms.........OK [1 fields]
    [junit]     test: terms, freq, prox...ERROR: java.lang.ArrayIndexOutOfBoundsException: -1

    [junit] java.lang.ArrayIndexOutOfBoundsException: -1
    [junit]     at org.apache.lucene.index.codecs.preflex.TermInfosReader.seekEnum(TermInfosReader.java:212)
    [junit]     at org.apache.lucene.index.codecs.preflex.TermInfosReader.seekEnum(TermInfosReader.java:301)
    [junit]     at org.apache.lucene.index.codecs.preflex.TermInfosReader.get(TermInfosReader.java:234)
    [junit]     at org.apache.lucene.index.codecs.preflex.TermInfosReader.terms(TermInfosReader.java:371)
    [junit]     at org.apache.lucene.index.codecs.preflex.PreFlexFields$PreTermsEnum.reset(PreFlexFields.java:719)
    [junit]     at org.apache.lucene.index.codecs.preflex.PreFlexFields$PreTerms.iterator(PreFlexFields.java:249)
    [junit]     at org.apache.lucene.index.PerFieldCodecWrapper$FieldsReader$FieldsIterator.terms(PerFieldCodecWrapper.java:147)
    [junit]     at org.apache.lucene.index.CheckIndex.testTermIndex(CheckIndex.java:610)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:495)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:154)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:144)
    [junit]     at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:477)
    [junit]     at org.apache.lucene.index.TestIndexWriter.testEmptyFieldName(TestIndexWriter.java:857)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)
    [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit]     at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1362)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1280)
    [junit]     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    [junit]     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    [junit]     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:422)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:931)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:758)
    [junit]     test: stored fields.......OK [0 total field count; avg 0 fields per doc]
    [junit]     test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
    [junit] FAILED

    [junit]     WARNING: fixIndex() would remove reference to this segment; full exception:
    [junit] java.lang.RuntimeException: Term Index test failed
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:508)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:154)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:144)
    [junit]     at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:477)
    [junit]     at org.apache.lucene.index.TestIndexWriter.testEmptyFieldName(TestIndexWriter.java:857)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)
    [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit]     at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1362)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1280)
    [junit]     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    [junit]     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    [junit]     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:422)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:931)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:758)
    [junit] 
    [junit] WARNING: 1 broken segments (containing 1 documents) detected
    [junit] 
    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testEmptyFieldName -Dtests.seed=-3770357642070518646:-3121175410586002489 -Dtests.multiplier=3
    [junit] NOTE: test params are: codec=PreFlex, locale=zh, timezone=Indian/Antananarivo
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestDateTools, TestDeletionPolicy, TestDocsAndPositions, TestFlex, TestIndexReaderCloneNorms, TestIndexWriter]
    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=85972280,total=232521728
    [junit] ------------- ---------------- ---------------
    [junit] TEST org.apache.lucene.index.TestIndexWriter FAILED
{code}"
1,"TestIndexWriterExceptions fails (reproducible){noformat}
ant test -Dtestcase=TestIndexWriterExceptions -Dtestmethod=testIllegalPositions -Dtests.seed=-228094d3d2f35cf2:-496e33eec9bbd57c:36a1c54f4e1bb32 -Dargs=""-Dfile.encoding=UTF-8""

    [junit] junit.framework.AssertionFailedError: position=-2 lastPosition=0
    [junit]     at org.apache.lucene.codecs.lucene40.Lucene40PostingsWriter.addPosition(Lucene40PostingsWriter.java:215)
    [junit]     at org.apache.lucene.index.FreqProxTermsWriterPerField.flush(FreqProxTermsWriterPerField.java:519)
    [junit]     at org.apache.lucene.index.FreqProxTermsWriter.flush(FreqProxTermsWriter.java:92)
    [junit]     at org.apache.lucene.index.TermsHash.flush(TermsHash.java:117)
    [junit]     at org.apache.lucene.index.DocInverter.flush(DocInverter.java:53)
    [junit]     at org.apache.lucene.index.DocFieldProcessor.flush(DocFieldProcessor.java:81)
    [junit]     at org.apache.lucene.index.DocumentsWriterPerThread.flush(DocumentsWriterPerThread.java:475)
    [junit]     at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:422)
    [junit]     at org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:553)
    [junit]     at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:2640)
    [junit]     at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:2616)
    [junit]     at org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:851)
    [junit]     at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:810)
    [junit]     at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:774)
    [junit]     at org.apache.lucene.index.TestIndexWriterExceptions.testIllegalPositions(TestIndexWriterExceptions.java:1517)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)
    [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
    [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
    [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
    [junit]     at org.apache.lucene.util.LuceneTestCase$SubclassSetupTeardownRule$1.evaluate(LuceneTestCase.java:729)
    [junit]     at org.apache.lucene.util.LuceneTestCase$InternalSetupTeardownRule$1.evaluate(LuceneTestCase.java:645)
    [junit]     at org.apache.lucene.util.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:22)
    [junit]     at org.apache.lucene.util.LuceneTestCase$TestResultInterceptorRule$1.evaluate(LuceneTestCase.java:556)
    [junit]     at org.apache.lucene.util.UncaughtExceptionsRule$1.evaluate(UncaughtExceptionsRule.java:51)
    [junit]     at org.apache.lucene.util.LuceneTestCase$RememberThreadRule$1.evaluate(LuceneTestCase.java:618)
    [junit]     at org.junit.rules.RunRules.evaluate(RunRules.java:18)
    [junit]     at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
    [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
    [junit]     at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:164)
    [junit]     at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)
    [junit]     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
    [junit]     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
    [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
    [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
    [junit]     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
    [junit]     at org.apache.lucene.util.UncaughtExceptionsRule$1.evaluate(UncaughtExceptionsRule.java:51)
    [junit]     at org.apache.lucene.util.StoreClassNameRule$1.evaluate(StoreClassNameRule.java:21)
    [junit]     at org.apache.lucene.util.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:22)
    [junit]     at org.junit.rules.RunRules.evaluate(RunRules.java:18)
    [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
    [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:518)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1052)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:906)
    [junit] 
{noformat}"
0,"Document.fields() only returns stored fieldsDocument.fields() only returns stored fields, not those which are indexed but not 
stored. This is confusing, as there's a isStored() Method which doesn't make much 
sense then. 
 
Actually fields() returns all fields only just after Document.add(new Field(...)), even the 
ones which are not stored. Sounds confusing? :-) I'll attach a small program that 
demonstrates this. 
 
This should either be fixed so that all fields are always returned or it should be 
documented."
0,"Add FixedBitSet.and(other/DISI), andNot(other/DISI)For the parent issue, and() and andNot() on DISIs and other FixedBitSets are missing. This issue will add those methods.

The DISI methods (also the already existing or(DISI)) method will check for OpenBitSetIterator and do an inplace operation using the bits as optimization."
0,"revisit segments.gen sleepingin LUCENE-3601, i worked up a change where we intentionally crash() all un-fsynced files 
in tests to ensure that we are calling sync on files when we should.

I think this would be nice to do always (and with some fixes all tests pass).

But this is super-slow sometimes because when we corrupt the unsynced segments.gen, it causes
SIS.read to take 500ms each time (and in checkindex for some reason we do this twice, which seems wrong).

I can workaround this for now for tests (just do a partial crash that avoids corrupting the segments.gen),
but I wanted to create this issue for discussion about the sleeping/non-fsyncing of segments.gen, just
because i guess its possible someone could hit this slowness.
 "
0,"Let Codec consume entire documentCurrently the codec API is limited to consume Terms & Postings upon a segment flush. To enable stored fields & DocValues to make use of the Codec abstraction codecs should allow to pull a consumer ahead of flush time and consume all values from a document's field though a consumer API. An alternative to consuming the entire document would be extending FieldsConsumer to return a StoredValueConsumer / DocValuesConsumer like it is done in DocValues - Branch right now side by side to the TermsConsumer. Yet, extending this has proven to be very tricky and error prone for several reasons:
* FieldsConsumer requires SegmentWriteState which might be different upon flush compared to when the document is consumed. SegmentWriteState must therefor be created twice 1. when the first docvalues field is indexed 2. when flushed. 
* FieldsConsumer are current pulled for each indexed field no matter if there are terms to be indexed or not. Yet, if we use something like DocValuesCodec which essentially wraps another codec and creates FieldConsumer on demand the wrapped codecs consumer might not be initialized even if the field is indexed. This causes problems once such a field is opened but missing the required files for that codec. I added some harsh logic to work around this which should be prevented.
* SegmentCodecs are created for each SegmentWriteState which might yield wrong codec IDs depending on how fields numbers are assigned. We currently depend on the fact that all fields for a segment and therefore their codecs are known when SegmentCodecs are build. To enable consuming perDoc values in codecs we need to do that incrementally

Codecs should instead provide a DocumentConsumer side by side with the FieldsConsumer created prior to flush. This is also a prerequisite for LUCENE-2621"
1,"SimpleText sumTotalTermFreq is wrong if only positions are omittedant test -Dtestcase=TestOmitPositions -Dtestmethod=testBasic -Dtests.seed=-6c9bd4a6197b9463:-71d0d11bc2db9a15:697690b3dff2369 -Dargs=""-Dfile.encoding=UTF-8""
    [junit] java.lang.AssertionError: sumTotalTermFreq=0,sumDocFreq=400
    [junit] 	at org.apache.lucene.search.CollectionStatistics.<init>(CollectionStatistics.java:38)

This assert fails because #of positions for the field is < #of postings, which is impossible.

From memory i think SimpleText calculates sumTotalTermFreq ""one the fly"" by reading the positions from its text file...
In this case it should write the stat explicitly."
0,"Add katakana stem filter to better deal with certain katakana spelling variantsMany Japanese katakana words end in a long sound that is sometimes optional.

For example, パーティー and パーティ are both perfectly valid for ""party"".  Similarly we have センター and センタ that are variants of ""center"" as well as サーバー and サーバ for ""server"".

I'm proposing that we add a katakana stemmer that removes this long sound if the terms are longer than a configurable length.  It's also possible to add the variant as a synonym, but I think stemming is preferred from a ranking point of view.

"
0,"Make IndexReader/Searcher ctors readOnly=true by defaultAnother ""change the defaults"" in 3.0.

Right now you get a read/write reader from IndexReader.open and new IndexSearcher(...), and reserving the right to write causes thread contention (on isDeleted).

In 3.0 let's make readOnly reader the default, but still allow opening a read/write IndexReader."
0,"Improve tests to work easier from IDEsAs reported by Paolo Castagna on the mailing lists, some tests fail when you run them from eclipse.

Some of the failures he reports are actually code problems such as base test classes not being 
abstract when they should be... we should fix things like that."
0,"Improve Greek analysis* Changed tokenstreams to CharTermAttribute
* Moved stopwords out of private String[] to a txt file (for use by Solr, etc)
* Removed TODO / fixed unicode conformance of GreekLowerCaseFilter
* Reformatted touched files to normal indentation
* Added inflectional stemmer (Ntais algorithm)

all the changes are backwards compatible with Version."
0,"Refactor FieldCacheRangeFilter.FieldCacheDocIdSet to be separate class and fix the dangerous matchDoc() throws AIOOBE requirementFollowup from LUCENE-3593:
The FieldCacheRangeFilter.FieldCacheDocIdSet class has a strange requirement on the abstract matchDoc(): It should throw AIOOBE if the docId is > maxDoc. This check should be done by caller as especially on trunk, e.g. FieldCacheTermsFilter does not seem to always throw this exception correctly (getOrd() is a method and no array in TermsIndex cache).

Also in 3.x the Filter does not correctly respect deletions when a FieldCache based on a reopened reader is used.

This issue will refactor this and fix the bugs and moves the docId check up to the iterator."
0,"Add fixed size DocValues int variants & expose Arrays where possiblecurrently we only have variable bit packed ints implementation. for flexible scoring or loading field caches it is desirable to have fixed int implementations for 8, 16, 32 and 64 bit. "
1,"Index corruption when using RAMDirectory( Directory) constructorLUCENE-475 introduced a bug in creating RAMDirectories for large indexes. It truncates the length of the file to an int, from its original long value. Any files that are larger than an int are truncated. Patch to fix is attached."
0,"Remove Priority-Queue size trap in MultiTermQuery.TopTermsBooleanQueryRewriteThese APIs are new in 3.x, so we can do this with no backwards-compatibility issue:

Before 3.1, FuzzyQuery had its own internal rewrite method.
We exposed this in 3.x as TopTermsBooleanQueryRewrite, and then as subclasses for Scoring and Boost-only variants.

The problem I have is that the PQ has a default (large) size of Integer.MAX_VALUE... of course its later limited by
the value of BooleanQuery's maxClauseCount, but I think this is a trap.

Instead its better to simply remove these defaults and force the user to provide a default (reasonable) size.
"
0,"don't use finalizers for FSIndexInput clonesfinalizers are expensive, and we should avoid using them where possible.
It looks like this helped to tickle some kind of bug (looks like a JVM bug?)
http://www.nabble.com/15-minute-hang-in-IndexInput.clone%28%29-involving-finalizers-tf2826906.html#a7891015"
0,[PATCH] Mention RangeFilter in javadoc for maxClauseCount 
0,"Kuromoji code donation - a new Japanese morphological analyzerAtilika Inc. (アティリカ株式会社) would like to donate the Kuromoji Japanese morphological analyzer to the Apache Software Foundation in the hope that it will be useful to Lucene and Solr users in Japan and elsewhere.

The project was started in 2010 since we couldn't find any high-quality, actively maintained and easy-to-use Java-based Japanese morphological analyzers, and these become many of our design goals for Kuromoji.

Kuromoji also has a segmentation mode that is particularly useful for search, which we hope will interest Lucene and Solr users.  Compound-nouns, such as 関西国際空港 (Kansai International Airport) and 日本経済新聞 (Nikkei Newspaper), are segmented as one token with most analyzers.  As a result, a search for 空港 (airport) or 新聞 (newspaper) will not give you a for in these words.  Kuromoji can segment these words into 関西 国際 空港 and 日本 経済 新聞, which is generally what you would want for search and you'll get a hit.

We also wanted to make sure the technology has a license that makes it compatible with other Apache Software Foundation software to maximize its usefulness.  Kuromoji has an Apache License 2.0 and all code is currently owned by Atilika Inc.  The software has been developed by my good friend and ex-colleague Masaru Hasegawa and myself.

Kuromoji uses the so-called IPADIC for its dictionary/statistical model and its license terms are described in NOTICE.txt.

I'll upload code distributions and their corresponding hashes and I'd very much like to start the code grant process.  I'm also happy to provide patches to integrate Kuromoji into the codebase, if you prefer that.

Please advise on how you'd like me to proceed with this.  Thank you.
"
0,"String.intern() faster alternativeBy using our own interned string pool on top of default, String.intern() can be greatly optimized.

On my setup (java 6) this alternative runs ~15.8x faster for already interned strings, and ~2.2x faster for 'new String(interned)'
For java 5 and 4 speedup is lower, but still considerable."
1,"Standard analyzer does not correctly tokenize combining character U+0364 COMBINING LATIN SMALL LETTRE EStandard analyzer does not correctly tokenize combining character U+0364 COMBINING LATIN SMALL LETTRE E.
The word ""moͤchte"" is incorrectly tokenized into ""mo"" ""chte"", the combining character is lost.
Expected result is only on token ""moͤchte""."
1,"TermAttributeImpl.equals() does not check termLengthIf you look at the code from equals(), I think it misses this check :

other.termLength==this.termLength

This check must be before the comparison of the arrays."
0,"Move & rename the terms dict, index, abstract postings out of oal.index.codecs.standardThe terms dict components that current live under Standard codec
(oal.index.codecs.standard.*) are in fact very generic, and in no way
particular to the Standard codec.  Already we have many other codecs
(sep, fixed int block, var int block, pulsing, appending) that re-use
the terms dict writer/reader components.

So I'd like to move these out into oal.index.codecs, and rename them:

  * StandardTermsDictWriter/Reader -> PrefixCodedTermsWriter/Reader
  * StandardTermsIndexWriter/Reader -> AbstractTermsIndexWriter/Reader
  * SimpleStandardTermsIndexWriter/Reader -> SimpleTermsIndexWriter/Reader
  * StandardPostingsWriter/Reader -> AbstractPostingsWriter/Reader
  * StandardPostingsWriterImpl/ReaderImple -> StandardPostingsWriter/Reader

With this move we have a nice reusable terms dict impl.  The terms
index impl is still well-decoupled so eg we could [in theory] explore
a variable gap terms index.

Many codecs, I expect, don't need/want to implement their own terms
dict....

There are no code/index format changes here, besides the renaming &
fixing all imports/usages of the renamed class.
"
0,"Custom build.xml for binary distributionsThe binary files of a distribution come with the demo sources
and a build.xml file. However, the build.xml doesn't work for
the binary distribution, so it can't be used to build the 
demos.

This problem was notices the first time when release 2.1 was
made. Before we ship 2.2 we should fix this."
0,Create a analysis/uima module for UIMA based tokenizers/analyzersAs discussed in SOLR-3013 the UIMA Tokenizers/Analyzer should be refactored out in a separate module (modules/analysis/uima) as they can be used in plain Lucene. Then the solr/contrib/uima will contain only the related factories.
0,"Change Visibility of fields[] in MultiFieldQueryParserIn MultiFieldQueryParser the two methods 

  protected Query getFuzzyQuery(String field, String termStr, float minSimilarity) throws ParseException
  protected Query getWildcardQuery(String field, String termStr) throws ParseException

are intended to be overwritten if one would like to avoid fuzzy and wildcard queries. However, the String[] fields attribute of this class is private and hence it is not accessible in subclasses of MFQParser. If you just change it to protected this issue should be solved."
1,"False assertion of >0 position delta in StandardPostingsWriterImplStandardPostingsWriterImpl line 159 is:
{code:java}
    assert delta > 0 || position == 0 || position == -1: ""position="" + position + "" lastPosition="" + lastPosition;            // not quite right (if pos=0 is repeated twice we don't catch it)
{code}

I enable assertions when I run my unit tests and I've found this assertion to fail when delta is 0 which occurs when the same position value is sent in twice in arrow.  Once I added RemoveDuplicatesTokenFilter, this problem went away.  Should I really be forced to add this filter?  I think delta >= 0 would be a better assertion."
0,"add offsets into lucene40 postingsLUCENE-3684 added support for IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS, but
only SimpleText implements it.

I think we should implement it in the other 4.0 codecs (starting with Lucene40PostingsFormat)."
1,"NPE crash in case of out of memoryThe attached class makes Lucene crash with an NPE when starting it with -Xmx10M, although there's probably an OutOfMemory problem. The stacktrace:

Exception in thread ""main"" java.lang.NullPointerException
	at java.util.Arrays.fill(Unknown Source)
	at org.apache.lucene.index.DocumentsWriter$ByteBlockPool.reset(DocumentsWriter.java:2873)
	at org.apache.lucene.index.DocumentsWriter$ThreadState.resetPostings(DocumentsWriter.java:637)
	at org.apache.lucene.index.DocumentsWriter.resetPostingsData(DocumentsWriter.java:458)
	at org.apache.lucene.index.DocumentsWriter.abort(DocumentsWriter.java:423)
	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:2433)
	at org.apache.lucene.index.DocumentsWriter.addDocument(DocumentsWriter.java:2397)
	at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1445)
	at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1424)
	at LuceneCrash.myrun(LuceneCrash.java:32)
	at LuceneCrash.main(LuceneCrash.java:19)

The documents are quite big (some hundred KB each), I cannot attach them but I can send them via private mail if needed. The crash happens the first time reset() is called, after indexing 10 documents. I assume the bug is just that the error is misleading, there maybe should be an OOM error.
"
0,"deprecated method used in fieldsReader / setOmitTf()setOmitTf(boolean) is deprecated and should not be used by core classes. One place where it appears is FieldsReader , this patch fixes it. It was necessary to change Fieldable to AbstractField at two places, only local variables.   "
0,"TestCollectionUtil fails on IBM JRE    [junit] Testcase: testEmptyArraySort(org.apache.lucene.util.TestCollectionUtil):    Caused an ERROR
    [junit] CollectionUtil can only sort random access lists in-place."
1,"IndexWriter.unlock does does nothing if NativeFSLockFactory is usedIf NativeFSLockFactory is used, IndexWriter.unlock will return, silently doing nothing. The reason is that NativeFSLockFactory's makeLock always creates a new NativeFSLock. NativeFSLock's release first checks if its lock is not null. However, only if obtain() is called, that lock is not null. So release actually does nothing, and so IndexWriter.unlock does not delete the lock, or fail w/ exception.
This is only a problem in NativeFSLock, and not in other Lock implementations, at least as I was able to see.

Need to think first how to reproduce in a test, and then fix it. I'll work on it."
1,"CachingWrapperFilter crashes if you call both bits() and getDocIdSet()CachingWrapperFilter uses only a single cache, so calling bits() after calling getDocIdSet() will result in a type error. Additionally, more code than is necessary is wrapped in the @synchronized blocks."
0,"Make retrieveTerms(int docNum) public in MoreLikeThisIt would be useful if 
{code}
private PriorityQueue retrieveTerms(int docNum) throws IOException {
{code}

were public, since it is similar in use to 
{code}
public PriorityQueue retrieveTerms(Reader r) throws IOException {
{code}

It also seems useful to add 
{code}
public String [] retrieveInterestingTerms(int docNum) throws IOException{
{code}
to mirror the one that works on Reader.

"
0,"fix getting started / demo docsOpening a new issue for this since there are a number of problems...:

  * We should get the versions right, eg when we explain how to do a src checkout it should point to the path for that release

  * Source checkout / build JARs instructions must be updated for the merger

  * Analyzers JAR must be on the classpath too

  * Demo sources are no longer shipped in a binary release

  * Fixup from LUCENE-2923 (remove web app, new command-line ops for IndexFiles, etc.)"
0,"A few new benchmark tasksSome tasks that would be helpful to see added. Might want some expansion, but here are some basic ones I have been using:

CommitIndexTask
ReopenReaderTask
SearchWithSortTask

I do the sort in a similar way that the highlighting was done, but another method may be better. Just would be great to have sorting.
Also, since there is no great field for sorting (reuters date always appears to be the same) I changed the id field from doc+id to just id. Again maybe not the best solution, but here I am to get the ball rolling :)"
0,"Enable IndexWriter to open an arbitrary commit pointWith a 2-phase commit involving multiple resources, each resource
first does its prepareCommit and then if all are successful they each
commit.  If an exception or timeout/power loss is hit in any of the
resources during prepareCommit or commit, all of the resources must
then rollback.

But, because IndexWriter always opens the most recent commit, getting
Lucene to rollback after commit() has been called is not easy, unless
you make Lucene the last resource to commit.  A simple workaround is
to simply remove the segments_N files of the newer commits but that's
sort of a hassle.

To fix this, we just need to add a ctor to IndexWriter that takes an
IndexCommit.  We recently added this for IndexReader (LUCENE-1311) as
well.  This ctor is definitely an ""expert"" method, and only makes
sense if you have a custom DeletionPolicy that preserves more than
just the most recent commit.
"
0,"MultiReader should not use PQ for its Term/sEnum if it has only 1 readerRelated to LUCENE-2130....

Even though we've switched to segment-based searching, there are still times when the Term/sEnum is used against the top-level reader.  I think Solr does this, and from LUCENE-2130, certain rewrite modes of MTQ will do this as well.

Currently, on an optimized index, MTQ is still using a PQ to present the terms, which is silly because this just adds a sizable amount of overhead.  In such cases we should simply delecate to the single segment.

Note that the single segment can have deletions, and we should still delegate.  Ie, the index need not be optimized, just have a single segment."
0,"Enable Throttling only during nightly buildsSome of my tests take forever even on a big :) machine. In order to speed up our tests we should default the IO throttling to NEVER and only run in during nightly.

"
1,"NumericTokenStream.NumericTermAttribute does not support cloning -> Solr analysis request handlers failDuring converting Solr's AnalysisRequestHandlers (LUCENE-2374) I noticed, that the current implementation of NumericTokenStream fails on cloneAttributes(), which is needed to buffer the tokens for structured display.

This issue should fix this by refactoring the inner class."
1,JEDirectory delete issueJEDirectory is not deleting files properly.  Blocks are left behind due to an error in cursor operations.
1,SimpleDateFormat used in a non thread safe mannerAs Mike pointed out in http://www.mail-archive.com/java-dev@lucene.apache.org/msg10831.html SimpleDateFormat is not thread safe and hence DocMakers need to maintain it in a ThreadLocal.
0,"contrib/benchmark build doesn't handle checking if content is properly extractedThe contrib/benchmark build does not properly handle checking to see if the content (such as Reuters coll.) is properly extracted.  It only checks to see if the directory exists.  Thus, it is possible that the directory gets created and the extraction fails.  Then, the next time it is run, it skips the extraction part and tries to continue on running the benchmark.

The workaround is to manually delete the extraction directory."
0,"Most 4.0 PostingsReaders don't obey DISI contractWhile trying to do some refactoring, I noticed funky things going on with some codecs.

One problem is that DocIdSetIterator says the following:
{noformat}
Returns the following:
   * <ul>
   * <li>-1 or {@link #NO_MORE_DOCS} if {@link #nextDoc()} or
   * {@link #advance(int)} were not called yet.
{noformat}

But most 4.0 Docs/DocsAndPositionsEnums don't actually do this (e.g. return 0). instead we 
are relying on Scorers to 'cover' for them, which is inconsistent. Some scorers actually rely
upon this behavior, for example look at ReqExclScorer.toNonExcluded(), it calls docId() on the
excluded part before it calls nextDoc()

So we need to either fix these enums, change these enums to not extend DocIdSetIterator (and redefine
what the actual contract should be for these enums), change DocIdSetIterator, or something else.

Fixing the enums to return -1 here when they are uninitialized kinda sucks for the ones summing up
document deltas...
"
0,"JavaCC 4.0 fails to generate QueryParser.javaWhen generating the Java source for QueryParser via the ant task 'javacc-QueryParser' against Subversion trunk (updated Jan. 25, 2006), JavaCC 4.0 gives the following error:

javacc-QueryParser:
   [javacc] Java Compiler Compiler Version 4.0 (Parser Generator)
   [javacc] (type ""javacc"" with no arguments for help)
   [javacc] Reading from file [...]/src/java/org/apache/lucene/queryParser/QueryParser.jj . . .
   [javacc] org.javacc.parser.ParseException: Encountered ""<<"" at line 754, column 3.
   [javacc] Was expecting one of:
   [javacc]     <STRING_LITERAL> ...
   [javacc]     ""<"" ...
   [javacc]     
   [javacc] Detected 1 errors and 0 warnings.

BUILD FAILED
"
0,"BoostingNearQuery class (prototype)This patch implements term boosting for SpanNearQuery. Refer to: http://www.gossamer-threads.com/lists/lucene/java-user/62779

This patch works but probably needs more work. I don't like the use of 'instanceof', but I didn't want to touch Spans or TermSpans. Also, the payload code is mostly a copy of what's in BoostingTermQuery and could be common-sourced somewhere. Feel free to throw darts at it :)
"
0,"Make SlowMultiReaderWrapper wrap always so close() is safeThe overhead when wrapping an atomic reader using SlowMultiReaderWrapper is very low, the work done in the static wrap method is much higher (instantiate ArrayList, recusively went through all subreaders), just to check the number of readers than simply always wrapping.

MultiFields already is optimized when called by one-segment or atomic readers, so there is no overhead at all. So this patch removes the static wrap method and you simply wrap like a TokenFilter with ctor: new SlowMultiReaderWrapper(reader)

When this is done, there is also no risk to close a SegmentReader (which you should not do), when wrap() returns a single SegmentReader. This help in parent issue with cleaning up the case in close().

The patch aölso removes the now useless mainReader/reader variables and simply closes the wrapper."
0,"MockDirectoryWrapper should wrap the lockfactoryAfter applying the patch from LUCENE-3147, I added a line to make the test fail if it cannot remove its temporary directory.

I ran 'ant test' on linux 50 times, and it passed all 50 times.
But on windows, it failed often because of write.lock... this is because of unclosed writers in the test.

MockDirectoryWrapper is currently unaware of this write.lock, I think it should wrap the lockfactory so that .close() will fail if there are any outstanding locks.
Then hopefully these tests would fail on linux too.
"
0,"NoLockFactory should have a private constructorNoLockFactory documents in its javadocs that one should use the static getNoLockFactory() method. However the class does not declare a private empty constructor, which breaks its Singleton purpose. We cannot add the empty private constructor because that'd break break-compat (even though I think it's very low chance someone actually instantiates the class), therefore we'll add a @deprecated warning to the class about this, and add the method in 4.0. I personally prefer to add an empty constructor w/ the @deprecated method, but am fine either way.

Don't know if a patch is needed, as this is a trivial change. "
0,"Some Lucene tests try and use a Junit Assert in new threadsThere are a few cases in Lucene tests where JUnit Asserts are used inside a new threads run method - this won't work because Junit throws an exception when a call to Assert fails - that will kill the thread, but the exception will not propagate to JUnit - so unless a failure is caused later from the thread termination, the Asserts are invalid.

TestThreadSafe
TestStressIndexing2
TestStringIntern"
0,Don't throw TooManyClauses exceptionI wonder if it would make sense to fall back to a ConstantScoreQuery instead of throwing a TooManyClauses exception?
0,"Refactoring Lucene collectors (HitCollector and extensions)This issue is a result of a recent discussion we've had on the mailing list. You can read the thread [here|http://www.nabble.com/Is-TopDocCollector%27s-collect()-implementation-correct--td22557419.html].

We have agreed to do the following refactoring:
* Rename MultiReaderHitCollector to Collector, with the purpose that it will be the base class for all Collector implementations.
* Deprecate HitCollector in favor of the new Collector.
* Introduce new methods in IndexSearcher that accept Collector, and deprecate those that accept HitCollector.
** Create a final class HitCollectorWrapper, and use it in the deprecated methods in IndexSearcher, wrapping the given HitCollector.
** HitCollectorWrapper will be marked deprecated, so we can remove it in 3.0, when we remove HitCollector.
** It will remove any instanceof checks that currently exist in IndexSearcher code.
* Create a new (abstract) TopDocsCollector, which will:
** Leave collect and setNextReader unimplemented.
** Introduce protected members PriorityQueue and totalHits.
** Introduce a single protected constructor which accepts a PriorityQueue.
** Implement topDocs() and getTotalHits() using the PQ and totalHits members. These can be used as-are by extending classes, as well as be overridden.
** Introduce a new topDocs(start, howMany) method which will be used a convenience method when implementing a search application which allows paging through search results. It will also attempt to improve the memory allocation, by allocating a ScoreDoc[] of the requested size only.
* Change TopScoreDocCollector to extend TopDocsCollector, use the topDocs() and getTotalHits() implementations as they are from TopDocsCollector. The class will also be made final.
* Change TopFieldCollector to extend TopDocsCollector, and make the class final. Also implement topDocs(start, howMany).
* Change TopFieldDocCollector (deprecated) to extend TopDocsCollector, instead of TopScoreDocCollector. Implement topDocs(start, howMany)
* Review other places where HitCollector is used, such as in Scorer, deprecate those places and use Collector instead.

Additionally, the following proposal was made w.r.t. decoupling score from collect():
* Change collect to accecpt only a doc Id (unbased).
* Introduce a setScorer(Scorer) method.
* If during collect the implementation needs the score, it can call scorer.score().
If we do this, then we need to review all places in the code where collect(doc, score) is called, and assert whether Scorer can be passed. Also this raises few questions:
* What if during collect() Scorer is null? (i.e., not set) - is it even possible?
* I noticed that many (if not all) of the collect() implementations discard the document if its score is not greater than 0. Doesn't it mean that score is needed in collect() always?

Open issues:
* The name for Collector
* TopDocsCollector was mentioned on the thread as TopResultsCollector, but that was when we thought to call Colletor ResultsColletor. Since we decided (so far) on Collector, I think TopDocsCollector makes sense, because of its TopDocs output.
* Decoupling score from collect().

I will post a patch a bit later, as this is expected to be a very large patch. I will split it into 2: (1) code patch (2) test cases (moving to use Collector instead of HitCollector, as well as testing the new topDocs(start, howMany) method.
There might be even a 3rd patch which handles the setScorer thing in Collector (maybe even a different issue?)"
0,"Remove recursion in NumericRangeTermEnumThe current FilteredTermEnum in NRQ uses setEnum() which itsself calls next(). This may lead to a recursion that can overflow stack, if the index is empty and a large range with low precStep is used. With 64 bit numbers and precStep == 1 there may be 127 recursions, as each sub-range would hit no term on empty index and the setEnum call would then call next() which itsself calls setEnum again. This leads to recursion depth of 256.

Attached is a patch that converts to iterative approach. setEnum is now unused and throws UOE (like endEnum())."
0,"IndexReader.document always return a doc with all the stored fields loaded. And this can be slow for the indexed document contain huge fieldswhen generating digest for some documents with huge fields, it should be unnecessary to load the field but just interesting part of the field with the offset information. but indexreader always return the whole field content. afterward, the customized storedfieldsreader will got a repeated loading"
1,"MultiFields.getUniqueFieldCount is brokenthis returns terms.size(), but terms is lazy-initted. So it wrongly returns 0.

Simplest fix would be to return -1."
1,"NRT can temporarily lose deletions at high indexing ratesOK, I found a sneaky case where NRT can temporarily lose deletions.
The deletions aren't permanently lost - they are seen on the next
opened NRT reader.

It happens like this (in IW.getReader):

  * First flush() is called, to flush added docs & materialize the
    deletes.

  * The very next statement enters a sync'd block to open the reader,
    but, if indexing rate is very high, and threads get scheduled
    ""appropriately"", a ""natural"" flush (due to RAM buffer being full
    or flush doc count being reached) could be hit before the sync
    block is entered, in which case that 2nd flush won't materialize
    the deletes associated with it, and the returned NRT reader will
    only see its adds until it's next reopened.

The fix is simple -- we should materialize deletes inside the sync
block, not during the flush.
"
0,"Adding norms, properties indexing and writer.infoStream support to benchmarkI would like to add the following support in benchmark:
# Ability to specify whether norms should be stored in the index.
# Ability to specify whether norms should be stored for the body field (assuming norms are usually stored for that field in real life applications, make it explicit)
# Ability to specify an infoStream for IndexWriter
# Ability to specify whether to index the properties returned on DocData (for content sources like TREC, these may include arbitrary <meta> tags, which we may not want to index).

Patch to come shortly."
0,"[PATCH] IndexWriter.maybeMergeSegments() takes lots of CPU resourcesNote: I believe this to be the same situation with 1.4.3 as with SVN HEAD.

Analysis using hprof utility shows that during index creation with many
documents highlights that the CPU spends a large portion of it's time in
IndexWriter.maybeMergeSegments(), which seems to be a 'waste' compared with
other valuable CPU intensive operations such as tokenization etc.

Using the following test snippet to retrieve some rows from the db and create an
index:

        Analyzer a = new StandardAnalyzer();
        writer = new IndexWriter(indexDir, a, true);
        writer.setMergeFactor(1000);
        writer.setMaxBufferedDocs(10000);
        writer.setUseCompoundFile(false);
        connection = DriverManager.getConnection(
                ""jdbc:inetdae7:tower.aconex.com?database=<somedb>"", ""secret"",
                ""squirrel"");
        String sql = ""select userid, userfirstname, userlastname, email from userx"";
        LOG.info(""sql="" + sql);
        Statement statement = connection.createStatement();
        statement.setFetchSize(5000);
        LOG.info(""Executing sql"");
        ResultSet rs = statement.executeQuery(sql);
        LOG.info(""ResultSet retrieved"");
        int row = 0;

        LOG.info(""Indexing users"");
        long begin = System.currentTimeMillis();
        while (rs.next()) {
            int userid = rs.getInt(1);
            String firstname = rs.getString(2);
            String lastname = rs.getString(3);
            String email = rs.getString(4);
            String fullName = firstname + "" "" + lastname;
            Document doc = new Document();
            doc.add(Field.Keyword(""userid"", userid+""""));
            doc.add(Field.Keyword(""firstname"", firstname.toLowerCase()));
            doc.add(Field.Keyword(""lastname"", lastname.toLowerCase()));
            doc.add(Field.Text(""name"", fullName.toLowerCase()));
            doc.add(Field.Keyword(""email"", email.toLowerCase()));
            writer.addDocument(doc);
            row++;
            if((row % 100)==0){
                LOG.info(row + "" indexed"");
            }
        }
        double end = System.currentTimeMillis();
        double diff = (end-begin)/1000;
        double rate = row/diff;
        LOG.info(""rate:"" +rate);

On my 1.5GHz PowerBook with 1.5Gb RAM and a 5400 RPM drive, my CPU is maxed out,
and I end up getting a rate of indexing between 490-515 documents/second run
over 10 times in succession.  

By applying a simple patch to IndexWriter (see attached shortly), which defers
the calling of maybeMergeSegments() so that it is only called every 2000
times(an arbitrary figure), I appear to get a new rate of between 945-970
documents/second.  Using Luke to look inside each index created between these 2
there does not appear to be any difference.  Same number of Documents, same
number of Terms.

I'm not suggesting one should apply this patch, I'm just highlighting the
difference in performance that this sort of change gives you.  

We are about to use Lucene to index 4 million construction document records, and
so speeding up the indexing process is in our best interest! :)  If one
considers the amount of CPU time spent in maybeMergeSegments over the initial
index creation of 4 million documents, I think one could see how it would be
ideal to try to speed this area up (at least move the bottleneck to IO). 

I woul appreciate anyone taking a moment to comment on this."
0,"omitTF is viral, but omitNorms is anti-viral.omitTF is viral. if you add document 1 with field ""foo"" as omitTF, then document 2 has field ""foo"" without omitTF, they are both treated as omitTF.

but omitNorms is the opposite. if you have a million documents with field ""foo"" with omitNorms, then you add just one document without omitting norms, 
now you suddenly have a million 'real norms'.

I think it would be good for omitNorms to be viral too, just for consistency, and also to prevent huge byte[]'s.
but another option is to make omitTF anti-viral, which is more ""schemaless"" i guess.
"
0,"improve BaseTokenStreamTestCase random string generationMost analysis tests use mocktokenizer (which splits on whitespace), but
its rare that we generate a string with 'many tokens'. So I think we should
try to generate more realistic test strings."
0,"remove random juggling in tests, add -Dtests.seedSince we added newIndexWriterConfig/newDirectory, etc, a lot of tests are juggling randoms around.

Instead this patch:
* changes it so LuceneTestCase[J4] manage the random.
* allow you to set -Dtests.seed=23432432432 to reproduce a test, rather than editing the code
* removes random arguments from newIndexWriterConfig, newDirectory.

I want to do this before looking at doing things like newField so we can vary term vectors, etc.

I also fixed the solr contrib builds so they arent hiding the exceptions i noted in SOLR-2002."
0,"Implement various ranking models as SimilaritiesWith [LUCENE-3174|https://issues.apache.org/jira/browse/LUCENE-3174] done, we can finally work on implementing the standard ranking models. Currently DFR, BM25 and LM are on the menu.

Done:
 * {{EasyStats}}: contains all statistics that might be relevant for a ranking algorithm
 * {{EasySimilarity}}: the ancestor of all the other similarities. Hides the DocScorers and as much implementation detail as possible
 * _BM25_: the current ""mock"" implementation might be OK
 * _LM_
 * _DFR_
 * The so-called _Information-Based Models_

"
0,"merge LuceneTestCase and LuceneTestCaseJ4We added Junit4 support, but as a separate test class.

So unfortunately, we have two separate base classes to maintain: LuceneTestCase and LuceneTestCaseJ4.
This creates a mess and is difficult to manage.

Instead, I propose a single base test class that works both junit3 and junit4 style.

I modified our LuceneTestCaseJ4 in the following way:
* the methods to run are not limited to the ones annotated with @Test, but also any void no-arg methods that start with ""test"", like junit3. this means you dont have to sprinkle @Test everywhere.
* of course, @Ignore works as expected everywhere.
* LuceneTestCaseJ4 extends TestCase so you dont have to import static Assert.* to get all the asserts.

for most tests, no changes are required. but a few very minor things had to be changed:
* setUp() and tearDown() must be public, not protected.
* useless ctors must be removed, such as TestFoo(String name) { super(name); }
* LocalizedTestCase is gone, instead of
{code}
public class TestQueryParser extends LocalizedTestCase {
{code}
it is now
{code}
@RunWith(LuceneTestCase.LocalizedTestCaseRunner.class)
public class TestQueryParser extends LuceneTestCase {
{code}
* Same with MultiCodecTestCase: (LuceneTestCase.MultiCodecTestCaseRunner.class}

I only did the core tests in the patch as a start, and i just made an empty LuceneTestCase extends LuceneTestCaseJ4.

I'd like to do contrib and solr and rename this LuceneTestCaseJ4 to only a single class: LuceneTestCase.
"
1,"Random Test Failure org.apache.lucene.TestExternalCodecs.testPerFieldCodec (from TestExternalCodecs)Error Message

state.ord=54 startOrd=0 ir.isIndexTerm=true state.docFreq=1
Stacktrace

junit.framework.AssertionFailedError: state.ord=54 startOrd=0 ir.isIndexTerm=true state.docFreq=1
	at org.apache.lucene.index.codecs.standard.StandardTermsDictReader$FieldReader$SegmentTermsEnum.seek(StandardTermsDictReader.java:395)
	at org.apache.lucene.index.DocumentsWriter.applyDeletes(DocumentsWriter.java:1099)
	at org.apache.lucene.index.DocumentsWriter.applyDeletes(DocumentsWriter.java:1028)
	at org.apache.lucene.index.IndexWriter.applyDeletes(IndexWriter.java:4213)
	at org.apache.lucene.index.IndexWriter.doFlushInternal(IndexWriter.java:3381)
	at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:3221)
	at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3211)
	at org.apache.lucene.index.IndexWriter.optimize(IndexWriter.java:2345)
	at org.apache.lucene.index.IndexWriter.optimize(IndexWriter.java:2323)
	at org.apache.lucene.index.IndexWriter.optimize(IndexWriter.java:2293)
	at org.apache.lucene.TestExternalCodecs.testPerFieldCodec(TestExternalCodecs.java:645)
	at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:381)
	at org.apache.lucene.util.LuceneTestCase.run(LuceneTestCase.java:373)
Standard Output

NOTE: random codec of testcase 'testPerFieldCodec' was: MockFixedIntBlock(blockSize=1327)
NOTE: random locale of testcase 'testPerFieldCodec' was: lt_LT
NOTE: random timezone of testcase 'testPerFieldCodec' was: Africa/Lusaka
NOTE: random seed of testcase 'testPerFieldCodec' was: 812019387131615618"
0,"build.xml in cnotrib/benchmark should auto build core java and demo if requiredCurrently one needs to build core jar and demo jar before building/running benchmark.
This is not very convenient. 
Change it to 
- use core classes and demo classes (instead of jars).
- build core and demo by dependency if required."
0,"Token type as BitSet: typeBits()It is sometimes useful to have a more compact, easy to parse, type representation for Token than the current type() String.  This patch adds a BitSet onto Token, defaulting to null, with accessors for setting bit flags on a Token.  This is useful for communicating information about a token to TokenFilters further down the chain.  

For example, in the WikipediaTokenizer, the possibility exists that a token could be both a category and bold (or many other variations), yet it is difficult to communicate this without adding in a lot of different Strings for type.  Unlike using the payload information (which could serve this purpose), the BitSet does not get added to the index (although one could easily convert it to a payload.)"
1,"fix compound-file/NoSuchDirectoryException bugs in NRTCachingDirfound some bugs over on LUCENE-3374, but we should fix these separately from whether or not we move it to core,
and the bugs apply to 3.x too.

here we can just add explicit tests for the problems."
0,"Consolidate Near Real Time and Reopen API semanticsWe should consolidate the IndexWriter.getReader and the IndexReader.reopen semantics, since most people are already using the IR.reopen() method, we should simply add::
{code}
IR.reopen(IndexWriter)
{code}

Initially, it could just call the IW.getReader(), but it probably should switch to just using package private methods for sharing the internals"
1,"FilteredQuery explanation inaccuracy with boostThe value of explanation is different than the product of its part if boost > 1.
This is exposed after tightening the explanation check (part of LUCENE-446).

"
0,"disable positions for spellchecker ngram fieldsIn LUCENE-2391 we optimized spellchecker (re)build time/ram usage by omitting frequencies/positions/norms for single-valued fields,
among other things.

Now that we can disable positions but keep freqs, we should disable them for the n-gram fields, because the spellchecker does
not use positional queries.
"
1,"CJK char listSeems the character list in the CJK section of the StandardTokenizer.jj is not quite complete. Following is a more complete list:

< CJK:                                          // non-alphabets
      [
	   ""\u1100""-""\u11ff"",
       ""\u3040""-""\u30ff"",
       ""\u3130""-""\u318f"",
       ""\u31f0""-""\u31ff"",
       ""\u3300""-""\u337f"",
       ""\u3400""-""\u4dbf"",
       ""\u4e00""-""\u9fff"",
       ""\uac00""-""\ud7a3"",
       ""\uf900""-""\ufaff"",
       ""\uff65""-""\uffdc""       
      ]
  >

"
0,"stop writing shared doc stores across segmentsShared doc stores enables the files for stored fields and term vectors to be shared across multiple segments.  We've had this optimization since 2.1 I think.

It works best against a new index, where you open an IW, add lots of docs, and then close it.  In that case all of the written segments will reference slices a single shared doc store segment.

This was a good optimization because it means we never need to merge these files.  But, when you open another IW on that index, it writes a new set of doc stores, and then whenever merges take place across doc stores, they must now be merged.

However, since we switched to shared doc stores, there have been two optimizations for merging the stores.  First, we now bulk-copy the bytes in these files if the field name/number assignment is ""congruent"".  Second, we now force congruent field name/number mapping in IndexWriter.  This means this optimization is much less potent than it used to be.

Furthermore, the optimization adds *a lot* of hair to IndexWriter/DocumentsWriter; this has been the source of sneaky bugs over time, and causes odd behavior like a merge possibly forcing a flush when it starts.  Finally, with DWPT (LUCENE-2324), which gets us truly concurrent flushing, we can no longer share doc stores.

So, I think we should turn off the write-side of shared doc stores to pave the path for DWPT to land on trunk and simplify IW/DW.  We still must support reading them (until 5.0), but the read side is far less hairy."
0,"FieldCacheSanityChecker called directly by FieldCache.get*As suggested by McCandless in LUCENE-1749, we can make FieldCacheImpl a client of the FieldCacheSanityChecker and have it sanity check itself each time it creates a new cache entry, and log a warning if it thinks there is a problem.  (although we'd probably only want to do this if the caller has set some sort of infoStream/warningStream type property on the FieldCache object."
0,"Cosmetic JavaDoc updatesI've taken the liberty of making a few cosmetic updates to various JavaDocs:

* MergePolicy (minor cosmetic change)
* LogMergePolicy (minor cosmetic change)
* IndexWriter (major cleanup in class description, changed anchors to JavaDoc links [now works in Eclipse], no content change)

Attached diff from SVN r780545.

I would appreciate if whomever goes over this can let me know if my issue parameter choices were correct (yeah, blame my OCD), and if there's a more practical/convenient way to send these in, please let me know :-)
"
0,"remove dead code from oal.util.cacheWe have dead cache impls in oal.util.cache*; we only use DBLRUCache.

These are internal APIs; I'd like to remove all but DBLRUcache."
1,"Unable to set LockFactory implementation via ${org.apache.lucene.store.FSDirectoryLockFactoryClass}While trying to move from Lucene 2.0 to Lucene 2.1 I noticed a problem with the LockFactory instantiation code.
During previous tests we successfully specified the LockFactory implementation by setting the property
${org.apache.lucene.store.FSDirectoryLockFactoryClass} to ""org.apache.lucene.store.NativeFSLockFactory"".
This does no longer work due to a bug in the FSDirectory class. The problem is caused from the fact that this
class tries to invoke the default constructor of the specified LockFactory class. However neither NativeFSLockFactory
nor SimpleFSLockFactory do have a default constructor.

FSDirectory, Line 285:
          try {
            lockFactory = (LockFactory) c.newInstance();          
          } catch (IllegalAccessException e) {
            throw new IOException(""IllegalAccessException when instantiating LockClass "" + lockClassName);
          } catch (InstantiationException e) {
            throw new IOException(""InstantiationException when instantiating LockClass "" + lockClassName);
          } catch (ClassCastException e) {
            throw new IOException(""unable to cast LockClass "" + lockClassName + "" instance to a LockFactory"");
          }

A possible workaround is to not set the property at all and call FSDirectory.setLockFactory(...) instead. "
0,"TieredMergePolicy should expose control over how aggressively segments with deletions are targeted TMP today always does a linear pro-rating of a merge's score according to what pctg of the documents are deleted; I'd like to 1) put a power factor in (score is multiplicative), and 2) default it to stronger favoring of merging away deletions."
0,"Remove ExtendedFieldCache by rolling functionality into FieldCacheIt is silly that we have ExtendedFieldCache.  It is a workaround to our supposed back compatibility problem.  This patch will merge the ExtendedFieldCache interface into FieldCache, thereby breaking back compatibility, but creating a much simpler API for FieldCache."
0,"Extended javadocs in spellcheckerAdded some javadocs that explains why the spellchecker does not work as one might expect it to.

http://www.nabble.com/SpellChecker%3A%3AsuggestSimilar%28%29-Question-tf3118660.html#a8640395

> Without having looked at the code for a long time, I think the problem is what the
> lucene scoring consider to be best. First the grams are searched, resulting in a number
> of hits. Then the edit-distance is calculated on each hit. ""Genetics"" is appearently the
> third most similar hit according to Lucene, but the best according to Levenshtein.
>
> I.e. Lucene does not use edit-distance as similarity. You need to get a bunch of best hits
> in order to find the one with the smallest edit-distance.

I took a look at the code, and my assessment seems to be right."
0,Remove deprecated query componentsRemove the rest of the deprecated query components.
0,"remove field param from computeNorm, scorePayload ; remove UOE'd lengthNorm, switch SweetSpot to per-field In LUCENE-2236 we switched sim to per field (SimilarityProvider returns a per-field similarity).

But we didn't completely cleanup there... I think we should now do this:
* SweetSpotSimilarity loses all its hashmaps. Instead, just configure one per field and return it in your SimilarityProvider. this means for example, all its TF factors can now be configured per-field too, not just the length normalization factors.
* computeNorm and scorePayload lose their field parameter, as its redundant and confusing.
* the UOE'd obselete lengthNorm is removed. I also updated javadocs that were pointing to it (this is bad!).

"
0,"Use Float.floatToRawIntBits over Float.floatToIntBitsCopied From my Email:
  Float.floatToRawIntBits (in Java1.4) gives the raw float bits without
normalization (like *(int*)&floatvar would in C).  Since it doesn't do
normalization of NaN values, it's faster (and hopefully optimized to a
simple inline machine instruction by the JVM).

On my Pentium4, using floatToRawIntBits is over 5 times as fast as
floatToIntBits.
That can really add up in something like Similarity.floatToByte() for
encoding norms, especially if used as a way to compress an array of
float during query time as suggested by Doug."
1,IndexReader.undeleteAll can mess up the deletion count stored in the segments fileSpinoff from LUCENE-1474.  I'll attach a test case showing the issue.
0,"Optimize SegmentMerger to work on atomic (Segment)Readers where possibleThis is a spin-off from LUCENE-2769:

Currently SegmentMerger has some optimizations when it merges segments that are SegmentReaders (e.g. when doing normal indexing or optimizing). But when you do IndexWriter.addIndexes(IndexReader...) the listed IndexReaders may not really be per-segment. SegmentMerger should track down all passed in reads down to the lowest level (Segment)Reader (or other atomic readers like SlowMultiReaderWrapper) and then merge. We can then remove most MultiFields usage (except term merging itsself) and clean up the code.

This especially saves lots of memory for merging norms, as no longer the duplicate norms arrays are created when MultiReaders are used!"
1,"BooleanQuery.rewrite does not work properly for minNumberShouldMatchBooleanQuery.rewrite does not respect minNumberShouldMatch if the number of clauses is 1. This causes inconsistencies for the queries ""+def"" and ""+abc +def"", while setting the minNumShouldMatch to '1' for both.
For the first query, results are returned although there are no SHOULD clauses in the query.
For the second query no results are returned.
The reason lies in the optimization BooleanQuery.rewrite has for one clauses queries.
Patch included - optimize the query for a single clause only if the minNumShouldMatch <= 0."
1,"Merging multiple indexes does not maintain document order.When I merge multiple indexes into a single, empty index, the document addition order is not being maintained.

Self contained test case coming (as soon as I figure out how to attach it)"
0,Use IndexWriterConfig in benchmarkWe should use IndexWriterConfig instead of deprecated methods in benchmark. 
0,"Add support for Ideographic Space to the queryparser - also know as fullwith space and wide-spaceThe Ideographic Space is a space character that is as wide as a normal CJK character cell.
It is also known as wide-space or fullwith space.This type of space is used in CJK languages.

This patch adds support for the wide space, making the queryparser component more friendly
to queries that contain CJK text.

Reference:
'http://en.wikipedia.org/wiki/Space_(punctuation)' - see Table of spaces, char U+3000.

I also added a new testcase that fails before the patch.
After the patch is applied all junits pass."
0,"isOpen needs to be accessible by subclasses of DirectoryThe Directory abstract class has a member variable named isOpen which is package accessible. The usage of the variable is such that it should be readable and must be writable (in order to implement close())  by any concrete implementation of directory. Because of the current accessibility of this variable is is not possible to create a Directory implementation that is not also in the org.apache.lucene.store.

I propose that either the isOpen variable either needs to be declared protected or that there should be getter/setter methods that are protected."
0,Enable Java asserts in the Junit testsFor background see http://www.mail-archive.com/java-dev@lucene.apache.org/msg10307.html
0,"CollationKeyFilter: convert tokens into CollationKeys encoded using IndexableBinaryStringToolsConverts each token into its CollationKey using the provided collator, and then encodes the CollationKey with IndexableBinaryStringTools, to allow it to be stored as an index term.

This will allow for efficient range searches and Sorts over fields that need collation for proper ordering.
"
0,"CheckIndex prints wrong version number on 3.1 indexes (and posibly also in trunk)When you run CheckIndex on an index created/updated with 3.1, it prints about the SegmentInfos:

{noformat}
Segments file=segments_g19 numSegments=5 version=-11 [Lucene 1.3 or prior]
{noformat}

We should fix CheckIndex and also verify other cases where version numbers are printed out. In trunk the issue may be more complicated!"
0,"[PATCH] BufferedIndexOutput - optimized writeBytes() methodI have created a patch that optimize writeBytes metod:

  public void writeBytes(byte[] b, int length) throws IOException {
    if (bufferPosition > 0) // flush buffer
      flush();
 
    if (length < BUFFER_SIZE) {
      flushBuffer(b, length);
      bufferStart += length;
    } else {
      int pos = 0;
      int size;
      while (pos < length) {
        if (length - pos < BUFFER_SIZE) {
          size = length - pos;
        } else {
          size = BUFFER_SIZE;
        }
        System.arraycopy(b, pos, buffer, 0, size);
        pos += size;
        flushBuffer(buffer, size);
        bufferStart += size;
      }
    }
  }

Its a much more faster now. I know that for indexing this not help much, but for copying files in the IndexStore this is so big improvement. Its about 400% faster that old implementation.

The patch was tested with 300MB data, ""ant test"" sucessfuly finished with no errors."
1,"position increment bug: smartcnIf i use LUCENE_VERSION >= 2.9 with smart chinese analyzer, it will crash indexwriter with any reasonable amount of chinese text.

its especially annoying because it happens in 2.9.1 RC as well.

this is because the position increments for tokens after stopwords are bogus:

Here's an example (from test case), where the position increment should be 2, but is instead 91975314!

{code}
  public void testChineseStopWords2() throws Exception {
    Analyzer ca = new SmartChineseAnalyzer(Version.LUCENE_CURRENT); /* will load stopwords */
    String sentence = ""Title:San""; // : is a stopword
    String result[] = { ""titl"", ""san""};
    int startOffsets[] = { 0, 6 };
    int endOffsets[] = { 5, 9 };
    int posIncr[] = { 1, 2 };
    assertAnalyzesTo(ca, sentence, result, startOffsets, endOffsets, posIncr);
  }
{code}

junit.framework.AssertionFailedError: posIncrement 1 expected:<2> but was:<91975314>
	at junit.framework.Assert.fail(Assert.java:47)
	at junit.framework.Assert.failNotEquals(Assert.java:280)
	at junit.framework.Assert.assertEquals(Assert.java:64)
	at junit.framework.Assert.assertEquals(Assert.java:198)
	at org.apache.lucene.analysis.BaseTokenStreamTestCase.assertTokenStreamContents(BaseTokenStreamTestCase.java:83)
	...




"
0,"Add meta keywords to HTMLParser
It would be good if the HTMLParser could give us the keywords specified in the meta tags, so that we can index them.

In HTMLParser.jj:

  void addMetaTag() {
      metaTags.setProperty(currentMetaTag, currentMetaContent);
      currentMetaTag = null;
      currentMetaContent = null;
      return;
  }

One way to do it:

  void addMetaTag() throws IOException {
      metaTags.setProperty(currentMetaTag, currentMetaContent);
      if (currentMetaTag.equalsIgnoreCase(""keywords"")) {
          pipeOut.write(currentMetaContent);
      }
      currentMetaTag = null;
      currentMetaContent = null;
      return;
  }
"
0,"FuzzyQuery should not be finalI am trying to extend the FuzzyQuery to further filter the TermEnum. (I am indexing stem forms and original forms, but I only want to match original forms with a fuzzy term, otherwise I get to much noise). However, FuzzyQuery is a final class and I cannot extend it.  

As discussed in the mailing list (http://www.gossamer-threads.com/lists/lucene/java-dev/38756), we want to make the private variables and inner classes protected.

I am attaching a patch for FuzzyQuery.java that implements this. I ran all unit tests and they passed without errors.

Andreas."
1,benchmark alg can't handle negative relative priorityWe can now set thread relative priority when we run BG tasks... but if you use a negative number it's parsing it as 0.
1,"Constants causing NullPointerException when fetching metadata ""Implementation Version"" in MANIFESTIf the MANIFEST.MF file does not contain the metadata IMPLEMENTATION_VERSION, a null value is returned, causing NullPointerException during commit:

Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.lucene.store.IndexOutput.writeString(IndexOutput.java:109)
	at org.apache.lucene.store.IndexOutput.writeStringStringMap(IndexOutput.java:229)
	at org.apache.lucene.index.SegmentInfo.write(SegmentInfo.java:558)
	at org.apache.lucene.index.SegmentInfos.write(SegmentInfos.java:337)
	at org.apache.lucene.index.SegmentInfos.prepareCommit(SegmentInfos.java:808)
	at org.apache.lucene.index.IndexWriter.startCommit(IndexWriter.java:5319)
	at org.apache.lucene.index.IndexWriter.prepareCommit(IndexWriter.java:3895)
	at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:3956)

This happened after having build a jar assembly using Maven, the original MANIFEST.MF of lucene jar has been overwritten, and didn't contain anynore the implementation version metadata.

Path attached."
1,"EnwikiContentSource does not properly identify the name/id of the Wikipedia articleThe EnwikiContentSource does not properly identify the id (name in benchmark parlance) of the documents.  It currently produces assigns the id on the last <id> tag it sees in the document, as opposed to the id of the document.  Most documents have multiple <id> tags in them.  This prevents the ContentSource from being used effectively in producing documents for updating.

Example doc:
{quote}
<page>
    <title>AlgeriA</title>
    <id>5</id>
    <revision>
      <id>133452200</id>
      <timestamp>2007-05-25T17:11:48Z</timestamp>
      <contributor>
        <username>Gurch</username>
        <id>241822</id>
      </contributor>
      <minor />
      <comment>[[WP:AES|â<86><90>]]Redirected page to [[Algeria]]</comment>
      <text xml:space=""preserve"">#REDIRECT [[Algeria]] {{R from CamelCase}}</text>
    </revision>
  </page>
{quote}

In this case, the getName() return 241822 instead of 5.  page/id is unique according to the schema at  http://www.mediawiki.org/xml/export-0.3.xsd, so we should just get that one."
1,"ThaiAnalyzer assumes things about your jreThe ThaiAnalyzer/ThaiWordFilter depends on the fact that BreakIterator.getWordInstance(new Locale(""th"")) returns a dictionary-based break iterator that can segment thai phrases into words (it does not use whitespace).

But this is non-standard that the JRE will specialize this locale in this way, its nice, but you can't depend on it.
For example, if you are running on IBM JRE, this analyzer/wordfilter is completely ""broken"" in the sense it won't do what it claims to do.

At the minimum, we need to document this and suggest users look at ICUTokenizer for thai, which always has this breakiterator and is not jre-dependent.

Better, would be to check statically that the thing actually works.
when creating a new ThaiWordFilter we could clone() the BreakIterator, which is often cheaper than making a new one anyway.
we could throw an exception, if its not supported, and add a boolean so the user knows it works.
and we could refer to this boolean with Assert.assume in its tests.
"
0,"Don't use ensureOpen() excessively in IndexReader and IndexWriterA spin off from here: http://www.nabble.com/Excessive-use-of-ensureOpen()-td24127806.html.

We should stop calling this method when it's not necessary for any internal Lucene code. Currently, this code seems to hurt properly written apps, unnecessarily.

Will post a patch soon"
1,"BytesRef reuse bugs in QueryParser and analysis.jspSome code uses BytesRef as if it were a ""String"", in this case consumers of TermToBytesRefAttribute.
The thing is, while our general implementation works on char[] and then populates the consumers BytesRef,
not all TermToBytesRefAttribute implementations do this, specifically ICU collation, it reuses the bytes and simply sets the pointers:
{noformat}
  @Override
  public int toBytesRef(BytesRef target) {
    collator.getRawCollationKey(toString(), key);
    target.bytes = key.bytes;
    target.offset = 0;
    target.length = key.size;
    return target.hashCode();
  }
{noformat}

Most of the blame falls on me as I added this to the queryparser in LUCENE-2514.

Attached is a patch so that these consumers re-use a 'spare' and copy the bytes when they are going to make a long lasting object such as a Term.
"
0,"lazily create SegmentMergeInfo.docMapSince creating the docMap is expensive, and it's only used during segment merging, not searching, defer creation until it is requested.

SegmentMergeInfo is also used in MultiTermEnum, the term enumerator for a MultiReader.  TermEnum is used by queries such as PrefixQuery, RangeQuery, WildcardQuery, as well as RangeFilter, DateFilter, and sorting the first time (filling the FieldCache).

Performance Results:
  A simple single field index with 555,555 documents, and 1000 random deletions was queried 1000 times with a PrefixQuery matching a single document.

Performance Before Patch:
  indexing time = 121,656 ms
  querying time = 58,812 ms

Performance After Patch:
  indexing time = 121,000 ms
  querying time =         598 ms

A 100 fold increase in query performance!

All lucene unit tests pass."
1,"MMapDirectory is missing newly added openInput method to FSDirectoryThis issue was caused by the optimizations in LUCENE-888.  The new
openInput(String name, int bufferSize) added to FSDirectory was not
also overridden by MMapDirectory.
"
1,"FSDirectory.copyBytes isn't safe for SimpleFSDirectorythe copyBytes optimization from LUCENE-2574 is not safe for SimpleFSDirectory, but works fine for NIOFSDirectory.

With SimpleFSDirectory, the copyBytes optimization causes index corruption.

see http://www.lucidimagination.com/search/document/36d2dbfc691909d5/bug_triggered_by_testindexwriter_testrandomstoredfields for background

here are my steps to reproduce (most of the time, at least on windows):
{noformat}
1. edit line 87 of TestIndexWriter to plugin the seed:
    random = newRandom(3312389322103990899L);
2. edit line 5138 of TestIndexWriter to force SimpleFSDirectory:
    Directory dir = new SimpleFSDirectory(index);
3. run this command:
    ant clean test-core -Dtestcase=TestIndexWriter
-Dtestmethod=testRandomStoredFields -Dtests.iter=10
-Dtests.codec=""MockVariableIntBlock(29)""
{noformat}
"
1,"The creation of a spell index from a LuceneDictionary via SpellChecker.indexDictionary (Dictionary dict) fails starting with 1.9.1 (up to current svn version)Two different errors in 1.9.1/2.0.0 and current svn version.

1.9.1/2.0.0:
at the end of indexDictionary (Dictionary dict) 
the IndexReader-instance reader is closed.
This causes a NullpointerException because reader has not been initialized before (neither in that method nor in the constructor).
Uncommenting this line (reader.close()) seems to resolve that issue.

current svn:
the constructor tries to create an IndexSearcher-instance for the specified path;
as there is no index in that path - it is not created yet -  an exception is thrown.

"
0,"search.function - (1) score based on field value, (2) simple score customizabilityFunctionQuery can return a score based on a field's value or on it's ordinal value.

FunctionFactory subclasses define the details of the function.  There is currently a LinearFloatFunction (a line specified by slope and intercept).

Field values are typically obtained from FieldValueSourceFactory.  Implementations include FloatFieldSource, IntFieldSource, and OrdFieldSource."
0,"Changes for TrieRange in FilteredTermEnum and MultiTermQuery improvementThis is a patch, that is needed for the MultiTermQuery-rewrite of TrieRange (LUCENE-1602):
- Make the private members protected, to have access to them from the very special TrieRangeTermEnum 
- Fix a small inconsistency (docFreq() now only returns a value, if a valid term is existing)
- Improvement of MultiTermFilter.getDocIdSet to return DocIdSet.EMPTY_DOCIDSET, if the TermEnum is empty (less memory usage) and faster.
- Add the getLastNumberOfTerms() to MultiTermQuery for statistics on different multi term queries and how may terms they affect, using this new functionality, the improvement of TrieRange can be shown (extract from test case there, 10000 docs index, long values):
{code}
[junit] Average number of terms during random search on 'field8':
[junit]  Trie query: 244.2
[junit]  Classical query: 3136.94
[junit] Average number of terms during random search on 'field4':
[junit]  Trie query: 38.3
[junit]  Classical query: 3018.68
[junit] Average number of terms during random search on 'field2':
[junit]  Trie query: 18.04
[junit]  Classical query: 3539.42
{code}

All core tests pass.
"
0,"disable atime for DirectIOLinuxDirectoryIn Linux's open():
O_NOATIME
    (Since Linux 2.6.8) Do not update the file last access time (st_atime in the inode) when the file is read(2). This flag is intended for use by indexing or backup programs, where its use can significantly reduce the amount of disk activity. This flag may not be effective on all filesystems. One example is NFS, where the server maintains the access time.

So we should do this in our linux-specific DirectIOLinuxDirectory.

Separately (offtopic), it would be better if this was a LinuxDirectory that only uses O_DIRECT when it should :)
It would be nice to think about an optional modules/native for common platforms similar to what tomcat provides
Its easier to test directories like this now (-Dtests.directory)...
"
0,"Javadocs should explain possible causes for IOExceptions
Most methods in Lucene reserve the right to throw an IOException.  This can occur for nearly all methods from low level problems like wrong permissions, transient IO errors, bad hard drive or corrupted file system, corrupted index, etc, but for some methods there are also more interesting causes that we should try to document.

Spinoff of this thread:

    http://www.gossamer-threads.com/lists/lucene/java-user/44929"
0,"Improve readability of StandardTermsDictWriterOne variable is named indexWriter, but it is a termsIndexWriter. Also some layout."
0,FieldDoc.toString only returns super.toStringThe FieldDoc.toString method very carefully builds a StringBuffer sb containing the information for the FieldDoc instance and then just returns super.toString() instead of sb.toString()
0,"SearcherManager and NRTManager should be in the same packageI didnt even know NRTManager was still around, because its in the .index package, whereas SearcherManager is in the .search package.

Separately, I don't like that this stuff is so 'hard' with core lucene... would it be so bad if this stuff was added to core?

I suspect a lot of people have issues with this stuff (see http://www.lucidimagination.com/search/document/37964e5f0e5d733b) for example.

Worst case is just that, combine mistakes with trying to manage this stuff with MMap unmapping and total lack of error detection
for searching closed readers (LUCENE-3439) and its a mess.
"
0,"contrib/benchmark unit testsThe need came up in this thread: 
http://www.mail-archive.com/java-dev@lucene.apache.org/msg09260.html

: We might want to start thinking about Unit Tests...  :-)  Seems kind
: of weird to have tests for tests, but this is becoming sufficiently
: complex that it should have some tests.
"
0,"make the build more friendly to apache harmonyas part of improved testing, i thought it would be a good idea to make the build (ant test) more friendly
to working under apache harmony.

i'm not suggesting we de-optimize code for sun jvms or anything crazy like that, only use it as a tool.

for example:
* bugs in tests/code: for example i found a test that expected ArrayIOOBE 
  when really the javadoc contract for the method is just IOOBE... it just happens to
  pass always on sun jvm because thats the implementation it always throws.
* better reproduction of bugs: for example [2 months out of the year|http://en.wikipedia.org/wiki/Unusual_software_bug#Phase_of_the_Moon_bug]
  it seems TestQueryParser fails with thai locale in a difficult-to-reproduce way.
  but i *always* get similar failures like this with harmony for this test class.
* better stability and portability: we should try (if reasonable) to avoid depending
  upon internal details. the same kinds of things that fail in harmony might suddenly
  fail in a future sun jdk. because its such a different impl, it brings out a lot of interesting stuff.

at the moment there are currently a lot of failures, I think a lot might be caused by this: http://permalink.gmane.org/gmane.comp.java.harmony.devel/39484
"
0,Refactoring and slight extension of regex testing code.
0,"Typo on query parser syntax web page.On the web page http://lucene.apache.org/java/docs/queryparsersyntax.html#N10126 the text says:

""To search for documents that must contain ""jakarta"" and may contain ""lucene"" use the query:""

The example says:

+jakarta apache

The problem:
The example uses apache where the text says lucene."
0,"Complete parallelizaton of ParallelMultiSearcherParallelMultiSearcher is parallel only for the method signatures of 'search'.

Part of a query process calls the method docFreq(). There was a TODO comment to parallelize this. Parallelizing this method actually increases the performance of a query on multiple indexes, especially remotely.

"
1,"BooleanQuery assumes everything else implements skipToskipTo seems to be optional functionality on the Scorer class (BooleanScorer
doesn't implement it).  BooleanQuery.scorer() tests all subclauses using
""instanceof BooleanQuery"" to determine if it can use a ConjunctionScorer that
requires skipTo functionality.

This means that any other new Query/Scorer that don't implement skipTo will get
into trouble when included in a BooleanQuery.

If skipTo is really optional, then there should be some way of telling by the
Scorer or the Query in a more generic manner.

Some options:
1) have a ""boolean Scorer.hasSkipTo()"" method
2) have a ""boolean Query.hasSkipTo()"" method
3) remove Scorer.skipTo and have a ""public interface ScorerSkipTo{boolean
skipTo(int doc)}"" that scorers may implement"
1,"""fdx size mismatch"" exception in StoredFieldsWriter.closeDocStore() when closing index with 500M documentsWhen closing index that contains 500,000,000 randomly generated documents, an exception is thrown:

java.lang.RuntimeException: after flush: fdx size mismatch: 500000000 docs vs 4000000004 length in bytes of _0.fdx
	at org.apache.lucene.index.StoredFieldsWriter.closeDocStore(StoredFieldsWriter.java:94)
	at org.apache.lucene.index.DocFieldConsumers.closeDocStore(DocFieldConsumers.java:83)
	at org.apache.lucene.index.DocFieldProcessor.closeDocStore(DocFieldProcessor.java:47)
	at org.apache.lucene.index.DocumentsWriter.closeDocStore(DocumentsWriter.java:367)
	at org.apache.lucene.index.IndexWriter.flushDocStores(IndexWriter.java:1688)
	at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:3518)
	at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3442)
	at org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:1623)
	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1588)
	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1562)
        ...

This appears to be a bug at StoredFieldsWriter.java:93:

      if (4+state.numDocsInStore*8 != state.directory.fileLength(state.docStoreSegmentName + ""."" + IndexFileNames.FIELDS_INDEX_EXTENSION))

where the multiplication by 8 is causing integer overflow. The fix would be to cast state.numDocsInStore to long before multiplying.

It appears that this is another instance of the mistake that caused bug LUCENE-1519. I did a cursory seach for \*8 against the code to see if there might be yet more instances of the same mistake, but found none. 

"
0,"Provide Summary Information on the Files in the Lucene indexI find myself often having to remember, by file extension, what is in a particular index file.  The information is all contained in the File Formats, but not summarized.  This patch provides a simple table that describes the extensions and provides links to the relevant section."
0,"IndexReader currently has javadoc errorsCurrent trunk has some javadoc errors in IndexReader and some more in contrib.
"
0,Add Highlighter test for RegexQuery
0,Consolidate Solr's and Lucene's OpenBitSet classesSee SOLR-875 for details.
0,"Wrap messages output with a check of InfoStream != nullI've found several places in the code where messages are output w/o first checking if infoStream != null. The result is that in most of the time, unnecessary strings are created but never output (because infoStream is not set). We should follow Java's logging best practices, where a log message is always output in the following format:
if (logger.isLoggable(leve)) {
    logger.log(level, msg);
}

Log messages are usually created w/o paying too much attention to performance (such as string concatenation using '+' instead of StringBuffer). Therefore, at runtime it is important to avoid creating those messages, if they will be discarded eventually.

I will add a method to IndexWriter messagesEnabled() and then use it wherever a call to iw.message() is made.

Patch will follow"
1,"IndexSearcher fails to pass docBase to Collector when using ExecutorServiceThis bug is causing the failure in TestSearchAfter.

We are now always passing docBase 0 to Collector when you use ExecutorService with IndexSearcher.

This doesn't affect trunk (AtomicReaderContext carries the right docBase); only 3.x.
"
0,"TestIndexWriterNRTIsCurrent failurefound by jenkins: 

https://builds.apache.org/job/Lucene-Solr-tests-only-trunk/12492/

make your computer busy (e.g. run tests in another checkout) then,

ant test-core -Dtests.iter=100 -Dtestcase=TestIndexWriterNRTIsCurrent -Dtestmethod=testIsCurrentWithThreads -Dtests.seed=-78f6fa16b849cf27:382126da79c1e146:-d2cdec79e86e1b3 -Dtests.multiplier=3 -Dargs=""-Dfile.encoding=ISO8859-1""

takes a few tries till it pops...

{noformat}
junit-sequential:
    [junit] Testsuite: org.apache.lucene.index.TestIndexWriterNRTIsCurrent
    [junit] Tests run: 100, Failures: 1, Errors: 1, Time elapsed: 277.818 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] WARNING: you are using -Dtests.iter=n where n > 1, not all tests support this option.
    [junit] Some may crash or fail: this is not a bug.
    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterNRTIsCurrent -Dtestmethod=testIsCurrentWithThreads -Dtests.seed=-78f6fa16b849cf27:382126da79c1e146:-d2cdec79e86e1b3 -Dtests.multiplier=3 -Dargs=""-Dfile.encoding=ISO8859-1""
    [junit] The following exceptions were thrown by threads:
    [junit] *** Thread: Lucene Merge Thread #17 ***
    [junit] org.apache.lucene.index.MergePolicy$MergeException: java.lang.AssertionError
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:520)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:480)
    [junit] Caused by: java.lang.AssertionError
    [junit] 	at org.apache.lucene.index.IndexWriter$ReadersAndLiveDocs.initWritableLiveDocs(IndexWriter.java:580)
    [junit] 	at org.apache.lucene.index.IndexWriter.commitMergedDeletes(IndexWriter.java:3061)
    [junit] 	at org.apache.lucene.index.IndexWriter.commitMerge(IndexWriter.java:3137)
    [junit] 	at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3718)
    [junit] 	at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3257)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:382)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:451)
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterNRTIsCurrent -Dtestmethod=testIsCurrentWithThreads -Dtests.seed=-78f6fa16b849cf27:382126da79c1e146:-d2cdec79e86e1b3 -Dtests.multiplier=3 -Dargs=""-Dfile.encoding=ISO8859-1""
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterNRTIsCurrent -Dtestmethod=testIsCurrentWithThreads -Dtests.seed=-78f6fa16b849cf27:382126da79c1e146:-d2cdec79e86e1b3 -Dtests.multiplier=3 -Dargs=""-Dfile.encoding=ISO8859-1""
    [junit] NOTE: test params are: codec=Lucene40: {id=MockFixedIntBlock(blockSize=525)}, sim=DefaultSimilarity, locale=es_PY, timezone=Africa/Luanda
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestIndexWriterNRTIsCurrent]
    [junit] NOTE: Linux 3.0.0-14-generic amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=8,threads=1,free=74907448,total=255787008
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testIsCurrentWithThreads(org.apache.lucene.index.TestIndexWriterNRTIsCurrent):	FAILED
    [junit] info=_qx(4.0):C1/1 isn't live
    [junit] junit.framework.AssertionFailedError: info=_qx(4.0):C1/1 isn't live
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:165)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)
    [junit] 	at org.apache.lucene.index.IndexWriter$ReaderPool.infoIsLive(IndexWriter.java:663)
    [junit] 	at org.apache.lucene.index.IndexWriter$ReaderPool.dropAll(IndexWriter.java:717)
    [junit] 	at org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:1136)
    [junit] 	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1069)
    [junit] 	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1033)
    [junit] 	at org.apache.lucene.index.TestIndexWriterNRTIsCurrent.testIsCurrentWithThreads(TestIndexWriterNRTIsCurrent.java:68)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$SubclassSetupTeardownRule$1.evaluate(LuceneTestCase.java:707)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$InternalSetupTeardownRule$1.evaluate(LuceneTestCase.java:606)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$TestResultInterceptorRule$1.evaluate(LuceneTestCase.java:511)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$RememberThreadRule$1.evaluate(LuceneTestCase.java:569)
    [junit] 
    [junit] 
    [junit] Testcase: testIsCurrentWithThreads(org.apache.lucene.index.TestIndexWriterNRTIsCurrent):	Caused an ERROR
    [junit] java.lang.AssertionError: Some threads threw uncaught exceptions!
    [junit] java.lang.RuntimeException: java.lang.AssertionError: Some threads threw uncaught exceptions!
    [junit] 	at org.apache.lucene.util.LuceneTestCase.tearDownInternal(LuceneTestCase.java:780)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.access$1000(LuceneTestCase.java:138)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$InternalSetupTeardownRule$1.evaluate(LuceneTestCase.java:607)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$TestResultInterceptorRule$1.evaluate(LuceneTestCase.java:511)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$RememberThreadRule$1.evaluate(LuceneTestCase.java:569)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:165)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.checkUncaughtExceptionsAfter(LuceneTestCase.java:808)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.tearDownInternal(LuceneTestCase.java:752)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.index.TestIndexWriterNRTIsCurrent FAILED

BUILD FAILED
{noformat}"
0,tests for verifying that assertions are enabled do nothing since they ignore AssertionErrorFollow-up from LUCENE-3501
0,"Support inclusive/exclusive for TrieRangeQuery/-Filter, remove default trie variant setters/gettersTrieRangeQuery/Filter is missing one thing: Ranges that have exclusive bounds. For TrieRangeQuery this may not be important for ranges on long or Date (==long) values (because [1..5] is the same like ]0..6[ or ]0..5]). This is not so simple for doubles because you must add/substract 1 from the trie encoded unsigned long.

To be conform with the other range queries, I will submit a patch that has two additional boolean parameters in the ctors to support inclusive/exclusive ranges for both ends. Internally it will be implemented using TrieUtils.incrementTrieCoded/decrementTrieCoded() but makes life simplier for double ranges (a simple exclusive replacement for the floating point range [0.0..1.0] is not possible without having the underlying unsigned long).

In December, when trie contrib was included (LUCENE-1470), 3 trie variants were supplied by TrieUtils. For new APIs a statically configureable default Trie variant does not conform to an API we want in Lucene (currently we want to deprecate all these static setters/getters). The important thing: It does not make code shorter or easier to understand, its more error prone. Before release of 2.9 it is a good time to remove the default trie variant and always force the parameter in TrieRangeQuery/Filter. It is better to choose the variant in the application and do not automatically manage it.

As Lucene 2.9 was not yet released, I will change the ctors and not preserve the old ones."
0,"Some small fixes to contrib/benchmarkI've fixed a few small issues I've hit in contrib/benchmark.

First, this alg was only doing work on the first round.  All
subsequent rounds immediately finished:

{code}
analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
doc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker
work.dir = /lucene/work
docs.file=work/reuters.lines.txt
doc.maker.forever=false
directory=FSDirectory
doc.add.log.step=3000

{ ""Rounds""
  ResetSystemErase
  CreateIndex
  { ""AddDocs"" AddDoc > : *
  CloseIndex
  NewRound
} : 3
{code}

I think this is because we are failing to reset ""exhausted"" to false
in PerfTask.doLogic(), so I added that.  Plus I had to re-open the
file in LineDocMaker.

Second, I made a small optimization to not call updateExhausted unless
any of the child tasks are TaskSequence or ResetInputsTask (which I
compute up-front).

Finally, we were not allowing flushing by RAM and doc count, so I
fixed the logic in Create/OpenIndexTask to set both RAMBufferSizeMB
and MaxBufferedDocs.
"
0,"New tool for  reseting the (length)norm of fields after changing SimilarityI've written a little tool that seems like it can/will be very handy as I tweak my custom similarity.  I think it would make a good addition to contrib/miscellaneous.

Class and Tests to be attached shortly..."
0,"IndexReader # doCommit - typo nit about v3.0 in trunkTrunk is already in 3.0.1+ . But the documentation says -  ""In 3.0, this will become ... "".  Since it is already in 3.0, it might as well be removed. 
"
0,"divorce defaultsimilarityprovider from defaultsimilarityIn LUCENE-2236 as a start, we made DefaultSimilarity which implements the factory interface (SimilarityProvider), and also extends Similarity.

Its factory interface just returns itself always by default.

Doron mentioned it would be cleaner to split the two, and I thought it would be good to revisit it later.

Today as I was looking at SOLR-2338, it became pretty clear that we should do this, it makes things a lot cleaner. I think currently its confusing to users to see the two apis mixed if they are trying to subclass.
"
0,"add LuceneTestCase[J4].newFieldI think it would be good to vary the different field options in tests.

For example, we do this with IW settings (newIndexWriterConfig), and directories (newDirectory).

This patch adds newField(), it works just like new Field(), except it will sometimes turns on extra options:
Stored fields, term vectors, additional term vectors data, etc.
"
0,"make applying deletes optional when pulling a new NRT readerUsually when you pull an NRT reader, you want all deletes to be applied.

But in some expert cases you may not need it (eg you just want to validate that the doc was indexed).  Since it's costly to apply deletes, and trivial to add this boolean (we already have a boolean internally), I think we should add it.

The deletes are still buffered, and you can always later pull another reader (for ""real"" searching) with deletes applied."
0,"DisjunctionMaxScorer allocates 2 arrays per scored docIt has this:
{noformat}
  @Override
  public float score() throws IOException {
    int doc = subScorers[0].docID();
    float[] sum = { subScorers[0].score() }, max = { sum[0] };
    int size = numScorers;
    scoreAll(1, size, doc, sum, max);
    scoreAll(2, size, doc, sum, max);
    return max[0] + (sum[0] - max[0]) * tieBreakerMultiplier;
  }
{noformat}

They are thread-private arrays so possibly/likely JVM can optimize this case (allocate only on the stack) but still I think instead it should have private instance vars for the score/max."
0,Allow tests to use random codec per fieldSince we now have a real per field codec support we should enable to run the tests with a random codec per field. When I change something related to codecs internally I would like to ensure that whatever combination of codecs (except of preflex) I use the code works just fine. I created a RandomCodecProvider in LuceneTestCase that randomly selects the codec for fields when it sees them the first time. I disabled the test by default to leave the old randomize codec support in as it was / is.
0,"New TokenStream APIThis is a very early version of the new TokenStream API that 
we started to discuss here:

http://www.gossamer-threads.com/lists/lucene/java-dev/66227

This implementation is a bit different from what I initially
proposed in the thread above. I introduced a new class called
AttributedToken, which contains the same termBuffer logic 
from Token. In addition it has a lazily-initialized map of
Class<? extends Attribute> -> Attribute. Attribute is also a
new class in a new package, plus several implementations like
PositionIncrementAttribute, PayloadAttribute, etc.

Similar to my initial proposal is the prototypeToken() method
which the consumer (e. g. DocumentsWriter) needs to call.
The token is created by the tokenizer at the end of the chain
and pushed through all filters to the end consumer. The 
tokenizer and also all filters can add Attributes to the 
token and can keep references to the actual types of the
attributes that they need to read of modify. This way, when
boolean nextToken() is called, no casting is necessary.

I added a class called TestNewTokenStreamAPI which is not 
really a test case yet, but has a static demo() method, which
demonstrates how to use the new API.

The reason to not merge Token and TokenStream into one class 
is that we might have caching (or tee/sink) filters in the 
chain that might want to store cloned copies of the tokens
in a cache. I added a new class NewCachingTokenStream that
shows how such a class could work. I also implemented a deep
clone method in AttributedToken and a 
copyFrom(AttributedToken) method, which is needed for the 
caching. Both methods have to iterate over the list of 
attributes. The Attribute subclasses itself also have a
copyFrom(Attribute) method, which unfortunately has to down-
cast to the actual type. I first thought that might be very
inefficient, but it's not so bad. Well, if you add all
Attributes to the AttributedToken that our old Token class
had (like offsets, payload, posIncr), then the performance
of the caching is somewhat slower (~40%). However, if you 
add less attributes, because not all might be needed, then
the performance is even slightly faster than with the old API.
Also the new API is flexible enough so that someone could
implement a custom caching filter that knows all attributes
the token can have, then the caching should be just as 
fast as with the old API.


This patch is not nearly ready, there are lot's of things 
missing:

- unit tests
- change DocumentsWriter to use new API 
  (in backwards-compatible fashion)
- patch is currently java 1.5; need to change before 
  commiting to 2.9
- all TokenStreams and -Filters should be changed to use 
  new API
- javadocs incorrect or missing
- hashcode and equals methods missing in Attributes and 
  AttributedToken
  
I wanted to submit it already for brave people to give me 
early feedback before I spend more time working on this."
1,"MultiPhraseQuery assigns different scores to identical docs when using 0 pos-incrIf you have two identical docs with tokens a b c all zero pos-incr (ie
they occur on the same position), and you run a MultiPhraseQuery with
[a, b] and [c] (all pos incr 0)... then the two docs will get
different scores despite being identical.

Admittedly it's a strange query... but I think the scorer ought to
count the phrase as having tf=1 for each doc.

The problem is that we are missing a tie-breaker for the PhraseQuery
used by ExactPhraseScorer, and so the PQ ends up flip/flopping such
that every other document gets the same score.  Ie, even docIDs all
get one score and odd docIDs all get another score.

Once I added the hard tie-breaker (ord) the scores are the same.

However... there's a separate bug, that can over-count the tf, such
that if I create the MPQ like this:
{noformat}
  mpq.add(new Term[] {new Term(""field"", ""a"")}, 0);
  mpq.add(new Term[] {new Term(""field"", ""b""), new Term(""field"", ""c"")}, 0);
{noformat}

I get tf=2 per doc, but if I create it like this:

{noformat}
  mpq.add(new Term[] {new Term(""field"", ""b""), new Term(""field"", ""c"")}, 0);
  mpq.add(new Term[] {new Term(""field"", ""a"")}, 0);
{noformat}

I get tf=1 (which I think is correct?).

This happens because MultipleTermPositions freely returns the same
position more than once: it just unions the positions of the two
streams, so when both have their term at pos=0, you'll get pos=0
twice, which is not good and leads to over-counting tf.

Unfortunately, I don't see a performant way to fix that... and I'm not
sure that it really matters that much in practice.




"
1,"CharReader should delegate reset/mark/markSupportedThe final class CharReader should delegate reset/mark/markSupported to its wrapped reader. Otherwise clients will get ""reset() not supported"" exception."
1,"lazy fields don't enforce binary vs string valueIf you have a binary field, and load it lazy, and then ask that field
for its stringValue, it will incorrectly give you a String back (and
then will refuse to give a binaryValue).  And, vice-versa."
1,"TestIndexWriterException fails with NPE on realtime{noformat}
   [junit] Testsuite: org.apache.lucene.index.TestIndexWriterExceptions
    [junit] Testcase: testRandomExceptionsThreads(org.apache.lucene.index.TestIndexWriterExceptions):	Caused an ERROR
    [junit] (null)
    [junit] java.lang.NullPointerException
    [junit] 	at org.apache.lucene.index.DocumentsWriterPerThread.prepareFlush(DocumentsWriterPerThread.java:329)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:378)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:512)
    [junit] 	at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:2619)
    [junit] 	at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:2594)
    [junit] 	at org.apache.lucene.index.IndexWriter.prepareCommit(IndexWriter.java:2464)
    [junit] 	at org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:2537)
    [junit] 	at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2519)
    [junit] 	at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2503)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions.testRandomExceptionsThreads(TestIndexWriterExceptions.java:230)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1226)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1154)
    [junit] 
    [junit] 
    [junit] Tests run: 18, Failures: 0, Errors: 1, Time elapsed: 22.548 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterExceptions -Dtestmethod=testRandomExceptionsThreads -Dtests.seed=-5079747362001734044:1572064802119081373
    [junit] WARNING: test method: 'testRandomExceptionsThreads' left thread running: merge thread: _25(4.0):cv2/1 _29(4.0):cv2/1 _20(4.0):cv3/1 into _2m
    [junit] RESOURCE LEAK: test method: 'testRandomExceptionsThreads' left 1 thread(s) running
    [junit] NOTE: test params are: codec=RandomCodecProvider: {content=Pulsing(freqCutoff=2), field=MockSep, id=Pulsing(freqCutoff=2), other=MockSep, contents=SimpleText, content1=MockSep, content2=SimpleText, content4=MockRandom, content5=MockRandom, content6=MockVariableIntBlock(baseBlockSize=41), crash=Standard, content7=MockFixedIntBlock(blockSize=1633)}, locale=en_GB, timezone=Europe/Vaduz
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestToken, TestDateTools, Test2BTerms, TestAddIndexes, TestFilterIndexReader, TestIndexWriterExceptions]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=8,threads=1,free=155417240,total=292945920
    [junit] ------------- ---------------- ---------------
{noformat}"
0,optimize MultiTermEnum/MultiTermDocsOptimize MultiTermEnum and MultiTermDocs to avoid seeks on TermDocs that don't match the term.
0,"Add more methods to manipulate QueryNodeProcessorPipeline elementsQueryNodeProcessorPipeline allows the user to define a list of processors to process a query tree. However, it's not very flexible when the user wants to extend/modify an already created pipeline, because it only provides an add method, which only allows the user to append a new processor to the pipeline.

So, I propose to add new methods to manipulate the processor in a pipeline. I think the methods should not consider an index position when modifying the pipeline, hence the index position in a pipeline does not mean anything, a processor has a meaning when it's after or before another processor. Therefore, I suggest the methods should always consider another processor when inserting/modifying the pipeline. For example, insertAfter(processor, newProcessor), which will insert the ""newProcessor"" after the ""processor""."
0,Improved Kuromoji search mode segmentation/decompoundingKuromoji has a segmentation mode for search that uses a heuristic to promote additional segmentation of long candidate tokens to get a decompounding effect.  This heuristic has been improved.  Patch is coming up.
1,IndexWriter.doAfterFlush not being called when there are no deletions flushedIt should be called when flushing either added docs or deletions.  The fix is trivial.  I'll commit shortly to trunk & 2.3.2.
0,"Cleanup Test TokenStreams so they are reusableMany TokenStreams created in tests are not reusable.  Some do some really messy things which prevent their reuse so we may have to change the tests themselves.

We'll target back porting this to 3x."
0,"Change superclass of TrieRangeQueryThis patch changes the superclass of TrieRangeQuery to ConstantScoreQuery. The current implementation is using rewrite() and was copied from early RangeQueries. But this is not needed, the TrieRangeQuery can easily subclassed from ConstantScoreQuery.

If LUCENE-1345 is solved, the whole TrieRangeQuery can be removed, as TrieRangeFilter can be added to BooleanQueries. The whole TrieRangeQuery class is just a convenience class for easier usage of the trie contrib."
0,"remove contrib/misc and contrib/wordnet's dependencies on analyzers moduleThese contribs don't actually analyze any text.

After this patch, only the contrib/demo relies upon the analyzers module... we can separately try to figure that one out (I don't think any of these lucene contribs needs to reach back into modules/)

"
1,"QueryNodeImpl.containsTag(String) should lowercase the tag keyQueryNodeImpl.containsTag(String key): tag keys are  supposed to be case insensitive, however QueryNodeImpl.containsTag method is considering the case when looking up for tag.

*Bug found by Karsten Fissmer"
0,"Add support for ICU's Normalizer2While there are separate Case Folding, Normalization, and Ignorable-removal filters in LUCENE-1488,
the new ICU Normalizer2 API does this all at once with nfkc_cf (based on the new NFKC_Casefold property in Unicode).

This is great, because it provides a ton of unicode functionality that is really needed.
And the new Normalizer2 API takes CharSequence and writes to Appendable...
"
1,"DeleteByPercentTask hits NPEI'm building up Wiki indices for testing search perf across 3.x/4.0, but hit NPE when creating deletions in 4.0 due to flex cutover..."
0,Allow for specification of spell checker accuracy when calling suggestSimilarThere is really no need for accuracy to be a class variable in the Spellchecker
0,"Make WildcardTermEnum#difference() non-finalThe method WildcardTermEnum#difference() is declared final. I found it very useful to subclass WildcardTermEnum to implement different scoring for exact vs. partial matches. The change is rather trivial (attached)  but I guess it could make life easier for a couple of users.

I attached two patches:
 - one which contains the single change to make difference() non-final (WildcardTermEnum.patch)
 - one which does also contain some minor cleanup of WildcardTermEnum. I removed unnecessary member initialization and made those final. ( WildcardTermEnum_cleanup.patch)

Thanks simon"
1,"RAMInputStream hits false EOF if you seek to EOF then seek back then readBytesTestLazyLoadThreadSafety fails in hudson, possibly an issue with RAMDirectory.
If you hack lucene testcase to return another directory, the same seed will pass."
0,"Test compilation errorThe Lucence test fails with the following error: (latest revision from SVN)

compile-test:
   [mkdir] Created dir: /opt/lucene/lucene/build/classes/test
   [javac] Compiling 79 source files to /opt/lucene/lucene/build/classes/test
   [javac]
/opt/lucene/lucene/src/test/org/apache/lucene/index/TermInfosTest.java:89:
cannot resolve symbol
   [javac] symbol  : constructor TermInfosWriter
(org.apache.lucene.store.Directory,java.lang.String,org.apache.lucene.index.FieldInfos)
   [javac] location: class org.apache.lucene.index.TermInfosWriter
   [javac]     TermInfosWriter writer = new TermInfosWriter(store,
""words"", fis);

I see that TermInfosWriter was changed two days back to add another
argument (interval) to its constructor.

Mailing lists for Lucene do not seem to be responding (sending emails to
subscribe bounces back and also I do not see any mails after March 2nd being
archived in either the user or dev lists). So I am using the bug database to
inform the test failure and submit a simple patch that uses the value of 128
(the default in the TermInfosWriter class) as interval in the test case."
0,Backport FieldCacheTermsFilter code duplication removal to 3.xIn trunk I already cleaned up FieldCacheTermsFilter to not duplicate code of FieldCacheRangeFilter. This issue simply backports this.
0,"improved snowball testingSnowball project has test vocabulary files for each language in their svn repository, along with expected output.

We should use these tests to ensure all languages are working correctly, and it might be helpful in the future for identifying back breaks/changes if we ever want to upgrade snowball, etc.
"
0,"Change AtomicReaderContext.leaves() to return itsself as only leave to simplify code and remove an otherwise unneeded ReaderUtil methodThe documentation of IndexReaderContext.leaves() states that it returns (for convenience) all leave nodes, if the context is top-level (directly got from IndexReader), otherwise returns null. This is not correct for AtomicReaderContext, where it returns null always.

To make it consistent, the convenience method should simply return itsself as only leave for atomic contexts. This makes the utility method ReaderUtil.leaves() obsolete and simplifies code."
0,"Javadocs for Scorer.java and TermScorer.javaJavadocs for Scorer.java and TermScorer.java 
Also changed build.xml to use package access for the 
javadocs target. That caused some minor error javadoc messages 
in CompoundFileReader.java and FieldInfos.java, which are also fixed. 
 
The patch posted earlier for Weight.java 
(a broken javadoc link) is also included. 
 
The attached patch is for all 5 files against the CVS top directory 
of 28 July 2004. The only dependency is that package access 
is needed for TermScorer.java. 
 
This might be changed by declaring TermScorer as a public class, 
but I preferred to use javadoc package access in build.xml 
over changing java code. 
 
Using package access for javadocs shows some more undocumented 
classes, eg. in the doc page of the search package. This might 
encourage folks to write more javadocs... 
 
Regards, 
Paul"
0,"Allow to plug in a Cache Eviction Listener to IndexReader to eagerly clean custom caches that use the IndexReader (getFieldCacheKey)Allow to plug in a Cache Eviction Listener to IndexReader to eagerly clean custom caches that use the IndexReader (getFieldCacheKey).

A spin of: https://issues.apache.org/jira/browse/LUCENE-2468. Basically, its make a lot of sense to cache things based on IndexReader#getFieldCacheKey, even Lucene itself uses it, for example, with the CachingWrapperFilter. FieldCache enjoys being called explicitly to purge its cache when possible (which is tricky to know from the ""outside"", especially when using NRT - reader attack of the clones).

The provided patch allows to plug a CacheEvictionListener which will be called when the cache should be purged for an IndexReader."
1,"score and explain don't matchI've faced this problem recently. I'll attach a program to reproduce the problem soon. The program outputs the following:

{noformat}
** score = 0.10003257
** explain
0.050016284 = (MATCH) product of:
  0.15004885 = (MATCH) sum of:
    0.15004885 = weight(f1:""note book"" in 0), product of:
      0.3911943 = queryWeight(f1:""note book""), product of:
        0.61370564 = idf(f1: note=1 book=1)
        0.6374299 = queryNorm
      0.38356602 = fieldWeight(f1:""note book"" in 0), product of:
        1.0 = tf(phraseFreq=1.0)
        0.61370564 = idf(f1: note=1 book=1)
        0.625 = fieldNorm(field=f1, doc=0)
  0.33333334 = coord(1/3)
{noformat}
"
0,"add UnicodeUtil.nextValidUTF16String In flex branch, TermRef must not contain unpaired surrogates, etc.
But in trunk/previous releases, people could (and do) seek to these.
Also some lucene multitermqueries will generate these invalid seek locations, even now (which we should separately fix)
I think the common case is already handled with a hack in SegmentReader.LegacyTermEnum, but we should clean up this hack and handle all cases.

I would also like to use this nextValidUTF16String in LUCENE-1606, and there might be other places it could be used for better bw compat.
"
0,"Cleanup suggester APICurrently the suggester api and especially TermFreqIterator don't play that nice with BytesRef and other paradigms we use in lucene, further the java iterator pattern isn't that useful when it gets to work with TermsEnum, BytesRef etc. We should try to clean up this api step by step moving over to BytesRef including the Lookup class and its interface..."
1,"Missing a null check in BooleanQuery.toString(String)Our queryParser/tokenizer in some situations creates null query and was added as a clause to Boolean query.
When we try to log the query, NPE is thrown from log(booleanQuery).

In BooleanQuery.toString(String), a simple null check is overlooked.
"
0,"Optimize Memory Use for Short-Lived Indexes (Do not load TermInfoIndex if you know the queries ahead of time)Summary: Provide a way to avoid loading the TermInfoIndex into memory if you know all the terms you are ever going to query.

In our search environment, we have a large number of indexes (many thousands), any of which may be queried by any number of hosts.  These indexes may be very large (~1M document), but since we have a low term/doc ratio, we have 7-11M terms.  With an index interval of 128, that means ~70-90K terms.  On loading the index, it instantiates a Term, a TermInfo, a String, and a char[].  When the document is long lived, this makes some sense because you can quickly search the list of terms using binary search.  However, since we throw away the Indexes very often, a lot of garbage is created per query

Here's an example where we load a large index 10 times.  This corresponds to 7MB of garbage per query.
          percent          live          alloc'ed  stack class
 rank   self  accum     bytes objs     bytes  objs trace name
    1  4.48%  4.48%   4678736 128946  23393680 644730 387749 char[]
    3  3.95% 12.61%   4126272 128946  20631360 644730 387751 org.apache.lucene.index.TermInfo
    6  2.96% 22.71%   3094704 128946  15473520 644730 387748 java.lang.String
    8  1.98% 26.97%   2063136 128946  10315680 644730 387750 org.apache.lucene.index.Term

This adds up after a while.  Since we know exactly which Terms we're going to search for before even opening the index, there's no need to allocate this much memory.  Upon opening the index, we can go through the TII in sequential order and retrieve the entries into the main term dictionary and reduce the storage requirements dramatically.  This reduces the amount of garbage generated by querying by about 60% if you only make 1 query/index with a 77% increase in throughput.

This is accomplished by factoring out the ""index loading"" aspects of TermInfosReader into a new file, SegmentTermInfosReader.  TermInfosReader becomes a base class to allow access to terms.  A new class, PrefetchedTermInfosReader will, upon startup, sort the passed in terms and retrieve the IndexEntries for those terms.  IndexReader and SegmentReader are modified to take new constructor methods that take a Collection of Terms that correspond to the total set of terms that will ever be searched in the life of the index.

In order to support the ""skipping"" behavior, some changes need to be made to SegmentTermEnum: specifically, we need to be able to go back an entry in order to retrieve the previous TermInfo and IndexPointer.  This is because, unlike the normal case, with the index  we want to return the value right before the intended field (so that we can be behind the desired termin the main dictionary).   For example, if we're looking for  ""apple"" in the index,  and the two adjacent values are ""abba"" and ""argon"", we want to return ""abba"" instead of ""argon"".  That way we won't miss any terms in the real index.   This code is confusing; it should probably be moved to an subclass of TermBuffer, but that required more code.  Not wanting to modify TermBuffer to keep it small, also lead to the odd NPE catch in SegmentTermEnum.java.  Stickler for contracts may want to rename SegmentTermEnum.skipTo() to a different name because it implements a different contract: but it would be useful for anyone trying to skip around in the TII, so I figured it was the right thing to do."
0,"Improve LuceneTestCase javadocsNow that the Lucene test-framework javadocs will be published, they should get some attention."
0,"Move TrieRange to coreTrieRange was iterated many times and seems stable now (LUCENE-1470, LUCENE-1582, LUCENE-1602). There is lots of user interest, Solr added it to its default FieldTypes (SOLR-940) and if possible I want to move it to core before release of 2.9.
Before this can be done, there are some things to think about:
# There are now classes called LongTrieRangeQuery, IntTrieRangeQuery, how should they be called in core? I would suggest to leave it as it is. On the other hand, if this keeps our only numeric query implementation, we could call it LongRangeQuery, IntRangeQuery or NumericRangeQuery (see below, here are problems). Same for the TokenStreams and Filters.
# Maybe the pairs of classes for indexing and searching should be moved into one class: NumericTokenStream, NumericRangeQuery, NumericRangeFilter. The problem here: ctors must be able to pass int, long, double, float as range parameters. For the end user, mixing these 4 types in one class is hard to handle. If somebody forgets to add a L to a long, it suddenly instantiates a int version of range query, hitting no results and so on. Same with other types. Maybe accept java.lang.Number as parameter (because nullable for half-open bounds) and one enum for the type.
# TrieUtils move into o.a.l.util? or document or?
# Move TokenStreams into o.a.l.analysis, ShiftAttribute into o.a.l.analysis.tokenattributes? Somewhere else?
# If we rename the classes, should Solr stay with Trie (because there are different impls)?
# Maybe add a subclass of AbstractField, that automatically creates these TokenStreams and omits norms/tf per default for easier addition to Document instances?"
0,"Remove unused importsWith all of the churn recently, now seems like the opportune time to do some import cleanup"
0,"FST should offer lookup-by-output API when output strictly increasesSpinoff from ""FST and FieldCache"" java-dev thread http://lucene.markmail.org/thread/swoawlv3fq4dntvl

FST is able to associate arbitrary outputs with the sorted input keys, but in the special (and, common) case where the function is strictly monotonic (each output only ""increases"" vs prior outputs), such as mapping to term ords or mapping to file offsets in the terms dict, we should offer a lookup-by-output API that efficiently walks the FST and locates input key (exact or floor or ceil) matching that output.
"
1,"Token div exceeds length of provided text sized 4114I have a doc which contains html codes. I want to strip html tags and make the test clear after then apply highlighter on the clear text . But highlighter throws an exceptions if I strip out the html characters  , if i don't strip out , it works fine. It just confuses me at the moment 

I copy paste 3 thing here from the console as it may contain special characters which might cause the problem.


1 -) Here is the html text 

          <h2>Starter</h2>
          <div id=""tab1-content"" class=""tabContent selected"">
            <div class=""head""></div>
            <div class=""body"">
             <div class=""subject-header"">Learning path: History</div>
              <h3>Key question</h3>
              <p>Did transport fuel the industrial revolution?</p>
              <h3>Learning Objective</h3>
	      <ul>
              <li>To categorise points as for or against an argument</li>
              </ul>
	      <p>
              <h3>What to do?</h3>
              <ul>
                <li>Watch the clip: <em>Transport fuelled the industrial revolution.</em></li>
              </ul>
              <p>The clips claims that transport fuelled the industrial revolution. Some historians argue that the industrial revolution only happened because of developments in transport.</p>
			  <ul>
			  	<li>Read the statements below and decide which points are <em>for</em> and which points are <em>against</em> the argument that industry expanded in the 18th and 19th centuries because of developments in transport.</li>
			</ul>
			
			<ol type=""a"">
				<li>Industry expanded because of inventions and the discovery of steam power.</li>
				<li>Improvements in transport allowed goods to be sold all over the country and all over the world so there were more customers to develop industry for.</li>
				<li>Developments in transport allowed resources, such as coal from mines and cotton from America to come together to manufacture products.</li>
				<li>Transport only developed because industry needed it. It was slow to develop as money was spent on improving roads, then building canals and the replacing them with railways in order to keep up with industry.</li>
			</ol>
			
			<p>Now try to think of 2 more statements of your own.</p>
			
            </div>
            <div class=""foot""></div>
          </div>
          <h2>Main activity</h2>
          <div id=""tab2-content"" class=""tabContent"">
            <div class=""head""></div>
            <div class=""body""><div class=""subject-header"">Learning path: History</div>
              <h3>Learning Objective</h3>
              <ul>
                <li>To select evidence to support points</li>
              </ul>
              <h3>What to do?</h3>
              <!--<ul>
                <li>Watch the clip: <em>Windmill and water mill</em></li>
              </ul>-->
              <ul><li>Choose the 4 points that you think are most important - try to be balanced by having two <strong>for</strong> and two <strong>against</strong>.</li>
			  <li>Write one in each of the point boxes of the paragraphs on the sheet <a href=""lp_history_industry_transport_ws1.html"" class=""link-internal"">Constructing a balanced argument</a>.</li></ul> <p>You might like to re write the points in your own words and use connectives to link the paragraphs.</p>
              
			  <p>In history and in any argument, you need evidence to support your points.</p>
			  <ul><li>Find evidence from these sources and from your own knowledge to support each of your points:</li></ul>
			  <ol>
                <li><a href=""../servlet/link?template=vid&macro=setResource&resourceID=2044"" class=""link-internal"">At a toll gate</a></li>
                <li><a href=""../servlet/link?macro=setResource&template=vid&resourceID=2046"" class=""link-internal"">Canals</a></li>
                <li><a href=""../servlet/link?macro=setResource&template=vid&resourceID=2043"" class=""link-internal"">Growing cities: traffic</a></li>
				<li><a href=""../servlet/link?macro=setResource&template=vid&resourceID=2047"" class=""link-internal"">Impact of the railway</a> </li>
				<li><a href=""../servlet/link?macro=setResource&template=vid&resourceID=2048"" class=""link-internal"">Sailing ships</a> </li>
				<li><a href=""../servlet/link?macro=setResource&template=vid&resourceID=2050"" class=""link-internal"">Liverpool: Capital of Culture</a> </li>
              </ol>
			  <p>Try to be specific in your evidence - use named examples of places or people. Use dates if you can.</p>
            </div>
            <div class=""foot""></div>
          </div>
          <h2>Plenary</h2>
          <div id=""tab3-content"" class=""tabContent"">
            <div class=""head""></div>
            <div class=""body""><div class=""subject-header"">Learning path: History</div>
              <h3>Learning Objective</h3>
              <ul>
                <li>To judge which of the arguments is most valid</li>
              </ul>
              <h3>What to do?</h3>
<!--              <ul>
                <li>Watch the clip: <em>Food of the rich</em></li>
              </ul>-->
              <p>In order to be a good historian, and get good marks in exams, you need to show your evaluation skills and make a judgement. Having been through the evidence which point do you think is most important? Why? Is there more evidence? Is the evidence more convincing?</p>
			  <ul><li>In the final box on your worksheet write a conclusion explaining whether on balance the evidence is enough to convince you that transport fuelled the industrial revolution.</li></ul>
            </div>
            <div class=""foot""></div>
          </div>
          <h2>Extension</h2>
          <div id=""tab4-content"" class=""tabContent"">
            <div class=""head""></div>
            <div class=""body""><div class=""subject-header"">Learning path: History</div>
              <h3>What to do?</h3>
              <p>Watch the clip <em>Stress in a ski resort</em></p>
			  <p>New industries, such as tourism, can now be said to be fuelled by transport improvements.</p>
              <ul><li>Search Clipbank, using the Related clip lists as well as the search function, to find examples from around the world of how transport has helped industry.</li></ul>              
            </div>
            <div class=""foot""></div>
          </div>
          
          
2-) here is the text after stripped html tags  out 

           Starter 
           
              
             
              Learning path: History 
               Key question 
               Did transport fuel the industrial revolution? 
               Learning Objective 
	       
               To categorise points as for or against an argument 
               
	       
               What to do? 
               
                 Watch the clip:  Transport fuelled the industrial revolution.  
               
               The clips claims that transport fuelled the industrial revolution. Some historians argue that the industrial revolution only happened because of developments in transport. 
			   
			  	 Read the statements below and decide which points are  for  and which points are  against  the argument that industry expanded in the 18th and 19th centuries because of developments in transport. 
			 
			
			 
				 Industry expanded because of inventions and the discovery of steam power. 
				 Improvements in transport allowed goods to be sold all over the country and all over the world so there were more customers to develop industry for. 
				 Developments in transport allowed resources, such as coal from mines and cotton from America to come together to manufacture products. 
				 Transport only developed because industry needed it. It was slow to develop as money was spent on improving roads, then building canals and the replacing them with railways in order to keep up with industry. 
			 
			
			 Now try to think of 2 more statements of your own. 
			
             
              
           
           Main activity 
           
              
              Learning path: History 
               Learning Objective 
               
                 To select evidence to support points 
               
               What to do? 
               
                Choose the 4 points that you think are most important - try to be balanced by having two  for  and two  against . 
			   Write one in each of the point boxes of the paragraphs on the sheet  Constructing a balanced argument .    You might like to re write the points in your own words and use connectives to link the paragraphs. 
              
			   In history and in any argument, you need evidence to support your points. 
			    Find evidence from these sources and from your own knowledge to support each of your points:  
			   
                  At a toll gate  
                  Canals  
                  Growing cities: traffic  
				  Impact of the railway   
				  Sailing ships   
				  Liverpool: Capital of Culture   
               
			   Try to be specific in your evidence - use named examples of places or people. Use dates if you can. 
             
              
           
           Plenary 
           
              
              Learning path: History 
               Learning Objective 
               
                 To judge which of the arguments is most valid 
               
               What to do? 
 
               In order to be a good historian, and get good marks in exams, you need to show your evaluation skills and make a judgement. Having been through the evidence which point do you think is most important? Why? Is there more evidence? Is the evidence more convincing? 
			    In the final box on your worksheet write a conclusion explaining whether on balance the evidence is enough to convince you that transport fuelled the industrial revolution.  
             
              
           
           Extension 
           
              
              Learning path: History 
               What to do? 
               Watch the clip  Stress in a ski resort  
			   New industries, such as tourism, can now be said to be fuelled by transport improvements. 
                Search Clipbank, using the Related clip lists as well as the search function, to find examples from around the world of how transport has helped industry.                
             
              
           
          
         3-) here is the exception I get

org.apache.lucene.search.highlight.InvalidTokenOffsetsException: Token div exceeds length of provided text sized 4114
	at org.apache.lucene.search.highlight.Highlighter.getBestTextFragments(Highlighter.java:228)
	at org.apache.lucene.search.highlight.Highlighter.getBestFragments(Highlighter.java:158)
	at org.apache.lucene.search.highlight.Highlighter.getBestFragments(Highlighter.java:462)

"
0,"new Arabic Analyzer (Apache license)I've noticed there is no Arabic analyzer for Lucene, most likely because Tim Buckwalter's morphological dictionary is GPL.

However, it is not necessary  to have full morphological analysis engine for a quality arabic search. 
This implementation implements the light-8s algorithm present in the following paper: http://ciir.cs.umass.edu/pubfiles/ir-249.pdf

As you can see from the paper, improvement via this method over searching surface forms (as lucene currently does) is significant, with almost 100% improvement in average precision.

While I personally don't think all the choices were the best, and some easily improvements are still possible, the major motivation for implementing it exactly the way it is presented in the paper is that the algorithm is TREC-tested, so the precision/recall improvements to lucene are already documented.

For a stopword list, I used a list present at http://members.unine.ch/jacques.savoy/clef/index.html simply because the creator of this list documents the data as BSD-licensed.

This implementation (Analyzer) consists of above mentioned stopword list plus two filters:
 ArabicNormalizationFilter: performs orthographic normalization (such as hamza seated on alif, alif maksura, teh marbuta, removal of harakat, tatweel, etc)
 ArabicStemFilter: performs arabic light stemming

Both filters operate directly on termbuffer for maximum performance. There is no object creation in this Analyzer.

There are no external dependencies. I've indexed about half a billion words of arabic text and tested against that.

If there are any issues with this implementation I am willing to fix them. I use lucene on a daily basis and would like to give something back. Thanks.
"
0,"AttributeSource.addAttribute should only accept interfaces, the missing test leads to problems with Token.TOKEN_ATTRIBUTE_FACTORYThis is a blocker, because you can call addAttribute(Token.class) without getting an error message.

I will commit the fix and restart the vote for 3.0. This also applies to 2.9, but there is no Token Attribute Factory. But I will merge to 2.9, too, if a 2.9.2 comes."
0,"Make term offsets work in MemoryIndexFix the logic for retrieving term offsets from DocsAndPositionsEnum on a MemoryIndex, and allow subclasses to access them."
1,"You cannot sort on fields that don't existWhile it's possible to search for fields that don't exist (you'll get 0 hits),  
you'll get an exception if you try to sort by a field that has no values. The  
exception is this:  
  
if (termEnum.term() == null) {  
  throw new RuntimeException (""no terms in field "" + field);  
}  
  
I'll attach a change suggested by Yonik Seeley that removes this exception. 
 
Also, the if-condition above is incomplete anyway, so currently the exception 
is not always thrown (as termEnum .term() might well be != null but point to a 
term in a different field already)"
0,"nightly build/javadocs for sandboxThis isn't something i think is crucial, but since i've been on the lucene-users
mailing list (less then 2 months) I've seen several people post questions asking
where they can find documentation on some module available in the sandbox.

the answer of course is usually that they should download the source and build
the javadocs themselves, but since it keeps coming up, I figured i'd suggest
setting up a nightly ""build"" of the whole sandbox, including the javadocs.

if nothing else, it will cut down on the number of questions -- but i think it
may also have an added benefit to the size of the user base.  In my experience,
people tend to be more willing to download/install something and try it out
after they've read the docs online."
1,"BooleanScorer2 fails to update this.doc when its the top scorerWhen BooleanScorer2 runs the top collection loop (one of its
score(Collector)) methods, it uses a local ""doc"" var, ie:

{code}
public void score(Collector collector) throws IOException {
    collector.setScorer(this);
    int doc;
    while ((doc = countingSumScorer.nextDoc()) != NO_MORE_DOCS) {
      collector.collect(doc);
    }
}
{code}

The problem is, if the child collector calls scorer.doc() it will
always get -1.  Most Collectors don't actually call scorer.doc(), but
one important one that does is ScoreCachingWrapperScorer, as it uses
the doc to know when to invalidate its cache.  Since this always
returns -1, the ScoreCachingWrapperScorer keeps returning score=0.0 to
its caller, thus messing up a SortField.SCORE comparator instance if
it's included in the sort fields.
"
1,"TestOmitTf.testMixedMerge random seed failureVersion: trunk r1091638

ant test -Dtests.seed=-6595054217575280191:5576532348905930588


    [junit] ------------- Standard Error -----------------
    [junit] WARNING: test method: 'testDeMorgan' left thread running: Thread[NRT search threads-1691-thread-2,5,main]
    [junit] RESOURCE LEAK: test method: 'testDeMorgan' left 1 thread(s) running
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestBooleanQuery -Dtestmethod=testDeMorgan -Dtests.seed=-6595054217575280191:5576532348905930588
    [junit] ------------- ---------------- ---------------
    [junit] Testsuite: org.apache.lucene.index.TestNorms
    [junit] Tests run: 2, Failures: 0, Errors: 0, Time elapsed: 5.064 sec
    [junit] 
    [junit] Testsuite: org.apache.lucene.index.TestOmitTf
    [junit] Testcase: testMixedMerge(org.apache.lucene.index.TestOmitTf):	Caused an ERROR
    [junit] CheckIndex failed
    [junit] java.lang.RuntimeException: CheckIndex failed
    [junit] 	at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:152)
    [junit] 	at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:138)
    [junit] 	at org.apache.lucene.index.TestOmitTf.testMixedMerge(TestOmitTf.java:155)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1232)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1160)
    [junit] 
    [junit] 
    [junit] Tests run: 5, Failures: 0, Errors: 1, Time elapsed: 0.851 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] CheckIndex failed
    [junit] Segments file=segments_1 numSegments=1 version=FORMAT_4_0 [Lucene 4.0]
    [junit]   1 of 1: name=_12 docCount=60
    [junit]     codec=SegmentCodecs [codecs=[MockRandom, MockVariableIntBlock(baseBlockSize=112)], provider=RandomCodecProvider: {f1=MockRandom, f2=MockVariableIntBlock(baseBlockSize=112)}]
    [junit]     compound=false
    [junit]     hasProx=false
    [junit]     numFiles=16
    [junit]     size (MB)=0,01
    [junit]     diagnostics = {optimize=true, mergeFactor=2, os.version=2.6.37-gentoo, os=Linux, lucene.version=4.0-SNAPSHOT, source=merge, os.arch=amd64, java.version=1.6.0_24, java.vendor=Sun Microsystems Inc.}
    [junit]     no deletions
    [junit]     test: open reader.........OK
    [junit]     test: fields..............OK [2 fields]
    [junit]     test: field norms.........OK [2 fields]
    [junit]     test: terms, freq, prox...ERROR: java.io.IOException: Read past EOF
    [junit] java.io.IOException: Read past EOF
    [junit] 	at org.apache.lucene.store.RAMInputStream.switchCurrentBuffer(RAMInputStream.java:90)
    [junit] 	at org.apache.lucene.store.RAMInputStream.readByte(RAMInputStream.java:63)
    [junit] 	at org.apache.lucene.store.MockIndexInputWrapper.readByte(MockIndexInputWrapper.java:105)
    [junit] 	at org.apache.lucene.store.DataInput.readVInt(DataInput.java:94)
    [junit] 	at org.apache.lucene.index.codecs.sep.SepSkipListReader.readSkipData(SepSkipListReader.java:188)
    [junit] 	at org.apache.lucene.index.codecs.MultiLevelSkipListReader.loadNextSkip(MultiLevelSkipListReader.java:142)
    [junit] 	at org.apache.lucene.index.codecs.MultiLevelSkipListReader.skipTo(MultiLevelSkipListReader.java:112)
    [junit] 	at org.apache.lucene.index.codecs.sep.SepPostingsReaderImpl$SepDocsEnum.advance(SepPostingsReaderImpl.java:454)
    [junit] 	at org.apache.lucene.index.CheckIndex.testTermIndex(CheckIndex.java:782)
    [junit] 	at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:495)
    [junit] 	at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:148)
    [junit] 	at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:138)
    [junit] 	at org.apache.lucene.index.TestOmitTf.testMixedMerge(TestOmitTf.java:155)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit] 	at java.lang.reflect.Method.invoke(Method.java:597)
    [junit] 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit] 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit] 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit] 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit] 	at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1232)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1160)
    [junit] 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    [junit] 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    [junit] 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    [junit] 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    [junit] 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    [junit] 	at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:422)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:931)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:758)
    [junit]     test: stored fields.......OK [60 total field count; avg 1 fields per doc]
    [junit]     test: term vectors........OK [120 total vector count; avg 2 term/freq vector fields per doc]
    [junit] FAILED
    [junit]     WARNING: fixIndex() would remove reference to this segment; full exception:
    [junit] java.lang.RuntimeException: Term Index test failed
    [junit] 	at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:508)
    [junit] 	at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:148)
    [junit] 	at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:138)
    [junit] 	at org.apache.lucene.index.TestOmitTf.testMixedMerge(TestOmitTf.java:155)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit] 	at java.lang.reflect.Method.invoke(Method.java:597)
    [junit] 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit] 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit] 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit] 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit] 	at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1232)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1160)
    [junit] 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    [junit] 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    [junit] 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    [junit] 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    [junit] 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    [junit] 	at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:422)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:931)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:758)
    [junit] 
    [junit] WARNING: 1 broken segments (containing 60 documents) detected
    [junit] 
    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestOmitTf -Dtestmethod=testMixedMerge -Dtests.seed=-6595054217575280191:5576532348905930588
    [junit] NOTE: test params are: codec=RandomCodecProvider: {noTf=MockSep, tf=Standard, f1=MockRandom, f2=MockVariableIntBlock(baseBlockSize=112)}, locale=cs_CZ, timezone=Chile/Continental
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestAssertions, TestCachingTokenFilter, TestDocument, TestDirectoryReader, TestFlex, TestIndexWriterConfig, TestIndexWriterMerging, TestIndexWriterOnJRECrash, TestMultiReader, TestNewestSegment, TestNorms, TestOmitTf]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=8,threads=1,free=94021800,total=126484480
    [junit] ------------- ---------------- ---------------"
1,"EnglishPossessiveFilter should work with Unicode right single quotation markThe current EnglishPossessiveFilter (used in EnglishAnalyzer) removes possessives using only the '\'' character (plus 's' or 'S'), but some common systems (German?) insert the Unicode ""\u2019"" (RIGHT SINGLE QUOTATION MARK) instead and this is not removed when processing UTF-8 text. I propose to change EnglishPossesiveFilter to support '\u2019' as an alternative to '\''."
1,"TestIndexWriterOnDiskFull.testAddDocumentOnDiskFull seed failureversion: trunk r1155278
reproduce-able: always

{code}
    [junit] Testsuite: org.apache.lucene.index.TestIndexWriterOnDiskFull
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.847 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterOnDiskFull -Dtestmethod=testAddDocumentOnDiskFull -Dtests.seed=-3cc23002ebad518d:70ae722281b31c9f:57406021f8789a22
    [junit] NOTE: test params are: codec=RandomCodecProvider: {content=MockFixedIntBlock(blockSize=1081)}, locale=hr_HR, timezone=Atlantic/Jan_Mayen
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestIndexWriterOnDiskFull]
    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_26 (64-bit)/cpus=8,threads=1,free=85252968,total=125632512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testAddDocumentOnDiskFull(org.apache.lucene.index.TestIndexWriterOnDiskFull):     Caused an ERROR
    [junit] no segments* file found in MockDirWrapper(org.apache.lucene.store.RAMDirectory@65dcc2a3 lockFactory=MockLockFactoryWrapper(org.apache.lucene.store.SingleInstanceLockFactory@6e8f94)): files: [_1.cfs, _1.cfe, _0.cfe, _0.cfs]
    [junit] org.apache.lucene.index.IndexNotFoundException: no segments* file found in MockDirWrapper(org.apache.lucene.store.RAMDirectory@65dcc2a3 lockFactory=MockLockFactoryWrapper(org.apache.lucene.store.SingleInstanceLockFactory@6e8f94)): files: [_1.cfs, _1.cfe, _0.cfe, _0.cfs]
    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:657)
    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:534)
    [junit]     at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:284)
    [junit]     at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:277)
    [junit]     at org.apache.lucene.index.TestIndexWriter.assertNoUnreferencedFiles(TestIndexWriter.java:158)
    [junit]     at org.apache.lucene.index.TestIndexWriterOnDiskFull.testAddDocumentOnDiskFull(TestIndexWriterOnDiskFull.java:114)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1526)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1428)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.index.TestIndexWriterOnDiskFull FAILED
{code}

"
0,"Share the Term -> TermInfo cache across threadsRight now each thread creates its own (thread private) SimpleLRUCache,
holding up to 1024 terms.

This is rather wasteful, since if there are a high number of threads
that come through Lucene, you're multiplying the RAM usage.  You're
also cutting way back on likelihood of a cache hit (except the known
multiple times we lookup a term within-query, which uses one thread).
In NRT search we open new SegmentReaders (on tiny segments) often
which each thread must then spend CPU/RAM creating & populating.

Now that we are on 1.5 we can use java.util.concurrent.*, eg
ConcurrentHashMap.  One simple approach could be a double-barrel LRU
cache, using 2 maps (primary, secondary).  You check the cache by
first checking primary; if that's a miss, you check secondary and if
you get a hit you promote it to primary.  Once primary is full you
clear secondary and swap them.

Or... any other suggested approach?
"
1,"IW.commit() writes but fails to fsync the N.fnx fileIn making a unit test for NRTCachingDir (LUCENE-3092) I hit this surprising bug!

Because the new N.fnx file is written at the ""last minute"" along with the segments file, it's not included in the sis.files() that IW uses to figure out which files to sync.

This bug means one could call IW.commit(), successfully, return, and then the machine could crash and when it comes back up your index could be corrupted.

We should hopefully first fix TestCrash so that it hits this bug (maybe it needs more/better randomization?), then fix the bug...."
0,"Remove ""synchonized"" from FuzzyTermEnum#similarity(final String target)The similarity method in FuzzyTermEnum is synchronized which is stupid because of:
- TermEnums are the iterator pattern and so are single-thread per definition
- The method is private, so nobody could ever create a fake FuzzyTermEnum just to have this method and use it multithreaded.
- The method is not static and has no static fields - so instances do not affect each other

The root of this comes from LUCENE-296, but was never reviewd and simply committed. The argument for making it synchronized is wrong."
0,"backport suggest module to branch 3.xIt would be nice to develop a plan to expose the autosuggest functionality to Lucene users in 3.x

There are some complications, such as seeing if we can backport the FST-based functionality,
which might require a good bit of work. But I think this would be well-worth it.
"
0,"Create a MaxFieldLengthAnalyzer to wrap any other Analyzer and provide the same functionality as MaxFieldLength provided on IndexWriterA spinoff from LUCENE-2294. Instead of asking the user to specify on IndexWriter his requested MFL limit, we can get rid of this setting entirely by providing an Analyzer which will wrap any other Analyzer and its TokenStream with a TokenFilter that keeps track of the number of tokens produced and stop when the limit has reached.

This will remove any count tracking in IW's indexing, which is done even if I specified UNLIMITED for MFL.

Let's try to do it for 3.1."
1,"[PATCH] npe if java.io.tmpdir does not existIn org.apache.lucene.store.FSDirectory from Lucene-1.3-final, on line 170-171:

File tmpdir = new File(System.getProperty(""java.io.tmpdir""));
files = tmpdir.list();

if the directory specified by the property ""java.io.tmpdir"" does not exist, a
null pointer exception is thrown.  Perhaps a check to see if the directory
exists is in order, and if it doesn't, use a directory you know exists (e.g. a
/temp directory in the directory created earlier in the create() method)."
1,SynFilter doesn't set offsets for outputs that hang off the end of the input tokensIf you have syn rule a -> x y and input a then output is a/x y but... what should y's offsets be?  Right now we set to 0/0.
0,"remove old static main methods in coreWe have a few random static main methods that I think are very rarely used... we should remove them (IndexReader, UTF32ToUTF8, English).

The IndexReader main lets you list / extract the sub-files from a CFS... I think we should move this to a new tool in contrib/misc."
1,StringHelper#stringDifference is wrong about supplementary chars StringHelper#stringDifference does not take supplementary characters into account. Since this is not used internally at all we should think about removing it but I guess since it is not too complex we should just or fix it for bwcompat reasons. For released versions we should really fix it since folks might use it though. For trunk we could just drop it.
0,"DirectoryTaxonomyWriter extensions should be able to set internal index writer config attributes such as info streamCurrent protected openIndexWriter(Directory directory, OpenMode openMode) does not provide access to the IWC it creates.
So extensions must reimplement this method completely in order to set e.f. info stream for the internal index writer.
This came up in [user question: Taxonomy indexer debug |http://lucene.472066.n3.nabble.com/Taxonomy-indexer-debug-td3533341.html]"
0,"Improve speed of ThaiWordFilter by CharacterIterator, factor out LowerCasing and also fix some bugs (empty tokens stop iteration)The ThaiWordFilter creates new Strings out of term buffer before passing to The BreakIterator., But BreakIterator can take a CharacterIterator and directly process on it without buffer copying.
As Java itsself does not provide a CharacterIterator implementation in java.text, we can use the javax.swing.text.Segment class, that operates on a char[] and is even reuseable! This class is very strange but it works and is in JDK 1.4+ and not deprecated.

The filter also had a bug: It stopped iterating tokens when an empty token occurred. Also the lowercasing for non-thai words was removed and put into the Analyzer by adding LowerCaseFilter."
0,"French elision filter should use CharArraySetFrench elision filter creates new strings, lowercases them, etc just to check against a Set<String>.
trivial patch to use chararrayset instead."
0,"GData Server - TestCase Deadlock  StorageModifierSolfed the racecondition deadlock while closing the StorageController.
This occured the first time hossman tried to run the test cases. 

Concurrent Modification Exception while iteration over a collection in a sepereate thread -- ModifiedEntryFilter replaced list with array.

@Hossman if you can get to it, could you try the testcases again.

@all If you guys do have time you could run the testcases on different environment, that would help to resolve bugs in the test cases and the server.

simon"
0,"GData's TestGdataIndexer.testDestroy() intermittently hits spin loop & causes build timeoutSeveral nightly builds (at least #136, #143 and #144) have failed due
to timeout at 45 minutes while running the TestGdataIndexer.testDestroy()
test case.

I tracked it down to this line:

      // wait active for the commit
      while(this.indexer.writer != null){}

Intermittently, that while loop will spin forever.  I can only get the
failure to happen on Linux: it doesn't happen on Mac OS X (haven't
tried windows).  The nightly build runs on Solaris 10, so it also
happens there.

It turns out, this is due to the fact that ""writer"" is not declared as
""volatile"".  This is because one thread is closing the indexer, which
sets writer to null, but another thread is running the while loop.
If this.indexer.writer was set to null before that while loop starts,
the test will run through fine; else, it won't.

I plan to fix this by adding this method to GDataIndexer class:

    // Used only for testing
    protected synchronized IndexWriter getWriter() {
      return this.writer;
    }

and changing unit test to call that method."
0,"Reduce usage of String.intern(), performance is terribleI profiled a simple MatchAllDocsQuery() against ~1.5 million documents (8 fields of short text, Field.Store.YES,Field.Index.NOT_ANALYZED_NO_NORMS), then retrieved all documents via searcher.doc(i, fs). String.intern() showed up as a top hotspot (see attached screenshot), so i implemented a small optimization to not intern() for every new Field(), instead forcing the intern in the FieldInfos class and adding a optional ""internName"" constructor to Field. This reduced execution time for searching and iterating through all documents by 35%. Results were similar for -server and -client.


TRUNK (2.9) w/out patch: matched 1435563 in 8884 ms/search
TRUNK (2.9) w/patch: matched 1435563 in 5786 ms/search"
1,"Memory leak when sortingThis is the same post I sended two days before to the Lucene user's list. This 
bug seems to have something in common with bug no. 30628 but that bug is closed 
as invalid.

I'm sending test code that everyone can try. The code is singular, don't say 
there is no sense in reopening the same index. I can only show, that reopening 
leaks memory. The index is filled by pseudo-real data, they aren't significant 
and the process of index creation as well. 

The problem must be in field caching code used by sort.

Affected versions of Lucene:
1.4.1
CVS 1.5-rc1-dev

This code survives only few first iterations if you run java with -Xmx5m. With 
Lucene 1.4-final ends regulary.

import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.document.Document;
import org.apache.lucene.document.Field;
import org.apache.lucene.index.IndexReader;
import org.apache.lucene.index.IndexWriter;
import org.apache.lucene.index.Term;
import org.apache.lucene.search.Hits;
import org.apache.lucene.search.IndexSearcher;
import org.apache.lucene.search.Searcher;
import org.apache.lucene.search.Sort;
import org.apache.lucene.search.SortField;
import org.apache.lucene.search.TermQuery;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.RAMDirectory;

import java.io.IOException;
import java.text.SimpleDateFormat;
import java.util.Calendar;
import java.util.Date;

/**
 * Run this test with Lucene 1.4.1 and -Xmx5m
 */
public class ReopenTest
{
    private static long mem_last = 0;

    public static void main(String[] args) throws IOException
    {
        Directory directory = create_index();

        for (int i = 1; i < 100; i++) {
            System.err.println(""loop "" + i + "", index version: "" + IndexReader.
getCurrentVersion(directory));
            search_index(directory);
            add_to_index(directory, i);
        }
    }

    private static void add_to_index(Directory directory, int i) throws 
IOException
    {
        IndexWriter writer = new IndexWriter(directory, new StandardAnalyzer(), 
false);

        SimpleDateFormat df = new SimpleDateFormat(""yyyy-MM-dd"");
        Document doc = new Document();

        doc.add(Field.Keyword(""date"", 
          df.format(new Date(System.currentTimeMillis()))));
        doc.add(Field.Keyword(""id"", ""CD"" + String.valueOf(i)));
        doc.add(Field.Text(""text"", ""Tohle neni text "" + i));
        writer.addDocument(doc);

        System.err.println(""index size: "" + writer.docCount());
        writer.close();
    }

    private static void search_index(Directory directory) throws IOException
    {
        IndexReader reader = IndexReader.open(directory);
        Searcher searcher = new IndexSearcher(reader);

        print_mem(""search 1"");
        SortField[] fields = new SortField[2];
        fields[0] = new SortField(""date"", SortField.STRING, true);
        fields[1] = new SortField(""id"", SortField.STRING, false);
        Sort sort = new Sort(fields);
        TermQuery query = new TermQuery(new Term(""text"", ""\""text 5\""""));

        print_mem(""search 2"");
        Hits hits = searcher.search(query, sort);
        print_mem(""search 3"");

        for (int i = 0; i < hits.length(); i++) {
            Document doc = hits.doc(i);
            System.out.println(""doc "" + i + "": "" + doc.toString());
        }
        print_mem(""search 4"");
        searcher.close();
        reader.close();
    }

    private static void print_mem(String log)
    {
        long mem_free = Runtime.getRuntime().freeMemory();
        long mem_total = Runtime.getRuntime().totalMemory();
        long mem_max = Runtime.getRuntime().maxMemory();

        long delta = (mem_last - mem_free) * -1;

        System.out.println(log + ""= delta: "" + delta + "", free: "" + mem_free + 
"", used: "" + (mem_total-mem_free) + "", total: "" + mem_total + "", max: "" + 
mem_max);

        mem_last = mem_free;
    }

    private static Directory create_index() throws IOException
    {
        print_mem(""create 1"");
        Directory directory = new RAMDirectory();

        Calendar c = Calendar.getInstance();
        SimpleDateFormat df = new SimpleDateFormat(""yyyy-MM-dd"");
        IndexWriter writer = new IndexWriter(directory, new StandardAnalyzer(), 
true);
        for (int i = 0; i < 365 * 15; i++) {
            Document doc = new Document();

            doc.add(Field.Keyword(""date"", 
               df.format(new Date(c.getTimeInMillis()))));
            doc.add(Field.Keyword(""id"", ""AB"" + String.valueOf(i)));
            doc.add(Field.Text(""text"", ""Tohle je text "" + i));
            writer.addDocument(doc);

            doc = new Document();

            doc.add(Field.Keyword(""date"", 
               df.format(new Date(c.getTimeInMillis()))));
            doc.add(Field.Keyword(""id"", ""ef"" + String.valueOf(i)));
            doc.add(Field.Text(""text"", ""Je tohle text "" + i));
            writer.addDocument(doc);

            c.add(Calendar.DAY_OF_YEAR, 1);
        }
        writer.optimize();
        System.err.println(""index size: "" + writer.docCount());
        writer.close();

        print_mem(""create 2"");
        return directory;
    }
}"
1,"TestUTF32ToUTF8 fails on IBM's JREThis is because AutomatonTestUtil.RandomAcceptedString is returning an invalid UTF32 int[] -- it has an unpaired surrogate, and IBM's JRE handles this differently than Oracle's."
0,"spellchecker cleanupSome cleanup, attached here so it can be tracked if necessary: javadoc improvements; don't print exceptions to stderr but re-throw them; new constructor for a new test case. I will commit this soon."
0,Include all CHANGES.txt under contribWe already include to root CHANGES.txt but fail to include the ones under contrib.
0,"Remove usage of deprecated method Document.fields()The classes DocumentWriter, FieldsWriter, and ParallelReader use the deprecated method Document.fields(). This simple patch changes these three classes to use Document.getFields() instead.

All unit tests pass."
